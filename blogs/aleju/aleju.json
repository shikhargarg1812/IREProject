{
    "0": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Deep_Clustering_for_Unsupervised_Learning_of_Visual_Features.md",
        "transcript": "\nWhat\n\nThey describe a strategy to train CNNs in unsupervised fashion.\nThe method is based on iterating k-means clustering in feature space, followed by training the CNN to predict the cluster labels.\nThe method achieves sizeable improvements over the state of the art in unsupervised learning on ImageNet.\n\n They describe a strategy to train CNNs in unsupervised fashion. The method is based on iterating k-means clustering in feature space, followed by training the CNN to predict the cluster labels. The method achieves sizeable improvements over the state of the art in unsupervised learning on ImageNet. \nHow\n\nMethod\n\nThey start with a CNN (not pretrained)\nThey then iterate the following steps:\n\nThey apply the CNN to their dataset, converting each image to a feature vector. (At the start, this will be mostly noise.)\nThey apply k-means to these feature vectors.\nThey use the resulting clustering as (pseudo-)labels for the images.\nThey train the CNN for some batches to predict these (pseudo-)labels.\nThey start again at (1).\n\n\nVisualization:\n\n\n\n\n\n\nAvoiding trivial solutions\n\nEmpty Clusters\n\nWhile k-means is iterating, some clusters may become empty.\nIf that happens, they move the centroid of such a cluster (e.g. call it \"cluster A\" here) to another cluster (e.g. \"cluster B\"), where B must have associated feature vectors.\nAfter moving, they perturb the centroid of cluster A's location by a small amount.\nThey then split the feature vectors of cluster B between A and B.\n\n\nTrivial Parameterization\n\nIf one cluster becomes dominating, i.e. if most of the feature vectors are associated to it,\nthe CNN can start to learn trivial parameterizations that only focus on differentiating between that one cluster and everything else,\nbecause.\nThey avoid this by sampling images based on a uniform distribution over the (pseudo-)labels.\n\n\n\n\nOther stuff\n\nTheir test their methods mainly with AlexNet. Also a bit with VGG. (Both modified to use BN.)\nThey update the clusters once per epoch on ImageNet.\nThey train on ImageNet for 500 epochs.\nThey apply a Sobel-based filter before their models (i.e. they do not get colors as inputs).\n\n\n\n Method\n\nThey start with a CNN (not pretrained)\nThey then iterate the following steps:\n\nThey apply the CNN to their dataset, converting each image to a feature vector. (At the start, this will be mostly noise.)\nThey apply k-means to these feature vectors.\nThey use the resulting clustering as (pseudo-)labels for the images.\nThey train the CNN for some batches to predict these (pseudo-)labels.\nThey start again at (1).\n\n\nVisualization:\n\n\n\n\n\n They start with a CNN (not pretrained) They then iterate the following steps:\n\nThey apply the CNN to their dataset, converting each image to a feature vector. (At the start, this will be mostly noise.)\nThey apply k-means to these feature vectors.\nThey use the resulting clustering as (pseudo-)labels for the images.\nThey train the CNN for some batches to predict these (pseudo-)labels.\nThey start again at (1).\n\n They apply the CNN to their dataset, converting each image to a feature vector. (At the start, this will be mostly noise.) They apply k-means to these feature vectors. They use the resulting clustering as (pseudo-)labels for the images. They train the CNN for some batches to predict these (pseudo-)labels. They start again at (1). Visualization:\n\n\n\n  Avoiding trivial solutions\n\nEmpty Clusters\n\nWhile k-means is iterating, some clusters may become empty.\nIf that happens, they move the centroid of such a cluster (e.g. call it \"cluster A\" here) to another cluster (e.g. \"cluster B\"), where B must have associated feature vectors.\nAfter moving, they perturb the centroid of cluster A's location by a small amount.\nThey then split the feature vectors of cluster B between A and B.\n\n\nTrivial Parameterization\n\nIf one cluster becomes dominating, i.e. if most of the feature vectors are associated to it,\nthe CNN can start to learn trivial parameterizations that only focus on differentiating between that one cluster and everything else,\nbecause.\nThey avoid this by sampling images based on a uniform distribution over the (pseudo-)labels.\n\n\n\n Empty Clusters\n\nWhile k-means is iterating, some clusters may become empty.\nIf that happens, they move the centroid of such a cluster (e.g. call it \"cluster A\" here) to another cluster (e.g. \"cluster B\"), where B must have associated feature vectors.\nAfter moving, they perturb the centroid of cluster A's location by a small amount.\nThey then split the feature vectors of cluster B between A and B.\n\n While k-means is iterating, some clusters may become empty. If that happens, they move the centroid of such a cluster (e.g. call it \"cluster A\" here) to another cluster (e.g. \"cluster B\"), where B must have associated feature vectors. After moving, they perturb the centroid of cluster A's location by a small amount. They then split the feature vectors of cluster B between A and B. Trivial Parameterization\n\nIf one cluster becomes dominating, i.e. if most of the feature vectors are associated to it,\nthe CNN can start to learn trivial parameterizations that only focus on differentiating between that one cluster and everything else,\nbecause.\nThey avoid this by sampling images based on a uniform distribution over the (pseudo-)labels.\n\n If one cluster becomes dominating, i.e. if most of the feature vectors are associated to it,\nthe CNN can start to learn trivial parameterizations that only focus on differentiating between that one cluster and everything else,\nbecause. They avoid this by sampling images based on a uniform distribution over the (pseudo-)labels. Other stuff\n\nTheir test their methods mainly with AlexNet. Also a bit with VGG. (Both modified to use BN.)\nThey update the clusters once per epoch on ImageNet.\nThey train on ImageNet for 500 epochs.\nThey apply a Sobel-based filter before their models (i.e. they do not get colors as inputs).\n\n Their test their methods mainly with AlexNet. Also a bit with VGG. (Both modified to use BN.) They update the clusters once per epoch on ImageNet. They train on ImageNet for 500 epochs. They apply a Sobel-based filter before their models (i.e. they do not get colors as inputs). \nResults\n\nImageNet\n\nThey measure the normalized mutual information between cluster assignments and ImageNet labels.\nThis mutual information increases over time, indicating that the cluster assignments matches more and more the labels (which were not used during training).\nThey measure the normalized mutual information of cluster assignments between each pair of two consecutive epochs.\nThis mutual information increases over time, indicating that the clustering becomes more stable.\nThey measure the achieved mAP on Pascal VOC 2007 (I guess they use the model as pretrained weights?)\nbased on the number of clusters in k-means.\nThey achieve best results with 10,000 clusters.\nAccuracy per layer\n\nAfter training they place a linear classifier on top of their model and train it on the labels. They do repeat this for each layer to get the accuracy per layer.\nThey significantly outperform all competing methods (by several percentage points, depending on layer).\nAt conv5 (max conv layer in AlexNet) they are beaten by supervised learning by ~12 points (50.5 vs 38.2).\nThe difference is lower for conv3 (44.2 vs 41.0).\nThey also compare models pretrained on ImageNet, with the linear classifier fine-tuned on the Places dataset.\nIn this case they can keep up more with the model pretrained in supervised fashion (38.7 vs 34.7 at conv5, 39.4 vs 39.8 at conv4).\n\n\nStats:\n\n\n\n\n\n\nYFCC100M\n\nThey train on ImageNet and then search on YFCC100M for images leading to high activations in filters.\nImages with high activations and generated \"ideal\" images for that filter:\n\n\n\n\nImages with high activations for filters in layer conv5:\n\n\nThey note that some filters in conv5 seem to just re-learn patterns that are already covered by filters in lower layers.\n\n\n\n\nPascal VOC\n\nThey pretrain on ImageNet and then finetune on Pascal VOC for classification, object detection and segmentation.\nThey are still behind a model pretrained with labels. Depending on the task between ~1 (detection), ~3 (segmentation) or ~6 (classification) points.\nThey outperform other unsupervised methods by several points.\nThey outperform a randomly initialized (i.e. not pretrained) model significantly.\n(This is even more the case if only the last fully connected layers are finetuned, not the convolutional layers.)\nThey train with VGG16 instead of AlexNet and can improve their accuracy by an amount comparable to when the same model switch is done for the supervised training.\n\n\n\n ImageNet\n\nThey measure the normalized mutual information between cluster assignments and ImageNet labels.\nThis mutual information increases over time, indicating that the cluster assignments matches more and more the labels (which were not used during training).\nThey measure the normalized mutual information of cluster assignments between each pair of two consecutive epochs.\nThis mutual information increases over time, indicating that the clustering becomes more stable.\nThey measure the achieved mAP on Pascal VOC 2007 (I guess they use the model as pretrained weights?)\nbased on the number of clusters in k-means.\nThey achieve best results with 10,000 clusters.\nAccuracy per layer\n\nAfter training they place a linear classifier on top of their model and train it on the labels. They do repeat this for each layer to get the accuracy per layer.\nThey significantly outperform all competing methods (by several percentage points, depending on layer).\nAt conv5 (max conv layer in AlexNet) they are beaten by supervised learning by ~12 points (50.5 vs 38.2).\nThe difference is lower for conv3 (44.2 vs 41.0).\nThey also compare models pretrained on ImageNet, with the linear classifier fine-tuned on the Places dataset.\nIn this case they can keep up more with the model pretrained in supervised fashion (38.7 vs 34.7 at conv5, 39.4 vs 39.8 at conv4).\n\n\nStats:\n\n\n\n\n\n They measure the normalized mutual information between cluster assignments and ImageNet labels.\nThis mutual information increases over time, indicating that the cluster assignments matches more and more the labels (which were not used during training). They measure the normalized mutual information of cluster assignments between each pair of two consecutive epochs.\nThis mutual information increases over time, indicating that the clustering becomes more stable. They measure the achieved mAP on Pascal VOC 2007 (I guess they use the model as pretrained weights?)\nbased on the number of clusters in k-means.\nThey achieve best results with 10,000 clusters. Accuracy per layer\n\nAfter training they place a linear classifier on top of their model and train it on the labels. They do repeat this for each layer to get the accuracy per layer.\nThey significantly outperform all competing methods (by several percentage points, depending on layer).\nAt conv5 (max conv layer in AlexNet) they are beaten by supervised learning by ~12 points (50.5 vs 38.2).\nThe difference is lower for conv3 (44.2 vs 41.0).\nThey also compare models pretrained on ImageNet, with the linear classifier fine-tuned on the Places dataset.\nIn this case they can keep up more with the model pretrained in supervised fashion (38.7 vs 34.7 at conv5, 39.4 vs 39.8 at conv4).\n\n After training they place a linear classifier on top of their model and train it on the labels. They do repeat this for each layer to get the accuracy per layer. They significantly outperform all competing methods (by several percentage points, depending on layer). At conv5 (max conv layer in AlexNet) they are beaten by supervised learning by ~12 points (50.5 vs 38.2). The difference is lower for conv3 (44.2 vs 41.0). They also compare models pretrained on ImageNet, with the linear classifier fine-tuned on the Places dataset.\nIn this case they can keep up more with the model pretrained in supervised fashion (38.7 vs 34.7 at conv5, 39.4 vs 39.8 at conv4). Stats:\n\n\n\n  YFCC100M\n\nThey train on ImageNet and then search on YFCC100M for images leading to high activations in filters.\nImages with high activations and generated \"ideal\" images for that filter:\n\n\n\n\nImages with high activations for filters in layer conv5:\n\n\nThey note that some filters in conv5 seem to just re-learn patterns that are already covered by filters in lower layers.\n\n\n\n They train on ImageNet and then search on YFCC100M for images leading to high activations in filters. Images with high activations and generated \"ideal\" images for that filter:\n\n\n\n  Images with high activations for filters in layer conv5:\n\n\nThey note that some filters in conv5 seem to just re-learn patterns that are already covered by filters in lower layers.\n\n  They note that some filters in conv5 seem to just re-learn patterns that are already covered by filters in lower layers. Pascal VOC\n\nThey pretrain on ImageNet and then finetune on Pascal VOC for classification, object detection and segmentation.\nThey are still behind a model pretrained with labels. Depending on the task between ~1 (detection), ~3 (segmentation) or ~6 (classification) points.\nThey outperform other unsupervised methods by several points.\nThey outperform a randomly initialized (i.e. not pretrained) model significantly.\n(This is even more the case if only the last fully connected layers are finetuned, not the convolutional layers.)\nThey train with VGG16 instead of AlexNet and can improve their accuracy by an amount comparable to when the same model switch is done for the supervised training.\n\n They pretrain on ImageNet and then finetune on Pascal VOC for classification, object detection and segmentation. They are still behind a model pretrained with labels. Depending on the task between ~1 (detection), ~3 (segmentation) or ~6 (classification) points. They outperform other unsupervised methods by several points. They outperform a randomly initialized (i.e. not pretrained) model significantly.\n(This is even more the case if only the last fully connected layers are finetuned, not the convolutional layers.) They train with VGG16 instead of AlexNet and can improve their accuracy by an amount comparable to when the same model switch is done for the supervised training. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.pdf"
    },
    "1": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Deep_Continuous_Fusion_for_Multi-Sensor_3D_Object_Detection.md",
        "transcript": "\nWhat\n\nThey propose an object detector that fuses 2D information (from images) and 3D information (from LiDAR, i.e. point cloud) to predict 3D objects in birds eye view (BEV).\nTheir method is based on projecting 2D images into point clouds (usually it's the other way round, i.e. point clouds are projected into 2D images).\nThey propose a layer to perform the 2D-3D fusion at multiple image scales.\nThe result is a fast and fairly accurate object detector.\n\n They propose an object detector that fuses 2D information (from images) and 3D information (from LiDAR, i.e. point cloud) to predict 3D objects in birds eye view (BEV). Their method is based on projecting 2D images into point clouds (usually it's the other way round, i.e. point clouds are projected into 2D images). They propose a layer to perform the 2D-3D fusion at multiple image scales. The result is a fast and fairly accurate object detector. \nHow\n\nBasic Architecture\n\nThey feed the images through a ResNet18 branch (initialized via ImageNet pretraining).\nThey feed the point cloud through a custom residual network.\n\nInput is the voxelized point cloud in BEV.\nThe network is comparable to ResNet18.\nThey use a total of 36 convolutions in five blocks, with a growing number of convolutions per block and downscaling at the start of each block.\nThis network is initialized via Xavier initialization.\n\n\nThey features extracted by the image branch are fused at multiple scales into the point cloud BEV branch using Continuous Fusion layers.\nThe output of the point cloud BEV branch is comparable to many other object detectors:\n\nPer spatial location classification (object class + background) and regression predictions (x, y, z and width, length, height).\nThey use two anchor boxes, one for objects with a rotation of 0 degrees and one for objects with a rotation of 90 degrees.\nAs usually, they apply Non-Maximum-Suppression to the resulting bounding boxes.\n\n\nVisualization:\n\n\n\n\n\n\nContinuous Fusion Layer\n\nThe Continuous Fusion Layer fuses 2D (camera) and 3D (LiDAR) features.\nIts basic principle is to project 2D information into 3D. (Counterintuitively, it still performs first a projection from 3D to 2D.)\nAs their network works in BEV, the fusion merges information from a dense 2D grid (camera) with another dense 2D grid (LiDAR in BEV) instead of a point cloud.\nAssume you have a LIDAR BEV feature map with one channel, i.e. a 2D grid.\nYou want to merge information from the camera into one single spatial location (e.g. column 10, row 20) of the grid.\nThe following steps are then performed:\n\nFor the given spatial location, find the K nearest neighbour points in the BEV point cloud. (E.g. find the K neighbours of y=10, x=20.)\nRe-add the z coordinate to each of these K nearest neighbours (projects them from BEV 3D back to full 3D).\nProject these 3D points onto the 2D image plane. This indicates which image areas might contain information relevant for the 3D BEV location.\nUse bilinear interpolation to estimate the image's features at the projected locations.\nFor each location also estimate the offset vector from the BEV spatial location (e.g. y=10, x=20) to the 3D point (I guess in BEV?).\nConcat the feature vector and offset vector from step (4) to one vector.\nThen apply fully connected layers to that (in their case three layers).\nThis generates a vector per projected point.\nSum these vectors.\nConcat the resulting vector to the point cloud BEV feature map from step (1) at the chosen starting location (e.g. y=10, x=20).\n\n\nVisualization of the steps:\n\n\n\n\n\n\nOther stuff\n\nAs is common, their classification loss is binary cross entropy and their regression loss is smooth L1.\nAs is common, they encode the x-, y- and z-coordinates (predicted via the regression branch) is relative offsets to the anchor's center.\nAs is common, they encode the widths, lengths and heights logarithmically, e.g. log(width/width_anchor).\nThey use Adam without weight decay.\nFor training on KITTI, they augment the dataset, e.g. via scaling, translation and rotation of the point cloud and camera image\n(matched to each other, so that projections from one sensor to the other remain sensible).\nThey crop the point cloud to 70m in front of the ego vehicle and 40m to the left and right.\nThey voxelize the point cloud to 512x448x32 voxels. (Sounds like they apply 2D convs to an input with 32 channels encoding height information.)\n\n\n\n Basic Architecture\n\nThey feed the images through a ResNet18 branch (initialized via ImageNet pretraining).\nThey feed the point cloud through a custom residual network.\n\nInput is the voxelized point cloud in BEV.\nThe network is comparable to ResNet18.\nThey use a total of 36 convolutions in five blocks, with a growing number of convolutions per block and downscaling at the start of each block.\nThis network is initialized via Xavier initialization.\n\n\nThey features extracted by the image branch are fused at multiple scales into the point cloud BEV branch using Continuous Fusion layers.\nThe output of the point cloud BEV branch is comparable to many other object detectors:\n\nPer spatial location classification (object class + background) and regression predictions (x, y, z and width, length, height).\nThey use two anchor boxes, one for objects with a rotation of 0 degrees and one for objects with a rotation of 90 degrees.\nAs usually, they apply Non-Maximum-Suppression to the resulting bounding boxes.\n\n\nVisualization:\n\n\n\n\n\n They feed the images through a ResNet18 branch (initialized via ImageNet pretraining). They feed the point cloud through a custom residual network.\n\nInput is the voxelized point cloud in BEV.\nThe network is comparable to ResNet18.\nThey use a total of 36 convolutions in five blocks, with a growing number of convolutions per block and downscaling at the start of each block.\nThis network is initialized via Xavier initialization.\n\n Input is the voxelized point cloud in BEV. The network is comparable to ResNet18. They use a total of 36 convolutions in five blocks, with a growing number of convolutions per block and downscaling at the start of each block. This network is initialized via Xavier initialization. They features extracted by the image branch are fused at multiple scales into the point cloud BEV branch using Continuous Fusion layers. The output of the point cloud BEV branch is comparable to many other object detectors:\n\nPer spatial location classification (object class + background) and regression predictions (x, y, z and width, length, height).\nThey use two anchor boxes, one for objects with a rotation of 0 degrees and one for objects with a rotation of 90 degrees.\nAs usually, they apply Non-Maximum-Suppression to the resulting bounding boxes.\n\n Per spatial location classification (object class + background) and regression predictions (x, y, z and width, length, height). They use two anchor boxes, one for objects with a rotation of 0 degrees and one for objects with a rotation of 90 degrees. As usually, they apply Non-Maximum-Suppression to the resulting bounding boxes. Visualization:\n\n\n\n  Continuous Fusion Layer\n\nThe Continuous Fusion Layer fuses 2D (camera) and 3D (LiDAR) features.\nIts basic principle is to project 2D information into 3D. (Counterintuitively, it still performs first a projection from 3D to 2D.)\nAs their network works in BEV, the fusion merges information from a dense 2D grid (camera) with another dense 2D grid (LiDAR in BEV) instead of a point cloud.\nAssume you have a LIDAR BEV feature map with one channel, i.e. a 2D grid.\nYou want to merge information from the camera into one single spatial location (e.g. column 10, row 20) of the grid.\nThe following steps are then performed:\n\nFor the given spatial location, find the K nearest neighbour points in the BEV point cloud. (E.g. find the K neighbours of y=10, x=20.)\nRe-add the z coordinate to each of these K nearest neighbours (projects them from BEV 3D back to full 3D).\nProject these 3D points onto the 2D image plane. This indicates which image areas might contain information relevant for the 3D BEV location.\nUse bilinear interpolation to estimate the image's features at the projected locations.\nFor each location also estimate the offset vector from the BEV spatial location (e.g. y=10, x=20) to the 3D point (I guess in BEV?).\nConcat the feature vector and offset vector from step (4) to one vector.\nThen apply fully connected layers to that (in their case three layers).\nThis generates a vector per projected point.\nSum these vectors.\nConcat the resulting vector to the point cloud BEV feature map from step (1) at the chosen starting location (e.g. y=10, x=20).\n\n\nVisualization of the steps:\n\n\n\n\n\n The Continuous Fusion Layer fuses 2D (camera) and 3D (LiDAR) features. Its basic principle is to project 2D information into 3D. (Counterintuitively, it still performs first a projection from 3D to 2D.) As their network works in BEV, the fusion merges information from a dense 2D grid (camera) with another dense 2D grid (LiDAR in BEV) instead of a point cloud. Assume you have a LIDAR BEV feature map with one channel, i.e. a 2D grid.\nYou want to merge information from the camera into one single spatial location (e.g. column 10, row 20) of the grid.\nThe following steps are then performed:\n\nFor the given spatial location, find the K nearest neighbour points in the BEV point cloud. (E.g. find the K neighbours of y=10, x=20.)\nRe-add the z coordinate to each of these K nearest neighbours (projects them from BEV 3D back to full 3D).\nProject these 3D points onto the 2D image plane. This indicates which image areas might contain information relevant for the 3D BEV location.\nUse bilinear interpolation to estimate the image's features at the projected locations.\nFor each location also estimate the offset vector from the BEV spatial location (e.g. y=10, x=20) to the 3D point (I guess in BEV?).\nConcat the feature vector and offset vector from step (4) to one vector.\nThen apply fully connected layers to that (in their case three layers).\nThis generates a vector per projected point.\nSum these vectors.\nConcat the resulting vector to the point cloud BEV feature map from step (1) at the chosen starting location (e.g. y=10, x=20).\n\n For the given spatial location, find the K nearest neighbour points in the BEV point cloud. (E.g. find the K neighbours of y=10, x=20.) Re-add the z coordinate to each of these K nearest neighbours (projects them from BEV 3D back to full 3D). Project these 3D points onto the 2D image plane. This indicates which image areas might contain information relevant for the 3D BEV location. Use bilinear interpolation to estimate the image's features at the projected locations.\nFor each location also estimate the offset vector from the BEV spatial location (e.g. y=10, x=20) to the 3D point (I guess in BEV?). Concat the feature vector and offset vector from step (4) to one vector.\nThen apply fully connected layers to that (in their case three layers).\nThis generates a vector per projected point.\nSum these vectors.\nConcat the resulting vector to the point cloud BEV feature map from step (1) at the chosen starting location (e.g. y=10, x=20). Visualization of the steps:\n\n\n\n  Other stuff\n\nAs is common, their classification loss is binary cross entropy and their regression loss is smooth L1.\nAs is common, they encode the x-, y- and z-coordinates (predicted via the regression branch) is relative offsets to the anchor's center.\nAs is common, they encode the widths, lengths and heights logarithmically, e.g. log(width/width_anchor).\nThey use Adam without weight decay.\nFor training on KITTI, they augment the dataset, e.g. via scaling, translation and rotation of the point cloud and camera image\n(matched to each other, so that projections from one sensor to the other remain sensible).\nThey crop the point cloud to 70m in front of the ego vehicle and 40m to the left and right.\nThey voxelize the point cloud to 512x448x32 voxels. (Sounds like they apply 2D convs to an input with 32 channels encoding height information.)\n\n As is common, their classification loss is binary cross entropy and their regression loss is smooth L1. As is common, they encode the x-, y- and z-coordinates (predicted via the regression branch) is relative offsets to the anchor's center. As is common, they encode the widths, lengths and heights logarithmically, e.g. log(width/width_anchor). They use Adam without weight decay. For training on KITTI, they augment the dataset, e.g. via scaling, translation and rotation of the point cloud and camera image\n(matched to each other, so that projections from one sensor to the other remain sensible). They crop the point cloud to 70m in front of the ego vehicle and 40m to the left and right. They voxelize the point cloud to 512x448x32 voxels. (Sounds like they apply 2D convs to an input with 32 channels encoding height information.) \nResults\n\nKITTI\n\nNote: They only evaluate on the class \"car\", arguing that -- in the case of their model -- there is not enough training data for the other classes in KITTI.\nThey achieve accuracies roughly comparable with other well-performing 3D object detectors. (Though not state of the art.)\nTheir detector runs at about 15fps or 66ms per scene, beating most other models.\n\nThey are roughly twice as fast as AVOD-FPN, but quite a bit less accurate (~5 percentage points for moderate 3D AP).\n\n\nUsing only LiDAR (and not images) as input significantly worsens results.\nUsing a fusion strategy similar in which only the BEV locations closest to 3D points are projected to the image plane (no K-NN) and without using offset vectors significantly worsens results.\nRemoving the offset vector significantly worsens results.\nPicking K>1 for the K-nearest-neighbours search does not produce better results than K=1.\nLimiting the maximum distance during the K-nearest-neighbour search to 10m has only a tiny positive impact on AP.\nAblation results (\"geometric feature\" is the offset vector):\n\n\n\n\n\n\n\n KITTI\n\nNote: They only evaluate on the class \"car\", arguing that -- in the case of their model -- there is not enough training data for the other classes in KITTI.\nThey achieve accuracies roughly comparable with other well-performing 3D object detectors. (Though not state of the art.)\nTheir detector runs at about 15fps or 66ms per scene, beating most other models.\n\nThey are roughly twice as fast as AVOD-FPN, but quite a bit less accurate (~5 percentage points for moderate 3D AP).\n\n\nUsing only LiDAR (and not images) as input significantly worsens results.\nUsing a fusion strategy similar in which only the BEV locations closest to 3D points are projected to the image plane (no K-NN) and without using offset vectors significantly worsens results.\nRemoving the offset vector significantly worsens results.\nPicking K>1 for the K-nearest-neighbours search does not produce better results than K=1.\nLimiting the maximum distance during the K-nearest-neighbour search to 10m has only a tiny positive impact on AP.\nAblation results (\"geometric feature\" is the offset vector):\n\n\n\n\n\n Note: They only evaluate on the class \"car\", arguing that -- in the case of their model -- there is not enough training data for the other classes in KITTI. They achieve accuracies roughly comparable with other well-performing 3D object detectors. (Though not state of the art.) Their detector runs at about 15fps or 66ms per scene, beating most other models.\n\nThey are roughly twice as fast as AVOD-FPN, but quite a bit less accurate (~5 percentage points for moderate 3D AP).\n\n They are roughly twice as fast as AVOD-FPN, but quite a bit less accurate (~5 percentage points for moderate 3D AP). Using only LiDAR (and not images) as input significantly worsens results. Using a fusion strategy similar in which only the BEV locations closest to 3D points are projected to the image plane (no K-NN) and without using offset vectors significantly worsens results. Removing the offset vector significantly worsens results. Picking K>1 for the K-nearest-neighbours search does not produce better results than K=1. Limiting the maximum distance during the K-nearest-neighbour search to 10m has only a tiny positive impact on AP. Ablation results (\"geometric feature\" is the offset vector):\n\n\n\n  \nTOR4D (Uber dataset)\n\nThey evaluate here on more classes and increase the forward distance to 100m.\nThey decrease the parameters in the network to keep the time per scene roughly comparable (despite increase in max forward distance).\nThey evaluate the achieved AP by class and distance:\n\n\n\n\n\n They evaluate here on more classes and increase the forward distance to 100m. They decrease the parameters in the network to keep the time per scene roughly comparable (despite increase in max forward distance). They evaluate the achieved AP by class and distance:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.pdf"
    },
    "2": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Audio-Visual_Scene_Analysis_with_Self-Supervised_Multisensory_Features.md",
        "transcript": "\nWhat\n\nThey propose a method to pretrain jointly on video and corresponding audio data without labels and then use the results to:\n\nperform sound source logalicization (estimate what in an image causes the current sound),\nperform audio-visual action recognition,\nperform audio-separation (e.g. remove speech from person not visible in an image, remove speech from a specific visible person).\n\n\n\n They propose a method to pretrain jointly on video and corresponding audio data without labels and then use the results to:\n\nperform sound source logalicization (estimate what in an image causes the current sound),\nperform audio-visual action recognition,\nperform audio-separation (e.g. remove speech from person not visible in an image, remove speech from a specific visible person).\n\n perform sound source logalicization (estimate what in an image causes the current sound), perform audio-visual action recognition, perform audio-separation (e.g. remove speech from person not visible in an image, remove speech from a specific visible person). \nHow\n\nBasic method\n\nThey train their network to differentiate between real frames (4.2 seconds) and their corresponding audio data on the one hand\nand real frames but misaligned audio data on the other hand.\nMisaligned here means that the audio is shifted by a few seconds (by 2.0 to 5.8 seconds to be precise).\nTo solve this task, the network has to learn to understand the scene in order to spot small misalignments.\nThe network can learn this task with self-supervision, i.e. without annotations.\n\n\nArchitecture\n\nTheir network starts with two branches.\nOne for frames from videos.\nOne for audio data.\nBoth branches are then merged and finally end in a fully convolutional layer.\nVisualization:\n\n\n\n\n\n\nSound Source Localization\n\nThey train their model as described.\nThe features maps of the final convolutional layer then contain information about the spatial location of the sound source.\nThey convert these feature maps to a heatmap visualization by first weighting each map (based on its weight to produce the final binary output),\nthen summing the maps and lastly normalizing them to e.g. 0 to 1.0.\n\n\nAction Recognition\n\nThey first pretrain using their described method, then fine-tune on the UCF-101 dataset (using 2.56 second videos with augmentation and small audio-shifts down to one frame).\n\n\nOn/off-screen audio-visual source separation\n\nTheir goal is to separate sound from (a) sources visible in the video and (b) sources not visible in the video.\n\nSources in (b) can be outside of the camera view, but can also be sources intentionally hidden, e.g. by setting the image area of a known source in all frames to zero.\nThey focus here in speech by people (i.e. the model has to learn to separate between speeches by people visible in the video and those not visible).\n\n\nThey first pretrain a model as described above.\nThen they create a new model:\n\nIt has one subnetwork for video+audio processing (can reuse the weights of the pretrained network).\nIt has a second subnetwork to process the audio spectrogram in a U-Net form.\nThe features of the first subnetwork get merged into the second subnetwork at various resolutions.\nThe output are two spectrograms: one for the on-screen audio, one for the off-screen audio.\nVisualization:\n\n\n\n\n\n\nThey train this by pairing video sequences from VoxCeleb with their audio (on-screen) and other audio from a random clip (off-screen).\nThey use a loss with two L1-based regression losses:\n\n\nwhere x_F is the on-screen (foreground) audio, f_F(.) its prediction and analogously x_B and f_B(.) the background (off-screen) audio and its prediction.\nOne can make this loss permutation invariant by using min(L(x_M, I), L(I, x_M)).\n\n\n\n\n\n Basic method\n\nThey train their network to differentiate between real frames (4.2 seconds) and their corresponding audio data on the one hand\nand real frames but misaligned audio data on the other hand.\nMisaligned here means that the audio is shifted by a few seconds (by 2.0 to 5.8 seconds to be precise).\nTo solve this task, the network has to learn to understand the scene in order to spot small misalignments.\nThe network can learn this task with self-supervision, i.e. without annotations.\n\n They train their network to differentiate between real frames (4.2 seconds) and their corresponding audio data on the one hand\nand real frames but misaligned audio data on the other hand. Misaligned here means that the audio is shifted by a few seconds (by 2.0 to 5.8 seconds to be precise). To solve this task, the network has to learn to understand the scene in order to spot small misalignments. The network can learn this task with self-supervision, i.e. without annotations. Architecture\n\nTheir network starts with two branches.\nOne for frames from videos.\nOne for audio data.\nBoth branches are then merged and finally end in a fully convolutional layer.\nVisualization:\n\n\n\n\n\n Their network starts with two branches. One for frames from videos. One for audio data. Both branches are then merged and finally end in a fully convolutional layer. Visualization:\n\n\n\n  Sound Source Localization\n\nThey train their model as described.\nThe features maps of the final convolutional layer then contain information about the spatial location of the sound source.\nThey convert these feature maps to a heatmap visualization by first weighting each map (based on its weight to produce the final binary output),\nthen summing the maps and lastly normalizing them to e.g. 0 to 1.0.\n\n They train their model as described. The features maps of the final convolutional layer then contain information about the spatial location of the sound source. They convert these feature maps to a heatmap visualization by first weighting each map (based on its weight to produce the final binary output),\nthen summing the maps and lastly normalizing them to e.g. 0 to 1.0. Action Recognition\n\nThey first pretrain using their described method, then fine-tune on the UCF-101 dataset (using 2.56 second videos with augmentation and small audio-shifts down to one frame).\n\n They first pretrain using their described method, then fine-tune on the UCF-101 dataset (using 2.56 second videos with augmentation and small audio-shifts down to one frame). On/off-screen audio-visual source separation\n\nTheir goal is to separate sound from (a) sources visible in the video and (b) sources not visible in the video.\n\nSources in (b) can be outside of the camera view, but can also be sources intentionally hidden, e.g. by setting the image area of a known source in all frames to zero.\nThey focus here in speech by people (i.e. the model has to learn to separate between speeches by people visible in the video and those not visible).\n\n\nThey first pretrain a model as described above.\nThen they create a new model:\n\nIt has one subnetwork for video+audio processing (can reuse the weights of the pretrained network).\nIt has a second subnetwork to process the audio spectrogram in a U-Net form.\nThe features of the first subnetwork get merged into the second subnetwork at various resolutions.\nThe output are two spectrograms: one for the on-screen audio, one for the off-screen audio.\nVisualization:\n\n\n\n\n\n\nThey train this by pairing video sequences from VoxCeleb with their audio (on-screen) and other audio from a random clip (off-screen).\nThey use a loss with two L1-based regression losses:\n\n\nwhere x_F is the on-screen (foreground) audio, f_F(.) its prediction and analogously x_B and f_B(.) the background (off-screen) audio and its prediction.\nOne can make this loss permutation invariant by using min(L(x_M, I), L(I, x_M)).\n\n\n\n Their goal is to separate sound from (a) sources visible in the video and (b) sources not visible in the video.\n\nSources in (b) can be outside of the camera view, but can also be sources intentionally hidden, e.g. by setting the image area of a known source in all frames to zero.\nThey focus here in speech by people (i.e. the model has to learn to separate between speeches by people visible in the video and those not visible).\n\n Sources in (b) can be outside of the camera view, but can also be sources intentionally hidden, e.g. by setting the image area of a known source in all frames to zero. They focus here in speech by people (i.e. the model has to learn to separate between speeches by people visible in the video and those not visible). They first pretrain a model as described above. Then they create a new model:\n\nIt has one subnetwork for video+audio processing (can reuse the weights of the pretrained network).\nIt has a second subnetwork to process the audio spectrogram in a U-Net form.\nThe features of the first subnetwork get merged into the second subnetwork at various resolutions.\nThe output are two spectrograms: one for the on-screen audio, one for the off-screen audio.\nVisualization:\n\n\n\n\n\n It has one subnetwork for video+audio processing (can reuse the weights of the pretrained network). It has a second subnetwork to process the audio spectrogram in a U-Net form. The features of the first subnetwork get merged into the second subnetwork at various resolutions. The output are two spectrograms: one for the on-screen audio, one for the off-screen audio. Visualization:\n\n\n\n  They train this by pairing video sequences from VoxCeleb with their audio (on-screen) and other audio from a random clip (off-screen). They use a loss with two L1-based regression losses:\n\n\nwhere x_F is the on-screen (foreground) audio, f_F(.) its prediction and analogously x_B and f_B(.) the background (off-screen) audio and its prediction.\nOne can make this loss permutation invariant by using min(L(x_M, I), L(I, x_M)).\n\n  where x_F is the on-screen (foreground) audio, f_F(.) its prediction and analogously x_B and f_B(.) the background (off-screen) audio and its prediction. One can make this loss permutation invariant by using min(L(x_M, I), L(I, x_M)). \nResults\n\nSound Source Localization\n\nThey train on 750k videos from the AudioSet dataset.\nThey reached 59.9% accuracy during pretraining (i.e. when discriminating between non-shifted and shifted audio). Humans reached around 66.6% (under easier test conditions, e.g. stronger audio shifts).\nQualitative results, showing via CAM method the test examples with strongest activation maps:\n\n\nNote that AudioSet did not only include human speech. The model just seemed to react strongly to that.\n\n\nSame as before, but test examples with the weakest activation maps:\n\n\n\n\nStrongest activation maps, separated for three example classes (trained on Kinetics):\n\n\n\n\n\n\nAction Recognition\n\nThey used the UCF-101 dataset, as mentioned above.\nThey achieve significantly higher scores than other self-supervised methods.\nThey are still quite a bit behind the best method (82.1% vs 94.5%), though that one is a supervised method and was pretrained on two large datasets (ImageNet + Kinetics).\nThey test here pairing videos with audio from random other videos (instead of shifted audio from the same video).\nThis reduced performance from 84.2% to 78.7%, likely because it is a too easy task that often does not require precise understanding of e.g. the motions in the video.\nSetting the audio branch's activation after pretraining to zero and then training on UCF-101 resulted in an accuracy drop of 5 percentage points.\nUsing no self-supervised pretraining resulted in an accuracy drop of 14 percentage points.\nUsing spectrograms as audio input performed comparable to raw wave inputs.\n\n\nOn/off-screen audio-visual source separation\n\nThey trained on the VoxCeleb dataset, as described above.\n\nThey made sure that the speaker identities were disjoint between train and test sets.\nThey sampled 2.1s clips from each 5s video.\n\n\nResults are shown in the video (see at top of this file).\nTraining the model only on a single frame instead of the full video worsened the loss from 11.4 to 14.8.\n\nThis drop was especially pronounced in videos with two persons of the same gender, where lip motion is an important clue.\n\n\nUsing the state-of-the-art I3D net pretrained on Kinect as audio-visual feature inputs (instead of their pretrained net) worsened the loss from 11.4 to 12.3.\n\nThey suspect that this is because action recognition can work decently with just single-frame analysis, while their pretraining (and this source seperation task) require in-depth motion analysis.\n\n\n\n\n\n Sound Source Localization\n\nThey train on 750k videos from the AudioSet dataset.\nThey reached 59.9% accuracy during pretraining (i.e. when discriminating between non-shifted and shifted audio). Humans reached around 66.6% (under easier test conditions, e.g. stronger audio shifts).\nQualitative results, showing via CAM method the test examples with strongest activation maps:\n\n\nNote that AudioSet did not only include human speech. The model just seemed to react strongly to that.\n\n\nSame as before, but test examples with the weakest activation maps:\n\n\n\n\nStrongest activation maps, separated for three example classes (trained on Kinetics):\n\n\n\n\n\n They train on 750k videos from the AudioSet dataset. They reached 59.9% accuracy during pretraining (i.e. when discriminating between non-shifted and shifted audio). Humans reached around 66.6% (under easier test conditions, e.g. stronger audio shifts). Qualitative results, showing via CAM method the test examples with strongest activation maps:\n\n\nNote that AudioSet did not only include human speech. The model just seemed to react strongly to that.\n\n  Note that AudioSet did not only include human speech. The model just seemed to react strongly to that. Same as before, but test examples with the weakest activation maps:\n\n\n\n  Strongest activation maps, separated for three example classes (trained on Kinetics):\n\n\n\n  Action Recognition\n\nThey used the UCF-101 dataset, as mentioned above.\nThey achieve significantly higher scores than other self-supervised methods.\nThey are still quite a bit behind the best method (82.1% vs 94.5%), though that one is a supervised method and was pretrained on two large datasets (ImageNet + Kinetics).\nThey test here pairing videos with audio from random other videos (instead of shifted audio from the same video).\nThis reduced performance from 84.2% to 78.7%, likely because it is a too easy task that often does not require precise understanding of e.g. the motions in the video.\nSetting the audio branch's activation after pretraining to zero and then training on UCF-101 resulted in an accuracy drop of 5 percentage points.\nUsing no self-supervised pretraining resulted in an accuracy drop of 14 percentage points.\nUsing spectrograms as audio input performed comparable to raw wave inputs.\n\n They used the UCF-101 dataset, as mentioned above. They achieve significantly higher scores than other self-supervised methods. They are still quite a bit behind the best method (82.1% vs 94.5%), though that one is a supervised method and was pretrained on two large datasets (ImageNet + Kinetics). They test here pairing videos with audio from random other videos (instead of shifted audio from the same video).\nThis reduced performance from 84.2% to 78.7%, likely because it is a too easy task that often does not require precise understanding of e.g. the motions in the video. Setting the audio branch's activation after pretraining to zero and then training on UCF-101 resulted in an accuracy drop of 5 percentage points. Using no self-supervised pretraining resulted in an accuracy drop of 14 percentage points. Using spectrograms as audio input performed comparable to raw wave inputs. On/off-screen audio-visual source separation\n\nThey trained on the VoxCeleb dataset, as described above.\n\nThey made sure that the speaker identities were disjoint between train and test sets.\nThey sampled 2.1s clips from each 5s video.\n\n\nResults are shown in the video (see at top of this file).\nTraining the model only on a single frame instead of the full video worsened the loss from 11.4 to 14.8.\n\nThis drop was especially pronounced in videos with two persons of the same gender, where lip motion is an important clue.\n\n\nUsing the state-of-the-art I3D net pretrained on Kinect as audio-visual feature inputs (instead of their pretrained net) worsened the loss from 11.4 to 12.3.\n\nThey suspect that this is because action recognition can work decently with just single-frame analysis, while their pretraining (and this source seperation task) require in-depth motion analysis.\n\n\n\n They trained on the VoxCeleb dataset, as described above.\n\nThey made sure that the speaker identities were disjoint between train and test sets.\nThey sampled 2.1s clips from each 5s video.\n\n They made sure that the speaker identities were disjoint between train and test sets. They sampled 2.1s clips from each 5s video. Results are shown in the video (see at top of this file). Training the model only on a single frame instead of the full video worsened the loss from 11.4 to 14.8.\n\nThis drop was especially pronounced in videos with two persons of the same gender, where lip motion is an important clue.\n\n This drop was especially pronounced in videos with two persons of the same gender, where lip motion is an important clue. Using the state-of-the-art I3D net pretrained on Kinect as audio-visual feature inputs (instead of their pretrained net) worsened the loss from 11.4 to 12.3.\n\nThey suspect that this is because action recognition can work decently with just single-frame analysis, while their pretraining (and this source seperation task) require in-depth motion analysis.\n\n They suspect that this is because action recognition can work decently with just single-frame analysis, while their pretraining (and this source seperation task) require in-depth motion analysis. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Andrew_Owens_Audio-Visual_Scene_Analysis_ECCV_2018_paper.pdf"
    },
    "3": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Towards_Realistic_Predictors.md",
        "transcript": "\nWhat\n\nThey propose a method for classifiers to estimate how hard/difficult an example is (for the specific classifier).\nThey call that \"realistic predictors\" (in the sense of \"being realistic with respect to one's own abilities\").\nTheir method allows them to\n\nfocus training on hard examples,\nreject too difficult inputs,\nbased on point (2) guarantee average accuracies by rejecting inputs with certain thresholds of diffculty-estimates.\n(This is useful e.g. for some safety critical systems, where no decision might be better than a wrong decision.\nIt can also be used for combinations of models and humans, where a model would automate easy tasks and a few humans deal with the remaining hard tasks.)\n\n\n\n They propose a method for classifiers to estimate how hard/difficult an example is (for the specific classifier). They call that \"realistic predictors\" (in the sense of \"being realistic with respect to one's own abilities\"). Their method allows them to\n\nfocus training on hard examples,\nreject too difficult inputs,\nbased on point (2) guarantee average accuracies by rejecting inputs with certain thresholds of diffculty-estimates.\n(This is useful e.g. for some safety critical systems, where no decision might be better than a wrong decision.\nIt can also be used for combinations of models and humans, where a model would automate easy tasks and a few humans deal with the remaining hard tasks.)\n\n focus training on hard examples, reject too difficult inputs, based on point (2) guarantee average accuracies by rejecting inputs with certain thresholds of diffculty-estimates.\n(This is useful e.g. for some safety critical systems, where no decision might be better than a wrong decision.\nIt can also be used for combinations of models and humans, where a model would automate easy tasks and a few humans deal with the remaining hard tasks.) \nHow\n\nArchitecture\n\nThey have a given classifier F.\nThey need per example i a hardness estimate s_i denoting how difficult that input is for F.\nThey predict that value with HP-Net (hardness predictor), a network completely separate from F.\nThe estimated hardness values s_i influence the training of F for hard negative mining.\nThe predictions of F are used during the training of HP-Net.\nVisualization:\n\n\n\n\n\n\nLoss\n\nFor F (classifier):\n\nF predicts for multiple classes a probability value (after softmax).\nThey use a crossentropy loss weighted by the hardness:\n\n\nHere, s_i is the hardness if the i-th example and p_i^c is the probability predicted by F for the correct class of the i-th example.\nThis increases the loss for hard examples (high s_i value) and decreases it for easy examples (low s_i).\n\n\n\n\nFor HP-Net (hardness predictor):\n\nHP-Net predicts the inverse of the probability that F is going to predict for the correct class.\nSo if F predicts for the correct class a high probability, HP-Net is trained to predict a low hardness value and vice versa.\nThey use a binary crossentropy loss between p_i^c (F's prediction for the correct class) and s_i (hardness):\n\n\n\n\n\n\n\n\nTraining Schedule and Refinement\n\nThe full training schedules happens in the following steps.\n\nTrain F and HP-Net jointly on a training set.\nDo that training iteratively (one batch for F, then one batch for HP-Net).\nOtherwise the networks would not converge for them.\nUse HP-Net to eliminate hard examples from the dataset. (E.g. the hardest 5%.)\nTrain a new F on reduced dataset, result is F' (with higher accuracy for these easier examples).\nReuse HP-Net from (1) during this training, keep its weights fixed.\nOutput pair (F', HP-Net).\n\n\n\n\nThen, at test time examples above a certain hardness threshold are rejected (similar to step 2) and F' is applied to the remaining examples.\nVisualization:\n\n\n\n\n\n Architecture\n\nThey have a given classifier F.\nThey need per example i a hardness estimate s_i denoting how difficult that input is for F.\nThey predict that value with HP-Net (hardness predictor), a network completely separate from F.\nThe estimated hardness values s_i influence the training of F for hard negative mining.\nThe predictions of F are used during the training of HP-Net.\nVisualization:\n\n\n\n\n\n They have a given classifier F. They need per example i a hardness estimate s_i denoting how difficult that input is for F. They predict that value with HP-Net (hardness predictor), a network completely separate from F. The estimated hardness values s_i influence the training of F for hard negative mining. The predictions of F are used during the training of HP-Net. Visualization:\n\n\n\n  Loss\n\nFor F (classifier):\n\nF predicts for multiple classes a probability value (after softmax).\nThey use a crossentropy loss weighted by the hardness:\n\n\nHere, s_i is the hardness if the i-th example and p_i^c is the probability predicted by F for the correct class of the i-th example.\nThis increases the loss for hard examples (high s_i value) and decreases it for easy examples (low s_i).\n\n\n\n\nFor HP-Net (hardness predictor):\n\nHP-Net predicts the inverse of the probability that F is going to predict for the correct class.\nSo if F predicts for the correct class a high probability, HP-Net is trained to predict a low hardness value and vice versa.\nThey use a binary crossentropy loss between p_i^c (F's prediction for the correct class) and s_i (hardness):\n\n\n\n\n\n\n\n For F (classifier):\n\nF predicts for multiple classes a probability value (after softmax).\nThey use a crossentropy loss weighted by the hardness:\n\n\nHere, s_i is the hardness if the i-th example and p_i^c is the probability predicted by F for the correct class of the i-th example.\nThis increases the loss for hard examples (high s_i value) and decreases it for easy examples (low s_i).\n\n\n\n F predicts for multiple classes a probability value (after softmax). They use a crossentropy loss weighted by the hardness:\n\n\nHere, s_i is the hardness if the i-th example and p_i^c is the probability predicted by F for the correct class of the i-th example.\nThis increases the loss for hard examples (high s_i value) and decreases it for easy examples (low s_i).\n\n  Here, s_i is the hardness if the i-th example and p_i^c is the probability predicted by F for the correct class of the i-th example. This increases the loss for hard examples (high s_i value) and decreases it for easy examples (low s_i). For HP-Net (hardness predictor):\n\nHP-Net predicts the inverse of the probability that F is going to predict for the correct class.\nSo if F predicts for the correct class a high probability, HP-Net is trained to predict a low hardness value and vice versa.\nThey use a binary crossentropy loss between p_i^c (F's prediction for the correct class) and s_i (hardness):\n\n\n\n\n\n HP-Net predicts the inverse of the probability that F is going to predict for the correct class.\nSo if F predicts for the correct class a high probability, HP-Net is trained to predict a low hardness value and vice versa. They use a binary crossentropy loss between p_i^c (F's prediction for the correct class) and s_i (hardness):\n\n\n\n  Training Schedule and Refinement\n\nThe full training schedules happens in the following steps.\n\nTrain F and HP-Net jointly on a training set.\nDo that training iteratively (one batch for F, then one batch for HP-Net).\nOtherwise the networks would not converge for them.\nUse HP-Net to eliminate hard examples from the dataset. (E.g. the hardest 5%.)\nTrain a new F on reduced dataset, result is F' (with higher accuracy for these easier examples).\nReuse HP-Net from (1) during this training, keep its weights fixed.\nOutput pair (F', HP-Net).\n\n\n\n The full training schedules happens in the following steps.\n\nTrain F and HP-Net jointly on a training set.\nDo that training iteratively (one batch for F, then one batch for HP-Net).\nOtherwise the networks would not converge for them.\nUse HP-Net to eliminate hard examples from the dataset. (E.g. the hardest 5%.)\nTrain a new F on reduced dataset, result is F' (with higher accuracy for these easier examples).\nReuse HP-Net from (1) during this training, keep its weights fixed.\nOutput pair (F', HP-Net).\n\n Train F and HP-Net jointly on a training set.\nDo that training iteratively (one batch for F, then one batch for HP-Net).\nOtherwise the networks would not converge for them. Use HP-Net to eliminate hard examples from the dataset. (E.g. the hardest 5%.) Train a new F on reduced dataset, result is F' (with higher accuracy for these easier examples).\nReuse HP-Net from (1) during this training, keep its weights fixed. Output pair (F', HP-Net). Then, at test time examples above a certain hardness threshold are rejected (similar to step 2) and F' is applied to the remaining examples. Visualization:\n\n\n\n  \nResults\n\nThey test on MNIST, MIT67 and ImageNet.\nThey test for both F and HP-Net the networks VGG, ResNet, kerasNet and LeNet5. (They slightly modify the heads for HP-Net.)\nThey test with and without weight sharing between F and HP-Net.\nThey observe that hardness scores predicted by HP-Net start with high values and then shift towards lower values during training. (As expected.)\nThey achieve significantly worse accuracies if using weight sharing between F and HP-Net. This indicates that the two networks solve fairly different tasks.\nThey achieve the highest accuracies when using the same network architectures for F and HP-Net (e.g. both ResNet). This indicates that the HP-Net has to operate in a similar way to F in order to predict its predictions.\nOn ImageNet\n\nThey get almost 1 percentage point higher accuracy, even when not rejecting any examples. This is likely due to the indirect hard negative mining in the loss of F.\nRejecting the 5% most difficult examples results in 0.7 points higher accuracy (on the remaining ones). At 10% rejection rate they get 1.3 higher accuracy.\nOne can reject examples based on the hardness score or based on the highest class probability predicted by F.\nDoing it based on the hardness score performs marginally better (+0.1 points) for high rejection rates and decently better (+0.7 points) for a rejection rate of 5%.\nRefining on the reduced dataset (from F to F') gives around 0.1 points higher accuracy.\nTo reach with VGG the accuracy of ResNet at 2% rejection rate, one has to set VGG's rejection rate to 10%.\n\n\n\n They test on MNIST, MIT67 and ImageNet. They test for both F and HP-Net the networks VGG, ResNet, kerasNet and LeNet5. (They slightly modify the heads for HP-Net.) They test with and without weight sharing between F and HP-Net. They observe that hardness scores predicted by HP-Net start with high values and then shift towards lower values during training. (As expected.) They achieve significantly worse accuracies if using weight sharing between F and HP-Net. This indicates that the two networks solve fairly different tasks. They achieve the highest accuracies when using the same network architectures for F and HP-Net (e.g. both ResNet). This indicates that the HP-Net has to operate in a similar way to F in order to predict its predictions. On ImageNet\n\nThey get almost 1 percentage point higher accuracy, even when not rejecting any examples. This is likely due to the indirect hard negative mining in the loss of F.\nRejecting the 5% most difficult examples results in 0.7 points higher accuracy (on the remaining ones). At 10% rejection rate they get 1.3 higher accuracy.\nOne can reject examples based on the hardness score or based on the highest class probability predicted by F.\nDoing it based on the hardness score performs marginally better (+0.1 points) for high rejection rates and decently better (+0.7 points) for a rejection rate of 5%.\nRefining on the reduced dataset (from F to F') gives around 0.1 points higher accuracy.\nTo reach with VGG the accuracy of ResNet at 2% rejection rate, one has to set VGG's rejection rate to 10%.\n\n They get almost 1 percentage point higher accuracy, even when not rejecting any examples. This is likely due to the indirect hard negative mining in the loss of F. Rejecting the 5% most difficult examples results in 0.7 points higher accuracy (on the remaining ones). At 10% rejection rate they get 1.3 higher accuracy. One can reject examples based on the hardness score or based on the highest class probability predicted by F.\nDoing it based on the hardness score performs marginally better (+0.1 points) for high rejection rates and decently better (+0.7 points) for a rejection rate of 5%. Refining on the reduced dataset (from F to F') gives around 0.1 points higher accuracy. To reach with VGG the accuracy of ResNet at 2% rejection rate, one has to set VGG's rejection rate to 10%. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper.pdf"
    },
    "4": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Acquisition_of_Localization_Confidence_for_Accurate_OD.md",
        "transcript": "\nWhat\n\nCurrent object detectors predict usually a set of bounding boxes, each having a classification score (\"how likely it is to be of an object class\")\nand regressed bounding box dimensions (\"where is the box and how high/wide is it\").\nThese results are then filtered through NMS, suppressing all bounding boxes that overlap strongly with another box that has higher classification score.\nSometimes the regressed dimensions are also iteratively refined (e.g. in two-stage detectors one can argue that there is one refinement after the RPN).\nThe authors observe that this structure is problematic, as there is no uncertainty-related information regarding the regressed dimensions.\n\nFor instance in NMS, the suppression works based on classification scores,\nbut e.g. a too large bounding box might get a higher classification score than one with optimal fit,\ndue to including more context and thereby making the classification more certain.\nThe classification scores are also very non-monotonic with respect to the bounding box dimensions:\nIncreasing the bounding box size might first lead to higher classification scores, then lower ones, then higher again.\n\nThis makes iterative refinement hard or impossible.\n\n\n\n\nThey propose IoU-Net, which predicts for each bounding box the expected IoU with the ground truth.\n\nThat predicted IoU is then used in NMS as a replacement for the classification score,\nthereby selecting for the bounding boxes that the model thinks have highest IoU.\nThe predicted IoU can also be used to perform iterative refinement, as it is more monotonic than the classification score.\n\n\nThey propose \"Precise RoI Pooling\" (PrRoI Pooling), which is a version of RoI-Pooling/RoI-Align that does not suffer from quantization inaccuracies.\n\n Current object detectors predict usually a set of bounding boxes, each having a classification score (\"how likely it is to be of an object class\")\nand regressed bounding box dimensions (\"where is the box and how high/wide is it\"). These results are then filtered through NMS, suppressing all bounding boxes that overlap strongly with another box that has higher classification score. Sometimes the regressed dimensions are also iteratively refined (e.g. in two-stage detectors one can argue that there is one refinement after the RPN). The authors observe that this structure is problematic, as there is no uncertainty-related information regarding the regressed dimensions.\n\nFor instance in NMS, the suppression works based on classification scores,\nbut e.g. a too large bounding box might get a higher classification score than one with optimal fit,\ndue to including more context and thereby making the classification more certain.\nThe classification scores are also very non-monotonic with respect to the bounding box dimensions:\nIncreasing the bounding box size might first lead to higher classification scores, then lower ones, then higher again.\n\nThis makes iterative refinement hard or impossible.\n\n\n\n For instance in NMS, the suppression works based on classification scores,\nbut e.g. a too large bounding box might get a higher classification score than one with optimal fit,\ndue to including more context and thereby making the classification more certain. The classification scores are also very non-monotonic with respect to the bounding box dimensions:\nIncreasing the bounding box size might first lead to higher classification scores, then lower ones, then higher again.\n\nThis makes iterative refinement hard or impossible.\n\n This makes iterative refinement hard or impossible. They propose IoU-Net, which predicts for each bounding box the expected IoU with the ground truth.\n\nThat predicted IoU is then used in NMS as a replacement for the classification score,\nthereby selecting for the bounding boxes that the model thinks have highest IoU.\nThe predicted IoU can also be used to perform iterative refinement, as it is more monotonic than the classification score.\n\n That predicted IoU is then used in NMS as a replacement for the classification score,\nthereby selecting for the bounding boxes that the model thinks have highest IoU. The predicted IoU can also be used to perform iterative refinement, as it is more monotonic than the classification score. They propose \"Precise RoI Pooling\" (PrRoI Pooling), which is a version of RoI-Pooling/RoI-Align that does not suffer from quantization inaccuracies. \nHow\n\nPredicting IoUs\n\nThey feed each RoI's features through two fully connected layers to regress an IoU value for that RoI.\nThey use one such branch per class.\nDuring training they augment the ground truth bounding boxes to generate examples with lower IoUs.\nVisualization:\n\n\n\n\nRelationship between predicted IoUs and real IoUs (right) vs. classification scores and real IoUs (left):\n\n\n\n\n\n\nIoU-guided NMS\n\nThis is essentially the same as classical NMS.\nThey use the predicted IoU values to estimate which box to keep if two are sufficiently overlapping.\nWhen one box is suppressed, the remaining box's class score is updated to max(s_1, s_2),\nwhere s_1 and s_2 are the class scores of the two boxes.\n\n\nOptimization-based refinement of bounding boxes\n\nThe branch to predict IoUs is differentiable.\nThey use this to iteratively improve predicted bounding boxes so that the predicted IoU is maximized.\n\nI.e. they backpropagate to the bounding box coordinates and change them according to the gradient.\nThey scale up the gradients according to the bounding box size on the given axis, which is similar to optimizing in log-space.\n\n\nThey add some early stopping criteria to not execute this indefinitely.\n\n\nPrecise RoI Pooling\n\nDuring bounding box refinement they use a more accurate (quantization free) RoI-Pooling.\nThey use bilinear sampling to approximate the feature value of any continuous location within the feature map.\nThen they use integration to calculate the pooled value (makes it sound like they always use global pooling?).\nSo while this method computes the integrals over all possible values within the given range, RoI Align only samples at N=4 locations per side.\nVisualization of the differences:\n\n\n\n\n\n\n\n Predicting IoUs\n\nThey feed each RoI's features through two fully connected layers to regress an IoU value for that RoI.\nThey use one such branch per class.\nDuring training they augment the ground truth bounding boxes to generate examples with lower IoUs.\nVisualization:\n\n\n\n\nRelationship between predicted IoUs and real IoUs (right) vs. classification scores and real IoUs (left):\n\n\n\n\n\n They feed each RoI's features through two fully connected layers to regress an IoU value for that RoI. They use one such branch per class. During training they augment the ground truth bounding boxes to generate examples with lower IoUs. Visualization:\n\n\n\n  Relationship between predicted IoUs and real IoUs (right) vs. classification scores and real IoUs (left):\n\n\n\n  IoU-guided NMS\n\nThis is essentially the same as classical NMS.\nThey use the predicted IoU values to estimate which box to keep if two are sufficiently overlapping.\nWhen one box is suppressed, the remaining box's class score is updated to max(s_1, s_2),\nwhere s_1 and s_2 are the class scores of the two boxes.\n\n This is essentially the same as classical NMS. They use the predicted IoU values to estimate which box to keep if two are sufficiently overlapping. When one box is suppressed, the remaining box's class score is updated to max(s_1, s_2),\nwhere s_1 and s_2 are the class scores of the two boxes. Optimization-based refinement of bounding boxes\n\nThe branch to predict IoUs is differentiable.\nThey use this to iteratively improve predicted bounding boxes so that the predicted IoU is maximized.\n\nI.e. they backpropagate to the bounding box coordinates and change them according to the gradient.\nThey scale up the gradients according to the bounding box size on the given axis, which is similar to optimizing in log-space.\n\n\nThey add some early stopping criteria to not execute this indefinitely.\n\n The branch to predict IoUs is differentiable. They use this to iteratively improve predicted bounding boxes so that the predicted IoU is maximized.\n\nI.e. they backpropagate to the bounding box coordinates and change them according to the gradient.\nThey scale up the gradients according to the bounding box size on the given axis, which is similar to optimizing in log-space.\n\n I.e. they backpropagate to the bounding box coordinates and change them according to the gradient. They scale up the gradients according to the bounding box size on the given axis, which is similar to optimizing in log-space. They add some early stopping criteria to not execute this indefinitely. Precise RoI Pooling\n\nDuring bounding box refinement they use a more accurate (quantization free) RoI-Pooling.\nThey use bilinear sampling to approximate the feature value of any continuous location within the feature map.\nThen they use integration to calculate the pooled value (makes it sound like they always use global pooling?).\nSo while this method computes the integrals over all possible values within the given range, RoI Align only samples at N=4 locations per side.\nVisualization of the differences:\n\n\n\n\n\n During bounding box refinement they use a more accurate (quantization free) RoI-Pooling. They use bilinear sampling to approximate the feature value of any continuous location within the feature map. Then they use integration to calculate the pooled value (makes it sound like they always use global pooling?). So while this method computes the integrals over all possible values within the given range, RoI Align only samples at N=4 locations per side. Visualization of the differences:\n\n\n\n  \nResults\n\nThey train and test on COCO.\nTheir model runs at about 300ms per image on a Titan X.\nIoU-guided NMS\n\nIoU-guided NMS performs slightly better than Soft-NMS and significantly better than classical NMS.\nIoU-guides NMS shines especially for high IoU APs, improving over Soft-NMS by 1 to 3 percentage points.\n\n\nOptimization-based refinement\n\nUsing optimization-based refinement also improves APs by around 1 to 3 percentage points (over no refinement).\nAgain, the improvement is most significant for high IoUs, reaching 5 to 6 percentage points better values.\n\n\nJoint Training\n\nTraining the IoU prediction branch jointly with a base network leads to slightly better features, improving APs by 0.4 to 0.6 points.\nCombining this with guided-NMS and optimization-based refinement, they improve the AP of a ResNet101+FPN baseline by 2.1 points.\n\n\n\n\n They train and test on COCO. Their model runs at about 300ms per image on a Titan X. IoU-guided NMS\n\nIoU-guided NMS performs slightly better than Soft-NMS and significantly better than classical NMS.\nIoU-guides NMS shines especially for high IoU APs, improving over Soft-NMS by 1 to 3 percentage points.\n\n IoU-guided NMS performs slightly better than Soft-NMS and significantly better than classical NMS. IoU-guides NMS shines especially for high IoU APs, improving over Soft-NMS by 1 to 3 percentage points. Optimization-based refinement\n\nUsing optimization-based refinement also improves APs by around 1 to 3 percentage points (over no refinement).\nAgain, the improvement is most significant for high IoUs, reaching 5 to 6 percentage points better values.\n\n Using optimization-based refinement also improves APs by around 1 to 3 percentage points (over no refinement). Again, the improvement is most significant for high IoUs, reaching 5 to 6 percentage points better values. Joint Training\n\nTraining the IoU prediction branch jointly with a base network leads to slightly better features, improving APs by 0.4 to 0.6 points.\nCombining this with guided-NMS and optimization-based refinement, they improve the AP of a ResNet101+FPN baseline by 2.1 points.\n\n\n Training the IoU prediction branch jointly with a base network leads to slightly better features, improving APs by 0.4 to 0.6 points. Combining this with guided-NMS and optimization-based refinement, they improve the AP of a ResNet101+FPN baseline by 2.1 points.  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper.pdf"
    },
    "5": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/CornerNet.md",
        "transcript": "\nWhat\n\nCurrent object detectors follow either a two-stage or single-stage approach.\n\nTwo-stage: Accurate, but rather slow as they use two different networks to predict RoIs and then classify them.\nSingle-stage\n\nAlmost as accurate as two-stage detectors nowadays and faster than them.\nSingle-stage detectors place a dense grid of anchor boxes on the image and classify for each of them whether they match an object in the image.\nThere has to be a large number of anchor boxes for different object sizes (and also possible more when the detectors works on multiple scales).\nThis increases memory demands, decreases performance and introduces difficult hyperparameter choices (number of anchors boxes and their sizes).\n\n\n\n\nThey develop an object detector that is single-stage and free of anchor boxes.\nTheir system predicts bounding boxes by localizing the top left and bottom right corner of an object.\n\nThey argue that predicting corners is easier than the anchor-based approach.\nThere are essentially exactly two points to predict per bounding box,\nwhile there can be many more valid anchors per object (that are than fine-tuned via regression to fit the object).\n\n\nThey also propose a corner pooling layer that complements their technique.\n\nPools along the horizontal/vertical axis.\nThis is useful for objects where no object parts are locally in the corners (e.g. imagine a frontal-view on a human with stretched out arms).\nThey argue that this pooling layer encodes prior knowledge of the prediction task.\n\n\n\n Current object detectors follow either a two-stage or single-stage approach.\n\nTwo-stage: Accurate, but rather slow as they use two different networks to predict RoIs and then classify them.\nSingle-stage\n\nAlmost as accurate as two-stage detectors nowadays and faster than them.\nSingle-stage detectors place a dense grid of anchor boxes on the image and classify for each of them whether they match an object in the image.\nThere has to be a large number of anchor boxes for different object sizes (and also possible more when the detectors works on multiple scales).\nThis increases memory demands, decreases performance and introduces difficult hyperparameter choices (number of anchors boxes and their sizes).\n\n\n\n Two-stage: Accurate, but rather slow as they use two different networks to predict RoIs and then classify them. Single-stage\n\nAlmost as accurate as two-stage detectors nowadays and faster than them.\nSingle-stage detectors place a dense grid of anchor boxes on the image and classify for each of them whether they match an object in the image.\nThere has to be a large number of anchor boxes for different object sizes (and also possible more when the detectors works on multiple scales).\nThis increases memory demands, decreases performance and introduces difficult hyperparameter choices (number of anchors boxes and their sizes).\n\n Almost as accurate as two-stage detectors nowadays and faster than them. Single-stage detectors place a dense grid of anchor boxes on the image and classify for each of them whether they match an object in the image. There has to be a large number of anchor boxes for different object sizes (and also possible more when the detectors works on multiple scales). This increases memory demands, decreases performance and introduces difficult hyperparameter choices (number of anchors boxes and their sizes). They develop an object detector that is single-stage and free of anchor boxes. Their system predicts bounding boxes by localizing the top left and bottom right corner of an object.\n\nThey argue that predicting corners is easier than the anchor-based approach.\nThere are essentially exactly two points to predict per bounding box,\nwhile there can be many more valid anchors per object (that are than fine-tuned via regression to fit the object).\n\n They argue that predicting corners is easier than the anchor-based approach.\nThere are essentially exactly two points to predict per bounding box,\nwhile there can be many more valid anchors per object (that are than fine-tuned via regression to fit the object). They also propose a corner pooling layer that complements their technique.\n\nPools along the horizontal/vertical axis.\nThis is useful for objects where no object parts are locally in the corners (e.g. imagine a frontal-view on a human with stretched out arms).\nThey argue that this pooling layer encodes prior knowledge of the prediction task.\n\n Pools along the horizontal/vertical axis. This is useful for objects where no object parts are locally in the corners (e.g. imagine a frontal-view on a human with stretched out arms). They argue that this pooling layer encodes prior knowledge of the prediction task. \nHow\n\nCorner Detection\n\nThey predict for each object class two heatmaps: One for top-left corners and one for bottom-right corners.\nThe ground truth heatmaps contain positive values at the locations of corners.\nGaussian Ground Truth\n\nThey argue that corners that are slightly off are still good hits reaching high IoUs.\nThey set the desired target IoU to t=0.7 and derive from that per corner pair k a radius r_k describing how far the corners can be off to still fulfill t.\nThey place on each ground truth location an unnormalized 2D gaussian e^(-(x^2 + y^2)/2sigma^2), where sigma=r_k/3.\n\n\nCorner Position Loss\n\nThey use a variant of Focal Loss, applied once for the top-left corner heatmaps and once for bottom-right corners.\n\nwhere\n\ny_cij is ground truth corner heatmap for class c at location y=i and x=j.\np_cij is the predicted heatmap.\nalpha=2, beta=4 are focal loss hyperparameters.\nN is the number of objects in one image.\n\n\n\n\nOffsets\n\nPredicted heatmaps are often downsampled compared to the ground truth heatmaps.\nThat can lead to predicted locations being slightly off compared to true locations.\nThey compensate for that by letting the network learn that offset (per spatial location).\nThe ground truth for the offset value is:\n\n\nwhere n is the downsampling factor.\n\n\nThey apply a smooth L1 loss to the offset predictions.\nThey apply the loss only at the ground truth corner locations.\n\n\n\n\nGrouping Corners\n\nTop-left and bottom-right corners have to be matched in order to create a full bounding box.\nThey do this by predicting embedding vectors for each top-left and bottom-right corner.\n\nThey train these to be similar for corners belonging to the same objects.\nThey train these to be different for corners belonging to different objects.\n\n\nFor (1) they use a pull loss and for (2) a push loss:\n\n\nwhere\n\ne_t_k is the embedding of the top left corner of object k.\ne_b_k is analogously is the embedding of the bottom right corner.\ne_k is the average of e_t_k and e_b_k.\nDelta=1.\nNote that there is no ground truth here, because only the distances matter.\n\n\n\n\nThey apply these losses only at the ground truth corner locations.\n\n\nCorner Pooling\n\nIn many cases, there is no local evidence for an object around its top-left or bottom-right corners.\nBut there is evidence around its top and left sides (analogously bottom and right sides).\nVisualization of the problem:\n\n\n\n\nThey compensate for that by max-pooling along the sides of each potential object.\nE.g. for the top-left corner they would max-pool along the corner's row and to the right,\nas well as along the corner's column und to the bottom.\nThey sum both of these results.\nVisualization:\n\n\n\n\n\n\nArchitecture\n\nThey use stacked hourglass networks as their backbone (with minor modifications, e.g. striding instead of max-pooling).\nThey place a corner pooling layer in residual fashion on top of the backbone.\nThey then apply three branches:\n\nCorner heatmaps branch (predicts 2*C channels for C classes).\nEmbeddings branch (predicts ?*C channels).\nOffsets branch (predicts 2*C channels).\n\n\nVisualization:\n\n\n\n\n\n\nBounding box extraction\n\nTo get bounding boxes out of their corner predictions, they first apply non-maximum suppression to their corner heatmaps via a 3x3 max pooling layer.\nThen they extract the top 100 top-left and bottom-right corners over all classes.\nThey shift them by the predicted offsets.\nThey pair per class the top-left and bottom-right corners with the most similar embeddings, rejecting anything with an L1 distance above 0.5.\nTo these bounding box candidates they apply soft-NMS to remove strongly overlapping bounding boxes.\n\n\n\n Corner Detection\n\nThey predict for each object class two heatmaps: One for top-left corners and one for bottom-right corners.\nThe ground truth heatmaps contain positive values at the locations of corners.\nGaussian Ground Truth\n\nThey argue that corners that are slightly off are still good hits reaching high IoUs.\nThey set the desired target IoU to t=0.7 and derive from that per corner pair k a radius r_k describing how far the corners can be off to still fulfill t.\nThey place on each ground truth location an unnormalized 2D gaussian e^(-(x^2 + y^2)/2sigma^2), where sigma=r_k/3.\n\n\nCorner Position Loss\n\nThey use a variant of Focal Loss, applied once for the top-left corner heatmaps and once for bottom-right corners.\n\nwhere\n\ny_cij is ground truth corner heatmap for class c at location y=i and x=j.\np_cij is the predicted heatmap.\nalpha=2, beta=4 are focal loss hyperparameters.\nN is the number of objects in one image.\n\n\n\n\nOffsets\n\nPredicted heatmaps are often downsampled compared to the ground truth heatmaps.\nThat can lead to predicted locations being slightly off compared to true locations.\nThey compensate for that by letting the network learn that offset (per spatial location).\nThe ground truth for the offset value is:\n\n\nwhere n is the downsampling factor.\n\n\nThey apply a smooth L1 loss to the offset predictions.\nThey apply the loss only at the ground truth corner locations.\n\n\n\n They predict for each object class two heatmaps: One for top-left corners and one for bottom-right corners. The ground truth heatmaps contain positive values at the locations of corners. Gaussian Ground Truth\n\nThey argue that corners that are slightly off are still good hits reaching high IoUs.\nThey set the desired target IoU to t=0.7 and derive from that per corner pair k a radius r_k describing how far the corners can be off to still fulfill t.\nThey place on each ground truth location an unnormalized 2D gaussian e^(-(x^2 + y^2)/2sigma^2), where sigma=r_k/3.\n\n They argue that corners that are slightly off are still good hits reaching high IoUs. They set the desired target IoU to t=0.7 and derive from that per corner pair k a radius r_k describing how far the corners can be off to still fulfill t. They place on each ground truth location an unnormalized 2D gaussian e^(-(x^2 + y^2)/2sigma^2), where sigma=r_k/3. Corner Position Loss\n\nThey use a variant of Focal Loss, applied once for the top-left corner heatmaps and once for bottom-right corners.\n\nwhere\n\ny_cij is ground truth corner heatmap for class c at location y=i and x=j.\np_cij is the predicted heatmap.\nalpha=2, beta=4 are focal loss hyperparameters.\nN is the number of objects in one image.\n\n\n\n They use a variant of Focal Loss, applied once for the top-left corner heatmaps and once for bottom-right corners.  where\n\ny_cij is ground truth corner heatmap for class c at location y=i and x=j.\np_cij is the predicted heatmap.\nalpha=2, beta=4 are focal loss hyperparameters.\nN is the number of objects in one image.\n\n y_cij is ground truth corner heatmap for class c at location y=i and x=j. p_cij is the predicted heatmap. alpha=2, beta=4 are focal loss hyperparameters. N is the number of objects in one image. Offsets\n\nPredicted heatmaps are often downsampled compared to the ground truth heatmaps.\nThat can lead to predicted locations being slightly off compared to true locations.\nThey compensate for that by letting the network learn that offset (per spatial location).\nThe ground truth for the offset value is:\n\n\nwhere n is the downsampling factor.\n\n\nThey apply a smooth L1 loss to the offset predictions.\nThey apply the loss only at the ground truth corner locations.\n\n Predicted heatmaps are often downsampled compared to the ground truth heatmaps. That can lead to predicted locations being slightly off compared to true locations. They compensate for that by letting the network learn that offset (per spatial location). The ground truth for the offset value is:\n\n\nwhere n is the downsampling factor.\n\n  where n is the downsampling factor. They apply a smooth L1 loss to the offset predictions. They apply the loss only at the ground truth corner locations. Grouping Corners\n\nTop-left and bottom-right corners have to be matched in order to create a full bounding box.\nThey do this by predicting embedding vectors for each top-left and bottom-right corner.\n\nThey train these to be similar for corners belonging to the same objects.\nThey train these to be different for corners belonging to different objects.\n\n\nFor (1) they use a pull loss and for (2) a push loss:\n\n\nwhere\n\ne_t_k is the embedding of the top left corner of object k.\ne_b_k is analogously is the embedding of the bottom right corner.\ne_k is the average of e_t_k and e_b_k.\nDelta=1.\nNote that there is no ground truth here, because only the distances matter.\n\n\n\n\nThey apply these losses only at the ground truth corner locations.\n\n Top-left and bottom-right corners have to be matched in order to create a full bounding box. They do this by predicting embedding vectors for each top-left and bottom-right corner.\n\nThey train these to be similar for corners belonging to the same objects.\nThey train these to be different for corners belonging to different objects.\n\n They train these to be similar for corners belonging to the same objects. They train these to be different for corners belonging to different objects. For (1) they use a pull loss and for (2) a push loss:\n\n\nwhere\n\ne_t_k is the embedding of the top left corner of object k.\ne_b_k is analogously is the embedding of the bottom right corner.\ne_k is the average of e_t_k and e_b_k.\nDelta=1.\nNote that there is no ground truth here, because only the distances matter.\n\n\n\n  where\n\ne_t_k is the embedding of the top left corner of object k.\ne_b_k is analogously is the embedding of the bottom right corner.\ne_k is the average of e_t_k and e_b_k.\nDelta=1.\nNote that there is no ground truth here, because only the distances matter.\n\n e_t_k is the embedding of the top left corner of object k. e_b_k is analogously is the embedding of the bottom right corner. e_k is the average of e_t_k and e_b_k. Delta=1. Note that there is no ground truth here, because only the distances matter. They apply these losses only at the ground truth corner locations. Corner Pooling\n\nIn many cases, there is no local evidence for an object around its top-left or bottom-right corners.\nBut there is evidence around its top and left sides (analogously bottom and right sides).\nVisualization of the problem:\n\n\n\n\nThey compensate for that by max-pooling along the sides of each potential object.\nE.g. for the top-left corner they would max-pool along the corner's row and to the right,\nas well as along the corner's column und to the bottom.\nThey sum both of these results.\nVisualization:\n\n\n\n\n\n In many cases, there is no local evidence for an object around its top-left or bottom-right corners. But there is evidence around its top and left sides (analogously bottom and right sides). Visualization of the problem:\n\n\n\n  They compensate for that by max-pooling along the sides of each potential object.\nE.g. for the top-left corner they would max-pool along the corner's row and to the right,\nas well as along the corner's column und to the bottom.\nThey sum both of these results. Visualization:\n\n\n\n  Architecture\n\nThey use stacked hourglass networks as their backbone (with minor modifications, e.g. striding instead of max-pooling).\nThey place a corner pooling layer in residual fashion on top of the backbone.\nThey then apply three branches:\n\nCorner heatmaps branch (predicts 2*C channels for C classes).\nEmbeddings branch (predicts ?*C channels).\nOffsets branch (predicts 2*C channels).\n\n\nVisualization:\n\n\n\n\n\n They use stacked hourglass networks as their backbone (with minor modifications, e.g. striding instead of max-pooling). They place a corner pooling layer in residual fashion on top of the backbone. They then apply three branches:\n\nCorner heatmaps branch (predicts 2*C channels for C classes).\nEmbeddings branch (predicts ?*C channels).\nOffsets branch (predicts 2*C channels).\n\n Corner heatmaps branch (predicts 2*C channels for C classes). Embeddings branch (predicts ?*C channels). Offsets branch (predicts 2*C channels). Visualization:\n\n\n\n  Bounding box extraction\n\nTo get bounding boxes out of their corner predictions, they first apply non-maximum suppression to their corner heatmaps via a 3x3 max pooling layer.\nThen they extract the top 100 top-left and bottom-right corners over all classes.\nThey shift them by the predicted offsets.\nThey pair per class the top-left and bottom-right corners with the most similar embeddings, rejecting anything with an L1 distance above 0.5.\nTo these bounding box candidates they apply soft-NMS to remove strongly overlapping bounding boxes.\n\n To get bounding boxes out of their corner predictions, they first apply non-maximum suppression to their corner heatmaps via a 3x3 max pooling layer. Then they extract the top 100 top-left and bottom-right corners over all classes. They shift them by the predicted offsets. They pair per class the top-left and bottom-right corners with the most similar embeddings, rejecting anything with an L1 distance above 0.5. To these bounding box candidates they apply soft-NMS to remove strongly overlapping bounding boxes. \nResults\n\nLoss weightings: They weight their corner heatmaps loss with 1.0, the offset loss also with 1.0 and the pull and push losses for the embeddings with 0.1 each.\nThey train on 10 PASCAL Titan X.\nFor inference they zero-pad images to the desired input size (511x511) and feed the padded image as well as its horizontally flipped version through the network.\nThey need about 244ms per image for inference.\nThey train and test on COCO.\nCorner Loss\n\nCorner Pooling is essential for the performance of the network.\nIt improves AP by about 2 percentage points.\nIt is especially important for large objects (+3.7 AP), not for small objects (+0.1 AP).\n\n\nLocation Penalty (via gaussians)\n\nThey investigate whether it is necessary to reduce the location penalty in the corner location heatmaps by using gaussians.\nThey compare using\n\nno penalty reduction (just set corner locations to 1, everything else to 0),\nplacing gaussians with a fixed radius of 2.5 and\nplacing gaussians with object-dependent radii.\n\n\nOption (3) performs best, (1) worst. (2) is in between the two options, usually half-way from (1) to (3).\nThey observe increases of AP between 5 and 6 percentage points when using (3) as opposed to (1).\nThey difference is more pronounced for large objects (about 12 points) as opposed to small objects (2.3 points).\n\n\nImportance of each branch\n\nThey evaluate which branch (corner location heatmaps, offsets, embeddings) has most influence on the AP.\nThey replace the predicted corner location heatmaps with ground truth heatmaps and increase AP by about 35 points (to 74.0%),\nsuggesting that their corner heatmap prediction is the main bottleneck.\nThey then add ground truth offsets and improve by 13.1 points (to 87.1%), suggesting that the offset prediction still has a significant impact on overall AP.\nThis leaves 12.9 points for the other components (embedding prediction, bounding box extraction).\n\n\nFinal results\n\nThey reach 42.1 AP in a multi-scale approach.\n(I guess they feed the images in at multiple scales? Not really explained.\nIn previous chapters they write specifically they don't use multi-scale feature maps,\nbut only the final feature map.)\nThey beat the best multi-scale competitor (RefineDet512) by 0.3 points.\nThey reach 40.5 AP in a single-scale approach. They beat the best single-scale competitor (RetinaNet800) by 1.4 points.\nExample predictions and extracted bounding boxes (each left: top-left corner, each right: bottom-right):\n\n\n\n\n\n\n\n Loss weightings: They weight their corner heatmaps loss with 1.0, the offset loss also with 1.0 and the pull and push losses for the embeddings with 0.1 each. They train on 10 PASCAL Titan X. For inference they zero-pad images to the desired input size (511x511) and feed the padded image as well as its horizontally flipped version through the network. They need about 244ms per image for inference. They train and test on COCO. Corner Loss\n\nCorner Pooling is essential for the performance of the network.\nIt improves AP by about 2 percentage points.\nIt is especially important for large objects (+3.7 AP), not for small objects (+0.1 AP).\n\n Corner Pooling is essential for the performance of the network. It improves AP by about 2 percentage points. It is especially important for large objects (+3.7 AP), not for small objects (+0.1 AP). Location Penalty (via gaussians)\n\nThey investigate whether it is necessary to reduce the location penalty in the corner location heatmaps by using gaussians.\nThey compare using\n\nno penalty reduction (just set corner locations to 1, everything else to 0),\nplacing gaussians with a fixed radius of 2.5 and\nplacing gaussians with object-dependent radii.\n\n\nOption (3) performs best, (1) worst. (2) is in between the two options, usually half-way from (1) to (3).\nThey observe increases of AP between 5 and 6 percentage points when using (3) as opposed to (1).\nThey difference is more pronounced for large objects (about 12 points) as opposed to small objects (2.3 points).\n\n They investigate whether it is necessary to reduce the location penalty in the corner location heatmaps by using gaussians. They compare using\n\nno penalty reduction (just set corner locations to 1, everything else to 0),\nplacing gaussians with a fixed radius of 2.5 and\nplacing gaussians with object-dependent radii.\n\n no penalty reduction (just set corner locations to 1, everything else to 0), placing gaussians with a fixed radius of 2.5 and placing gaussians with object-dependent radii. Option (3) performs best, (1) worst. (2) is in between the two options, usually half-way from (1) to (3). They observe increases of AP between 5 and 6 percentage points when using (3) as opposed to (1). They difference is more pronounced for large objects (about 12 points) as opposed to small objects (2.3 points). Importance of each branch\n\nThey evaluate which branch (corner location heatmaps, offsets, embeddings) has most influence on the AP.\nThey replace the predicted corner location heatmaps with ground truth heatmaps and increase AP by about 35 points (to 74.0%),\nsuggesting that their corner heatmap prediction is the main bottleneck.\nThey then add ground truth offsets and improve by 13.1 points (to 87.1%), suggesting that the offset prediction still has a significant impact on overall AP.\nThis leaves 12.9 points for the other components (embedding prediction, bounding box extraction).\n\n They evaluate which branch (corner location heatmaps, offsets, embeddings) has most influence on the AP. They replace the predicted corner location heatmaps with ground truth heatmaps and increase AP by about 35 points (to 74.0%),\nsuggesting that their corner heatmap prediction is the main bottleneck. They then add ground truth offsets and improve by 13.1 points (to 87.1%), suggesting that the offset prediction still has a significant impact on overall AP. This leaves 12.9 points for the other components (embedding prediction, bounding box extraction). Final results\n\nThey reach 42.1 AP in a multi-scale approach.\n(I guess they feed the images in at multiple scales? Not really explained.\nIn previous chapters they write specifically they don't use multi-scale feature maps,\nbut only the final feature map.)\nThey beat the best multi-scale competitor (RefineDet512) by 0.3 points.\nThey reach 40.5 AP in a single-scale approach. They beat the best single-scale competitor (RetinaNet800) by 1.4 points.\nExample predictions and extracted bounding boxes (each left: top-left corner, each right: bottom-right):\n\n\n\n\n\n They reach 42.1 AP in a multi-scale approach.\n(I guess they feed the images in at multiple scales? Not really explained.\nIn previous chapters they write specifically they don't use multi-scale feature maps,\nbut only the final feature map.)\nThey beat the best multi-scale competitor (RefineDet512) by 0.3 points. They reach 40.5 AP in a single-scale approach. They beat the best single-scale competitor (RetinaNet800) by 1.4 points. Example predictions and extracted bounding boxes (each left: top-left corner, each right: bottom-right):\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Hei_Law_CornerNet_Detecting_Objects_ECCV_2018_paper.pdf"
    },
    "6": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Group_Normalization.md",
        "transcript": "\nWhat\n\nThey propose Group Normalization (GN), a normalization technique similar to Batch Normalization (BN).\nIt works examplewise and splits the channels into groups before normalizing similar to BN.\n\nThey argue that this is similar to e.g. HOG or SIFT normalizing over groups.\n\n\nAs GN works examplewise it is not negatively affected by small batch sizes or batches with non-iid data.\n\nThis allows to train at tiny batch sizes, opening up GPU memory for larger models.\n\n\nTheir normalization is partially motivated from neuroscience, where some models assume normalization across cell responses.\n\n They propose Group Normalization (GN), a normalization technique similar to Batch Normalization (BN). It works examplewise and splits the channels into groups before normalizing similar to BN.\n\nThey argue that this is similar to e.g. HOG or SIFT normalizing over groups.\n\n They argue that this is similar to e.g. HOG or SIFT normalizing over groups. As GN works examplewise it is not negatively affected by small batch sizes or batches with non-iid data.\n\nThis allows to train at tiny batch sizes, opening up GPU memory for larger models.\n\n This allows to train at tiny batch sizes, opening up GPU memory for larger models. Their normalization is partially motivated from neuroscience, where some models assume normalization across cell responses. \nHow\n\nThere are multiple existing normalization techniques similar to GN, which normalize an (N, C, H, W) (examples, channels, height, width) based\non z-transformations, i.e. x'_i = (x_i - mean_i)/std_i. Each normalization technique works in different ways:\n\nBatch Normalization: Normalizes each channel along the (N, H, W) axes, i.e. computes statistics over all examples in the batch.\nHence it is dependent on the batch size and each example is influenced by all other examples in the batch.\n\nBN is the same as Instance Normalization for batch size 1.\n\n\nLayer Normalization: Normalizes along the (C, H, W) axes, i.e. computes statistics per example (one mean and one standard deviation value per example).\nInstance Normalization: Normalizes long the (H, W) axes, i.e. computes one mean and standard deviation per example and channel.\nOther examples or channels do not influence the computation.\nGroup Normalization (their method): Normalizes roughly along the (C, H, W) axes, similar to Layer Normalization.\nHowever, the channel axis is subdivided into groups of channels and the normalization happens over all channels within the same group\n(instead of over all channels in the example as in Layer Normalization).\n\nGN becomes Instance Normalization when the number of groups is set to the number of channels.\nGN becomes Layer Normalization when the number of groups is set to 1.\n\n\nVisualization of the relationship:\n\n\n\n\n\n\n\n There are multiple existing normalization techniques similar to GN, which normalize an (N, C, H, W) (examples, channels, height, width) based\non z-transformations, i.e. x'_i = (x_i - mean_i)/std_i. Each normalization technique works in different ways:\n\nBatch Normalization: Normalizes each channel along the (N, H, W) axes, i.e. computes statistics over all examples in the batch.\nHence it is dependent on the batch size and each example is influenced by all other examples in the batch.\n\nBN is the same as Instance Normalization for batch size 1.\n\n\nLayer Normalization: Normalizes along the (C, H, W) axes, i.e. computes statistics per example (one mean and one standard deviation value per example).\nInstance Normalization: Normalizes long the (H, W) axes, i.e. computes one mean and standard deviation per example and channel.\nOther examples or channels do not influence the computation.\nGroup Normalization (their method): Normalizes roughly along the (C, H, W) axes, similar to Layer Normalization.\nHowever, the channel axis is subdivided into groups of channels and the normalization happens over all channels within the same group\n(instead of over all channels in the example as in Layer Normalization).\n\nGN becomes Instance Normalization when the number of groups is set to the number of channels.\nGN becomes Layer Normalization when the number of groups is set to 1.\n\n\nVisualization of the relationship:\n\n\n\n\n\n Batch Normalization: Normalizes each channel along the (N, H, W) axes, i.e. computes statistics over all examples in the batch.\nHence it is dependent on the batch size and each example is influenced by all other examples in the batch.\n\nBN is the same as Instance Normalization for batch size 1.\n\n BN is the same as Instance Normalization for batch size 1. Layer Normalization: Normalizes along the (C, H, W) axes, i.e. computes statistics per example (one mean and one standard deviation value per example). Instance Normalization: Normalizes long the (H, W) axes, i.e. computes one mean and standard deviation per example and channel.\nOther examples or channels do not influence the computation. Group Normalization (their method): Normalizes roughly along the (C, H, W) axes, similar to Layer Normalization.\nHowever, the channel axis is subdivided into groups of channels and the normalization happens over all channels within the same group\n(instead of over all channels in the example as in Layer Normalization).\n\nGN becomes Instance Normalization when the number of groups is set to the number of channels.\nGN becomes Layer Normalization when the number of groups is set to 1.\n\n GN becomes Instance Normalization when the number of groups is set to the number of channels. GN becomes Layer Normalization when the number of groups is set to 1. Visualization of the relationship:\n\n\n\n  \nResults\n\nThey test on ImageNet, training on 8 GPUs. They use ResNet-50. Their default number of groups in GN is 32.\nAt batch size 32: They observe 0.5 percentage points higher validation error with GN as opposed to BN (layer normalization: 1.7 points worse, instance normalization: 4.8 points worse).\n\nThey observe slightly lower training error of GN compared to BN.\nThey argue that BN profits from a regularizing effect from the examples in each batch being randomly picked (i.e. mean and variance are a bit stochastic).\n\n\nAt batch size 2: GN's accuracy remains constant (compared to batch size 32), while BN degrades significantly. GN beats BN by 10.6 points.\n\nThey also compare to Batch Renormalization with well chosen hyperparameters and find that it improves BN's performance, but it still performs 2.1 points worse than GN.\n\n\nAccuracy vs. batch size:\n\n\n\n\nChannels per group\n\nThey found on ImageNet (with ResNet-50) that around 8 or more channels per group performs best.\n\n\n\nThey get similar results when training with ResNet-101.\nVGG-16\n\nThey trained VGG-16 on ImageNet with no normalization, BN and GN.\nThey observed that the feature maps before normalization and ReLU had a significantly wider value range (-80 to +20)\nwhen using no normalization as opposed to using normalization (BN: -3 to +3, GN: -4 to +3).\n\n\nObject Detection and Segmentation on COCO\n\nThey compare BN and GN for object detection and segmentation on COCO using Mask R-CNN with different backbones.\nResNet-50 backbone\n\nUsing GN instead of (frozen) BN improves box AP by 1.1 points and mask AP by 0.8 points.\n\n\nFeature Pyramid Network (FPN) backbone\n\nThey compare using BN and GN only in the head of Mask R-CNN.\nBN performs significantly worse (9 points) than GN, even though the batch size is at 512 RoIs.\nThey argue that this is because the examples are not iid (they all come from the same image).\nGN in the backbone improves AP by 0.5 points.\nUsing GN also allows them to train the whole model from scratch (i.e. no pretraining) without training instabilities.\nThey reach scores very close to the pretrained versions.\n\n\n\n\nVideo classification on Kinetics\n\nThey test on the Kinetics dataset, which requires classifying videos and hence leads to large memory consumption per example.\nThey use a ResNet-50 Inflated 3D (I3D) network as baseline.\nNormalization is extended from (H,W) axes to (T,H,W) axes, adding the temporal dimension.\nGN performs overall superior to BN by up to 1.2 points accuracy.\nGN profits from not sub-sampling frames (e.g. using only every second frame as input).\nBN however does not improve its accuracy from deactivating subsampling, because that also required reducing the batch size from 8 to 4 examples.\nThe positive effect of deactivating subsampling was masked by the negative effect of reducing the batch size,\nleading to a false conclusion that subsampling does not degrade accuracy.\n\n\n\n They test on ImageNet, training on 8 GPUs. They use ResNet-50. Their default number of groups in GN is 32. At batch size 32: They observe 0.5 percentage points higher validation error with GN as opposed to BN (layer normalization: 1.7 points worse, instance normalization: 4.8 points worse).\n\nThey observe slightly lower training error of GN compared to BN.\nThey argue that BN profits from a regularizing effect from the examples in each batch being randomly picked (i.e. mean and variance are a bit stochastic).\n\n They observe slightly lower training error of GN compared to BN.\nThey argue that BN profits from a regularizing effect from the examples in each batch being randomly picked (i.e. mean and variance are a bit stochastic). At batch size 2: GN's accuracy remains constant (compared to batch size 32), while BN degrades significantly. GN beats BN by 10.6 points.\n\nThey also compare to Batch Renormalization with well chosen hyperparameters and find that it improves BN's performance, but it still performs 2.1 points worse than GN.\n\n They also compare to Batch Renormalization with well chosen hyperparameters and find that it improves BN's performance, but it still performs 2.1 points worse than GN. Accuracy vs. batch size:\n\n\n\n  Channels per group\n\nThey found on ImageNet (with ResNet-50) that around 8 or more channels per group performs best.\n\n\n They found on ImageNet (with ResNet-50) that around 8 or more channels per group performs best.  They get similar results when training with ResNet-101. VGG-16\n\nThey trained VGG-16 on ImageNet with no normalization, BN and GN.\nThey observed that the feature maps before normalization and ReLU had a significantly wider value range (-80 to +20)\nwhen using no normalization as opposed to using normalization (BN: -3 to +3, GN: -4 to +3).\n\n They trained VGG-16 on ImageNet with no normalization, BN and GN. They observed that the feature maps before normalization and ReLU had a significantly wider value range (-80 to +20)\nwhen using no normalization as opposed to using normalization (BN: -3 to +3, GN: -4 to +3). Object Detection and Segmentation on COCO\n\nThey compare BN and GN for object detection and segmentation on COCO using Mask R-CNN with different backbones.\nResNet-50 backbone\n\nUsing GN instead of (frozen) BN improves box AP by 1.1 points and mask AP by 0.8 points.\n\n\nFeature Pyramid Network (FPN) backbone\n\nThey compare using BN and GN only in the head of Mask R-CNN.\nBN performs significantly worse (9 points) than GN, even though the batch size is at 512 RoIs.\nThey argue that this is because the examples are not iid (they all come from the same image).\nGN in the backbone improves AP by 0.5 points.\nUsing GN also allows them to train the whole model from scratch (i.e. no pretraining) without training instabilities.\nThey reach scores very close to the pretrained versions.\n\n\n\n They compare BN and GN for object detection and segmentation on COCO using Mask R-CNN with different backbones. ResNet-50 backbone\n\nUsing GN instead of (frozen) BN improves box AP by 1.1 points and mask AP by 0.8 points.\n\n Using GN instead of (frozen) BN improves box AP by 1.1 points and mask AP by 0.8 points. Feature Pyramid Network (FPN) backbone\n\nThey compare using BN and GN only in the head of Mask R-CNN.\nBN performs significantly worse (9 points) than GN, even though the batch size is at 512 RoIs.\nThey argue that this is because the examples are not iid (they all come from the same image).\nGN in the backbone improves AP by 0.5 points.\nUsing GN also allows them to train the whole model from scratch (i.e. no pretraining) without training instabilities.\nThey reach scores very close to the pretrained versions.\n\n They compare using BN and GN only in the head of Mask R-CNN.\nBN performs significantly worse (9 points) than GN, even though the batch size is at 512 RoIs.\nThey argue that this is because the examples are not iid (they all come from the same image). GN in the backbone improves AP by 0.5 points. Using GN also allows them to train the whole model from scratch (i.e. no pretraining) without training instabilities.\nThey reach scores very close to the pretrained versions. Video classification on Kinetics\n\nThey test on the Kinetics dataset, which requires classifying videos and hence leads to large memory consumption per example.\nThey use a ResNet-50 Inflated 3D (I3D) network as baseline.\nNormalization is extended from (H,W) axes to (T,H,W) axes, adding the temporal dimension.\nGN performs overall superior to BN by up to 1.2 points accuracy.\nGN profits from not sub-sampling frames (e.g. using only every second frame as input).\nBN however does not improve its accuracy from deactivating subsampling, because that also required reducing the batch size from 8 to 4 examples.\nThe positive effect of deactivating subsampling was masked by the negative effect of reducing the batch size,\nleading to a false conclusion that subsampling does not degrade accuracy.\n\n They test on the Kinetics dataset, which requires classifying videos and hence leads to large memory consumption per example. They use a ResNet-50 Inflated 3D (I3D) network as baseline. Normalization is extended from (H,W) axes to (T,H,W) axes, adding the temporal dimension. GN performs overall superior to BN by up to 1.2 points accuracy. GN profits from not sub-sampling frames (e.g. using only every second frame as input).\nBN however does not improve its accuracy from deactivating subsampling, because that also required reducing the batch size from 8 to 4 examples.\nThe positive effect of deactivating subsampling was masked by the negative effect of reducing the batch size,\nleading to a false conclusion that subsampling does not degrade accuracy. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf"
    },
    "7": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Convolutional_Networks_with_Adaptive_Inference_Graphs.md",
        "transcript": "\nWhat\n\nThey introduce a dynamic gating mechanism for ResNet that decides on-the-fly which layers to execute for a given input image.\nThis improves performance, classification accuracy and robustness to adversarial attacks.\nNote that the decision of whether to execute a layer during training is stochastic, making it closely related to stochastic depth.\n\n They introduce a dynamic gating mechanism for ResNet that decides on-the-fly which layers to execute for a given input image. This improves performance, classification accuracy and robustness to adversarial attacks. Note that the decision of whether to execute a layer during training is stochastic, making it closely related to stochastic depth. \nHow\n\nTheir technique is based on residual connections, i.e. on ResNet.\nThey add a gating function, so that x_l = x_{l-1} + F(x_{l-1}) becomes x_l = x_{l-1} + gate(x_{l-1}) * F(x_{l-1}).\nGating\n\nThe gating function produces a discrete output (0 or 1), enabling to completely skip a layer.\nThey implement the gating function by applying the following steps to an input feature map:\n\nGlobal average pooling, leads to Cx1x1 output.\nFlatten to vector\nApply fully connected layer with d outputs and ReLU\nApply fully connected layer with 2 outputs\nApply straight-through gumbel-softmax to output\n\n\nStraight-through Gumbel-softmax is a standard technique that is comparable to softmax,\nbut produces discrete 0/1 outputs during the forward pass and uses a softmax during the backward pass to approximate gradients.\nFor small d, the additional execution time of the gating function is negligible.\nVisualization:\n\n\n\n\n\n\nThey also introduce a target rate loss, which pushes the network to execute t percent of all layers (i.e. let t percent of all gates produce 1 as the output):\n\n\nwhere z_l is the fraction of gates that produced 1 for layer l within the minibatch\nand t is the target rate.\nNote that this will probably cause problems for small minibatches.\n\n\n\n Their technique is based on residual connections, i.e. on ResNet. They add a gating function, so that x_l = x_{l-1} + F(x_{l-1}) becomes x_l = x_{l-1} + gate(x_{l-1}) * F(x_{l-1}). Gating\n\nThe gating function produces a discrete output (0 or 1), enabling to completely skip a layer.\nThey implement the gating function by applying the following steps to an input feature map:\n\nGlobal average pooling, leads to Cx1x1 output.\nFlatten to vector\nApply fully connected layer with d outputs and ReLU\nApply fully connected layer with 2 outputs\nApply straight-through gumbel-softmax to output\n\n\nStraight-through Gumbel-softmax is a standard technique that is comparable to softmax,\nbut produces discrete 0/1 outputs during the forward pass and uses a softmax during the backward pass to approximate gradients.\nFor small d, the additional execution time of the gating function is negligible.\nVisualization:\n\n\n\n\n\n The gating function produces a discrete output (0 or 1), enabling to completely skip a layer. They implement the gating function by applying the following steps to an input feature map:\n\nGlobal average pooling, leads to Cx1x1 output.\nFlatten to vector\nApply fully connected layer with d outputs and ReLU\nApply fully connected layer with 2 outputs\nApply straight-through gumbel-softmax to output\n\n Global average pooling, leads to Cx1x1 output. Flatten to vector Apply fully connected layer with d outputs and ReLU Apply fully connected layer with 2 outputs Apply straight-through gumbel-softmax to output Straight-through Gumbel-softmax is a standard technique that is comparable to softmax,\nbut produces discrete 0/1 outputs during the forward pass and uses a softmax during the backward pass to approximate gradients. For small d, the additional execution time of the gating function is negligible. Visualization:\n\n\n\n  They also introduce a target rate loss, which pushes the network to execute t percent of all layers (i.e. let t percent of all gates produce 1 as the output):\n\n\nwhere z_l is the fraction of gates that produced 1 for layer l within the minibatch\nand t is the target rate.\nNote that this will probably cause problems for small minibatches.\n\n  where z_l is the fraction of gates that produced 1 for layer l within the minibatch and t is the target rate. Note that this will probably cause problems for small minibatches. \nResults\n\nCIFAR-10\n\nThey choose d=16, adds 0.01% FLOPS and 4.8% parameters to ResNet-110.\nAt t=0.7 they observe that on average 82% of all layers are executed.\n\nThe model chooses to always execute downsampling layers. They seem to be important.\n\n\nWhen always executing all layers and scaling their outputs with their likelihood of being executed (similar to Dropout, Stochastic Depth),\nthey achieve higher score than a network trained with stochastic depth. This indicates that their results do not just come from a regularizing effect.\n\n\n\nImageNet\n\nThey compare gated ResNet-50 and ResNet-110.\nIn ResNet-110 they always execute the first couple of layers (ungated), in ResNet-50 they gate all layers.\nTrade-off between FLOPs and accuracy (ResNet-50/101 vs their model \"ConvNet-AIG\" at different t):\n\n\nTheir model becomes more accurate as t is lowered down from 1.0 to 0.7. After that the accuracy starts to drop.\n\n\nVisualization of layerwise execution rate:\n\n\nThe model learns to always execute downsampling layers.\nThe model seems to treat all man-made objects and all animals as two different groups with similar layers.\n\n\nExecution distribution and execution frequency during training:\n\n\nThe model quickly learns to always execute the last layer.\n\n\nThe model requires especially many layers to classify non-iconic views of objects (e.g. dog face from unusual perspective instead of whole dog from the side).\nWhen running adversarial attacks on ResNet-50 and their ConvNet-AIG-50 (via Fast Gradient Sign Attack) they observe that:\n\nTheir model is overall more robust to adversarial attacks, i.e. keeps higher accuracy (both with/without protection via JPEG compression).\nThe executed layers remain mostly the same in their model. They seem to not get attacked.\n\n\n\n\n\n CIFAR-10\n\nThey choose d=16, adds 0.01% FLOPS and 4.8% parameters to ResNet-110.\nAt t=0.7 they observe that on average 82% of all layers are executed.\n\nThe model chooses to always execute downsampling layers. They seem to be important.\n\n\nWhen always executing all layers and scaling their outputs with their likelihood of being executed (similar to Dropout, Stochastic Depth),\nthey achieve higher score than a network trained with stochastic depth. This indicates that their results do not just come from a regularizing effect.\n\n\n They choose d=16, adds 0.01% FLOPS and 4.8% parameters to ResNet-110. At t=0.7 they observe that on average 82% of all layers are executed.\n\nThe model chooses to always execute downsampling layers. They seem to be important.\n\n The model chooses to always execute downsampling layers. They seem to be important. When always executing all layers and scaling their outputs with their likelihood of being executed (similar to Dropout, Stochastic Depth),\nthey achieve higher score than a network trained with stochastic depth. This indicates that their results do not just come from a regularizing effect.  ImageNet\n\nThey compare gated ResNet-50 and ResNet-110.\nIn ResNet-110 they always execute the first couple of layers (ungated), in ResNet-50 they gate all layers.\nTrade-off between FLOPs and accuracy (ResNet-50/101 vs their model \"ConvNet-AIG\" at different t):\n\n\nTheir model becomes more accurate as t is lowered down from 1.0 to 0.7. After that the accuracy starts to drop.\n\n\nVisualization of layerwise execution rate:\n\n\nThe model learns to always execute downsampling layers.\nThe model seems to treat all man-made objects and all animals as two different groups with similar layers.\n\n\nExecution distribution and execution frequency during training:\n\n\nThe model quickly learns to always execute the last layer.\n\n\nThe model requires especially many layers to classify non-iconic views of objects (e.g. dog face from unusual perspective instead of whole dog from the side).\nWhen running adversarial attacks on ResNet-50 and their ConvNet-AIG-50 (via Fast Gradient Sign Attack) they observe that:\n\nTheir model is overall more robust to adversarial attacks, i.e. keeps higher accuracy (both with/without protection via JPEG compression).\nThe executed layers remain mostly the same in their model. They seem to not get attacked.\n\n\n\n They compare gated ResNet-50 and ResNet-110. In ResNet-110 they always execute the first couple of layers (ungated), in ResNet-50 they gate all layers. Trade-off between FLOPs and accuracy (ResNet-50/101 vs their model \"ConvNet-AIG\" at different t):\n\n\nTheir model becomes more accurate as t is lowered down from 1.0 to 0.7. After that the accuracy starts to drop.\n\n  Their model becomes more accurate as t is lowered down from 1.0 to 0.7. After that the accuracy starts to drop. Visualization of layerwise execution rate:\n\n\nThe model learns to always execute downsampling layers.\nThe model seems to treat all man-made objects and all animals as two different groups with similar layers.\n\n  The model learns to always execute downsampling layers. The model seems to treat all man-made objects and all animals as two different groups with similar layers. Execution distribution and execution frequency during training:\n\n\nThe model quickly learns to always execute the last layer.\n\n  The model quickly learns to always execute the last layer. The model requires especially many layers to classify non-iconic views of objects (e.g. dog face from unusual perspective instead of whole dog from the side). When running adversarial attacks on ResNet-50 and their ConvNet-AIG-50 (via Fast Gradient Sign Attack) they observe that:\n\nTheir model is overall more robust to adversarial attacks, i.e. keeps higher accuracy (both with/without protection via JPEG compression).\nThe executed layers remain mostly the same in their model. They seem to not get attacked.\n\n Their model is overall more robust to adversarial attacks, i.e. keeps higher accuracy (both with/without protection via JPEG compression). The executed layers remain mostly the same in their model. They seem to not get attacked. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Andreas_Veit_Convolutional_Networks_with_ECCV_2018_paper.pdf"
    },
    "8": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/STN.md",
        "transcript": "\nWhat:\n\nThey introduced a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network.\n\n\n They introduced a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network.\n \nHow:\n\nSpatial Transformer allows the spatial manipulation of the data (any feature map or particularly input image). This differentiable module can be inserted into any CNN, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself.\nThe action of the spatial transformer is conditioned on individual data samples, with the appropriate behavior learned during training for the task in question.\nNo additional supervision or modification of the optimization process is required.\nSpatial manipulation consists of cropping, translation, rotation, scale, and skew.\n \nSTN structure:\n\nLocalization net: predicts parameters of the transform theta. For 2d case, it's 2 x 3 matrix. For 3d case, it's 3 x 4 matrix.\nGrid generator: Uses predictions of Localization net to create a sampling grid, which is a set of points where the input map should be sampled to produce the transformed output.\nSampler: Produces the output map sampled from the input feature map at the predicted grid points.\n\n\n\n Spatial Transformer allows the spatial manipulation of the data (any feature map or particularly input image). This differentiable module can be inserted into any CNN, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself. The action of the spatial transformer is conditioned on individual data samples, with the appropriate behavior learned during training for the task in question. No additional supervision or modification of the optimization process is required. Spatial manipulation consists of cropping, translation, rotation, scale, and skew.\n  STN structure:\n\nLocalization net: predicts parameters of the transform theta. For 2d case, it's 2 x 3 matrix. For 3d case, it's 3 x 4 matrix.\nGrid generator: Uses predictions of Localization net to create a sampling grid, which is a set of points where the input map should be sampled to produce the transformed output.\nSampler: Produces the output map sampled from the input feature map at the predicted grid points.\n\n Localization net: predicts parameters of the transform theta. For 2d case, it's 2 x 3 matrix. For 3d case, it's 3 x 4 matrix. Grid generator: Uses predictions of Localization net to create a sampling grid, which is a set of points where the input map should be sampled to produce the transformed output. Sampler: Produces the output map sampled from the input feature map at the predicted grid points. \nNotes:\n\nLocalization net can predict several transformations(thetas) for subsequent transformation applied to the input image(feature map).\n\nThe final regression layer should be initialized to regress the identity transform (zero weights, identity transform bias).\n\n\nGrid generator and Transforms:\n\nThe transformation can have any parameterized form, provided that it is differentiable with respect to the parameters\nThe most popular is just a 2d affine transform: \n\nor particularly an attention mechanism: \n\nThe source/target transformation and sampling is equivalent to the standard texture mapping and coordinates used in graphics.\n\n\nSampler:\n\nThe key why STN works. They introduced a (sub-)differentiable sampling mechanism that allows loss gradients to flow back not only to the \"input\" feature map, but also to the sampling grid coordinates, and therefore back to the transformation parameters \u03b8 and Localisation Net.\n\n\n\n Localization net can predict several transformations(thetas) for subsequent transformation applied to the input image(feature map).\n\nThe final regression layer should be initialized to regress the identity transform (zero weights, identity transform bias).\n\n The final regression layer should be initialized to regress the identity transform (zero weights, identity transform bias). Grid generator and Transforms:\n\nThe transformation can have any parameterized form, provided that it is differentiable with respect to the parameters\nThe most popular is just a 2d affine transform: \n\nor particularly an attention mechanism: \n\nThe source/target transformation and sampling is equivalent to the standard texture mapping and coordinates used in graphics.\n\n The transformation can have any parameterized form, provided that it is differentiable with respect to the parameters The most popular is just a 2d affine transform: \n\nor particularly an attention mechanism: \n The source/target transformation and sampling is equivalent to the standard texture mapping and coordinates used in graphics. Sampler:\n\nThe key why STN works. They introduced a (sub-)differentiable sampling mechanism that allows loss gradients to flow back not only to the \"input\" feature map, but also to the sampling grid coordinates, and therefore back to the transformation parameters \u03b8 and Localisation Net.\n\n The key why STN works. They introduced a (sub-)differentiable sampling mechanism that allows loss gradients to flow back not only to the \"input\" feature map, but also to the sampling grid coordinates, and therefore back to the transformation parameters \u03b8 and Localisation Net. \nResults:\n\nStreet View House Numbers multi-digit recognition:\n\nDistored MNIST:\n\nCUB-200-2011 birds dataset:\n\nMNIST addition:\n\n\n Street View House Numbers multi-digit recognition:\n Distored MNIST:\n CUB-200-2011 birds dataset:\n MNIST addition:\n \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.02025"
    },
    "9": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/HardNet.md",
        "transcript": "\nWhat:\n\nHardNet model which improves state-of-the-art in wide baseline stereo, patch matching, verification and image retrieval.\nThey introduced a new triplet-like loss function with built-in hard-negative mining.\n\n\n HardNet model which improves state-of-the-art in wide baseline stereo, patch matching, verification and image retrieval. They introduced a new triplet-like loss function with built-in hard-negative mining.  \nHow:\n\nHardNet Triplet loss is a regular Triplet-Loss, i.e. MAX(0, alpha + distances_to_positives - distances_to_negatives), where:\n\nalpha (sometimes called \"margin\") is a hyper-parameter\ndistance_to_positives are distances (here, L2 is used)\ndistance_to_negative are distances to the hardest negatives for each anchor in a batch.\n\n\nAs input HardNet operates with N * 2 images (N anchor/query images and N corresponding to them positives)\nMining algorithm:\n1. Compute distance matrix D between N anchors and N positives.\n2. distances_to_positives = trace of distance matrix (diagonal elements)\n3. For each row minimal non-diagonal element is taken as a distance to the hardest negatives (closest to anchor). From these chosen values distances_to_negatives are obtained.\n\nAll this can be rewritten as:\n\nLoss = MAX(0, alpha + Trace(D) + row_wise_min(D + I * inf)), where I is the identity matrix.\n\n\n\n\nArchitecture:\n\n\n\n\n\n HardNet Triplet loss is a regular Triplet-Loss, i.e. MAX(0, alpha + distances_to_positives - distances_to_negatives), where:\n\nalpha (sometimes called \"margin\") is a hyper-parameter\ndistance_to_positives are distances (here, L2 is used)\ndistance_to_negative are distances to the hardest negatives for each anchor in a batch.\n\n alpha (sometimes called \"margin\") is a hyper-parameter distance_to_positives are distances (here, L2 is used) distance_to_negative are distances to the hardest negatives for each anchor in a batch. As input HardNet operates with N * 2 images (N anchor/query images and N corresponding to them positives) Mining algorithm:\n1. Compute distance matrix D between N anchors and N positives.\n2. distances_to_positives = trace of distance matrix (diagonal elements)\n3. For each row minimal non-diagonal element is taken as a distance to the hardest negatives (closest to anchor). From these chosen values distances_to_negatives are obtained.\n\nAll this can be rewritten as:\n\nLoss = MAX(0, alpha + Trace(D) + row_wise_min(D + I * inf)), where I is the identity matrix.\n\n\n\n All this can be rewritten as:\n\nLoss = MAX(0, alpha + Trace(D) + row_wise_min(D + I * inf)), where I is the identity matrix.\n\n Loss = MAX(0, alpha + Trace(D) + row_wise_min(D + I * inf)), where I is the identity matrix. Architecture:\n\n\n\n  \nNotes:\n\nThe described mining procedure highly relies on a fact that all N anchors would should to N different classes.\nAnd from my personal point of view requires minor modification to handle such corner case.\nThe given loss/mining procedure is fast, but in contrast to other mining strategies doesn't provide hardest positive (furthest from anchor).\n\n The described mining procedure highly relies on a fact that all N anchors would should to N different classes.\nAnd from my personal point of view requires minor modification to handle such corner case. The given loss/mining procedure is fast, but in contrast to other mining strategies doesn't provide hardest positive (furthest from anchor). \nResults:\n\nWide baseline stereo example:\n\n\n\n\nThe bigger batch size, the better:\n\n\n\n\nPhotoTour Patch Verification Results:\n\n\n\n\nOxford 5k, Paris 6k Patch Verification Results:\n\n\n\n\n\n Wide baseline stereo example:\n\n\n\n  The bigger batch size, the better:\n\n\n\n  PhotoTour Patch Verification Results:\n\n\n\n  Oxford 5k, Paris 6k Patch Verification Results:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.10872"
    },
    "10": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/NAN_for_Video_Face_Recognition.md",
        "transcript": "What\n\nThey suggest a method to get cumulative/aggregated embedding from a sequence of embeddings (i.e get a single face embedding vector from a video).\n\n They suggest a method to get cumulative/aggregated embedding from a sequence of embeddings (i.e get a single face embedding vector from a video). \nHow\n\n\nUse attention mechanism to weight embeddings in a sequence.\n\n\nThey suggest two options:\n\n\nSingle attention block \u2013 Universal face feature quality measurement.\n \nwhere f_k = embedding for k-th image in a sequence, a_k = obtained weight corresponded to k-th embedding\nTrainable parameter is: q (shape = embedding size x 1)\n\n\nCascaded two attention blocks \u2013 Content-aware aggregation.\n\nThis q^1 replaces the q in above formula that computes coefficients a_k.\nTrainable parameters are: W (shape = embeddings size x embeddings size), b (shape = embedding size x 1).\n\n\n\n\nFace embedder (could be any CNN) and \"Attention blocks\" can be trained together in end-to-end manner or separately one-by-one.\n\n\nTraining procedure:\n\nFor verification problem they used siamese structure with contrastive loss.\nFor identification problem they used softmax and cross-entropy as loss function.\n\n\n\nNo recurrent blocks, but still input size independent.\n\n\nCoefficients a (from the first attention block) strongly correlates with face quality and it's usefulness for recognition.\n\n\n\n \nUse attention mechanism to weight embeddings in a sequence.\n \nThey suggest two options:\n\n\nSingle attention block \u2013 Universal face feature quality measurement.\n \nwhere f_k = embedding for k-th image in a sequence, a_k = obtained weight corresponded to k-th embedding\nTrainable parameter is: q (shape = embedding size x 1)\n\n\nCascaded two attention blocks \u2013 Content-aware aggregation.\n\nThis q^1 replaces the q in above formula that computes coefficients a_k.\nTrainable parameters are: W (shape = embeddings size x embeddings size), b (shape = embedding size x 1).\n\n\n \nSingle attention block \u2013 Universal face feature quality measurement.\n \nwhere f_k = embedding for k-th image in a sequence, a_k = obtained weight corresponded to k-th embedding\nTrainable parameter is: q (shape = embedding size x 1)\n \nCascaded two attention blocks \u2013 Content-aware aggregation.\n\nThis q^1 replaces the q in above formula that computes coefficients a_k.\nTrainable parameters are: W (shape = embeddings size x embeddings size), b (shape = embedding size x 1).\n \nFace embedder (could be any CNN) and \"Attention blocks\" can be trained together in end-to-end manner or separately one-by-one.\n \nTraining procedure:\n\nFor verification problem they used siamese structure with contrastive loss.\nFor identification problem they used softmax and cross-entropy as loss function.\n\n For verification problem they used siamese structure with contrastive loss. For identification problem they used softmax and cross-entropy as loss function. \nNo recurrent blocks, but still input size independent.\n \nCoefficients a (from the first attention block) strongly correlates with face quality and it's usefulness for recognition.\n\n \nResults\n\nShows better results than combining a single embedding by taking mean, median, l2/cos closest, etc.\nShows state-of-the-art performance on YouTubeFaces and IJB-A datasets.\n\n Shows better results than combining a single embedding by taking mean, median, l2/cos closest, etc. Shows state-of-the-art performance on YouTubeFaces and IJB-A datasets. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.05474"
    },
    "11": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Critical_Learning_Periods_in_Deep_Neural_Networks.md",
        "transcript": "\nWhat\n\nThey find that artificial neural networks show critical periods, similar to biological neural networks.\nCritical periods in biological neural networks are phases in the network's development\nduring which a malformed sensory input\ncan irreversibly harm the capabilities of the network.\n\nE. g. a disease causing blurry vision for some time during early development\ncan permanently harm the ability to interpret visual stimuli.\nThe same disease very early or later in life will cause no permanent damage (after being healing).\n\n\n\n They find that artificial neural networks show critical periods, similar to biological neural networks. Critical periods in biological neural networks are phases in the network's development\nduring which a malformed sensory input\ncan irreversibly harm the capabilities of the network.\n\nE. g. a disease causing blurry vision for some time during early development\ncan permanently harm the ability to interpret visual stimuli.\nThe same disease very early or later in life will cause no permanent damage (after being healing).\n\n E. g. a disease causing blurry vision for some time during early development\ncan permanently harm the ability to interpret visual stimuli.\nThe same disease very early or later in life will cause no permanent damage (after being healing). \nHow\n\nBlur\n\nThey train networks in CIFAR10 and blur all input images by down- and then upscaling.\nThey vary when the blurring is removed.\nThey always train for 300 epochs more after the blurring was removed.\nSo the network always sees the unaltered input images for at least the same amount of epochs.\n\n\nVertical flipping\n\nSame as blurring, but they vertically flip the image.\nThis is expected to keep low and mid level statistics the same. Only the final layers have to change.\nThis is expected to be easy to adapt to, even if the flipping is removed fairly late into the training.\n\n\nLabel permutation\n\nThey randomly permute the class labels and remove the effect after some epochs.\nThis is expected to only affect the last layer and hence should have similar effects to vertical flipping.\n\n\nSensory deprivation\n\nThey test the effect of sensory deprivation on neural nets.\nThey make the input of the networks uninformative by replacing it with gaussian noise.\nThis is assumed to have less effect than adding blur, because the network does not learn significantly wrong statistics (due to the input being uninformative).\n\n\nMutual Information Noise\n\nThey add random gaussian noise to each layer's output (or to the weights - not really clear).\nThey allow the network to learn the variance of that noise.\nThey add a regularization based on mutual information.\nThis adds a \"cost proportional to the quantity of mutual information I(w;D) that the weights retain about the training data D after the learning process\" (?).\nSo the network can retain more information, but has to pay for that.\nIt is expected to set the variance to low values for layers which are critical for the predictions.\n\n\n\n Blur\n\nThey train networks in CIFAR10 and blur all input images by down- and then upscaling.\nThey vary when the blurring is removed.\nThey always train for 300 epochs more after the blurring was removed.\nSo the network always sees the unaltered input images for at least the same amount of epochs.\n\n They train networks in CIFAR10 and blur all input images by down- and then upscaling. They vary when the blurring is removed. They always train for 300 epochs more after the blurring was removed.\nSo the network always sees the unaltered input images for at least the same amount of epochs. Vertical flipping\n\nSame as blurring, but they vertically flip the image.\nThis is expected to keep low and mid level statistics the same. Only the final layers have to change.\nThis is expected to be easy to adapt to, even if the flipping is removed fairly late into the training.\n\n Same as blurring, but they vertically flip the image. This is expected to keep low and mid level statistics the same. Only the final layers have to change. This is expected to be easy to adapt to, even if the flipping is removed fairly late into the training. Label permutation\n\nThey randomly permute the class labels and remove the effect after some epochs.\nThis is expected to only affect the last layer and hence should have similar effects to vertical flipping.\n\n They randomly permute the class labels and remove the effect after some epochs. This is expected to only affect the last layer and hence should have similar effects to vertical flipping. Sensory deprivation\n\nThey test the effect of sensory deprivation on neural nets.\nThey make the input of the networks uninformative by replacing it with gaussian noise.\nThis is assumed to have less effect than adding blur, because the network does not learn significantly wrong statistics (due to the input being uninformative).\n\n They test the effect of sensory deprivation on neural nets. They make the input of the networks uninformative by replacing it with gaussian noise. This is assumed to have less effect than adding blur, because the network does not learn significantly wrong statistics (due to the input being uninformative). Mutual Information Noise\n\nThey add random gaussian noise to each layer's output (or to the weights - not really clear).\nThey allow the network to learn the variance of that noise.\nThey add a regularization based on mutual information.\nThis adds a \"cost proportional to the quantity of mutual information I(w;D) that the weights retain about the training data D after the learning process\" (?).\nSo the network can retain more information, but has to pay for that.\nIt is expected to set the variance to low values for layers which are critical for the predictions.\n\n They add random gaussian noise to each layer's output (or to the weights - not really clear). They allow the network to learn the variance of that noise. They add a regularization based on mutual information.\nThis adds a \"cost proportional to the quantity of mutual information I(w;D) that the weights retain about the training data D after the learning process\" (?). So the network can retain more information, but has to pay for that. It is expected to set the variance to low values for layers which are critical for the predictions. \nResults\n\nBlur\n\nRemoving the blur early leads to only a small loss in final accuracy.\nRemoving the blur too late leads to a large and permanent loss in final accuracy.\nThe decline in accuracy is not linear with respect to when the blur is removed.\nThe effect is similar to biological neural nets.\n\nMaking the network deeper does not help, but instead worsens the effect.\nFixing the learning rate doesn't help either.\n\nThis is basically like starting to let the network learn once the blur is removed,\nbut using a weirdly bad initialization.\n(Weird in the sense that it starts with great accuracy, but is barely able to improve.)\n\n\nVertical flipping\n\nAs expected, adding vertical flips does not significantly affect long term accuracy.\n\n\nLabel permutation\n\nSame as for vertical flipping, only minor effect.\n\n\nSensory deprivation\n\nThis has worse effects than vertical flipping / label permutation.\nOverall less decrease in accuracy than with blur.\nThe effect is more linear with respect to the epoch (remove early: hardly any decline in accuracy, remove after half of training: medium decline, remove late: strong decline).\n\n\nMutual Information Noise\n\nWithout deficit, the network will put most weight (least amount of noise) on the middle layers (3-5 of 7).\nWith deficit, it will put more weight on the last layers and is only able to partially reconfigure if the deficit is removed early enough.\n\n\n\n\n Blur\n\nRemoving the blur early leads to only a small loss in final accuracy.\nRemoving the blur too late leads to a large and permanent loss in final accuracy.\nThe decline in accuracy is not linear with respect to when the blur is removed.\nThe effect is similar to biological neural nets.\n\nMaking the network deeper does not help, but instead worsens the effect.\nFixing the learning rate doesn't help either.\n\nThis is basically like starting to let the network learn once the blur is removed,\nbut using a weirdly bad initialization.\n(Weird in the sense that it starts with great accuracy, but is barely able to improve.)\n\n Removing the blur early leads to only a small loss in final accuracy. Removing the blur too late leads to a large and permanent loss in final accuracy. The decline in accuracy is not linear with respect to when the blur is removed. The effect is similar to biological neural nets.  Making the network deeper does not help, but instead worsens the effect. Fixing the learning rate doesn't help either.  This is basically like starting to let the network learn once the blur is removed,\nbut using a weirdly bad initialization.\n(Weird in the sense that it starts with great accuracy, but is barely able to improve.) Vertical flipping\n\nAs expected, adding vertical flips does not significantly affect long term accuracy.\n\n As expected, adding vertical flips does not significantly affect long term accuracy. Label permutation\n\nSame as for vertical flipping, only minor effect.\n\n Same as for vertical flipping, only minor effect. Sensory deprivation\n\nThis has worse effects than vertical flipping / label permutation.\nOverall less decrease in accuracy than with blur.\nThe effect is more linear with respect to the epoch (remove early: hardly any decline in accuracy, remove after half of training: medium decline, remove late: strong decline).\n\n This has worse effects than vertical flipping / label permutation. Overall less decrease in accuracy than with blur. The effect is more linear with respect to the epoch (remove early: hardly any decline in accuracy, remove after half of training: medium decline, remove late: strong decline). Mutual Information Noise\n\nWithout deficit, the network will put most weight (least amount of noise) on the middle layers (3-5 of 7).\nWith deficit, it will put more weight on the last layers and is only able to partially reconfigure if the deficit is removed early enough.\n\n\n Without deficit, the network will put most weight (least amount of noise) on the middle layers (3-5 of 7). With deficit, it will put more weight on the last layers and is only able to partially reconfigure if the deficit is removed early enough.  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.08856"
    },
    "12": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/High_Resolution_Image_Synthesis_with_Conditional_GANs.md",
        "transcript": "\nWhat\n\nThey suggest a method to generate (via GANs) high-resolution images that match given segmentation maps.\nThey introduce a new loss for GANs.\nThey suggest a method to easily change the appearances of object instances in the images.\n\n They suggest a method to generate (via GANs) high-resolution images that match given segmentation maps. They introduce a new loss for GANs. They suggest a method to easily change the appearances of object instances in the images. \nHow\n\nArchitecture\n\nThey train two generators (theoretically can be any number of generators).\nThe first (small-scale) generator gets a small-scale segmentation map as its input (e.g. a quarter of the intended output size).\nIt generates an image matching that segmentation map -- as in any other GAN.\nThe small scale generator is then trained for some time.\nThen they add a second generator (large-scale).\nAt each execution, first the small-scale generator is executed. It receives the small-scale segmentation map as input, transforms that into a feature map and uses that map to generate an output image.\nThen the large-scale generator is executed. It first uses convolutions to downscale the large-scale segmentation map. Then it adds (in residual fashion) the feature map from the small-scale generator. Then it uses these features to generate the large-scale output image.\nVisualization:\n\n\n\n\nAdditionally, they train three different discriminators.\nEach discriminator works completely independently and gets a different scale as input (e.g. D1: 1x downscaled, D2: 2x downscaled, D3: 4x downscaled).\nEach image produced by the generator is fed through all three discriminators.\nThis allows to do multi-scale discrimination (training the generator at the coarse and fine level).\n\n\nLoss\n\nThey add a feature matching loss, which incentivizes the generator to produce images that are projected by the discriminator onto features that are similar to real images.\nEquation:\n\n\n\n\nThey add a perceptual loss, which works similar to the feature matching loss, but the network to generate the features is VGG16 (instead of the discriminator).\nEquation:\n\n\n\n\nThey use LSGAN (least squares GAN) for everything else.\n\n\nInstance Segmentation\n\nThe generator gets a semantic segmentation map as its input.\nDownside: The map does not indicate where each object instance ends. This makes the image generation needlessly more complicated.\nTo fix that, they add instance information.\nJust adding the instance segmentation map is difficult, as there can be infinitely many instances, resulting in an input with infinitely many channels.\nInstead, they just use the segmentation map as one channel and add the boundaries of the object instances as a second channel.\nVisualization:\n\n\n\n\n\n\nInstance Manipulation\n\nTheir aim is to develop a tool in which one can select an object instance and easily change its appearance.\nThat appearance change could be created by changing the generator's input noise vector.\nDownside 1: This would also change everything else in the image.\nDownside 2: Potentially many noise vectors might result in practically the same appearance. So many vectors would have to be tried by the user.\nThey use an instance-wise feature embedding to fix that.\nStep 1:\n\nUse an autoencoder-like network E to transform each input image into a feature map.\nE has an encoder that encodes the image into a small (bottleneck) feature map.\nE has e decoder that decodes the bottleneck into a large feature map (not an image!) of the same size as the input image.\nE's output feature map could be added to the generator's input, i.e. no special autoencoder loss is needed.\n\n\nStep 2:\n\nE has transformed the input image to a feature map.\nApply average pooling to that feature map per object instance.\nThis effectively creates one feature vector per object instance (upscaled so that it cover's the object instance's area in the original image). The feature vector contains information about the object instance's appearance.\nThey set each feature vector to have three components (i.e. E's output has three channels).\nVisualization:\n\n\n\n\n\n\nStep 3:\n\nNow train the generator as before, but additionally to the semantic segmentation maps it gets the pooled feature maps as inputs.\nAs mentioned, E is trained in parallel.\n\n\nStep 4:\n\nGenerator and E are now trained.\nConvert each image in the dataset to a feature map via E. This results in a large number of instance-specific appearance vectors.\nRun K-Means clustering on these vectors with K=10 clusters per class.\nResult: Per object class 10 different appearance vectors. By changing the vector of on object instance to a different one (among these 10), the object's appearance in the image can be changed without significant effects on the remainder of the image.\n\n\n\n\n\n Architecture\n\nThey train two generators (theoretically can be any number of generators).\nThe first (small-scale) generator gets a small-scale segmentation map as its input (e.g. a quarter of the intended output size).\nIt generates an image matching that segmentation map -- as in any other GAN.\nThe small scale generator is then trained for some time.\nThen they add a second generator (large-scale).\nAt each execution, first the small-scale generator is executed. It receives the small-scale segmentation map as input, transforms that into a feature map and uses that map to generate an output image.\nThen the large-scale generator is executed. It first uses convolutions to downscale the large-scale segmentation map. Then it adds (in residual fashion) the feature map from the small-scale generator. Then it uses these features to generate the large-scale output image.\nVisualization:\n\n\n\n\nAdditionally, they train three different discriminators.\nEach discriminator works completely independently and gets a different scale as input (e.g. D1: 1x downscaled, D2: 2x downscaled, D3: 4x downscaled).\nEach image produced by the generator is fed through all three discriminators.\nThis allows to do multi-scale discrimination (training the generator at the coarse and fine level).\n\n They train two generators (theoretically can be any number of generators). The first (small-scale) generator gets a small-scale segmentation map as its input (e.g. a quarter of the intended output size). It generates an image matching that segmentation map -- as in any other GAN. The small scale generator is then trained for some time. Then they add a second generator (large-scale). At each execution, first the small-scale generator is executed. It receives the small-scale segmentation map as input, transforms that into a feature map and uses that map to generate an output image. Then the large-scale generator is executed. It first uses convolutions to downscale the large-scale segmentation map. Then it adds (in residual fashion) the feature map from the small-scale generator. Then it uses these features to generate the large-scale output image. Visualization:\n\n\n\n  Additionally, they train three different discriminators. Each discriminator works completely independently and gets a different scale as input (e.g. D1: 1x downscaled, D2: 2x downscaled, D3: 4x downscaled). Each image produced by the generator is fed through all three discriminators. This allows to do multi-scale discrimination (training the generator at the coarse and fine level). Loss\n\nThey add a feature matching loss, which incentivizes the generator to produce images that are projected by the discriminator onto features that are similar to real images.\nEquation:\n\n\n\n\nThey add a perceptual loss, which works similar to the feature matching loss, but the network to generate the features is VGG16 (instead of the discriminator).\nEquation:\n\n\n\n\nThey use LSGAN (least squares GAN) for everything else.\n\n They add a feature matching loss, which incentivizes the generator to produce images that are projected by the discriminator onto features that are similar to real images. Equation:\n\n\n\n  They add a perceptual loss, which works similar to the feature matching loss, but the network to generate the features is VGG16 (instead of the discriminator). Equation:\n\n\n\n  They use LSGAN (least squares GAN) for everything else. Instance Segmentation\n\nThe generator gets a semantic segmentation map as its input.\nDownside: The map does not indicate where each object instance ends. This makes the image generation needlessly more complicated.\nTo fix that, they add instance information.\nJust adding the instance segmentation map is difficult, as there can be infinitely many instances, resulting in an input with infinitely many channels.\nInstead, they just use the segmentation map as one channel and add the boundaries of the object instances as a second channel.\nVisualization:\n\n\n\n\n\n The generator gets a semantic segmentation map as its input. Downside: The map does not indicate where each object instance ends. This makes the image generation needlessly more complicated. To fix that, they add instance information. Just adding the instance segmentation map is difficult, as there can be infinitely many instances, resulting in an input with infinitely many channels. Instead, they just use the segmentation map as one channel and add the boundaries of the object instances as a second channel. Visualization:\n\n\n\n  Instance Manipulation\n\nTheir aim is to develop a tool in which one can select an object instance and easily change its appearance.\nThat appearance change could be created by changing the generator's input noise vector.\nDownside 1: This would also change everything else in the image.\nDownside 2: Potentially many noise vectors might result in practically the same appearance. So many vectors would have to be tried by the user.\nThey use an instance-wise feature embedding to fix that.\nStep 1:\n\nUse an autoencoder-like network E to transform each input image into a feature map.\nE has an encoder that encodes the image into a small (bottleneck) feature map.\nE has e decoder that decodes the bottleneck into a large feature map (not an image!) of the same size as the input image.\nE's output feature map could be added to the generator's input, i.e. no special autoencoder loss is needed.\n\n\nStep 2:\n\nE has transformed the input image to a feature map.\nApply average pooling to that feature map per object instance.\nThis effectively creates one feature vector per object instance (upscaled so that it cover's the object instance's area in the original image). The feature vector contains information about the object instance's appearance.\nThey set each feature vector to have three components (i.e. E's output has three channels).\nVisualization:\n\n\n\n\n\n\nStep 3:\n\nNow train the generator as before, but additionally to the semantic segmentation maps it gets the pooled feature maps as inputs.\nAs mentioned, E is trained in parallel.\n\n\nStep 4:\n\nGenerator and E are now trained.\nConvert each image in the dataset to a feature map via E. This results in a large number of instance-specific appearance vectors.\nRun K-Means clustering on these vectors with K=10 clusters per class.\nResult: Per object class 10 different appearance vectors. By changing the vector of on object instance to a different one (among these 10), the object's appearance in the image can be changed without significant effects on the remainder of the image.\n\n\n\n Their aim is to develop a tool in which one can select an object instance and easily change its appearance. That appearance change could be created by changing the generator's input noise vector. Downside 1: This would also change everything else in the image. Downside 2: Potentially many noise vectors might result in practically the same appearance. So many vectors would have to be tried by the user. They use an instance-wise feature embedding to fix that. Step 1:\n\nUse an autoencoder-like network E to transform each input image into a feature map.\nE has an encoder that encodes the image into a small (bottleneck) feature map.\nE has e decoder that decodes the bottleneck into a large feature map (not an image!) of the same size as the input image.\nE's output feature map could be added to the generator's input, i.e. no special autoencoder loss is needed.\n\n Use an autoencoder-like network E to transform each input image into a feature map. E has an encoder that encodes the image into a small (bottleneck) feature map. E has e decoder that decodes the bottleneck into a large feature map (not an image!) of the same size as the input image. E's output feature map could be added to the generator's input, i.e. no special autoencoder loss is needed. Step 2:\n\nE has transformed the input image to a feature map.\nApply average pooling to that feature map per object instance.\nThis effectively creates one feature vector per object instance (upscaled so that it cover's the object instance's area in the original image). The feature vector contains information about the object instance's appearance.\nThey set each feature vector to have three components (i.e. E's output has three channels).\nVisualization:\n\n\n\n\n\n E has transformed the input image to a feature map. Apply average pooling to that feature map per object instance. This effectively creates one feature vector per object instance (upscaled so that it cover's the object instance's area in the original image). The feature vector contains information about the object instance's appearance. They set each feature vector to have three components (i.e. E's output has three channels). Visualization:\n\n\n\n  Step 3:\n\nNow train the generator as before, but additionally to the semantic segmentation maps it gets the pooled feature maps as inputs.\nAs mentioned, E is trained in parallel.\n\n Now train the generator as before, but additionally to the semantic segmentation maps it gets the pooled feature maps as inputs. As mentioned, E is trained in parallel. Step 4:\n\nGenerator and E are now trained.\nConvert each image in the dataset to a feature map via E. This results in a large number of instance-specific appearance vectors.\nRun K-Means clustering on these vectors with K=10 clusters per class.\nResult: Per object class 10 different appearance vectors. By changing the vector of on object instance to a different one (among these 10), the object's appearance in the image can be changed without significant effects on the remainder of the image.\n\n Generator and E are now trained. Convert each image in the dataset to a feature map via E. This results in a large number of instance-specific appearance vectors. Run K-Means clustering on these vectors with K=10 clusters per class. Result: Per object class 10 different appearance vectors. By changing the vector of on object instance to a different one (among these 10), the object's appearance in the image can be changed without significant effects on the remainder of the image. \nResults\n\nSubjectively more realistic looking images than pix2pix and CRF at 2048x1024.\nTests with Mechanical Turk also show that their images look more realistic to most people.\nTests with Mechanical Turk indicate that both feature loss and perceptual loss (based on VGG16) improve image quality. (Though numbers for perceptual loss aren't that clear.)\nTests show that the generated images have high matching with the segmentation maps given to the generator.\nAdding instance boundaries as an input channel helps with image quality at object boundaries:\n\n\n\n\nEasy changing of object instance appearances, see video.\nThey also test on other datasets (NYU, ADE20K, Helen Faces) and get good results there too.\n\n Subjectively more realistic looking images than pix2pix and CRF at 2048x1024. Tests with Mechanical Turk also show that their images look more realistic to most people. Tests with Mechanical Turk indicate that both feature loss and perceptual loss (based on VGG16) improve image quality. (Though numbers for perceptual loss aren't that clear.) Tests show that the generated images have high matching with the segmentation maps given to the generator. Adding instance boundaries as an input channel helps with image quality at object boundaries:\n\n\n\n  Easy changing of object instance appearances, see video. They also test on other datasets (NYU, ADE20K, Helen Faces) and get good results there too. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.11585"
    },
    "13": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/mixed/Computer_Vision_for_Autonomous_Vehicles_Overview.md",
        "transcript": "\nWhat\n\nThey give an overview of the current state of the art for self-driving cars.\n\n They give an overview of the current state of the art for self-driving cars. \nHow\n\n(Nothing here, it's just an overview article.)\n\n (Nothing here, it's just an overview article.) \nResults\n\nThey focus their overview on autonomous vision, i.e. on components/models that use\ncomputer vision to analyze images and not on models that make the final driving/steering\ndecisions.\nHistory of Autonomous Driving\n\nITS = Intelligent Transportation Systems\nPROMETHEUS was an early (1986) european project to research self-driving vehicles.\n\nIn 1995 they drove from Munich to Odense (Denmark), being autonomous 95% of the time.\n\n\nNavlab (1988) was a similar US-american project. They completed an autonomous drive from Pittsburgh, PA to San Diego, CA in 1995.\nUS launched National Automated Highway System Consortium (NAHSC) in 1995.\nJapan launched Advanced Cruise Assist Highway System Research Association in 1996.\nPomerleau & Jochem (1996) drove from Washington DC to San Diego, being autonomous 98% of\nthe time (only steering, not accelerating/decelerating).\nTowards 2000, simple methods (e.g. lane following) became better and computational power was\nincreasingle available, but things like reflections, wet roads or direct sunlight still\ncaused problems.\nFranke et al. (2000) suggested a first system to drive in urban environments,\nincluding depth-based obstacle detection and traffic sign recognition.\nPrototype cars developed from VisLab approaches during 2000-2010 are ARGO, TerraMax, BRAiVE.\nBertozzi et al. (2011) drove semi-autonomous from Italy to China. Their system e.g. could\ndetect lane markings or preceding cars and could perform e.g. leader-following and stop-and-go.\nBroggi et al. (2015) drive in an urban environment in Parma, including e.g. roundabouts,\ntraffic lights and intersections.\nFurgale et al. (2013) drive autonomously with a car using close-to-market sensors and using\nvision-only navigation.\nGoogle started their self-driving car project in 2009 and drove 1.5 million miles\nautonomously (as of march 2016). They split the project off into Waymo in 2016.\nTesla rolled out a semi-autonomous system in 2015.\nZiegler et al. (2014) drove from Mannheim (Germany) to Pforzheim (Germany) nearly fully\nautonomous.\nBojarski et al. (2016) drove from Holmdel to Atlantic Highlands (NJ), being autonomous 98% of\nthe time.\nCompetitions\n\nELROB is a european competition, focusing on military applications and rough terrains.\nDARPA Grand Challange 2004 offered one million for the first team to drive fully\nautonomously a 150 mile route between California and Nevada. (First teams accomplished that\nin 2005.)\nDARPA Urban Challange 2007 similarly focused on urban driving, including other traffic and traffic\nregulations.\nGCDC 2011 and 2016 in the Netherlands focused on convoy driving.\n\n\n\n\nDatasets and Benchmarks\n\nKITTI and Cityscapes are the most well-known datasets for autonomous vehicles.\nCurrently, datasets with several thousand labeled examples are state of the art.\nReal-world datasets are often needed, but usually require tedious manual labeling.\nMechanical Turk eases that problem a bit, but label quality tends to be low.\nStereo/3D Reconstruction datasets\n\nMiddlebury stereo benchmark: Multi-frame stereo data for stereo matching algorithms, pixel-level labels (partly hand-annotated, partly algorithmically generated)\nMiddlebury v3: More data added to Middlebury with labels of higher quality.\nMiddlebury multi-view stereo (MVS) benchmark: Calibrated multi-view image datset for MVS approaches. Contains two scenes.\nTUD MVS dataset: Similar to Middlebury MVS, but with 124 scenes. Labels generated via structured light.\nSch\u00f6ps et al. (2017): High-resolution DSLR images synchromized with low-resolution stereo videos, annotated via high-resolution lasers.\n\n\nOptical Flow datasets\n\nMiddlebury flow benchmark: Non-rigid, synthetic motion sequences with labels automatically derived using hidden fluorescent textures painted onto the objects. Limited size, small movements, low lighting/shadow variation.\nJanai et al. (2017): 160 diverse real world scenes (1280x1024 resolution). Labels automatically generated using pixelwise tracking and high-speed cameras.\n\n\nObject Recognition and Segmentation datasets\n\nImageNet\nPASCAL VOC: Benchmark for object classification/detection/segmentation and action recognition. Real world images with high variance. Up to 20 classes, 11k images, 27k bounding boxes.\nMS COCO: For object detection, instance segmentation, contextual reasoning. 91 classes, 2.5M instances, 328k images.\nCityscapes\nTorontoCity\n\n\nTracking datasets\n\nMOTChallenge: 14 real world video sequences for pedestrian tracking. Cameras static and moving. Evaluation measures are Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP).\n\n\nAerial Image datasets\n\nISPRS: Aerial images. For urban object detection and 3d building reconstruction.\n\n\nAutonomous Driving datasets\n\nKITTI\n\nDataset for stereo vision, optical flow estimation, visual odometry/SLAM, 3D object detection\nCaptured via car. 6 hours of driving. High-resolution color camera, grayscale stereo cameras, 3D laser scanner, high-precision GPS/IMU inertial navigation system.\nFor stereo vision and optical flow: 194 train + 195 test images (1280x376). Only static scenes with moving camera.\nAdditional annotations for moving objects in 400 dynamic scenes by Menze & Geiger (2015).\nFor visual odometry / SLAM: 22 stereo scenes, length 39.2km.\nFor object detection: 7481 train + 7518 test images with annotated 3d bounding boxes. Vehicles, pedestrians, cyclists.\nFor road/lane detection: 600 diverse training and test images by Mattyus et al. (2016).\n\n\n\nHCI benchmark: 28k steoreo images with optical flow and stereo ground truth. Low diversity.\nCaltech Pedestrian Detection Benchmark: 250k frames recorded from vehicle. 350k bounding boxes of 2.3k unique pedestrians. With temporal correspondence and occlusion labels.\nCityscapes\n\nStereo image sequences recorded from a car (real world street scenes).\nAnnotations for pixel-level and instance-level semantic labeling.\n5k images with detailed annotations, 20k with coarse annotations.\n\n\nTorontoCity\n\nBuilding height estimation, road centerline and curb extraction, building instance segmentation, building contour extraction, semantic labeling, scene classificiation.\nImages from airplanes, drones, cars. Covers the Toronto area.\n\n\n\n\nLong-Term Autonomy datasets\n\nMaddern et al. (2016): Images, LiDAR, GPS data from 1k km driving in Oxford, spread over one year.\n\n\nSynthetic datasets\n\nMPI Sintel: Optical flow for 1.6k frames from the Sintel movie.\nDosovitskiy et al. (2015): 2D images of flying chairs in front of random background images. For optical flow estimation.\nMayer et al. (2016): 2D images of flying things in front of random background images. Also images of car models in front of houses and trees. For optical flow estimation.\nVirtual KITTI: 35 videos with 17k high resolution frames for object detection, tracking, semantic/instance segmentation, depth, optical flow. Varying environmental conditions (e.g. rain). Generated using a game engine.\nSYNTHIA: 13.4k images and 4 videos with 200k frames for semantic segmentation in urban scenarios. Generated using Unity Engine.\nRichter et al. (2016): 25k images from GTA5 with dense semantic annotations.\nQiu & Yuille (2016): A method to create virtual worlds using Unreal Engine 4 and how to connect them with Caffe.\n\n\n\n\nCamera Models and Calibration\n\nCalibration\n\nCalibration = estimate intrinsic & extrinsic parameters of sensors (e.g. range sensors, fish-eye cameras, ...)\nThis allows to relate 2D image points with 3D world points.\nCalibration is done with the help of fiducial markings on checkerboard patterns.\nDesired features of good calibration systems are: high accuracy, speed, robustness, minimal assumptions\n\n\nOmnidirectional Cameras\n\nOmnidirectional Cameras = cameras with 360 degrees field of view\nObviously good for self-driving cars, give full situational awareness.\nCatadioptric Cameras = combine standard camera and mirror (to generate 360 degrees field of view)\nDioptric Cameras = Use doptric fisheye lenses\nPolydioptric Cameras = Use multiple cameras with overlapping field of view\nOmnidirectional Cameras can be differentiated by the projection center: Central, noncentral\nFor central cameras, all light rays intersect in a single point in 3D. Benefit: Geometrically correct generated images => allows to use epipolar geometry on the images.\nDistortion in omnidirectional cameras is too high to perform calibration based on simple, linear projections (as opposed to pinhole cameras).\nOmnidirectional cameras are roughly comparable to LiDAR (radar systems), but are far cheaper, perceive color and do not suffer from rolling shutter effects.\nH\u00e4ne et al. (2014) predict dense depth maps from omnidirectional cameras in real time (GPU) based on the plaen-sweeping stereo matching algorithm.\n\n\nEvent Cameras\n\nEvent Cameras = Produce an event when a brightness change passes a threshold (sparse data); they work at microsecond level.\nInteresting for self-driving cars due to their very high speed (low reaction times).\nEvent Cameras also do not suffer from motion blur and have high dynamic range.\nData is difficult to handle due to algorithms being adapted towards full images.\nEvents can be accumulated over time to generate full images, but this runs contrary to the speed benefit.\n\n\n\n\nRepresentations\n\nPixel-based\n\nEach pixel is a random variable. E.g. in CNNs or PGMs.\nMost fine-grained.\nHigh complexity.\n\n\nSuperpixels\n\nDerived from segmentation of images into atomic regions with similar color/texture.\n\n\nStixels\n\nIntended for 3D traffic scenes.\nSomewhere between pixels and objects.\nEach stixel is a vertical stick in the landscape.\nEach stixel is defined by its position relative to the camera and its height.\nThe width is constant.\nThey are good for representing obstacles in front of the car.\n\n\n\n3D Primitives\n\nRepresent 3D geometry with primitives.\nSignificantly cheaper computationally.\n\n\n\n\nObject Detection\n\nUseful to detect other participants and obstacles.\nSensors\n\nCameras are cheapest.\nFor daytime, just ordinary cameras, i.e. visible spectrum (VS) cameras.\nFor nightime, use thermal infrared (TIR) cameras, which captures temperature differences.\nLasers can be used to detect range information, useful for 3d object detection and localization.\nWeather effects can influence sensors.\nSensor fusion combines information from different sensors.\n\n\nStandard Pipeline\n\nPreprocessing => RoI extraction (\"regions of interest\") => classification per RoI => Verification/Refinement\nPreprocessing: E.g. exposure/gain adjustment, camera calibration, ...\nRoI extraction: E.g. by sliding window\nClassification: Classify each RoI (e.g. cascade of weak classifiers in classic Viola Jones)\nPart-based Approaches: Develop one model per part of an object (simpler to learn per part), e.g. in Deformable Part Model (DPM)\n\n\n2D Object Detection\n\nPopular datasets for 2D object detection in autonomous cars: KITTI, Caltech-USA\nTask is dominated currently by CNNs, mainly R-CNN variations.\nKITTI contains a lot of small examples, but R-CNNs tend to be bad at detecting these.\nTherefore many current methods focus on how to improve R-CNNs for small bounding boxes.\n\n\n3D Object Detection from 2D Images\n\nNot much here, still behind 2D detectors.\n\n\n3D Object Detection from 3D Point Clouds\n\nLiDAR laser range sensor data is usually sparse and with limited spatial resolution.\nState of the art is behind in accuracy to imag based methods.\nCurrently best method on KITTI uses the LiDAR's point cloud from bird view and camera RGB images as a CNN input and then uses a RoI-Pooling based method for detection, similar to 2D bounding box detection.\n\n\nPerson Detection\n\nPeople are common obstacles for self-driving cars and are hard to detect due to high variation in poses and clothes.\nPedestrian Protection Systems (PPS) detect people around the car and warn the driver.\nThese systems have to be basically flawless.\nModern pedestrian detectors use CNNs.\nTemporal Cues\n\nAdding temporal information to object detectors improves results.\nFor pedestrians e.g. dynamic gait, motion parallax.\n\n\nScarcity of Target Class\n\nPositive examples of pedestrians are much rarer in example images than negative ones.\nEnzweiler & Gavrila (2008) suggest to use a generative model to generate synthetic examples and train a classifier on these.\n\n\nReal-time Pedestrian Detection\n\nStixel world representations can be used to reduce the search space, resulting in high performance.\n\n\n\n\nHuman Pose Estimation\n\nPose estimation is useful to e.g. estimate the intentions of a person.\nPreviously done with two-staged approach: First detect body parts, then estimate pose.\nNow done with CNNs.\n\n\nDiscussion\n\nDetection of large and obvious objects works well.\nSmaller and occluded objects cause problems.\nDifferentiating pedestrians and cyclists causes problems.\nCrowds of people can cause problems.\n\n\n\n\nSemantic Segmentation\n\nSemantic Segmentation = Assign each pixel in an image a class label\n\n\n\n\nPreviously done via MAP in CRFs (input either pixels or superpixels).\nBut not efficient and few simple features could be used.\nHigher-order features with long-range interactions were developed.\nMore efficient inference algorithms were developed.\nFully convolutional networks succeeded CRFs for semantic segmentation.\nThe networks face the challange to both reason on a coarse level as well as assigning classes on a fine level (pixel-wise).\n\nSolved e.g. with dilated convolutions.\nSegNet memorizes the indices of max-pooling cells to reverse the process during upsampling (saves parameters for upsampling steps).\nUsing skip connections between resolutions is also an option.\nAnother option is to upsample lower scales and then fuse scales of various resolutions in additional convolutions.\nResidual architectures can also help.\n\n\nCNNs have also been combined with CRFs. Usually, the CNN is executed first and the CRF then refines the predictions at the pixel-level.\nCurrently, models perform semantic segmentation well at the coarse level, but have more problems at the fine level.\nSemantic Instance Segmentation\n\nSemantic Instance Segmentation = Detect individual objects, classify them and segment them\nTwo approaches: Proposal-based Instance Segmentation and Proposal-free Instance Segmentation\nProposal-based Instance Segmentation first predicts region proposals (e.g. bounding boxes) and then segments each object within the region proposal. Works similarly to R-CNNs.\nProposal-free Instance Segmentation predicts instance-level segmentation and class-level segmentation in one step.\nModels are currently much weaker at instance level segmentation than semantic segmentation.\n\n\nLabel Propagation\n\nAnnotations for Semantic/Instance Segmentation are very labour intensive to produce.\nOne possible solution is to leverage information from previous/future frames to simplify the annotation process.\nI.e. annotate few keyframes, then propagate the annotations to the frames in between, based on e.g. pixelwise color information.\n\n\nSemantic Segmentation with Multiple Frames\n\nWhen multiple frames are available, temporal information can be used to improve segmentation accuracy.\n\n\nSemantic Segmentation of 3D Data\n\nSegmentation can also be performed in 3D space, instead of 2D.\nThis is less common, but probably more useful for self-driving cars.\nE.g. done by 3D CNNs in voxel space. This can be expensive with regards to computation and memory. Riegler et al. (2017) suggest to operate in an Octree space, thereby exploiting the sparseness of the output.\n\n\nSemantic Segmentation of Street Side Views\n\nOne subtask of semantic segmentation is to segment street side views, e.g. views of buildings.\nThis is e.g. useful for navigation/localization.\n\n\nSemantic Segmentation of Aerial Images\n\nUsed to find urban objects in aerial/satellite images.\nUseful e.g. for navigation - even in remote areas and in case of very recent changes.\nDifficult, because the objects have heterogeneous appearances that also can't be expressed with simple priors.\nLots of CRF/MRF solutions available.\nAerial images are usually of low resolution (e.g. 1 meter = 1 pixel), whereas ground-based images are of high resolution.\nSome solutions therefore combine aerial and ground based images, e.g. using a joint MRF.\nThe ISPRS segmentation challange contains data for (urban) semantic segmenation of airborne sensors.\nThe best solutions on ISPRS use CNN+CRF or just CNNs.\n\n\nRoad Segmentation\n\nCorrect road segmentation obviously important for self-driving cars.\nAgain, CNN solutions available.\nSome solutions to decrease labeling effort. E.g. one maps OpenStreetMap annotations onto the image using GPS coordinates and car pose.\nFree Space Estimation is a subfield of Road Segmentation\n\nDeals with detecting cars and other objects that stick out of the ground or somehow block the self-driving car.\nSolutions often employ stereo cameras for depth estimation.\nLong Range Obstacle Detection can also use radars, as their error does not increase quadratically in depth (as opposed to stereo cameras).\n\n\n\n\n\n\nReconstruction\n\nStereo\n\nStereo estimation = extract 3D information (i.e. depth) from 2D stereo camera images\nStereo cameras consist of two cameras mounted next to each other.\nStereo estimation algorithms compute correspondences between images taken by the two cameras at the same points in time.\nStereo estimation is useful for 3D reconstruction or e.g. free space estimation\nTaxonomies\n\nFeature-based vs. area-based methods: Feature-based methods provide sparse edge-based depth maps, area-based methods generate dense outputs.\nLocal vs global: Local methods compute image disparities based on best matches (winner takes all), global methods use energy-minimization.\n\n\nMatching Cost Function\n\nUsed for matching points between left and right camera images.\nUsually assume that both cameras are rectified, reducing search space to horizontal line.\nCost function computes cost for each pixel and possible disparity. Should take lowest values at true disparity.\nAlgorithms make assumption of constant appearance between matching points. Strong lighting changes can cause problems.\n\n\nSemi-Global Matching (SGM) is a high-speed high-accuracy method for stereo estimation. It is optionally performed on top of CNN features. It is theoretically similar to belief propagation.\nUsing sensible priors/regularization helps with ambiguities in depth estimation.\nTotal Variation is one used prior.\nCNNs are nowadays common and fast for stereo estimation.\nE.g. one can embed both left and right images into a feature space and then compute the correlation between the two (when shifting an image an the x-axis).\n\n\n\n\n\n\nMulti-View 3D Reconstruction\n\nDeals with reconstructing 3D geometry from many images.\nAlso makes use of sensible priors.\nMethods usually differ by: form of photo-consistency function, scene representation, visibility computation, priors, initialization requirements.\nScene representation can be: Depth map, point cloud, (triangular) mesh/surface, volumetric/voxels\nCommon algorithm for depth maps: Plane Sweeping Stereo algorithm\nCommon algorithm for point clouds: Patch-based Multi-View Stereo (PMVS)\nA voxel grid can be converted to surfaces using the Marching Cubes algorithm.\nUrban reconstruction algorithms: Methods to automatically reconstruct the 3D geometry of cities (here from images gathered by self-driving cars).\nMost common data for multi-view reconstruction are ground based images (i.e. from cars), aerial/satellite images, LiDAR scans. Aerial is readily available but coarse.\n\n\nReconstruction and Recognition\n\nPerforming 2D semantic segmentation and 3D reconstruction in one step/model should be beneficial for the accuracy of both tasks, e.g. all pixels belonging to cars can be expected to have certain possible geometric shapes.\n\n\n\n\nCan be done on small scale (single buildings) or large scale (whole cities).\nLarge scale is difficult due to memory constraints. (Notice also that labels may increase, potentially increasing memory demands even further.) Octrees can help.\nPriors for 3D shapes can be derived e.g. using PCA or Gaussian Process Latent Vriable Models (GP-LVM).\n\n\n\n\nMotion & Pose Estimation\n\n2D Motion Estimation - Optical Flow\n\nOptical Flow = two dimensional motion of brightness patterns between two images\nUseful e.g. for tracking of objects, ego-motion estimation, structure-from-motion\nOptical flow predicts motion vectors. Each vector has two components. Two constraints are used: (a) the brightness of each pixel over time is assumed to be constant, (b) movements are assumed to be smooth between neighbours.\nOcclusions, large displacement and fine details are still challenging today.\nPixel intensity is assumed to be constant over time, but that is not always the case due to e.g. reflections or transparency.\nOriginal optical flow method was based on variational formulation. It assumed pixel intensity to be constant and added a smoothness constraint.\nLater algorithms used more robust methods than just assuming constant pixel intensity.\nA common approach is to first estimate the optical flow at a coarse level and then refine towards finer levels.\nSome methods use nearest neighbour search of 2D patches, optionally in a high-dimensional feature space.\nMany accurate optical flow algorithms are accurate but far too slow for self-driving cars. Some newer methods based on CNNs though perform well.\nTraining of optical flow algorithms often happens based on Sintel and KITTI datasets.\nComparison of methods on KITTI happens based on absolute endpoint error (EBE). An error is any predicted motion vector which's endpoint is more than 3 pixels or 5% of the image dimension away from its true endpoint. The errors are measured in percent of all vectors (i.e. lower is better) and are split into background, foreground and other regions. The final EBE is the average of these error categories.\nFor self-driving cars, it can be sensible to predict optical flow as following epipolar lines, radiating from the center of the image.\nAnother optimization is to first perform instance segmentation and then predict optical flow per instance.\n\n\n3D Motion Estimation - Scene Flow\n\nScene flow = optical flow for 3D scenes\nUsually by predicting motion vectors on surfaces of objects.\nDense prediction preferred, but sparse prediction is faster.\n\n\nEgo-Motion Estimation\n\nEgo-Motion Estimation = Estimate position and orientation of the car\nWheel odometry was previously used for that: Estimate position and orientation of the car by measuring how much the wheels spinned. This causes problems e.g. on when the wheels are slipping on wet surfaces.\nVisual/LiDAR-based odometry: Estimate position and orientation of the car by using images/LiDAR. More robust and can recover from errors by recognizing visited places (loop closure).\nVisual odometry estimates the changes between two images and then accumulates these over time.\nVisual odometry can be split into feature-based and direct formulations.\nFeature-based formulations first convert images into feature spaces, direct formulations work with the raw input data.\nFeature-based formulations often use keypoint extraction, which can be problematic with street images due to their lack of corners. This makes direct formulations usually more accurate.\nDrift\n\nPredicting changes between pairs of images (incremental approach) tends to accumulate errors over time (drift).\nThis is fixed by an iterative refinement (often computationally expensive). E.g. done by minimizing errors of 3D points or by using information from SLAM (simultaneous localization and mapping) used to recognize known locations (loop closure).\n\n\nKITTI is the only large dataset available for visual odometry.\nMonocular methods for visual odometry perform worse than methods that use 3D data.\nLiDAR-based odometry outperforms monocular visual odometry for ego-motion estimation. Stereo cameras though perform almost as good as LiDARs.\nCurrent solutions for ego-motion estimation have mostly problems crowded highways, narrow streets and when strong turns are taken.\n\n\nSimultaneous Localization and Mapping (SLAM)\n\nSLAM = Navigate through an environment and create a map of it at the same time\nSLAM must be able to deal with changing environments.\nIn case of autonomous vehicles it also has to deal with large scale environments and real-time application.\nEarly solutions used extended kalman filters or particle filters.\n\n\nLocalization\n\nLocalization = Determine position in the world\nCan be indoor or outdoors.\nMeasurements might be noisy.\nIs a subroutine of SLAM (for loop closure detection and drift correction).\nUsed sensors are GPS and camera (visual inputs).\nGPS is only accurate to about 5m (sometimes more accurate, but not reliable).\nVisual detection often works based on recognizing landmarks (\"place recognition\") and requires some kind of similarity measure.\nMonte Carlo methods are sometimes used for visual localization.\nVisual localization can be split into: Topological methods, metric methods.\nTopological localization is coarse but reliable. (Predicts position from a finite set possible positions.)\nMetric localization is fine but unreliable (especially for longer journeys).\nTopometric localization: Mixture of topological and metric localization.\nCurrent methods are on the order of 1m accuracy on short known routes or 4m accuracy on larger areas.\nStructure-based Localization: Predict (camera) location as well as all camera parameters (orientation, intrinsics) simultaneously.\nCross-view Localization: Match ground based images (rare but detailed) with aerial/satellite images (easy to get but coarse). This allows to perform geo-localization.\nOne way to do cross-view localization is to do (1) apply a CNN to ground based images to convert them to features, (2) apply a CNN to aerial images showing the same locations as the ground based images to convert them to features, (3) train networks from step 1 and 2 to produce as similar features as possible. Then do feature matching.\n\n\n\n\nTracking\n\nTracking = Estimate state of one/multiple objects over time given measurements from sensor\nState = location, velocity, acceleration (usually)\nUseful e.g. for automatic distance control or to anticipate driving maneuvers of other traffic participants\nPrediction of the behaviour of pedestrians and bicyclists is difficult, as they can abruptly change it.\nDifficulties: Variation of appearance within the same class (e.g. different poses of pedestrians), occlusions (possibly by objects of the same class), strong lighting, reflections in mirrors/windows.\nTracking is often formulated as a bayesian inference problem. Goal: Estimate posterior probability density function of current observation, given previous state. Updated recursively.\nE.g. implemented via extended kalman filters and particle filters.\nRecursive formulations have problems with recovering from errors.\nNon-recursive formulations have gained popularity because of that. These optimize energy functions with respect to all trajectories in a temporal window. The search space for this can be large and should be somehow minimized.\nDatasets for multi object tracking are PETS (static camera), TUD (static), MOT (dynamic), KITTI (dynamic).\n\n\nScene Understanding\n\nScene understandig = A full understanding of the surrounding area of a self-driving car\nInvolves many subtasks, such as depth estimation, object detection, ...\nSpecifically challenging are urban and sub-urban scenes: Many independently moving objects, high variance in appaerance of objects along the road, more variance in road layouts, more difficult lighting.\nOne solution uses street view images and combines them with information from OpenStreetMap. This way, the authors can automatically derive for the street view images e.g. the number of lanes, the road curvature or the distance up to the next intersection and train the model predict these information.\n\n\nEnd-to-End Learning of Sensorimotor Control\n\nTraditional systems for self-driving cars contain many components, e.g. for object detection (traffic signs, pedestrians, ...) or tracking.\nThese components' predictions are then combines using some kind of rule based system in order to create driving decisions.\nModern system though lean more towards end-to-end learning, where the model e.g. gets the front facing camera's image and ground truth steering decisions and learns this way to steer.\nOne method also uses inverse reinforcement learning to extract the unknown reward function (which rewards correct steering decisions).\n\n\n\n They focus their overview on autonomous vision, i.e. on components/models that use\ncomputer vision to analyze images and not on models that make the final driving/steering\ndecisions. History of Autonomous Driving\n\nITS = Intelligent Transportation Systems\nPROMETHEUS was an early (1986) european project to research self-driving vehicles.\n\nIn 1995 they drove from Munich to Odense (Denmark), being autonomous 95% of the time.\n\n\nNavlab (1988) was a similar US-american project. They completed an autonomous drive from Pittsburgh, PA to San Diego, CA in 1995.\nUS launched National Automated Highway System Consortium (NAHSC) in 1995.\nJapan launched Advanced Cruise Assist Highway System Research Association in 1996.\nPomerleau & Jochem (1996) drove from Washington DC to San Diego, being autonomous 98% of\nthe time (only steering, not accelerating/decelerating).\nTowards 2000, simple methods (e.g. lane following) became better and computational power was\nincreasingle available, but things like reflections, wet roads or direct sunlight still\ncaused problems.\nFranke et al. (2000) suggested a first system to drive in urban environments,\nincluding depth-based obstacle detection and traffic sign recognition.\nPrototype cars developed from VisLab approaches during 2000-2010 are ARGO, TerraMax, BRAiVE.\nBertozzi et al. (2011) drove semi-autonomous from Italy to China. Their system e.g. could\ndetect lane markings or preceding cars and could perform e.g. leader-following and stop-and-go.\nBroggi et al. (2015) drive in an urban environment in Parma, including e.g. roundabouts,\ntraffic lights and intersections.\nFurgale et al. (2013) drive autonomously with a car using close-to-market sensors and using\nvision-only navigation.\nGoogle started their self-driving car project in 2009 and drove 1.5 million miles\nautonomously (as of march 2016). They split the project off into Waymo in 2016.\nTesla rolled out a semi-autonomous system in 2015.\nZiegler et al. (2014) drove from Mannheim (Germany) to Pforzheim (Germany) nearly fully\nautonomous.\nBojarski et al. (2016) drove from Holmdel to Atlantic Highlands (NJ), being autonomous 98% of\nthe time.\nCompetitions\n\nELROB is a european competition, focusing on military applications and rough terrains.\nDARPA Grand Challange 2004 offered one million for the first team to drive fully\nautonomously a 150 mile route between California and Nevada. (First teams accomplished that\nin 2005.)\nDARPA Urban Challange 2007 similarly focused on urban driving, including other traffic and traffic\nregulations.\nGCDC 2011 and 2016 in the Netherlands focused on convoy driving.\n\n\n\n ITS = Intelligent Transportation Systems PROMETHEUS was an early (1986) european project to research self-driving vehicles.\n\nIn 1995 they drove from Munich to Odense (Denmark), being autonomous 95% of the time.\n\n In 1995 they drove from Munich to Odense (Denmark), being autonomous 95% of the time. Navlab (1988) was a similar US-american project. They completed an autonomous drive from Pittsburgh, PA to San Diego, CA in 1995. US launched National Automated Highway System Consortium (NAHSC) in 1995. Japan launched Advanced Cruise Assist Highway System Research Association in 1996. Pomerleau & Jochem (1996) drove from Washington DC to San Diego, being autonomous 98% of\nthe time (only steering, not accelerating/decelerating). Towards 2000, simple methods (e.g. lane following) became better and computational power was\nincreasingle available, but things like reflections, wet roads or direct sunlight still\ncaused problems. Franke et al. (2000) suggested a first system to drive in urban environments,\nincluding depth-based obstacle detection and traffic sign recognition. Prototype cars developed from VisLab approaches during 2000-2010 are ARGO, TerraMax, BRAiVE. Bertozzi et al. (2011) drove semi-autonomous from Italy to China. Their system e.g. could\ndetect lane markings or preceding cars and could perform e.g. leader-following and stop-and-go. Broggi et al. (2015) drive in an urban environment in Parma, including e.g. roundabouts,\ntraffic lights and intersections. Furgale et al. (2013) drive autonomously with a car using close-to-market sensors and using\nvision-only navigation. Google started their self-driving car project in 2009 and drove 1.5 million miles\nautonomously (as of march 2016). They split the project off into Waymo in 2016. Tesla rolled out a semi-autonomous system in 2015. Ziegler et al. (2014) drove from Mannheim (Germany) to Pforzheim (Germany) nearly fully\nautonomous. Bojarski et al. (2016) drove from Holmdel to Atlantic Highlands (NJ), being autonomous 98% of\nthe time. Competitions\n\nELROB is a european competition, focusing on military applications and rough terrains.\nDARPA Grand Challange 2004 offered one million for the first team to drive fully\nautonomously a 150 mile route between California and Nevada. (First teams accomplished that\nin 2005.)\nDARPA Urban Challange 2007 similarly focused on urban driving, including other traffic and traffic\nregulations.\nGCDC 2011 and 2016 in the Netherlands focused on convoy driving.\n\n ELROB is a european competition, focusing on military applications and rough terrains. DARPA Grand Challange 2004 offered one million for the first team to drive fully\nautonomously a 150 mile route between California and Nevada. (First teams accomplished that\nin 2005.) DARPA Urban Challange 2007 similarly focused on urban driving, including other traffic and traffic\nregulations. GCDC 2011 and 2016 in the Netherlands focused on convoy driving. Datasets and Benchmarks\n\nKITTI and Cityscapes are the most well-known datasets for autonomous vehicles.\nCurrently, datasets with several thousand labeled examples are state of the art.\nReal-world datasets are often needed, but usually require tedious manual labeling.\nMechanical Turk eases that problem a bit, but label quality tends to be low.\nStereo/3D Reconstruction datasets\n\nMiddlebury stereo benchmark: Multi-frame stereo data for stereo matching algorithms, pixel-level labels (partly hand-annotated, partly algorithmically generated)\nMiddlebury v3: More data added to Middlebury with labels of higher quality.\nMiddlebury multi-view stereo (MVS) benchmark: Calibrated multi-view image datset for MVS approaches. Contains two scenes.\nTUD MVS dataset: Similar to Middlebury MVS, but with 124 scenes. Labels generated via structured light.\nSch\u00f6ps et al. (2017): High-resolution DSLR images synchromized with low-resolution stereo videos, annotated via high-resolution lasers.\n\n\nOptical Flow datasets\n\nMiddlebury flow benchmark: Non-rigid, synthetic motion sequences with labels automatically derived using hidden fluorescent textures painted onto the objects. Limited size, small movements, low lighting/shadow variation.\nJanai et al. (2017): 160 diverse real world scenes (1280x1024 resolution). Labels automatically generated using pixelwise tracking and high-speed cameras.\n\n\nObject Recognition and Segmentation datasets\n\nImageNet\nPASCAL VOC: Benchmark for object classification/detection/segmentation and action recognition. Real world images with high variance. Up to 20 classes, 11k images, 27k bounding boxes.\nMS COCO: For object detection, instance segmentation, contextual reasoning. 91 classes, 2.5M instances, 328k images.\nCityscapes\nTorontoCity\n\n\nTracking datasets\n\nMOTChallenge: 14 real world video sequences for pedestrian tracking. Cameras static and moving. Evaluation measures are Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP).\n\n\nAerial Image datasets\n\nISPRS: Aerial images. For urban object detection and 3d building reconstruction.\n\n\nAutonomous Driving datasets\n\nKITTI\n\nDataset for stereo vision, optical flow estimation, visual odometry/SLAM, 3D object detection\nCaptured via car. 6 hours of driving. High-resolution color camera, grayscale stereo cameras, 3D laser scanner, high-precision GPS/IMU inertial navigation system.\nFor stereo vision and optical flow: 194 train + 195 test images (1280x376). Only static scenes with moving camera.\nAdditional annotations for moving objects in 400 dynamic scenes by Menze & Geiger (2015).\nFor visual odometry / SLAM: 22 stereo scenes, length 39.2km.\nFor object detection: 7481 train + 7518 test images with annotated 3d bounding boxes. Vehicles, pedestrians, cyclists.\nFor road/lane detection: 600 diverse training and test images by Mattyus et al. (2016).\n\n\n\nHCI benchmark: 28k steoreo images with optical flow and stereo ground truth. Low diversity.\nCaltech Pedestrian Detection Benchmark: 250k frames recorded from vehicle. 350k bounding boxes of 2.3k unique pedestrians. With temporal correspondence and occlusion labels.\nCityscapes\n\nStereo image sequences recorded from a car (real world street scenes).\nAnnotations for pixel-level and instance-level semantic labeling.\n5k images with detailed annotations, 20k with coarse annotations.\n\n\nTorontoCity\n\nBuilding height estimation, road centerline and curb extraction, building instance segmentation, building contour extraction, semantic labeling, scene classificiation.\nImages from airplanes, drones, cars. Covers the Toronto area.\n\n\n\n\nLong-Term Autonomy datasets\n\nMaddern et al. (2016): Images, LiDAR, GPS data from 1k km driving in Oxford, spread over one year.\n\n\nSynthetic datasets\n\nMPI Sintel: Optical flow for 1.6k frames from the Sintel movie.\nDosovitskiy et al. (2015): 2D images of flying chairs in front of random background images. For optical flow estimation.\nMayer et al. (2016): 2D images of flying things in front of random background images. Also images of car models in front of houses and trees. For optical flow estimation.\nVirtual KITTI: 35 videos with 17k high resolution frames for object detection, tracking, semantic/instance segmentation, depth, optical flow. Varying environmental conditions (e.g. rain). Generated using a game engine.\nSYNTHIA: 13.4k images and 4 videos with 200k frames for semantic segmentation in urban scenarios. Generated using Unity Engine.\nRichter et al. (2016): 25k images from GTA5 with dense semantic annotations.\nQiu & Yuille (2016): A method to create virtual worlds using Unreal Engine 4 and how to connect them with Caffe.\n\n\n\n KITTI and Cityscapes are the most well-known datasets for autonomous vehicles. Currently, datasets with several thousand labeled examples are state of the art. Real-world datasets are often needed, but usually require tedious manual labeling.\nMechanical Turk eases that problem a bit, but label quality tends to be low. Stereo/3D Reconstruction datasets\n\nMiddlebury stereo benchmark: Multi-frame stereo data for stereo matching algorithms, pixel-level labels (partly hand-annotated, partly algorithmically generated)\nMiddlebury v3: More data added to Middlebury with labels of higher quality.\nMiddlebury multi-view stereo (MVS) benchmark: Calibrated multi-view image datset for MVS approaches. Contains two scenes.\nTUD MVS dataset: Similar to Middlebury MVS, but with 124 scenes. Labels generated via structured light.\nSch\u00f6ps et al. (2017): High-resolution DSLR images synchromized with low-resolution stereo videos, annotated via high-resolution lasers.\n\n Middlebury stereo benchmark: Multi-frame stereo data for stereo matching algorithms, pixel-level labels (partly hand-annotated, partly algorithmically generated) Middlebury v3: More data added to Middlebury with labels of higher quality. Middlebury multi-view stereo (MVS) benchmark: Calibrated multi-view image datset for MVS approaches. Contains two scenes. TUD MVS dataset: Similar to Middlebury MVS, but with 124 scenes. Labels generated via structured light. Sch\u00f6ps et al. (2017): High-resolution DSLR images synchromized with low-resolution stereo videos, annotated via high-resolution lasers. Optical Flow datasets\n\nMiddlebury flow benchmark: Non-rigid, synthetic motion sequences with labels automatically derived using hidden fluorescent textures painted onto the objects. Limited size, small movements, low lighting/shadow variation.\nJanai et al. (2017): 160 diverse real world scenes (1280x1024 resolution). Labels automatically generated using pixelwise tracking and high-speed cameras.\n\n Middlebury flow benchmark: Non-rigid, synthetic motion sequences with labels automatically derived using hidden fluorescent textures painted onto the objects. Limited size, small movements, low lighting/shadow variation. Janai et al. (2017): 160 diverse real world scenes (1280x1024 resolution). Labels automatically generated using pixelwise tracking and high-speed cameras. Object Recognition and Segmentation datasets\n\nImageNet\nPASCAL VOC: Benchmark for object classification/detection/segmentation and action recognition. Real world images with high variance. Up to 20 classes, 11k images, 27k bounding boxes.\nMS COCO: For object detection, instance segmentation, contextual reasoning. 91 classes, 2.5M instances, 328k images.\nCityscapes\nTorontoCity\n\n ImageNet PASCAL VOC: Benchmark for object classification/detection/segmentation and action recognition. Real world images with high variance. Up to 20 classes, 11k images, 27k bounding boxes. MS COCO: For object detection, instance segmentation, contextual reasoning. 91 classes, 2.5M instances, 328k images. Cityscapes TorontoCity Tracking datasets\n\nMOTChallenge: 14 real world video sequences for pedestrian tracking. Cameras static and moving. Evaluation measures are Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP).\n\n MOTChallenge: 14 real world video sequences for pedestrian tracking. Cameras static and moving. Evaluation measures are Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP). Aerial Image datasets\n\nISPRS: Aerial images. For urban object detection and 3d building reconstruction.\n\n ISPRS: Aerial images. For urban object detection and 3d building reconstruction. Autonomous Driving datasets\n\nKITTI\n\nDataset for stereo vision, optical flow estimation, visual odometry/SLAM, 3D object detection\nCaptured via car. 6 hours of driving. High-resolution color camera, grayscale stereo cameras, 3D laser scanner, high-precision GPS/IMU inertial navigation system.\nFor stereo vision and optical flow: 194 train + 195 test images (1280x376). Only static scenes with moving camera.\nAdditional annotations for moving objects in 400 dynamic scenes by Menze & Geiger (2015).\nFor visual odometry / SLAM: 22 stereo scenes, length 39.2km.\nFor object detection: 7481 train + 7518 test images with annotated 3d bounding boxes. Vehicles, pedestrians, cyclists.\nFor road/lane detection: 600 diverse training and test images by Mattyus et al. (2016).\n\n\n\nHCI benchmark: 28k steoreo images with optical flow and stereo ground truth. Low diversity.\nCaltech Pedestrian Detection Benchmark: 250k frames recorded from vehicle. 350k bounding boxes of 2.3k unique pedestrians. With temporal correspondence and occlusion labels.\nCityscapes\n\nStereo image sequences recorded from a car (real world street scenes).\nAnnotations for pixel-level and instance-level semantic labeling.\n5k images with detailed annotations, 20k with coarse annotations.\n\n\nTorontoCity\n\nBuilding height estimation, road centerline and curb extraction, building instance segmentation, building contour extraction, semantic labeling, scene classificiation.\nImages from airplanes, drones, cars. Covers the Toronto area.\n\n\n\n KITTI\n\nDataset for stereo vision, optical flow estimation, visual odometry/SLAM, 3D object detection\nCaptured via car. 6 hours of driving. High-resolution color camera, grayscale stereo cameras, 3D laser scanner, high-precision GPS/IMU inertial navigation system.\nFor stereo vision and optical flow: 194 train + 195 test images (1280x376). Only static scenes with moving camera.\nAdditional annotations for moving objects in 400 dynamic scenes by Menze & Geiger (2015).\nFor visual odometry / SLAM: 22 stereo scenes, length 39.2km.\nFor object detection: 7481 train + 7518 test images with annotated 3d bounding boxes. Vehicles, pedestrians, cyclists.\nFor road/lane detection: 600 diverse training and test images by Mattyus et al. (2016).\n\n\n Dataset for stereo vision, optical flow estimation, visual odometry/SLAM, 3D object detection Captured via car. 6 hours of driving. High-resolution color camera, grayscale stereo cameras, 3D laser scanner, high-precision GPS/IMU inertial navigation system. For stereo vision and optical flow: 194 train + 195 test images (1280x376). Only static scenes with moving camera. Additional annotations for moving objects in 400 dynamic scenes by Menze & Geiger (2015). For visual odometry / SLAM: 22 stereo scenes, length 39.2km. For object detection: 7481 train + 7518 test images with annotated 3d bounding boxes. Vehicles, pedestrians, cyclists. For road/lane detection: 600 diverse training and test images by Mattyus et al. (2016).  HCI benchmark: 28k steoreo images with optical flow and stereo ground truth. Low diversity. Caltech Pedestrian Detection Benchmark: 250k frames recorded from vehicle. 350k bounding boxes of 2.3k unique pedestrians. With temporal correspondence and occlusion labels. Cityscapes\n\nStereo image sequences recorded from a car (real world street scenes).\nAnnotations for pixel-level and instance-level semantic labeling.\n5k images with detailed annotations, 20k with coarse annotations.\n\n Stereo image sequences recorded from a car (real world street scenes). Annotations for pixel-level and instance-level semantic labeling. 5k images with detailed annotations, 20k with coarse annotations. TorontoCity\n\nBuilding height estimation, road centerline and curb extraction, building instance segmentation, building contour extraction, semantic labeling, scene classificiation.\nImages from airplanes, drones, cars. Covers the Toronto area.\n\n Building height estimation, road centerline and curb extraction, building instance segmentation, building contour extraction, semantic labeling, scene classificiation. Images from airplanes, drones, cars. Covers the Toronto area. Long-Term Autonomy datasets\n\nMaddern et al. (2016): Images, LiDAR, GPS data from 1k km driving in Oxford, spread over one year.\n\n Maddern et al. (2016): Images, LiDAR, GPS data from 1k km driving in Oxford, spread over one year. Synthetic datasets\n\nMPI Sintel: Optical flow for 1.6k frames from the Sintel movie.\nDosovitskiy et al. (2015): 2D images of flying chairs in front of random background images. For optical flow estimation.\nMayer et al. (2016): 2D images of flying things in front of random background images. Also images of car models in front of houses and trees. For optical flow estimation.\nVirtual KITTI: 35 videos with 17k high resolution frames for object detection, tracking, semantic/instance segmentation, depth, optical flow. Varying environmental conditions (e.g. rain). Generated using a game engine.\nSYNTHIA: 13.4k images and 4 videos with 200k frames for semantic segmentation in urban scenarios. Generated using Unity Engine.\nRichter et al. (2016): 25k images from GTA5 with dense semantic annotations.\nQiu & Yuille (2016): A method to create virtual worlds using Unreal Engine 4 and how to connect them with Caffe.\n\n MPI Sintel: Optical flow for 1.6k frames from the Sintel movie. Dosovitskiy et al. (2015): 2D images of flying chairs in front of random background images. For optical flow estimation. Mayer et al. (2016): 2D images of flying things in front of random background images. Also images of car models in front of houses and trees. For optical flow estimation. Virtual KITTI: 35 videos with 17k high resolution frames for object detection, tracking, semantic/instance segmentation, depth, optical flow. Varying environmental conditions (e.g. rain). Generated using a game engine. SYNTHIA: 13.4k images and 4 videos with 200k frames for semantic segmentation in urban scenarios. Generated using Unity Engine. Richter et al. (2016): 25k images from GTA5 with dense semantic annotations. Qiu & Yuille (2016): A method to create virtual worlds using Unreal Engine 4 and how to connect them with Caffe. Camera Models and Calibration\n\nCalibration\n\nCalibration = estimate intrinsic & extrinsic parameters of sensors (e.g. range sensors, fish-eye cameras, ...)\nThis allows to relate 2D image points with 3D world points.\nCalibration is done with the help of fiducial markings on checkerboard patterns.\nDesired features of good calibration systems are: high accuracy, speed, robustness, minimal assumptions\n\n\nOmnidirectional Cameras\n\nOmnidirectional Cameras = cameras with 360 degrees field of view\nObviously good for self-driving cars, give full situational awareness.\nCatadioptric Cameras = combine standard camera and mirror (to generate 360 degrees field of view)\nDioptric Cameras = Use doptric fisheye lenses\nPolydioptric Cameras = Use multiple cameras with overlapping field of view\nOmnidirectional Cameras can be differentiated by the projection center: Central, noncentral\nFor central cameras, all light rays intersect in a single point in 3D. Benefit: Geometrically correct generated images => allows to use epipolar geometry on the images.\nDistortion in omnidirectional cameras is too high to perform calibration based on simple, linear projections (as opposed to pinhole cameras).\nOmnidirectional cameras are roughly comparable to LiDAR (radar systems), but are far cheaper, perceive color and do not suffer from rolling shutter effects.\nH\u00e4ne et al. (2014) predict dense depth maps from omnidirectional cameras in real time (GPU) based on the plaen-sweeping stereo matching algorithm.\n\n\nEvent Cameras\n\nEvent Cameras = Produce an event when a brightness change passes a threshold (sparse data); they work at microsecond level.\nInteresting for self-driving cars due to their very high speed (low reaction times).\nEvent Cameras also do not suffer from motion blur and have high dynamic range.\nData is difficult to handle due to algorithms being adapted towards full images.\nEvents can be accumulated over time to generate full images, but this runs contrary to the speed benefit.\n\n\n\n Calibration\n\nCalibration = estimate intrinsic & extrinsic parameters of sensors (e.g. range sensors, fish-eye cameras, ...)\nThis allows to relate 2D image points with 3D world points.\nCalibration is done with the help of fiducial markings on checkerboard patterns.\nDesired features of good calibration systems are: high accuracy, speed, robustness, minimal assumptions\n\n Calibration = estimate intrinsic & extrinsic parameters of sensors (e.g. range sensors, fish-eye cameras, ...) This allows to relate 2D image points with 3D world points. Calibration is done with the help of fiducial markings on checkerboard patterns. Desired features of good calibration systems are: high accuracy, speed, robustness, minimal assumptions Omnidirectional Cameras\n\nOmnidirectional Cameras = cameras with 360 degrees field of view\nObviously good for self-driving cars, give full situational awareness.\nCatadioptric Cameras = combine standard camera and mirror (to generate 360 degrees field of view)\nDioptric Cameras = Use doptric fisheye lenses\nPolydioptric Cameras = Use multiple cameras with overlapping field of view\nOmnidirectional Cameras can be differentiated by the projection center: Central, noncentral\nFor central cameras, all light rays intersect in a single point in 3D. Benefit: Geometrically correct generated images => allows to use epipolar geometry on the images.\nDistortion in omnidirectional cameras is too high to perform calibration based on simple, linear projections (as opposed to pinhole cameras).\nOmnidirectional cameras are roughly comparable to LiDAR (radar systems), but are far cheaper, perceive color and do not suffer from rolling shutter effects.\nH\u00e4ne et al. (2014) predict dense depth maps from omnidirectional cameras in real time (GPU) based on the plaen-sweeping stereo matching algorithm.\n\n Omnidirectional Cameras = cameras with 360 degrees field of view Obviously good for self-driving cars, give full situational awareness. Catadioptric Cameras = combine standard camera and mirror (to generate 360 degrees field of view) Dioptric Cameras = Use doptric fisheye lenses Polydioptric Cameras = Use multiple cameras with overlapping field of view Omnidirectional Cameras can be differentiated by the projection center: Central, noncentral For central cameras, all light rays intersect in a single point in 3D. Benefit: Geometrically correct generated images => allows to use epipolar geometry on the images. Distortion in omnidirectional cameras is too high to perform calibration based on simple, linear projections (as opposed to pinhole cameras). Omnidirectional cameras are roughly comparable to LiDAR (radar systems), but are far cheaper, perceive color and do not suffer from rolling shutter effects. H\u00e4ne et al. (2014) predict dense depth maps from omnidirectional cameras in real time (GPU) based on the plaen-sweeping stereo matching algorithm. Event Cameras\n\nEvent Cameras = Produce an event when a brightness change passes a threshold (sparse data); they work at microsecond level.\nInteresting for self-driving cars due to their very high speed (low reaction times).\nEvent Cameras also do not suffer from motion blur and have high dynamic range.\nData is difficult to handle due to algorithms being adapted towards full images.\nEvents can be accumulated over time to generate full images, but this runs contrary to the speed benefit.\n\n Event Cameras = Produce an event when a brightness change passes a threshold (sparse data); they work at microsecond level. Interesting for self-driving cars due to their very high speed (low reaction times). Event Cameras also do not suffer from motion blur and have high dynamic range. Data is difficult to handle due to algorithms being adapted towards full images. Events can be accumulated over time to generate full images, but this runs contrary to the speed benefit. Representations\n\nPixel-based\n\nEach pixel is a random variable. E.g. in CNNs or PGMs.\nMost fine-grained.\nHigh complexity.\n\n\nSuperpixels\n\nDerived from segmentation of images into atomic regions with similar color/texture.\n\n\nStixels\n\nIntended for 3D traffic scenes.\nSomewhere between pixels and objects.\nEach stixel is a vertical stick in the landscape.\nEach stixel is defined by its position relative to the camera and its height.\nThe width is constant.\nThey are good for representing obstacles in front of the car.\n\n\n\n3D Primitives\n\nRepresent 3D geometry with primitives.\nSignificantly cheaper computationally.\n\n\n\n Pixel-based\n\nEach pixel is a random variable. E.g. in CNNs or PGMs.\nMost fine-grained.\nHigh complexity.\n\n Each pixel is a random variable. E.g. in CNNs or PGMs. Most fine-grained. High complexity. Superpixels\n\nDerived from segmentation of images into atomic regions with similar color/texture.\n\n Derived from segmentation of images into atomic regions with similar color/texture. Stixels\n\nIntended for 3D traffic scenes.\nSomewhere between pixels and objects.\nEach stixel is a vertical stick in the landscape.\nEach stixel is defined by its position relative to the camera and its height.\nThe width is constant.\nThey are good for representing obstacles in front of the car.\n\n\n Intended for 3D traffic scenes. Somewhere between pixels and objects. Each stixel is a vertical stick in the landscape. Each stixel is defined by its position relative to the camera and its height. The width is constant. They are good for representing obstacles in front of the car.  3D Primitives\n\nRepresent 3D geometry with primitives.\nSignificantly cheaper computationally.\n\n Represent 3D geometry with primitives. Significantly cheaper computationally. Object Detection\n\nUseful to detect other participants and obstacles.\nSensors\n\nCameras are cheapest.\nFor daytime, just ordinary cameras, i.e. visible spectrum (VS) cameras.\nFor nightime, use thermal infrared (TIR) cameras, which captures temperature differences.\nLasers can be used to detect range information, useful for 3d object detection and localization.\nWeather effects can influence sensors.\nSensor fusion combines information from different sensors.\n\n\nStandard Pipeline\n\nPreprocessing => RoI extraction (\"regions of interest\") => classification per RoI => Verification/Refinement\nPreprocessing: E.g. exposure/gain adjustment, camera calibration, ...\nRoI extraction: E.g. by sliding window\nClassification: Classify each RoI (e.g. cascade of weak classifiers in classic Viola Jones)\nPart-based Approaches: Develop one model per part of an object (simpler to learn per part), e.g. in Deformable Part Model (DPM)\n\n\n2D Object Detection\n\nPopular datasets for 2D object detection in autonomous cars: KITTI, Caltech-USA\nTask is dominated currently by CNNs, mainly R-CNN variations.\nKITTI contains a lot of small examples, but R-CNNs tend to be bad at detecting these.\nTherefore many current methods focus on how to improve R-CNNs for small bounding boxes.\n\n\n3D Object Detection from 2D Images\n\nNot much here, still behind 2D detectors.\n\n\n3D Object Detection from 3D Point Clouds\n\nLiDAR laser range sensor data is usually sparse and with limited spatial resolution.\nState of the art is behind in accuracy to imag based methods.\nCurrently best method on KITTI uses the LiDAR's point cloud from bird view and camera RGB images as a CNN input and then uses a RoI-Pooling based method for detection, similar to 2D bounding box detection.\n\n\nPerson Detection\n\nPeople are common obstacles for self-driving cars and are hard to detect due to high variation in poses and clothes.\nPedestrian Protection Systems (PPS) detect people around the car and warn the driver.\nThese systems have to be basically flawless.\nModern pedestrian detectors use CNNs.\nTemporal Cues\n\nAdding temporal information to object detectors improves results.\nFor pedestrians e.g. dynamic gait, motion parallax.\n\n\nScarcity of Target Class\n\nPositive examples of pedestrians are much rarer in example images than negative ones.\nEnzweiler & Gavrila (2008) suggest to use a generative model to generate synthetic examples and train a classifier on these.\n\n\nReal-time Pedestrian Detection\n\nStixel world representations can be used to reduce the search space, resulting in high performance.\n\n\n\n\nHuman Pose Estimation\n\nPose estimation is useful to e.g. estimate the intentions of a person.\nPreviously done with two-staged approach: First detect body parts, then estimate pose.\nNow done with CNNs.\n\n\nDiscussion\n\nDetection of large and obvious objects works well.\nSmaller and occluded objects cause problems.\nDifferentiating pedestrians and cyclists causes problems.\nCrowds of people can cause problems.\n\n\n\n Useful to detect other participants and obstacles. Sensors\n\nCameras are cheapest.\nFor daytime, just ordinary cameras, i.e. visible spectrum (VS) cameras.\nFor nightime, use thermal infrared (TIR) cameras, which captures temperature differences.\nLasers can be used to detect range information, useful for 3d object detection and localization.\nWeather effects can influence sensors.\nSensor fusion combines information from different sensors.\n\n Cameras are cheapest. For daytime, just ordinary cameras, i.e. visible spectrum (VS) cameras. For nightime, use thermal infrared (TIR) cameras, which captures temperature differences. Lasers can be used to detect range information, useful for 3d object detection and localization. Weather effects can influence sensors. Sensor fusion combines information from different sensors. Standard Pipeline\n\nPreprocessing => RoI extraction (\"regions of interest\") => classification per RoI => Verification/Refinement\nPreprocessing: E.g. exposure/gain adjustment, camera calibration, ...\nRoI extraction: E.g. by sliding window\nClassification: Classify each RoI (e.g. cascade of weak classifiers in classic Viola Jones)\nPart-based Approaches: Develop one model per part of an object (simpler to learn per part), e.g. in Deformable Part Model (DPM)\n\n Preprocessing => RoI extraction (\"regions of interest\") => classification per RoI => Verification/Refinement Preprocessing: E.g. exposure/gain adjustment, camera calibration, ... RoI extraction: E.g. by sliding window Classification: Classify each RoI (e.g. cascade of weak classifiers in classic Viola Jones) Part-based Approaches: Develop one model per part of an object (simpler to learn per part), e.g. in Deformable Part Model (DPM) 2D Object Detection\n\nPopular datasets for 2D object detection in autonomous cars: KITTI, Caltech-USA\nTask is dominated currently by CNNs, mainly R-CNN variations.\nKITTI contains a lot of small examples, but R-CNNs tend to be bad at detecting these.\nTherefore many current methods focus on how to improve R-CNNs for small bounding boxes.\n\n Popular datasets for 2D object detection in autonomous cars: KITTI, Caltech-USA Task is dominated currently by CNNs, mainly R-CNN variations. KITTI contains a lot of small examples, but R-CNNs tend to be bad at detecting these.\nTherefore many current methods focus on how to improve R-CNNs for small bounding boxes. 3D Object Detection from 2D Images\n\nNot much here, still behind 2D detectors.\n\n Not much here, still behind 2D detectors. 3D Object Detection from 3D Point Clouds\n\nLiDAR laser range sensor data is usually sparse and with limited spatial resolution.\nState of the art is behind in accuracy to imag based methods.\nCurrently best method on KITTI uses the LiDAR's point cloud from bird view and camera RGB images as a CNN input and then uses a RoI-Pooling based method for detection, similar to 2D bounding box detection.\n\n LiDAR laser range sensor data is usually sparse and with limited spatial resolution. State of the art is behind in accuracy to imag based methods. Currently best method on KITTI uses the LiDAR's point cloud from bird view and camera RGB images as a CNN input and then uses a RoI-Pooling based method for detection, similar to 2D bounding box detection. Person Detection\n\nPeople are common obstacles for self-driving cars and are hard to detect due to high variation in poses and clothes.\nPedestrian Protection Systems (PPS) detect people around the car and warn the driver.\nThese systems have to be basically flawless.\nModern pedestrian detectors use CNNs.\nTemporal Cues\n\nAdding temporal information to object detectors improves results.\nFor pedestrians e.g. dynamic gait, motion parallax.\n\n\nScarcity of Target Class\n\nPositive examples of pedestrians are much rarer in example images than negative ones.\nEnzweiler & Gavrila (2008) suggest to use a generative model to generate synthetic examples and train a classifier on these.\n\n\nReal-time Pedestrian Detection\n\nStixel world representations can be used to reduce the search space, resulting in high performance.\n\n\n\n People are common obstacles for self-driving cars and are hard to detect due to high variation in poses and clothes. Pedestrian Protection Systems (PPS) detect people around the car and warn the driver.\nThese systems have to be basically flawless. Modern pedestrian detectors use CNNs. Temporal Cues\n\nAdding temporal information to object detectors improves results.\nFor pedestrians e.g. dynamic gait, motion parallax.\n\n Adding temporal information to object detectors improves results. For pedestrians e.g. dynamic gait, motion parallax. Scarcity of Target Class\n\nPositive examples of pedestrians are much rarer in example images than negative ones.\nEnzweiler & Gavrila (2008) suggest to use a generative model to generate synthetic examples and train a classifier on these.\n\n Positive examples of pedestrians are much rarer in example images than negative ones. Enzweiler & Gavrila (2008) suggest to use a generative model to generate synthetic examples and train a classifier on these. Real-time Pedestrian Detection\n\nStixel world representations can be used to reduce the search space, resulting in high performance.\n\n Stixel world representations can be used to reduce the search space, resulting in high performance. Human Pose Estimation\n\nPose estimation is useful to e.g. estimate the intentions of a person.\nPreviously done with two-staged approach: First detect body parts, then estimate pose.\nNow done with CNNs.\n\n Pose estimation is useful to e.g. estimate the intentions of a person. Previously done with two-staged approach: First detect body parts, then estimate pose. Now done with CNNs. Discussion\n\nDetection of large and obvious objects works well.\nSmaller and occluded objects cause problems.\nDifferentiating pedestrians and cyclists causes problems.\nCrowds of people can cause problems.\n\n Detection of large and obvious objects works well. Smaller and occluded objects cause problems. Differentiating pedestrians and cyclists causes problems. Crowds of people can cause problems. Semantic Segmentation\n\nSemantic Segmentation = Assign each pixel in an image a class label\n\n\n\n\nPreviously done via MAP in CRFs (input either pixels or superpixels).\nBut not efficient and few simple features could be used.\nHigher-order features with long-range interactions were developed.\nMore efficient inference algorithms were developed.\nFully convolutional networks succeeded CRFs for semantic segmentation.\nThe networks face the challange to both reason on a coarse level as well as assigning classes on a fine level (pixel-wise).\n\nSolved e.g. with dilated convolutions.\nSegNet memorizes the indices of max-pooling cells to reverse the process during upsampling (saves parameters for upsampling steps).\nUsing skip connections between resolutions is also an option.\nAnother option is to upsample lower scales and then fuse scales of various resolutions in additional convolutions.\nResidual architectures can also help.\n\n\nCNNs have also been combined with CRFs. Usually, the CNN is executed first and the CRF then refines the predictions at the pixel-level.\nCurrently, models perform semantic segmentation well at the coarse level, but have more problems at the fine level.\nSemantic Instance Segmentation\n\nSemantic Instance Segmentation = Detect individual objects, classify them and segment them\nTwo approaches: Proposal-based Instance Segmentation and Proposal-free Instance Segmentation\nProposal-based Instance Segmentation first predicts region proposals (e.g. bounding boxes) and then segments each object within the region proposal. Works similarly to R-CNNs.\nProposal-free Instance Segmentation predicts instance-level segmentation and class-level segmentation in one step.\nModels are currently much weaker at instance level segmentation than semantic segmentation.\n\n\nLabel Propagation\n\nAnnotations for Semantic/Instance Segmentation are very labour intensive to produce.\nOne possible solution is to leverage information from previous/future frames to simplify the annotation process.\nI.e. annotate few keyframes, then propagate the annotations to the frames in between, based on e.g. pixelwise color information.\n\n\nSemantic Segmentation with Multiple Frames\n\nWhen multiple frames are available, temporal information can be used to improve segmentation accuracy.\n\n\nSemantic Segmentation of 3D Data\n\nSegmentation can also be performed in 3D space, instead of 2D.\nThis is less common, but probably more useful for self-driving cars.\nE.g. done by 3D CNNs in voxel space. This can be expensive with regards to computation and memory. Riegler et al. (2017) suggest to operate in an Octree space, thereby exploiting the sparseness of the output.\n\n\nSemantic Segmentation of Street Side Views\n\nOne subtask of semantic segmentation is to segment street side views, e.g. views of buildings.\nThis is e.g. useful for navigation/localization.\n\n\nSemantic Segmentation of Aerial Images\n\nUsed to find urban objects in aerial/satellite images.\nUseful e.g. for navigation - even in remote areas and in case of very recent changes.\nDifficult, because the objects have heterogeneous appearances that also can't be expressed with simple priors.\nLots of CRF/MRF solutions available.\nAerial images are usually of low resolution (e.g. 1 meter = 1 pixel), whereas ground-based images are of high resolution.\nSome solutions therefore combine aerial and ground based images, e.g. using a joint MRF.\nThe ISPRS segmentation challange contains data for (urban) semantic segmenation of airborne sensors.\nThe best solutions on ISPRS use CNN+CRF or just CNNs.\n\n\nRoad Segmentation\n\nCorrect road segmentation obviously important for self-driving cars.\nAgain, CNN solutions available.\nSome solutions to decrease labeling effort. E.g. one maps OpenStreetMap annotations onto the image using GPS coordinates and car pose.\nFree Space Estimation is a subfield of Road Segmentation\n\nDeals with detecting cars and other objects that stick out of the ground or somehow block the self-driving car.\nSolutions often employ stereo cameras for depth estimation.\nLong Range Obstacle Detection can also use radars, as their error does not increase quadratically in depth (as opposed to stereo cameras).\n\n\n\n\n\n Semantic Segmentation = Assign each pixel in an image a class label\n\n\n\n  Previously done via MAP in CRFs (input either pixels or superpixels). But not efficient and few simple features could be used. Higher-order features with long-range interactions were developed. More efficient inference algorithms were developed. Fully convolutional networks succeeded CRFs for semantic segmentation. The networks face the challange to both reason on a coarse level as well as assigning classes on a fine level (pixel-wise).\n\nSolved e.g. with dilated convolutions.\nSegNet memorizes the indices of max-pooling cells to reverse the process during upsampling (saves parameters for upsampling steps).\nUsing skip connections between resolutions is also an option.\nAnother option is to upsample lower scales and then fuse scales of various resolutions in additional convolutions.\nResidual architectures can also help.\n\n Solved e.g. with dilated convolutions. SegNet memorizes the indices of max-pooling cells to reverse the process during upsampling (saves parameters for upsampling steps). Using skip connections between resolutions is also an option. Another option is to upsample lower scales and then fuse scales of various resolutions in additional convolutions. Residual architectures can also help. CNNs have also been combined with CRFs. Usually, the CNN is executed first and the CRF then refines the predictions at the pixel-level. Currently, models perform semantic segmentation well at the coarse level, but have more problems at the fine level. Semantic Instance Segmentation\n\nSemantic Instance Segmentation = Detect individual objects, classify them and segment them\nTwo approaches: Proposal-based Instance Segmentation and Proposal-free Instance Segmentation\nProposal-based Instance Segmentation first predicts region proposals (e.g. bounding boxes) and then segments each object within the region proposal. Works similarly to R-CNNs.\nProposal-free Instance Segmentation predicts instance-level segmentation and class-level segmentation in one step.\nModels are currently much weaker at instance level segmentation than semantic segmentation.\n\n Semantic Instance Segmentation = Detect individual objects, classify them and segment them Two approaches: Proposal-based Instance Segmentation and Proposal-free Instance Segmentation Proposal-based Instance Segmentation first predicts region proposals (e.g. bounding boxes) and then segments each object within the region proposal. Works similarly to R-CNNs. Proposal-free Instance Segmentation predicts instance-level segmentation and class-level segmentation in one step. Models are currently much weaker at instance level segmentation than semantic segmentation. Label Propagation\n\nAnnotations for Semantic/Instance Segmentation are very labour intensive to produce.\nOne possible solution is to leverage information from previous/future frames to simplify the annotation process.\nI.e. annotate few keyframes, then propagate the annotations to the frames in between, based on e.g. pixelwise color information.\n\n Annotations for Semantic/Instance Segmentation are very labour intensive to produce. One possible solution is to leverage information from previous/future frames to simplify the annotation process. I.e. annotate few keyframes, then propagate the annotations to the frames in between, based on e.g. pixelwise color information. Semantic Segmentation with Multiple Frames\n\nWhen multiple frames are available, temporal information can be used to improve segmentation accuracy.\n\n When multiple frames are available, temporal information can be used to improve segmentation accuracy. Semantic Segmentation of 3D Data\n\nSegmentation can also be performed in 3D space, instead of 2D.\nThis is less common, but probably more useful for self-driving cars.\nE.g. done by 3D CNNs in voxel space. This can be expensive with regards to computation and memory. Riegler et al. (2017) suggest to operate in an Octree space, thereby exploiting the sparseness of the output.\n\n Segmentation can also be performed in 3D space, instead of 2D. This is less common, but probably more useful for self-driving cars. E.g. done by 3D CNNs in voxel space. This can be expensive with regards to computation and memory. Riegler et al. (2017) suggest to operate in an Octree space, thereby exploiting the sparseness of the output. Semantic Segmentation of Street Side Views\n\nOne subtask of semantic segmentation is to segment street side views, e.g. views of buildings.\nThis is e.g. useful for navigation/localization.\n\n One subtask of semantic segmentation is to segment street side views, e.g. views of buildings. This is e.g. useful for navigation/localization. Semantic Segmentation of Aerial Images\n\nUsed to find urban objects in aerial/satellite images.\nUseful e.g. for navigation - even in remote areas and in case of very recent changes.\nDifficult, because the objects have heterogeneous appearances that also can't be expressed with simple priors.\nLots of CRF/MRF solutions available.\nAerial images are usually of low resolution (e.g. 1 meter = 1 pixel), whereas ground-based images are of high resolution.\nSome solutions therefore combine aerial and ground based images, e.g. using a joint MRF.\nThe ISPRS segmentation challange contains data for (urban) semantic segmenation of airborne sensors.\nThe best solutions on ISPRS use CNN+CRF or just CNNs.\n\n Used to find urban objects in aerial/satellite images. Useful e.g. for navigation - even in remote areas and in case of very recent changes. Difficult, because the objects have heterogeneous appearances that also can't be expressed with simple priors. Lots of CRF/MRF solutions available. Aerial images are usually of low resolution (e.g. 1 meter = 1 pixel), whereas ground-based images are of high resolution. Some solutions therefore combine aerial and ground based images, e.g. using a joint MRF. The ISPRS segmentation challange contains data for (urban) semantic segmenation of airborne sensors. The best solutions on ISPRS use CNN+CRF or just CNNs. Road Segmentation\n\nCorrect road segmentation obviously important for self-driving cars.\nAgain, CNN solutions available.\nSome solutions to decrease labeling effort. E.g. one maps OpenStreetMap annotations onto the image using GPS coordinates and car pose.\nFree Space Estimation is a subfield of Road Segmentation\n\nDeals with detecting cars and other objects that stick out of the ground or somehow block the self-driving car.\nSolutions often employ stereo cameras for depth estimation.\nLong Range Obstacle Detection can also use radars, as their error does not increase quadratically in depth (as opposed to stereo cameras).\n\n\n\n Correct road segmentation obviously important for self-driving cars. Again, CNN solutions available. Some solutions to decrease labeling effort. E.g. one maps OpenStreetMap annotations onto the image using GPS coordinates and car pose. Free Space Estimation is a subfield of Road Segmentation\n\nDeals with detecting cars and other objects that stick out of the ground or somehow block the self-driving car.\nSolutions often employ stereo cameras for depth estimation.\nLong Range Obstacle Detection can also use radars, as their error does not increase quadratically in depth (as opposed to stereo cameras).\n\n Deals with detecting cars and other objects that stick out of the ground or somehow block the self-driving car. Solutions often employ stereo cameras for depth estimation. Long Range Obstacle Detection can also use radars, as their error does not increase quadratically in depth (as opposed to stereo cameras). Reconstruction\n\nStereo\n\nStereo estimation = extract 3D information (i.e. depth) from 2D stereo camera images\nStereo cameras consist of two cameras mounted next to each other.\nStereo estimation algorithms compute correspondences between images taken by the two cameras at the same points in time.\nStereo estimation is useful for 3D reconstruction or e.g. free space estimation\nTaxonomies\n\nFeature-based vs. area-based methods: Feature-based methods provide sparse edge-based depth maps, area-based methods generate dense outputs.\nLocal vs global: Local methods compute image disparities based on best matches (winner takes all), global methods use energy-minimization.\n\n\nMatching Cost Function\n\nUsed for matching points between left and right camera images.\nUsually assume that both cameras are rectified, reducing search space to horizontal line.\nCost function computes cost for each pixel and possible disparity. Should take lowest values at true disparity.\nAlgorithms make assumption of constant appearance between matching points. Strong lighting changes can cause problems.\n\n\nSemi-Global Matching (SGM) is a high-speed high-accuracy method for stereo estimation. It is optionally performed on top of CNN features. It is theoretically similar to belief propagation.\nUsing sensible priors/regularization helps with ambiguities in depth estimation.\nTotal Variation is one used prior.\nCNNs are nowadays common and fast for stereo estimation.\nE.g. one can embed both left and right images into a feature space and then compute the correlation between the two (when shifting an image an the x-axis).\n\n\n\n\n\n\nMulti-View 3D Reconstruction\n\nDeals with reconstructing 3D geometry from many images.\nAlso makes use of sensible priors.\nMethods usually differ by: form of photo-consistency function, scene representation, visibility computation, priors, initialization requirements.\nScene representation can be: Depth map, point cloud, (triangular) mesh/surface, volumetric/voxels\nCommon algorithm for depth maps: Plane Sweeping Stereo algorithm\nCommon algorithm for point clouds: Patch-based Multi-View Stereo (PMVS)\nA voxel grid can be converted to surfaces using the Marching Cubes algorithm.\nUrban reconstruction algorithms: Methods to automatically reconstruct the 3D geometry of cities (here from images gathered by self-driving cars).\nMost common data for multi-view reconstruction are ground based images (i.e. from cars), aerial/satellite images, LiDAR scans. Aerial is readily available but coarse.\n\n\nReconstruction and Recognition\n\nPerforming 2D semantic segmentation and 3D reconstruction in one step/model should be beneficial for the accuracy of both tasks, e.g. all pixels belonging to cars can be expected to have certain possible geometric shapes.\n\n\n\n\nCan be done on small scale (single buildings) or large scale (whole cities).\nLarge scale is difficult due to memory constraints. (Notice also that labels may increase, potentially increasing memory demands even further.) Octrees can help.\nPriors for 3D shapes can be derived e.g. using PCA or Gaussian Process Latent Vriable Models (GP-LVM).\n\n\n\n Stereo\n\nStereo estimation = extract 3D information (i.e. depth) from 2D stereo camera images\nStereo cameras consist of two cameras mounted next to each other.\nStereo estimation algorithms compute correspondences between images taken by the two cameras at the same points in time.\nStereo estimation is useful for 3D reconstruction or e.g. free space estimation\nTaxonomies\n\nFeature-based vs. area-based methods: Feature-based methods provide sparse edge-based depth maps, area-based methods generate dense outputs.\nLocal vs global: Local methods compute image disparities based on best matches (winner takes all), global methods use energy-minimization.\n\n\nMatching Cost Function\n\nUsed for matching points between left and right camera images.\nUsually assume that both cameras are rectified, reducing search space to horizontal line.\nCost function computes cost for each pixel and possible disparity. Should take lowest values at true disparity.\nAlgorithms make assumption of constant appearance between matching points. Strong lighting changes can cause problems.\n\n\nSemi-Global Matching (SGM) is a high-speed high-accuracy method for stereo estimation. It is optionally performed on top of CNN features. It is theoretically similar to belief propagation.\nUsing sensible priors/regularization helps with ambiguities in depth estimation.\nTotal Variation is one used prior.\nCNNs are nowadays common and fast for stereo estimation.\nE.g. one can embed both left and right images into a feature space and then compute the correlation between the two (when shifting an image an the x-axis).\n\n\n\n\n\n Stereo estimation = extract 3D information (i.e. depth) from 2D stereo camera images Stereo cameras consist of two cameras mounted next to each other. Stereo estimation algorithms compute correspondences between images taken by the two cameras at the same points in time. Stereo estimation is useful for 3D reconstruction or e.g. free space estimation Taxonomies\n\nFeature-based vs. area-based methods: Feature-based methods provide sparse edge-based depth maps, area-based methods generate dense outputs.\nLocal vs global: Local methods compute image disparities based on best matches (winner takes all), global methods use energy-minimization.\n\n Feature-based vs. area-based methods: Feature-based methods provide sparse edge-based depth maps, area-based methods generate dense outputs. Local vs global: Local methods compute image disparities based on best matches (winner takes all), global methods use energy-minimization. Matching Cost Function\n\nUsed for matching points between left and right camera images.\nUsually assume that both cameras are rectified, reducing search space to horizontal line.\nCost function computes cost for each pixel and possible disparity. Should take lowest values at true disparity.\nAlgorithms make assumption of constant appearance between matching points. Strong lighting changes can cause problems.\n\n Used for matching points between left and right camera images. Usually assume that both cameras are rectified, reducing search space to horizontal line. Cost function computes cost for each pixel and possible disparity. Should take lowest values at true disparity. Algorithms make assumption of constant appearance between matching points. Strong lighting changes can cause problems. Semi-Global Matching (SGM) is a high-speed high-accuracy method for stereo estimation. It is optionally performed on top of CNN features. It is theoretically similar to belief propagation. Using sensible priors/regularization helps with ambiguities in depth estimation. Total Variation is one used prior. CNNs are nowadays common and fast for stereo estimation. E.g. one can embed both left and right images into a feature space and then compute the correlation between the two (when shifting an image an the x-axis).\n\n\n\n  Multi-View 3D Reconstruction\n\nDeals with reconstructing 3D geometry from many images.\nAlso makes use of sensible priors.\nMethods usually differ by: form of photo-consistency function, scene representation, visibility computation, priors, initialization requirements.\nScene representation can be: Depth map, point cloud, (triangular) mesh/surface, volumetric/voxels\nCommon algorithm for depth maps: Plane Sweeping Stereo algorithm\nCommon algorithm for point clouds: Patch-based Multi-View Stereo (PMVS)\nA voxel grid can be converted to surfaces using the Marching Cubes algorithm.\nUrban reconstruction algorithms: Methods to automatically reconstruct the 3D geometry of cities (here from images gathered by self-driving cars).\nMost common data for multi-view reconstruction are ground based images (i.e. from cars), aerial/satellite images, LiDAR scans. Aerial is readily available but coarse.\n\n Deals with reconstructing 3D geometry from many images. Also makes use of sensible priors. Methods usually differ by: form of photo-consistency function, scene representation, visibility computation, priors, initialization requirements. Scene representation can be: Depth map, point cloud, (triangular) mesh/surface, volumetric/voxels Common algorithm for depth maps: Plane Sweeping Stereo algorithm Common algorithm for point clouds: Patch-based Multi-View Stereo (PMVS) A voxel grid can be converted to surfaces using the Marching Cubes algorithm. Urban reconstruction algorithms: Methods to automatically reconstruct the 3D geometry of cities (here from images gathered by self-driving cars). Most common data for multi-view reconstruction are ground based images (i.e. from cars), aerial/satellite images, LiDAR scans. Aerial is readily available but coarse. Reconstruction and Recognition\n\nPerforming 2D semantic segmentation and 3D reconstruction in one step/model should be beneficial for the accuracy of both tasks, e.g. all pixels belonging to cars can be expected to have certain possible geometric shapes.\n\n\n\n\nCan be done on small scale (single buildings) or large scale (whole cities).\nLarge scale is difficult due to memory constraints. (Notice also that labels may increase, potentially increasing memory demands even further.) Octrees can help.\nPriors for 3D shapes can be derived e.g. using PCA or Gaussian Process Latent Vriable Models (GP-LVM).\n\n Performing 2D semantic segmentation and 3D reconstruction in one step/model should be beneficial for the accuracy of both tasks, e.g. all pixels belonging to cars can be expected to have certain possible geometric shapes.\n\n\n\n  Can be done on small scale (single buildings) or large scale (whole cities). Large scale is difficult due to memory constraints. (Notice also that labels may increase, potentially increasing memory demands even further.) Octrees can help. Priors for 3D shapes can be derived e.g. using PCA or Gaussian Process Latent Vriable Models (GP-LVM). Motion & Pose Estimation\n\n2D Motion Estimation - Optical Flow\n\nOptical Flow = two dimensional motion of brightness patterns between two images\nUseful e.g. for tracking of objects, ego-motion estimation, structure-from-motion\nOptical flow predicts motion vectors. Each vector has two components. Two constraints are used: (a) the brightness of each pixel over time is assumed to be constant, (b) movements are assumed to be smooth between neighbours.\nOcclusions, large displacement and fine details are still challenging today.\nPixel intensity is assumed to be constant over time, but that is not always the case due to e.g. reflections or transparency.\nOriginal optical flow method was based on variational formulation. It assumed pixel intensity to be constant and added a smoothness constraint.\nLater algorithms used more robust methods than just assuming constant pixel intensity.\nA common approach is to first estimate the optical flow at a coarse level and then refine towards finer levels.\nSome methods use nearest neighbour search of 2D patches, optionally in a high-dimensional feature space.\nMany accurate optical flow algorithms are accurate but far too slow for self-driving cars. Some newer methods based on CNNs though perform well.\nTraining of optical flow algorithms often happens based on Sintel and KITTI datasets.\nComparison of methods on KITTI happens based on absolute endpoint error (EBE). An error is any predicted motion vector which's endpoint is more than 3 pixels or 5% of the image dimension away from its true endpoint. The errors are measured in percent of all vectors (i.e. lower is better) and are split into background, foreground and other regions. The final EBE is the average of these error categories.\nFor self-driving cars, it can be sensible to predict optical flow as following epipolar lines, radiating from the center of the image.\nAnother optimization is to first perform instance segmentation and then predict optical flow per instance.\n\n\n3D Motion Estimation - Scene Flow\n\nScene flow = optical flow for 3D scenes\nUsually by predicting motion vectors on surfaces of objects.\nDense prediction preferred, but sparse prediction is faster.\n\n\nEgo-Motion Estimation\n\nEgo-Motion Estimation = Estimate position and orientation of the car\nWheel odometry was previously used for that: Estimate position and orientation of the car by measuring how much the wheels spinned. This causes problems e.g. on when the wheels are slipping on wet surfaces.\nVisual/LiDAR-based odometry: Estimate position and orientation of the car by using images/LiDAR. More robust and can recover from errors by recognizing visited places (loop closure).\nVisual odometry estimates the changes between two images and then accumulates these over time.\nVisual odometry can be split into feature-based and direct formulations.\nFeature-based formulations first convert images into feature spaces, direct formulations work with the raw input data.\nFeature-based formulations often use keypoint extraction, which can be problematic with street images due to their lack of corners. This makes direct formulations usually more accurate.\nDrift\n\nPredicting changes between pairs of images (incremental approach) tends to accumulate errors over time (drift).\nThis is fixed by an iterative refinement (often computationally expensive). E.g. done by minimizing errors of 3D points or by using information from SLAM (simultaneous localization and mapping) used to recognize known locations (loop closure).\n\n\nKITTI is the only large dataset available for visual odometry.\nMonocular methods for visual odometry perform worse than methods that use 3D data.\nLiDAR-based odometry outperforms monocular visual odometry for ego-motion estimation. Stereo cameras though perform almost as good as LiDARs.\nCurrent solutions for ego-motion estimation have mostly problems crowded highways, narrow streets and when strong turns are taken.\n\n\nSimultaneous Localization and Mapping (SLAM)\n\nSLAM = Navigate through an environment and create a map of it at the same time\nSLAM must be able to deal with changing environments.\nIn case of autonomous vehicles it also has to deal with large scale environments and real-time application.\nEarly solutions used extended kalman filters or particle filters.\n\n\nLocalization\n\nLocalization = Determine position in the world\nCan be indoor or outdoors.\nMeasurements might be noisy.\nIs a subroutine of SLAM (for loop closure detection and drift correction).\nUsed sensors are GPS and camera (visual inputs).\nGPS is only accurate to about 5m (sometimes more accurate, but not reliable).\nVisual detection often works based on recognizing landmarks (\"place recognition\") and requires some kind of similarity measure.\nMonte Carlo methods are sometimes used for visual localization.\nVisual localization can be split into: Topological methods, metric methods.\nTopological localization is coarse but reliable. (Predicts position from a finite set possible positions.)\nMetric localization is fine but unreliable (especially for longer journeys).\nTopometric localization: Mixture of topological and metric localization.\nCurrent methods are on the order of 1m accuracy on short known routes or 4m accuracy on larger areas.\nStructure-based Localization: Predict (camera) location as well as all camera parameters (orientation, intrinsics) simultaneously.\nCross-view Localization: Match ground based images (rare but detailed) with aerial/satellite images (easy to get but coarse). This allows to perform geo-localization.\nOne way to do cross-view localization is to do (1) apply a CNN to ground based images to convert them to features, (2) apply a CNN to aerial images showing the same locations as the ground based images to convert them to features, (3) train networks from step 1 and 2 to produce as similar features as possible. Then do feature matching.\n\n\n\n 2D Motion Estimation - Optical Flow\n\nOptical Flow = two dimensional motion of brightness patterns between two images\nUseful e.g. for tracking of objects, ego-motion estimation, structure-from-motion\nOptical flow predicts motion vectors. Each vector has two components. Two constraints are used: (a) the brightness of each pixel over time is assumed to be constant, (b) movements are assumed to be smooth between neighbours.\nOcclusions, large displacement and fine details are still challenging today.\nPixel intensity is assumed to be constant over time, but that is not always the case due to e.g. reflections or transparency.\nOriginal optical flow method was based on variational formulation. It assumed pixel intensity to be constant and added a smoothness constraint.\nLater algorithms used more robust methods than just assuming constant pixel intensity.\nA common approach is to first estimate the optical flow at a coarse level and then refine towards finer levels.\nSome methods use nearest neighbour search of 2D patches, optionally in a high-dimensional feature space.\nMany accurate optical flow algorithms are accurate but far too slow for self-driving cars. Some newer methods based on CNNs though perform well.\nTraining of optical flow algorithms often happens based on Sintel and KITTI datasets.\nComparison of methods on KITTI happens based on absolute endpoint error (EBE). An error is any predicted motion vector which's endpoint is more than 3 pixels or 5% of the image dimension away from its true endpoint. The errors are measured in percent of all vectors (i.e. lower is better) and are split into background, foreground and other regions. The final EBE is the average of these error categories.\nFor self-driving cars, it can be sensible to predict optical flow as following epipolar lines, radiating from the center of the image.\nAnother optimization is to first perform instance segmentation and then predict optical flow per instance.\n\n Optical Flow = two dimensional motion of brightness patterns between two images Useful e.g. for tracking of objects, ego-motion estimation, structure-from-motion Optical flow predicts motion vectors. Each vector has two components. Two constraints are used: (a) the brightness of each pixel over time is assumed to be constant, (b) movements are assumed to be smooth between neighbours. Occlusions, large displacement and fine details are still challenging today. Pixel intensity is assumed to be constant over time, but that is not always the case due to e.g. reflections or transparency. Original optical flow method was based on variational formulation. It assumed pixel intensity to be constant and added a smoothness constraint. Later algorithms used more robust methods than just assuming constant pixel intensity. A common approach is to first estimate the optical flow at a coarse level and then refine towards finer levels. Some methods use nearest neighbour search of 2D patches, optionally in a high-dimensional feature space. Many accurate optical flow algorithms are accurate but far too slow for self-driving cars. Some newer methods based on CNNs though perform well. Training of optical flow algorithms often happens based on Sintel and KITTI datasets. Comparison of methods on KITTI happens based on absolute endpoint error (EBE). An error is any predicted motion vector which's endpoint is more than 3 pixels or 5% of the image dimension away from its true endpoint. The errors are measured in percent of all vectors (i.e. lower is better) and are split into background, foreground and other regions. The final EBE is the average of these error categories. For self-driving cars, it can be sensible to predict optical flow as following epipolar lines, radiating from the center of the image. Another optimization is to first perform instance segmentation and then predict optical flow per instance. 3D Motion Estimation - Scene Flow\n\nScene flow = optical flow for 3D scenes\nUsually by predicting motion vectors on surfaces of objects.\nDense prediction preferred, but sparse prediction is faster.\n\n Scene flow = optical flow for 3D scenes Usually by predicting motion vectors on surfaces of objects. Dense prediction preferred, but sparse prediction is faster. Ego-Motion Estimation\n\nEgo-Motion Estimation = Estimate position and orientation of the car\nWheel odometry was previously used for that: Estimate position and orientation of the car by measuring how much the wheels spinned. This causes problems e.g. on when the wheels are slipping on wet surfaces.\nVisual/LiDAR-based odometry: Estimate position and orientation of the car by using images/LiDAR. More robust and can recover from errors by recognizing visited places (loop closure).\nVisual odometry estimates the changes between two images and then accumulates these over time.\nVisual odometry can be split into feature-based and direct formulations.\nFeature-based formulations first convert images into feature spaces, direct formulations work with the raw input data.\nFeature-based formulations often use keypoint extraction, which can be problematic with street images due to their lack of corners. This makes direct formulations usually more accurate.\nDrift\n\nPredicting changes between pairs of images (incremental approach) tends to accumulate errors over time (drift).\nThis is fixed by an iterative refinement (often computationally expensive). E.g. done by minimizing errors of 3D points or by using information from SLAM (simultaneous localization and mapping) used to recognize known locations (loop closure).\n\n\nKITTI is the only large dataset available for visual odometry.\nMonocular methods for visual odometry perform worse than methods that use 3D data.\nLiDAR-based odometry outperforms monocular visual odometry for ego-motion estimation. Stereo cameras though perform almost as good as LiDARs.\nCurrent solutions for ego-motion estimation have mostly problems crowded highways, narrow streets and when strong turns are taken.\n\n Ego-Motion Estimation = Estimate position and orientation of the car Wheel odometry was previously used for that: Estimate position and orientation of the car by measuring how much the wheels spinned. This causes problems e.g. on when the wheels are slipping on wet surfaces. Visual/LiDAR-based odometry: Estimate position and orientation of the car by using images/LiDAR. More robust and can recover from errors by recognizing visited places (loop closure). Visual odometry estimates the changes between two images and then accumulates these over time. Visual odometry can be split into feature-based and direct formulations. Feature-based formulations first convert images into feature spaces, direct formulations work with the raw input data. Feature-based formulations often use keypoint extraction, which can be problematic with street images due to their lack of corners. This makes direct formulations usually more accurate. Drift\n\nPredicting changes between pairs of images (incremental approach) tends to accumulate errors over time (drift).\nThis is fixed by an iterative refinement (often computationally expensive). E.g. done by minimizing errors of 3D points or by using information from SLAM (simultaneous localization and mapping) used to recognize known locations (loop closure).\n\n Predicting changes between pairs of images (incremental approach) tends to accumulate errors over time (drift). This is fixed by an iterative refinement (often computationally expensive). E.g. done by minimizing errors of 3D points or by using information from SLAM (simultaneous localization and mapping) used to recognize known locations (loop closure). KITTI is the only large dataset available for visual odometry. Monocular methods for visual odometry perform worse than methods that use 3D data. LiDAR-based odometry outperforms monocular visual odometry for ego-motion estimation. Stereo cameras though perform almost as good as LiDARs. Current solutions for ego-motion estimation have mostly problems crowded highways, narrow streets and when strong turns are taken. Simultaneous Localization and Mapping (SLAM)\n\nSLAM = Navigate through an environment and create a map of it at the same time\nSLAM must be able to deal with changing environments.\nIn case of autonomous vehicles it also has to deal with large scale environments and real-time application.\nEarly solutions used extended kalman filters or particle filters.\n\n SLAM = Navigate through an environment and create a map of it at the same time SLAM must be able to deal with changing environments. In case of autonomous vehicles it also has to deal with large scale environments and real-time application. Early solutions used extended kalman filters or particle filters. Localization\n\nLocalization = Determine position in the world\nCan be indoor or outdoors.\nMeasurements might be noisy.\nIs a subroutine of SLAM (for loop closure detection and drift correction).\nUsed sensors are GPS and camera (visual inputs).\nGPS is only accurate to about 5m (sometimes more accurate, but not reliable).\nVisual detection often works based on recognizing landmarks (\"place recognition\") and requires some kind of similarity measure.\nMonte Carlo methods are sometimes used for visual localization.\nVisual localization can be split into: Topological methods, metric methods.\nTopological localization is coarse but reliable. (Predicts position from a finite set possible positions.)\nMetric localization is fine but unreliable (especially for longer journeys).\nTopometric localization: Mixture of topological and metric localization.\nCurrent methods are on the order of 1m accuracy on short known routes or 4m accuracy on larger areas.\nStructure-based Localization: Predict (camera) location as well as all camera parameters (orientation, intrinsics) simultaneously.\nCross-view Localization: Match ground based images (rare but detailed) with aerial/satellite images (easy to get but coarse). This allows to perform geo-localization.\nOne way to do cross-view localization is to do (1) apply a CNN to ground based images to convert them to features, (2) apply a CNN to aerial images showing the same locations as the ground based images to convert them to features, (3) train networks from step 1 and 2 to produce as similar features as possible. Then do feature matching.\n\n Localization = Determine position in the world Can be indoor or outdoors. Measurements might be noisy. Is a subroutine of SLAM (for loop closure detection and drift correction). Used sensors are GPS and camera (visual inputs). GPS is only accurate to about 5m (sometimes more accurate, but not reliable). Visual detection often works based on recognizing landmarks (\"place recognition\") and requires some kind of similarity measure. Monte Carlo methods are sometimes used for visual localization. Visual localization can be split into: Topological methods, metric methods. Topological localization is coarse but reliable. (Predicts position from a finite set possible positions.) Metric localization is fine but unreliable (especially for longer journeys). Topometric localization: Mixture of topological and metric localization. Current methods are on the order of 1m accuracy on short known routes or 4m accuracy on larger areas. Structure-based Localization: Predict (camera) location as well as all camera parameters (orientation, intrinsics) simultaneously. Cross-view Localization: Match ground based images (rare but detailed) with aerial/satellite images (easy to get but coarse). This allows to perform geo-localization. One way to do cross-view localization is to do (1) apply a CNN to ground based images to convert them to features, (2) apply a CNN to aerial images showing the same locations as the ground based images to convert them to features, (3) train networks from step 1 and 2 to produce as similar features as possible. Then do feature matching. Tracking\n\nTracking = Estimate state of one/multiple objects over time given measurements from sensor\nState = location, velocity, acceleration (usually)\nUseful e.g. for automatic distance control or to anticipate driving maneuvers of other traffic participants\nPrediction of the behaviour of pedestrians and bicyclists is difficult, as they can abruptly change it.\nDifficulties: Variation of appearance within the same class (e.g. different poses of pedestrians), occlusions (possibly by objects of the same class), strong lighting, reflections in mirrors/windows.\nTracking is often formulated as a bayesian inference problem. Goal: Estimate posterior probability density function of current observation, given previous state. Updated recursively.\nE.g. implemented via extended kalman filters and particle filters.\nRecursive formulations have problems with recovering from errors.\nNon-recursive formulations have gained popularity because of that. These optimize energy functions with respect to all trajectories in a temporal window. The search space for this can be large and should be somehow minimized.\nDatasets for multi object tracking are PETS (static camera), TUD (static), MOT (dynamic), KITTI (dynamic).\n\n Tracking = Estimate state of one/multiple objects over time given measurements from sensor State = location, velocity, acceleration (usually) Useful e.g. for automatic distance control or to anticipate driving maneuvers of other traffic participants Prediction of the behaviour of pedestrians and bicyclists is difficult, as they can abruptly change it. Difficulties: Variation of appearance within the same class (e.g. different poses of pedestrians), occlusions (possibly by objects of the same class), strong lighting, reflections in mirrors/windows. Tracking is often formulated as a bayesian inference problem. Goal: Estimate posterior probability density function of current observation, given previous state. Updated recursively. E.g. implemented via extended kalman filters and particle filters. Recursive formulations have problems with recovering from errors. Non-recursive formulations have gained popularity because of that. These optimize energy functions with respect to all trajectories in a temporal window. The search space for this can be large and should be somehow minimized. Datasets for multi object tracking are PETS (static camera), TUD (static), MOT (dynamic), KITTI (dynamic). Scene Understanding\n\nScene understandig = A full understanding of the surrounding area of a self-driving car\nInvolves many subtasks, such as depth estimation, object detection, ...\nSpecifically challenging are urban and sub-urban scenes: Many independently moving objects, high variance in appaerance of objects along the road, more variance in road layouts, more difficult lighting.\nOne solution uses street view images and combines them with information from OpenStreetMap. This way, the authors can automatically derive for the street view images e.g. the number of lanes, the road curvature or the distance up to the next intersection and train the model predict these information.\n\n Scene understandig = A full understanding of the surrounding area of a self-driving car Involves many subtasks, such as depth estimation, object detection, ... Specifically challenging are urban and sub-urban scenes: Many independently moving objects, high variance in appaerance of objects along the road, more variance in road layouts, more difficult lighting. One solution uses street view images and combines them with information from OpenStreetMap. This way, the authors can automatically derive for the street view images e.g. the number of lanes, the road curvature or the distance up to the next intersection and train the model predict these information. End-to-End Learning of Sensorimotor Control\n\nTraditional systems for self-driving cars contain many components, e.g. for object detection (traffic signs, pedestrians, ...) or tracking.\nThese components' predictions are then combines using some kind of rule based system in order to create driving decisions.\nModern system though lean more towards end-to-end learning, where the model e.g. gets the front facing camera's image and ground truth steering decisions and learns this way to steer.\nOne method also uses inverse reinforcement learning to extract the unknown reward function (which rewards correct steering decisions).\n\n Traditional systems for self-driving cars contain many components, e.g. for object detection (traffic signs, pedestrians, ...) or tracking. These components' predictions are then combines using some kind of rule based system in order to create driving decisions. Modern system though lean more towards end-to-end learning, where the model e.g. gets the front facing camera's image and ground truth steering decisions and learns this way to steer. One method also uses inverse reinforcement learning to extract the unknown reward function (which rewards correct steering decisions). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.05519v1"
    },
    "14": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Progressive_Growing_of_GANs.md",
        "transcript": "\nWhat\n\nThey suggest a new, progressive training method for GANs.\nThe method enables the training of high resolution GANs (1024x1024) that still produce good-looking, diverse images.\n\nThey also introduce two new normalization techniques.\n\n\nThey also suggest a new method to estimate/score the quality of the generated images.\nThey introduce CelebA-HQ, a variation of CelebA containing high resolution images.\n\n They suggest a new, progressive training method for GANs.\nThe method enables the training of high resolution GANs (1024x1024) that still produce good-looking, diverse images.\n\nThey also introduce two new normalization techniques.\n\n They also introduce two new normalization techniques. They also suggest a new method to estimate/score the quality of the generated images. They introduce CelebA-HQ, a variation of CelebA containing high resolution images. \nHow\n\nProgressive growing/training\n\nThey train their GANs resolution by resolution, starting with 4x4 and going up to 1024x1024 (a bit similar to LAPGAN).\nVisualization:\n\n\n\n\nInitially, their generator produces 4x4 images and the discriminator receives 4x4 images.\nOnce training at 4x4 does not improve any more (measured by their new score, see below), they add an upscaling module (to 8x8) to the generator and add a downscaling one to the discriminator.\nThey don't switch to the added convolutions instantly/suddenly,\nbut give the model a grace period during which the upscaled features are computed from (1-alpha)*A + alpha*B,\nwhere A are the features after just upscaling, B are the features after upscaling AND the convolutions\nand alpha is the overlay factor, which is gradually increased over time.\nThis is done for both the generator and the discriminator and at all resolutions.\nVisualization:\n\n\n\n\nNote that all layers are always trained (after they were added to the models). Training for the earlier layers does not stop.\nTraining in this way focuses most of the computation on the earlier resolutions.\nIt also seems to increase stability, as the model does not have to learn all features of all resolutions at the same time.\n\n\nMinibatch Standard Deviation\n\nThey try to improve diversity by adding a method very similar to minibatch discrimination.\nThey compute the standard deviation of each feature per spatial location (for one of the disciminator's last layers).\nThey do this per example in each minibatch, resulting in B*H*W*C standard deviations. (B = batch size, H = height, W = width, C = channels/filters)\nThey average these values to one value, then replicate them to size H*W and concatenate that to the layer's output.\nThis adds a channel with one constant value to each example in the minibatch. The value is the same for all examples.\n\n\nEqualized Learning Rate\n\nThey use Adam for their training.\nAdam updates weights roughly based on mean(gradient)/variance(gradient) (per weight).\nThey argue that this has the downside of equalizing all weight's stepsizes.\nBut some weights might require larger stepsizes and other smaller ones (large/small \"dynamic range\").\nAs a result, the learning rate will be too small for some weights and too large for others.\nTo evade this problem, they first stop using modern weight initialization techniques and instead simply sample weights from the standard normal distribution N(0,1).\nThen, they rescale each weight w_i continuously during runtime to w_i/c, where c is the per-layer normalization from He's initializer. (TODO exact formula for c?)\n\n(This looks an aweful lot like weight normalization.)\n\n\nUsing simpler weight initialization equalizes the dynamic range of parameters. Doing the normalization then fixes problems related to the simpler weight initialization.\n\n\nPixelwise Feature Vector Normalization in the Generator\n\nThey argue that collapses in GANs come from the discriminator making some temporary error, leading to high gradients, leading to bad outputs of the generator, leading to more problems in the discriminator and ultimately making both spiral out of control.\nThey fix this by normalizing feature vectors in the generator, similar to local response normalization.\nThey apply the following equation in the generator (per spatial location (x, y) with N = number of filters):\n\n\n\n\n\n\nScoring Images\n\nThey suggest a new method to score images generated by the generator.\nThey perform the following steps:\n\nSample 16384 images from the generator and the dataset.\nBuild a Laplacian Pyramid of each image.\nIt begins at a 16x16 resolution of the image and progressively doubles that until the final image resolution.\nEach level of the pyramid only contains the difference between the sum of the previous scales and the final image (i.e. each step is a difference image, containing a frequency band).\nSample per image 128 7x7 neighbourhoods/patches (randomly?) from each pyramid level.\nPer image set (generator/real) and pyramid level, compute the mean and standard deviations of each color channels of the sampled patches.\nNormalize each patch with respect to the computed means and standard deviations.\nUse Sliced Wasserstein Distance (SWD) to compute the similarity between the image sets (generator/real).\n\n\nThe result is one value. Lower values are better.\n\n\nCelebA-HQ\n\nThey derive from CelebA images a new dataset containing 30k 1024x1024 images of celebrity faces.\nThey use a convolutional autoencoder to remove JPEG artifacts from the CelebA images.\nThey use an adversarially-trained superresolution model to upscale the images.\nThey crop faces from the dataset based on their facial landmarks, so that each final face has a normalized position and rotation.\nThey rescale the images to 1024x1024 using bilinear sampling and box filters.\nThey manually select the 30k best looking images.\n\n\nOther stuff\n\nThey use Adam for training (alpha=0.001, beta1=0, beta2=0.99).\nThey use the WGAN-WP method for training, but LSGAN also works.\n\nThey set gamma to 750 (from 1) for CIFAR-10, incentivizing fast transitions.\nThey also add regularization loss on the discriminator, punishing outputs that are very far away from 0.\n\n\nTheir model for CelebA-HQ training is similar to a standard DCGAN model.\nThe generator uses two convolutions after each upscaling, the discriminator analogously two convolutions after each downscaling.\nThey start with 512 filters in the generator and end in 16 (before the output) - same for the discriminator.\nThey use leaky ReLUs in the generator and discriminator.\nThey remove batch normalization everywhere.\n\n\n\n Progressive growing/training\n\nThey train their GANs resolution by resolution, starting with 4x4 and going up to 1024x1024 (a bit similar to LAPGAN).\nVisualization:\n\n\n\n\nInitially, their generator produces 4x4 images and the discriminator receives 4x4 images.\nOnce training at 4x4 does not improve any more (measured by their new score, see below), they add an upscaling module (to 8x8) to the generator and add a downscaling one to the discriminator.\nThey don't switch to the added convolutions instantly/suddenly,\nbut give the model a grace period during which the upscaled features are computed from (1-alpha)*A + alpha*B,\nwhere A are the features after just upscaling, B are the features after upscaling AND the convolutions\nand alpha is the overlay factor, which is gradually increased over time.\nThis is done for both the generator and the discriminator and at all resolutions.\nVisualization:\n\n\n\n\nNote that all layers are always trained (after they were added to the models). Training for the earlier layers does not stop.\nTraining in this way focuses most of the computation on the earlier resolutions.\nIt also seems to increase stability, as the model does not have to learn all features of all resolutions at the same time.\n\n They train their GANs resolution by resolution, starting with 4x4 and going up to 1024x1024 (a bit similar to LAPGAN). Visualization:\n\n\n\n  Initially, their generator produces 4x4 images and the discriminator receives 4x4 images. Once training at 4x4 does not improve any more (measured by their new score, see below), they add an upscaling module (to 8x8) to the generator and add a downscaling one to the discriminator. They don't switch to the added convolutions instantly/suddenly,\nbut give the model a grace period during which the upscaled features are computed from (1-alpha)*A + alpha*B,\nwhere A are the features after just upscaling, B are the features after upscaling AND the convolutions\nand alpha is the overlay factor, which is gradually increased over time.\nThis is done for both the generator and the discriminator and at all resolutions. Visualization:\n\n\n\n  Note that all layers are always trained (after they were added to the models). Training for the earlier layers does not stop. Training in this way focuses most of the computation on the earlier resolutions.\nIt also seems to increase stability, as the model does not have to learn all features of all resolutions at the same time. Minibatch Standard Deviation\n\nThey try to improve diversity by adding a method very similar to minibatch discrimination.\nThey compute the standard deviation of each feature per spatial location (for one of the disciminator's last layers).\nThey do this per example in each minibatch, resulting in B*H*W*C standard deviations. (B = batch size, H = height, W = width, C = channels/filters)\nThey average these values to one value, then replicate them to size H*W and concatenate that to the layer's output.\nThis adds a channel with one constant value to each example in the minibatch. The value is the same for all examples.\n\n They try to improve diversity by adding a method very similar to minibatch discrimination. They compute the standard deviation of each feature per spatial location (for one of the disciminator's last layers). They do this per example in each minibatch, resulting in B*H*W*C standard deviations. (B = batch size, H = height, W = width, C = channels/filters) They average these values to one value, then replicate them to size H*W and concatenate that to the layer's output. This adds a channel with one constant value to each example in the minibatch. The value is the same for all examples. Equalized Learning Rate\n\nThey use Adam for their training.\nAdam updates weights roughly based on mean(gradient)/variance(gradient) (per weight).\nThey argue that this has the downside of equalizing all weight's stepsizes.\nBut some weights might require larger stepsizes and other smaller ones (large/small \"dynamic range\").\nAs a result, the learning rate will be too small for some weights and too large for others.\nTo evade this problem, they first stop using modern weight initialization techniques and instead simply sample weights from the standard normal distribution N(0,1).\nThen, they rescale each weight w_i continuously during runtime to w_i/c, where c is the per-layer normalization from He's initializer. (TODO exact formula for c?)\n\n(This looks an aweful lot like weight normalization.)\n\n\nUsing simpler weight initialization equalizes the dynamic range of parameters. Doing the normalization then fixes problems related to the simpler weight initialization.\n\n They use Adam for their training. Adam updates weights roughly based on mean(gradient)/variance(gradient) (per weight). They argue that this has the downside of equalizing all weight's stepsizes.\nBut some weights might require larger stepsizes and other smaller ones (large/small \"dynamic range\").\nAs a result, the learning rate will be too small for some weights and too large for others. To evade this problem, they first stop using modern weight initialization techniques and instead simply sample weights from the standard normal distribution N(0,1). Then, they rescale each weight w_i continuously during runtime to w_i/c, where c is the per-layer normalization from He's initializer. (TODO exact formula for c?)\n\n(This looks an aweful lot like weight normalization.)\n\n (This looks an aweful lot like weight normalization.) Using simpler weight initialization equalizes the dynamic range of parameters. Doing the normalization then fixes problems related to the simpler weight initialization. Pixelwise Feature Vector Normalization in the Generator\n\nThey argue that collapses in GANs come from the discriminator making some temporary error, leading to high gradients, leading to bad outputs of the generator, leading to more problems in the discriminator and ultimately making both spiral out of control.\nThey fix this by normalizing feature vectors in the generator, similar to local response normalization.\nThey apply the following equation in the generator (per spatial location (x, y) with N = number of filters):\n\n\n\n\n\n They argue that collapses in GANs come from the discriminator making some temporary error, leading to high gradients, leading to bad outputs of the generator, leading to more problems in the discriminator and ultimately making both spiral out of control. They fix this by normalizing feature vectors in the generator, similar to local response normalization. They apply the following equation in the generator (per spatial location (x, y) with N = number of filters):\n\n\n\n  Scoring Images\n\nThey suggest a new method to score images generated by the generator.\nThey perform the following steps:\n\nSample 16384 images from the generator and the dataset.\nBuild a Laplacian Pyramid of each image.\nIt begins at a 16x16 resolution of the image and progressively doubles that until the final image resolution.\nEach level of the pyramid only contains the difference between the sum of the previous scales and the final image (i.e. each step is a difference image, containing a frequency band).\nSample per image 128 7x7 neighbourhoods/patches (randomly?) from each pyramid level.\nPer image set (generator/real) and pyramid level, compute the mean and standard deviations of each color channels of the sampled patches.\nNormalize each patch with respect to the computed means and standard deviations.\nUse Sliced Wasserstein Distance (SWD) to compute the similarity between the image sets (generator/real).\n\n\nThe result is one value. Lower values are better.\n\n They suggest a new method to score images generated by the generator. They perform the following steps:\n\nSample 16384 images from the generator and the dataset.\nBuild a Laplacian Pyramid of each image.\nIt begins at a 16x16 resolution of the image and progressively doubles that until the final image resolution.\nEach level of the pyramid only contains the difference between the sum of the previous scales and the final image (i.e. each step is a difference image, containing a frequency band).\nSample per image 128 7x7 neighbourhoods/patches (randomly?) from each pyramid level.\nPer image set (generator/real) and pyramid level, compute the mean and standard deviations of each color channels of the sampled patches.\nNormalize each patch with respect to the computed means and standard deviations.\nUse Sliced Wasserstein Distance (SWD) to compute the similarity between the image sets (generator/real).\n\n Sample 16384 images from the generator and the dataset. Build a Laplacian Pyramid of each image.\nIt begins at a 16x16 resolution of the image and progressively doubles that until the final image resolution.\nEach level of the pyramid only contains the difference between the sum of the previous scales and the final image (i.e. each step is a difference image, containing a frequency band). Sample per image 128 7x7 neighbourhoods/patches (randomly?) from each pyramid level. Per image set (generator/real) and pyramid level, compute the mean and standard deviations of each color channels of the sampled patches. Normalize each patch with respect to the computed means and standard deviations. Use Sliced Wasserstein Distance (SWD) to compute the similarity between the image sets (generator/real). The result is one value. Lower values are better. CelebA-HQ\n\nThey derive from CelebA images a new dataset containing 30k 1024x1024 images of celebrity faces.\nThey use a convolutional autoencoder to remove JPEG artifacts from the CelebA images.\nThey use an adversarially-trained superresolution model to upscale the images.\nThey crop faces from the dataset based on their facial landmarks, so that each final face has a normalized position and rotation.\nThey rescale the images to 1024x1024 using bilinear sampling and box filters.\nThey manually select the 30k best looking images.\n\n They derive from CelebA images a new dataset containing 30k 1024x1024 images of celebrity faces. They use a convolutional autoencoder to remove JPEG artifacts from the CelebA images. They use an adversarially-trained superresolution model to upscale the images. They crop faces from the dataset based on their facial landmarks, so that each final face has a normalized position and rotation. They rescale the images to 1024x1024 using bilinear sampling and box filters. They manually select the 30k best looking images. Other stuff\n\nThey use Adam for training (alpha=0.001, beta1=0, beta2=0.99).\nThey use the WGAN-WP method for training, but LSGAN also works.\n\nThey set gamma to 750 (from 1) for CIFAR-10, incentivizing fast transitions.\nThey also add regularization loss on the discriminator, punishing outputs that are very far away from 0.\n\n\nTheir model for CelebA-HQ training is similar to a standard DCGAN model.\nThe generator uses two convolutions after each upscaling, the discriminator analogously two convolutions after each downscaling.\nThey start with 512 filters in the generator and end in 16 (before the output) - same for the discriminator.\nThey use leaky ReLUs in the generator and discriminator.\nThey remove batch normalization everywhere.\n\n They use Adam for training (alpha=0.001, beta1=0, beta2=0.99). They use the WGAN-WP method for training, but LSGAN also works.\n\nThey set gamma to 750 (from 1) for CIFAR-10, incentivizing fast transitions.\nThey also add regularization loss on the discriminator, punishing outputs that are very far away from 0.\n\n They set gamma to 750 (from 1) for CIFAR-10, incentivizing fast transitions. They also add regularization loss on the discriminator, punishing outputs that are very far away from 0. Their model for CelebA-HQ training is similar to a standard DCGAN model.\nThe generator uses two convolutions after each upscaling, the discriminator analogously two convolutions after each downscaling.\nThey start with 512 filters in the generator and end in 16 (before the output) - same for the discriminator.\nThey use leaky ReLUs in the generator and discriminator.\nThey remove batch normalization everywhere. \nResults\n\nScores\n\nResults, according to their new scoring measure (Sliced Wasserstein Distance) and MS-SSIM measure:\n\n\n\n\nSo progressive growing (b) significantly improves results.\nSame -- to a smaller degree -- for minibatch standard deviation (e), equalized learning rate (f) and pixelwise normalization (g).\nMinibatch discrimination worsened the results.\nUsing small batch sizes also worsened the results.\nIn (d) they \"adjusted the hyperparameters\" (??) and removed batch normalization.\n\n\nThey generate 1024x1024 CelebA images, while maintaining pixelwise quality compared to previous models.\nThey achieve an Inception Score of 8.80 on CIFAR-10. Images look improved.\nCelebA-HQ example results:\n\n\n\n\nLSUN dining room, horse, kitchen, churches:\n\n\n\n\n\n\n\n\n Scores\n\nResults, according to their new scoring measure (Sliced Wasserstein Distance) and MS-SSIM measure:\n\n\n\n\nSo progressive growing (b) significantly improves results.\nSame -- to a smaller degree -- for minibatch standard deviation (e), equalized learning rate (f) and pixelwise normalization (g).\nMinibatch discrimination worsened the results.\nUsing small batch sizes also worsened the results.\nIn (d) they \"adjusted the hyperparameters\" (??) and removed batch normalization.\n\n Results, according to their new scoring measure (Sliced Wasserstein Distance) and MS-SSIM measure:\n\n\n\n  So progressive growing (b) significantly improves results.\nSame -- to a smaller degree -- for minibatch standard deviation (e), equalized learning rate (f) and pixelwise normalization (g).\nMinibatch discrimination worsened the results.\nUsing small batch sizes also worsened the results.\nIn (d) they \"adjusted the hyperparameters\" (??) and removed batch normalization. They generate 1024x1024 CelebA images, while maintaining pixelwise quality compared to previous models. They achieve an Inception Score of 8.80 on CIFAR-10. Images look improved. CelebA-HQ example results:\n\n\n\n  LSUN dining room, horse, kitchen, churches:\n\n\n\n\n\n\n     \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2017gan-paper.pdf"
    },
    "15": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Systematic_Testing_of_CNNs_for_Autonomous_Driving.md",
        "transcript": "\nWhat\n\nThey suggest a framework that can be used to test CNNs (for self-driving cars).\nThe framework makes use of synthetically generated examples.\n\n They suggest a framework that can be used to test CNNs (for self-driving cars). The framework makes use of synthetically generated examples. \nHow\n\nTheir framework is split into three parts: Image generator, sampling methods and visualization tools.\nImage generator\n\nThe image generator uses real images from a dataset as input.\nIt may change these images via basic transformations (e.g. brightness, saturation) or by drawing objects on them (e.g. cars).\nThough they seem to only apply contrast changes and project cars (at x/y-positions).\nThe car projection also makes use of the road location in order to draw cars smaller if they are further away.\nVisualization:\n\n\n\n\n\n\nSampling methods\n\nEach modification applied by the image generator can be expressed as one or more components of a vector (e.g. 2.0 for \"increase brightness to 2x the initial value\").\nSuch a generated example/vector is then a point in a space. The space contains all possible images that can be generated.\nGenerating images and testing a CNN on them is expensive. So ideally one wants have a good search method to find images that confuse the CNN.\nThey generate candidate points in the example space using Halton and lattice-based sequences.\n(Uniform sampling of candidate points would be possible, but would not cover the space optimally.)\nUsing that method, they can generate example inputs and the corresponding CNN scores.\nThey then train a gaussian process model on these (image, CNN score) pairs.\nThis allows them to more efficiently generate images that probably cause the CNN to fail.\n\n\nVisualization tools\n\nThey generate plots from example/input vectors and measured scores (confidence, IoU).\n\n\n\n Their framework is split into three parts: Image generator, sampling methods and visualization tools. Image generator\n\nThe image generator uses real images from a dataset as input.\nIt may change these images via basic transformations (e.g. brightness, saturation) or by drawing objects on them (e.g. cars).\nThough they seem to only apply contrast changes and project cars (at x/y-positions).\nThe car projection also makes use of the road location in order to draw cars smaller if they are further away.\nVisualization:\n\n\n\n\n\n The image generator uses real images from a dataset as input. It may change these images via basic transformations (e.g. brightness, saturation) or by drawing objects on them (e.g. cars). Though they seem to only apply contrast changes and project cars (at x/y-positions).\nThe car projection also makes use of the road location in order to draw cars smaller if they are further away. Visualization:\n\n\n\n  Sampling methods\n\nEach modification applied by the image generator can be expressed as one or more components of a vector (e.g. 2.0 for \"increase brightness to 2x the initial value\").\nSuch a generated example/vector is then a point in a space. The space contains all possible images that can be generated.\nGenerating images and testing a CNN on them is expensive. So ideally one wants have a good search method to find images that confuse the CNN.\nThey generate candidate points in the example space using Halton and lattice-based sequences.\n(Uniform sampling of candidate points would be possible, but would not cover the space optimally.)\nUsing that method, they can generate example inputs and the corresponding CNN scores.\nThey then train a gaussian process model on these (image, CNN score) pairs.\nThis allows them to more efficiently generate images that probably cause the CNN to fail.\n\n Each modification applied by the image generator can be expressed as one or more components of a vector (e.g. 2.0 for \"increase brightness to 2x the initial value\"). Such a generated example/vector is then a point in a space. The space contains all possible images that can be generated. Generating images and testing a CNN on them is expensive. So ideally one wants have a good search method to find images that confuse the CNN. They generate candidate points in the example space using Halton and lattice-based sequences.\n(Uniform sampling of candidate points would be possible, but would not cover the space optimally.) Using that method, they can generate example inputs and the corresponding CNN scores. They then train a gaussian process model on these (image, CNN score) pairs. This allows them to more efficiently generate images that probably cause the CNN to fail. Visualization tools\n\nThey generate plots from example/input vectors and measured scores (confidence, IoU).\n\n They generate plots from example/input vectors and measured scores (confidence, IoU). \nResults\n\nExample results for SqueezeDet and Yolo bounding box detectors, applied to a single test image with a car projected onto various locations:\n\n\nThe size of each point indicates the IoU, i.e. small points indicate that the CNN missed the car or predicted a bad bounding box).\nBlue points indicate that the network predicted a low confidence value.\nSqueezeDet seems to have problems with cars at the center and right of the roud.\nYolo's confidence value seem to follow the distance of the car. Its achieved IoUs seem to be lower in general with a blind spot on the right of the road.\n\n\nCorresponding 3d plots of points, organized by x/y position of the car, IoU and confidence value (:\n\n\n\n\n\n Example results for SqueezeDet and Yolo bounding box detectors, applied to a single test image with a car projected onto various locations:\n\n\nThe size of each point indicates the IoU, i.e. small points indicate that the CNN missed the car or predicted a bad bounding box).\nBlue points indicate that the network predicted a low confidence value.\nSqueezeDet seems to have problems with cars at the center and right of the roud.\nYolo's confidence value seem to follow the distance of the car. Its achieved IoUs seem to be lower in general with a blind spot on the right of the road.\n\n  The size of each point indicates the IoU, i.e. small points indicate that the CNN missed the car or predicted a bad bounding box). Blue points indicate that the network predicted a low confidence value. SqueezeDet seems to have problems with cars at the center and right of the roud. Yolo's confidence value seem to follow the distance of the car. Its achieved IoUs seem to be lower in general with a blind spot on the right of the road. Corresponding 3d plots of points, organized by x/y position of the car, IoU and confidence value (:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1708.03309"
    },
    "16": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Fast_Scene_Understanding_for_Autonomous_Driving.md",
        "transcript": "\nWhat\n\nThey suggest a method to predict for images the semantic segmentation maps, instance segmentation maps and depth maps.\n\nSemantic segmentation = \"assign color X to all pixels showing a car, color Y to all pixels showing people, ...\"\nInstance segmentation = \"assign color X to all pixels showing car number 1, color Y to all pixels showing car number 2, ...\"\n\n\nTheir method is optimized to run fast and with low memory demands.\nThe method is aimed at self-driving cars.\n\n They suggest a method to predict for images the semantic segmentation maps, instance segmentation maps and depth maps.\n\nSemantic segmentation = \"assign color X to all pixels showing a car, color Y to all pixels showing people, ...\"\nInstance segmentation = \"assign color X to all pixels showing car number 1, color Y to all pixels showing car number 2, ...\"\n\n Semantic segmentation = \"assign color X to all pixels showing a car, color Y to all pixels showing people, ...\" Instance segmentation = \"assign color X to all pixels showing car number 1, color Y to all pixels showing car number 2, ...\" Their method is optimized to run fast and with low memory demands. The method is aimed at self-driving cars. \nHow\n\nArchitecture\n\nThey base their model on ENet.\nENet is a network for fast segmentation.\nIt uses three blocks of (several) residual convolutions for downscaling, followed by two blocks of upscaling.\nSome convolutions are dilated.\nNumber of filters does not exceed 128.\nThey share two blocks of downscaling between all three branches (semantic segmentation / instance segmentation / depth maps).\nEach branch gets one unique downscaling module and two upscaling modules.\n\n\nLosses\n\nSemantic Segmentation: Pixelwise cross-entropy.\nInstance Segmentation:\n\nThey do not directly predict some kind of instance flag per pixel.\nInstead they generate per pixel an embedding vector.\nThese are supposed to be similar for pixels belonging to the same instance (and dissimilar for different instances).\nThey can then cluster these vectors to find all pixels belonging to an instance (they use GPU based mean shift clustering for that).\nIn order to train the network to generate such embeddings,\nthey view each instance as a class and design the losses so that the intra-class variance (distances) is minimized,\nwhile the intra-class distances are maximized.\nSo they get a variance term/loss and a distance term/loss.\nBoth of these are hinged (so e.g. once the variance goes below a certain threshold, the corresponding loss just becomes zero).\n\nL_var = intraclass variance loss, L_dist = interclass variance loss, L_reg = regularization loss\n||.|| = L2 distance, [.]_+ = max(0, x) = hinge\nC = class, N_c = pixels in class, mu_c = mean embedding, x_i = pixel embedding, delta_v = variance hinge, delta_d = interclass distance hinge\n\n\nDepth Estimation:\n\nCommon loss here (according to the authors) would be the L2 distance with a scale invariance term and local structure similarity term.\nBut they don't use these as it performs worse than their loss.\nInstead they use a reverse Huber loss, which seems to have some similarity with a combination of L1 and L2 loss:\n\n\n\n\n\n\n\n\nOther\n\nInput image size is 1024x512.\nThey use Adam with learning rate 5e-4 and batch size 10.\nThey keep the batch norm parameters fixed (no explanation why).\n\n\n\n Architecture\n\nThey base their model on ENet.\nENet is a network for fast segmentation.\nIt uses three blocks of (several) residual convolutions for downscaling, followed by two blocks of upscaling.\nSome convolutions are dilated.\nNumber of filters does not exceed 128.\nThey share two blocks of downscaling between all three branches (semantic segmentation / instance segmentation / depth maps).\nEach branch gets one unique downscaling module and two upscaling modules.\n\n They base their model on ENet. ENet is a network for fast segmentation.\nIt uses three blocks of (several) residual convolutions for downscaling, followed by two blocks of upscaling.\nSome convolutions are dilated.\nNumber of filters does not exceed 128. They share two blocks of downscaling between all three branches (semantic segmentation / instance segmentation / depth maps). Each branch gets one unique downscaling module and two upscaling modules. Losses\n\nSemantic Segmentation: Pixelwise cross-entropy.\nInstance Segmentation:\n\nThey do not directly predict some kind of instance flag per pixel.\nInstead they generate per pixel an embedding vector.\nThese are supposed to be similar for pixels belonging to the same instance (and dissimilar for different instances).\nThey can then cluster these vectors to find all pixels belonging to an instance (they use GPU based mean shift clustering for that).\nIn order to train the network to generate such embeddings,\nthey view each instance as a class and design the losses so that the intra-class variance (distances) is minimized,\nwhile the intra-class distances are maximized.\nSo they get a variance term/loss and a distance term/loss.\nBoth of these are hinged (so e.g. once the variance goes below a certain threshold, the corresponding loss just becomes zero).\n\nL_var = intraclass variance loss, L_dist = interclass variance loss, L_reg = regularization loss\n||.|| = L2 distance, [.]_+ = max(0, x) = hinge\nC = class, N_c = pixels in class, mu_c = mean embedding, x_i = pixel embedding, delta_v = variance hinge, delta_d = interclass distance hinge\n\n\nDepth Estimation:\n\nCommon loss here (according to the authors) would be the L2 distance with a scale invariance term and local structure similarity term.\nBut they don't use these as it performs worse than their loss.\nInstead they use a reverse Huber loss, which seems to have some similarity with a combination of L1 and L2 loss:\n\n\n\n\n\n\n\n Semantic Segmentation: Pixelwise cross-entropy. Instance Segmentation:\n\nThey do not directly predict some kind of instance flag per pixel.\nInstead they generate per pixel an embedding vector.\nThese are supposed to be similar for pixels belonging to the same instance (and dissimilar for different instances).\nThey can then cluster these vectors to find all pixels belonging to an instance (they use GPU based mean shift clustering for that).\nIn order to train the network to generate such embeddings,\nthey view each instance as a class and design the losses so that the intra-class variance (distances) is minimized,\nwhile the intra-class distances are maximized.\nSo they get a variance term/loss and a distance term/loss.\nBoth of these are hinged (so e.g. once the variance goes below a certain threshold, the corresponding loss just becomes zero).\n\nL_var = intraclass variance loss, L_dist = interclass variance loss, L_reg = regularization loss\n||.|| = L2 distance, [.]_+ = max(0, x) = hinge\nC = class, N_c = pixels in class, mu_c = mean embedding, x_i = pixel embedding, delta_v = variance hinge, delta_d = interclass distance hinge\n\n They do not directly predict some kind of instance flag per pixel. Instead they generate per pixel an embedding vector.\nThese are supposed to be similar for pixels belonging to the same instance (and dissimilar for different instances).\nThey can then cluster these vectors to find all pixels belonging to an instance (they use GPU based mean shift clustering for that). In order to train the network to generate such embeddings,\nthey view each instance as a class and design the losses so that the intra-class variance (distances) is minimized,\nwhile the intra-class distances are maximized. So they get a variance term/loss and a distance term/loss.\nBoth of these are hinged (so e.g. once the variance goes below a certain threshold, the corresponding loss just becomes zero).  L_var = intraclass variance loss, L_dist = interclass variance loss, L_reg = regularization loss ||.|| = L2 distance, [.]_+ = max(0, x) = hinge C = class, N_c = pixels in class, mu_c = mean embedding, x_i = pixel embedding, delta_v = variance hinge, delta_d = interclass distance hinge Depth Estimation:\n\nCommon loss here (according to the authors) would be the L2 distance with a scale invariance term and local structure similarity term.\nBut they don't use these as it performs worse than their loss.\nInstead they use a reverse Huber loss, which seems to have some similarity with a combination of L1 and L2 loss:\n\n\n\n\n\n Common loss here (according to the authors) would be the L2 distance with a scale invariance term and local structure similarity term.\nBut they don't use these as it performs worse than their loss. Instead they use a reverse Huber loss, which seems to have some similarity with a combination of L1 and L2 loss:\n\n\n\n  Other\n\nInput image size is 1024x512.\nThey use Adam with learning rate 5e-4 and batch size 10.\nThey keep the batch norm parameters fixed (no explanation why).\n\n Input image size is 1024x512. They use Adam with learning rate 5e-4 and batch size 10. They keep the batch norm parameters fixed (no explanation why). \nResults\n\nFor semantic segmentation on Cityscapes they reach the same score as ENet (59.3 class IoU, 80.4 category IoU).\nFor instance segmentation on Cityscapes they reach 21.0 AP as opposed to 46.9 AP of Mask R-CNN.\nFor depth map prediction on Cityscapes their result differ by distance of objects.\nThe difference is lower for close objects (MAE of 1.5m for objects <25m away) and higher for the ones further apart (MAE of 7.5m for objects <100m away).\nVisualization of predicted depth vs real depth:\n\n\n\n\nExample results:\n\n\n\n\nThey reach 21fps (that should be around 3x or so faster than Mask R-CNN) and 1.2GB memory footprint.\nTraining all branches jointly in one network (as described above) vs. training them completely separately (fully disjoint networks) improves accuracy by a bit.\n\n For semantic segmentation on Cityscapes they reach the same score as ENet (59.3 class IoU, 80.4 category IoU). For instance segmentation on Cityscapes they reach 21.0 AP as opposed to 46.9 AP of Mask R-CNN. For depth map prediction on Cityscapes their result differ by distance of objects.\nThe difference is lower for close objects (MAE of 1.5m for objects <25m away) and higher for the ones further apart (MAE of 7.5m for objects <100m away). Visualization of predicted depth vs real depth:\n\n\n\n  Example results:\n\n\n\n  They reach 21fps (that should be around 3x or so faster than Mask R-CNN) and 1.2GB memory footprint. Training all branches jointly in one network (as described above) vs. training them completely separately (fully disjoint networks) improves accuracy by a bit. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1708.02550"
    },
    "17": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Arguing_Machines.md",
        "transcript": "\nWhat\n\nThey present a method to detect hard edge cases in datasets of self-driving cars.\nThese are cases that might lead to accidents.\nThe method is based on running two models simultaneously and measuring their disagreement with each other.\n\n They present a method to detect hard edge cases in datasets of self-driving cars. These are cases that might lead to accidents. The method is based on running two models simultaneously and measuring their disagreement with each other. \nHow\n\nThey use two different models:\n\nTesla-AD: Proprietary autopilot from Tesla.\nCNN: Their own CNN, trained on a Tesla dataset (images from car's perspective + steering & acceleration annotation; 420 hours of driving).\nThe network's architecture is similar to the one in the NVIDIA paper.\n\n\nTheir considered five different inputs for their own CNN:\n\nM5: RGB images (same as in the NVIDIA paper)\nM4: Images of edges in each color channel (i.e. three edge maps).\n(No detailed explanation on how the edges were detected.)\nM3: Grayscale images, from t-20, t-10 and t (where t is the current frame).\nM2: Grayscale difference images t - t-10 (i.e. difference between the current frame and the one 10 frames ago), t - t-5, t - t-1.\nM1: Grayscale difference images t-20 - t-30, t-10 - t-20, t - t-10.\nVisualization:\n\n\n\n\n\n\nTraining\n\nThey split the data into subgroups, each being defined up by the steering wheel angle.\nE.g. all example with an angle in the range [-10, 10] end in one group.\nThey select equally from all groups and get 100k training and 50k validation images.\nThis prevents the model from only training on examples showing straight driving.\n\n\nDisagreement\n\nTheir intention is to find hard example / edge cases.\nThey do this by measuring the disagreement between the models (Tesla-AD and CNN).\nTo do that, they first clip the predicted angles to the range [-10, 10].\nThen they normalize the result to the range [-1, 1].\nThey sum the differences of these predictions over a one second window (30 examples).\nIf the sum exceeds a threshold delta, they consider the models to be in disagreement and the example to be an edge case.\nThey use delta=10.\n\n\n\n They use two different models:\n\nTesla-AD: Proprietary autopilot from Tesla.\nCNN: Their own CNN, trained on a Tesla dataset (images from car's perspective + steering & acceleration annotation; 420 hours of driving).\nThe network's architecture is similar to the one in the NVIDIA paper.\n\n Tesla-AD: Proprietary autopilot from Tesla. CNN: Their own CNN, trained on a Tesla dataset (images from car's perspective + steering & acceleration annotation; 420 hours of driving).\nThe network's architecture is similar to the one in the NVIDIA paper. Their considered five different inputs for their own CNN:\n\nM5: RGB images (same as in the NVIDIA paper)\nM4: Images of edges in each color channel (i.e. three edge maps).\n(No detailed explanation on how the edges were detected.)\nM3: Grayscale images, from t-20, t-10 and t (where t is the current frame).\nM2: Grayscale difference images t - t-10 (i.e. difference between the current frame and the one 10 frames ago), t - t-5, t - t-1.\nM1: Grayscale difference images t-20 - t-30, t-10 - t-20, t - t-10.\nVisualization:\n\n\n\n\n\n M5: RGB images (same as in the NVIDIA paper) M4: Images of edges in each color channel (i.e. three edge maps).\n(No detailed explanation on how the edges were detected.) M3: Grayscale images, from t-20, t-10 and t (where t is the current frame). M2: Grayscale difference images t - t-10 (i.e. difference between the current frame and the one 10 frames ago), t - t-5, t - t-1. M1: Grayscale difference images t-20 - t-30, t-10 - t-20, t - t-10. Visualization:\n\n\n\n  Training\n\nThey split the data into subgroups, each being defined up by the steering wheel angle.\nE.g. all example with an angle in the range [-10, 10] end in one group.\nThey select equally from all groups and get 100k training and 50k validation images.\nThis prevents the model from only training on examples showing straight driving.\n\n They split the data into subgroups, each being defined up by the steering wheel angle.\nE.g. all example with an angle in the range [-10, 10] end in one group. They select equally from all groups and get 100k training and 50k validation images. This prevents the model from only training on examples showing straight driving. Disagreement\n\nTheir intention is to find hard example / edge cases.\nThey do this by measuring the disagreement between the models (Tesla-AD and CNN).\nTo do that, they first clip the predicted angles to the range [-10, 10].\nThen they normalize the result to the range [-1, 1].\nThey sum the differences of these predictions over a one second window (30 examples).\nIf the sum exceeds a threshold delta, they consider the models to be in disagreement and the example to be an edge case.\nThey use delta=10.\n\n Their intention is to find hard example / edge cases. They do this by measuring the disagreement between the models (Tesla-AD and CNN). To do that, they first clip the predicted angles to the range [-10, 10].\nThen they normalize the result to the range [-1, 1]. They sum the differences of these predictions over a one second window (30 examples). If the sum exceeds a threshold delta, they consider the models to be in disagreement and the example to be an edge case. They use delta=10. \nResults\n\nInput M1 performed best, followed by M2, M3, M4 and M5 (in that order).\n(Measured via MAE of CNN predictions vs. ground truth annotations.)\nThey can predict with 90% accuracy whether there will be a disengagement in the next 5 seconds (i.e. human driver took control in the dataset).\n\n Input M1 performed best, followed by M2, M3, M4 and M5 (in that order).\n(Measured via MAE of CNN predictions vs. ground truth annotations.) They can predict with 90% accuracy whether there will be a disengagement in the next 5 seconds (i.e. human driver took control in the dataset). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1710.04459"
    },
    "18": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Virtual_to_Real_RL_for_AD.md",
        "transcript": "\nWhat\n\nThey suggest a method to train a model for self-driving cars in a (virtual) game, while letting each frame look like a real one.\nThis allows to learn driving policies via reinforcement learning in a virtual environment, yet (later on) use them in real cars.\n\n They suggest a method to train a model for self-driving cars in a (virtual) game, while letting each frame look like a real one. This allows to learn driving policies via reinforcement learning in a virtual environment, yet (later on) use them in real cars. \nHow\n\nBasics\n\nThe method is based on GANs, similar to something like CycleGAN.\nThe basic idea is to transform each game image/frame into a (semantic) segmentation map.\nThen the segmentation map is transformed into a realistic looking image.\nBoth steps use GANs.\nThe model is then trained on the realistic looking images, instead of the game frames.\n(Why not just train it on the segmentation maps...?)\nThey argue that the segmentation maps can be viewed as the semantic representation between both (fake & real) images.\n(Similar to how machine translation models often convert each sentence to a vector representing the semantics before generating the translated sentence.)\nVisualization of the architecture:\n\n\n\n\n\n\nLoss\n\nThey use conditional GANs.\nThe generator gets the frame image x and a noise vector z and has to generate a segmentation map s.\nThe discriminator gets the frame image x and a segmentation map s and has to tell whether s is real or fake.\nA second pair of generator and discriminator is then used to turn s into real images.\nThey use the standard GAN loss.\nThey add an L1 loss to \"suppress blurring\".\n(??? GANs shouldn't generate blurry images.\nThis sounds more like they train G to predict s using the L1 loss.\nThey then end up with blurry images, so they add the GAN loss to make them sharp.)\nFull loss:\n\n\n\n\n\n\nAgent\n\nThey use the A3C algorithm for training. (12 threads)\nTheir reward function incentivizes fast speeds with the car being close to the road's center.\nThey punish collisions.\nReward function:\n\n\nv_t is the speed in m/s\nalpha is the angle in rad\nbeta = 0.006\ngamma = -0.025\n\n\nThey predict 9 actions: left/straight/right, each with accelerate/brake/nothing.\n\n\nGame\n\nThey train on the game \"TORCS\". (I guess that game provides segmentation maps for each game frame?)\n\n\n\n Basics\n\nThe method is based on GANs, similar to something like CycleGAN.\nThe basic idea is to transform each game image/frame into a (semantic) segmentation map.\nThen the segmentation map is transformed into a realistic looking image.\nBoth steps use GANs.\nThe model is then trained on the realistic looking images, instead of the game frames.\n(Why not just train it on the segmentation maps...?)\nThey argue that the segmentation maps can be viewed as the semantic representation between both (fake & real) images.\n(Similar to how machine translation models often convert each sentence to a vector representing the semantics before generating the translated sentence.)\nVisualization of the architecture:\n\n\n\n\n\n The method is based on GANs, similar to something like CycleGAN. The basic idea is to transform each game image/frame into a (semantic) segmentation map.\nThen the segmentation map is transformed into a realistic looking image.\nBoth steps use GANs. The model is then trained on the realistic looking images, instead of the game frames.\n(Why not just train it on the segmentation maps...?) They argue that the segmentation maps can be viewed as the semantic representation between both (fake & real) images.\n(Similar to how machine translation models often convert each sentence to a vector representing the semantics before generating the translated sentence.) Visualization of the architecture:\n\n\n\n  Loss\n\nThey use conditional GANs.\nThe generator gets the frame image x and a noise vector z and has to generate a segmentation map s.\nThe discriminator gets the frame image x and a segmentation map s and has to tell whether s is real or fake.\nA second pair of generator and discriminator is then used to turn s into real images.\nThey use the standard GAN loss.\nThey add an L1 loss to \"suppress blurring\".\n(??? GANs shouldn't generate blurry images.\nThis sounds more like they train G to predict s using the L1 loss.\nThey then end up with blurry images, so they add the GAN loss to make them sharp.)\nFull loss:\n\n\n\n\n\n They use conditional GANs. The generator gets the frame image x and a noise vector z and has to generate a segmentation map s. The discriminator gets the frame image x and a segmentation map s and has to tell whether s is real or fake. A second pair of generator and discriminator is then used to turn s into real images. They use the standard GAN loss. They add an L1 loss to \"suppress blurring\".\n(??? GANs shouldn't generate blurry images.\nThis sounds more like they train G to predict s using the L1 loss.\nThey then end up with blurry images, so they add the GAN loss to make them sharp.) Full loss:\n\n\n\n  Agent\n\nThey use the A3C algorithm for training. (12 threads)\nTheir reward function incentivizes fast speeds with the car being close to the road's center.\nThey punish collisions.\nReward function:\n\n\nv_t is the speed in m/s\nalpha is the angle in rad\nbeta = 0.006\ngamma = -0.025\n\n\nThey predict 9 actions: left/straight/right, each with accelerate/brake/nothing.\n\n They use the A3C algorithm for training. (12 threads) Their reward function incentivizes fast speeds with the car being close to the road's center.\nThey punish collisions. Reward function:\n\n\nv_t is the speed in m/s\nalpha is the angle in rad\nbeta = 0.006\ngamma = -0.025\n\n  v_t is the speed in m/s alpha is the angle in rad beta = 0.006 gamma = -0.025 They predict 9 actions: left/straight/right, each with accelerate/brake/nothing. Game\n\nThey train on the game \"TORCS\". (I guess that game provides segmentation maps for each game frame?)\n\n They train on the game \"TORCS\". (I guess that game provides segmentation maps for each game frame?) \nResults\n\nThey train their model in TORCS on track X and evaluate on Y.\nThey achieve slightly better scores than a competing model trained on several tracks (A, B, C, D, ..., but not on X).\nA model trained directly on X peforms significantly better.\n\n\n\n\nThey test their model on a dataset associated with the NVIDIA self-driving paper.\nThey reach 43% correct, while the supervised method reaches 53%.\nA competing model \"B-RL\" reaches 28% (reinforcement learned, but only on game images).\nExample translations from game to real:\n\n\n\n\n\n They train their model in TORCS on track X and evaluate on Y.\nThey achieve slightly better scores than a competing model trained on several tracks (A, B, C, D, ..., but not on X).\nA model trained directly on X peforms significantly better.\n\n\n\n  They test their model on a dataset associated with the NVIDIA self-driving paper.\nThey reach 43% correct, while the supervised method reaches 53%.\nA competing model \"B-RL\" reaches 28% (reinforcement learned, but only on game images). Example translations from game to real:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.03952"
    },
    "19": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/End_to_End_Learning_for_Self-Driving_Cars.md",
        "transcript": "\nWhat\n\nThey describe a model that automatically steers cars on roads.\nThe model runs in realtime.\nThe model is trained from images and corresponding (correct) steering wheel angles.\n\n They describe a model that automatically steers cars on roads. The model runs in realtime. The model is trained from images and corresponding (correct) steering wheel angles. \nHow\n\nArchitecture\n\nTheir model is just a standard CNN with a few convolutional and fully connected layers.\nThey have a single output: the correct steering wheel angle predicted by the model.\n(The model does not predict whether to accelerate or brake.)\n\nTo be more precise: The steering wheel angle is predicted as 1/r, where r is the turning radius of the car.\nThis way, the prediction is more independent of the used car.\nThey use 1/r instead of r in order to avoid getting infinity when driving straight.\n\n\nTheir input images are in YUV color space.\nThey use a hard coded (not learned) normalization layer (no further explanations in paper).\nVisualization:\n\n\n\n\n\n\nData collection\n\nThey drive on roads with cars, collecting photos and corresponding steering wheel angles.\nThey drive on different road settings (e.g. highway, tunnels, residential roads) and weather (sunny, cloudy, foggy, ...).\nThey collected about 72 hours of driving.\nThey annotate each example with the road center location, road type and lane switching information (\"stays in lane\"/\"is switching lanes\").\n\n\nData augmentation\n\nTraining on just the collected images is not going to work, because they only show correct driving.\nSooner or later the model would make a mistake and end up going slightly off road.\nThis would then be a situation that it has never seen before and the predicted output becomes more or less random, leading to crashes.\nThey attached two more cameras to the cars during data collection, one pointing to the left and one to the right.\nThey use these images as examples of bad situations and change the ground truth steering wheel angle\nso that it would steer the car back onto the road within 2 seconds.\nThey also generate additional images between the center and left/right cameras.\nThey do this by viewpoint transformation and assume that all point above the horizon line are infinitely far away,\nwhile all below the horizon line are on flat grounds.\n(No further explanation here on how they transform images and annotations exactly. And where do they get the horizon line from?)\nThey also seem to apply random rotations and translations to the images (or maybe that is meant by viewpoint transformations?).\n\n\nTraining\n\nThey only train on examples where the driver stays in a lane (as opposed to switching lanes).\nThey subsample the input at 10fps (collection was at 30fps) in order to not get too similar examples.\nTraining and application happens in Torch7.\nThe system can predict at 30fps.\n\n\n\n Architecture\n\nTheir model is just a standard CNN with a few convolutional and fully connected layers.\nThey have a single output: the correct steering wheel angle predicted by the model.\n(The model does not predict whether to accelerate or brake.)\n\nTo be more precise: The steering wheel angle is predicted as 1/r, where r is the turning radius of the car.\nThis way, the prediction is more independent of the used car.\nThey use 1/r instead of r in order to avoid getting infinity when driving straight.\n\n\nTheir input images are in YUV color space.\nThey use a hard coded (not learned) normalization layer (no further explanations in paper).\nVisualization:\n\n\n\n\n\n Their model is just a standard CNN with a few convolutional and fully connected layers. They have a single output: the correct steering wheel angle predicted by the model.\n(The model does not predict whether to accelerate or brake.)\n\nTo be more precise: The steering wheel angle is predicted as 1/r, where r is the turning radius of the car.\nThis way, the prediction is more independent of the used car.\nThey use 1/r instead of r in order to avoid getting infinity when driving straight.\n\n To be more precise: The steering wheel angle is predicted as 1/r, where r is the turning radius of the car.\nThis way, the prediction is more independent of the used car.\nThey use 1/r instead of r in order to avoid getting infinity when driving straight. Their input images are in YUV color space. They use a hard coded (not learned) normalization layer (no further explanations in paper). Visualization:\n\n\n\n  Data collection\n\nThey drive on roads with cars, collecting photos and corresponding steering wheel angles.\nThey drive on different road settings (e.g. highway, tunnels, residential roads) and weather (sunny, cloudy, foggy, ...).\nThey collected about 72 hours of driving.\nThey annotate each example with the road center location, road type and lane switching information (\"stays in lane\"/\"is switching lanes\").\n\n They drive on roads with cars, collecting photos and corresponding steering wheel angles. They drive on different road settings (e.g. highway, tunnels, residential roads) and weather (sunny, cloudy, foggy, ...). They collected about 72 hours of driving. They annotate each example with the road center location, road type and lane switching information (\"stays in lane\"/\"is switching lanes\"). Data augmentation\n\nTraining on just the collected images is not going to work, because they only show correct driving.\nSooner or later the model would make a mistake and end up going slightly off road.\nThis would then be a situation that it has never seen before and the predicted output becomes more or less random, leading to crashes.\nThey attached two more cameras to the cars during data collection, one pointing to the left and one to the right.\nThey use these images as examples of bad situations and change the ground truth steering wheel angle\nso that it would steer the car back onto the road within 2 seconds.\nThey also generate additional images between the center and left/right cameras.\nThey do this by viewpoint transformation and assume that all point above the horizon line are infinitely far away,\nwhile all below the horizon line are on flat grounds.\n(No further explanation here on how they transform images and annotations exactly. And where do they get the horizon line from?)\nThey also seem to apply random rotations and translations to the images (or maybe that is meant by viewpoint transformations?).\n\n Training on just the collected images is not going to work, because they only show correct driving.\nSooner or later the model would make a mistake and end up going slightly off road.\nThis would then be a situation that it has never seen before and the predicted output becomes more or less random, leading to crashes. They attached two more cameras to the cars during data collection, one pointing to the left and one to the right. They use these images as examples of bad situations and change the ground truth steering wheel angle\nso that it would steer the car back onto the road within 2 seconds. They also generate additional images between the center and left/right cameras.\nThey do this by viewpoint transformation and assume that all point above the horizon line are infinitely far away,\nwhile all below the horizon line are on flat grounds.\n(No further explanation here on how they transform images and annotations exactly. And where do they get the horizon line from?) They also seem to apply random rotations and translations to the images (or maybe that is meant by viewpoint transformations?). Training\n\nThey only train on examples where the driver stays in a lane (as opposed to switching lanes).\nThey subsample the input at 10fps (collection was at 30fps) in order to not get too similar examples.\nTraining and application happens in Torch7.\nThe system can predict at 30fps.\n\n They only train on examples where the driver stays in a lane (as opposed to switching lanes). They subsample the input at 10fps (collection was at 30fps) in order to not get too similar examples. Training and application happens in Torch7. The system can predict at 30fps. \nResults\n\nSimulation\n\nThey simulate on the collected data how the model would steer.\nI.e. they predict steering wheel angles and then transform future images as if that angle had been chosen.\nThey measure how far the simulated car would be away from the road center.\nThey measure an virtual, human intervention (of 6 seconds) whenever the car gets too far away from the center.\nThey can then estimate the simulated time that the car would have been autonomous.\nThe actual number though is nowhere in the paper. x(\n\n\nThey drive in Monmouth County, being autonomous roughly 98% of the time.\nThey drive 16.09km on Garden State Parkway, being autonomous 100% of the time.\n\n Simulation\n\nThey simulate on the collected data how the model would steer.\nI.e. they predict steering wheel angles and then transform future images as if that angle had been chosen.\nThey measure how far the simulated car would be away from the road center.\nThey measure an virtual, human intervention (of 6 seconds) whenever the car gets too far away from the center.\nThey can then estimate the simulated time that the car would have been autonomous.\nThe actual number though is nowhere in the paper. x(\n\n They simulate on the collected data how the model would steer. I.e. they predict steering wheel angles and then transform future images as if that angle had been chosen. They measure how far the simulated car would be away from the road center. They measure an virtual, human intervention (of 6 seconds) whenever the car gets too far away from the center. They can then estimate the simulated time that the car would have been autonomous. The actual number though is nowhere in the paper. x( They drive in Monmouth County, being autonomous roughly 98% of the time. They drive 16.09km on Garden State Parkway, being autonomous 100% of the time. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.07316"
    },
    "20": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Snapshot_Ensembles.md",
        "transcript": "\nWhat\n\nThey suggest a method to generate ensembles of models requiring less total training time.\nThe method is based on \"saving\" intermediary versions of models.\n\n They suggest a method to generate ensembles of models requiring less total training time. The method is based on \"saving\" intermediary versions of models. \nHow\n\nThey save every M epochs an intermediary version of the model (i.e. they save the parameters).\nThen they combine the last n models to an ensemble.\nTo make each version more dissimilar from the other ones, they cycle the learning rate using a cosine function.\nThat means that they increase the learning rate significantly, then keep it at the high level for a short time,\nthen decrease it fast, then keep it at a low level for a short time.\nFormula for the (cycled) learning rate:\n\n\nalpha_0 is the initial learning rate,\nT the total number of training iterations\nM the number of training iterations per cycle (after each cycle, the learning rate is increased)\n\n\nVisualization of the cycled learning rate:\n\n\n(Note that after the third cycle the model is already at nearly optimal accuracy, despite having suffered two times from\nincreasing the learning rate. With a better learning rate schedule it might have been able to reach the optimum in\n2-3 cycles. So it is kinda unhonest to say that this ensembling method adds no training it, it just probably adds\nless than \"normal\" ensembling from scratch.)\n(Also note here that the blue learning rate schedule that they are comparing against is probably far away from being optimal.)\n\n\nThey argue that cycling the learning rate is also useful to \"jump\" between local minima.\nI.e. the model reaches a local minima, then escapes it using a high learning rate,\nthen descends into a new local minima using the lowered learning rate.\nThen the ensemble would consist of models in different local minima.\n\n(Note here though that the current state of science is that there aren't really local minima in deep NNs,\nonly saddle points.)\n(Also note that this means that the ensembled models probably are often fairly similar.\nA proper ensemble consisting only of models trained from scratch might perform better.)\n\n\n\n They save every M epochs an intermediary version of the model (i.e. they save the parameters).\nThen they combine the last n models to an ensemble. To make each version more dissimilar from the other ones, they cycle the learning rate using a cosine function.\nThat means that they increase the learning rate significantly, then keep it at the high level for a short time,\nthen decrease it fast, then keep it at a low level for a short time. Formula for the (cycled) learning rate:\n\n\nalpha_0 is the initial learning rate,\nT the total number of training iterations\nM the number of training iterations per cycle (after each cycle, the learning rate is increased)\n\n  alpha_0 is the initial learning rate, T the total number of training iterations M the number of training iterations per cycle (after each cycle, the learning rate is increased) Visualization of the cycled learning rate:\n\n\n(Note that after the third cycle the model is already at nearly optimal accuracy, despite having suffered two times from\nincreasing the learning rate. With a better learning rate schedule it might have been able to reach the optimum in\n2-3 cycles. So it is kinda unhonest to say that this ensembling method adds no training it, it just probably adds\nless than \"normal\" ensembling from scratch.)\n(Also note here that the blue learning rate schedule that they are comparing against is probably far away from being optimal.)\n\n  (Note that after the third cycle the model is already at nearly optimal accuracy, despite having suffered two times from\nincreasing the learning rate. With a better learning rate schedule it might have been able to reach the optimum in\n2-3 cycles. So it is kinda unhonest to say that this ensembling method adds no training it, it just probably adds\nless than \"normal\" ensembling from scratch.) (Also note here that the blue learning rate schedule that they are comparing against is probably far away from being optimal.) They argue that cycling the learning rate is also useful to \"jump\" between local minima.\nI.e. the model reaches a local minima, then escapes it using a high learning rate,\nthen descends into a new local minima using the lowered learning rate.\nThen the ensemble would consist of models in different local minima.\n\n(Note here though that the current state of science is that there aren't really local minima in deep NNs,\nonly saddle points.)\n(Also note that this means that the ensembled models probably are often fairly similar.\nA proper ensemble consisting only of models trained from scratch might perform better.)\n\n (Note here though that the current state of science is that there aren't really local minima in deep NNs,\nonly saddle points.) (Also note that this means that the ensembled models probably are often fairly similar.\nA proper ensemble consisting only of models trained from scratch might perform better.) \nResults\n\nUsing roughly the 3 last snapshots for the ensemble seems to be the best compromise (at alpha_0=0.1).\nUsing too many snaphots can worsen the results.\nUsing alpha_0=0.2 seems to be a better choice than alpha_0=0.1.\nThey argue that the high learning rate between cycles leads to more diverse local minima.\nOnly running one learning rate cycle and collecting the ensemble models from that leads to worse results (as opposed to running multiple cycles).\nThe following visualization shows the effect of using a single cycle vs. multiple (in relations to the training iterations).\n\n\n\n\nThey perform overall better than models without ensembling.\n\n\n\n\nTrue ensembles reach quite a bit better accuracy still.\n\n\n\n\nRunning models with interpolated snaphots (e.g. set each weight to 30% of snapshot 1 and 70% of snaphot 5),\nthe test accuracy is improved if the snapshots are close to each other (e.g. snaphot 4 and 5).\nThis indicates that the parameters change more and more with each cycle.\n\n Using roughly the 3 last snapshots for the ensemble seems to be the best compromise (at alpha_0=0.1). Using too many snaphots can worsen the results. Using alpha_0=0.2 seems to be a better choice than alpha_0=0.1.\nThey argue that the high learning rate between cycles leads to more diverse local minima. Only running one learning rate cycle and collecting the ensemble models from that leads to worse results (as opposed to running multiple cycles).\nThe following visualization shows the effect of using a single cycle vs. multiple (in relations to the training iterations).\n\n\n\n  They perform overall better than models without ensembling.\n\n\n\n  True ensembles reach quite a bit better accuracy still.\n\n\n\n  Running models with interpolated snaphots (e.g. set each weight to 30% of snapshot 1 and 70% of snaphot 5),\nthe test accuracy is improved if the snapshots are close to each other (e.g. snaphot 4 and 5).\nThis indicates that the parameters change more and more with each cycle. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.00109"
    },
    "21": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Image_Crowd_Counting_using_CNN_and_MRF.md",
        "transcript": "\nWhat\n\nThey suggest a method to estimate the number of visible people in images of crowds.\nTheir method is based on a combination of CNNs and MRFs.\n\n They suggest a method to estimate the number of visible people in images of crowds. Their method is based on a combination of CNNs and MRFs. \nHow\n\nThey split each image of crowds into overlapping square patches.\nEach patch is resized to 224x224 and fed through ResNet-152. (Sounds like they don't fine-tune that model.)\nThey extract the features from the last layer fc1000. (A bit weird considering those are the class probabilities.)\nThey apply a few fully connected layers to that result in order to arrive at one value, which regresses the number of people visible in that patch.\nThose last fully connected layers are trained via a simple mean squared (or also absolute) error.\nThe ground truth labels are computed from the total count of visible people in the image (sum of patch counts equals total number of people).\nThey argue that count values should be similar between adjacent patches and smoothen them using a MRF:\n\n\np is a patch (same for q) and P are all patches\nc_p/c_q is the count of patch p/q\nD_p(c_p) is the cost of assigning count c_p to patch p, usually given by lambda * (ground_truth - c_p)^2, where lambda is a trained(?) weight (per patch?)\nV(c_p - c_q) is the smoothness cost, usually given by (c_p - c_q)^2\n\n\nTraining goal of the MRF formulation is to minimize the energy E(c),\nwhich is achieved by mostly assigning ground truth counts (D(.)), but also keeping the counts smooth to the neighbors (V(.)).\nThe energy function is minimized using belief propagation.\n(But doesn't that have to be reexecuted for every single image?\nOnly lambda is apparently trained, which is image-specific.\nThat would probably be slow and requires the usage of ground truth annotation for all images, including validation/test?!)\nVisualization of the steps:\n\n\n\n\n\n They split each image of crowds into overlapping square patches. Each patch is resized to 224x224 and fed through ResNet-152. (Sounds like they don't fine-tune that model.) They extract the features from the last layer fc1000. (A bit weird considering those are the class probabilities.) They apply a few fully connected layers to that result in order to arrive at one value, which regresses the number of people visible in that patch. Those last fully connected layers are trained via a simple mean squared (or also absolute) error.\nThe ground truth labels are computed from the total count of visible people in the image (sum of patch counts equals total number of people). They argue that count values should be similar between adjacent patches and smoothen them using a MRF:\n\n\np is a patch (same for q) and P are all patches\nc_p/c_q is the count of patch p/q\nD_p(c_p) is the cost of assigning count c_p to patch p, usually given by lambda * (ground_truth - c_p)^2, where lambda is a trained(?) weight (per patch?)\nV(c_p - c_q) is the smoothness cost, usually given by (c_p - c_q)^2\n\n  p is a patch (same for q) and P are all patches c_p/c_q is the count of patch p/q D_p(c_p) is the cost of assigning count c_p to patch p, usually given by lambda * (ground_truth - c_p)^2, where lambda is a trained(?) weight (per patch?) V(c_p - c_q) is the smoothness cost, usually given by (c_p - c_q)^2 Training goal of the MRF formulation is to minimize the energy E(c),\nwhich is achieved by mostly assigning ground truth counts (D(.)), but also keeping the counts smooth to the neighbors (V(.)). The energy function is minimized using belief propagation.\n(But doesn't that have to be reexecuted for every single image?\nOnly lambda is apparently trained, which is image-specific.\nThat would probably be slow and requires the usage of ground truth annotation for all images, including validation/test?!) Visualization of the steps:\n\n\n\n  \nResults\n\nUCF dataset\n\nContains 50 grayscale images with 63705 annotated people (each head is annotated).\nThey achieve the best score among all competitors (about 10% lower in MAE compared to best alternative).\n\n\nShanghaitech dataset\n\nContains 1198 images with 330165 annotated people (also head annotations).\nThey achieve the best score among all competitors (about 30% lower in MAE compared to best alternative).\n\n\nVisualization of smoothing from MRF:\n\n\n\n\n\n UCF dataset\n\nContains 50 grayscale images with 63705 annotated people (each head is annotated).\nThey achieve the best score among all competitors (about 10% lower in MAE compared to best alternative).\n\n Contains 50 grayscale images with 63705 annotated people (each head is annotated). They achieve the best score among all competitors (about 10% lower in MAE compared to best alternative). Shanghaitech dataset\n\nContains 1198 images with 330165 annotated people (also head annotations).\nThey achieve the best score among all competitors (about 30% lower in MAE compared to best alternative).\n\n Contains 1198 images with 330165 annotated people (also head annotations). They achieve the best score among all competitors (about 30% lower in MAE compared to best alternative). Visualization of smoothing from MRF:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.03686"
    },
    "22": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Rainbow.md",
        "transcript": "\nWhat\n\nThey combine several previous improvements for reinforcement learning to one algorithm.\nThe combination beats previous methods by a good margin.\nThey analyze which of the used improvements has most influence on the result.\n\n They combine several previous improvements for reinforcement learning to one algorithm. The combination beats previous methods by a good margin. They analyze which of the used improvements has most influence on the result. \nHow\n\nThey use the following improvements:\n\nDouble Q-learning\n\nUses two networks during training.\nOne predicts Q-values, the other is updated.\nUsually done by using a copy with freezed parameters.\n\n\nPrioritized Replay\n\nSamples experiences from the replay memory based on the difference between predicted and real Q-values.\nRecent experiences also get higher priority.\n\n\nDueling Networks\n\nSplits the Q-value prediction into a value stream (mean value over all values) and an advantage stream (advantage of specific actions over others).\n\n\nMulti-Step Learning\n\nSplits direct reward + Q(next state, next action) into direct reward + direct reward of next N actions + Q(next Nth state, next Nth action) (weighted with discrount factor).\nI.e. per training example, some experiences from the future are directly used to compute the rewards and only after some point the Q-function is used.\n\n\nDistributional RL\n\nSeems to be nothing else but switching the regression (of Q-values) to classification in order to avoid standard mode problems with regression.\nThe range of possible reward values is partitioned into N bins.\n\n\nNoisy Nets\n\nGets rid of the exploration factor in epsilon-greedy strategies.\nInstead it uses a noisy fully connected layer where the noise weights are learned by the network.\nAs the network becomes more accurate at predicting good Q-values, it automatically decreases the noise.\n\n\n\n\nThey combine all of the mentioned methods.\nThey use a KL term for weighting in Prioritized Replay (to account for the Distributional RL).\nTraining\n\nThey start training after 80k frames.\nThey use Adam.\nWhen using epsilon-greedy instead of noise nets, they anneal epsilon to 0.01 at 250k frames.\nFor multi-step learning they use n=3 future experiences.\n\n\n\n They use the following improvements:\n\nDouble Q-learning\n\nUses two networks during training.\nOne predicts Q-values, the other is updated.\nUsually done by using a copy with freezed parameters.\n\n\nPrioritized Replay\n\nSamples experiences from the replay memory based on the difference between predicted and real Q-values.\nRecent experiences also get higher priority.\n\n\nDueling Networks\n\nSplits the Q-value prediction into a value stream (mean value over all values) and an advantage stream (advantage of specific actions over others).\n\n\nMulti-Step Learning\n\nSplits direct reward + Q(next state, next action) into direct reward + direct reward of next N actions + Q(next Nth state, next Nth action) (weighted with discrount factor).\nI.e. per training example, some experiences from the future are directly used to compute the rewards and only after some point the Q-function is used.\n\n\nDistributional RL\n\nSeems to be nothing else but switching the regression (of Q-values) to classification in order to avoid standard mode problems with regression.\nThe range of possible reward values is partitioned into N bins.\n\n\nNoisy Nets\n\nGets rid of the exploration factor in epsilon-greedy strategies.\nInstead it uses a noisy fully connected layer where the noise weights are learned by the network.\nAs the network becomes more accurate at predicting good Q-values, it automatically decreases the noise.\n\n\n\n Double Q-learning\n\nUses two networks during training.\nOne predicts Q-values, the other is updated.\nUsually done by using a copy with freezed parameters.\n\n Uses two networks during training. One predicts Q-values, the other is updated. Usually done by using a copy with freezed parameters. Prioritized Replay\n\nSamples experiences from the replay memory based on the difference between predicted and real Q-values.\nRecent experiences also get higher priority.\n\n Samples experiences from the replay memory based on the difference between predicted and real Q-values. Recent experiences also get higher priority. Dueling Networks\n\nSplits the Q-value prediction into a value stream (mean value over all values) and an advantage stream (advantage of specific actions over others).\n\n Splits the Q-value prediction into a value stream (mean value over all values) and an advantage stream (advantage of specific actions over others). Multi-Step Learning\n\nSplits direct reward + Q(next state, next action) into direct reward + direct reward of next N actions + Q(next Nth state, next Nth action) (weighted with discrount factor).\nI.e. per training example, some experiences from the future are directly used to compute the rewards and only after some point the Q-function is used.\n\n Splits direct reward + Q(next state, next action) into direct reward + direct reward of next N actions + Q(next Nth state, next Nth action) (weighted with discrount factor). I.e. per training example, some experiences from the future are directly used to compute the rewards and only after some point the Q-function is used. Distributional RL\n\nSeems to be nothing else but switching the regression (of Q-values) to classification in order to avoid standard mode problems with regression.\nThe range of possible reward values is partitioned into N bins.\n\n Seems to be nothing else but switching the regression (of Q-values) to classification in order to avoid standard mode problems with regression. The range of possible reward values is partitioned into N bins. Noisy Nets\n\nGets rid of the exploration factor in epsilon-greedy strategies.\nInstead it uses a noisy fully connected layer where the noise weights are learned by the network.\nAs the network becomes more accurate at predicting good Q-values, it automatically decreases the noise.\n\n Gets rid of the exploration factor in epsilon-greedy strategies. Instead it uses a noisy fully connected layer where the noise weights are learned by the network. As the network becomes more accurate at predicting good Q-values, it automatically decreases the noise. They combine all of the mentioned methods. They use a KL term for weighting in Prioritized Replay (to account for the Distributional RL). Training\n\nThey start training after 80k frames.\nThey use Adam.\nWhen using epsilon-greedy instead of noise nets, they anneal epsilon to 0.01 at 250k frames.\nFor multi-step learning they use n=3 future experiences.\n\n They start training after 80k frames. They use Adam. When using epsilon-greedy instead of noise nets, they anneal epsilon to 0.01 at 250k frames. For multi-step learning they use n=3 future experiences. \nResults\n\nThey evaluate Rainbow 57 Atari games.\nRainbow beats all other methods used on their own, both in learning speed and maximum skill level.\nIt performs far better than the classic DQN approach.\nAverage performance:\n\n\n\n\nAblation\n\nRemoving the priority replay, multi-step learning or distributional RL significantly worsens the performance.\nRemoving noise nets also harms the performance, although a bit less.\nRemoving double Q-learning or dueling networks seems to have no significiant effect.\nVisualization:\n\n\n\n\n\n\nLearning curves by game:\n\n\n\n\n\n They evaluate Rainbow 57 Atari games. Rainbow beats all other methods used on their own, both in learning speed and maximum skill level. It performs far better than the classic DQN approach. Average performance:\n\n\n\n  Ablation\n\nRemoving the priority replay, multi-step learning or distributional RL significantly worsens the performance.\nRemoving noise nets also harms the performance, although a bit less.\nRemoving double Q-learning or dueling networks seems to have no significiant effect.\nVisualization:\n\n\n\n\n\n Removing the priority replay, multi-step learning or distributional RL significantly worsens the performance. Removing noise nets also harms the performance, although a bit less. Removing double Q-learning or dueling networks seems to have no significiant effect. Visualization:\n\n\n\n  Learning curves by game:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1710.02298"
    },
    "23": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Learning_to_Navigate_in_Complex_Environments.md",
        "transcript": "\nWhat\n\nThey describe a model based on the A3C algorithm.\nThe model is optimized so that it learns fast to navigate through mazes/games.\nMost of the modification is based on adding auxiliary losses (depth prediction and \"have I been here before?\").\n\n They describe a model based on the A3C algorithm. The model is optimized so that it learns fast to navigate through mazes/games. Most of the modification is based on adding auxiliary losses (depth prediction and \"have I been here before?\"). \nHow\n\nTheir model is based on the A3C algorithm and works mostly in the same way.\nThey describe four variations of the model:\n\nFF A3C: Standard A3C based agent with a simple CNN.\nLSTM A3C: Standard A3C based agent with a simple CNN+LSTM.\nNav A3C: Like \"LSTM A3C\", but uses two stacked LSTMs and gets three additional inputs: last received reward, last performed action, current velocity vector (lateral + rotation).\nNav A3C +D1D2L: Like \"Nav A3C\", but additionally predicts depth information and loop information (see below).\nVisualizations:\n\n\n\n\n\n\nAll of these models predict (at least) a policy (pi(a_t|s_t)) and value function (V(s_t)).\nDepth prediction\n\nThey let the model predict the depth map of the currently visible scene/screen.\nThey could also feed the depth map as an additional input into the model,\nbut argue that it helps more with understanding and learning the navigation task to let the model predict it.\nThey try prediction both via regression and classification.\n(Where classification uses non-uniform bins, so that there are more bins for far away depths.)\n\n\nLoop closure prediction\n\nThey let the model predict whether it\n\nhas been at (roughly) the current position t timesteps in the past\nAND also moved sufficiently far away from that position between now and t timesteps in the past (in order to not make the loss too simple)\n\n\n\n\n\n Their model is based on the A3C algorithm and works mostly in the same way. They describe four variations of the model:\n\nFF A3C: Standard A3C based agent with a simple CNN.\nLSTM A3C: Standard A3C based agent with a simple CNN+LSTM.\nNav A3C: Like \"LSTM A3C\", but uses two stacked LSTMs and gets three additional inputs: last received reward, last performed action, current velocity vector (lateral + rotation).\nNav A3C +D1D2L: Like \"Nav A3C\", but additionally predicts depth information and loop information (see below).\nVisualizations:\n\n\n\n\n\n FF A3C: Standard A3C based agent with a simple CNN. LSTM A3C: Standard A3C based agent with a simple CNN+LSTM. Nav A3C: Like \"LSTM A3C\", but uses two stacked LSTMs and gets three additional inputs: last received reward, last performed action, current velocity vector (lateral + rotation). Nav A3C +D1D2L: Like \"Nav A3C\", but additionally predicts depth information and loop information (see below). Visualizations:\n\n\n\n  All of these models predict (at least) a policy (pi(a_t|s_t)) and value function (V(s_t)). Depth prediction\n\nThey let the model predict the depth map of the currently visible scene/screen.\nThey could also feed the depth map as an additional input into the model,\nbut argue that it helps more with understanding and learning the navigation task to let the model predict it.\nThey try prediction both via regression and classification.\n(Where classification uses non-uniform bins, so that there are more bins for far away depths.)\n\n They let the model predict the depth map of the currently visible scene/screen. They could also feed the depth map as an additional input into the model,\nbut argue that it helps more with understanding and learning the navigation task to let the model predict it. They try prediction both via regression and classification.\n(Where classification uses non-uniform bins, so that there are more bins for far away depths.) Loop closure prediction\n\nThey let the model predict whether it\n\nhas been at (roughly) the current position t timesteps in the past\nAND also moved sufficiently far away from that position between now and t timesteps in the past (in order to not make the loss too simple)\n\n\n\n They let the model predict whether it\n\nhas been at (roughly) the current position t timesteps in the past\nAND also moved sufficiently far away from that position between now and t timesteps in the past (in order to not make the loss too simple)\n\n has been at (roughly) the current position t timesteps in the past AND also moved sufficiently far away from that position between now and t timesteps in the past (in order to not make the loss too simple) \nResults\n\nThey use a maze game from DeepMind Labs for training and testing.\nIn that maze, the agent can accelerate forward/backward/sideways/rotational.\nThe agent must find a goal within a short amount of time to gain 10 reward. Occasionally there are other reward objects giving +1 or +2.\nThey use the following maze types:\nStatic maze: Goal locations are always the same, but the agent is placed at a random location after finding them.\nDynamic maze / random goal maze: Goal locations vary per episode.\nI-Maze: Static maze layout. Goal is at a random one of four locations per episode.\nObservations:\n\nPredicting depth via regression works significantly worse than via classification.\nFF A3C (raw feed-forward CNN without RNN-memory) can sometimes still navigate quite well.\nMemory is apparently not always required for navigation, despite ambiguity.\nAdding the last action, last reward and velocity as an input seems to usually help the agent, but not always.\nAdding the auxiliary losses helps significantly with learning.\n\n\nLearning curves:\n\n\nLegend: -L = with loop closure prediction, -D1 = with depth prediction after first LSTM layer, -D2 = with depth prediction after second LSTM layer.\n\n\n\n They use a maze game from DeepMind Labs for training and testing. In that maze, the agent can accelerate forward/backward/sideways/rotational. The agent must find a goal within a short amount of time to gain 10 reward. Occasionally there are other reward objects giving +1 or +2. They use the following maze types: Static maze: Goal locations are always the same, but the agent is placed at a random location after finding them. Dynamic maze / random goal maze: Goal locations vary per episode. I-Maze: Static maze layout. Goal is at a random one of four locations per episode. Observations:\n\nPredicting depth via regression works significantly worse than via classification.\nFF A3C (raw feed-forward CNN without RNN-memory) can sometimes still navigate quite well.\nMemory is apparently not always required for navigation, despite ambiguity.\nAdding the last action, last reward and velocity as an input seems to usually help the agent, but not always.\nAdding the auxiliary losses helps significantly with learning.\n\n Predicting depth via regression works significantly worse than via classification. FF A3C (raw feed-forward CNN without RNN-memory) can sometimes still navigate quite well.\nMemory is apparently not always required for navigation, despite ambiguity. Adding the last action, last reward and velocity as an input seems to usually help the agent, but not always. Adding the auxiliary losses helps significantly with learning. Learning curves:\n\n\nLegend: -L = with loop closure prediction, -D1 = with depth prediction after first LSTM layer, -D2 = with depth prediction after second LSTM layer.\n\n  Legend: -L = with loop closure prediction, -D1 = with depth prediction after first LSTM layer, -D2 = with depth prediction after second LSTM layer. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1611.03673"
    },
    "24": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Unsupervised_Image-to-Image_Translation_Networks.md",
        "transcript": "\nWhat\n\nThey present a method to learn mapping functions that transform images from one style to another style. (E.g. photos from daylight to nighttime.)\nTheir method only requires example images for both styles (i.e. class labels per image).\n\n They present a method to learn mapping functions that transform images from one style to another style. (E.g. photos from daylight to nighttime.) Their method only requires example images for both styles (i.e. class labels per image). \nHow\n\nArchitecture\n\nTheir method is based on VAEs (i.e. autoencoders) and GANs.\nTheir architecture is kinda similar to an autoencoder.\nFor an image style A, an encoder E first transform an image to a vector representation z.\nThen a generator G transforms z into an image.\nThere are two encoders (E1, E2), one per image style (A, B).\nThere are two generators (G1, G2), one per image style (A, B).\nThere are two discriminators (D1, D2), one per generator (and therefore style).\nAn image can be changed in style from A to B using e.g. G2(E1(I_A)).\nThe weights of the encoders are mostly tied/shared. Only the last layers are not-shared.\nThe weights of the generators are mostly tied/shared. Only the last layers are not-shared.\nThey use 3 convs + 4 residual blocks for the encoders and 4 residual blocks + 3 transposes convs for the generators.\nThey use normal convs for the discriminators. Nonlinearities are LeakyReLUs.\nThe encoders are VAEs and trained with common VAE-losses (i.e. lower bound optimization).\nHowever, they only predict mean values per component in z, not variances.\nThe variances are all 1.\nVisualization of the architecture:\n\n\n\n\n\n\nLoss\n\nTheir loss consists of three components:\n\nVAE-loss: Reconstruction loss (absolute distance) and KL term on z (to keep it close to the standard normal distribution).\nMost weight is put on the reconstruction loss.\nGAN-loss: Standard as in other GANs, i.e. cross-entropy.\nCycle-Consistency-loss: For an image I_A, it is expected to look the same after switching back and forth between image styles, i.e.\nI_A = G1(E2( G2(E1(I_A)) )) (switch from style A to B, then from B to A).\nThe cycle consistency loss uses a reconstruction loss and two KL-terms (one for the first E(.) and one for the second).\n\n\n\n\n\n Architecture\n\nTheir method is based on VAEs (i.e. autoencoders) and GANs.\nTheir architecture is kinda similar to an autoencoder.\nFor an image style A, an encoder E first transform an image to a vector representation z.\nThen a generator G transforms z into an image.\nThere are two encoders (E1, E2), one per image style (A, B).\nThere are two generators (G1, G2), one per image style (A, B).\nThere are two discriminators (D1, D2), one per generator (and therefore style).\nAn image can be changed in style from A to B using e.g. G2(E1(I_A)).\nThe weights of the encoders are mostly tied/shared. Only the last layers are not-shared.\nThe weights of the generators are mostly tied/shared. Only the last layers are not-shared.\nThey use 3 convs + 4 residual blocks for the encoders and 4 residual blocks + 3 transposes convs for the generators.\nThey use normal convs for the discriminators. Nonlinearities are LeakyReLUs.\nThe encoders are VAEs and trained with common VAE-losses (i.e. lower bound optimization).\nHowever, they only predict mean values per component in z, not variances.\nThe variances are all 1.\nVisualization of the architecture:\n\n\n\n\n\n Their method is based on VAEs (i.e. autoencoders) and GANs. Their architecture is kinda similar to an autoencoder. For an image style A, an encoder E first transform an image to a vector representation z.\nThen a generator G transforms z into an image. There are two encoders (E1, E2), one per image style (A, B). There are two generators (G1, G2), one per image style (A, B). There are two discriminators (D1, D2), one per generator (and therefore style). An image can be changed in style from A to B using e.g. G2(E1(I_A)). The weights of the encoders are mostly tied/shared. Only the last layers are not-shared. The weights of the generators are mostly tied/shared. Only the last layers are not-shared. They use 3 convs + 4 residual blocks for the encoders and 4 residual blocks + 3 transposes convs for the generators.\nThey use normal convs for the discriminators. Nonlinearities are LeakyReLUs. The encoders are VAEs and trained with common VAE-losses (i.e. lower bound optimization).\nHowever, they only predict mean values per component in z, not variances.\nThe variances are all 1. Visualization of the architecture:\n\n\n\n  Loss\n\nTheir loss consists of three components:\n\nVAE-loss: Reconstruction loss (absolute distance) and KL term on z (to keep it close to the standard normal distribution).\nMost weight is put on the reconstruction loss.\nGAN-loss: Standard as in other GANs, i.e. cross-entropy.\nCycle-Consistency-loss: For an image I_A, it is expected to look the same after switching back and forth between image styles, i.e.\nI_A = G1(E2( G2(E1(I_A)) )) (switch from style A to B, then from B to A).\nThe cycle consistency loss uses a reconstruction loss and two KL-terms (one for the first E(.) and one for the second).\n\n\n\n Their loss consists of three components:\n\nVAE-loss: Reconstruction loss (absolute distance) and KL term on z (to keep it close to the standard normal distribution).\nMost weight is put on the reconstruction loss.\nGAN-loss: Standard as in other GANs, i.e. cross-entropy.\nCycle-Consistency-loss: For an image I_A, it is expected to look the same after switching back and forth between image styles, i.e.\nI_A = G1(E2( G2(E1(I_A)) )) (switch from style A to B, then from B to A).\nThe cycle consistency loss uses a reconstruction loss and two KL-terms (one for the first E(.) and one for the second).\n\n VAE-loss: Reconstruction loss (absolute distance) and KL term on z (to keep it close to the standard normal distribution).\nMost weight is put on the reconstruction loss. GAN-loss: Standard as in other GANs, i.e. cross-entropy. Cycle-Consistency-loss: For an image I_A, it is expected to look the same after switching back and forth between image styles, i.e.\nI_A = G1(E2( G2(E1(I_A)) )) (switch from style A to B, then from B to A).\nThe cycle consistency loss uses a reconstruction loss and two KL-terms (one for the first E(.) and one for the second). \nResults\n\nWhen testing on the (satellite) map dataset:\n\nWeight sharing between encoders and between generators improved accuracy.\nThe cycle consistency loss improved accuracy.\nUsing 4-6 layers (as opposed to just 3) in the discriminator improved accuracy.\n\n\nTranslations that added details (e.g. night to day) were harder for the model.\nAfter training, the features from each discriminator seem to be quite good for the respective dataset (i.e. unsupervised learned features).\nExample translations:\n\n\n\n\n\n When testing on the (satellite) map dataset:\n\nWeight sharing between encoders and between generators improved accuracy.\nThe cycle consistency loss improved accuracy.\nUsing 4-6 layers (as opposed to just 3) in the discriminator improved accuracy.\n\n Weight sharing between encoders and between generators improved accuracy. The cycle consistency loss improved accuracy. Using 4-6 layers (as opposed to just 3) in the discriminator improved accuracy. Translations that added details (e.g. night to day) were harder for the model. After training, the features from each discriminator seem to be quite good for the respective dataset (i.e. unsupervised learned features). Example translations:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.00848"
    },
    "25": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Dilated_Recurrent_Neural_Networks.md",
        "transcript": "\nWhat\n\nThey add a dilation factor to recurrent neural networks (e.g. LSTMs), similar to dilation in convolutions.\nThis enables learning of long-term dependencies, prevents vanishing/exploding gradients and makes the networks sometimes more parallelizable.\n\n They add a dilation factor to recurrent neural networks (e.g. LSTMs), similar to dilation in convolutions. This enables learning of long-term dependencies, prevents vanishing/exploding gradients and makes the networks sometimes more parallelizable. \nHow\n\nDilation\n\nDilation d here simply means, that each timestep gets input from the d-th previous timestep.\nWith d=1, this is identical to a normal reccurent network. With d=2 there is a gap of 1 between each timestep.\nThey suggest to let the dilation exponentially increase per layer, e.g. 1, 2, 4, ...\nVisualization:\n\n\n\n\nUsing dilation can make it possible to execute some steps of the RNN in parallel.\nThe following visualization shows that:\n\n\n\n\nThe dilation may start at a value higher than 1.\nThis however should be compensated before generating the output vector.\nTo do that, output of the last layer at timestep t and t-1 has to be used. Visualization:\n\n\n\n\n\n\nMemory capacity\n\nThey show that the memory capacity of dilated RNNs is better than in skip RNNs, i.e. the average path length in the network is shorter.\n\n\n\n Dilation\n\nDilation d here simply means, that each timestep gets input from the d-th previous timestep.\nWith d=1, this is identical to a normal reccurent network. With d=2 there is a gap of 1 between each timestep.\nThey suggest to let the dilation exponentially increase per layer, e.g. 1, 2, 4, ...\nVisualization:\n\n\n\n\nUsing dilation can make it possible to execute some steps of the RNN in parallel.\nThe following visualization shows that:\n\n\n\n\nThe dilation may start at a value higher than 1.\nThis however should be compensated before generating the output vector.\nTo do that, output of the last layer at timestep t and t-1 has to be used. Visualization:\n\n\n\n\n\n Dilation d here simply means, that each timestep gets input from the d-th previous timestep. With d=1, this is identical to a normal reccurent network. With d=2 there is a gap of 1 between each timestep. They suggest to let the dilation exponentially increase per layer, e.g. 1, 2, 4, ... Visualization:\n\n\n\n  Using dilation can make it possible to execute some steps of the RNN in parallel.\nThe following visualization shows that:\n\n\n\n  The dilation may start at a value higher than 1.\nThis however should be compensated before generating the output vector.\nTo do that, output of the last layer at timestep t and t-1 has to be used. Visualization:\n\n\n\n  Memory capacity\n\nThey show that the memory capacity of dilated RNNs is better than in skip RNNs, i.e. the average path length in the network is shorter.\n\n They show that the memory capacity of dilated RNNs is better than in skip RNNs, i.e. the average path length in the network is shorter. \nResults\n\nCopy-Task\n\nThis task involves copying of inputs.\nI.e. the network gets some integers at the start, then T={500, 1000} timesteps pass, then it has to output the input values.\nThey use 9 layers with a dilation of up to 256. (This means that it is really almost raw copying of data for the network.)\nDilated RNNs perform best here, followed by dilated GRUs and dilated LSTMs.\nAll non-dilated networks resort to random guessing (i.e. fail to learn anything).\n\n\nMNIST\n\nThey predict classes on MNIST, where each image is turned into a 784-element vector.\nAll networks, including competitors, can handle that task.\nThey make the task harder by padding the vectors to 1000 and 2000 elements length.\nThen only their dilated networks, dilated (1d-)CNNs and RNNs with skip connections can handle the task.\nTheir dilated RNNs learn faster the more layers they have.\nWith 2 layers they learn nothing.\nWith few layers they can sometimes also have major swings in accuracy during training.\n\n\n\n\nWhen increasing the minimum dilation, they find that they can drop layers and still achieve almost the same accuracy,\nleading to much faster training (wall-clock time).\nTraining with minimum dilation 2 leads to same accuracy at half the training time.\n\n\nLanguage modelling\n\nThey test their models on Penn Treebank character predictions.\nTheir models get beaten by LayerNorm HM-LSTM, HyperNetworks, Zoneout.\nThey argue though that their models achieve the highest scores among models that do not use any normalization.\n\n\nSpeaker identification from raw waveform\n\nThey train on VCTK.\nTheir dilated models achieve much higher accuracy than non-dilated ones and can compete with models using MFCC features.\n\n\n\n Copy-Task\n\nThis task involves copying of inputs.\nI.e. the network gets some integers at the start, then T={500, 1000} timesteps pass, then it has to output the input values.\nThey use 9 layers with a dilation of up to 256. (This means that it is really almost raw copying of data for the network.)\nDilated RNNs perform best here, followed by dilated GRUs and dilated LSTMs.\nAll non-dilated networks resort to random guessing (i.e. fail to learn anything).\n\n This task involves copying of inputs. I.e. the network gets some integers at the start, then T={500, 1000} timesteps pass, then it has to output the input values. They use 9 layers with a dilation of up to 256. (This means that it is really almost raw copying of data for the network.) Dilated RNNs perform best here, followed by dilated GRUs and dilated LSTMs. All non-dilated networks resort to random guessing (i.e. fail to learn anything). MNIST\n\nThey predict classes on MNIST, where each image is turned into a 784-element vector.\nAll networks, including competitors, can handle that task.\nThey make the task harder by padding the vectors to 1000 and 2000 elements length.\nThen only their dilated networks, dilated (1d-)CNNs and RNNs with skip connections can handle the task.\nTheir dilated RNNs learn faster the more layers they have.\nWith 2 layers they learn nothing.\nWith few layers they can sometimes also have major swings in accuracy during training.\n\n\n\n\nWhen increasing the minimum dilation, they find that they can drop layers and still achieve almost the same accuracy,\nleading to much faster training (wall-clock time).\nTraining with minimum dilation 2 leads to same accuracy at half the training time.\n\n They predict classes on MNIST, where each image is turned into a 784-element vector. All networks, including competitors, can handle that task. They make the task harder by padding the vectors to 1000 and 2000 elements length.\nThen only their dilated networks, dilated (1d-)CNNs and RNNs with skip connections can handle the task. Their dilated RNNs learn faster the more layers they have.\nWith 2 layers they learn nothing.\nWith few layers they can sometimes also have major swings in accuracy during training.\n\n\n\n  When increasing the minimum dilation, they find that they can drop layers and still achieve almost the same accuracy,\nleading to much faster training (wall-clock time).\nTraining with minimum dilation 2 leads to same accuracy at half the training time. Language modelling\n\nThey test their models on Penn Treebank character predictions.\nTheir models get beaten by LayerNorm HM-LSTM, HyperNetworks, Zoneout.\nThey argue though that their models achieve the highest scores among models that do not use any normalization.\n\n They test their models on Penn Treebank character predictions. Their models get beaten by LayerNorm HM-LSTM, HyperNetworks, Zoneout. They argue though that their models achieve the highest scores among models that do not use any normalization. Speaker identification from raw waveform\n\nThey train on VCTK.\nTheir dilated models achieve much higher accuracy than non-dilated ones and can compete with models using MFCC features.\n\n They train on VCTK. Their dilated models achieve much higher accuracy than non-dilated ones and can compete with models using MFCC features. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1710.02224"
    },
    "26": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Detect_to_Track_and_Track_to_Detect.md",
        "transcript": "\nWhat\n\nThey suggest a variation of Faster R-CNN architectures that also tracks objects in videos\n(additionally to detecting them, i.e. additionally to bounding box detection).\nThe model does detection and tracking in one forward pass.\nThe \"tracking\" here is rather primitive: It only signals where an object frame t will likely be in frame t+x.\nThat information can then be used to compute chains of bounding boxes over time.\n\n They suggest a variation of Faster R-CNN architectures that also tracks objects in videos\n(additionally to detecting them, i.e. additionally to bounding box detection). The model does detection and tracking in one forward pass. The \"tracking\" here is rather primitive: It only signals where an object frame t will likely be in frame t+x.\nThat information can then be used to compute chains of bounding boxes over time. \nHow\n\nArchitecture\n\nThey base their model on R-FCN, which is similar to Faster R-CNN.\nThey have two images/frames (t and t+x) as inputs.\nSame as in Faster R-CNN:\n\nThey apply a base network (e.g. ResNet) to both of them.\nThey use an RPN to locate RoIs in the feature maps.\nThey use RoI-Pooling to extract and pool each RoI's features.\nThey apply classification (object class) and regression (object location/dimensions) to each RoI.\n\n\nThey compute correlations between the feature maps of both frames.\nThey then extract and pool each RoI from the feature and correlation maps of from t.\nThey then apply a tracking branch, which predicts the new position (x, y) and dimensions (height, width) of the RoI in frame t+x.\nThat prediction can be used to find a matching bounding box in frame t+x, which again can be used to track objects over time.\nDuring training, they use a smooth L1 loss for the tracking branch.\nVisualization of the architecture:\n\n\n\n\nVisualization of the tracking process:\n\n\n\n\n\n\nCorrelation\n\nCorrelations are estimated between the feature maps of frame t and t+x.\nIn the simplest form, correlation is measured by extracting some point (i,j) from the feature maps t and t+x (resulting in two vectors)\nand then computing the scalar product (resulting in a scalar).\nThey use a more complex correlation, which compares point (i,j) in frame t not only to (i,j) in t+x, but to (i+d,j+q),\nwhere d and q come from an interval. I.e. they compare to a neighbourhood in frame t+x.\nThey use d=8 (same for q), resulting in 64 correlation values per (i,j).\nFurthermore, they compute correlations for multiple scales (i.e. early and late in the base network).\n(With striding so that the correlation maps end up having the same sizes.)\n\n\nTubelets\n\nThe previous architecture only regresses the new location of a bounding box in frame t+x.\nFrom that information they derive tubelets, tracks of bounding boxes over multiple frames (i.e. objects tracked over time).\nCore of the method to do that are scores between pairs of bounding boxes (between frame t and t+x).\nThe scores are computed per class, i.e. no score between two bounding boxes of different classes.\nEach score has three components:\n\n(a) Probability of the bounding box in frame t having the specified class,\n(b) Probability of the bounding box in frame t+x having the specified class,\n(c) A flag whether the bounding box in frame t+x matches the expected future position of the bounding box in frame t.\nThe future position is estimated using the new tracking branch.\nThe flag has a value of 1 if the IoU between future bounding box and expectation is >0.5.\n\n\nOnce all scores are computed, the Viterbi algorithm can be used to compute optimal paths over the frames, leading to tracked objects,\nwhich are the tubelets.\nAfter generating a tubelet, they increase the object detection scores of the respective class of all bounding boxes in the tubelet.\n(Because future frames are now giving support that a bounding box really does have a specific object class.)\n\n\n\n Architecture\n\nThey base their model on R-FCN, which is similar to Faster R-CNN.\nThey have two images/frames (t and t+x) as inputs.\nSame as in Faster R-CNN:\n\nThey apply a base network (e.g. ResNet) to both of them.\nThey use an RPN to locate RoIs in the feature maps.\nThey use RoI-Pooling to extract and pool each RoI's features.\nThey apply classification (object class) and regression (object location/dimensions) to each RoI.\n\n\nThey compute correlations between the feature maps of both frames.\nThey then extract and pool each RoI from the feature and correlation maps of from t.\nThey then apply a tracking branch, which predicts the new position (x, y) and dimensions (height, width) of the RoI in frame t+x.\nThat prediction can be used to find a matching bounding box in frame t+x, which again can be used to track objects over time.\nDuring training, they use a smooth L1 loss for the tracking branch.\nVisualization of the architecture:\n\n\n\n\nVisualization of the tracking process:\n\n\n\n\n\n They base their model on R-FCN, which is similar to Faster R-CNN. They have two images/frames (t and t+x) as inputs. Same as in Faster R-CNN:\n\nThey apply a base network (e.g. ResNet) to both of them.\nThey use an RPN to locate RoIs in the feature maps.\nThey use RoI-Pooling to extract and pool each RoI's features.\nThey apply classification (object class) and regression (object location/dimensions) to each RoI.\n\n They apply a base network (e.g. ResNet) to both of them. They use an RPN to locate RoIs in the feature maps. They use RoI-Pooling to extract and pool each RoI's features. They apply classification (object class) and regression (object location/dimensions) to each RoI. They compute correlations between the feature maps of both frames. They then extract and pool each RoI from the feature and correlation maps of from t. They then apply a tracking branch, which predicts the new position (x, y) and dimensions (height, width) of the RoI in frame t+x. That prediction can be used to find a matching bounding box in frame t+x, which again can be used to track objects over time. During training, they use a smooth L1 loss for the tracking branch. Visualization of the architecture:\n\n\n\n  Visualization of the tracking process:\n\n\n\n  Correlation\n\nCorrelations are estimated between the feature maps of frame t and t+x.\nIn the simplest form, correlation is measured by extracting some point (i,j) from the feature maps t and t+x (resulting in two vectors)\nand then computing the scalar product (resulting in a scalar).\nThey use a more complex correlation, which compares point (i,j) in frame t not only to (i,j) in t+x, but to (i+d,j+q),\nwhere d and q come from an interval. I.e. they compare to a neighbourhood in frame t+x.\nThey use d=8 (same for q), resulting in 64 correlation values per (i,j).\nFurthermore, they compute correlations for multiple scales (i.e. early and late in the base network).\n(With striding so that the correlation maps end up having the same sizes.)\n\n Correlations are estimated between the feature maps of frame t and t+x. In the simplest form, correlation is measured by extracting some point (i,j) from the feature maps t and t+x (resulting in two vectors)\nand then computing the scalar product (resulting in a scalar). They use a more complex correlation, which compares point (i,j) in frame t not only to (i,j) in t+x, but to (i+d,j+q),\nwhere d and q come from an interval. I.e. they compare to a neighbourhood in frame t+x. They use d=8 (same for q), resulting in 64 correlation values per (i,j). Furthermore, they compute correlations for multiple scales (i.e. early and late in the base network).\n(With striding so that the correlation maps end up having the same sizes.) Tubelets\n\nThe previous architecture only regresses the new location of a bounding box in frame t+x.\nFrom that information they derive tubelets, tracks of bounding boxes over multiple frames (i.e. objects tracked over time).\nCore of the method to do that are scores between pairs of bounding boxes (between frame t and t+x).\nThe scores are computed per class, i.e. no score between two bounding boxes of different classes.\nEach score has three components:\n\n(a) Probability of the bounding box in frame t having the specified class,\n(b) Probability of the bounding box in frame t+x having the specified class,\n(c) A flag whether the bounding box in frame t+x matches the expected future position of the bounding box in frame t.\nThe future position is estimated using the new tracking branch.\nThe flag has a value of 1 if the IoU between future bounding box and expectation is >0.5.\n\n\nOnce all scores are computed, the Viterbi algorithm can be used to compute optimal paths over the frames, leading to tracked objects,\nwhich are the tubelets.\nAfter generating a tubelet, they increase the object detection scores of the respective class of all bounding boxes in the tubelet.\n(Because future frames are now giving support that a bounding box really does have a specific object class.)\n\n The previous architecture only regresses the new location of a bounding box in frame t+x. From that information they derive tubelets, tracks of bounding boxes over multiple frames (i.e. objects tracked over time). Core of the method to do that are scores between pairs of bounding boxes (between frame t and t+x). The scores are computed per class, i.e. no score between two bounding boxes of different classes. Each score has three components:\n\n(a) Probability of the bounding box in frame t having the specified class,\n(b) Probability of the bounding box in frame t+x having the specified class,\n(c) A flag whether the bounding box in frame t+x matches the expected future position of the bounding box in frame t.\nThe future position is estimated using the new tracking branch.\nThe flag has a value of 1 if the IoU between future bounding box and expectation is >0.5.\n\n (a) Probability of the bounding box in frame t having the specified class, (b) Probability of the bounding box in frame t+x having the specified class, (c) A flag whether the bounding box in frame t+x matches the expected future position of the bounding box in frame t.\nThe future position is estimated using the new tracking branch.\nThe flag has a value of 1 if the IoU between future bounding box and expectation is >0.5. Once all scores are computed, the Viterbi algorithm can be used to compute optimal paths over the frames, leading to tracked objects,\nwhich are the tubelets. After generating a tubelet, they increase the object detection scores of the respective class of all bounding boxes in the tubelet.\n(Because future frames are now giving support that a bounding box really does have a specific object class.) \nResults\n\nThey train on ImageNet VID dataset (after first training on standard ImageNet).\nTraining with the tracking branch improves raw object detection by about 1.6 points mAP.\nPredicting on two frames (e.g. t and t+1) with tracking improves object detection scores for some classes significantly (e.g. +9 points for rabbits).\nAdding a significant gap between the frames (t and t+10) is still enough to improve object detection scores (a bit less though).\nResults overview:\n\n\n\n\nTheir tracking branch barely adds runtime to the model. Only the tubelet generation adds significant time.\nThey modify it (somehow, not described) to a non-causal(?) version, which runs faster.\nAdditional runtime per frame with tracking is then 14ms.\n\n They train on ImageNet VID dataset (after first training on standard ImageNet). Training with the tracking branch improves raw object detection by about 1.6 points mAP. Predicting on two frames (e.g. t and t+1) with tracking improves object detection scores for some classes significantly (e.g. +9 points for rabbits). Adding a significant gap between the frames (t and t+10) is still enough to improve object detection scores (a bit less though). Results overview:\n\n\n\n  Their tracking branch barely adds runtime to the model. Only the tubelet generation adds significant time.\nThey modify it (somehow, not described) to a non-causal(?) version, which runs faster.\nAdditional runtime per frame with tracking is then 14ms. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1710.03958"
    },
    "27": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Dilated_Residual_Networks.md",
        "transcript": "\nWhat\n\nThey change standard ResNets to contain higher resolution feature maps (i.e. more height/width) and to use dilated convolutions.\nThis makes them more useful for e.g. segmentation.\nThey identify a gridding related problem and how to fix it.\n\n They change standard ResNets to contain higher resolution feature maps (i.e. more height/width) and to use dilated convolutions. This makes them more useful for e.g. segmentation. They identify a gridding related problem and how to fix it. \nHow\n\nChanges to ResNet\n\nResNets are organized in five blocks of (each multiple) residual convolutions.\nThey increase the feature map resolutions of the fourth and fifth block (to 2x and 4x).\nThey increase simultanously the dilation of each convolution in the fourth and fifth block (to dilation 2 and 4).\nThis is a common practice and known as the \"\u00e1 trous trick\".\n\n\nDegridding\n\nWhen using dilated convolutions one can end up with grid-like patterns in the generated feature maps.\nThis happens when the image has higher-frequency content than the sampling of the dilated convolution.\nThe below image shows an example for a convolution with dilation 2 and equal weights for all 3x3 parameters:\n\n\n\n\nThese grid-like patterns make the network less useful for segmentation tasks.\nThey fix the gridding problems with two steps:\n\nThey remove the max pooling at the start of the network and replace it with convolutions.\nThis is done, because max pooling led to high-frequency high-amplitude components in their tests.\nThey add convolutions with less or no dilation after the dilated convolutions (i.e. at the end of the network).\nThese \"smoothen\" the feature maps, thereby fixing the grid-like patterns.\n\n\nThey get three networks:\n\nDRN-A-18: ResNet with 18 layers and dilation (2 in block 4 and 4 in block 5).\nDRN-B-26: Like DRN-A-18, but max pooling is replaced by four residual convolutions (in two blocks, each two convs).\nThey also add four residual convolutions at the end of the network (in two blocks, each two convs).\nDRN-C-26: Like DRN-B-26, but the four convolutions at the end of the network are not residual.\n\n\nThe motivation to use non-residual convolutions in DRN-C-26 is that residual ones can propagate the problems (grid-like patterns) more easily to the output.\nVisualization of the architectures:\n\n\n\n\nVisualization of the effect of using max pooling (DRN-A-XX) and replacing it with convolutions (DRN-B-XX):\n\n\n\n\n\n\n\n Changes to ResNet\n\nResNets are organized in five blocks of (each multiple) residual convolutions.\nThey increase the feature map resolutions of the fourth and fifth block (to 2x and 4x).\nThey increase simultanously the dilation of each convolution in the fourth and fifth block (to dilation 2 and 4).\nThis is a common practice and known as the \"\u00e1 trous trick\".\n\n ResNets are organized in five blocks of (each multiple) residual convolutions. They increase the feature map resolutions of the fourth and fifth block (to 2x and 4x). They increase simultanously the dilation of each convolution in the fourth and fifth block (to dilation 2 and 4). This is a common practice and known as the \"\u00e1 trous trick\". Degridding\n\nWhen using dilated convolutions one can end up with grid-like patterns in the generated feature maps.\nThis happens when the image has higher-frequency content than the sampling of the dilated convolution.\nThe below image shows an example for a convolution with dilation 2 and equal weights for all 3x3 parameters:\n\n\n\n\nThese grid-like patterns make the network less useful for segmentation tasks.\nThey fix the gridding problems with two steps:\n\nThey remove the max pooling at the start of the network and replace it with convolutions.\nThis is done, because max pooling led to high-frequency high-amplitude components in their tests.\nThey add convolutions with less or no dilation after the dilated convolutions (i.e. at the end of the network).\nThese \"smoothen\" the feature maps, thereby fixing the grid-like patterns.\n\n\nThey get three networks:\n\nDRN-A-18: ResNet with 18 layers and dilation (2 in block 4 and 4 in block 5).\nDRN-B-26: Like DRN-A-18, but max pooling is replaced by four residual convolutions (in two blocks, each two convs).\nThey also add four residual convolutions at the end of the network (in two blocks, each two convs).\nDRN-C-26: Like DRN-B-26, but the four convolutions at the end of the network are not residual.\n\n\nThe motivation to use non-residual convolutions in DRN-C-26 is that residual ones can propagate the problems (grid-like patterns) more easily to the output.\nVisualization of the architectures:\n\n\n\n\nVisualization of the effect of using max pooling (DRN-A-XX) and replacing it with convolutions (DRN-B-XX):\n\n\n\n\n\n When using dilated convolutions one can end up with grid-like patterns in the generated feature maps. This happens when the image has higher-frequency content than the sampling of the dilated convolution. The below image shows an example for a convolution with dilation 2 and equal weights for all 3x3 parameters:\n\n\n\n  These grid-like patterns make the network less useful for segmentation tasks. They fix the gridding problems with two steps:\n\nThey remove the max pooling at the start of the network and replace it with convolutions.\nThis is done, because max pooling led to high-frequency high-amplitude components in their tests.\nThey add convolutions with less or no dilation after the dilated convolutions (i.e. at the end of the network).\nThese \"smoothen\" the feature maps, thereby fixing the grid-like patterns.\n\n They remove the max pooling at the start of the network and replace it with convolutions.\nThis is done, because max pooling led to high-frequency high-amplitude components in their tests. They add convolutions with less or no dilation after the dilated convolutions (i.e. at the end of the network).\nThese \"smoothen\" the feature maps, thereby fixing the grid-like patterns. They get three networks:\n\nDRN-A-18: ResNet with 18 layers and dilation (2 in block 4 and 4 in block 5).\nDRN-B-26: Like DRN-A-18, but max pooling is replaced by four residual convolutions (in two blocks, each two convs).\nThey also add four residual convolutions at the end of the network (in two blocks, each two convs).\nDRN-C-26: Like DRN-B-26, but the four convolutions at the end of the network are not residual.\n\n DRN-A-18: ResNet with 18 layers and dilation (2 in block 4 and 4 in block 5). DRN-B-26: Like DRN-A-18, but max pooling is replaced by four residual convolutions (in two blocks, each two convs).\nThey also add four residual convolutions at the end of the network (in two blocks, each two convs). DRN-C-26: Like DRN-B-26, but the four convolutions at the end of the network are not residual. The motivation to use non-residual convolutions in DRN-C-26 is that residual ones can propagate the problems (grid-like patterns) more easily to the output. Visualization of the architectures:\n\n\n\n  Visualization of the effect of using max pooling (DRN-A-XX) and replacing it with convolutions (DRN-B-XX):\n\n\n\n  \nResults\n\nExample visualizations of the generated feature maps by each network:\n\n\n\n\nImageNet classification\n\nTheir networks perform quite a bit better than ResNets of comparable size.\nThe ranking of their networks seems to be: 1. DRN-C-XX, 2. DRN-B-XX, 3. DRN-C-XX.\nThe difference between B and C isn't that big, but between A and B it is.\nTo a degree this is expected, as B and C have four more convolutions than A (due to max pooling being replaced by convs).\nThe effect though seems to be a bit stronger than just that.\nTheir DRN-C-42 network is only a bit less accurate than ResNet-101.\n\n\nObject Localization\n\nThey suggest a method for bounding box detection without explicit training for that (i.e. when only training on ImageNet for classification).\nSounds like they just \"try\" every possible bounding box within a range of heights/widths at every location in the final feature map.\nThen they pick the one with highest activation, if it is above a threshold.\nWhen applying that method to their networks and ResNets they get significantly better results with their networks.\nNot that surprising, considering their final feature map resolutions are four times higher than in ResNet.\n\n\nSemantic Segmentation\n\nThey apply their models to the Cityscapes dataset.\nThey get a 70.9 mean IoU for DRN-C-42, while the reported best value for ResNet-101 is 66.6.\n\n\nNo information regarding runtimes in the paper.\nDilated ResNets are most likely going to run slower as they work with higher resolution feature maps.\nThey will also require more RAM.\n\n Example visualizations of the generated feature maps by each network:\n\n\n\n  ImageNet classification\n\nTheir networks perform quite a bit better than ResNets of comparable size.\nThe ranking of their networks seems to be: 1. DRN-C-XX, 2. DRN-B-XX, 3. DRN-C-XX.\nThe difference between B and C isn't that big, but between A and B it is.\nTo a degree this is expected, as B and C have four more convolutions than A (due to max pooling being replaced by convs).\nThe effect though seems to be a bit stronger than just that.\nTheir DRN-C-42 network is only a bit less accurate than ResNet-101.\n\n Their networks perform quite a bit better than ResNets of comparable size. The ranking of their networks seems to be: 1. DRN-C-XX, 2. DRN-B-XX, 3. DRN-C-XX.\nThe difference between B and C isn't that big, but between A and B it is.\nTo a degree this is expected, as B and C have four more convolutions than A (due to max pooling being replaced by convs).\nThe effect though seems to be a bit stronger than just that. Their DRN-C-42 network is only a bit less accurate than ResNet-101. Object Localization\n\nThey suggest a method for bounding box detection without explicit training for that (i.e. when only training on ImageNet for classification).\nSounds like they just \"try\" every possible bounding box within a range of heights/widths at every location in the final feature map.\nThen they pick the one with highest activation, if it is above a threshold.\nWhen applying that method to their networks and ResNets they get significantly better results with their networks.\nNot that surprising, considering their final feature map resolutions are four times higher than in ResNet.\n\n They suggest a method for bounding box detection without explicit training for that (i.e. when only training on ImageNet for classification). Sounds like they just \"try\" every possible bounding box within a range of heights/widths at every location in the final feature map.\nThen they pick the one with highest activation, if it is above a threshold. When applying that method to their networks and ResNets they get significantly better results with their networks.\nNot that surprising, considering their final feature map resolutions are four times higher than in ResNet. Semantic Segmentation\n\nThey apply their models to the Cityscapes dataset.\nThey get a 70.9 mean IoU for DRN-C-42, while the reported best value for ResNet-101 is 66.6.\n\n They apply their models to the Cityscapes dataset. They get a 70.9 mean IoU for DRN-C-42, while the reported best value for ResNet-101 is 66.6. No information regarding runtimes in the paper.\nDilated ResNets are most likely going to run slower as they work with higher resolution feature maps.\nThey will also require more RAM. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.09914"
    },
    "28": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Feature_Pyramid_Networks_for_Object_Detection.md",
        "transcript": "\nWhat\n\nThey suggest a modified network architecture for object detectors (i.e. bounding box detectors).\nThe architecture aggregates features from many scales (i.e. before each pooling layer) to detect both small and large object.\nThe network is shaped similar to an hourglass.\n\n They suggest a modified network architecture for object detectors (i.e. bounding box detectors). The architecture aggregates features from many scales (i.e. before each pooling layer) to detect both small and large object. The network is shaped similar to an hourglass. \nHow\n\nArchitecture\n\nThey have two branches.\nThe first one is similar to any normal network:\nConvolutions and pooling.\nThe exact choice of convolutions (e.g. how many) and pooling is determined by the used base network (e.g. ~50 convolutions with ~5x pooling in ResNet-50).\nThe second branch starts at the first one's output.\nIt uses nearest neighbour upsampling to re-increase the resolution back to the original one.\nIt does not contain convolutions.\nAll layers have 256 channels.\nThere are connections between the layers of the first and second branch.\nThese connections are simply 1x1 convolutions followed by an addition (similar to residual connections).\nOnly layers with similar height and width are connected.\nVisualization:\n\n\n\n\n\n\nIntegration with Faster R-CNN\n\nThey base the RPN on their second branch.\nWhile usually an RPN is applied to a single feature map of one scale, in their case it is applied to many feature maps of varying scales.\nThe RPN uses the same parameters for all scales.\nThey use anchor boxes, but only of different aspect ratios, not of different scales (as scales are already covered by their feature map heights/widths).\nGround truth bounding boxes are associated with the best matching anchor box (i.e. one box among all scales).\nEverything else is the same as in Faster R-CNN.\n\n\nIntegration with Fast R-CNN\n\nFast R-CNN does not use an RPN, but instead usually uses Selective Search to find region proposals (and applies RoI-Pooling to them).\nHere, they simply RoI-Pool from the FPN's output of the second branch.\nThey do not pool over all scales. Instead they pick only the scale/layer that matches the region proposal's size (based on its height/width).\nThey process each pooled RoI using two 1024-dimensional fully connected layers (initalizes randomly).\nEverything else is the same as in Fast R-CNN.\n\n\n\n Architecture\n\nThey have two branches.\nThe first one is similar to any normal network:\nConvolutions and pooling.\nThe exact choice of convolutions (e.g. how many) and pooling is determined by the used base network (e.g. ~50 convolutions with ~5x pooling in ResNet-50).\nThe second branch starts at the first one's output.\nIt uses nearest neighbour upsampling to re-increase the resolution back to the original one.\nIt does not contain convolutions.\nAll layers have 256 channels.\nThere are connections between the layers of the first and second branch.\nThese connections are simply 1x1 convolutions followed by an addition (similar to residual connections).\nOnly layers with similar height and width are connected.\nVisualization:\n\n\n\n\n\n They have two branches. The first one is similar to any normal network:\nConvolutions and pooling.\nThe exact choice of convolutions (e.g. how many) and pooling is determined by the used base network (e.g. ~50 convolutions with ~5x pooling in ResNet-50). The second branch starts at the first one's output.\nIt uses nearest neighbour upsampling to re-increase the resolution back to the original one.\nIt does not contain convolutions.\nAll layers have 256 channels. There are connections between the layers of the first and second branch.\nThese connections are simply 1x1 convolutions followed by an addition (similar to residual connections).\nOnly layers with similar height and width are connected. Visualization:\n\n\n\n  Integration with Faster R-CNN\n\nThey base the RPN on their second branch.\nWhile usually an RPN is applied to a single feature map of one scale, in their case it is applied to many feature maps of varying scales.\nThe RPN uses the same parameters for all scales.\nThey use anchor boxes, but only of different aspect ratios, not of different scales (as scales are already covered by their feature map heights/widths).\nGround truth bounding boxes are associated with the best matching anchor box (i.e. one box among all scales).\nEverything else is the same as in Faster R-CNN.\n\n They base the RPN on their second branch. While usually an RPN is applied to a single feature map of one scale, in their case it is applied to many feature maps of varying scales. The RPN uses the same parameters for all scales. They use anchor boxes, but only of different aspect ratios, not of different scales (as scales are already covered by their feature map heights/widths). Ground truth bounding boxes are associated with the best matching anchor box (i.e. one box among all scales). Everything else is the same as in Faster R-CNN. Integration with Fast R-CNN\n\nFast R-CNN does not use an RPN, but instead usually uses Selective Search to find region proposals (and applies RoI-Pooling to them).\nHere, they simply RoI-Pool from the FPN's output of the second branch.\nThey do not pool over all scales. Instead they pick only the scale/layer that matches the region proposal's size (based on its height/width).\nThey process each pooled RoI using two 1024-dimensional fully connected layers (initalizes randomly).\nEverything else is the same as in Fast R-CNN.\n\n Fast R-CNN does not use an RPN, but instead usually uses Selective Search to find region proposals (and applies RoI-Pooling to them). Here, they simply RoI-Pool from the FPN's output of the second branch. They do not pool over all scales. Instead they pick only the scale/layer that matches the region proposal's size (based on its height/width). They process each pooled RoI using two 1024-dimensional fully connected layers (initalizes randomly). Everything else is the same as in Fast R-CNN. \nResults\n\nFaster R-CNN\n\nFPN improves recall on COCO by about 8 points, compared to using standard RPN.\nImprovement is stronger for small objects (about 12 points).\nFor some reason no AP values here, only recall.\nThe RPN uses some convolutions to transform each feature map into region proposals.\nSharing the features of these convolutions marginally improves results.\n\n\nFast R-CNN\n\nFPN improves AP on COCO by about 2 points.\nImprovement is stronger for small objects (about 2.1 points).\n\n\n\n Faster R-CNN\n\nFPN improves recall on COCO by about 8 points, compared to using standard RPN.\nImprovement is stronger for small objects (about 12 points).\nFor some reason no AP values here, only recall.\nThe RPN uses some convolutions to transform each feature map into region proposals.\nSharing the features of these convolutions marginally improves results.\n\n FPN improves recall on COCO by about 8 points, compared to using standard RPN. Improvement is stronger for small objects (about 12 points). For some reason no AP values here, only recall. The RPN uses some convolutions to transform each feature map into region proposals.\nSharing the features of these convolutions marginally improves results. Fast R-CNN\n\nFPN improves AP on COCO by about 2 points.\nImprovement is stronger for small objects (about 2.1 points).\n\n FPN improves AP on COCO by about 2 points. Improvement is stronger for small objects (about 2.1 points). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.03144"
    },
    "29": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/SSD.md",
        "transcript": "\nWhat\n\nThey suggest a new bounding box detector.\nTheir detector works without an RPN and RoI-Pooling, making it very fast (almost 60fps).\nTheir detector works at multiple scales, making it better at detecting small and large objects.\nThey achieve scores similar to Faster R-CNN.\n\n They suggest a new bounding box detector. Their detector works without an RPN and RoI-Pooling, making it very fast (almost 60fps). Their detector works at multiple scales, making it better at detecting small and large objects. They achieve scores similar to Faster R-CNN. \nHow\n\nArchitecture\n\nSimilar to Faster R-CNN, they use a base network (modified version of VGG16) to transform images to feature maps.\nThey do not use an RPN.\nThey predict via convolutions for each location in the feature maps:\n\n(a) one confidence value per class (high confidence indicates that there is a bounding box of that class at the given location)\n(b) x/y offsets that indicate where exactly the center of the bounding box is (e.g. a bit to the left or top of the feature map cell's center)\n(c) height/width values that reflect the (logarithm of) the height/width of the bounding box\n\n\nSimilar to Faster R-CNN, they also use the concept of anchor boxes.\nSo they generate the described values not only once per location, but several times for several anchor boxes (they use six anchor boxes).\nEach anchor box has different height/width and optionally scale.\nVisualization of the predictions and anchor boxes:\n\n\n\n\nThey generate these predictions not only for the final feature map, but also for various feature maps in between (e.g. before pooling layers).\nThis makes it easier for the network to detect small (as well as large) bounding boxes (multi-scale detection).\nVisualization of the multi-scale architecture:\n\n\n\n\n\n\nTraining\n\nGround truth bounding boxes have to be matched with anchor boxes (at multiple scales) to determine correct outputs.\nTo do this, anchor boxes and ground truth bounding boxes are matched if their jaccard overlap is 0.5 or higher.\nAny unmatched ground truth bounding box is matched to the anchor box with highest jaccard overlap.\nNote that this means that a ground truth bounding box can be assigned to multiple anchor boxes (in Faster R-CNN it is always only one).\nThe loss function is similar to Faster R-CNN, i.e. a mixture of confidence loss (classification) and location loss (regression).\nThey use softmax with crossentropy for the confidence loss and smooth L1 loss for the location.\nSimilar to Faster R-CNN, they perform hard negative mining.\nInstead of training every anchor box at every scale they only train the ones with the highest loss (per example image).\nWhile doing that, they also pick the anchor boxes to be trained so that 3 in 4 boxes are negative examples (and 1 in 4 positive).\nData Augmentation: They sample patches from images using a wide range of possible sizes and aspect ratios.\nThey also horizontally flip images, perform cropping and padding and perform some photo-metric distortions.\n\n\nNon-Maximum-Suppression (NMS)\n\nUpon inference, they remove all bounding boxes that have a confidence below 0.01.\nThey then apply NMS, removing bounding boxes if there is already a similar one (measured by jaccard overlap of 0.45 or more).\n\n\n\n Architecture\n\nSimilar to Faster R-CNN, they use a base network (modified version of VGG16) to transform images to feature maps.\nThey do not use an RPN.\nThey predict via convolutions for each location in the feature maps:\n\n(a) one confidence value per class (high confidence indicates that there is a bounding box of that class at the given location)\n(b) x/y offsets that indicate where exactly the center of the bounding box is (e.g. a bit to the left or top of the feature map cell's center)\n(c) height/width values that reflect the (logarithm of) the height/width of the bounding box\n\n\nSimilar to Faster R-CNN, they also use the concept of anchor boxes.\nSo they generate the described values not only once per location, but several times for several anchor boxes (they use six anchor boxes).\nEach anchor box has different height/width and optionally scale.\nVisualization of the predictions and anchor boxes:\n\n\n\n\nThey generate these predictions not only for the final feature map, but also for various feature maps in between (e.g. before pooling layers).\nThis makes it easier for the network to detect small (as well as large) bounding boxes (multi-scale detection).\nVisualization of the multi-scale architecture:\n\n\n\n\n\n Similar to Faster R-CNN, they use a base network (modified version of VGG16) to transform images to feature maps. They do not use an RPN. They predict via convolutions for each location in the feature maps:\n\n(a) one confidence value per class (high confidence indicates that there is a bounding box of that class at the given location)\n(b) x/y offsets that indicate where exactly the center of the bounding box is (e.g. a bit to the left or top of the feature map cell's center)\n(c) height/width values that reflect the (logarithm of) the height/width of the bounding box\n\n (a) one confidence value per class (high confidence indicates that there is a bounding box of that class at the given location) (b) x/y offsets that indicate where exactly the center of the bounding box is (e.g. a bit to the left or top of the feature map cell's center) (c) height/width values that reflect the (logarithm of) the height/width of the bounding box Similar to Faster R-CNN, they also use the concept of anchor boxes.\nSo they generate the described values not only once per location, but several times for several anchor boxes (they use six anchor boxes).\nEach anchor box has different height/width and optionally scale. Visualization of the predictions and anchor boxes:\n\n\n\n  They generate these predictions not only for the final feature map, but also for various feature maps in between (e.g. before pooling layers).\nThis makes it easier for the network to detect small (as well as large) bounding boxes (multi-scale detection). Visualization of the multi-scale architecture:\n\n\n\n  Training\n\nGround truth bounding boxes have to be matched with anchor boxes (at multiple scales) to determine correct outputs.\nTo do this, anchor boxes and ground truth bounding boxes are matched if their jaccard overlap is 0.5 or higher.\nAny unmatched ground truth bounding box is matched to the anchor box with highest jaccard overlap.\nNote that this means that a ground truth bounding box can be assigned to multiple anchor boxes (in Faster R-CNN it is always only one).\nThe loss function is similar to Faster R-CNN, i.e. a mixture of confidence loss (classification) and location loss (regression).\nThey use softmax with crossentropy for the confidence loss and smooth L1 loss for the location.\nSimilar to Faster R-CNN, they perform hard negative mining.\nInstead of training every anchor box at every scale they only train the ones with the highest loss (per example image).\nWhile doing that, they also pick the anchor boxes to be trained so that 3 in 4 boxes are negative examples (and 1 in 4 positive).\nData Augmentation: They sample patches from images using a wide range of possible sizes and aspect ratios.\nThey also horizontally flip images, perform cropping and padding and perform some photo-metric distortions.\n\n Ground truth bounding boxes have to be matched with anchor boxes (at multiple scales) to determine correct outputs.\nTo do this, anchor boxes and ground truth bounding boxes are matched if their jaccard overlap is 0.5 or higher.\nAny unmatched ground truth bounding box is matched to the anchor box with highest jaccard overlap. Note that this means that a ground truth bounding box can be assigned to multiple anchor boxes (in Faster R-CNN it is always only one). The loss function is similar to Faster R-CNN, i.e. a mixture of confidence loss (classification) and location loss (regression).\nThey use softmax with crossentropy for the confidence loss and smooth L1 loss for the location. Similar to Faster R-CNN, they perform hard negative mining.\nInstead of training every anchor box at every scale they only train the ones with the highest loss (per example image).\nWhile doing that, they also pick the anchor boxes to be trained so that 3 in 4 boxes are negative examples (and 1 in 4 positive). Data Augmentation: They sample patches from images using a wide range of possible sizes and aspect ratios.\nThey also horizontally flip images, perform cropping and padding and perform some photo-metric distortions. Non-Maximum-Suppression (NMS)\n\nUpon inference, they remove all bounding boxes that have a confidence below 0.01.\nThey then apply NMS, removing bounding boxes if there is already a similar one (measured by jaccard overlap of 0.45 or more).\n\n Upon inference, they remove all bounding boxes that have a confidence below 0.01. They then apply NMS, removing bounding boxes if there is already a similar one (measured by jaccard overlap of 0.45 or more). \nResults\n\nPascal VOC 2007\n\nThey achieve around 1-3 points mAP better results than Faster R-CNN.\n\nDespite the multi-scale method, the model's performance is still significantly worse for small objects than for large ones.\nAdding data augmentation significantly improved the results compared to no data augmentation (around 6 points mAP).\nUsing more than one anchor box also had noticeable effects on the results (around 2 mAP or more).\nUsing multiple feature maps to predict outputs (multi-scale architecture) significantly improves the results (around 10 mAP).\nThough adding very coarse (high-level) feature maps seems to rather hurt than help.\n\n\nPascal VOC 2012\n\nAround 4 mAP better results than Faster R-CNN.\n\n\nCOCO\n\nBetween 1 and 4 mAP better results than Faster R-CNN.\n\n\nTimes\n\nAt a batch size of 1, SSD runs at about 46 fps at input resolution 300x300 (74.3 mAP on Pascal VOC) and 19 fps at input resolution 512x512 (76.8 mAP on Pascal VOC).\n\n\n\n\n Pascal VOC 2007\n\nThey achieve around 1-3 points mAP better results than Faster R-CNN.\n\nDespite the multi-scale method, the model's performance is still significantly worse for small objects than for large ones.\nAdding data augmentation significantly improved the results compared to no data augmentation (around 6 points mAP).\nUsing more than one anchor box also had noticeable effects on the results (around 2 mAP or more).\nUsing multiple feature maps to predict outputs (multi-scale architecture) significantly improves the results (around 10 mAP).\nThough adding very coarse (high-level) feature maps seems to rather hurt than help.\n\n They achieve around 1-3 points mAP better results than Faster R-CNN.  Despite the multi-scale method, the model's performance is still significantly worse for small objects than for large ones. Adding data augmentation significantly improved the results compared to no data augmentation (around 6 points mAP). Using more than one anchor box also had noticeable effects on the results (around 2 mAP or more). Using multiple feature maps to predict outputs (multi-scale architecture) significantly improves the results (around 10 mAP).\nThough adding very coarse (high-level) feature maps seems to rather hurt than help. Pascal VOC 2012\n\nAround 4 mAP better results than Faster R-CNN.\n\n Around 4 mAP better results than Faster R-CNN. COCO\n\nBetween 1 and 4 mAP better results than Faster R-CNN.\n\n Between 1 and 4 mAP better results than Faster R-CNN. Times\n\nAt a batch size of 1, SSD runs at about 46 fps at input resolution 300x300 (74.3 mAP on Pascal VOC) and 19 fps at input resolution 512x512 (76.8 mAP on Pascal VOC).\n\n\n At a batch size of 1, SSD runs at about 46 fps at input resolution 300x300 (74.3 mAP on Pascal VOC) and 19 fps at input resolution 512x512 (76.8 mAP on Pascal VOC).  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.02325"
    },
    "30": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/MobileNets.md",
        "transcript": "\nWhat\n\nThey suggest a factorization of standard 3x3 convolutions that is more efficient.\nThey build a model based on that factorization. The model has hyperparameters to choose higher performance or higher accuracy.\n\n They suggest a factorization of standard 3x3 convolutions that is more efficient. They build a model based on that factorization. The model has hyperparameters to choose higher performance or higher accuracy. \nHow\n\nFactorization\n\nThey factorize the standard 3x3 convolution into one depthwise 3x3 convolution, followed by a pointwise convoluton.\nNormal 3x3 convolution:\n\nComputes per filter and location a weighted average over all filters.\nFor kernel height kH, width kW and number of input filters/planes Fin, it requires kH*kW*Fin computations per location.\n\n\nDepthwise 3x3 convolution:\n\nComputes per filter and location a weighted average over one input filter. E.g. the 13th filter would only computed weighted averages over the 13th input filter/plane and ignore all the other input filters/planes.\nThis requires kH*kW*1 computations per location, i.e. drastically less than a normal convolution.\n\n\nPointwise convolution:\n\nThis is just another name for a normal 1x1 convolution.\nThis is placed after a depthwise convolution in order to compensate the fact that every (depthwise) filter only sees a single input plane.\nAs the kernel size is 1, this is rather fast to compute.\n\n\nVisualization of normal vs factorized convolution:\n\n\n\n\n\n\nModels\n\nThey use two hyperparameters for their models.\nalpha: Multiplier for the width in the range (0, 1]. A value of 0.5 means that every layer has half as many filters.\nroh: Multiplier for the resolution. In practice this is simply the input image size, having a value of {224, 192, 160, 128}.\n\n\n\n Factorization\n\nThey factorize the standard 3x3 convolution into one depthwise 3x3 convolution, followed by a pointwise convoluton.\nNormal 3x3 convolution:\n\nComputes per filter and location a weighted average over all filters.\nFor kernel height kH, width kW and number of input filters/planes Fin, it requires kH*kW*Fin computations per location.\n\n\nDepthwise 3x3 convolution:\n\nComputes per filter and location a weighted average over one input filter. E.g. the 13th filter would only computed weighted averages over the 13th input filter/plane and ignore all the other input filters/planes.\nThis requires kH*kW*1 computations per location, i.e. drastically less than a normal convolution.\n\n\nPointwise convolution:\n\nThis is just another name for a normal 1x1 convolution.\nThis is placed after a depthwise convolution in order to compensate the fact that every (depthwise) filter only sees a single input plane.\nAs the kernel size is 1, this is rather fast to compute.\n\n\nVisualization of normal vs factorized convolution:\n\n\n\n\n\n They factorize the standard 3x3 convolution into one depthwise 3x3 convolution, followed by a pointwise convoluton. Normal 3x3 convolution:\n\nComputes per filter and location a weighted average over all filters.\nFor kernel height kH, width kW and number of input filters/planes Fin, it requires kH*kW*Fin computations per location.\n\n Computes per filter and location a weighted average over all filters. For kernel height kH, width kW and number of input filters/planes Fin, it requires kH*kW*Fin computations per location. Depthwise 3x3 convolution:\n\nComputes per filter and location a weighted average over one input filter. E.g. the 13th filter would only computed weighted averages over the 13th input filter/plane and ignore all the other input filters/planes.\nThis requires kH*kW*1 computations per location, i.e. drastically less than a normal convolution.\n\n Computes per filter and location a weighted average over one input filter. E.g. the 13th filter would only computed weighted averages over the 13th input filter/plane and ignore all the other input filters/planes. This requires kH*kW*1 computations per location, i.e. drastically less than a normal convolution. Pointwise convolution:\n\nThis is just another name for a normal 1x1 convolution.\nThis is placed after a depthwise convolution in order to compensate the fact that every (depthwise) filter only sees a single input plane.\nAs the kernel size is 1, this is rather fast to compute.\n\n This is just another name for a normal 1x1 convolution. This is placed after a depthwise convolution in order to compensate the fact that every (depthwise) filter only sees a single input plane. As the kernel size is 1, this is rather fast to compute. Visualization of normal vs factorized convolution:\n\n\n\n  Models\n\nThey use two hyperparameters for their models.\nalpha: Multiplier for the width in the range (0, 1]. A value of 0.5 means that every layer has half as many filters.\nroh: Multiplier for the resolution. In practice this is simply the input image size, having a value of {224, 192, 160, 128}.\n\n They use two hyperparameters for their models. alpha: Multiplier for the width in the range (0, 1]. A value of 0.5 means that every layer has half as many filters. roh: Multiplier for the resolution. In practice this is simply the input image size, having a value of {224, 192, 160, 128}. \nResults\n\nImageNet\n\nCompared to VGG16, they achieve 1 percentage point less accuracy, while using only about 4% of VGG's multiply and additions (mult-adds) and while using only about 3% of the parameters.\nCompared to GoogleNet, they achieve about 1 percentage point more accuracy, while using only about 36% of the mult-adds and 61% of the parameters.\nNote that they don't compare to ResNet.\nResults for architecture choices vs. accuracy on ImageNet:\n\n\n\n\nRelation between mult-adds and accuracy on ImageNet:\n\n\n\n\n\n\nObject Detection\n\nTheir mAP is a bit on COCO when combining MobileNet with SSD (as opposed to using VGG or Inception v2).\nTheir mAP is quite a bit worse on COCO when combining MobileNet with Faster R-CNN.\n\n\nReducing the number of filters (alpha) influences the results more than reducing the input image resolution (roh).\nMaking the models shallower influences the results more than making them thinner.\n\n ImageNet\n\nCompared to VGG16, they achieve 1 percentage point less accuracy, while using only about 4% of VGG's multiply and additions (mult-adds) and while using only about 3% of the parameters.\nCompared to GoogleNet, they achieve about 1 percentage point more accuracy, while using only about 36% of the mult-adds and 61% of the parameters.\nNote that they don't compare to ResNet.\nResults for architecture choices vs. accuracy on ImageNet:\n\n\n\n\nRelation between mult-adds and accuracy on ImageNet:\n\n\n\n\n\n Compared to VGG16, they achieve 1 percentage point less accuracy, while using only about 4% of VGG's multiply and additions (mult-adds) and while using only about 3% of the parameters. Compared to GoogleNet, they achieve about 1 percentage point more accuracy, while using only about 36% of the mult-adds and 61% of the parameters. Note that they don't compare to ResNet. Results for architecture choices vs. accuracy on ImageNet:\n\n\n\n  Relation between mult-adds and accuracy on ImageNet:\n\n\n\n  Object Detection\n\nTheir mAP is a bit on COCO when combining MobileNet with SSD (as opposed to using VGG or Inception v2).\nTheir mAP is quite a bit worse on COCO when combining MobileNet with Faster R-CNN.\n\n Their mAP is a bit on COCO when combining MobileNet with SSD (as opposed to using VGG or Inception v2). Their mAP is quite a bit worse on COCO when combining MobileNet with Faster R-CNN. Reducing the number of filters (alpha) influences the results more than reducing the input image resolution (roh). Making the models shallower influences the results more than making them thinner. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.04861"
    },
    "31": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Mask_R-CNN.md",
        "transcript": "\nWhat\n\nThey suggest a variation of Faster R-CNN.\nTheir network detects bounding boxes (e.g. of people, cars) in images and also segments the objects within these bounding boxes (i.e. classifies for each pixel whether it is part of the object or background).\nThe model runs roughly at the same speed as Faster R-CNN.\n\n They suggest a variation of Faster R-CNN. Their network detects bounding boxes (e.g. of people, cars) in images and also segments the objects within these bounding boxes (i.e. classifies for each pixel whether it is part of the object or background). The model runs roughly at the same speed as Faster R-CNN. \nHow\n\nThe architecture and training is mostly the same as in Faster R-CNN:\n\nInput is an image.\nThe backbone network transforms the input image into feature maps. It consists of convolutions, e.g. initialized with ResNet's weights.\nThe RPN (Region Proposal Network) takes the feature maps and classifies for each location whether there is a bounding box at that point (with some other stuff to regress height/width and offsets).\nThis leads to a large number of bounding box candidates (region proposals) per image.\nRoIAlign: Each region proposal's \"area\" is extracted from the feature maps and converted into a fixed-size 7x7xF feature map (with F input filters). (See below.)\nThe head uses the region proposal's features to perform\n\nClassification: \"is the bounding box of a person/car/.../background\"\nRegression: \"bounding box should have width/height/offset so and so\"\nSegmentation: \"pixels so and so are part of this object's mask\"\n\n\nRough visualization of the architecture:\n\n\n\n\n\n\nRoIAlign\n\nThis is very similar to RoIPooling in Faster R-CNN.\nFor each RoI, RoIPooling first \"finds\" the features in the feature maps that lie within the RoI's rectangle. Then it max-pools them to create a fixed size vector.\nProblem: The coordinates where an RoI starts and ends may be non-integers. E.g. the top left corner might have coordinates (x=2.5, y=4.7).\nRoIPooling simply rounds these values to the nearest integers (e.g. (x=2, y=5)).\nBut that can create pooled RoIs that are significantly off, as the feature maps with which RoIPooling works have high (total) stride (e.g. 32 pixels in standard ResNets).\nSo being just one cell off can easily lead to being 32 pixels off on the input image.\nFor classification, being some pixels off is usually not that bad. For masks however it can significantly worsen the results, as these have to be pixel-accurate.\nIn RoIAlign this is compensated by not rounding the coordinates and instead using bilinear interpolation to interpolate between the feature map's cells.\nEach RoI is pooled by RoIAlign to a fixed sized feature map of size (H, W, F), with H and W usually being 7 or 14. (It can also generate different sizes, e.g. 7x7xF for classification and more accurate 14x14xF for masks.)\nIf H and W are 7, this leads to 49 cells within each plane of the pooled feature maps.\nEach cell again is a rectangle -- similar to the RoIs -- and pooled with bilinear interpolation.\nMore exactly, each cell is split up into four sub-cells (top left, top right, bottom right, bottom left).\nEach of these sub-cells is pooled via bilinear interpolation, leading to four values per cell.\nThe final cell value is then computed using either an average or a maximum over the four sub-values.\n\n\nSegmentation\n\nThey add an additional branch to the head that gets pooled RoI as inputs and processes them seperately from the classification and regression (no connections between the branches).\nThat branch does segmentation. It is fully convolutional, similar to many segmentation networks.\nThe result is one mask per class.\nThere is no softmax per pixel over the classes, as classification is done by a different branch.\n\n\nBase networks\n\nTheir backbone networks are either ResNet or ResNeXt (in the 50 or 102 layer variations).\nTheir head is either the fourth/fifth module from ResNet/ResNeXt (called C4 (fourth) or C5 (fifth)) or they use the second half from the FPN network (called FPN).\nThey denote their networks via backbone-head, i.e. ResNet-101-FPN means that their backbone is ResNet-101 and their head is FPN.\nVisualization of the different heads:\n\n\n\n\n\n\nTraining\n\nTraining happens in basically the same way as Faster R-CNN.\nThey just add an additional loss term to the total loss (L = L_classification + L_regression + L_mask). L_mask is based on binary cross-entropy.\nFor each predicted RoI, the correct mask is the intersection between that RoI's area and the correct mask.\nThey only train masks for RoIs that are positive (overlap with ground truth bounding boxes).\nThey train for 120k iterations at learning rate 0.02 and 40k at 0.002 with weight decay 0.0002 and momentum 0.9.\n\n\nTest\n\nFor the C4-head they sample up to 300 region proposals from the RPN (those with highest confidence values). For the FPN head they sample up to 1000, as FPN is faster.\nThey sample masks only for the 100 proposals with highest confidence values.\nEach mask is turned into a binary mask using a threshold of 0.5.\n\n\n\n The architecture and training is mostly the same as in Faster R-CNN:\n\nInput is an image.\nThe backbone network transforms the input image into feature maps. It consists of convolutions, e.g. initialized with ResNet's weights.\nThe RPN (Region Proposal Network) takes the feature maps and classifies for each location whether there is a bounding box at that point (with some other stuff to regress height/width and offsets).\nThis leads to a large number of bounding box candidates (region proposals) per image.\nRoIAlign: Each region proposal's \"area\" is extracted from the feature maps and converted into a fixed-size 7x7xF feature map (with F input filters). (See below.)\nThe head uses the region proposal's features to perform\n\nClassification: \"is the bounding box of a person/car/.../background\"\nRegression: \"bounding box should have width/height/offset so and so\"\nSegmentation: \"pixels so and so are part of this object's mask\"\n\n\nRough visualization of the architecture:\n\n\n\n\n\n Input is an image. The backbone network transforms the input image into feature maps. It consists of convolutions, e.g. initialized with ResNet's weights. The RPN (Region Proposal Network) takes the feature maps and classifies for each location whether there is a bounding box at that point (with some other stuff to regress height/width and offsets).\nThis leads to a large number of bounding box candidates (region proposals) per image. RoIAlign: Each region proposal's \"area\" is extracted from the feature maps and converted into a fixed-size 7x7xF feature map (with F input filters). (See below.) The head uses the region proposal's features to perform\n\nClassification: \"is the bounding box of a person/car/.../background\"\nRegression: \"bounding box should have width/height/offset so and so\"\nSegmentation: \"pixels so and so are part of this object's mask\"\n\n Classification: \"is the bounding box of a person/car/.../background\" Regression: \"bounding box should have width/height/offset so and so\" Segmentation: \"pixels so and so are part of this object's mask\" Rough visualization of the architecture:\n\n\n\n  RoIAlign\n\nThis is very similar to RoIPooling in Faster R-CNN.\nFor each RoI, RoIPooling first \"finds\" the features in the feature maps that lie within the RoI's rectangle. Then it max-pools them to create a fixed size vector.\nProblem: The coordinates where an RoI starts and ends may be non-integers. E.g. the top left corner might have coordinates (x=2.5, y=4.7).\nRoIPooling simply rounds these values to the nearest integers (e.g. (x=2, y=5)).\nBut that can create pooled RoIs that are significantly off, as the feature maps with which RoIPooling works have high (total) stride (e.g. 32 pixels in standard ResNets).\nSo being just one cell off can easily lead to being 32 pixels off on the input image.\nFor classification, being some pixels off is usually not that bad. For masks however it can significantly worsen the results, as these have to be pixel-accurate.\nIn RoIAlign this is compensated by not rounding the coordinates and instead using bilinear interpolation to interpolate between the feature map's cells.\nEach RoI is pooled by RoIAlign to a fixed sized feature map of size (H, W, F), with H and W usually being 7 or 14. (It can also generate different sizes, e.g. 7x7xF for classification and more accurate 14x14xF for masks.)\nIf H and W are 7, this leads to 49 cells within each plane of the pooled feature maps.\nEach cell again is a rectangle -- similar to the RoIs -- and pooled with bilinear interpolation.\nMore exactly, each cell is split up into four sub-cells (top left, top right, bottom right, bottom left).\nEach of these sub-cells is pooled via bilinear interpolation, leading to four values per cell.\nThe final cell value is then computed using either an average or a maximum over the four sub-values.\n\n This is very similar to RoIPooling in Faster R-CNN. For each RoI, RoIPooling first \"finds\" the features in the feature maps that lie within the RoI's rectangle. Then it max-pools them to create a fixed size vector. Problem: The coordinates where an RoI starts and ends may be non-integers. E.g. the top left corner might have coordinates (x=2.5, y=4.7).\nRoIPooling simply rounds these values to the nearest integers (e.g. (x=2, y=5)).\nBut that can create pooled RoIs that are significantly off, as the feature maps with which RoIPooling works have high (total) stride (e.g. 32 pixels in standard ResNets).\nSo being just one cell off can easily lead to being 32 pixels off on the input image. For classification, being some pixels off is usually not that bad. For masks however it can significantly worsen the results, as these have to be pixel-accurate. In RoIAlign this is compensated by not rounding the coordinates and instead using bilinear interpolation to interpolate between the feature map's cells. Each RoI is pooled by RoIAlign to a fixed sized feature map of size (H, W, F), with H and W usually being 7 or 14. (It can also generate different sizes, e.g. 7x7xF for classification and more accurate 14x14xF for masks.) If H and W are 7, this leads to 49 cells within each plane of the pooled feature maps. Each cell again is a rectangle -- similar to the RoIs -- and pooled with bilinear interpolation.\nMore exactly, each cell is split up into four sub-cells (top left, top right, bottom right, bottom left).\nEach of these sub-cells is pooled via bilinear interpolation, leading to four values per cell.\nThe final cell value is then computed using either an average or a maximum over the four sub-values. Segmentation\n\nThey add an additional branch to the head that gets pooled RoI as inputs and processes them seperately from the classification and regression (no connections between the branches).\nThat branch does segmentation. It is fully convolutional, similar to many segmentation networks.\nThe result is one mask per class.\nThere is no softmax per pixel over the classes, as classification is done by a different branch.\n\n They add an additional branch to the head that gets pooled RoI as inputs and processes them seperately from the classification and regression (no connections between the branches). That branch does segmentation. It is fully convolutional, similar to many segmentation networks. The result is one mask per class. There is no softmax per pixel over the classes, as classification is done by a different branch. Base networks\n\nTheir backbone networks are either ResNet or ResNeXt (in the 50 or 102 layer variations).\nTheir head is either the fourth/fifth module from ResNet/ResNeXt (called C4 (fourth) or C5 (fifth)) or they use the second half from the FPN network (called FPN).\nThey denote their networks via backbone-head, i.e. ResNet-101-FPN means that their backbone is ResNet-101 and their head is FPN.\nVisualization of the different heads:\n\n\n\n\n\n Their backbone networks are either ResNet or ResNeXt (in the 50 or 102 layer variations). Their head is either the fourth/fifth module from ResNet/ResNeXt (called C4 (fourth) or C5 (fifth)) or they use the second half from the FPN network (called FPN). They denote their networks via backbone-head, i.e. ResNet-101-FPN means that their backbone is ResNet-101 and their head is FPN. Visualization of the different heads:\n\n\n\n  Training\n\nTraining happens in basically the same way as Faster R-CNN.\nThey just add an additional loss term to the total loss (L = L_classification + L_regression + L_mask). L_mask is based on binary cross-entropy.\nFor each predicted RoI, the correct mask is the intersection between that RoI's area and the correct mask.\nThey only train masks for RoIs that are positive (overlap with ground truth bounding boxes).\nThey train for 120k iterations at learning rate 0.02 and 40k at 0.002 with weight decay 0.0002 and momentum 0.9.\n\n Training happens in basically the same way as Faster R-CNN. They just add an additional loss term to the total loss (L = L_classification + L_regression + L_mask). L_mask is based on binary cross-entropy. For each predicted RoI, the correct mask is the intersection between that RoI's area and the correct mask. They only train masks for RoIs that are positive (overlap with ground truth bounding boxes). They train for 120k iterations at learning rate 0.02 and 40k at 0.002 with weight decay 0.0002 and momentum 0.9. Test\n\nFor the C4-head they sample up to 300 region proposals from the RPN (those with highest confidence values). For the FPN head they sample up to 1000, as FPN is faster.\nThey sample masks only for the 100 proposals with highest confidence values.\nEach mask is turned into a binary mask using a threshold of 0.5.\n\n For the C4-head they sample up to 300 region proposals from the RPN (those with highest confidence values). For the FPN head they sample up to 1000, as FPN is faster. They sample masks only for the 100 proposals with highest confidence values. Each mask is turned into a binary mask using a threshold of 0.5. \nResults\n\nInstance Segmentation\n\nThey train and test on COCO.\nThey can outperform the best competitor by a decent margin (AP 37.1 vs 33.6 for FCIS+++ with OHEM).\nTheir model especially performs much better when there is overlap between bounding boxes.\nRanking of their models: ResNeXt-101-FPN > ResNet-101-FPN > ResNet-50-FPN > ResNet-101-C4 > ResNet-50-C4.\nUsing sigmoid instead of softmax (over classes) for the mask prediction significantly improves results by 5.5 to 7.1 points AP (depending on measurement method).\nPredicting only one mask per RoI (class-agnostic) instead of C masks (where C is the number of classes) only has a small negative effect on AP (about 0.6 points).\nUsing RoIAlign instead of RoIPooling has significant positive effects on the AP of around 5 to 10 points (if a network with C5 head is chosen, which has a high stride of 32). Effects are smaller for small strides and FPN head.\nUsing fully convolutional networks for the mask branch performs better than fully connected layers (1-3 points AP).\nExamples results on COCO vs FCIS (note the better handling of overlap):\n\n\n\n\n\n\nBounding-Box-Detection\n\nTraining additionally on masks seems to improve AP for bounding boxes by around 1 point (benefit from multi-task learning).\n\n\nTiming\n\nAround 200ms for ResNet-101-FPN. (M40 GPU)\nAround 400ms for ResNet-101-C4.\n\n\nHuman Pose Estimation\n\nThe mask branch can be used to predict keypoint (landmark) locations on human bodies (i.e. locations of hands, feet etc.).\nThis is done by using one mask per keypoint, initializing it to 0 and setting the keypoint location to 1.\nBy doing this, Mask R-CNN can predict keypoints roughly as good as the current leading models (on COCO), while running at 5fps.\n\n\nCityscapes\n\nThey test their model on the cityscapes dataset.\nThey beat previous models with significant margins. This is largely due to their better handling of overlapping instances.\nThey get their best scores using a model that was pre-trained on COCO.\nExamples results on cityscapes:\n\n\n\n\n\n\n\n Instance Segmentation\n\nThey train and test on COCO.\nThey can outperform the best competitor by a decent margin (AP 37.1 vs 33.6 for FCIS+++ with OHEM).\nTheir model especially performs much better when there is overlap between bounding boxes.\nRanking of their models: ResNeXt-101-FPN > ResNet-101-FPN > ResNet-50-FPN > ResNet-101-C4 > ResNet-50-C4.\nUsing sigmoid instead of softmax (over classes) for the mask prediction significantly improves results by 5.5 to 7.1 points AP (depending on measurement method).\nPredicting only one mask per RoI (class-agnostic) instead of C masks (where C is the number of classes) only has a small negative effect on AP (about 0.6 points).\nUsing RoIAlign instead of RoIPooling has significant positive effects on the AP of around 5 to 10 points (if a network with C5 head is chosen, which has a high stride of 32). Effects are smaller for small strides and FPN head.\nUsing fully convolutional networks for the mask branch performs better than fully connected layers (1-3 points AP).\nExamples results on COCO vs FCIS (note the better handling of overlap):\n\n\n\n\n\n They train and test on COCO. They can outperform the best competitor by a decent margin (AP 37.1 vs 33.6 for FCIS+++ with OHEM). Their model especially performs much better when there is overlap between bounding boxes. Ranking of their models: ResNeXt-101-FPN > ResNet-101-FPN > ResNet-50-FPN > ResNet-101-C4 > ResNet-50-C4. Using sigmoid instead of softmax (over classes) for the mask prediction significantly improves results by 5.5 to 7.1 points AP (depending on measurement method). Predicting only one mask per RoI (class-agnostic) instead of C masks (where C is the number of classes) only has a small negative effect on AP (about 0.6 points). Using RoIAlign instead of RoIPooling has significant positive effects on the AP of around 5 to 10 points (if a network with C5 head is chosen, which has a high stride of 32). Effects are smaller for small strides and FPN head. Using fully convolutional networks for the mask branch performs better than fully connected layers (1-3 points AP). Examples results on COCO vs FCIS (note the better handling of overlap):\n\n\n\n  Bounding-Box-Detection\n\nTraining additionally on masks seems to improve AP for bounding boxes by around 1 point (benefit from multi-task learning).\n\n Training additionally on masks seems to improve AP for bounding boxes by around 1 point (benefit from multi-task learning). Timing\n\nAround 200ms for ResNet-101-FPN. (M40 GPU)\nAround 400ms for ResNet-101-C4.\n\n Around 200ms for ResNet-101-FPN. (M40 GPU) Around 400ms for ResNet-101-C4. Human Pose Estimation\n\nThe mask branch can be used to predict keypoint (landmark) locations on human bodies (i.e. locations of hands, feet etc.).\nThis is done by using one mask per keypoint, initializing it to 0 and setting the keypoint location to 1.\nBy doing this, Mask R-CNN can predict keypoints roughly as good as the current leading models (on COCO), while running at 5fps.\n\n The mask branch can be used to predict keypoint (landmark) locations on human bodies (i.e. locations of hands, feet etc.). This is done by using one mask per keypoint, initializing it to 0 and setting the keypoint location to 1. By doing this, Mask R-CNN can predict keypoints roughly as good as the current leading models (on COCO), while running at 5fps. Cityscapes\n\nThey test their model on the cityscapes dataset.\nThey beat previous models with significant margins. This is largely due to their better handling of overlapping instances.\nThey get their best scores using a model that was pre-trained on COCO.\nExamples results on cityscapes:\n\n\n\n\n\n They test their model on the cityscapes dataset. They beat previous models with significant margins. This is largely due to their better handling of overlapping instances. They get their best scores using a model that was pre-trained on COCO. Examples results on cityscapes:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.06870"
    },
    "32": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Multi-view_Face_Detection_Using_Deep_Convolutional_Neural_Networks.md",
        "transcript": "\nWhat\n\nThey propose a CNN-based approach to detect faces in a wide range of orientations using a single model. However, since the training set is skewed, the network is more confident about up-right faces.\nThe model does not require additional components such as segmentation, bounding-box regression, segmentation, or SVM classifiers\n\n They propose a CNN-based approach to detect faces in a wide range of orientations using a single model. However, since the training set is skewed, the network is more confident about up-right faces. The model does not require additional components such as segmentation, bounding-box regression, segmentation, or SVM classifiers \nHow\n\nData augmentation: to increase the number of positive samples (24K face annotations), the authors used randomly sampled sub-windows of the images with IOU > 50% and also randomly flipped these images. In total, there were 20K positive and 20M negative training samples.\nCNN Architecture: 5 convolutional layers followed by 3 fully-connected. The fully-connected layers were converted to convolutional layers. Non-Maximal Suppression is applied to merge predicted bounding boxes.\nTraining: the CNN was trained using Caffe Library in the AFLW dataset with the following parameters:\n\nFine-tuning with AlexNet model\nInput image size = 227x227\nBatch size = 128 (32+, 96-)\nStride = 32\n\n\nTest: the model was evaluated on PASCAL FACE, AFW, and FDDB dataset.\nRunning time: since the fully-connected layers were converted to convolutional layers, the input image in running time may be of any size, obtaining a heat map as output. To detect faces of different sizes though, the image is scaled up/down and new heatmaps are obtained. The authors found that rescaling image 3 times per octave gives reasonable good performance.\n\nThe authors realized that the model is more confident about up-right faces than rotated/occluded ones. This trend is because the lack of good training examples to represent such faces in the training process. Better results can be achieved by using better sampling strategies and more sophisticated data augmentation techniques.\n\nThe authors tested different strategies for NMS and the effect of bounding-box regression for improving face detection. They NMS-avg had better performance compared to NMS-max in terms of average precision. On the other hand, adding a bounding-box regressor degraded the performance for both NMS strategies due to the mismatch between annotations of the training set and the test set. This mismatch is mostly for side-view faces.\n\n Data augmentation: to increase the number of positive samples (24K face annotations), the authors used randomly sampled sub-windows of the images with IOU > 50% and also randomly flipped these images. In total, there were 20K positive and 20M negative training samples. CNN Architecture: 5 convolutional layers followed by 3 fully-connected. The fully-connected layers were converted to convolutional layers. Non-Maximal Suppression is applied to merge predicted bounding boxes. Training: the CNN was trained using Caffe Library in the AFLW dataset with the following parameters:\n\nFine-tuning with AlexNet model\nInput image size = 227x227\nBatch size = 128 (32+, 96-)\nStride = 32\n\n Fine-tuning with AlexNet model Input image size = 227x227 Batch size = 128 (32+, 96-) Stride = 32 Test: the model was evaluated on PASCAL FACE, AFW, and FDDB dataset. Running time: since the fully-connected layers were converted to convolutional layers, the input image in running time may be of any size, obtaining a heat map as output. To detect faces of different sizes though, the image is scaled up/down and new heatmaps are obtained. The authors found that rescaling image 3 times per octave gives reasonable good performance.\n The authors realized that the model is more confident about up-right faces than rotated/occluded ones. This trend is because the lack of good training examples to represent such faces in the training process. Better results can be achieved by using better sampling strategies and more sophisticated data augmentation techniques.\n The authors tested different strategies for NMS and the effect of bounding-box regression for improving face detection. They NMS-avg had better performance compared to NMS-max in terms of average precision. On the other hand, adding a bounding-box regressor degraded the performance for both NMS strategies due to the mismatch between annotations of the training set and the test set. This mismatch is mostly for side-view faces. \nResults:\n\nIn comparison to R-CNN, the proposed face detector had significantly better performance independent of the NMS strategy. The authors believe the inferior performance of R-CNN due to the loss of recall since selective search may miss some of the face regions; and loss in localization since bounding-box regression is not perfect and may not be able to fully align the segmentation bounding-boxes, provided by selective search, with the ground truth.\nIn comparison to other state-of-art methods like structural model, TSM and cascade-based methods the DDFD achieve similar or better results. However, this comparison is not completely fair since the most of methods use extra information of pose annotation or information about facial landmarks during the training.\n\n In comparison to R-CNN, the proposed face detector had significantly better performance independent of the NMS strategy. The authors believe the inferior performance of R-CNN due to the loss of recall since selective search may miss some of the face regions; and loss in localization since bounding-box regression is not perfect and may not be able to fully align the segmentation bounding-boxes, provided by selective search, with the ground truth. In comparison to other state-of-art methods like structural model, TSM and cascade-based methods the DDFD achieve similar or better results. However, this comparison is not completely fair since the most of methods use extra information of pose annotation or information about facial landmarks during the training. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1502.02766"
    },
    "33": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/On_The_Effects_of_BN_and_WN_in_GANs.md",
        "transcript": "\nWhat\n\nThey analyze the effects of using Batch Normalization (BN) and Weight Normalization (WN) in GANs (classical algorithm, like DCGAN).\nThey introduce a new measure to rate the quality of the generated images over time.\n\n They analyze the effects of using Batch Normalization (BN) and Weight Normalization (WN) in GANs (classical algorithm, like DCGAN). They introduce a new measure to rate the quality of the generated images over time. \nHow\n\nThey use BN as it is usually defined.\nThey use WN with the following formulas:\n\nStrict weight-normalized layer:\n\n\n\n\nAffine weight-normalized layer:\n\n\n\n\nAs activation units they use Translated ReLUs (aka \"threshold functions\"):\n\n\nalpha is a learned parameter.\nTReLUs play better with their WN layers than normal ReLUs.\n\n\n\n\nReconstruction measure\n\nTo evaluate the quality of the generated images during training, they introduce a new measure.\nThe measure is based on a L2-Norm (MSE) between (1) a real image and (2) an image created by the generator that is as similar as possible to the real image.\nThey generate (2) by starting G(z) with a noise vector z that is filled with zeros. The desired output is the real image. They compute a MSE between the generated and real image and backpropagate the result. Then they use the generated gradient to update z, while leaving the parameters of G unaltered. They repeat this for a defined number of steps.\nNote that the above described method is fairly time-consuming, so they don't do it often.\n\n\nNetworks\n\nTheir networks are fairly standard.\nGenerator: Starts at 1024 filters, goes down to 64 (then 3 for the output). Upsampling via fractionally strided convs.\nDiscriminator: Starts at 64 filters, goes to 1024 (then 1 for the output). Downsampling via strided convolutions.\nThey test three variations of these networks:\n\nVanilla: No normalization. PReLUs in both G and D.\nBN: BN in G and D, but not in the last layers and not in the first layer of D. PReLUs in both G and D.\nWN: Strict weight-normalized layers in G and D, except for the last layers, which are affine weight-normalized layers. TPReLUs (Translated PReLUs) in both G and D.\n\n\n\n\nOther\n\nThey train with RMSProp and batch size 32.\n\n\n\n They use BN as it is usually defined. They use WN with the following formulas:\n\nStrict weight-normalized layer:\n\n\n\n\nAffine weight-normalized layer:\n\n\n\n\nAs activation units they use Translated ReLUs (aka \"threshold functions\"):\n\n\nalpha is a learned parameter.\nTReLUs play better with their WN layers than normal ReLUs.\n\n\n\n Strict weight-normalized layer:\n\n\n\n  Affine weight-normalized layer:\n\n\n\n  As activation units they use Translated ReLUs (aka \"threshold functions\"):\n\n\nalpha is a learned parameter.\nTReLUs play better with their WN layers than normal ReLUs.\n\n  alpha is a learned parameter. TReLUs play better with their WN layers than normal ReLUs. Reconstruction measure\n\nTo evaluate the quality of the generated images during training, they introduce a new measure.\nThe measure is based on a L2-Norm (MSE) between (1) a real image and (2) an image created by the generator that is as similar as possible to the real image.\nThey generate (2) by starting G(z) with a noise vector z that is filled with zeros. The desired output is the real image. They compute a MSE between the generated and real image and backpropagate the result. Then they use the generated gradient to update z, while leaving the parameters of G unaltered. They repeat this for a defined number of steps.\nNote that the above described method is fairly time-consuming, so they don't do it often.\n\n To evaluate the quality of the generated images during training, they introduce a new measure. The measure is based on a L2-Norm (MSE) between (1) a real image and (2) an image created by the generator that is as similar as possible to the real image. They generate (2) by starting G(z) with a noise vector z that is filled with zeros. The desired output is the real image. They compute a MSE between the generated and real image and backpropagate the result. Then they use the generated gradient to update z, while leaving the parameters of G unaltered. They repeat this for a defined number of steps. Note that the above described method is fairly time-consuming, so they don't do it often. Networks\n\nTheir networks are fairly standard.\nGenerator: Starts at 1024 filters, goes down to 64 (then 3 for the output). Upsampling via fractionally strided convs.\nDiscriminator: Starts at 64 filters, goes to 1024 (then 1 for the output). Downsampling via strided convolutions.\nThey test three variations of these networks:\n\nVanilla: No normalization. PReLUs in both G and D.\nBN: BN in G and D, but not in the last layers and not in the first layer of D. PReLUs in both G and D.\nWN: Strict weight-normalized layers in G and D, except for the last layers, which are affine weight-normalized layers. TPReLUs (Translated PReLUs) in both G and D.\n\n\n\n Their networks are fairly standard. Generator: Starts at 1024 filters, goes down to 64 (then 3 for the output). Upsampling via fractionally strided convs. Discriminator: Starts at 64 filters, goes to 1024 (then 1 for the output). Downsampling via strided convolutions. They test three variations of these networks:\n\nVanilla: No normalization. PReLUs in both G and D.\nBN: BN in G and D, but not in the last layers and not in the first layer of D. PReLUs in both G and D.\nWN: Strict weight-normalized layers in G and D, except for the last layers, which are affine weight-normalized layers. TPReLUs (Translated PReLUs) in both G and D.\n\n Vanilla: No normalization. PReLUs in both G and D. BN: BN in G and D, but not in the last layers and not in the first layer of D. PReLUs in both G and D. WN: Strict weight-normalized layers in G and D, except for the last layers, which are affine weight-normalized layers. TPReLUs (Translated PReLUs) in both G and D. Other\n\nThey train with RMSProp and batch size 32.\n\n They train with RMSProp and batch size 32. \nResults\n\nTheir WN formulation trains stable, provided the learning rate is set to 0.0002 or lower.\nThey argue, that their achieved stability is similar to the one in WGAN.\nBN had significant swings in quality.\nVanilla collapsed sooner or later.\nBoth BN and Vanilla reached an optimal point shortly after the start of the training. After that, the quality of the generated images only worsened.\nPlot of their quality measure:\n\n\n\n\nTheir quality measure is based on reconstruction of input images. The below image shows examples for that reconstruction (each person: original image, vanilla reconstruction, BN rec., WN rec.).\n\n\n\n\nExamples generated by their WN network:\n\n\n\n\n\n Their WN formulation trains stable, provided the learning rate is set to 0.0002 or lower. They argue, that their achieved stability is similar to the one in WGAN. BN had significant swings in quality. Vanilla collapsed sooner or later. Both BN and Vanilla reached an optimal point shortly after the start of the training. After that, the quality of the generated images only worsened. Plot of their quality measure:\n\n\n\n  Their quality measure is based on reconstruction of input images. The below image shows examples for that reconstruction (each person: original image, vanilla reconstruction, BN rec., WN rec.).\n\n\n\n  Examples generated by their WN network:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.03971"
    },
    "34": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/BEGAN.md",
        "transcript": "\nWhat\n\nThey suggest a GAN algorithm that is based on an autoencoder with Wasserstein distance.\nTheir method generates highly realistic human faces.\nTheir method has a convergence measure, which reflects the quality of the generates images.\nTheir method has a diversity hyperparameter, which can be used to set the tradeoff between image diversity and image quality.\n\n They suggest a GAN algorithm that is based on an autoencoder with Wasserstein distance. Their method generates highly realistic human faces. Their method has a convergence measure, which reflects the quality of the generates images. Their method has a diversity hyperparameter, which can be used to set the tradeoff between image diversity and image quality. \nHow\n\nLike other GANs, their method uses a generator G and a discriminator D.\nGenerator\n\nThe generator is fairly standard.\nIt gets a noise vector z as input and uses upsampling+convolutions to generate images.\nIt uses ELUs and no BN.\n\n\nDiscriminator\n\nThe discriminator is a full autoencoder (i.e. it converts input images to 8x8x3 tensors, then reconstructs them back to images).\nIt has skip-connections from the 8x8x3 layer to each upsampling layer.\nIt also uses ELUs and no BN.\n\n\nTheir method now has the following steps:\n\nCollect real images x_real.\nGenerate fake images x_fake = G(z).\nReconstruct the real images r_real = D(x_real).\nReconstruct the fake images r_fake = D(x_fake).\nUsing an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of real images d_real = Lp(x_real, r_real).\nUsing an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of fake images d_fake = Lp(x_fake, r_fake).\nThe loss of D is now L_D = d_real - d_fake.\nThe loss of G is now L_G = -L_D.\n\n\nAbout the loss\n\nr_real and r_fake are really losses (e.g. L1-loss or L2-loss). In the paper they use L(...) for that. Here they are referenced as d_* in order to avoid confusion.\nThe loss L_D is based on the Wasserstein distance, as in WGAN.\nL_D assumes, that the losses d_real and d_fake are normally distributed and tries to move their mean values. Ideally, the discriminator produces very different means for real/fake images, while the generator leads to very similar means.\nTheir formulation of the Wasserstein distance does not require K-Lipschitz functions, which is why they don't have the weight clipping from WGAN.\n\n\nEquilibrium\n\nThe generator and discriminator are at equilibrium, if E[r_fake] = E[r_real]. (That's undesirable, because it means that D can't differentiate between fake and real images, i.e. G doesn't get a proper gradient any more.)\nLet g = E[r_fake] / E[r_real], then:\n\nLow g means that E[r_fake] is low and/or E[r_real] is high, which means that real images are not as well reconstructed as fake images. This means, that the discriminator will be more heavily trained towards reconstructing real images correctly (as that is the main source of error).\nHigh g conversely means that real images are well reconstructed (compared to fake ones) and that the discriminator will be trained more towards fake ones.\ng gives information about how much G and D should be trained each (so that none of the two overwhelms the other).\n\n\nThey introduce a hyperparameter gamma (from interval [0,1]), which reflects the target value of the balance g.\nUsing gamma, they change their losses L_D and L_G slightly:\n\nL_D = d_real - k_t d_fake\nL_G = r_fake\nk_t+1 = k_t + lambda_k (gamma d_real - d_fake).\n\n\nk_t is a control term that controls how much D is supposed to focus on the fake images. It changes with every batch.\nk_t is clipped to [0,1] and initialized at 0 (max focus on reconstructing real images).\nlambda_k is like the learning rate of the control term, set to 0.001.\nNote that gamma d_real - d_fake = 0 <=> gamma d_real = d_fake <=> gamma = d_fake / d_real.\n\n\nConvergence measure\n\nThey measure the convergence of their model using M:\nM = d_real + |gamma d_real - d_fake|\nM goes down, if d_real goes down (D becomes better at autoencoding real images).\nM goes down, if the difference in reconstruction error between real and fake images goes down, i.e. if G becomes better at generating fake images.\n\n\nOther\n\nThey use Adam with learning rate 0.0001. They decrease it by a factor of 2 whenever M stalls.\nHigher initial learning rate could lead to model collapse or visual artifacs.\nThey generate images of max size 128x128.\nThey don't use more than 128 filters per conv layer.\n\n\n\n Like other GANs, their method uses a generator G and a discriminator D. Generator\n\nThe generator is fairly standard.\nIt gets a noise vector z as input and uses upsampling+convolutions to generate images.\nIt uses ELUs and no BN.\n\n The generator is fairly standard. It gets a noise vector z as input and uses upsampling+convolutions to generate images. It uses ELUs and no BN. Discriminator\n\nThe discriminator is a full autoencoder (i.e. it converts input images to 8x8x3 tensors, then reconstructs them back to images).\nIt has skip-connections from the 8x8x3 layer to each upsampling layer.\nIt also uses ELUs and no BN.\n\n The discriminator is a full autoencoder (i.e. it converts input images to 8x8x3 tensors, then reconstructs them back to images). It has skip-connections from the 8x8x3 layer to each upsampling layer. It also uses ELUs and no BN. Their method now has the following steps:\n\nCollect real images x_real.\nGenerate fake images x_fake = G(z).\nReconstruct the real images r_real = D(x_real).\nReconstruct the fake images r_fake = D(x_fake).\nUsing an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of real images d_real = Lp(x_real, r_real).\nUsing an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of fake images d_fake = Lp(x_fake, r_fake).\nThe loss of D is now L_D = d_real - d_fake.\nThe loss of G is now L_G = -L_D.\n\n Collect real images x_real. Generate fake images x_fake = G(z). Reconstruct the real images r_real = D(x_real). Reconstruct the fake images r_fake = D(x_fake). Using an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of real images d_real = Lp(x_real, r_real). Using an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of fake images d_fake = Lp(x_fake, r_fake). The loss of D is now L_D = d_real - d_fake. The loss of G is now L_G = -L_D. About the loss\n\nr_real and r_fake are really losses (e.g. L1-loss or L2-loss). In the paper they use L(...) for that. Here they are referenced as d_* in order to avoid confusion.\nThe loss L_D is based on the Wasserstein distance, as in WGAN.\nL_D assumes, that the losses d_real and d_fake are normally distributed and tries to move their mean values. Ideally, the discriminator produces very different means for real/fake images, while the generator leads to very similar means.\nTheir formulation of the Wasserstein distance does not require K-Lipschitz functions, which is why they don't have the weight clipping from WGAN.\n\n r_real and r_fake are really losses (e.g. L1-loss or L2-loss). In the paper they use L(...) for that. Here they are referenced as d_* in order to avoid confusion. The loss L_D is based on the Wasserstein distance, as in WGAN. L_D assumes, that the losses d_real and d_fake are normally distributed and tries to move their mean values. Ideally, the discriminator produces very different means for real/fake images, while the generator leads to very similar means. Their formulation of the Wasserstein distance does not require K-Lipschitz functions, which is why they don't have the weight clipping from WGAN. Equilibrium\n\nThe generator and discriminator are at equilibrium, if E[r_fake] = E[r_real]. (That's undesirable, because it means that D can't differentiate between fake and real images, i.e. G doesn't get a proper gradient any more.)\nLet g = E[r_fake] / E[r_real], then:\n\nLow g means that E[r_fake] is low and/or E[r_real] is high, which means that real images are not as well reconstructed as fake images. This means, that the discriminator will be more heavily trained towards reconstructing real images correctly (as that is the main source of error).\nHigh g conversely means that real images are well reconstructed (compared to fake ones) and that the discriminator will be trained more towards fake ones.\ng gives information about how much G and D should be trained each (so that none of the two overwhelms the other).\n\n\nThey introduce a hyperparameter gamma (from interval [0,1]), which reflects the target value of the balance g.\nUsing gamma, they change their losses L_D and L_G slightly:\n\nL_D = d_real - k_t d_fake\nL_G = r_fake\nk_t+1 = k_t + lambda_k (gamma d_real - d_fake).\n\n\nk_t is a control term that controls how much D is supposed to focus on the fake images. It changes with every batch.\nk_t is clipped to [0,1] and initialized at 0 (max focus on reconstructing real images).\nlambda_k is like the learning rate of the control term, set to 0.001.\nNote that gamma d_real - d_fake = 0 <=> gamma d_real = d_fake <=> gamma = d_fake / d_real.\n\n The generator and discriminator are at equilibrium, if E[r_fake] = E[r_real]. (That's undesirable, because it means that D can't differentiate between fake and real images, i.e. G doesn't get a proper gradient any more.) Let g = E[r_fake] / E[r_real], then:\n\nLow g means that E[r_fake] is low and/or E[r_real] is high, which means that real images are not as well reconstructed as fake images. This means, that the discriminator will be more heavily trained towards reconstructing real images correctly (as that is the main source of error).\nHigh g conversely means that real images are well reconstructed (compared to fake ones) and that the discriminator will be trained more towards fake ones.\ng gives information about how much G and D should be trained each (so that none of the two overwhelms the other).\n\n Low g means that E[r_fake] is low and/or E[r_real] is high, which means that real images are not as well reconstructed as fake images. This means, that the discriminator will be more heavily trained towards reconstructing real images correctly (as that is the main source of error). High g conversely means that real images are well reconstructed (compared to fake ones) and that the discriminator will be trained more towards fake ones. g gives information about how much G and D should be trained each (so that none of the two overwhelms the other). They introduce a hyperparameter gamma (from interval [0,1]), which reflects the target value of the balance g. Using gamma, they change their losses L_D and L_G slightly:\n\nL_D = d_real - k_t d_fake\nL_G = r_fake\nk_t+1 = k_t + lambda_k (gamma d_real - d_fake).\n\n L_D = d_real - k_t d_fake L_G = r_fake k_t+1 = k_t + lambda_k (gamma d_real - d_fake). k_t is a control term that controls how much D is supposed to focus on the fake images. It changes with every batch. k_t is clipped to [0,1] and initialized at 0 (max focus on reconstructing real images). lambda_k is like the learning rate of the control term, set to 0.001. Note that gamma d_real - d_fake = 0 <=> gamma d_real = d_fake <=> gamma = d_fake / d_real. Convergence measure\n\nThey measure the convergence of their model using M:\nM = d_real + |gamma d_real - d_fake|\nM goes down, if d_real goes down (D becomes better at autoencoding real images).\nM goes down, if the difference in reconstruction error between real and fake images goes down, i.e. if G becomes better at generating fake images.\n\n They measure the convergence of their model using M: M = d_real + |gamma d_real - d_fake| M goes down, if d_real goes down (D becomes better at autoencoding real images). M goes down, if the difference in reconstruction error between real and fake images goes down, i.e. if G becomes better at generating fake images. Other\n\nThey use Adam with learning rate 0.0001. They decrease it by a factor of 2 whenever M stalls.\nHigher initial learning rate could lead to model collapse or visual artifacs.\nThey generate images of max size 128x128.\nThey don't use more than 128 filters per conv layer.\n\n They use Adam with learning rate 0.0001. They decrease it by a factor of 2 whenever M stalls. Higher initial learning rate could lead to model collapse or visual artifacs. They generate images of max size 128x128. They don't use more than 128 filters per conv layer. \nResults\n\nNOTES:\n\nBelow example images are NOT from generators trained on CelebA. They used a custom dataset of celebrity images. They don't show any example images from the dataset. The generated images look like there is less background around the faces, making the task easier.\nFew example images. Unclear how much cherry picking was involved. Though the results from the tensorflow example (see like at top) make it look like the examples are representative (aside from speckle-artifacts).\nNo LSUN Bedrooms examples. Human faces are comparatively easy to generate.\n\n\nExample images at 128x128:\n\n\n\n\nEffect of changing the target balance gamma:\n\n\nHigh gamma leads to more diversity at lower quality.\n\n\nInterpolations:\n\n\n\n\nConvergence measure M and associated image quality during the training:\n\n\n\n\n\n NOTES:\n\nBelow example images are NOT from generators trained on CelebA. They used a custom dataset of celebrity images. They don't show any example images from the dataset. The generated images look like there is less background around the faces, making the task easier.\nFew example images. Unclear how much cherry picking was involved. Though the results from the tensorflow example (see like at top) make it look like the examples are representative (aside from speckle-artifacts).\nNo LSUN Bedrooms examples. Human faces are comparatively easy to generate.\n\n Below example images are NOT from generators trained on CelebA. They used a custom dataset of celebrity images. They don't show any example images from the dataset. The generated images look like there is less background around the faces, making the task easier. Few example images. Unclear how much cherry picking was involved. Though the results from the tensorflow example (see like at top) make it look like the examples are representative (aside from speckle-artifacts). No LSUN Bedrooms examples. Human faces are comparatively easy to generate. Example images at 128x128:\n\n\n\n  Effect of changing the target balance gamma:\n\n\nHigh gamma leads to more diversity at lower quality.\n\n  High gamma leads to more diversity at lower quality. Interpolations:\n\n\n\n  Convergence measure M and associated image quality during the training:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.10717"
    },
    "35": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/StackGAN.md",
        "transcript": "\nWhat\n\nThey propose a two-stage GAN architecture that generates 256x256 images of (relatively) high quality.\nThe model gets text as an additional input and the images match the text.\n\n They propose a two-stage GAN architecture that generates 256x256 images of (relatively) high quality. The model gets text as an additional input and the images match the text. \nHow\n\nMost of the architecture is the same as in any GAN:\n\nGenerator G generates images.\nDiscriminator D discriminates betweens fake and real images.\nG gets a noise variable z, so that it doesn't always do the same thing.\n\n\nTwo-staged image generation:\n\nInstead of one step, as in most GANs, they use two steps, each consisting of a G and D.\nThe first generator creates 64x64 images via upsampling.\nThe first discriminator judges these images via downsampling convolutions.\nThe second generator takes the image from the first generator, downsamples it via convolutions, then applies some residual convolutions and then re-upsamples it to 256x256.\nThe second discriminator is comparable to the first one (downsampling convolutions).\nNote that the second generator does not get an additional noise term z, only the first one gets it.\nFor upsampling, they use 3x3 convolutions with ReLUs, BN and nearest neighbour upsampling.\nFor downsampling, they use 4x4 convolutions with stride 2, Leaky ReLUs and BN (the first convolution doesn't seem to use BN).\n\n\nText embedding:\n\nThe generated images are supposed to match input texts.\nThese input texts are embedded to vectors.\nThese vectors are added as:\n\nAn additional input to the first generator.\nAn additional input to the second generator (concatenated after the downsampling and before the residual convolutions).\nAn additional input to the first discriminator (concatenated after the downsampling).\nAn additional input to the second discriminator (concatenated after the downsampling).\n\n\nIn case the text embeddings need to be matrices, the values are simply reshaped to (N, 1, 1) and then repeated to (N, H, W).\nThe texts are converted to embeddings via a network at the start of the model.\n\nInput to that vector: Unclear. (Concatenated word vectors? Seems to not be described in the text.)\nThe input is transformed to a vector via a fully connected layer (the text model is apparently not recurrent).\nThe vector is transformed via fully connected layers to a mean vector and a sigma vector.\nThese are then interpreted as normal distributions, from which the final output vector is sampled. This uses the reparameterization trick, similar to the method in VAEs.\nJust like in VAEs, a KL-divergence term is added to the loss, which prevents each single normal distribution from deviating too far from the unit normal distribution N(0,1).\nThe authors argue, that using the VAE-like formulation -- instead of directly predicting an output vector (via FC layers) -- compensated for the lack of labels (smoother manifold).\nNote: This way of generating text embeddings seems very simple. (No recurrence, only about two layers.) It probably won't do much more than just roughly checking for the existence of specific words and word combinations (e.g. \"red head\").\n\n\n\n\nVisualization of the architecture:\n\n\n\n\n\n Most of the architecture is the same as in any GAN:\n\nGenerator G generates images.\nDiscriminator D discriminates betweens fake and real images.\nG gets a noise variable z, so that it doesn't always do the same thing.\n\n Generator G generates images. Discriminator D discriminates betweens fake and real images. G gets a noise variable z, so that it doesn't always do the same thing. Two-staged image generation:\n\nInstead of one step, as in most GANs, they use two steps, each consisting of a G and D.\nThe first generator creates 64x64 images via upsampling.\nThe first discriminator judges these images via downsampling convolutions.\nThe second generator takes the image from the first generator, downsamples it via convolutions, then applies some residual convolutions and then re-upsamples it to 256x256.\nThe second discriminator is comparable to the first one (downsampling convolutions).\nNote that the second generator does not get an additional noise term z, only the first one gets it.\nFor upsampling, they use 3x3 convolutions with ReLUs, BN and nearest neighbour upsampling.\nFor downsampling, they use 4x4 convolutions with stride 2, Leaky ReLUs and BN (the first convolution doesn't seem to use BN).\n\n Instead of one step, as in most GANs, they use two steps, each consisting of a G and D. The first generator creates 64x64 images via upsampling. The first discriminator judges these images via downsampling convolutions. The second generator takes the image from the first generator, downsamples it via convolutions, then applies some residual convolutions and then re-upsamples it to 256x256. The second discriminator is comparable to the first one (downsampling convolutions). Note that the second generator does not get an additional noise term z, only the first one gets it. For upsampling, they use 3x3 convolutions with ReLUs, BN and nearest neighbour upsampling. For downsampling, they use 4x4 convolutions with stride 2, Leaky ReLUs and BN (the first convolution doesn't seem to use BN). Text embedding:\n\nThe generated images are supposed to match input texts.\nThese input texts are embedded to vectors.\nThese vectors are added as:\n\nAn additional input to the first generator.\nAn additional input to the second generator (concatenated after the downsampling and before the residual convolutions).\nAn additional input to the first discriminator (concatenated after the downsampling).\nAn additional input to the second discriminator (concatenated after the downsampling).\n\n\nIn case the text embeddings need to be matrices, the values are simply reshaped to (N, 1, 1) and then repeated to (N, H, W).\nThe texts are converted to embeddings via a network at the start of the model.\n\nInput to that vector: Unclear. (Concatenated word vectors? Seems to not be described in the text.)\nThe input is transformed to a vector via a fully connected layer (the text model is apparently not recurrent).\nThe vector is transformed via fully connected layers to a mean vector and a sigma vector.\nThese are then interpreted as normal distributions, from which the final output vector is sampled. This uses the reparameterization trick, similar to the method in VAEs.\nJust like in VAEs, a KL-divergence term is added to the loss, which prevents each single normal distribution from deviating too far from the unit normal distribution N(0,1).\nThe authors argue, that using the VAE-like formulation -- instead of directly predicting an output vector (via FC layers) -- compensated for the lack of labels (smoother manifold).\nNote: This way of generating text embeddings seems very simple. (No recurrence, only about two layers.) It probably won't do much more than just roughly checking for the existence of specific words and word combinations (e.g. \"red head\").\n\n\n\n The generated images are supposed to match input texts. These input texts are embedded to vectors. These vectors are added as:\n\nAn additional input to the first generator.\nAn additional input to the second generator (concatenated after the downsampling and before the residual convolutions).\nAn additional input to the first discriminator (concatenated after the downsampling).\nAn additional input to the second discriminator (concatenated after the downsampling).\n\n An additional input to the first generator. An additional input to the second generator (concatenated after the downsampling and before the residual convolutions). An additional input to the first discriminator (concatenated after the downsampling). An additional input to the second discriminator (concatenated after the downsampling). In case the text embeddings need to be matrices, the values are simply reshaped to (N, 1, 1) and then repeated to (N, H, W). The texts are converted to embeddings via a network at the start of the model.\n\nInput to that vector: Unclear. (Concatenated word vectors? Seems to not be described in the text.)\nThe input is transformed to a vector via a fully connected layer (the text model is apparently not recurrent).\nThe vector is transformed via fully connected layers to a mean vector and a sigma vector.\nThese are then interpreted as normal distributions, from which the final output vector is sampled. This uses the reparameterization trick, similar to the method in VAEs.\nJust like in VAEs, a KL-divergence term is added to the loss, which prevents each single normal distribution from deviating too far from the unit normal distribution N(0,1).\nThe authors argue, that using the VAE-like formulation -- instead of directly predicting an output vector (via FC layers) -- compensated for the lack of labels (smoother manifold).\nNote: This way of generating text embeddings seems very simple. (No recurrence, only about two layers.) It probably won't do much more than just roughly checking for the existence of specific words and word combinations (e.g. \"red head\").\n\n Input to that vector: Unclear. (Concatenated word vectors? Seems to not be described in the text.) The input is transformed to a vector via a fully connected layer (the text model is apparently not recurrent). The vector is transformed via fully connected layers to a mean vector and a sigma vector. These are then interpreted as normal distributions, from which the final output vector is sampled. This uses the reparameterization trick, similar to the method in VAEs. Just like in VAEs, a KL-divergence term is added to the loss, which prevents each single normal distribution from deviating too far from the unit normal distribution N(0,1). The authors argue, that using the VAE-like formulation -- instead of directly predicting an output vector (via FC layers) -- compensated for the lack of labels (smoother manifold). Note: This way of generating text embeddings seems very simple. (No recurrence, only about two layers.) It probably won't do much more than just roughly checking for the existence of specific words and word combinations (e.g. \"red head\"). Visualization of the architecture:\n\n\n\n  \nResults\n\nNote: No example images of the two-stage architecture for LSUN bedrooms.\nUsing only the first stage of the architecture (first G and D) reduces the Inception score significantly.\nAdding the text to both the first and second generator improves the Inception score slightly.\nAdding the VAE-like text embedding generation (as opposed to only FC layers) improves the Inception score slightly.\nGenerating images at higher resolution (256x256 instead of 128x128) improves the Inception score significantly\n\nNote: The 256x256 architecture has more residual convolutions than the 128x128 one.\nNote: The 128x128 and the 256x256 are both upscaled to 299x299 images before computing the Inception score. That should make the 128x128 images quite blurry and hence of low quality.\n\n\nExample images, with text and stage 1/2 results:\n\n\n\n\nMore examples of birds:\n\n\n\n\nExamples of failures:\n\n\nThe authors argue, that most failure cases happen when stage 1 messes up.\n\n\n\n Note: No example images of the two-stage architecture for LSUN bedrooms. Using only the first stage of the architecture (first G and D) reduces the Inception score significantly. Adding the text to both the first and second generator improves the Inception score slightly. Adding the VAE-like text embedding generation (as opposed to only FC layers) improves the Inception score slightly. Generating images at higher resolution (256x256 instead of 128x128) improves the Inception score significantly\n\nNote: The 256x256 architecture has more residual convolutions than the 128x128 one.\nNote: The 128x128 and the 256x256 are both upscaled to 299x299 images before computing the Inception score. That should make the 128x128 images quite blurry and hence of low quality.\n\n Note: The 256x256 architecture has more residual convolutions than the 128x128 one. Note: The 128x128 and the 256x256 are both upscaled to 299x299 images before computing the Inception score. That should make the 128x128 images quite blurry and hence of low quality. Example images, with text and stage 1/2 results:\n\n\n\n  More examples of birds:\n\n\n\n  Examples of failures:\n\n\nThe authors argue, that most failure cases happen when stage 1 messes up.\n\n  The authors argue, that most failure cases happen when stage 1 messes up. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.03242"
    },
    "36": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Self-Normalizing_Neural_Networks.md",
        "transcript": "\nWhat\n\nThey suggest a variation of ELUs, which leads to networks being automatically normalized.\nThe effects are comparable to Batch Normalization, while requiring significantly less computation (barely more than a normal ReLU).\n\n They suggest a variation of ELUs, which leads to networks being automatically normalized. The effects are comparable to Batch Normalization, while requiring significantly less computation (barely more than a normal ReLU). \nHow\n\nThey define Self-Normalizing Neural Networks (SNNs) as neural networks, which automatically keep their activations at zero-mean and unit-variance (per neuron).\nSELUs\n\nThey use SELUs to turn their networks into SNNs.\nFormula:\n\n\nwith alpha = 1.6733 and lambda = 1.0507.\n\n\nThey proof that with properly normalized weights the activations approach a fixed point of zero-mean and unit-variance. (Different settings for alpha and lambda can lead to other fixed points.)\nThey proof that this is still the case when previous layer activations and weights do not have optimal values.\nThey proof that this is still the case when the variance of previous layer activations is very high or very low and argue that the mean of those activations is not so important.\nHence, SELUs with these hyperparameters should have self-normalizing properties.\nSELUs are here used as a basis because:\n\nThey can have negative and positive values, which allows to control the mean.\nThey have saturating regions, which allows to dampen high variances from previous layers.\nThey have a slope larger than one, which allows to increase low variances from previous layers.\nThey generate a continuous curve, which ensures that there is a fixed point between variance damping and increasing.\n\n\nReLUs, Leaky ReLUs, Sigmoids and Tanhs do not offer the above properties.\n\n\nInitialization\n\nSELUs for SNNs work best with normalized weights.\nThey suggest to make sure per layer that:\n\nThe first moment (sum of weights) is zero.\nThe second moment (sum of squared weights) is one.\n\n\nThis can be done by drawing weights from a normal distribution N(0, 1/n), where n is the number of neurons in the layer.\n\n\nAlpha-dropout\n\nSELUs don't perform as well with normal Dropout, because their point of low variance is not 0.\nThey suggest a modification of Dropout called Alpha-dropout.\nIn this technique, values are not dropped to 0 but to alpha' = -lambda * alpha = -1.0507 * 1.6733 = -1.7581.\nSimilar to dropout, activations are changed during training to compensate for the dropped units.\nEach activation x is changed to a(xd+alpha'(1-d))+b.\n\nd = B(1, q) is the dropout variable consisting of 1s and 0s.\na = (q + alpha'^2 q(1-q))^(-1/2)\nb = -(q + alpha'^2 q(1-q))^(-1/2) ((1-q)alpha')\n\n\nThey made good experiences with dropout rates around 0.05 to 0.1.\n\n\n\n They define Self-Normalizing Neural Networks (SNNs) as neural networks, which automatically keep their activations at zero-mean and unit-variance (per neuron). SELUs\n\nThey use SELUs to turn their networks into SNNs.\nFormula:\n\n\nwith alpha = 1.6733 and lambda = 1.0507.\n\n\nThey proof that with properly normalized weights the activations approach a fixed point of zero-mean and unit-variance. (Different settings for alpha and lambda can lead to other fixed points.)\nThey proof that this is still the case when previous layer activations and weights do not have optimal values.\nThey proof that this is still the case when the variance of previous layer activations is very high or very low and argue that the mean of those activations is not so important.\nHence, SELUs with these hyperparameters should have self-normalizing properties.\nSELUs are here used as a basis because:\n\nThey can have negative and positive values, which allows to control the mean.\nThey have saturating regions, which allows to dampen high variances from previous layers.\nThey have a slope larger than one, which allows to increase low variances from previous layers.\nThey generate a continuous curve, which ensures that there is a fixed point between variance damping and increasing.\n\n\nReLUs, Leaky ReLUs, Sigmoids and Tanhs do not offer the above properties.\n\n They use SELUs to turn their networks into SNNs. Formula:\n\n\nwith alpha = 1.6733 and lambda = 1.0507.\n\n  with alpha = 1.6733 and lambda = 1.0507. They proof that with properly normalized weights the activations approach a fixed point of zero-mean and unit-variance. (Different settings for alpha and lambda can lead to other fixed points.) They proof that this is still the case when previous layer activations and weights do not have optimal values. They proof that this is still the case when the variance of previous layer activations is very high or very low and argue that the mean of those activations is not so important. Hence, SELUs with these hyperparameters should have self-normalizing properties. SELUs are here used as a basis because:\n\nThey can have negative and positive values, which allows to control the mean.\nThey have saturating regions, which allows to dampen high variances from previous layers.\nThey have a slope larger than one, which allows to increase low variances from previous layers.\nThey generate a continuous curve, which ensures that there is a fixed point between variance damping and increasing.\n\n They can have negative and positive values, which allows to control the mean. They have saturating regions, which allows to dampen high variances from previous layers. They have a slope larger than one, which allows to increase low variances from previous layers. They generate a continuous curve, which ensures that there is a fixed point between variance damping and increasing. ReLUs, Leaky ReLUs, Sigmoids and Tanhs do not offer the above properties. Initialization\n\nSELUs for SNNs work best with normalized weights.\nThey suggest to make sure per layer that:\n\nThe first moment (sum of weights) is zero.\nThe second moment (sum of squared weights) is one.\n\n\nThis can be done by drawing weights from a normal distribution N(0, 1/n), where n is the number of neurons in the layer.\n\n SELUs for SNNs work best with normalized weights. They suggest to make sure per layer that:\n\nThe first moment (sum of weights) is zero.\nThe second moment (sum of squared weights) is one.\n\n The first moment (sum of weights) is zero. The second moment (sum of squared weights) is one. This can be done by drawing weights from a normal distribution N(0, 1/n), where n is the number of neurons in the layer. Alpha-dropout\n\nSELUs don't perform as well with normal Dropout, because their point of low variance is not 0.\nThey suggest a modification of Dropout called Alpha-dropout.\nIn this technique, values are not dropped to 0 but to alpha' = -lambda * alpha = -1.0507 * 1.6733 = -1.7581.\nSimilar to dropout, activations are changed during training to compensate for the dropped units.\nEach activation x is changed to a(xd+alpha'(1-d))+b.\n\nd = B(1, q) is the dropout variable consisting of 1s and 0s.\na = (q + alpha'^2 q(1-q))^(-1/2)\nb = -(q + alpha'^2 q(1-q))^(-1/2) ((1-q)alpha')\n\n\nThey made good experiences with dropout rates around 0.05 to 0.1.\n\n SELUs don't perform as well with normal Dropout, because their point of low variance is not 0. They suggest a modification of Dropout called Alpha-dropout. In this technique, values are not dropped to 0 but to alpha' = -lambda * alpha = -1.0507 * 1.6733 = -1.7581. Similar to dropout, activations are changed during training to compensate for the dropped units. Each activation x is changed to a(xd+alpha'(1-d))+b.\n\nd = B(1, q) is the dropout variable consisting of 1s and 0s.\na = (q + alpha'^2 q(1-q))^(-1/2)\nb = -(q + alpha'^2 q(1-q))^(-1/2) ((1-q)alpha')\n\n d = B(1, q) is the dropout variable consisting of 1s and 0s. a = (q + alpha'^2 q(1-q))^(-1/2) b = -(q + alpha'^2 q(1-q))^(-1/2) ((1-q)alpha') They made good experiences with dropout rates around 0.05 to 0.1. \nResults\n\nNote: All of their tests are with fully connected networks. No convolutions.\nExample training results:\n\n\nLeft: MNIST, Right: CIFAR10\nNetworks have N layers each, see legend. No convolutions.\n\n\n121 UCI Tasks\n\nThey manage to beat SVMs and RandomForests, while other networks (Layer Normalization, BN, Weight Normalization, Highway Networks, ResNet) perform significantly worse than their network (and usually don't beat SVMs/RFs).\n\n\nTox21\n\nThey achieve better results than other networks (again, Layer Normalization, BN, etc.).\nThey achive almost the same result as the so far best model on the dataset, which consists of a mixture of neural networks, SVMs and Random Forests.\n\n\nHTRU2\n\nThey achieve better results than other networks.\nThey beat the best non-neural method (Naive Bayes).\n\n\nAmong all tested other networks, MSRAinit performs best, which references a network withput any normalization, only ReLUs and Microsoft Weight Initialization (see paper: Delving deep into rectifiers: Surpassing human-level performance on imagenet classification).\n\n Note: All of their tests are with fully connected networks. No convolutions. Example training results:\n\n\nLeft: MNIST, Right: CIFAR10\nNetworks have N layers each, see legend. No convolutions.\n\n  Left: MNIST, Right: CIFAR10 Networks have N layers each, see legend. No convolutions. 121 UCI Tasks\n\nThey manage to beat SVMs and RandomForests, while other networks (Layer Normalization, BN, Weight Normalization, Highway Networks, ResNet) perform significantly worse than their network (and usually don't beat SVMs/RFs).\n\n They manage to beat SVMs and RandomForests, while other networks (Layer Normalization, BN, Weight Normalization, Highway Networks, ResNet) perform significantly worse than their network (and usually don't beat SVMs/RFs). Tox21\n\nThey achieve better results than other networks (again, Layer Normalization, BN, etc.).\nThey achive almost the same result as the so far best model on the dataset, which consists of a mixture of neural networks, SVMs and Random Forests.\n\n They achieve better results than other networks (again, Layer Normalization, BN, etc.). They achive almost the same result as the so far best model on the dataset, which consists of a mixture of neural networks, SVMs and Random Forests. HTRU2\n\nThey achieve better results than other networks.\nThey beat the best non-neural method (Naive Bayes).\n\n They achieve better results than other networks. They beat the best non-neural method (Naive Bayes). Among all tested other networks, MSRAinit performs best, which references a network withput any normalization, only ReLUs and Microsoft Weight Initialization (see paper: Delving deep into rectifiers: Surpassing human-level performance on imagenet classification). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.02515"
    },
    "37": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/WGAN.md",
        "transcript": "\nWhat\n\nThey suggest a slightly altered algorithm for GANs.\nThe new algorithm is more stable than previous ones.\n\n They suggest a slightly altered algorithm for GANs. The new algorithm is more stable than previous ones. \nHow\n\nEach GAN contains a Generator that generates (fake-)examples and a Discriminator that discriminates between fake and real examples.\nBoth fake and real examples can be interpreted as coming from a probability distribution.\nThe basis of each GAN algorithm is to somehow measure the difference between these probability distributions\nand change the network parameters of G so that the fake-distribution becomes more and more similar to the real distribution.\nThere are multiple distance measures to do that:\n\nTotal Variation (TV)\nKL-Divergence (KL)\nJensen-Shannon divergence (JS)\n\nThis one is based on the KL-Divergence and is the basis of the original GAN, as well as LAPGAN and DCGAN.\n\n\nEarth-Mover distance (EM), aka Wasserstein-1\n\nIntuitively, one can imagine both probability distributions as hilly surfaces. EM then reflects, how much mass has to be moved to convert the fake distribution to the real one.\n\n\n\n\nIdeally, a distance measure has everywhere nice values and gradients\n(e.g. no +/- infinity values; no binary 0 or 1 gradients; gradients that get continously smaller when the generator produces good outputs).\nIn that regard, EM beats JS and JS beats TV and KL (roughly speaking). So they use EM.\nEM\n\nEM is defined as\n\n\n(inf = infinum, more or less a minimum)\n\n\nwhich is intractable, but following the Kantorovich-Rubinstein duality it can also be calculated via\n\n\n(sup = supremum, more or less a maximum)\n\n\nHowever, the second formula is here only valid if the network is a K-Lipschitz function (under every set of parameters).\nThis can be guaranteed by simply clipping the discriminator's weights to the range [-0.01, 0.01].\nThen in practice the following version of the tractable EM is used, where w are the parameters of the discriminator:\n\n\n\n\n\n\nThe full algorithm is mostly the same as for DCGAN:\n\n\nLine 2 leads to training the discriminator multiple times per batch (i.e. more often than the generator).\n\nThis is similar to the max w in W in the third formula (above).\nThis was already part of the original GAN algorithm, but is here more actively used.\nBecause of the EM distance, even a \"perfect\" discriminator still gives good gradient (in contrast to e.g. JS, where the discriminator should not be too far ahead). So the discriminator can be safely trained more often than the generator.\n\n\nLine 5 and 10 are derived from EM. Note that there is no more Sigmoid at the end of the discriminator!\nLine 7 is derived from the K-Lipschitz requirement (clipping of weights).\nHigh learning rates or using momentum-based optimizers (e.g. Adam) made the training unstable, which is why they use a small learning rate with RMSprop.\n\n\n\n Each GAN contains a Generator that generates (fake-)examples and a Discriminator that discriminates between fake and real examples. Both fake and real examples can be interpreted as coming from a probability distribution. The basis of each GAN algorithm is to somehow measure the difference between these probability distributions\nand change the network parameters of G so that the fake-distribution becomes more and more similar to the real distribution. There are multiple distance measures to do that:\n\nTotal Variation (TV)\nKL-Divergence (KL)\nJensen-Shannon divergence (JS)\n\nThis one is based on the KL-Divergence and is the basis of the original GAN, as well as LAPGAN and DCGAN.\n\n\nEarth-Mover distance (EM), aka Wasserstein-1\n\nIntuitively, one can imagine both probability distributions as hilly surfaces. EM then reflects, how much mass has to be moved to convert the fake distribution to the real one.\n\n\n\n Total Variation (TV) KL-Divergence (KL) Jensen-Shannon divergence (JS)\n\nThis one is based on the KL-Divergence and is the basis of the original GAN, as well as LAPGAN and DCGAN.\n\n This one is based on the KL-Divergence and is the basis of the original GAN, as well as LAPGAN and DCGAN. Earth-Mover distance (EM), aka Wasserstein-1\n\nIntuitively, one can imagine both probability distributions as hilly surfaces. EM then reflects, how much mass has to be moved to convert the fake distribution to the real one.\n\n Intuitively, one can imagine both probability distributions as hilly surfaces. EM then reflects, how much mass has to be moved to convert the fake distribution to the real one. Ideally, a distance measure has everywhere nice values and gradients\n(e.g. no +/- infinity values; no binary 0 or 1 gradients; gradients that get continously smaller when the generator produces good outputs). In that regard, EM beats JS and JS beats TV and KL (roughly speaking). So they use EM. EM\n\nEM is defined as\n\n\n(inf = infinum, more or less a minimum)\n\n\nwhich is intractable, but following the Kantorovich-Rubinstein duality it can also be calculated via\n\n\n(sup = supremum, more or less a maximum)\n\n\nHowever, the second formula is here only valid if the network is a K-Lipschitz function (under every set of parameters).\nThis can be guaranteed by simply clipping the discriminator's weights to the range [-0.01, 0.01].\nThen in practice the following version of the tractable EM is used, where w are the parameters of the discriminator:\n\n\n\n\n\n EM is defined as\n\n\n(inf = infinum, more or less a minimum)\n\n  (inf = infinum, more or less a minimum) which is intractable, but following the Kantorovich-Rubinstein duality it can also be calculated via\n\n\n(sup = supremum, more or less a maximum)\n\n  (sup = supremum, more or less a maximum) However, the second formula is here only valid if the network is a K-Lipschitz function (under every set of parameters). This can be guaranteed by simply clipping the discriminator's weights to the range [-0.01, 0.01]. Then in practice the following version of the tractable EM is used, where w are the parameters of the discriminator:\n\n\n\n  The full algorithm is mostly the same as for DCGAN:\n\n\nLine 2 leads to training the discriminator multiple times per batch (i.e. more often than the generator).\n\nThis is similar to the max w in W in the third formula (above).\nThis was already part of the original GAN algorithm, but is here more actively used.\nBecause of the EM distance, even a \"perfect\" discriminator still gives good gradient (in contrast to e.g. JS, where the discriminator should not be too far ahead). So the discriminator can be safely trained more often than the generator.\n\n\nLine 5 and 10 are derived from EM. Note that there is no more Sigmoid at the end of the discriminator!\nLine 7 is derived from the K-Lipschitz requirement (clipping of weights).\nHigh learning rates or using momentum-based optimizers (e.g. Adam) made the training unstable, which is why they use a small learning rate with RMSprop.\n\n  Line 2 leads to training the discriminator multiple times per batch (i.e. more often than the generator).\n\nThis is similar to the max w in W in the third formula (above).\nThis was already part of the original GAN algorithm, but is here more actively used.\nBecause of the EM distance, even a \"perfect\" discriminator still gives good gradient (in contrast to e.g. JS, where the discriminator should not be too far ahead). So the discriminator can be safely trained more often than the generator.\n\n This is similar to the max w in W in the third formula (above). This was already part of the original GAN algorithm, but is here more actively used. Because of the EM distance, even a \"perfect\" discriminator still gives good gradient (in contrast to e.g. JS, where the discriminator should not be too far ahead). So the discriminator can be safely trained more often than the generator. Line 5 and 10 are derived from EM. Note that there is no more Sigmoid at the end of the discriminator! Line 7 is derived from the K-Lipschitz requirement (clipping of weights). High learning rates or using momentum-based optimizers (e.g. Adam) made the training unstable, which is why they use a small learning rate with RMSprop. \nResults\n\nImproved stability. The method converges to decent images with models which failed completely when using JS-divergence (like in DCGAN).\n\nFor example, WGAN worked with generators that did not have batch normalization or only consisted of fully connected layers.\n\n\nApparently no more mode collapse. (Mode collapse in GANs = the generator starts to generate often/always the practically same image, independent of the noise input.)\nThere is a relationship between loss and image quality. Lower loss (at the generator) indicates higher image quality. Such a relationship did not exist for JS divergence.\nExample images:\n\n\n\n\n\n Improved stability. The method converges to decent images with models which failed completely when using JS-divergence (like in DCGAN).\n\nFor example, WGAN worked with generators that did not have batch normalization or only consisted of fully connected layers.\n\n For example, WGAN worked with generators that did not have batch normalization or only consisted of fully connected layers. Apparently no more mode collapse. (Mode collapse in GANs = the generator starts to generate often/always the practically same image, independent of the noise input.) There is a relationship between loss and image quality. Lower loss (at the generator) indicates higher image quality. Such a relationship did not exist for JS divergence. Example images:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1701.07875"
    },
    "38": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/YOLO9000.md",
        "transcript": "\nWhat\n\nThey suggest a new version of YOLO, a model to detect bounding boxes in images.\nTheir new version is more accurate, faster and is trained to recognize up to 9000 classes.\n\n They suggest a new version of YOLO, a model to detect bounding boxes in images. Their new version is more accurate, faster and is trained to recognize up to 9000 classes. \nHow\n\nTheir base model is the previous YOLOv1, which they improve here.\nAccuracy improvements\n\nThey add batch normalization to the network.\nPretraining usually happens on ImageNet at 224x224, fine tuning for bounding box detection then on another dataset, say Pascal VOC 2012, at higher resolutions, e.g. 448x448 in the case of YOLOv1.\nThis is problematic, because the pretrained network has to learn to deal with higher resolutions and a new task at the same time.\nThey instead first pretrain on low resolution ImageNet examples, then on higher resolution ImegeNet examples and only then switch to bounding box detection.\nThat improves their accuracy by about 4 percentage points mAP.\nThey switch to anchor boxes, similar to Faster R-CNN. That's largely the same as in YOLOv1. Classification is now done per tested anchor box shape, instead of per grid cell.\nThe regression of x/y-coordinates is now a bit smarter and uses sigmoids to only translate a box within a grid cell.\nIn Faster R-CNN the anchor box shapes are manually chosen (e.g. small squared boxes, large squared boxes, thin but high boxes, ...).\nHere instead they learn these shapes from data.\nThat is done by applying k-Means to the bounding boxes in a dataset.\nThey cluster them into k=5 clusters and then use the centroids as anchor box shapes.\nTheir accuracy this way is the same as with 9 manually chosen anchor boxes.\n(Using k=9 further increases their accuracy significantly, but also increases model complexity. As they want to predict 9000 classes they stay with k=5.)\nTo better predict small bounding boxes, they add a pass-through connection from a higher resolution layer to the end of the network.\nThey train their network now at multiple scales. (As the network is now fully convolutional, they can easily do that.)\n\n\nSpeed improvements\n\nThey get rid of their fully connected layers. Instead the network is now fully convolutional.\nThey have also removed a handful or so of their convolutional layers.\n\n\nCapability improvement (weakly supervised learning)\n\nThey suggest a method to predict bounding boxes of the 9000 most common classes in ImageNet.\nThey add a few more abstract classes to that (e.g. dog for all breeds of dogs) and arrive at over 9000 classes (9418 to be precise).\nThey train on ImageNet and MSCOCO.\nImageNet only contains class labels, no bounding boxes. MSCOCO only contains general classes (e.g. \"dog\" instead of the specific breed).\nThey train iteratively on both datasets. MSCOCO is used for detection and classification, while ImageNet is only used for classification.\nFor an ImageNet example of class c, they search among the predicted bounding boxes for the one that has highest predicted probability of being c\nand backpropagate only the classification loss for that box.\nIn order to compensate the problem of different abstraction levels on the classes (e.g. \"dog\" vs a specific breed), they make use of WordNet.\nBased on that data they generate a hierarchy/tree of classes, e.g. one path through that tree could be: object -> animal -> canine -> dog -> hunting dog -> terrier -> yorkshire terrier.\nThey let the network predict paths in that hierarchy, so that the prediction \"dog\" for a specific dog breed is not completely wrong.\nVisualization of the hierarchy:\n\n\n\n\nThey predict many small softmaxes for the paths in the hierarchy, one per node:\n\n\n\n\n\n\n\n Their base model is the previous YOLOv1, which they improve here. Accuracy improvements\n\nThey add batch normalization to the network.\nPretraining usually happens on ImageNet at 224x224, fine tuning for bounding box detection then on another dataset, say Pascal VOC 2012, at higher resolutions, e.g. 448x448 in the case of YOLOv1.\nThis is problematic, because the pretrained network has to learn to deal with higher resolutions and a new task at the same time.\nThey instead first pretrain on low resolution ImageNet examples, then on higher resolution ImegeNet examples and only then switch to bounding box detection.\nThat improves their accuracy by about 4 percentage points mAP.\nThey switch to anchor boxes, similar to Faster R-CNN. That's largely the same as in YOLOv1. Classification is now done per tested anchor box shape, instead of per grid cell.\nThe regression of x/y-coordinates is now a bit smarter and uses sigmoids to only translate a box within a grid cell.\nIn Faster R-CNN the anchor box shapes are manually chosen (e.g. small squared boxes, large squared boxes, thin but high boxes, ...).\nHere instead they learn these shapes from data.\nThat is done by applying k-Means to the bounding boxes in a dataset.\nThey cluster them into k=5 clusters and then use the centroids as anchor box shapes.\nTheir accuracy this way is the same as with 9 manually chosen anchor boxes.\n(Using k=9 further increases their accuracy significantly, but also increases model complexity. As they want to predict 9000 classes they stay with k=5.)\nTo better predict small bounding boxes, they add a pass-through connection from a higher resolution layer to the end of the network.\nThey train their network now at multiple scales. (As the network is now fully convolutional, they can easily do that.)\n\n They add batch normalization to the network. Pretraining usually happens on ImageNet at 224x224, fine tuning for bounding box detection then on another dataset, say Pascal VOC 2012, at higher resolutions, e.g. 448x448 in the case of YOLOv1.\nThis is problematic, because the pretrained network has to learn to deal with higher resolutions and a new task at the same time.\nThey instead first pretrain on low resolution ImageNet examples, then on higher resolution ImegeNet examples and only then switch to bounding box detection.\nThat improves their accuracy by about 4 percentage points mAP. They switch to anchor boxes, similar to Faster R-CNN. That's largely the same as in YOLOv1. Classification is now done per tested anchor box shape, instead of per grid cell.\nThe regression of x/y-coordinates is now a bit smarter and uses sigmoids to only translate a box within a grid cell. In Faster R-CNN the anchor box shapes are manually chosen (e.g. small squared boxes, large squared boxes, thin but high boxes, ...).\nHere instead they learn these shapes from data.\nThat is done by applying k-Means to the bounding boxes in a dataset.\nThey cluster them into k=5 clusters and then use the centroids as anchor box shapes.\nTheir accuracy this way is the same as with 9 manually chosen anchor boxes.\n(Using k=9 further increases their accuracy significantly, but also increases model complexity. As they want to predict 9000 classes they stay with k=5.) To better predict small bounding boxes, they add a pass-through connection from a higher resolution layer to the end of the network. They train their network now at multiple scales. (As the network is now fully convolutional, they can easily do that.) Speed improvements\n\nThey get rid of their fully connected layers. Instead the network is now fully convolutional.\nThey have also removed a handful or so of their convolutional layers.\n\n They get rid of their fully connected layers. Instead the network is now fully convolutional. They have also removed a handful or so of their convolutional layers. Capability improvement (weakly supervised learning)\n\nThey suggest a method to predict bounding boxes of the 9000 most common classes in ImageNet.\nThey add a few more abstract classes to that (e.g. dog for all breeds of dogs) and arrive at over 9000 classes (9418 to be precise).\nThey train on ImageNet and MSCOCO.\nImageNet only contains class labels, no bounding boxes. MSCOCO only contains general classes (e.g. \"dog\" instead of the specific breed).\nThey train iteratively on both datasets. MSCOCO is used for detection and classification, while ImageNet is only used for classification.\nFor an ImageNet example of class c, they search among the predicted bounding boxes for the one that has highest predicted probability of being c\nand backpropagate only the classification loss for that box.\nIn order to compensate the problem of different abstraction levels on the classes (e.g. \"dog\" vs a specific breed), they make use of WordNet.\nBased on that data they generate a hierarchy/tree of classes, e.g. one path through that tree could be: object -> animal -> canine -> dog -> hunting dog -> terrier -> yorkshire terrier.\nThey let the network predict paths in that hierarchy, so that the prediction \"dog\" for a specific dog breed is not completely wrong.\nVisualization of the hierarchy:\n\n\n\n\nThey predict many small softmaxes for the paths in the hierarchy, one per node:\n\n\n\n\n\n They suggest a method to predict bounding boxes of the 9000 most common classes in ImageNet.\nThey add a few more abstract classes to that (e.g. dog for all breeds of dogs) and arrive at over 9000 classes (9418 to be precise). They train on ImageNet and MSCOCO. ImageNet only contains class labels, no bounding boxes. MSCOCO only contains general classes (e.g. \"dog\" instead of the specific breed). They train iteratively on both datasets. MSCOCO is used for detection and classification, while ImageNet is only used for classification.\nFor an ImageNet example of class c, they search among the predicted bounding boxes for the one that has highest predicted probability of being c\nand backpropagate only the classification loss for that box. In order to compensate the problem of different abstraction levels on the classes (e.g. \"dog\" vs a specific breed), they make use of WordNet.\nBased on that data they generate a hierarchy/tree of classes, e.g. one path through that tree could be: object -> animal -> canine -> dog -> hunting dog -> terrier -> yorkshire terrier.\nThey let the network predict paths in that hierarchy, so that the prediction \"dog\" for a specific dog breed is not completely wrong. Visualization of the hierarchy:\n\n\n\n  They predict many small softmaxes for the paths in the hierarchy, one per node:\n\n\n\n  \nResults\n\nAccuracy\n\nThey reach about 73.4 mAP when training on Pascal VOC 2007 and 2012. That's slightly behind Faster R-CNN with VGG16 with 75.9 mAP, trained on MSCOCO+2007+2012.\n\n\nSpeed\n\nThey reach 91 fps (10ms/image) at image resolution 288x288 and 40 fps (25ms/image) at 544x544.\n\n\nWeakly supervised learning\n\nThey test their 9000-class-detection on ImageNet's detection task, which contains bounding boxes for 200 object classes.\nThey achieve 19.7 mAP for all classes and 16.0% mAP for the 156 classes which are not part of MSCOCO.\nFor some classes they get 0 mAP accuracy.\nThe system performs well for all kinds of animals, but struggles with not-living objects, like sunglasses.\nExample images (notice the class labels):\n\n\n\n\n\n\n\n Accuracy\n\nThey reach about 73.4 mAP when training on Pascal VOC 2007 and 2012. That's slightly behind Faster R-CNN with VGG16 with 75.9 mAP, trained on MSCOCO+2007+2012.\n\n They reach about 73.4 mAP when training on Pascal VOC 2007 and 2012. That's slightly behind Faster R-CNN with VGG16 with 75.9 mAP, trained on MSCOCO+2007+2012. Speed\n\nThey reach 91 fps (10ms/image) at image resolution 288x288 and 40 fps (25ms/image) at 544x544.\n\n They reach 91 fps (10ms/image) at image resolution 288x288 and 40 fps (25ms/image) at 544x544. Weakly supervised learning\n\nThey test their 9000-class-detection on ImageNet's detection task, which contains bounding boxes for 200 object classes.\nThey achieve 19.7 mAP for all classes and 16.0% mAP for the 156 classes which are not part of MSCOCO.\nFor some classes they get 0 mAP accuracy.\nThe system performs well for all kinds of animals, but struggles with not-living objects, like sunglasses.\nExample images (notice the class labels):\n\n\n\n\n\n They test their 9000-class-detection on ImageNet's detection task, which contains bounding boxes for 200 object classes. They achieve 19.7 mAP for all classes and 16.0% mAP for the 156 classes which are not part of MSCOCO. For some classes they get 0 mAP accuracy. The system performs well for all kinds of animals, but struggles with not-living objects, like sunglasses. Example images (notice the class labels):\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.08242"
    },
    "39": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/YOLO.md",
        "transcript": "What\n\nThey suggest a model (\"YOLO\") to detect bounding boxes in images.\nIn comparison to Faster R-CNN, this model is faster but less accurate.\n\n They suggest a model (\"YOLO\") to detect bounding boxes in images. In comparison to Faster R-CNN, this model is faster but less accurate. How\n\nArchitecture\n\nInput are images with a resolution of 448x448.\nOutput are S*S*(B*5 + C) values (per image).\n\nS is the grid size (default value: 7). Each image is split up into S*S cells.\nB is the number of \"tested\" bounding box shapes at each cell (default value: 2).\nSo at each cell, the network might try one large and one small bounding box.\nThe network predicts additionally for each such tested bounding box 5 values.\nThese cover the exact position (x, y) and scale (height, width) of the bounding box as well as a confidence value.\nThey allow the network to fine tune the bounding box shape and reject it, e.g. if there is no object in the grid cell.\nThe confidence value is zero if there is no object in the grid cell and otherwise matches the IoU between predicted and true bounding box.\nC is the number of classes in the dataset (e.g. 20 in Pascal VOC). For each grid cell, the model decides once to which of the C objects the cell belongs.\n\n\nRough overview of their outputs:\n\n\n\n\nIn contrast to Faster R-CNN, their model does not use a separate region proposal network (RPN).\nPer bounding box they actually predict the square root of height and width instead of the raw values.\nThat is supposed to result in similar errors/losses for small and big bounding boxes.\nThey use a total of 24 convolutional layers and 2 fully connected layers.\n\nSome of these convolutional layers are 1x1-convs that halve the number of channels (followed by 3x3s that double them again).\nOverview of the architecture:\n\n\n\n\n\n\nThey use Leaky ReLUs (alpha=0.1) throughout the network. The last layer uses linear activations (apparently even for the class prediction...!?).\nSimilarly to Faster R-CNN, they use a non maximum suppression that drops predicted bounding boxes if they are too similar to other predictions.\n\n\nTraining\n\nThey pretrain their network on ImageNet, then finetune on Pascal VOC.\nLoss\n\nThey use sum-squared losses (apparently even for the classification, i.e. the C values).\nThey dont propagate classification loss (for C) for grid cells that don't contain an object.\nFor each grid grid cell they \"test\" B example shapes of bounding boxes (see above).\nAmong these B shapes, they only propagate the bounding box losses (regarding x, y, width, height, confidence) for the shape that has highest IoU with a ground truth bounding box.\nMost grid cells don't contain a bounding box. Their confidence values will all be zero, potentialle dominating the total loss.\nTo prevent that, the weighting of the confidence values in the loss function is reduced relative to the regression components (x, y, height, width).\n\n\n\n\n\n Architecture\n\nInput are images with a resolution of 448x448.\nOutput are S*S*(B*5 + C) values (per image).\n\nS is the grid size (default value: 7). Each image is split up into S*S cells.\nB is the number of \"tested\" bounding box shapes at each cell (default value: 2).\nSo at each cell, the network might try one large and one small bounding box.\nThe network predicts additionally for each such tested bounding box 5 values.\nThese cover the exact position (x, y) and scale (height, width) of the bounding box as well as a confidence value.\nThey allow the network to fine tune the bounding box shape and reject it, e.g. if there is no object in the grid cell.\nThe confidence value is zero if there is no object in the grid cell and otherwise matches the IoU between predicted and true bounding box.\nC is the number of classes in the dataset (e.g. 20 in Pascal VOC). For each grid cell, the model decides once to which of the C objects the cell belongs.\n\n\nRough overview of their outputs:\n\n\n\n\nIn contrast to Faster R-CNN, their model does not use a separate region proposal network (RPN).\nPer bounding box they actually predict the square root of height and width instead of the raw values.\nThat is supposed to result in similar errors/losses for small and big bounding boxes.\nThey use a total of 24 convolutional layers and 2 fully connected layers.\n\nSome of these convolutional layers are 1x1-convs that halve the number of channels (followed by 3x3s that double them again).\nOverview of the architecture:\n\n\n\n\n\n\nThey use Leaky ReLUs (alpha=0.1) throughout the network. The last layer uses linear activations (apparently even for the class prediction...!?).\nSimilarly to Faster R-CNN, they use a non maximum suppression that drops predicted bounding boxes if they are too similar to other predictions.\n\n Input are images with a resolution of 448x448. Output are S*S*(B*5 + C) values (per image).\n\nS is the grid size (default value: 7). Each image is split up into S*S cells.\nB is the number of \"tested\" bounding box shapes at each cell (default value: 2).\nSo at each cell, the network might try one large and one small bounding box.\nThe network predicts additionally for each such tested bounding box 5 values.\nThese cover the exact position (x, y) and scale (height, width) of the bounding box as well as a confidence value.\nThey allow the network to fine tune the bounding box shape and reject it, e.g. if there is no object in the grid cell.\nThe confidence value is zero if there is no object in the grid cell and otherwise matches the IoU between predicted and true bounding box.\nC is the number of classes in the dataset (e.g. 20 in Pascal VOC). For each grid cell, the model decides once to which of the C objects the cell belongs.\n\n S is the grid size (default value: 7). Each image is split up into S*S cells. B is the number of \"tested\" bounding box shapes at each cell (default value: 2).\nSo at each cell, the network might try one large and one small bounding box.\nThe network predicts additionally for each such tested bounding box 5 values.\nThese cover the exact position (x, y) and scale (height, width) of the bounding box as well as a confidence value.\nThey allow the network to fine tune the bounding box shape and reject it, e.g. if there is no object in the grid cell.\nThe confidence value is zero if there is no object in the grid cell and otherwise matches the IoU between predicted and true bounding box. C is the number of classes in the dataset (e.g. 20 in Pascal VOC). For each grid cell, the model decides once to which of the C objects the cell belongs. Rough overview of their outputs:\n\n\n\n  In contrast to Faster R-CNN, their model does not use a separate region proposal network (RPN). Per bounding box they actually predict the square root of height and width instead of the raw values.\nThat is supposed to result in similar errors/losses for small and big bounding boxes. They use a total of 24 convolutional layers and 2 fully connected layers.\n\nSome of these convolutional layers are 1x1-convs that halve the number of channels (followed by 3x3s that double them again).\nOverview of the architecture:\n\n\n\n\n\n Some of these convolutional layers are 1x1-convs that halve the number of channels (followed by 3x3s that double them again). Overview of the architecture:\n\n\n\n  They use Leaky ReLUs (alpha=0.1) throughout the network. The last layer uses linear activations (apparently even for the class prediction...!?). Similarly to Faster R-CNN, they use a non maximum suppression that drops predicted bounding boxes if they are too similar to other predictions. Training\n\nThey pretrain their network on ImageNet, then finetune on Pascal VOC.\nLoss\n\nThey use sum-squared losses (apparently even for the classification, i.e. the C values).\nThey dont propagate classification loss (for C) for grid cells that don't contain an object.\nFor each grid grid cell they \"test\" B example shapes of bounding boxes (see above).\nAmong these B shapes, they only propagate the bounding box losses (regarding x, y, width, height, confidence) for the shape that has highest IoU with a ground truth bounding box.\nMost grid cells don't contain a bounding box. Their confidence values will all be zero, potentialle dominating the total loss.\nTo prevent that, the weighting of the confidence values in the loss function is reduced relative to the regression components (x, y, height, width).\n\n\n\n They pretrain their network on ImageNet, then finetune on Pascal VOC. Loss\n\nThey use sum-squared losses (apparently even for the classification, i.e. the C values).\nThey dont propagate classification loss (for C) for grid cells that don't contain an object.\nFor each grid grid cell they \"test\" B example shapes of bounding boxes (see above).\nAmong these B shapes, they only propagate the bounding box losses (regarding x, y, width, height, confidence) for the shape that has highest IoU with a ground truth bounding box.\nMost grid cells don't contain a bounding box. Their confidence values will all be zero, potentialle dominating the total loss.\nTo prevent that, the weighting of the confidence values in the loss function is reduced relative to the regression components (x, y, height, width).\n\n They use sum-squared losses (apparently even for the classification, i.e. the C values). They dont propagate classification loss (for C) for grid cells that don't contain an object. For each grid grid cell they \"test\" B example shapes of bounding boxes (see above).\nAmong these B shapes, they only propagate the bounding box losses (regarding x, y, width, height, confidence) for the shape that has highest IoU with a ground truth bounding box. Most grid cells don't contain a bounding box. Their confidence values will all be zero, potentialle dominating the total loss.\nTo prevent that, the weighting of the confidence values in the loss function is reduced relative to the regression components (x, y, height, width). Results\n\nThe coarse grid and B=2 setting lead to some problems. Namely, small objects are missed and bounding boxes can end up being dropped if they are too close to other bounding boxes.\nThe model also has problems with unusual bounding box shapes.\nOverall their accuracy is about 10 percentage points lower than Faster R-CNN with VGG16 (63.4% vs 73.2%, measured in mAP on Pascal VOC 2007).\nThey achieve 45fps (22ms/image), compared to 7fps (142ms/image) with Faster R-CNN + VGG16.\nOverview of results on Pascal VOC 2012:\n\n\n\n\nThey also suggest a faster variation of their model which reached 145fps (7ms/image) at a further drop of 10 percentage points mAP (to 52.7%).\nA significant part of their error seems to come from badly placed or sized bounding boxes (e.g. too wide or too much to the right).\nThey mistake background less often for objects than Fast R-CNN. They test combining both models with each other and can improve Fast R-CNN's accuracy by about 2.5 percentage points mAP.\nThey test their model on paintings/artwork (Picasso and People-Art datasets) and notice that it generalizes fairly well to that domain.\nExample results (notice the paintings at the top):\n\n\n\n\n\n The coarse grid and B=2 setting lead to some problems. Namely, small objects are missed and bounding boxes can end up being dropped if they are too close to other bounding boxes. The model also has problems with unusual bounding box shapes. Overall their accuracy is about 10 percentage points lower than Faster R-CNN with VGG16 (63.4% vs 73.2%, measured in mAP on Pascal VOC 2007). They achieve 45fps (22ms/image), compared to 7fps (142ms/image) with Faster R-CNN + VGG16. Overview of results on Pascal VOC 2012:\n\n\n\n  They also suggest a faster variation of their model which reached 145fps (7ms/image) at a further drop of 10 percentage points mAP (to 52.7%). A significant part of their error seems to come from badly placed or sized bounding boxes (e.g. too wide or too much to the right). They mistake background less often for objects than Fast R-CNN. They test combining both models with each other and can improve Fast R-CNN's accuracy by about 2.5 percentage points mAP. They test their model on paintings/artwork (Picasso and People-Art datasets) and notice that it generalizes fairly well to that domain. Example results (notice the paintings at the top):\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.02640"
    },
    "40": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/PVANET.md",
        "transcript": "What\n\nThey present a variation of Faster R-CNN.\n\nFaster R-CNN is a model that detects bounding boxes in images.\n\n\nTheir variation is about as accurate as the best performing versions of Faster R-CNN.\nTheir variation is significantly faster than these variations (roughly 50ms per image).\n\n They present a variation of Faster R-CNN.\n\nFaster R-CNN is a model that detects bounding boxes in images.\n\n Faster R-CNN is a model that detects bounding boxes in images. Their variation is about as accurate as the best performing versions of Faster R-CNN. Their variation is significantly faster than these variations (roughly 50ms per image). How\n\nPVANET reuses the standard Faster R-CNN architecture:\n\nA base network that transforms an image into a feature map.\nA region proposal network (RPN) that uses the feature map to predict bounding box candidates.\nA classifier that uses the feature map and the bounding box candidates to predict the final bounding boxes.\n\n\nPVANET modifies the base network and keeps the RPN and classifier the same.\nInception\n\nTheir base network uses eight Inception modules.\nThey argue that these are good choices here, because they are able to represent an image at different scales (aka at different receptive field sizes)\ndue to their mixture of 3x3 and 1x1 convolutions.\n\n\n\n\nRepresenting an image at different scales is useful here in order to detect both large and small bounding boxes.\nInception modules are also reasonably fast.\nVisualization of their Inception modules:\n\n\n\n\n\n\nConcatenated ReLUs\n\nBefore the eight Inception modules, they start the network with eight convolutions using concatenated ReLUs.\nThese CReLUs compute both the classic ReLU result (max(0, x)) and concatenate to that the negated result, i.e. something like f(x) = max(0, x <concat> (-1)*x).\nThat is done, because among the early one can often find pairs of convolution filters that are the negated variations of each other.\nSo by adding CReLUs, the network does not have to compute these any more, instead they are created (almost) for free, reducing the computation time by up to 50%.\nVisualization of their final CReLU block:\n\nTODO\n\n\n\n\n\nMulti-Scale output\n\nUsually one would generate the final feature map simply from the output of the last convolution.\nThey instead combine the outputs of three different convolutions, each resembling a different scale (or level of abstraction).\nThey take one from an early point of the network (downscaled), one from the middle part (kept the same) and one from the end (upscaled).\nThey concatenate these and apply a 1x1 convolution to generate the final output.\n\n\nOther stuff\n\nMost of their network uses residual connections (including the Inception modules) to facilitate learning.\nThey pretrain on ILSVRC2012 and then perform fine-tuning on MSCOCO, VOC 2007 and VOC 2012.\nThey use plateau detection for their learning rate, i.e. if a moving average of the loss does not improve any more, they decrease the learning rate. They say that this increases accuracy significantly.\nThe classifier in Faster R-CNN consists of fully connected layers. They compress these via Truncated SVD to speed things up. (That was already part of Fast R-CNN, I think.)\n\n\n\n PVANET reuses the standard Faster R-CNN architecture:\n\nA base network that transforms an image into a feature map.\nA region proposal network (RPN) that uses the feature map to predict bounding box candidates.\nA classifier that uses the feature map and the bounding box candidates to predict the final bounding boxes.\n\n A base network that transforms an image into a feature map. A region proposal network (RPN) that uses the feature map to predict bounding box candidates. A classifier that uses the feature map and the bounding box candidates to predict the final bounding boxes. PVANET modifies the base network and keeps the RPN and classifier the same. Inception\n\nTheir base network uses eight Inception modules.\nThey argue that these are good choices here, because they are able to represent an image at different scales (aka at different receptive field sizes)\ndue to their mixture of 3x3 and 1x1 convolutions.\n\n\n\n\nRepresenting an image at different scales is useful here in order to detect both large and small bounding boxes.\nInception modules are also reasonably fast.\nVisualization of their Inception modules:\n\n\n\n\n\n Their base network uses eight Inception modules. They argue that these are good choices here, because they are able to represent an image at different scales (aka at different receptive field sizes)\ndue to their mixture of 3x3 and 1x1 convolutions.\n\n\n\n  Representing an image at different scales is useful here in order to detect both large and small bounding boxes. Inception modules are also reasonably fast. Visualization of their Inception modules:\n\n\n\n  Concatenated ReLUs\n\nBefore the eight Inception modules, they start the network with eight convolutions using concatenated ReLUs.\nThese CReLUs compute both the classic ReLU result (max(0, x)) and concatenate to that the negated result, i.e. something like f(x) = max(0, x <concat> (-1)*x).\nThat is done, because among the early one can often find pairs of convolution filters that are the negated variations of each other.\nSo by adding CReLUs, the network does not have to compute these any more, instead they are created (almost) for free, reducing the computation time by up to 50%.\nVisualization of their final CReLU block:\n\nTODO\n\n\n\n\n Before the eight Inception modules, they start the network with eight convolutions using concatenated ReLUs. These CReLUs compute both the classic ReLU result (max(0, x)) and concatenate to that the negated result, i.e. something like f(x) = max(0, x <concat> (-1)*x). That is done, because among the early one can often find pairs of convolution filters that are the negated variations of each other.\nSo by adding CReLUs, the network does not have to compute these any more, instead they are created (almost) for free, reducing the computation time by up to 50%. Visualization of their final CReLU block:\n\nTODO\n\n\n TODO  Multi-Scale output\n\nUsually one would generate the final feature map simply from the output of the last convolution.\nThey instead combine the outputs of three different convolutions, each resembling a different scale (or level of abstraction).\nThey take one from an early point of the network (downscaled), one from the middle part (kept the same) and one from the end (upscaled).\nThey concatenate these and apply a 1x1 convolution to generate the final output.\n\n Usually one would generate the final feature map simply from the output of the last convolution. They instead combine the outputs of three different convolutions, each resembling a different scale (or level of abstraction). They take one from an early point of the network (downscaled), one from the middle part (kept the same) and one from the end (upscaled). They concatenate these and apply a 1x1 convolution to generate the final output. Other stuff\n\nMost of their network uses residual connections (including the Inception modules) to facilitate learning.\nThey pretrain on ILSVRC2012 and then perform fine-tuning on MSCOCO, VOC 2007 and VOC 2012.\nThey use plateau detection for their learning rate, i.e. if a moving average of the loss does not improve any more, they decrease the learning rate. They say that this increases accuracy significantly.\nThe classifier in Faster R-CNN consists of fully connected layers. They compress these via Truncated SVD to speed things up. (That was already part of Fast R-CNN, I think.)\n\n Most of their network uses residual connections (including the Inception modules) to facilitate learning. They pretrain on ILSVRC2012 and then perform fine-tuning on MSCOCO, VOC 2007 and VOC 2012. They use plateau detection for their learning rate, i.e. if a moving average of the loss does not improve any more, they decrease the learning rate. They say that this increases accuracy significantly. The classifier in Faster R-CNN consists of fully connected layers. They compress these via Truncated SVD to speed things up. (That was already part of Fast R-CNN, I think.) Results\n\nOn Pascal VOC 2012 they achieve 82.5% mAP at 46ms/image (Titan X GPU).\n\nFaster R-CNN + ResNet-101: 83.8% at 2.2s/image.\nFaster R-CNN + VGG16: 75.9% at 110ms/image.\nR-FCN + ResNet-101: 82.0% at 133ms/image.\n\n\nDecreasing the number of region proposals from 300 per image to 50 almost doubles the speed (to 27ms/image) at a small loss of 1.5 percentage points mAP.\nUsing Truncated SVD for the classifier reduces the required timer per image by about 30% at roughly 1 percentage point of mAP loss.\n\n On Pascal VOC 2012 they achieve 82.5% mAP at 46ms/image (Titan X GPU).\n\nFaster R-CNN + ResNet-101: 83.8% at 2.2s/image.\nFaster R-CNN + VGG16: 75.9% at 110ms/image.\nR-FCN + ResNet-101: 82.0% at 133ms/image.\n\n Faster R-CNN + ResNet-101: 83.8% at 2.2s/image. Faster R-CNN + VGG16: 75.9% at 110ms/image. R-FCN + ResNet-101: 82.0% at 133ms/image. Decreasing the number of region proposals from 300 per image to 50 almost doubles the speed (to 27ms/image) at a small loss of 1.5 percentage points mAP. Using Truncated SVD for the classifier reduces the required timer per image by about 30% at roughly 1 percentage point of mAP loss. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1608.08021"
    },
    "41": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/R-FCN.md",
        "transcript": "What\n\nThey present a variation of Faster R-CNN, i.e. a model that predicts bounding boxes in images and classifies them.\nIn contrast to Faster R-CNN, their model is fully convolutional.\nIn contrast to Faster R-CNN, the computation per bounding box candidate (region proposal) is very low.\n\n They present a variation of Faster R-CNN, i.e. a model that predicts bounding boxes in images and classifies them. In contrast to Faster R-CNN, their model is fully convolutional. In contrast to Faster R-CNN, the computation per bounding box candidate (region proposal) is very low. How\n\nThe basic architecture is the same as in Faster R-CNN:\n\nA base network transforms an image to a feature map. Here they use ResNet-101 to do that.\nA region proposal network (RPN) uses the feature map to locate bounding box candidates (\"region proposals\") in the image.\nA classifier uses the feature map and the bounding box candidates and classifies each one of them into C+1 classes,\nwhere C is the number of object classes to spot (e.g. \"person\", \"chair\", \"bottle\", ...) and 1 is added for the background.\n\nDuring that process, small subregions of the feature maps (those that match the bounding box candidates) must be extracted and converted to fixed-sizes matrices.\nThe method to do that is called \"Region of Interest Pooling\" (RoI-Pooling) and is based on max pooling.\nIt is mostly the same as in Faster R-CNN.\n\n\nVisualization of the basic architecture:\n\n\n\n\n\n\nPosition-sensitive classification\n\nFully convolutional bounding box detectors tend to not work well.\nThe authors argue, that the problems come from the translation-invariance of convolutions, which is a desirable property in the case of classification but not when precise localization of objects is required.\nThey tackle that problem by generating multiple heatmaps per object class, each one being slightly shifted (\"position-sensitive score maps\").\nMore precisely:\n\nThe classifier generates per object class c a total of k*k heatmaps.\nIn the simplest form k is equal to 1. Then only one heatmap is generated, which signals whether a pixel is part of an object of class c.\nThey use k=3*3. The first of those heatmaps signals, whether a pixel is part of the top left corner of a bounding box of class c. The second heatmap signals, whether a pixel is part of the top center of a bounding box of class c (and so on).\nThe RoI-Pooling is applied to these heatmaps.\nFor k=3*3, each bounding box candidate is converted to 3*3 values. The first one resembles the top left corner of the bounding box candidate. Its value is generated by taking the average of the values in that area in the first heatmap.\nOnce the 3*3 values are generated, the final score of class c for that bounding box candidate is computed by averaging the values.\nThat process is repeated for all classes and a softmax is used to determine the final class.\nThe graphic below shows examples for that:\n\n\n\n\n\n\nThe above described RoI-Pooling uses only averages and hence is almost (computationally) free.\n\nThey make use of that during the training by sampling many candidates and only backpropagating on those with high losses (online hard example mining, OHEM).\n\n\n\n\n\u00c0 trous trick\n\nIn order to increase accuracy for small bounding boxes they use the \u00e0 trous trick.\nThat means that they use a pretrained base network (here ResNet-101), then remove a pooling layer and set the \u00e0 trous rate (aka dilation) of all convolutions after the removed pooling layer to 2.\n\nThe \u00e1 trous rate describes the distance of sampling locations of a convolution. Usually that is 1 (sampled locations are right next to each other). If it is set to 2, there is one value \"skipped\" between each pair of neighbouring sampling location.\n\n\nBy doing that, the convolutions still behave as if the pooling layer existed (and therefore their weights can be reused). At the same time, they work at an increased resolution, making them more capable of classifying small objects. (Runtime increases though.)\n\n\nTraining of R-FCN happens similarly to Faster R-CNN.\n\n The basic architecture is the same as in Faster R-CNN:\n\nA base network transforms an image to a feature map. Here they use ResNet-101 to do that.\nA region proposal network (RPN) uses the feature map to locate bounding box candidates (\"region proposals\") in the image.\nA classifier uses the feature map and the bounding box candidates and classifies each one of them into C+1 classes,\nwhere C is the number of object classes to spot (e.g. \"person\", \"chair\", \"bottle\", ...) and 1 is added for the background.\n\nDuring that process, small subregions of the feature maps (those that match the bounding box candidates) must be extracted and converted to fixed-sizes matrices.\nThe method to do that is called \"Region of Interest Pooling\" (RoI-Pooling) and is based on max pooling.\nIt is mostly the same as in Faster R-CNN.\n\n\nVisualization of the basic architecture:\n\n\n\n\n\n A base network transforms an image to a feature map. Here they use ResNet-101 to do that. A region proposal network (RPN) uses the feature map to locate bounding box candidates (\"region proposals\") in the image. A classifier uses the feature map and the bounding box candidates and classifies each one of them into C+1 classes,\nwhere C is the number of object classes to spot (e.g. \"person\", \"chair\", \"bottle\", ...) and 1 is added for the background.\n\nDuring that process, small subregions of the feature maps (those that match the bounding box candidates) must be extracted and converted to fixed-sizes matrices.\nThe method to do that is called \"Region of Interest Pooling\" (RoI-Pooling) and is based on max pooling.\nIt is mostly the same as in Faster R-CNN.\n\n During that process, small subregions of the feature maps (those that match the bounding box candidates) must be extracted and converted to fixed-sizes matrices.\nThe method to do that is called \"Region of Interest Pooling\" (RoI-Pooling) and is based on max pooling.\nIt is mostly the same as in Faster R-CNN. Visualization of the basic architecture:\n\n\n\n  Position-sensitive classification\n\nFully convolutional bounding box detectors tend to not work well.\nThe authors argue, that the problems come from the translation-invariance of convolutions, which is a desirable property in the case of classification but not when precise localization of objects is required.\nThey tackle that problem by generating multiple heatmaps per object class, each one being slightly shifted (\"position-sensitive score maps\").\nMore precisely:\n\nThe classifier generates per object class c a total of k*k heatmaps.\nIn the simplest form k is equal to 1. Then only one heatmap is generated, which signals whether a pixel is part of an object of class c.\nThey use k=3*3. The first of those heatmaps signals, whether a pixel is part of the top left corner of a bounding box of class c. The second heatmap signals, whether a pixel is part of the top center of a bounding box of class c (and so on).\nThe RoI-Pooling is applied to these heatmaps.\nFor k=3*3, each bounding box candidate is converted to 3*3 values. The first one resembles the top left corner of the bounding box candidate. Its value is generated by taking the average of the values in that area in the first heatmap.\nOnce the 3*3 values are generated, the final score of class c for that bounding box candidate is computed by averaging the values.\nThat process is repeated for all classes and a softmax is used to determine the final class.\nThe graphic below shows examples for that:\n\n\n\n\n\n\nThe above described RoI-Pooling uses only averages and hence is almost (computationally) free.\n\nThey make use of that during the training by sampling many candidates and only backpropagating on those with high losses (online hard example mining, OHEM).\n\n\n\n Fully convolutional bounding box detectors tend to not work well. The authors argue, that the problems come from the translation-invariance of convolutions, which is a desirable property in the case of classification but not when precise localization of objects is required. They tackle that problem by generating multiple heatmaps per object class, each one being slightly shifted (\"position-sensitive score maps\"). More precisely:\n\nThe classifier generates per object class c a total of k*k heatmaps.\nIn the simplest form k is equal to 1. Then only one heatmap is generated, which signals whether a pixel is part of an object of class c.\nThey use k=3*3. The first of those heatmaps signals, whether a pixel is part of the top left corner of a bounding box of class c. The second heatmap signals, whether a pixel is part of the top center of a bounding box of class c (and so on).\nThe RoI-Pooling is applied to these heatmaps.\nFor k=3*3, each bounding box candidate is converted to 3*3 values. The first one resembles the top left corner of the bounding box candidate. Its value is generated by taking the average of the values in that area in the first heatmap.\nOnce the 3*3 values are generated, the final score of class c for that bounding box candidate is computed by averaging the values.\nThat process is repeated for all classes and a softmax is used to determine the final class.\nThe graphic below shows examples for that:\n\n\n\n\n\n The classifier generates per object class c a total of k*k heatmaps. In the simplest form k is equal to 1. Then only one heatmap is generated, which signals whether a pixel is part of an object of class c. They use k=3*3. The first of those heatmaps signals, whether a pixel is part of the top left corner of a bounding box of class c. The second heatmap signals, whether a pixel is part of the top center of a bounding box of class c (and so on). The RoI-Pooling is applied to these heatmaps. For k=3*3, each bounding box candidate is converted to 3*3 values. The first one resembles the top left corner of the bounding box candidate. Its value is generated by taking the average of the values in that area in the first heatmap. Once the 3*3 values are generated, the final score of class c for that bounding box candidate is computed by averaging the values. That process is repeated for all classes and a softmax is used to determine the final class. The graphic below shows examples for that:\n\n\n\n  The above described RoI-Pooling uses only averages and hence is almost (computationally) free.\n\nThey make use of that during the training by sampling many candidates and only backpropagating on those with high losses (online hard example mining, OHEM).\n\n They make use of that during the training by sampling many candidates and only backpropagating on those with high losses (online hard example mining, OHEM). \u00c0 trous trick\n\nIn order to increase accuracy for small bounding boxes they use the \u00e0 trous trick.\nThat means that they use a pretrained base network (here ResNet-101), then remove a pooling layer and set the \u00e0 trous rate (aka dilation) of all convolutions after the removed pooling layer to 2.\n\nThe \u00e1 trous rate describes the distance of sampling locations of a convolution. Usually that is 1 (sampled locations are right next to each other). If it is set to 2, there is one value \"skipped\" between each pair of neighbouring sampling location.\n\n\nBy doing that, the convolutions still behave as if the pooling layer existed (and therefore their weights can be reused). At the same time, they work at an increased resolution, making them more capable of classifying small objects. (Runtime increases though.)\n\n In order to increase accuracy for small bounding boxes they use the \u00e0 trous trick. That means that they use a pretrained base network (here ResNet-101), then remove a pooling layer and set the \u00e0 trous rate (aka dilation) of all convolutions after the removed pooling layer to 2.\n\nThe \u00e1 trous rate describes the distance of sampling locations of a convolution. Usually that is 1 (sampled locations are right next to each other). If it is set to 2, there is one value \"skipped\" between each pair of neighbouring sampling location.\n\n The \u00e1 trous rate describes the distance of sampling locations of a convolution. Usually that is 1 (sampled locations are right next to each other). If it is set to 2, there is one value \"skipped\" between each pair of neighbouring sampling location. By doing that, the convolutions still behave as if the pooling layer existed (and therefore their weights can be reused). At the same time, they work at an increased resolution, making them more capable of classifying small objects. (Runtime increases though.) Training of R-FCN happens similarly to Faster R-CNN. Results\n\nSimilar accuracy as the most accurate Faster R-CNN configurations at a lower runtime of roughly 170ms per image.\nSwitching to ResNet-50 decreases accuracy by about 2 percentage points mAP (at faster runtime). Switching to ResNet-152 seems to provide no measureable benefit.\nOHEM improves mAP by roughly 2 percentage points.\n\u00c0 trous trick improves mAP by roughly 2 percentage points.\nTraining on k=1 (one heatmap per class) results in a failure, i.e. a model that fails to predict bounding boxes. k=7 is slightly more accurate than k=3.\n\n Similar accuracy as the most accurate Faster R-CNN configurations at a lower runtime of roughly 170ms per image. Switching to ResNet-50 decreases accuracy by about 2 percentage points mAP (at faster runtime). Switching to ResNet-152 seems to provide no measureable benefit. OHEM improves mAP by roughly 2 percentage points. \u00c0 trous trick improves mAP by roughly 2 percentage points. Training on k=1 (one heatmap per class) results in a failure, i.e. a model that fails to predict bounding boxes. k=7 is slightly more accurate than k=3. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1605.06409"
    },
    "42": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Faster_R-CNN.md",
        "transcript": "\nWhat\n\nR-CNN and its successor Fast R-CNN both rely on a \"classical\" method to find region proposals in images (i.e. \"Which regions of the image look like they might be objects?\").\nThat classical method is selective search.\nSelective search is quite slow (about two seconds per image) and hence the bottleneck in Fast R-CNN.\nThey replace it with a neural network (region proposal network, aka RPN).\nThe RPN reuses the same features used for the remainder of the Fast R-CNN network, making the region proposal step almost free (about 10ms).\n\n R-CNN and its successor Fast R-CNN both rely on a \"classical\" method to find region proposals in images (i.e. \"Which regions of the image look like they might be objects?\"). That classical method is selective search. Selective search is quite slow (about two seconds per image) and hence the bottleneck in Fast R-CNN. They replace it with a neural network (region proposal network, aka RPN). The RPN reuses the same features used for the remainder of the Fast R-CNN network, making the region proposal step almost free (about 10ms). \nHow\n\nThey now have three components in their network:\n\nA model for feature extraction, called the \"feature extraction network\" (FEN). Initialized with the weights of a pretrained network (e.g. VGG16).\nA model to use these features and generate region proposals, called the \"Region Proposal Network\" (RPN).\nA model to use these features and region proposals to classify each regions proposal's object and readjust the bounding box, called the \"classification network\" (CN). Initialized with the weights of a pretrained network (e.g. VGG16).\nUsually, FEN will contain the convolutional layers of the pretrained model (e.g. VGG16), while CN will contain the fully connected layers.\n(Note: Only \"RPN\" really pops up in the paper, the other two remain more or less unnamed. I added the two names to simplify the description.)\nRough architecture outline:\n\n\n\n\n\n\nThe basic method at test is as follows:\n\nUse FEN to convert the image to features.\nApply RPN to the features to generate region proposals.\nUse Region of Interest Pooling (RoI-Pooling) to convert the features of each region proposal to a fixed sized vector.\nApply CN to the RoI-vectors to a) predict the class of each object (out of K object classes and 1 background class) and b) readjust the bounding box dimensions (top left coordinate, height, width).\n\n\nRPN\n\nBasic idea:\n\nPlace anchor points on the image, all with the same distance to each other (regular grid).\nAround each anchor point, extract rectangular image areas in various shapes and sizes (\"anchor boxes\"), e.g. thin/square/wide and small/medium/large rectangles. (More precisely: The features of these areas are extracted.)\nVisualization:\n\n\n\n\nFeed the features of these areas through a classifier and let it rate/predict the \"regionness\" of the rectangle in a range between 0 and 1. Values greater than 0.5 mean that the classifier thinks the rectangle might be a bounding box. (CN has to analyze that further.)\nFeed the features of these areas through a regressor and let it optimize the region size (top left coordinate, height, width). That way you get all kinds of possible bounding box shapes, even though you only use a few base shapes.\n\n\nImplementation:\n\nThe regular grid of anchor points naturally arises due to the downscaling of the FEN, it doesn't have to be implemented explicitly.\nThe extraction of anchor boxes and classification + regression can be efficiently implemented using convolutions.\n\nThey first apply a 3x3 convolution on the feature maps. Note that the convolution covers a large image area due to the downscaling.\n\nNot so clear, but sounds like they use 256 filters/kernels for that convolution.\n\n\nThen they apply some 1x1 convolutions for the classification and regression.\n\nThey use 2*k 1x1 convolutions for classification and 4*k 1x1 convolutions for regression, where k is the number of different shapes of anchor boxes.\n\n\n\n\nThey use k=9 anchor box types: Three sizes (small, medium, large), each in three shapes (thin, square, wide).\nThe way they build training examples (below) forces some 1x1 convolutions to react only to some anchor box types.\n\n\nTraining:\n\nPositive examples are anchor boxes that have an IoU with a ground truth bounding box of 0.7 or more. If no anchor point has such an IoU with a specific box, the one with the highest IoU is used instead.\nNegative examples are all anchor boxes that have IoU that do not exceed 0.3 for any bounding box.\nAny anchor point that falls in neither of these groups does not contribute to the loss.\nAnchor boxes that would violate image boundaries are not used as examples.\nThe loss is similar to the one in Fast R-CNN: A sum consisting of log loss for the classifier and smooth L1 loss (=smoother absolute distance) for regression.\nPer batch they only sample examples from one image (for efficiency).\nThey use 128 positive examples and 128 negative ones. If they can't come up with 128 positive examples, they add more negative ones.\n\n\nTest:\n\nThey use non-maximum suppression (NMS) to remove too identical region proposals, i.e. among all region proposals that have an IoU overlap of 0.7 or more, they pick the one that has highest score.\nThey use the 300 proposals with highest score after NMS (or less if there aren't that many).\n\n\n\n\nFeature sharing\n\nThey want to share the features of the FEN between the RPN and the CN.\nSo they need a special training method that fine-tunes all three components while keeping the features extracted by FEN useful for both RPN and CN at the same time (not only for one of them).\nTheir training methods are:\n\nAlternating traing: One batch for FEN+RPN, one batch for FEN+CN, then again one batch for FEN+RPN and so on.\nApproximate joint training: Train one network of FEN+RPN+CN. Merge the gradients of RPN and CN that arrive at FEN via simple summation. This method does not compute a gradient from CN through the RPN's regression task, as that is non-trivial. (This runs 25-50% faster than alternating training, accuracy is mostly the same.)\nNon-approximate joint training: This would compute the above mentioned missing gradient, but isn't implemented.\n4-step alternating training:\n\nClone FEN to FEN1 and FEN2.\nTrain the pair FEN1 + RPN.\nTrain the pair FEN2 + CN using the region proposals from the trained RPN.\nFine-tune the pair FEN2 + RPN. FEN2 is fixed, RPN takes the weights from step 2.\nFine-tune the pair FEN2 + CN. FEN2 is fixed, CN takes the weights from step 3, region proposals come from RPN from step 4.\n\n\n\n\n\n\n\n They now have three components in their network:\n\nA model for feature extraction, called the \"feature extraction network\" (FEN). Initialized with the weights of a pretrained network (e.g. VGG16).\nA model to use these features and generate region proposals, called the \"Region Proposal Network\" (RPN).\nA model to use these features and region proposals to classify each regions proposal's object and readjust the bounding box, called the \"classification network\" (CN). Initialized with the weights of a pretrained network (e.g. VGG16).\nUsually, FEN will contain the convolutional layers of the pretrained model (e.g. VGG16), while CN will contain the fully connected layers.\n(Note: Only \"RPN\" really pops up in the paper, the other two remain more or less unnamed. I added the two names to simplify the description.)\nRough architecture outline:\n\n\n\n\n\n A model for feature extraction, called the \"feature extraction network\" (FEN). Initialized with the weights of a pretrained network (e.g. VGG16). A model to use these features and generate region proposals, called the \"Region Proposal Network\" (RPN). A model to use these features and region proposals to classify each regions proposal's object and readjust the bounding box, called the \"classification network\" (CN). Initialized with the weights of a pretrained network (e.g. VGG16). Usually, FEN will contain the convolutional layers of the pretrained model (e.g. VGG16), while CN will contain the fully connected layers. (Note: Only \"RPN\" really pops up in the paper, the other two remain more or less unnamed. I added the two names to simplify the description.) Rough architecture outline:\n\n\n\n  The basic method at test is as follows:\n\nUse FEN to convert the image to features.\nApply RPN to the features to generate region proposals.\nUse Region of Interest Pooling (RoI-Pooling) to convert the features of each region proposal to a fixed sized vector.\nApply CN to the RoI-vectors to a) predict the class of each object (out of K object classes and 1 background class) and b) readjust the bounding box dimensions (top left coordinate, height, width).\n\n Use FEN to convert the image to features. Apply RPN to the features to generate region proposals. Use Region of Interest Pooling (RoI-Pooling) to convert the features of each region proposal to a fixed sized vector. Apply CN to the RoI-vectors to a) predict the class of each object (out of K object classes and 1 background class) and b) readjust the bounding box dimensions (top left coordinate, height, width). RPN\n\nBasic idea:\n\nPlace anchor points on the image, all with the same distance to each other (regular grid).\nAround each anchor point, extract rectangular image areas in various shapes and sizes (\"anchor boxes\"), e.g. thin/square/wide and small/medium/large rectangles. (More precisely: The features of these areas are extracted.)\nVisualization:\n\n\n\n\nFeed the features of these areas through a classifier and let it rate/predict the \"regionness\" of the rectangle in a range between 0 and 1. Values greater than 0.5 mean that the classifier thinks the rectangle might be a bounding box. (CN has to analyze that further.)\nFeed the features of these areas through a regressor and let it optimize the region size (top left coordinate, height, width). That way you get all kinds of possible bounding box shapes, even though you only use a few base shapes.\n\n\nImplementation:\n\nThe regular grid of anchor points naturally arises due to the downscaling of the FEN, it doesn't have to be implemented explicitly.\nThe extraction of anchor boxes and classification + regression can be efficiently implemented using convolutions.\n\nThey first apply a 3x3 convolution on the feature maps. Note that the convolution covers a large image area due to the downscaling.\n\nNot so clear, but sounds like they use 256 filters/kernels for that convolution.\n\n\nThen they apply some 1x1 convolutions for the classification and regression.\n\nThey use 2*k 1x1 convolutions for classification and 4*k 1x1 convolutions for regression, where k is the number of different shapes of anchor boxes.\n\n\n\n\nThey use k=9 anchor box types: Three sizes (small, medium, large), each in three shapes (thin, square, wide).\nThe way they build training examples (below) forces some 1x1 convolutions to react only to some anchor box types.\n\n\nTraining:\n\nPositive examples are anchor boxes that have an IoU with a ground truth bounding box of 0.7 or more. If no anchor point has such an IoU with a specific box, the one with the highest IoU is used instead.\nNegative examples are all anchor boxes that have IoU that do not exceed 0.3 for any bounding box.\nAny anchor point that falls in neither of these groups does not contribute to the loss.\nAnchor boxes that would violate image boundaries are not used as examples.\nThe loss is similar to the one in Fast R-CNN: A sum consisting of log loss for the classifier and smooth L1 loss (=smoother absolute distance) for regression.\nPer batch they only sample examples from one image (for efficiency).\nThey use 128 positive examples and 128 negative ones. If they can't come up with 128 positive examples, they add more negative ones.\n\n\nTest:\n\nThey use non-maximum suppression (NMS) to remove too identical region proposals, i.e. among all region proposals that have an IoU overlap of 0.7 or more, they pick the one that has highest score.\nThey use the 300 proposals with highest score after NMS (or less if there aren't that many).\n\n\n\n Basic idea:\n\nPlace anchor points on the image, all with the same distance to each other (regular grid).\nAround each anchor point, extract rectangular image areas in various shapes and sizes (\"anchor boxes\"), e.g. thin/square/wide and small/medium/large rectangles. (More precisely: The features of these areas are extracted.)\nVisualization:\n\n\n\n\nFeed the features of these areas through a classifier and let it rate/predict the \"regionness\" of the rectangle in a range between 0 and 1. Values greater than 0.5 mean that the classifier thinks the rectangle might be a bounding box. (CN has to analyze that further.)\nFeed the features of these areas through a regressor and let it optimize the region size (top left coordinate, height, width). That way you get all kinds of possible bounding box shapes, even though you only use a few base shapes.\n\n Place anchor points on the image, all with the same distance to each other (regular grid). Around each anchor point, extract rectangular image areas in various shapes and sizes (\"anchor boxes\"), e.g. thin/square/wide and small/medium/large rectangles. (More precisely: The features of these areas are extracted.) Visualization:\n\n\n\n  Feed the features of these areas through a classifier and let it rate/predict the \"regionness\" of the rectangle in a range between 0 and 1. Values greater than 0.5 mean that the classifier thinks the rectangle might be a bounding box. (CN has to analyze that further.) Feed the features of these areas through a regressor and let it optimize the region size (top left coordinate, height, width). That way you get all kinds of possible bounding box shapes, even though you only use a few base shapes. Implementation:\n\nThe regular grid of anchor points naturally arises due to the downscaling of the FEN, it doesn't have to be implemented explicitly.\nThe extraction of anchor boxes and classification + regression can be efficiently implemented using convolutions.\n\nThey first apply a 3x3 convolution on the feature maps. Note that the convolution covers a large image area due to the downscaling.\n\nNot so clear, but sounds like they use 256 filters/kernels for that convolution.\n\n\nThen they apply some 1x1 convolutions for the classification and regression.\n\nThey use 2*k 1x1 convolutions for classification and 4*k 1x1 convolutions for regression, where k is the number of different shapes of anchor boxes.\n\n\n\n\nThey use k=9 anchor box types: Three sizes (small, medium, large), each in three shapes (thin, square, wide).\nThe way they build training examples (below) forces some 1x1 convolutions to react only to some anchor box types.\n\n The regular grid of anchor points naturally arises due to the downscaling of the FEN, it doesn't have to be implemented explicitly. The extraction of anchor boxes and classification + regression can be efficiently implemented using convolutions.\n\nThey first apply a 3x3 convolution on the feature maps. Note that the convolution covers a large image area due to the downscaling.\n\nNot so clear, but sounds like they use 256 filters/kernels for that convolution.\n\n\nThen they apply some 1x1 convolutions for the classification and regression.\n\nThey use 2*k 1x1 convolutions for classification and 4*k 1x1 convolutions for regression, where k is the number of different shapes of anchor boxes.\n\n\n\n They first apply a 3x3 convolution on the feature maps. Note that the convolution covers a large image area due to the downscaling.\n\nNot so clear, but sounds like they use 256 filters/kernels for that convolution.\n\n Not so clear, but sounds like they use 256 filters/kernels for that convolution. Then they apply some 1x1 convolutions for the classification and regression.\n\nThey use 2*k 1x1 convolutions for classification and 4*k 1x1 convolutions for regression, where k is the number of different shapes of anchor boxes.\n\n They use 2*k 1x1 convolutions for classification and 4*k 1x1 convolutions for regression, where k is the number of different shapes of anchor boxes. They use k=9 anchor box types: Three sizes (small, medium, large), each in three shapes (thin, square, wide). The way they build training examples (below) forces some 1x1 convolutions to react only to some anchor box types. Training:\n\nPositive examples are anchor boxes that have an IoU with a ground truth bounding box of 0.7 or more. If no anchor point has such an IoU with a specific box, the one with the highest IoU is used instead.\nNegative examples are all anchor boxes that have IoU that do not exceed 0.3 for any bounding box.\nAny anchor point that falls in neither of these groups does not contribute to the loss.\nAnchor boxes that would violate image boundaries are not used as examples.\nThe loss is similar to the one in Fast R-CNN: A sum consisting of log loss for the classifier and smooth L1 loss (=smoother absolute distance) for regression.\nPer batch they only sample examples from one image (for efficiency).\nThey use 128 positive examples and 128 negative ones. If they can't come up with 128 positive examples, they add more negative ones.\n\n Positive examples are anchor boxes that have an IoU with a ground truth bounding box of 0.7 or more. If no anchor point has such an IoU with a specific box, the one with the highest IoU is used instead. Negative examples are all anchor boxes that have IoU that do not exceed 0.3 for any bounding box. Any anchor point that falls in neither of these groups does not contribute to the loss. Anchor boxes that would violate image boundaries are not used as examples. The loss is similar to the one in Fast R-CNN: A sum consisting of log loss for the classifier and smooth L1 loss (=smoother absolute distance) for regression. Per batch they only sample examples from one image (for efficiency). They use 128 positive examples and 128 negative ones. If they can't come up with 128 positive examples, they add more negative ones. Test:\n\nThey use non-maximum suppression (NMS) to remove too identical region proposals, i.e. among all region proposals that have an IoU overlap of 0.7 or more, they pick the one that has highest score.\nThey use the 300 proposals with highest score after NMS (or less if there aren't that many).\n\n They use non-maximum suppression (NMS) to remove too identical region proposals, i.e. among all region proposals that have an IoU overlap of 0.7 or more, they pick the one that has highest score. They use the 300 proposals with highest score after NMS (or less if there aren't that many). Feature sharing\n\nThey want to share the features of the FEN between the RPN and the CN.\nSo they need a special training method that fine-tunes all three components while keeping the features extracted by FEN useful for both RPN and CN at the same time (not only for one of them).\nTheir training methods are:\n\nAlternating traing: One batch for FEN+RPN, one batch for FEN+CN, then again one batch for FEN+RPN and so on.\nApproximate joint training: Train one network of FEN+RPN+CN. Merge the gradients of RPN and CN that arrive at FEN via simple summation. This method does not compute a gradient from CN through the RPN's regression task, as that is non-trivial. (This runs 25-50% faster than alternating training, accuracy is mostly the same.)\nNon-approximate joint training: This would compute the above mentioned missing gradient, but isn't implemented.\n4-step alternating training:\n\nClone FEN to FEN1 and FEN2.\nTrain the pair FEN1 + RPN.\nTrain the pair FEN2 + CN using the region proposals from the trained RPN.\nFine-tune the pair FEN2 + RPN. FEN2 is fixed, RPN takes the weights from step 2.\nFine-tune the pair FEN2 + CN. FEN2 is fixed, CN takes the weights from step 3, region proposals come from RPN from step 4.\n\n\n\n\n\n They want to share the features of the FEN between the RPN and the CN. So they need a special training method that fine-tunes all three components while keeping the features extracted by FEN useful for both RPN and CN at the same time (not only for one of them). Their training methods are:\n\nAlternating traing: One batch for FEN+RPN, one batch for FEN+CN, then again one batch for FEN+RPN and so on.\nApproximate joint training: Train one network of FEN+RPN+CN. Merge the gradients of RPN and CN that arrive at FEN via simple summation. This method does not compute a gradient from CN through the RPN's regression task, as that is non-trivial. (This runs 25-50% faster than alternating training, accuracy is mostly the same.)\nNon-approximate joint training: This would compute the above mentioned missing gradient, but isn't implemented.\n4-step alternating training:\n\nClone FEN to FEN1 and FEN2.\nTrain the pair FEN1 + RPN.\nTrain the pair FEN2 + CN using the region proposals from the trained RPN.\nFine-tune the pair FEN2 + RPN. FEN2 is fixed, RPN takes the weights from step 2.\nFine-tune the pair FEN2 + CN. FEN2 is fixed, CN takes the weights from step 3, region proposals come from RPN from step 4.\n\n\n\n Alternating traing: One batch for FEN+RPN, one batch for FEN+CN, then again one batch for FEN+RPN and so on. Approximate joint training: Train one network of FEN+RPN+CN. Merge the gradients of RPN and CN that arrive at FEN via simple summation. This method does not compute a gradient from CN through the RPN's regression task, as that is non-trivial. (This runs 25-50% faster than alternating training, accuracy is mostly the same.) Non-approximate joint training: This would compute the above mentioned missing gradient, but isn't implemented. 4-step alternating training:\n\nClone FEN to FEN1 and FEN2.\nTrain the pair FEN1 + RPN.\nTrain the pair FEN2 + CN using the region proposals from the trained RPN.\nFine-tune the pair FEN2 + RPN. FEN2 is fixed, RPN takes the weights from step 2.\nFine-tune the pair FEN2 + CN. FEN2 is fixed, CN takes the weights from step 3, region proposals come from RPN from step 4.\n\n Clone FEN to FEN1 and FEN2. Train the pair FEN1 + RPN. Train the pair FEN2 + CN using the region proposals from the trained RPN. Fine-tune the pair FEN2 + RPN. FEN2 is fixed, RPN takes the weights from step 2. Fine-tune the pair FEN2 + CN. FEN2 is fixed, CN takes the weights from step 3, region proposals come from RPN from step 4. \nResults\n\nExample images:\n\n\n\n\nPascal VOC (with VGG16 as FEN)\n\nUsing an RPN instead of SS (selective search) slightly improved mAP from 66.9% to 69.9%.\nTraining RPN and CN on the same FEN (sharing FEN's weights) does not worsen the mAP, but instead improves it slightly from 68.5% to 69.9%.\n\n\nUsing the RPN instead of SS significantly speeds up the network, from 1830ms/image (less than 0.5fps) to 198ms/image (5fps). (Both stats with VGG16. They also use ZF as the FEN, which puts them at 17fps, but mAP is lower.)\nUsing per anchor point more scales and shapes (ratios) for the anchor boxes improves results.\n\n1 scale, 1 ratio: 65.8% mAP (scale 128*128, ratio 1:1) or 66.7% mAP (scale 256*256, same ratio).\n3 scales, 3 ratios: 69.9% mAP (scales 128*128, 256*256, 512*512; ratios 1:1, 1:2, 2:1).\n\n\nTwo-staged vs one-staged\n\nInstead of the two-stage system (first, generate proposals via RPN, then classify them via CN), they try a one-staged system.\nIn the one-staged system they move a sliding window over the computed feature maps and regress at every location the bounding box sizes and classify the box.\nWhen doing this, their performance drops from 58.7% to about 54%.\n\n\n\n Example images:\n\n\n\n  Pascal VOC (with VGG16 as FEN)\n\nUsing an RPN instead of SS (selective search) slightly improved mAP from 66.9% to 69.9%.\nTraining RPN and CN on the same FEN (sharing FEN's weights) does not worsen the mAP, but instead improves it slightly from 68.5% to 69.9%.\n\n Using an RPN instead of SS (selective search) slightly improved mAP from 66.9% to 69.9%. Training RPN and CN on the same FEN (sharing FEN's weights) does not worsen the mAP, but instead improves it slightly from 68.5% to 69.9%. Using the RPN instead of SS significantly speeds up the network, from 1830ms/image (less than 0.5fps) to 198ms/image (5fps). (Both stats with VGG16. They also use ZF as the FEN, which puts them at 17fps, but mAP is lower.) Using per anchor point more scales and shapes (ratios) for the anchor boxes improves results.\n\n1 scale, 1 ratio: 65.8% mAP (scale 128*128, ratio 1:1) or 66.7% mAP (scale 256*256, same ratio).\n3 scales, 3 ratios: 69.9% mAP (scales 128*128, 256*256, 512*512; ratios 1:1, 1:2, 2:1).\n\n 1 scale, 1 ratio: 65.8% mAP (scale 128*128, ratio 1:1) or 66.7% mAP (scale 256*256, same ratio). 3 scales, 3 ratios: 69.9% mAP (scales 128*128, 256*256, 512*512; ratios 1:1, 1:2, 2:1). Two-staged vs one-staged\n\nInstead of the two-stage system (first, generate proposals via RPN, then classify them via CN), they try a one-staged system.\nIn the one-staged system they move a sliding window over the computed feature maps and regress at every location the bounding box sizes and classify the box.\nWhen doing this, their performance drops from 58.7% to about 54%.\n\n Instead of the two-stage system (first, generate proposals via RPN, then classify them via CN), they try a one-staged system. In the one-staged system they move a sliding window over the computed feature maps and regress at every location the bounding box sizes and classify the box. When doing this, their performance drops from 58.7% to about 54%. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.01497"
    },
    "43": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Fast_R-CNN.md",
        "transcript": "\nWhat\n\nThe original R-CNN had three major disadvantages:\n\nTwo-staged training pipeline: Instead of only training a CNN, one had to train first a CNN and then multiple SVMs.\nExpensive training: Training was slow and required lots of disk space (feature vectors needed to be written to disk for all region proposals (2000 per image) before training the SVMs).\nSlow test: Each region proposal had to be handled independently.\n\n\nFast R-CNN ist an improved version of R-CNN and tackles the mentioned problems.\n\nIt no longer uses SVMs, only CNNs (single-stage).\nIt does one single feature extraction per image instead of per region, making it much faster (9x faster at training, 213x faster at test).\nIt is more accurate than R-CNN.\n\n\n\n The original R-CNN had three major disadvantages:\n\nTwo-staged training pipeline: Instead of only training a CNN, one had to train first a CNN and then multiple SVMs.\nExpensive training: Training was slow and required lots of disk space (feature vectors needed to be written to disk for all region proposals (2000 per image) before training the SVMs).\nSlow test: Each region proposal had to be handled independently.\n\n Two-staged training pipeline: Instead of only training a CNN, one had to train first a CNN and then multiple SVMs. Expensive training: Training was slow and required lots of disk space (feature vectors needed to be written to disk for all region proposals (2000 per image) before training the SVMs). Slow test: Each region proposal had to be handled independently. Fast R-CNN ist an improved version of R-CNN and tackles the mentioned problems.\n\nIt no longer uses SVMs, only CNNs (single-stage).\nIt does one single feature extraction per image instead of per region, making it much faster (9x faster at training, 213x faster at test).\nIt is more accurate than R-CNN.\n\n It no longer uses SVMs, only CNNs (single-stage). It does one single feature extraction per image instead of per region, making it much faster (9x faster at training, 213x faster at test). It is more accurate than R-CNN. \nHow\n\nThe basic architecture, training and testing methods are mostly copied from R-CNN.\nFor each image at test time they do:\n\nThey generate region proposals via selective search.\nThey feed the image once through the convolutional layers of a pre-trained network, usually VGG16.\nFor each region proposal they extract the respective region from the features generated by the network.\nThe regions can have different sizes, but the following steps need fixed size vectors. So each region is downscaled via max-pooling so that it has a size of 7x7 (so apparently they ignore regions of sizes below 7x7...?).\n\nThis is called Region of Interest Pooling (RoI-Pooling).\nDuring the backwards pass, partial derivatives can be transferred to the maximum value (as usually in max pooling). That derivative values are summed up over different regions (in the same image).\n\n\nThey reshape the 7x7 regions to vectors of length F*7*7, where F was the number of filters in the last convolutional layer.\nThey feed these vectors through another network which predicts:\n\nThe class of the region (including background class).\nTop left x-coordinate, top left y-coordinate, log height and log width of the bounding box (i.e. it fine-tunes the region proposal's bounding box). These values are predicted once for every class (so K*4 values).\n\n\n\n\nArchitecture as image:\n\n\n\n\nSampling for training\n\nEfficiency\n\nIf batch size is B it is inefficient to sample regions proposals from B images as each image will require a full forward pass through the base network (e.g. VGG16).\nIt is much more efficient to use few images to share most of the computation between region proposals.\nThey use two images per batch (each 64 region proposals) during training.\nThis technique introduces correlations between examples in batches, but they did not observe any problems from that.\nThey call this technique \"hierarchical sampling\" (first images, then region proposals).\n\n\nIoUs\n\nPositive examples for specific classes during training are region proposals that have an IoU with ground truth bounding boxes of >=0.5.\nExamples for background region proposals during training have IoUs with any ground truth box in the interval (0.1, 0.5].\n\nNot picking IoUs below 0.1 is similar to hard negative mining.\n\n\n\n\nThey use 25% positive examples, 75% negative/background examples per batch.\nThey apply horizontal flipping as data augmentation, nothing else.\n\n\nOutputs\n\nFor their class predictions the use a simple softmax with negative log likelihood.\nFor their bounding box regression they use a smooth L1 loss (similar to mean absolute error, but switches to mean squared error for very low values).\n\nSmooth L1 loss is less sensitive to outliers and less likely to suffer from exploding gradients.\n\n\nThe smooth L1 loss is only active for positive examples (not background examples). (Not active means that it is zero.)\n\n\nTraining schedule\n\nThe use SGD.\nThey train 30k batches with learning rate 0.001, then 0.0001 for another 10k batches. (On Pascal VOC, they use more batches on larger datasets.)\nThey use twice the learning rate for the biases.\nThey use momentum of 0.9.\nThey use parameter decay of 0.0005.\n\n\nTruncated SVD\n\nThe final network for class prediction and bounding box regression has to be applied to every region proposal.\nIt contains one large fully connected hidden layer and one fully connected output layer (K+1 classes plus K*4 regression values).\nFor 2000 proposals that becomes slow.\nSo they compress the layers after training to less weights via truncated SVD.\n\nA weights matrix is approximated via \n\nU (u x t) are the first t left-singular vectors of W.\nSigma is a t x t diagonal matrix of the top t singular values.\nV (v x t) are the first t right-singular vectors of W.\n\n\nW is then replaced by two layers: One contains Sigma V^T as weights (no biases), the other contains U as weights (with original biases).\nParameter count goes down to t(u+v) from uv.\n\n\n\n\n\n The basic architecture, training and testing methods are mostly copied from R-CNN. For each image at test time they do:\n\nThey generate region proposals via selective search.\nThey feed the image once through the convolutional layers of a pre-trained network, usually VGG16.\nFor each region proposal they extract the respective region from the features generated by the network.\nThe regions can have different sizes, but the following steps need fixed size vectors. So each region is downscaled via max-pooling so that it has a size of 7x7 (so apparently they ignore regions of sizes below 7x7...?).\n\nThis is called Region of Interest Pooling (RoI-Pooling).\nDuring the backwards pass, partial derivatives can be transferred to the maximum value (as usually in max pooling). That derivative values are summed up over different regions (in the same image).\n\n\nThey reshape the 7x7 regions to vectors of length F*7*7, where F was the number of filters in the last convolutional layer.\nThey feed these vectors through another network which predicts:\n\nThe class of the region (including background class).\nTop left x-coordinate, top left y-coordinate, log height and log width of the bounding box (i.e. it fine-tunes the region proposal's bounding box). These values are predicted once for every class (so K*4 values).\n\n\n\n They generate region proposals via selective search. They feed the image once through the convolutional layers of a pre-trained network, usually VGG16. For each region proposal they extract the respective region from the features generated by the network. The regions can have different sizes, but the following steps need fixed size vectors. So each region is downscaled via max-pooling so that it has a size of 7x7 (so apparently they ignore regions of sizes below 7x7...?).\n\nThis is called Region of Interest Pooling (RoI-Pooling).\nDuring the backwards pass, partial derivatives can be transferred to the maximum value (as usually in max pooling). That derivative values are summed up over different regions (in the same image).\n\n This is called Region of Interest Pooling (RoI-Pooling). During the backwards pass, partial derivatives can be transferred to the maximum value (as usually in max pooling). That derivative values are summed up over different regions (in the same image). They reshape the 7x7 regions to vectors of length F*7*7, where F was the number of filters in the last convolutional layer. They feed these vectors through another network which predicts:\n\nThe class of the region (including background class).\nTop left x-coordinate, top left y-coordinate, log height and log width of the bounding box (i.e. it fine-tunes the region proposal's bounding box). These values are predicted once for every class (so K*4 values).\n\n The class of the region (including background class). Top left x-coordinate, top left y-coordinate, log height and log width of the bounding box (i.e. it fine-tunes the region proposal's bounding box). These values are predicted once for every class (so K*4 values). Architecture as image:\n\n\n\n  Sampling for training\n\nEfficiency\n\nIf batch size is B it is inefficient to sample regions proposals from B images as each image will require a full forward pass through the base network (e.g. VGG16).\nIt is much more efficient to use few images to share most of the computation between region proposals.\nThey use two images per batch (each 64 region proposals) during training.\nThis technique introduces correlations between examples in batches, but they did not observe any problems from that.\nThey call this technique \"hierarchical sampling\" (first images, then region proposals).\n\n\nIoUs\n\nPositive examples for specific classes during training are region proposals that have an IoU with ground truth bounding boxes of >=0.5.\nExamples for background region proposals during training have IoUs with any ground truth box in the interval (0.1, 0.5].\n\nNot picking IoUs below 0.1 is similar to hard negative mining.\n\n\n\n\nThey use 25% positive examples, 75% negative/background examples per batch.\nThey apply horizontal flipping as data augmentation, nothing else.\n\n Efficiency\n\nIf batch size is B it is inefficient to sample regions proposals from B images as each image will require a full forward pass through the base network (e.g. VGG16).\nIt is much more efficient to use few images to share most of the computation between region proposals.\nThey use two images per batch (each 64 region proposals) during training.\nThis technique introduces correlations between examples in batches, but they did not observe any problems from that.\nThey call this technique \"hierarchical sampling\" (first images, then region proposals).\n\n If batch size is B it is inefficient to sample regions proposals from B images as each image will require a full forward pass through the base network (e.g. VGG16). It is much more efficient to use few images to share most of the computation between region proposals. They use two images per batch (each 64 region proposals) during training. This technique introduces correlations between examples in batches, but they did not observe any problems from that. They call this technique \"hierarchical sampling\" (first images, then region proposals). IoUs\n\nPositive examples for specific classes during training are region proposals that have an IoU with ground truth bounding boxes of >=0.5.\nExamples for background region proposals during training have IoUs with any ground truth box in the interval (0.1, 0.5].\n\nNot picking IoUs below 0.1 is similar to hard negative mining.\n\n\n\n Positive examples for specific classes during training are region proposals that have an IoU with ground truth bounding boxes of >=0.5. Examples for background region proposals during training have IoUs with any ground truth box in the interval (0.1, 0.5].\n\nNot picking IoUs below 0.1 is similar to hard negative mining.\n\n Not picking IoUs below 0.1 is similar to hard negative mining. They use 25% positive examples, 75% negative/background examples per batch. They apply horizontal flipping as data augmentation, nothing else. Outputs\n\nFor their class predictions the use a simple softmax with negative log likelihood.\nFor their bounding box regression they use a smooth L1 loss (similar to mean absolute error, but switches to mean squared error for very low values).\n\nSmooth L1 loss is less sensitive to outliers and less likely to suffer from exploding gradients.\n\n\nThe smooth L1 loss is only active for positive examples (not background examples). (Not active means that it is zero.)\n\n For their class predictions the use a simple softmax with negative log likelihood. For their bounding box regression they use a smooth L1 loss (similar to mean absolute error, but switches to mean squared error for very low values).\n\nSmooth L1 loss is less sensitive to outliers and less likely to suffer from exploding gradients.\n\n Smooth L1 loss is less sensitive to outliers and less likely to suffer from exploding gradients. The smooth L1 loss is only active for positive examples (not background examples). (Not active means that it is zero.) Training schedule\n\nThe use SGD.\nThey train 30k batches with learning rate 0.001, then 0.0001 for another 10k batches. (On Pascal VOC, they use more batches on larger datasets.)\nThey use twice the learning rate for the biases.\nThey use momentum of 0.9.\nThey use parameter decay of 0.0005.\n\n The use SGD. They train 30k batches with learning rate 0.001, then 0.0001 for another 10k batches. (On Pascal VOC, they use more batches on larger datasets.) They use twice the learning rate for the biases. They use momentum of 0.9. They use parameter decay of 0.0005. Truncated SVD\n\nThe final network for class prediction and bounding box regression has to be applied to every region proposal.\nIt contains one large fully connected hidden layer and one fully connected output layer (K+1 classes plus K*4 regression values).\nFor 2000 proposals that becomes slow.\nSo they compress the layers after training to less weights via truncated SVD.\n\nA weights matrix is approximated via \n\nU (u x t) are the first t left-singular vectors of W.\nSigma is a t x t diagonal matrix of the top t singular values.\nV (v x t) are the first t right-singular vectors of W.\n\n\nW is then replaced by two layers: One contains Sigma V^T as weights (no biases), the other contains U as weights (with original biases).\nParameter count goes down to t(u+v) from uv.\n\n\n\n The final network for class prediction and bounding box regression has to be applied to every region proposal. It contains one large fully connected hidden layer and one fully connected output layer (K+1 classes plus K*4 regression values). For 2000 proposals that becomes slow. So they compress the layers after training to less weights via truncated SVD.\n\nA weights matrix is approximated via \n\nU (u x t) are the first t left-singular vectors of W.\nSigma is a t x t diagonal matrix of the top t singular values.\nV (v x t) are the first t right-singular vectors of W.\n\n\nW is then replaced by two layers: One contains Sigma V^T as weights (no biases), the other contains U as weights (with original biases).\nParameter count goes down to t(u+v) from uv.\n\n A weights matrix is approximated via \n\nU (u x t) are the first t left-singular vectors of W.\nSigma is a t x t diagonal matrix of the top t singular values.\nV (v x t) are the first t right-singular vectors of W.\n\n U (u x t) are the first t left-singular vectors of W. Sigma is a t x t diagonal matrix of the top t singular values. V (v x t) are the first t right-singular vectors of W. W is then replaced by two layers: One contains Sigma V^T as weights (no biases), the other contains U as weights (with original biases). Parameter count goes down to t(u+v) from uv. \nResults\n\nThey try three base models:\n\nAlexNet (Small, S)\nVGG-CNN-M-1024 (Medium, M)\nVGG16 (Large, L)\n\n\nOn VGG16 and Pascal VOC 2007, compared to original R-CNN:\n\nTraining time down to 9.5h from 84h (8.8x faster).\nTest rate with SVD (1024 singular values) improves from 47 seconds per image to 0.22 seconds per image (213x faster).\nTest rate without SVD improves similarly to 0.32 seconds per image.\nmAP improves from 66.0% to 66.6% (66.9% without SVD).\nPer class accuracy results:\n\nFast_R-CNN__pvoc2012.jpg\n\n\n\n\n\nFixing the weights of VGG16's convolutional layers and only fine-tuning the fully connected layers (those are applied to each region proposal), decreases the accuracy to 61.4%.\n\nThis decrease in accuracy is most significant for the later convolutional layers, but marginal for the first layers.\nTherefor they only train the convolutional layers starting with conv3_1 (9 out of 13 layers), which speeds up training.\n\n\nMulti-task training\n\nTraining models on classification and bounding box regression instead of only on classification improves the mAP (from 62.6% to 66.9%).\nDoing this in one hierarchy instead of two seperate models (one for classification, one for bounding box regression) increases mAP by roughly 2-3 percentage points.\n\n\nThey did not find a significant benefit of training the model on multiple scales (e.g. same image sometimes at 400x400, sometimes at 600x600, sometimes at 800x800 etc.).\n\nNote that their raw CNN (everything before RoI-Pooling) is fully convolutional, so they can feed the images at any scale through the network.\n\n\nIncreasing the amount of training data seemed to improve mAP a bit, but not as much as one might hope for.\nUsing a softmax loss instead of an SVM seemed to marginally increase mAP (0-1 percentage points).\nUsing more region proposals from selective search does not simply increase mAP. Instead it can lead to higher recall, but lower precision.\n\n\n\n\nUsing densely sampled region proposals (as in sliding window) significantly reduces mAP (from 59.2% to 52.9%). If SVMs instead of softmaxes are used, the results are even worse (49.3%).\n\n They try three base models:\n\nAlexNet (Small, S)\nVGG-CNN-M-1024 (Medium, M)\nVGG16 (Large, L)\n\n AlexNet (Small, S) VGG-CNN-M-1024 (Medium, M) VGG16 (Large, L) On VGG16 and Pascal VOC 2007, compared to original R-CNN:\n\nTraining time down to 9.5h from 84h (8.8x faster).\nTest rate with SVD (1024 singular values) improves from 47 seconds per image to 0.22 seconds per image (213x faster).\nTest rate without SVD improves similarly to 0.32 seconds per image.\nmAP improves from 66.0% to 66.6% (66.9% without SVD).\nPer class accuracy results:\n\nFast_R-CNN__pvoc2012.jpg\n\n\n\n\n Training time down to 9.5h from 84h (8.8x faster). Test rate with SVD (1024 singular values) improves from 47 seconds per image to 0.22 seconds per image (213x faster). Test rate without SVD improves similarly to 0.32 seconds per image. mAP improves from 66.0% to 66.6% (66.9% without SVD). Per class accuracy results:\n\nFast_R-CNN__pvoc2012.jpg\n\n\n Fast_R-CNN__pvoc2012.jpg  Fixing the weights of VGG16's convolutional layers and only fine-tuning the fully connected layers (those are applied to each region proposal), decreases the accuracy to 61.4%.\n\nThis decrease in accuracy is most significant for the later convolutional layers, but marginal for the first layers.\nTherefor they only train the convolutional layers starting with conv3_1 (9 out of 13 layers), which speeds up training.\n\n This decrease in accuracy is most significant for the later convolutional layers, but marginal for the first layers. Therefor they only train the convolutional layers starting with conv3_1 (9 out of 13 layers), which speeds up training. Multi-task training\n\nTraining models on classification and bounding box regression instead of only on classification improves the mAP (from 62.6% to 66.9%).\nDoing this in one hierarchy instead of two seperate models (one for classification, one for bounding box regression) increases mAP by roughly 2-3 percentage points.\n\n Training models on classification and bounding box regression instead of only on classification improves the mAP (from 62.6% to 66.9%). Doing this in one hierarchy instead of two seperate models (one for classification, one for bounding box regression) increases mAP by roughly 2-3 percentage points. They did not find a significant benefit of training the model on multiple scales (e.g. same image sometimes at 400x400, sometimes at 600x600, sometimes at 800x800 etc.).\n\nNote that their raw CNN (everything before RoI-Pooling) is fully convolutional, so they can feed the images at any scale through the network.\n\n Note that their raw CNN (everything before RoI-Pooling) is fully convolutional, so they can feed the images at any scale through the network. Increasing the amount of training data seemed to improve mAP a bit, but not as much as one might hope for. Using a softmax loss instead of an SVM seemed to marginally increase mAP (0-1 percentage points). Using more region proposals from selective search does not simply increase mAP. Instead it can lead to higher recall, but lower precision.\n\n\n\n  Using densely sampled region proposals (as in sliding window) significantly reduces mAP (from 59.2% to 52.9%). If SVMs instead of softmaxes are used, the results are even worse (49.3%). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1504.08083"
    },
    "44": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation.md",
        "transcript": "\nWhat\n\nPreviously, methods to detect bounding boxes in images were often based on the combination of manual feature extraction with SVMs.\nThey replace the manual feature extraction with a CNN, leading to significantly higher accuracy.\nThey use supervised pre-training on auxiliary datasets to deal with the small amount of labeled data (instead of the sometimes used unsupervised pre-training).\nThey call their method R-CNN (\"Regions with CNN features\").\n\n Previously, methods to detect bounding boxes in images were often based on the combination of manual feature extraction with SVMs. They replace the manual feature extraction with a CNN, leading to significantly higher accuracy. They use supervised pre-training on auxiliary datasets to deal with the small amount of labeled data (instead of the sometimes used unsupervised pre-training). They call their method R-CNN (\"Regions with CNN features\"). \nHow\n\nTheir system has three modules: 1) Region proposal generation, 2) CNN-based feature extraction per region proposal, 3) classification.\n\n\n\n\nRegion proposals generation\n\nA region proposal is a bounding box candidate that might contain an object.\nBy default they generate 2000 region proposals per image.\nThey suggest \"simple\" (i.e. not learned) algorithms for this step (e.g. objectneess, selective search, CPMC).\nThey use selective search (makes it comparable to previous systems).\n\n\nCNN features\n\nUses a CNN to extract features, applied to each region proposal (replaces the previously used manual feature extraction).\nSo each region proposal ist turned into a fixed length vector.\nThey use AlexNet by Krizhevsky et al. as their base CNN (takes 227x227 RGB images, converts them into 4096-dimensional vectors).\nThey add p=16 pixels to each side of every region proposal, extract the pixels and then simply resize them to 227x227 (ignoring aspect ratio, so images might end up distorted).\nThey generate one 4096d vector per image, which is less than what some previous manual feature extraction methods used. That enables faster classification, less memory usage and thus more possible classes.\n\n\nClassification\n\nA classifier that receives the extracted feature vectors (one per region proposal) and classifies them into a predefined set of available classes (e.g. \"person\", \"car\", \"bike\", \"background / no object\").\nThey use one SVM per available class.\nThe regions that were not classified as background might overlap (multiple bounding boxes on the same object).\n\nThey use greedy non-maximum suppresion to fix that problem (for each class individually).\nThat method simply rejects regions if they overlap strongly with another region that has higher score.\nOverlap is determined via Intersection of Union (IoU).\n\n\n\n\nTraining method\n\nPre-Training of CNN\n\nThey use AlexNet pretrained on Imagenet (1000 classes).\nThey replace the last fully connected layer with a randomly initialized one that leads to C+1 classes (C object classes, +1 for background).\n\n\nFine-Tuning of CNN\n\nThe use SGD with learning rate 0.001.\nBatch size is 128 (32 positive windows, 96 background windows).\nA region proposal is considered positive, if its IoU with any ground-truth bounding box is >=0.5.\n\n\nSVM\n\nThey train one SVM per class via hard negative mining.\nFor positive examples they use here an IoU threshold of >=0.3, which performed better than 0.5.\n\n\n\n\n\n Their system has three modules: 1) Region proposal generation, 2) CNN-based feature extraction per region proposal, 3) classification.\n\n\n\n  Region proposals generation\n\nA region proposal is a bounding box candidate that might contain an object.\nBy default they generate 2000 region proposals per image.\nThey suggest \"simple\" (i.e. not learned) algorithms for this step (e.g. objectneess, selective search, CPMC).\nThey use selective search (makes it comparable to previous systems).\n\n A region proposal is a bounding box candidate that might contain an object. By default they generate 2000 region proposals per image. They suggest \"simple\" (i.e. not learned) algorithms for this step (e.g. objectneess, selective search, CPMC). They use selective search (makes it comparable to previous systems). CNN features\n\nUses a CNN to extract features, applied to each region proposal (replaces the previously used manual feature extraction).\nSo each region proposal ist turned into a fixed length vector.\nThey use AlexNet by Krizhevsky et al. as their base CNN (takes 227x227 RGB images, converts them into 4096-dimensional vectors).\nThey add p=16 pixels to each side of every region proposal, extract the pixels and then simply resize them to 227x227 (ignoring aspect ratio, so images might end up distorted).\nThey generate one 4096d vector per image, which is less than what some previous manual feature extraction methods used. That enables faster classification, less memory usage and thus more possible classes.\n\n Uses a CNN to extract features, applied to each region proposal (replaces the previously used manual feature extraction). So each region proposal ist turned into a fixed length vector. They use AlexNet by Krizhevsky et al. as their base CNN (takes 227x227 RGB images, converts them into 4096-dimensional vectors). They add p=16 pixels to each side of every region proposal, extract the pixels and then simply resize them to 227x227 (ignoring aspect ratio, so images might end up distorted). They generate one 4096d vector per image, which is less than what some previous manual feature extraction methods used. That enables faster classification, less memory usage and thus more possible classes. Classification\n\nA classifier that receives the extracted feature vectors (one per region proposal) and classifies them into a predefined set of available classes (e.g. \"person\", \"car\", \"bike\", \"background / no object\").\nThey use one SVM per available class.\nThe regions that were not classified as background might overlap (multiple bounding boxes on the same object).\n\nThey use greedy non-maximum suppresion to fix that problem (for each class individually).\nThat method simply rejects regions if they overlap strongly with another region that has higher score.\nOverlap is determined via Intersection of Union (IoU).\n\n\n\n A classifier that receives the extracted feature vectors (one per region proposal) and classifies them into a predefined set of available classes (e.g. \"person\", \"car\", \"bike\", \"background / no object\"). They use one SVM per available class. The regions that were not classified as background might overlap (multiple bounding boxes on the same object).\n\nThey use greedy non-maximum suppresion to fix that problem (for each class individually).\nThat method simply rejects regions if they overlap strongly with another region that has higher score.\nOverlap is determined via Intersection of Union (IoU).\n\n They use greedy non-maximum suppresion to fix that problem (for each class individually). That method simply rejects regions if they overlap strongly with another region that has higher score. Overlap is determined via Intersection of Union (IoU). Training method\n\nPre-Training of CNN\n\nThey use AlexNet pretrained on Imagenet (1000 classes).\nThey replace the last fully connected layer with a randomly initialized one that leads to C+1 classes (C object classes, +1 for background).\n\n\nFine-Tuning of CNN\n\nThe use SGD with learning rate 0.001.\nBatch size is 128 (32 positive windows, 96 background windows).\nA region proposal is considered positive, if its IoU with any ground-truth bounding box is >=0.5.\n\n\nSVM\n\nThey train one SVM per class via hard negative mining.\nFor positive examples they use here an IoU threshold of >=0.3, which performed better than 0.5.\n\n\n\n Pre-Training of CNN\n\nThey use AlexNet pretrained on Imagenet (1000 classes).\nThey replace the last fully connected layer with a randomly initialized one that leads to C+1 classes (C object classes, +1 for background).\n\n They use AlexNet pretrained on Imagenet (1000 classes). They replace the last fully connected layer with a randomly initialized one that leads to C+1 classes (C object classes, +1 for background). Fine-Tuning of CNN\n\nThe use SGD with learning rate 0.001.\nBatch size is 128 (32 positive windows, 96 background windows).\nA region proposal is considered positive, if its IoU with any ground-truth bounding box is >=0.5.\n\n The use SGD with learning rate 0.001. Batch size is 128 (32 positive windows, 96 background windows). A region proposal is considered positive, if its IoU with any ground-truth bounding box is >=0.5. SVM\n\nThey train one SVM per class via hard negative mining.\nFor positive examples they use here an IoU threshold of >=0.3, which performed better than 0.5.\n\n They train one SVM per class via hard negative mining. For positive examples they use here an IoU threshold of >=0.3, which performed better than 0.5. \nResults\n\nPascal VOC 2010\n\nThey: 53.7% mAP\nClosest competitor (SegDPM): 40.4% mAP\nClosest competitor that uses the same region proposal method (UVA): 35.1% mAP\n\n\n\nILSVRC2013 detection\n\nThey: 31.4% mAP\nClosest competitor (OverFeat): 24.3% mAP\n\n\nThe feed a large number of region proposals through the network and log for each filter in the last conv-layer which images activated it the most:\n\n\n\n\nUsefulness of layers:\n\nThey remove later layers of the network and retrain in order to find out which layers are the most useful ones.\nTheir result is that both fully connected layers of AlexNet seemed to be very domain-specific and profit most from fine-tuning.\n\n\nUsing VGG16:\n\nUsing VGG16 instead of AlexNet increased mAP from 58.5% to 66.0% on Pascal VOC 2007.\nComputation time was 7 times higher.\n\n\nThey train a linear regression model that improves the bounding box dimensions based on the extracted features of the last pooling layer. That improved their mAP by 3-4 percentage points.\nThe region proposals generated by selective search have a recall of 98% on Pascal VOC and 91.6% on ILSVRC2013 (measured by IoU of >=0.5).\n\n Pascal VOC 2010\n\nThey: 53.7% mAP\nClosest competitor (SegDPM): 40.4% mAP\nClosest competitor that uses the same region proposal method (UVA): 35.1% mAP\n\n\n They: 53.7% mAP Closest competitor (SegDPM): 40.4% mAP Closest competitor that uses the same region proposal method (UVA): 35.1% mAP  ILSVRC2013 detection\n\nThey: 31.4% mAP\nClosest competitor (OverFeat): 24.3% mAP\n\n They: 31.4% mAP Closest competitor (OverFeat): 24.3% mAP The feed a large number of region proposals through the network and log for each filter in the last conv-layer which images activated it the most:\n\n\n\n  Usefulness of layers:\n\nThey remove later layers of the network and retrain in order to find out which layers are the most useful ones.\nTheir result is that both fully connected layers of AlexNet seemed to be very domain-specific and profit most from fine-tuning.\n\n They remove later layers of the network and retrain in order to find out which layers are the most useful ones. Their result is that both fully connected layers of AlexNet seemed to be very domain-specific and profit most from fine-tuning. Using VGG16:\n\nUsing VGG16 instead of AlexNet increased mAP from 58.5% to 66.0% on Pascal VOC 2007.\nComputation time was 7 times higher.\n\n Using VGG16 instead of AlexNet increased mAP from 58.5% to 66.0% on Pascal VOC 2007. Computation time was 7 times higher. They train a linear regression model that improves the bounding box dimensions based on the extracted features of the last pooling layer. That improved their mAP by 3-4 percentage points. The region proposals generated by selective search have a recall of 98% on Pascal VOC and 91.6% on ILSVRC2013 (measured by IoU of >=0.5). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1311.2524"
    },
    "45": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/mixed/Ten_Years_of_Pedestrian_Detection_What_Have_We_Learned.md",
        "transcript": "\nWhat\n\nThey compare the results of various models for pedestrian detection.\nThe various models were developed over the course of ~10 years (2003-2014).\nThey analyze which factors seemed to improve the results.\nThey derive new models for pedestrian detection from that.\n\n They compare the results of various models for pedestrian detection. The various models were developed over the course of ~10 years (2003-2014). They analyze which factors seemed to improve the results. They derive new models for pedestrian detection from that. \nComparison: Datasets\n\nAvailable datasets\n\nINRIA: Small dataset. Diverse images.\nETH: Video dataset. Stereo images.\nTUD-Brussels: Video dataset.\nDaimler: No color channel.\nDaimler stereo: Stereo images.\nCaltech-USA: Most often used. Large dataset.\nKITTI: Often used. Large dataset. Stereo images.\n\n\nAll datasets except KITTI are part of the \"unified evaluation toolbox\" that allows authors to easily test on all of these datasets.\nThe evaluation started initially with per-window (FPPW) and later changed to per-image (FPPI), because per-window skewed the results.\nCommon evaluation metrics:\n\nMR: Log-average miss-rate (lower is better)\nAUC: Area under the precision-recall curve (higher is better)\n\n\n\n Available datasets\n\nINRIA: Small dataset. Diverse images.\nETH: Video dataset. Stereo images.\nTUD-Brussels: Video dataset.\nDaimler: No color channel.\nDaimler stereo: Stereo images.\nCaltech-USA: Most often used. Large dataset.\nKITTI: Often used. Large dataset. Stereo images.\n\n INRIA: Small dataset. Diverse images. ETH: Video dataset. Stereo images. TUD-Brussels: Video dataset. Daimler: No color channel. Daimler stereo: Stereo images. Caltech-USA: Most often used. Large dataset. KITTI: Often used. Large dataset. Stereo images. All datasets except KITTI are part of the \"unified evaluation toolbox\" that allows authors to easily test on all of these datasets. The evaluation started initially with per-window (FPPW) and later changed to per-image (FPPI), because per-window skewed the results. Common evaluation metrics:\n\nMR: Log-average miss-rate (lower is better)\nAUC: Area under the precision-recall curve (higher is better)\n\n MR: Log-average miss-rate (lower is better) AUC: Area under the precision-recall curve (higher is better) \nComparison: Methods\n\nFamilies\n\nThey identified three families of methods: Deformable Parts Models, Deep Neural Networks, Decision Forests.\nDecision Forests was the most popular family.\nNo specific family seemed to perform better than other families.\nThere was no evidence that non-linearity in kernels was needed (given sophisticated features).\n\n\nAdditional data\n\nAdding (coarse) optical flow data to each image seemed to consistently improve results.\nThere was some indication that adding stereo data to each image improves the results.\n\n\nContext\n\nFor sliding window detectors, adding context from around the window seemed to improve the results.\nE.g. context can indicate whether there were detections next to the window as people tend to walk in groups.\n\n\nDeformable parts\n\nThey saw no evidence that deformable part models outperformed other models.\n\n\nMulti-Scale models\n\nTraining separate models for each sliding window scale seemed to improve results slightly.\n\n\nDeep architectures\n\nThey saw no evidence that deep neural networks outperformed other models. (Note: Paper is from 2014, might have changed already?)\n\n\nFeatures\n\nBest performance was usually achieved with simple HOG+LUV features, i.e. by converting each window into:\n\n6 channels of gradient orientations\n1 channel of gradient magnitude\n3 channels of LUV color space\n\n\nSome models use significantly more channels for gradient orientations, but there was no evidence that this was necessary to achieve good accuracy.\nHowever, using more different features (and more sophisticated ones) seemed to improve results.\n\n\n\n Families\n\nThey identified three families of methods: Deformable Parts Models, Deep Neural Networks, Decision Forests.\nDecision Forests was the most popular family.\nNo specific family seemed to perform better than other families.\nThere was no evidence that non-linearity in kernels was needed (given sophisticated features).\n\n They identified three families of methods: Deformable Parts Models, Deep Neural Networks, Decision Forests. Decision Forests was the most popular family. No specific family seemed to perform better than other families. There was no evidence that non-linearity in kernels was needed (given sophisticated features). Additional data\n\nAdding (coarse) optical flow data to each image seemed to consistently improve results.\nThere was some indication that adding stereo data to each image improves the results.\n\n Adding (coarse) optical flow data to each image seemed to consistently improve results. There was some indication that adding stereo data to each image improves the results. Context\n\nFor sliding window detectors, adding context from around the window seemed to improve the results.\nE.g. context can indicate whether there were detections next to the window as people tend to walk in groups.\n\n For sliding window detectors, adding context from around the window seemed to improve the results. E.g. context can indicate whether there were detections next to the window as people tend to walk in groups. Deformable parts\n\nThey saw no evidence that deformable part models outperformed other models.\n\n They saw no evidence that deformable part models outperformed other models. Multi-Scale models\n\nTraining separate models for each sliding window scale seemed to improve results slightly.\n\n Training separate models for each sliding window scale seemed to improve results slightly. Deep architectures\n\nThey saw no evidence that deep neural networks outperformed other models. (Note: Paper is from 2014, might have changed already?)\n\n They saw no evidence that deep neural networks outperformed other models. (Note: Paper is from 2014, might have changed already?) Features\n\nBest performance was usually achieved with simple HOG+LUV features, i.e. by converting each window into:\n\n6 channels of gradient orientations\n1 channel of gradient magnitude\n3 channels of LUV color space\n\n\nSome models use significantly more channels for gradient orientations, but there was no evidence that this was necessary to achieve good accuracy.\nHowever, using more different features (and more sophisticated ones) seemed to improve results.\n\n Best performance was usually achieved with simple HOG+LUV features, i.e. by converting each window into:\n\n6 channels of gradient orientations\n1 channel of gradient magnitude\n3 channels of LUV color space\n\n 6 channels of gradient orientations 1 channel of gradient magnitude 3 channels of LUV color space Some models use significantly more channels for gradient orientations, but there was no evidence that this was necessary to achieve good accuracy. However, using more different features (and more sophisticated ones) seemed to improve results. \nTheir new model:\n\nThey choose Decisions Forests as their model framework (2048 level-2 trees, i.e. 3 thresholds per tree).\nThey use features from the Integral Channels Features framework. (Basically just a mixture of common/simple features per window.)\nThey add optical flow as a feature.\nThey add context around the window as a feature. (A second detector that detects windows containing two persons.)\nTheir model significantly improves upon the state of the art (from 34 to 22% MR on Caltech dataset).\n\n They choose Decisions Forests as their model framework (2048 level-2 trees, i.e. 3 thresholds per tree). They use features from the Integral Channels Features framework. (Basically just a mixture of common/simple features per window.) They add optical flow as a feature. They add context around the window as a feature. (A second detector that detects windows containing two persons.) Their model significantly improves upon the state of the art (from 34 to 22% MR on Caltech dataset). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1411.4304"
    },
    "46": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Instance_Normalization_The_Missing_Ingredient_for_Fast_Stylization.md",
        "transcript": "\nWhat\n\nStyle transfer between images works - in its original form - by iteratively making changes to a content image, so that its style matches more and more the style of a chosen style image.\nThat iterative process is very slow.\nAlternatively, one can train a single feed-forward generator network to apply a style in one forward pass. The network is trained on a dataset of input images and their stylized versions (stylized versions can be generated using the iterative approach).\nSo far, these generator networks were much faster than the iterative approach, but their quality was lower.\nThey describe a simple change to these generator networks to increase the image quality (up to the same level as the iterative approach).\n\n Style transfer between images works - in its original form - by iteratively making changes to a content image, so that its style matches more and more the style of a chosen style image. That iterative process is very slow. Alternatively, one can train a single feed-forward generator network to apply a style in one forward pass. The network is trained on a dataset of input images and their stylized versions (stylized versions can be generated using the iterative approach). So far, these generator networks were much faster than the iterative approach, but their quality was lower. They describe a simple change to these generator networks to increase the image quality (up to the same level as the iterative approach). \nHow\n\nIn the generator networks, they simply replace all batch normalization layers with instance normalization layers.\nBatch normalization normalizes using the information from the whole batch, while instance normalization normalizes each feature map on its own.\nEquations\n\nLet H = Height, W = Width, T = Batch size\nBatch Normalization:\n\n\n\n\nInstance Normalization\n\n\n\n\n\n\nThey apply instance normalization at test time too (identically).\n\n In the generator networks, they simply replace all batch normalization layers with instance normalization layers. Batch normalization normalizes using the information from the whole batch, while instance normalization normalizes each feature map on its own. Equations\n\nLet H = Height, W = Width, T = Batch size\nBatch Normalization:\n\n\n\n\nInstance Normalization\n\n\n\n\n\n Let H = Height, W = Width, T = Batch size Batch Normalization:\n\n\n\n  Instance Normalization\n\n\n\n  They apply instance normalization at test time too (identically). \nResults\n\nSame image quality as iterative approach (at a fraction of the runtime).\nOne content image with two different styles using their approach:\n\n\n\n\n\n Same image quality as iterative approach (at a fraction of the runtime). One content image with two different styles using their approach:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1607.08022"
    },
    "47": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Stacked_Hourglass_Networks_for_Human_Pose_Estimation.md",
        "transcript": "\nWhat\n\nThey suggest a new model architecture for human pose estimation (i.e. \"lay a skeleton over a person\").\nTheir architecture is based progressive pooling followed by progressive upsampling, creating an hourglass form.\nInput are images showing a person's body.\nOutputs are K heatmaps (for K body joints), with each heatmap showing the likely position of a single joint on the person (e.g. \"akle\", \"wrist\", \"left hand\", ...).\n\n They suggest a new model architecture for human pose estimation (i.e. \"lay a skeleton over a person\"). Their architecture is based progressive pooling followed by progressive upsampling, creating an hourglass form. Input are images showing a person's body. Outputs are K heatmaps (for K body joints), with each heatmap showing the likely position of a single joint on the person (e.g. \"akle\", \"wrist\", \"left hand\", ...). \nHow\n\nBasic building block\n\nThey use residuals as their basic building block.\nEach residual has three layers: One 1x1 convolution for dimensionality reduction (from 256 to 128 channels), a 3x3 convolution, a 1x1 convolution for dimensionality increase (back to 256).\nVisualized:\n\n\n\n\n\n\nArchitecture\n\nTheir architecture starts with one standard 7x7 convolutions that has strides of (2, 2).\nThey use MaxPooling (2x2, strides of (2, 2)) to downsample the images/feature maps.\nThey use Nearest Neighbour upsampling (factor 2) to upsample the images/feature maps.\nAfter every pooling step they add three of their basic building blocks.\nBefore each pooling step they branch off the current feature map as a minor branch and apply three basic building blocks to it. Then they add it back to the main branch after that one has been upsampeled again to the original size.\nThe feature maps between each basic building block have (usually) 256 channels.\nTheir HourGlass ends in two 1x1 convolutions that create the heatmaps.\nThey stack two of their HourGlass networks after each other. Between them they place an intermediate loss. That way, the second network can learn to improve the predictions of the first network.\nArchitecture visualized:\n\n\n\n\n\n\nHeatmaps\n\nThe output generated by the network are heatmaps, one per joint.\nEach ground truth heatmap has a small gaussian peak at the correct position of a joint, everything else has value 0.\nIf a joint isn't visible, the ground truth heatmap for that joint is all zeros.\n\n\nOther stuff\n\nThey use batch normalization.\nActivation functions are ReLUs.\nThey use RMSprob as their optimizer.\nImplemented in Torch.\n\n\n\n Basic building block\n\nThey use residuals as their basic building block.\nEach residual has three layers: One 1x1 convolution for dimensionality reduction (from 256 to 128 channels), a 3x3 convolution, a 1x1 convolution for dimensionality increase (back to 256).\nVisualized:\n\n\n\n\n\n They use residuals as their basic building block. Each residual has three layers: One 1x1 convolution for dimensionality reduction (from 256 to 128 channels), a 3x3 convolution, a 1x1 convolution for dimensionality increase (back to 256). Visualized:\n\n\n\n  Architecture\n\nTheir architecture starts with one standard 7x7 convolutions that has strides of (2, 2).\nThey use MaxPooling (2x2, strides of (2, 2)) to downsample the images/feature maps.\nThey use Nearest Neighbour upsampling (factor 2) to upsample the images/feature maps.\nAfter every pooling step they add three of their basic building blocks.\nBefore each pooling step they branch off the current feature map as a minor branch and apply three basic building blocks to it. Then they add it back to the main branch after that one has been upsampeled again to the original size.\nThe feature maps between each basic building block have (usually) 256 channels.\nTheir HourGlass ends in two 1x1 convolutions that create the heatmaps.\nThey stack two of their HourGlass networks after each other. Between them they place an intermediate loss. That way, the second network can learn to improve the predictions of the first network.\nArchitecture visualized:\n\n\n\n\n\n Their architecture starts with one standard 7x7 convolutions that has strides of (2, 2). They use MaxPooling (2x2, strides of (2, 2)) to downsample the images/feature maps. They use Nearest Neighbour upsampling (factor 2) to upsample the images/feature maps. After every pooling step they add three of their basic building blocks. Before each pooling step they branch off the current feature map as a minor branch and apply three basic building blocks to it. Then they add it back to the main branch after that one has been upsampeled again to the original size. The feature maps between each basic building block have (usually) 256 channels. Their HourGlass ends in two 1x1 convolutions that create the heatmaps. They stack two of their HourGlass networks after each other. Between them they place an intermediate loss. That way, the second network can learn to improve the predictions of the first network. Architecture visualized:\n\n\n\n  Heatmaps\n\nThe output generated by the network are heatmaps, one per joint.\nEach ground truth heatmap has a small gaussian peak at the correct position of a joint, everything else has value 0.\nIf a joint isn't visible, the ground truth heatmap for that joint is all zeros.\n\n The output generated by the network are heatmaps, one per joint. Each ground truth heatmap has a small gaussian peak at the correct position of a joint, everything else has value 0. If a joint isn't visible, the ground truth heatmap for that joint is all zeros. Other stuff\n\nThey use batch normalization.\nActivation functions are ReLUs.\nThey use RMSprob as their optimizer.\nImplemented in Torch.\n\n They use batch normalization. Activation functions are ReLUs. They use RMSprob as their optimizer. Implemented in Torch. \nResults\n\nThey train and test on FLIC (only one HourGlass) and MPII (two stacked HourGlass networks).\nTraining is done with augmentations (horizontal flip, up to 30 degress rotation, scaling, no translation to keep the body of interest in the center of the image).\nEvaluation is done via PCK@0.2 (i.e. percentage of predicted keypoints that are within 0.2 head sizes of their ground truth annotation (head size of the specific body)).\nResults on FLIC are at >95%.\nResults on MPII are between 80.6% (ankle) and 97.6% (head). Average is 89.4%.\nUsing two stacked HourGlass networks performs around 3% better than one HourGlass network (even when adjusting for parameters).\nTraining time was 5 days on a Titan X (9xx generation).\n\n They train and test on FLIC (only one HourGlass) and MPII (two stacked HourGlass networks). Training is done with augmentations (horizontal flip, up to 30 degress rotation, scaling, no translation to keep the body of interest in the center of the image). Evaluation is done via PCK@0.2 (i.e. percentage of predicted keypoints that are within 0.2 head sizes of their ground truth annotation (head size of the specific body)). Results on FLIC are at >95%. Results on MPII are between 80.6% (ankle) and 97.6% (head). Average is 89.4%. Using two stacked HourGlass networks performs around 3% better than one HourGlass network (even when adjusting for parameters). Training time was 5 days on a Titan X (9xx generation). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.06937"
    },
    "48": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/DeepFace.md",
        "transcript": "\nWhat\n\nThey describe a CNN architecture that can be used to identify a person given an image of their face.\n\n They describe a CNN architecture that can be used to identify a person given an image of their face. \nHow\n\nThe expected input is the image of a face (i.e. it does not search for faces in images, the faces already have to be extracted by a different method).\nFace alignment / Frontalization\n\nTarget of this step: Get rid of variations within the face images, so that every face seems to look straight into the camera (\"frontalized\").\n2D alignment\n\nThey search for landmarks (fiducial points) on the face.\n\nThey use SVRs (features: LBPs) for that.\nAfter every application of the SVR, the localized landmarks are used to transform/normalize the face. Then the SVR is applied again. By doing this, the locations of the landmarks are gradually refined.\n\n\nThey use the detected landmarks to normalize the face images (via scaling, rotation and translation).\n\n\n3D alignment\n\nThe 2D alignment allows to normalize variations within the 2D-plane, not out-of-plane variations (e.g. seeing that face from its left/right side). To normalize out-of-plane variations they need a 3D transformation.\nThey detect an additional 67 landmarks on the faces (again via SVRs).\nThey construct a human face mesh from a dataset (USF Human-ID).\nThey map the 67 landmarks to that mesh.\nThey then use some more complicated steps to recover the frontalized face image.\n\n\n\n\nCNN architecture\n\nThe CNN receives the frontalized face images (152x152, RGB).\nIt then applies the following steps:\n\nConvolution, 32 filters, 11x11, ReLU (-> 32x142x142, CxHxW)\nMax pooling over 3x3, stride 2 (-> 32x71x71)\nConvolution, 16 filters, 9x9, ReLU (-> 16x63x63)\nLocal Convolution, 16 filters, 9x9, ReLU (-> 16x55x55)\nLocal Convolution, 16 filters, 7x7, ReLU (-> 16x25x25)\nLocal Convolution, 16 filters, 5x5, ReLU (-> 16x21x21)\nFully Connected, 4096, ReLU\nFully Connected, 4030, Softmax\n\n\nLocal Convolutions use a different set of learned weights at every \"pixel\" (while a normal convolution uses the same set of weights at all locations).\nThey can afford to use local convolutions because of their frontalization, which roughly forces specific landmarks to be at specific locations.\nThey use dropout (apparently only after the first fully connected layer).\nThey normalize \"the features\" (probably the 4096 fully connected layer). Each component is divided by its maximum value across a training set. Additionally, the whole vector is L2-normalized. The goal of this step is to make the network less sensitive to illumination changes.\nThe whole network has about 120 million parameters.\nVisualization of the architecture:\n\n\n\n\n\n\nTraining\n\nThe network receives images, each showing a face, and is trained to classify the identity of the face (e.g. gets image of Obama, has to return \"that's Obama\").\nThey use cross-entropy as their loss.\n\n\nFace verification\n\nIn order to tell whether two images of faces show the same person they try three different methods.\nEach of these relies on the vector extracted by the first fully connected layer in the network (4096d).\nLet these vectors be f1 (image 1) and f2 (image 2). The methods are then:\n\nInner product between f1 and f2. The classification (same person/not same person) is then done by a simple threshold.\nWeighted X^2 (chi-squared) distance. Equation, per vector component i: weight_i (f1[i] - f2[i])^2 / (f1[i] + f2[i]). The vector is then fed into an SVM.\nSiamese network. Means here simply that the absolute distance between f1 and f2 is calculated (|f1-f2|), each component is weighted by a learned weight and then the sum of the components is calculated. If the result is above a threshold, the faces are considered to show the same person.\n\n\n\n\n\n The expected input is the image of a face (i.e. it does not search for faces in images, the faces already have to be extracted by a different method). Face alignment / Frontalization\n\nTarget of this step: Get rid of variations within the face images, so that every face seems to look straight into the camera (\"frontalized\").\n2D alignment\n\nThey search for landmarks (fiducial points) on the face.\n\nThey use SVRs (features: LBPs) for that.\nAfter every application of the SVR, the localized landmarks are used to transform/normalize the face. Then the SVR is applied again. By doing this, the locations of the landmarks are gradually refined.\n\n\nThey use the detected landmarks to normalize the face images (via scaling, rotation and translation).\n\n\n3D alignment\n\nThe 2D alignment allows to normalize variations within the 2D-plane, not out-of-plane variations (e.g. seeing that face from its left/right side). To normalize out-of-plane variations they need a 3D transformation.\nThey detect an additional 67 landmarks on the faces (again via SVRs).\nThey construct a human face mesh from a dataset (USF Human-ID).\nThey map the 67 landmarks to that mesh.\nThey then use some more complicated steps to recover the frontalized face image.\n\n\n\n Target of this step: Get rid of variations within the face images, so that every face seems to look straight into the camera (\"frontalized\"). 2D alignment\n\nThey search for landmarks (fiducial points) on the face.\n\nThey use SVRs (features: LBPs) for that.\nAfter every application of the SVR, the localized landmarks are used to transform/normalize the face. Then the SVR is applied again. By doing this, the locations of the landmarks are gradually refined.\n\n\nThey use the detected landmarks to normalize the face images (via scaling, rotation and translation).\n\n They search for landmarks (fiducial points) on the face.\n\nThey use SVRs (features: LBPs) for that.\nAfter every application of the SVR, the localized landmarks are used to transform/normalize the face. Then the SVR is applied again. By doing this, the locations of the landmarks are gradually refined.\n\n They use SVRs (features: LBPs) for that. After every application of the SVR, the localized landmarks are used to transform/normalize the face. Then the SVR is applied again. By doing this, the locations of the landmarks are gradually refined. They use the detected landmarks to normalize the face images (via scaling, rotation and translation). 3D alignment\n\nThe 2D alignment allows to normalize variations within the 2D-plane, not out-of-plane variations (e.g. seeing that face from its left/right side). To normalize out-of-plane variations they need a 3D transformation.\nThey detect an additional 67 landmarks on the faces (again via SVRs).\nThey construct a human face mesh from a dataset (USF Human-ID).\nThey map the 67 landmarks to that mesh.\nThey then use some more complicated steps to recover the frontalized face image.\n\n The 2D alignment allows to normalize variations within the 2D-plane, not out-of-plane variations (e.g. seeing that face from its left/right side). To normalize out-of-plane variations they need a 3D transformation. They detect an additional 67 landmarks on the faces (again via SVRs). They construct a human face mesh from a dataset (USF Human-ID). They map the 67 landmarks to that mesh. They then use some more complicated steps to recover the frontalized face image. CNN architecture\n\nThe CNN receives the frontalized face images (152x152, RGB).\nIt then applies the following steps:\n\nConvolution, 32 filters, 11x11, ReLU (-> 32x142x142, CxHxW)\nMax pooling over 3x3, stride 2 (-> 32x71x71)\nConvolution, 16 filters, 9x9, ReLU (-> 16x63x63)\nLocal Convolution, 16 filters, 9x9, ReLU (-> 16x55x55)\nLocal Convolution, 16 filters, 7x7, ReLU (-> 16x25x25)\nLocal Convolution, 16 filters, 5x5, ReLU (-> 16x21x21)\nFully Connected, 4096, ReLU\nFully Connected, 4030, Softmax\n\n\nLocal Convolutions use a different set of learned weights at every \"pixel\" (while a normal convolution uses the same set of weights at all locations).\nThey can afford to use local convolutions because of their frontalization, which roughly forces specific landmarks to be at specific locations.\nThey use dropout (apparently only after the first fully connected layer).\nThey normalize \"the features\" (probably the 4096 fully connected layer). Each component is divided by its maximum value across a training set. Additionally, the whole vector is L2-normalized. The goal of this step is to make the network less sensitive to illumination changes.\nThe whole network has about 120 million parameters.\nVisualization of the architecture:\n\n\n\n\n\n The CNN receives the frontalized face images (152x152, RGB). It then applies the following steps:\n\nConvolution, 32 filters, 11x11, ReLU (-> 32x142x142, CxHxW)\nMax pooling over 3x3, stride 2 (-> 32x71x71)\nConvolution, 16 filters, 9x9, ReLU (-> 16x63x63)\nLocal Convolution, 16 filters, 9x9, ReLU (-> 16x55x55)\nLocal Convolution, 16 filters, 7x7, ReLU (-> 16x25x25)\nLocal Convolution, 16 filters, 5x5, ReLU (-> 16x21x21)\nFully Connected, 4096, ReLU\nFully Connected, 4030, Softmax\n\n Convolution, 32 filters, 11x11, ReLU (-> 32x142x142, CxHxW) Max pooling over 3x3, stride 2 (-> 32x71x71) Convolution, 16 filters, 9x9, ReLU (-> 16x63x63) Local Convolution, 16 filters, 9x9, ReLU (-> 16x55x55) Local Convolution, 16 filters, 7x7, ReLU (-> 16x25x25) Local Convolution, 16 filters, 5x5, ReLU (-> 16x21x21) Fully Connected, 4096, ReLU Fully Connected, 4030, Softmax Local Convolutions use a different set of learned weights at every \"pixel\" (while a normal convolution uses the same set of weights at all locations). They can afford to use local convolutions because of their frontalization, which roughly forces specific landmarks to be at specific locations. They use dropout (apparently only after the first fully connected layer). They normalize \"the features\" (probably the 4096 fully connected layer). Each component is divided by its maximum value across a training set. Additionally, the whole vector is L2-normalized. The goal of this step is to make the network less sensitive to illumination changes. The whole network has about 120 million parameters. Visualization of the architecture:\n\n\n\n  Training\n\nThe network receives images, each showing a face, and is trained to classify the identity of the face (e.g. gets image of Obama, has to return \"that's Obama\").\nThey use cross-entropy as their loss.\n\n The network receives images, each showing a face, and is trained to classify the identity of the face (e.g. gets image of Obama, has to return \"that's Obama\"). They use cross-entropy as their loss. Face verification\n\nIn order to tell whether two images of faces show the same person they try three different methods.\nEach of these relies on the vector extracted by the first fully connected layer in the network (4096d).\nLet these vectors be f1 (image 1) and f2 (image 2). The methods are then:\n\nInner product between f1 and f2. The classification (same person/not same person) is then done by a simple threshold.\nWeighted X^2 (chi-squared) distance. Equation, per vector component i: weight_i (f1[i] - f2[i])^2 / (f1[i] + f2[i]). The vector is then fed into an SVM.\nSiamese network. Means here simply that the absolute distance between f1 and f2 is calculated (|f1-f2|), each component is weighted by a learned weight and then the sum of the components is calculated. If the result is above a threshold, the faces are considered to show the same person.\n\n\n\n In order to tell whether two images of faces show the same person they try three different methods. Each of these relies on the vector extracted by the first fully connected layer in the network (4096d). Let these vectors be f1 (image 1) and f2 (image 2). The methods are then:\n\nInner product between f1 and f2. The classification (same person/not same person) is then done by a simple threshold.\nWeighted X^2 (chi-squared) distance. Equation, per vector component i: weight_i (f1[i] - f2[i])^2 / (f1[i] + f2[i]). The vector is then fed into an SVM.\nSiamese network. Means here simply that the absolute distance between f1 and f2 is calculated (|f1-f2|), each component is weighted by a learned weight and then the sum of the components is calculated. If the result is above a threshold, the faces are considered to show the same person.\n\n Inner product between f1 and f2. The classification (same person/not same person) is then done by a simple threshold. Weighted X^2 (chi-squared) distance. Equation, per vector component i: weight_i (f1[i] - f2[i])^2 / (f1[i] + f2[i]). The vector is then fed into an SVM. Siamese network. Means here simply that the absolute distance between f1 and f2 is calculated (|f1-f2|), each component is weighted by a learned weight and then the sum of the components is calculated. If the result is above a threshold, the faces are considered to show the same person. \nResults\n\nThey train their network on the Social Face Classification (SFC) dataset. That seems to be a Facebook-internal dataset (i.e. not public) with 4.4 million faces of 4k people.\nWhen applied to the LFW dataset:\n\nFace recognition (\"which person is shown in the image\") (apparently they retrained the whole model on LFW for this task?):\n\nSimple SVM with LBP (i.e. not their network): 91.4% mean accuracy.\nTheir model, with frontalization, with 2d alignment: ??? no value.\nTheir model, no frontalization (only 2d alignment): 94.3% mean accuracy.\nTheir model, no frontalization, no 2d alignment: 87.9% mean accuracy.\n\n\nFace verification (two images -> same/not same person) (apparently also trained on LFW? unclear):\n\nMethod 1 (inner product + threshold): 95.92% mean accuracy.\nMethod 2 (X^2 vector + SVM): 97.00% mean accurracy.\nMethod 3 (siamese): Apparently 96.17% accuracy alone, and 97.25% when used in an ensemble with other methods (under special training schedule using SFC dataset).\n\n\n\n\nWhen applied to the YTF dataset (YouTube video frames):\n\n92.5% accuracy via X^2-method.\n\n\n\n They train their network on the Social Face Classification (SFC) dataset. That seems to be a Facebook-internal dataset (i.e. not public) with 4.4 million faces of 4k people. When applied to the LFW dataset:\n\nFace recognition (\"which person is shown in the image\") (apparently they retrained the whole model on LFW for this task?):\n\nSimple SVM with LBP (i.e. not their network): 91.4% mean accuracy.\nTheir model, with frontalization, with 2d alignment: ??? no value.\nTheir model, no frontalization (only 2d alignment): 94.3% mean accuracy.\nTheir model, no frontalization, no 2d alignment: 87.9% mean accuracy.\n\n\nFace verification (two images -> same/not same person) (apparently also trained on LFW? unclear):\n\nMethod 1 (inner product + threshold): 95.92% mean accuracy.\nMethod 2 (X^2 vector + SVM): 97.00% mean accurracy.\nMethod 3 (siamese): Apparently 96.17% accuracy alone, and 97.25% when used in an ensemble with other methods (under special training schedule using SFC dataset).\n\n\n\n Face recognition (\"which person is shown in the image\") (apparently they retrained the whole model on LFW for this task?):\n\nSimple SVM with LBP (i.e. not their network): 91.4% mean accuracy.\nTheir model, with frontalization, with 2d alignment: ??? no value.\nTheir model, no frontalization (only 2d alignment): 94.3% mean accuracy.\nTheir model, no frontalization, no 2d alignment: 87.9% mean accuracy.\n\n Simple SVM with LBP (i.e. not their network): 91.4% mean accuracy. Their model, with frontalization, with 2d alignment: ??? no value. Their model, no frontalization (only 2d alignment): 94.3% mean accuracy. Their model, no frontalization, no 2d alignment: 87.9% mean accuracy. Face verification (two images -> same/not same person) (apparently also trained on LFW? unclear):\n\nMethod 1 (inner product + threshold): 95.92% mean accuracy.\nMethod 2 (X^2 vector + SVM): 97.00% mean accurracy.\nMethod 3 (siamese): Apparently 96.17% accuracy alone, and 97.25% when used in an ensemble with other methods (under special training schedule using SFC dataset).\n\n Method 1 (inner product + threshold): 95.92% mean accuracy. Method 2 (X^2 vector + SVM): 97.00% mean accurracy. Method 3 (siamese): Apparently 96.17% accuracy alone, and 97.25% when used in an ensemble with other methods (under special training schedule using SFC dataset). When applied to the YTF dataset (YouTube video frames):\n\n92.5% accuracy via X^2-method.\n\n 92.5% accuracy via X^2-method. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf"
    },
    "49": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Character-based_Neural_Machine_Translation.md",
        "transcript": "\nWhat\n\nMost neural machine translation models currently operate on word vectors or one hot vectors of words.\nThey instead generate the vector of each word on a character-level.\n\nThereby, the model can spot character-similarities between words and treat them in a similar way.\nThey do that only for the source language, not for the target language.\n\n\n\n Most neural machine translation models currently operate on word vectors or one hot vectors of words. They instead generate the vector of each word on a character-level.\n\nThereby, the model can spot character-similarities between words and treat them in a similar way.\nThey do that only for the source language, not for the target language.\n\n Thereby, the model can spot character-similarities between words and treat them in a similar way. They do that only for the source language, not for the target language. \nHow\n\nThey treat each word of the source text on its own.\nTo each word they then apply the model from Character-aware neural language models, i.e. they do per word:\n\nEmbed each character into a 620-dimensional space.\nStack these vectors next to each other, resulting in a 2d-tensor in which each column is one of the vectors (i.e. shape 620xN for N characters).\nApply convolutions of size 620xW to that tensor, where a few different values are used for W (i.e. some convolutions cover few characters, some cover many characters).\nApply a tanh after these convolutions.\nApply a max-over-time to the results of the convolutions, i.e. for each convolution use only the maximum value.\nReshape to 1d-vector.\nApply two highway-layers.\nThey get 1024-dimensional vectors (one per word).\nVisualization of their steps:\n\n\n\n\n\n\nAfterwards they apply the model from Neural Machine Translation by Jointly Learning to Align and Translate to these vectors, yielding a translation to a target language.\nWhenever that translation yields an unknown target-language-word (\"UNK\"), they replace it with the respective (untranslated) word from the source text.\n\n They treat each word of the source text on its own. To each word they then apply the model from Character-aware neural language models, i.e. they do per word:\n\nEmbed each character into a 620-dimensional space.\nStack these vectors next to each other, resulting in a 2d-tensor in which each column is one of the vectors (i.e. shape 620xN for N characters).\nApply convolutions of size 620xW to that tensor, where a few different values are used for W (i.e. some convolutions cover few characters, some cover many characters).\nApply a tanh after these convolutions.\nApply a max-over-time to the results of the convolutions, i.e. for each convolution use only the maximum value.\nReshape to 1d-vector.\nApply two highway-layers.\nThey get 1024-dimensional vectors (one per word).\nVisualization of their steps:\n\n\n\n\n\n Embed each character into a 620-dimensional space. Stack these vectors next to each other, resulting in a 2d-tensor in which each column is one of the vectors (i.e. shape 620xN for N characters). Apply convolutions of size 620xW to that tensor, where a few different values are used for W (i.e. some convolutions cover few characters, some cover many characters). Apply a tanh after these convolutions. Apply a max-over-time to the results of the convolutions, i.e. for each convolution use only the maximum value. Reshape to 1d-vector. Apply two highway-layers. They get 1024-dimensional vectors (one per word). Visualization of their steps:\n\n\n\n  Afterwards they apply the model from Neural Machine Translation by Jointly Learning to Align and Translate to these vectors, yielding a translation to a target language. Whenever that translation yields an unknown target-language-word (\"UNK\"), they replace it with the respective (untranslated) word from the source text. \nResults\n\nThey the German-English WMT dataset.\nBLEU improvemements (compared to neural translation without character-level words):\n\nGerman-English improves by about 1.5 points.\nEnglish-German improves by about 3 points.\n\n\nReduction in the number of unknown target-language-words (same baseline again):\n\nGerman-English goes down from about 1500 to about 1250.\nEnglish-German goes down from about 3150 to about 2650.\n\n\nTranslation examples (Phrase = phrase-based/non-neural translation, NN = non-character-based neural translation, CHAR = theirs):\n\n\n\n\n\n They the German-English WMT dataset. BLEU improvemements (compared to neural translation without character-level words):\n\nGerman-English improves by about 1.5 points.\nEnglish-German improves by about 3 points.\n\n German-English improves by about 1.5 points. English-German improves by about 3 points. Reduction in the number of unknown target-language-words (same baseline again):\n\nGerman-English goes down from about 1500 to about 1250.\nEnglish-German goes down from about 3150 to about 2650.\n\n German-English goes down from about 1500 to about 1250. English-German goes down from about 3150 to about 2650. Translation examples (Phrase = phrase-based/non-neural translation, NN = non-character-based neural translation, CHAR = theirs):\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1603.00810v3"
    },
    "50": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Convolutional_Pose_Machines.md",
        "transcript": "\nWhat\n\nThey suggest a new model for human pose estimation (i.e. to lay a \"skeleton\" over the image of a person).\nTheir model has a (more or less) recurrent architecture.\n\nInitial estimates of keypoint locations are refined in several steps.\nThe idea of the recurrent architecture is derived from message passing, unrolled into one feed-forward model.\n\n\n\n They suggest a new model for human pose estimation (i.e. to lay a \"skeleton\" over the image of a person). Their model has a (more or less) recurrent architecture.\n\nInitial estimates of keypoint locations are refined in several steps.\nThe idea of the recurrent architecture is derived from message passing, unrolled into one feed-forward model.\n\n Initial estimates of keypoint locations are refined in several steps. The idea of the recurrent architecture is derived from message passing, unrolled into one feed-forward model. \nHow\n\nArchitecture\n\nThey generate the end result in multiple steps, similar to a recurrent network.\nStep 1:\n\nReceives the image (368x368 resolution).\nApplies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).\n\n\nStep 2 and later:\n\n(Modified) Receives the image (368x368 resolution) and the previous likelihood scores.\n(Same) Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).\n(New) Concatenates the likelihoods with the likelihoods of the previous step.\n(New) Applies a few more convolutions to the concatenation to compute the final likelihood scores.\n\n\nVisualization of the architecture:\n\n\n\n\n\n\nLoss function\n\nThe basic loss function is a simple mean squared error between the expected output maps per keypoint and the predicted ones.\nIn the expected output maps they mark the correct positions of the keypoints using a small gaussian function.\nThey apply losses after each step in the architecture, argueing that this helps against vanishing gradients (they don't seem to be using BN).\nThe expected output maps of the first step actually have the positions of all keypoints of a certain type (e.g. neck) marked, i.e. if there are multiple people in the extracted image patch there might be multiple correct keypoint positions. Only at step 2 and later they reduce that to the expected person (i.e. one keypoint position per map).\n\n\n\n Architecture\n\nThey generate the end result in multiple steps, similar to a recurrent network.\nStep 1:\n\nReceives the image (368x368 resolution).\nApplies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).\n\n\nStep 2 and later:\n\n(Modified) Receives the image (368x368 resolution) and the previous likelihood scores.\n(Same) Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).\n(New) Concatenates the likelihoods with the likelihoods of the previous step.\n(New) Applies a few more convolutions to the concatenation to compute the final likelihood scores.\n\n\nVisualization of the architecture:\n\n\n\n\n\n They generate the end result in multiple steps, similar to a recurrent network. Step 1:\n\nReceives the image (368x368 resolution).\nApplies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).\n\n Receives the image (368x368 resolution). Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...). Step 2 and later:\n\n(Modified) Receives the image (368x368 resolution) and the previous likelihood scores.\n(Same) Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).\n(New) Concatenates the likelihoods with the likelihoods of the previous step.\n(New) Applies a few more convolutions to the concatenation to compute the final likelihood scores.\n\n (Modified) Receives the image (368x368 resolution) and the previous likelihood scores. (Same) Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...). (New) Concatenates the likelihoods with the likelihoods of the previous step. (New) Applies a few more convolutions to the concatenation to compute the final likelihood scores. Visualization of the architecture:\n\n\n\n  Loss function\n\nThe basic loss function is a simple mean squared error between the expected output maps per keypoint and the predicted ones.\nIn the expected output maps they mark the correct positions of the keypoints using a small gaussian function.\nThey apply losses after each step in the architecture, argueing that this helps against vanishing gradients (they don't seem to be using BN).\nThe expected output maps of the first step actually have the positions of all keypoints of a certain type (e.g. neck) marked, i.e. if there are multiple people in the extracted image patch there might be multiple correct keypoint positions. Only at step 2 and later they reduce that to the expected person (i.e. one keypoint position per map).\n\n The basic loss function is a simple mean squared error between the expected output maps per keypoint and the predicted ones. In the expected output maps they mark the correct positions of the keypoints using a small gaussian function. They apply losses after each step in the architecture, argueing that this helps against vanishing gradients (they don't seem to be using BN). The expected output maps of the first step actually have the positions of all keypoints of a certain type (e.g. neck) marked, i.e. if there are multiple people in the extracted image patch there might be multiple correct keypoint positions. Only at step 2 and later they reduce that to the expected person (i.e. one keypoint position per map). \nResults\n\nExample results:\n\n\n\n\nSelf-correction of predictions over several timesteps:\n\n\n\n\nThey beat existing methods on the datasets MPII, LSP and FLIC.\nApplying a loss function after each step (instead of only once after the last step) improved their results and reduced problems related to vanishing gradients.\nThe effective receptive field size of each step had a significant influence on the results. They increased it to up to 300px (about 80% of the image size) and saw continuous improvements in accuracy.\n\n\n\n\n\n Example results:\n\n\n\n  Self-correction of predictions over several timesteps:\n\n\n\n  They beat existing methods on the datasets MPII, LSP and FLIC. Applying a loss function after each step (instead of only once after the last step) improved their results and reduced problems related to vanishing gradients. The effective receptive field size of each step had a significant influence on the results. They increased it to up to 300px (about 80% of the image size) and saw continuous improvements in accuracy.\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.00134"
    },
    "51": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/HyperFace.md",
        "transcript": "\nWhat\n\nThey suggest a single architecture that tries to solve the following tasks:\n\nFace localization (\"Where are faces in the image?\")\nFace landmark localization (\"For a given face, where are its landmarks, e.g. eyes, nose and mouth?\")\nFace landmark visibility estimation (\"For a given face, which of its landmarks are actually visible and which of them are occluded by other objects/people?\")\nFace roll, pitch and yaw estimation (\"For a given face, what is its rotation on the x/y/z-axis?\")\nFace gender estimation (\"For a given face, which gender does the person have?\")\n\n\n\n They suggest a single architecture that tries to solve the following tasks:\n\nFace localization (\"Where are faces in the image?\")\nFace landmark localization (\"For a given face, where are its landmarks, e.g. eyes, nose and mouth?\")\nFace landmark visibility estimation (\"For a given face, which of its landmarks are actually visible and which of them are occluded by other objects/people?\")\nFace roll, pitch and yaw estimation (\"For a given face, what is its rotation on the x/y/z-axis?\")\nFace gender estimation (\"For a given face, which gender does the person have?\")\n\n Face localization (\"Where are faces in the image?\") Face landmark localization (\"For a given face, where are its landmarks, e.g. eyes, nose and mouth?\") Face landmark visibility estimation (\"For a given face, which of its landmarks are actually visible and which of them are occluded by other objects/people?\") Face roll, pitch and yaw estimation (\"For a given face, what is its rotation on the x/y/z-axis?\") Face gender estimation (\"For a given face, which gender does the person have?\") \nHow\n\nPretraining the base model\n\nThey start with a basic model following the architecture of AlexNet.\nThey train that model to classify whether the input images are faces or not faces.\nThey then remove the fully connected layers, leaving only the convolutional layers.\n\n\nLocating bounding boxes of face candidates\n\nThey then use a selective search and segmentation algorithm on images to extract bounding boxes of objects.\nEach bounding box is considered a possible face.\nEach bounding box is rescaled to 227x227.\n\n\nFeature extraction per face candidate\n\nThey feed each bounding box through the above mentioned pretrained network.\nThey extract the activations of the network from the layers max1 (27x27x96), conv3 (13x13x384) and pool5 (6x6x256).\nThey apply to the first two extracted tensors (from max1, conv3) convolutions so that their tensor shapes are reduced to 6x6xC.\nThey concatenate the three tensors to a 6x6x768 tensor.\nThey apply a 1x1 convolution to that tensor to reduce it to 6x6x192.\nThey feed the result through a fully connected layer resulting in 3072-dimensional vectors (per face candidate).\n\n\nClassification and regression\n\nThey feed each 3072-dimensional vector through 5 separate networks:\n\nDetection: Does the bounding box contain a face or no face. (2 outputs, i.e. yes/no)\nLandmark Localization: What are the coordinates of landmark features (e.g. mouth, nose, ...). (21 landmarks, each 2 values for x/y = 42 outputs total)\nLandmark Visibility: Which landmarks are visible. (21 yes/no outputs)\nPose estimation: Roll, pitch, yaw of the face. (3 outputs)\nGender estimation: Male/female face. (2 outputs)\n\n\nEach of these network contains a single fully connected layer with 512 nodes, followed by the output layer with the above mentioned number of nodes.\n\n\nArchitecture Visualization:\n\n\n\n\nTraining\n\nThe base model is trained once (see above).\nThe feature extraction layers and the five classification/regression networks are trained afterwards (jointly).\nThe loss functions for the five networks are:\n\nDetection: BCE (binary cross-entropy). Detected bounding boxes that have an overlap >=0.5 with an annotated face are considered positive samples, bounding boxes with overlap <0.35 are considered negative samples, everything in between is ignored.\nLandmark localization: Roughly MSE (mean squared error), with some weighting for visibility. Only bounding boxes with overlap >0.35 are considered. Coordinates are normalized with respect to the bounding boxes center, width and height.\nLandmark visibility: MSE (predicted visibility factor vs. expected visibility factor). Only for bounding boxes with overlap >0.35.\nPose estimation: MSE.\nGender estimation: BCE.\n\n\n\n\nTesting\n\nThey use two postprocessing methods for detected faces:\n\nIterative Region Proposals:\n\nThey localize landmarks per face region.\nThen they compute a more appropriate face bounding box based on the localized landmarks.\nThey feed that new bounding box through the network.\nThey compute the face score (face / not face, i.e. number between 0 and 1) for both bounding boxes and choose the one with the higher score.\nThis shrinks down bounding boxes that turned out to be too big.\nThe method visualized:\n\n\n\n\n\n\nLandmarks-based Non-Maximum Suppression:\n\nWhen multiple detected face bounding boxes overlap, one has to choose which of them to keep.\nA method to do that is to only keep the bounding box with the highest face-score.\nThey instead use a median-of-k method.\nTheir steps are:\n\nReduce every box in size so that it is a bounding box around the localized landmarks.\nFor every box, find all bounding boxes with a certain amount of overlap.\nAmong these bounding boxes, select the k ones with highest face score.\nBased on these boxes, create a new box which's size is derived from the median coordinates of the landmarks.\nCompute the median values for landmark coordinates, landmark visibility, gender, pose and use it as the respective values for the new box.\n\n\n\n\n\n\n\n\n\n Pretraining the base model\n\nThey start with a basic model following the architecture of AlexNet.\nThey train that model to classify whether the input images are faces or not faces.\nThey then remove the fully connected layers, leaving only the convolutional layers.\n\n They start with a basic model following the architecture of AlexNet. They train that model to classify whether the input images are faces or not faces. They then remove the fully connected layers, leaving only the convolutional layers. Locating bounding boxes of face candidates\n\nThey then use a selective search and segmentation algorithm on images to extract bounding boxes of objects.\nEach bounding box is considered a possible face.\nEach bounding box is rescaled to 227x227.\n\n They then use a selective search and segmentation algorithm on images to extract bounding boxes of objects. Each bounding box is considered a possible face. Each bounding box is rescaled to 227x227. Feature extraction per face candidate\n\nThey feed each bounding box through the above mentioned pretrained network.\nThey extract the activations of the network from the layers max1 (27x27x96), conv3 (13x13x384) and pool5 (6x6x256).\nThey apply to the first two extracted tensors (from max1, conv3) convolutions so that their tensor shapes are reduced to 6x6xC.\nThey concatenate the three tensors to a 6x6x768 tensor.\nThey apply a 1x1 convolution to that tensor to reduce it to 6x6x192.\nThey feed the result through a fully connected layer resulting in 3072-dimensional vectors (per face candidate).\n\n They feed each bounding box through the above mentioned pretrained network. They extract the activations of the network from the layers max1 (27x27x96), conv3 (13x13x384) and pool5 (6x6x256). They apply to the first two extracted tensors (from max1, conv3) convolutions so that their tensor shapes are reduced to 6x6xC. They concatenate the three tensors to a 6x6x768 tensor. They apply a 1x1 convolution to that tensor to reduce it to 6x6x192. They feed the result through a fully connected layer resulting in 3072-dimensional vectors (per face candidate). Classification and regression\n\nThey feed each 3072-dimensional vector through 5 separate networks:\n\nDetection: Does the bounding box contain a face or no face. (2 outputs, i.e. yes/no)\nLandmark Localization: What are the coordinates of landmark features (e.g. mouth, nose, ...). (21 landmarks, each 2 values for x/y = 42 outputs total)\nLandmark Visibility: Which landmarks are visible. (21 yes/no outputs)\nPose estimation: Roll, pitch, yaw of the face. (3 outputs)\nGender estimation: Male/female face. (2 outputs)\n\n\nEach of these network contains a single fully connected layer with 512 nodes, followed by the output layer with the above mentioned number of nodes.\n\n They feed each 3072-dimensional vector through 5 separate networks:\n\nDetection: Does the bounding box contain a face or no face. (2 outputs, i.e. yes/no)\nLandmark Localization: What are the coordinates of landmark features (e.g. mouth, nose, ...). (21 landmarks, each 2 values for x/y = 42 outputs total)\nLandmark Visibility: Which landmarks are visible. (21 yes/no outputs)\nPose estimation: Roll, pitch, yaw of the face. (3 outputs)\nGender estimation: Male/female face. (2 outputs)\n\n Detection: Does the bounding box contain a face or no face. (2 outputs, i.e. yes/no) Landmark Localization: What are the coordinates of landmark features (e.g. mouth, nose, ...). (21 landmarks, each 2 values for x/y = 42 outputs total) Landmark Visibility: Which landmarks are visible. (21 yes/no outputs) Pose estimation: Roll, pitch, yaw of the face. (3 outputs) Gender estimation: Male/female face. (2 outputs) Each of these network contains a single fully connected layer with 512 nodes, followed by the output layer with the above mentioned number of nodes. Architecture Visualization:\n\n\n\n  Training\n\nThe base model is trained once (see above).\nThe feature extraction layers and the five classification/regression networks are trained afterwards (jointly).\nThe loss functions for the five networks are:\n\nDetection: BCE (binary cross-entropy). Detected bounding boxes that have an overlap >=0.5 with an annotated face are considered positive samples, bounding boxes with overlap <0.35 are considered negative samples, everything in between is ignored.\nLandmark localization: Roughly MSE (mean squared error), with some weighting for visibility. Only bounding boxes with overlap >0.35 are considered. Coordinates are normalized with respect to the bounding boxes center, width and height.\nLandmark visibility: MSE (predicted visibility factor vs. expected visibility factor). Only for bounding boxes with overlap >0.35.\nPose estimation: MSE.\nGender estimation: BCE.\n\n\n\n The base model is trained once (see above). The feature extraction layers and the five classification/regression networks are trained afterwards (jointly). The loss functions for the five networks are:\n\nDetection: BCE (binary cross-entropy). Detected bounding boxes that have an overlap >=0.5 with an annotated face are considered positive samples, bounding boxes with overlap <0.35 are considered negative samples, everything in between is ignored.\nLandmark localization: Roughly MSE (mean squared error), with some weighting for visibility. Only bounding boxes with overlap >0.35 are considered. Coordinates are normalized with respect to the bounding boxes center, width and height.\nLandmark visibility: MSE (predicted visibility factor vs. expected visibility factor). Only for bounding boxes with overlap >0.35.\nPose estimation: MSE.\nGender estimation: BCE.\n\n Detection: BCE (binary cross-entropy). Detected bounding boxes that have an overlap >=0.5 with an annotated face are considered positive samples, bounding boxes with overlap <0.35 are considered negative samples, everything in between is ignored. Landmark localization: Roughly MSE (mean squared error), with some weighting for visibility. Only bounding boxes with overlap >0.35 are considered. Coordinates are normalized with respect to the bounding boxes center, width and height. Landmark visibility: MSE (predicted visibility factor vs. expected visibility factor). Only for bounding boxes with overlap >0.35. Pose estimation: MSE. Gender estimation: BCE. Testing\n\nThey use two postprocessing methods for detected faces:\n\nIterative Region Proposals:\n\nThey localize landmarks per face region.\nThen they compute a more appropriate face bounding box based on the localized landmarks.\nThey feed that new bounding box through the network.\nThey compute the face score (face / not face, i.e. number between 0 and 1) for both bounding boxes and choose the one with the higher score.\nThis shrinks down bounding boxes that turned out to be too big.\nThe method visualized:\n\n\n\n\n\n\nLandmarks-based Non-Maximum Suppression:\n\nWhen multiple detected face bounding boxes overlap, one has to choose which of them to keep.\nA method to do that is to only keep the bounding box with the highest face-score.\nThey instead use a median-of-k method.\nTheir steps are:\n\nReduce every box in size so that it is a bounding box around the localized landmarks.\nFor every box, find all bounding boxes with a certain amount of overlap.\nAmong these bounding boxes, select the k ones with highest face score.\nBased on these boxes, create a new box which's size is derived from the median coordinates of the landmarks.\nCompute the median values for landmark coordinates, landmark visibility, gender, pose and use it as the respective values for the new box.\n\n\n\n\n\n\n\n They use two postprocessing methods for detected faces:\n\nIterative Region Proposals:\n\nThey localize landmarks per face region.\nThen they compute a more appropriate face bounding box based on the localized landmarks.\nThey feed that new bounding box through the network.\nThey compute the face score (face / not face, i.e. number between 0 and 1) for both bounding boxes and choose the one with the higher score.\nThis shrinks down bounding boxes that turned out to be too big.\nThe method visualized:\n\n\n\n\n\n\nLandmarks-based Non-Maximum Suppression:\n\nWhen multiple detected face bounding boxes overlap, one has to choose which of them to keep.\nA method to do that is to only keep the bounding box with the highest face-score.\nThey instead use a median-of-k method.\nTheir steps are:\n\nReduce every box in size so that it is a bounding box around the localized landmarks.\nFor every box, find all bounding boxes with a certain amount of overlap.\nAmong these bounding boxes, select the k ones with highest face score.\nBased on these boxes, create a new box which's size is derived from the median coordinates of the landmarks.\nCompute the median values for landmark coordinates, landmark visibility, gender, pose and use it as the respective values for the new box.\n\n\n\n\n\n Iterative Region Proposals:\n\nThey localize landmarks per face region.\nThen they compute a more appropriate face bounding box based on the localized landmarks.\nThey feed that new bounding box through the network.\nThey compute the face score (face / not face, i.e. number between 0 and 1) for both bounding boxes and choose the one with the higher score.\nThis shrinks down bounding boxes that turned out to be too big.\nThe method visualized:\n\n\n\n\n\n They localize landmarks per face region. Then they compute a more appropriate face bounding box based on the localized landmarks. They feed that new bounding box through the network. They compute the face score (face / not face, i.e. number between 0 and 1) for both bounding boxes and choose the one with the higher score. This shrinks down bounding boxes that turned out to be too big. The method visualized:\n\n\n\n  Landmarks-based Non-Maximum Suppression:\n\nWhen multiple detected face bounding boxes overlap, one has to choose which of them to keep.\nA method to do that is to only keep the bounding box with the highest face-score.\nThey instead use a median-of-k method.\nTheir steps are:\n\nReduce every box in size so that it is a bounding box around the localized landmarks.\nFor every box, find all bounding boxes with a certain amount of overlap.\nAmong these bounding boxes, select the k ones with highest face score.\nBased on these boxes, create a new box which's size is derived from the median coordinates of the landmarks.\nCompute the median values for landmark coordinates, landmark visibility, gender, pose and use it as the respective values for the new box.\n\n\n\n When multiple detected face bounding boxes overlap, one has to choose which of them to keep. A method to do that is to only keep the bounding box with the highest face-score. They instead use a median-of-k method. Their steps are:\n\nReduce every box in size so that it is a bounding box around the localized landmarks.\nFor every box, find all bounding boxes with a certain amount of overlap.\nAmong these bounding boxes, select the k ones with highest face score.\nBased on these boxes, create a new box which's size is derived from the median coordinates of the landmarks.\nCompute the median values for landmark coordinates, landmark visibility, gender, pose and use it as the respective values for the new box.\n\n Reduce every box in size so that it is a bounding box around the localized landmarks. For every box, find all bounding boxes with a certain amount of overlap. Among these bounding boxes, select the k ones with highest face score. Based on these boxes, create a new box which's size is derived from the median coordinates of the landmarks. Compute the median values for landmark coordinates, landmark visibility, gender, pose and use it as the respective values for the new box. \nResults\n\nExample results:\n\n\n\n\nThey test on AFW, AFWL, PASCAL, FDDB, CelebA.\nThey achieve the best mean average precision values on PASCAL and AFW (compared to selected competitors).\nAFW results visualized:\n\n\n\n\nTheir approach achieve good performance on FDDB. It has some problems with small and/or blurry faces.\nIf the feature fusion is removed from their approach (i.e. extracting features only from one fully connected layer at the end of the base network instead of merging feature maps from different convolutional layers), the accuracy of the predictions goes down.\nTheir architecture ends in 5 shallow networks and shares many layers before them. If instead these networks share no or few layers, the accuracy of the predictions goes down.\nThe postprocessing of bounding boxes (via Iterative Region Proposals and Landmarks-based Non-Maximum Suppression) has a quite significant influence on the performance.\nProcessing time per image is 3s, of which 2s is the selective search algorithm (for the bounding boxes).\n\n Example results:\n\n\n\n  They test on AFW, AFWL, PASCAL, FDDB, CelebA. They achieve the best mean average precision values on PASCAL and AFW (compared to selected competitors). AFW results visualized:\n\n\n\n  Their approach achieve good performance on FDDB. It has some problems with small and/or blurry faces. If the feature fusion is removed from their approach (i.e. extracting features only from one fully connected layer at the end of the base network instead of merging feature maps from different convolutional layers), the accuracy of the predictions goes down. Their architecture ends in 5 shallow networks and shares many layers before them. If instead these networks share no or few layers, the accuracy of the predictions goes down. The postprocessing of bounding boxes (via Iterative Region Proposals and Landmarks-based Non-Maximum Suppression) has a quite significant influence on the performance. Processing time per image is 3s, of which 2s is the selective search algorithm (for the bounding boxes). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1603.01249v2"
    },
    "52": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Face_Attribute_Prediction_Using_Off-the-Shelf_CNN_Features.md",
        "transcript": "\nWhat\n\nWhen using pretrained networks (like VGG) to solve tasks, one has to use features generated by these networks.\nThese features come from specific layers, e.g. from the fully connected layers at the end of the network.\nThey test whether the features from fully connected layers or from the last convolutional layer are better suited for face attribute prediction.\n\n When using pretrained networks (like VGG) to solve tasks, one has to use features generated by these networks. These features come from specific layers, e.g. from the fully connected layers at the end of the network. They test whether the features from fully connected layers or from the last convolutional layer are better suited for face attribute prediction. \nHow\n\nBase networks\n\nThey use standard architectures for their test networks, specifically the architectures of FaceNet and VGG (very deep version).\nThey modify these architectures to both use PReLUs.\nThey do not use the pretrained weights, instead they train the networks on their own.\nThey train them on the WebFace dataset (350k images, 10k different identities) to classify the identity of the shown person.\n\n\nAttribute prediction\n\nAfter training of the base networks, they train a separate SVM to predict attributes of faces.\nThe datasets used for this step are CelebA (100k images, 10k identities) and LFWA (13k images, 6k identities).\nEach image in these datasets is annotated with 40 binary face attributes.\nExamples for attributes: Eyeglasses, bushy eyebrows, big lips, ...\nThe features for the SVM are extracted from the base networks (i.e. feed forward a face through the network, then take the activations of a specific layer).\nThe following features are tested:\n\nFC2: Activations of the second fully connected layer of the base network.\nFC1: As FC2, but the first fully connected layer.\nSpat 3x3: Activations of the last convolutional layer, max-pooled so that their widths and heights are both 3 (i.e. shape Cx3x3).\nSpat 1x1: Same as \"Spat 3x3\", but max-pooled to Cx1x1.\n\n\n\n\n\n Base networks\n\nThey use standard architectures for their test networks, specifically the architectures of FaceNet and VGG (very deep version).\nThey modify these architectures to both use PReLUs.\nThey do not use the pretrained weights, instead they train the networks on their own.\nThey train them on the WebFace dataset (350k images, 10k different identities) to classify the identity of the shown person.\n\n They use standard architectures for their test networks, specifically the architectures of FaceNet and VGG (very deep version). They modify these architectures to both use PReLUs. They do not use the pretrained weights, instead they train the networks on their own. They train them on the WebFace dataset (350k images, 10k different identities) to classify the identity of the shown person. Attribute prediction\n\nAfter training of the base networks, they train a separate SVM to predict attributes of faces.\nThe datasets used for this step are CelebA (100k images, 10k identities) and LFWA (13k images, 6k identities).\nEach image in these datasets is annotated with 40 binary face attributes.\nExamples for attributes: Eyeglasses, bushy eyebrows, big lips, ...\nThe features for the SVM are extracted from the base networks (i.e. feed forward a face through the network, then take the activations of a specific layer).\nThe following features are tested:\n\nFC2: Activations of the second fully connected layer of the base network.\nFC1: As FC2, but the first fully connected layer.\nSpat 3x3: Activations of the last convolutional layer, max-pooled so that their widths and heights are both 3 (i.e. shape Cx3x3).\nSpat 1x1: Same as \"Spat 3x3\", but max-pooled to Cx1x1.\n\n\n\n After training of the base networks, they train a separate SVM to predict attributes of faces. The datasets used for this step are CelebA (100k images, 10k identities) and LFWA (13k images, 6k identities). Each image in these datasets is annotated with 40 binary face attributes. Examples for attributes: Eyeglasses, bushy eyebrows, big lips, ... The features for the SVM are extracted from the base networks (i.e. feed forward a face through the network, then take the activations of a specific layer). The following features are tested:\n\nFC2: Activations of the second fully connected layer of the base network.\nFC1: As FC2, but the first fully connected layer.\nSpat 3x3: Activations of the last convolutional layer, max-pooled so that their widths and heights are both 3 (i.e. shape Cx3x3).\nSpat 1x1: Same as \"Spat 3x3\", but max-pooled to Cx1x1.\n\n FC2: Activations of the second fully connected layer of the base network. FC1: As FC2, but the first fully connected layer. Spat 3x3: Activations of the last convolutional layer, max-pooled so that their widths and heights are both 3 (i.e. shape Cx3x3). Spat 1x1: Same as \"Spat 3x3\", but max-pooled to Cx1x1. \nResults\n\nThe SVMs trained on \"Spat 1x1\" performed overall worst, the ones trained on \"Spat 3x3\" performed best.\nThe accuracy order was roughly: Spat 3x3 > FC1 > FC2 > Spat 1x1.\nThis effect was consistent for both networks (VGG, FaceNet) and for other training datasets as well.\nFC2 performed particularly bad for the \"blurry\" attribute (most likely because that was unimportant to the classification task).\nAccuracy comparison per attribute:\n\n\n\n\nThe conclusion is, that when using pretrained networks one should not only try the last fully connected layer. Many characteristics of the input image might not appear any more in that layer (and later ones in general) as they were unimportant to the classification task.\n\n The SVMs trained on \"Spat 1x1\" performed overall worst, the ones trained on \"Spat 3x3\" performed best. The accuracy order was roughly: Spat 3x3 > FC1 > FC2 > Spat 1x1. This effect was consistent for both networks (VGG, FaceNet) and for other training datasets as well. FC2 performed particularly bad for the \"blurry\" attribute (most likely because that was unimportant to the classification task). Accuracy comparison per attribute:\n\n\n\n  The conclusion is, that when using pretrained networks one should not only try the last fully connected layer. Many characteristics of the input image might not appear any more in that layer (and later ones in general) as they were unimportant to the classification task. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1602.03935v2"
    },
    "53": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/CMS-RCNN.md",
        "transcript": "\nWhat\n\nThey describe a model to locate faces in images.\nTheir model uses information from suspected face regions and from the corresponding suspected body regions to classify whether a region contains a face.\nThe intuition is, that seeing the region around the face (specifically where the body should be) can help in estimating whether a suspected face is really a face (e.g. it might also be part of a painting, statue or doll).\n\n They describe a model to locate faces in images. Their model uses information from suspected face regions and from the corresponding suspected body regions to classify whether a region contains a face. The intuition is, that seeing the region around the face (specifically where the body should be) can help in estimating whether a suspected face is really a face (e.g. it might also be part of a painting, statue or doll). \nHow\n\nTheir whole model is called \"CMS-RCNN\" (Contextual Multi-Scale Region-CNN).\nIt is based on the \"Faster R-CNN\" architecture.\nIt uses the VGG network.\nSubparts of their model are: MS-RPN, CMS-CNN.\nMS-RPN finds candidate face regions. CMS-CNN refines their bounding boxes and classifies them (face / not face).\nMS-RPN (Multi-Scale Region Proposal Network)\n\n\"Looks\" at the feature maps of the network (VGG) at multiple scales (i.e. before/after pooling layers) and suggests regions for possible faces.\nSteps:\n\nFeed an image through the VGG network.\nExtract the feature maps of the three last convolutions that are before a pooling layer.\nPool these feature maps so that they have the same heights and widths.\nApply L2 normalization to each feature map so that they all have the same scale.\nApply a 1x1 convolution to merge them to one feature map.\nRegress face bounding boxes from that feature map according to the Faster R-CNN technique.\n\n\n\n\nCMS-CNN (Contextual Multi-Scale CNN):\n\n\"Looks\" at feature maps of face candidates found by MS-RPN and classifies whether these regions contains faces.\nIt also uses the same multi-scale technique (i.e. take feature maps from convs before pooling layers).\nIt uses some area around these face regions as additional information (suspected regions of bodies).\nSteps:\n\nReceive face candidate regions from MS-RPN.\nDo per candidate region:\n\nCalculate the suspected coordinates of the body (only based on the x/y-position and size of the face region, i.e. not learned).\nExtract the feature maps of the face region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).\nExtract the feature maps of the body region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).\nL2-normalize each feature map.\nConcatenate the (RoI-pooled and normalized) feature maps of the face (at multiple scales) with each other (creates one tensor).\nConcatenate the (RoI-pooled and normalized) feature maps of the body (at multiple scales) with each other (creates another tensor).\nApply a 1x1 convolution to the face tensor.\nApply a 1x1 convolution to the body tensor.\nApply two fully connected layers to the face tensor, creating a vector.\nApply two fully connected layers to the body tensor, creating a vector.\nConcatenate both vectors.\nBased on that vector, make a classification of whether it is really a face.\nBased on that vector, make a regression of the face's final bounding box coordinates and dimensions.\n\n\n\n\n\n\nNote: They use in both networks the multi-scale approach in order to be able to find small or tiny faces. Otherwise, after pooling these small faces would be hard or impossible to detect.\n\n Their whole model is called \"CMS-RCNN\" (Contextual Multi-Scale Region-CNN). It is based on the \"Faster R-CNN\" architecture. It uses the VGG network. Subparts of their model are: MS-RPN, CMS-CNN. MS-RPN finds candidate face regions. CMS-CNN refines their bounding boxes and classifies them (face / not face). MS-RPN (Multi-Scale Region Proposal Network)\n\n\"Looks\" at the feature maps of the network (VGG) at multiple scales (i.e. before/after pooling layers) and suggests regions for possible faces.\nSteps:\n\nFeed an image through the VGG network.\nExtract the feature maps of the three last convolutions that are before a pooling layer.\nPool these feature maps so that they have the same heights and widths.\nApply L2 normalization to each feature map so that they all have the same scale.\nApply a 1x1 convolution to merge them to one feature map.\nRegress face bounding boxes from that feature map according to the Faster R-CNN technique.\n\n\n\n \"Looks\" at the feature maps of the network (VGG) at multiple scales (i.e. before/after pooling layers) and suggests regions for possible faces. Steps:\n\nFeed an image through the VGG network.\nExtract the feature maps of the three last convolutions that are before a pooling layer.\nPool these feature maps so that they have the same heights and widths.\nApply L2 normalization to each feature map so that they all have the same scale.\nApply a 1x1 convolution to merge them to one feature map.\nRegress face bounding boxes from that feature map according to the Faster R-CNN technique.\n\n Feed an image through the VGG network. Extract the feature maps of the three last convolutions that are before a pooling layer. Pool these feature maps so that they have the same heights and widths. Apply L2 normalization to each feature map so that they all have the same scale. Apply a 1x1 convolution to merge them to one feature map. Regress face bounding boxes from that feature map according to the Faster R-CNN technique. CMS-CNN (Contextual Multi-Scale CNN):\n\n\"Looks\" at feature maps of face candidates found by MS-RPN and classifies whether these regions contains faces.\nIt also uses the same multi-scale technique (i.e. take feature maps from convs before pooling layers).\nIt uses some area around these face regions as additional information (suspected regions of bodies).\nSteps:\n\nReceive face candidate regions from MS-RPN.\nDo per candidate region:\n\nCalculate the suspected coordinates of the body (only based on the x/y-position and size of the face region, i.e. not learned).\nExtract the feature maps of the face region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).\nExtract the feature maps of the body region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).\nL2-normalize each feature map.\nConcatenate the (RoI-pooled and normalized) feature maps of the face (at multiple scales) with each other (creates one tensor).\nConcatenate the (RoI-pooled and normalized) feature maps of the body (at multiple scales) with each other (creates another tensor).\nApply a 1x1 convolution to the face tensor.\nApply a 1x1 convolution to the body tensor.\nApply two fully connected layers to the face tensor, creating a vector.\nApply two fully connected layers to the body tensor, creating a vector.\nConcatenate both vectors.\nBased on that vector, make a classification of whether it is really a face.\nBased on that vector, make a regression of the face's final bounding box coordinates and dimensions.\n\n\n\n\n\n \"Looks\" at feature maps of face candidates found by MS-RPN and classifies whether these regions contains faces. It also uses the same multi-scale technique (i.e. take feature maps from convs before pooling layers). It uses some area around these face regions as additional information (suspected regions of bodies). Steps:\n\nReceive face candidate regions from MS-RPN.\nDo per candidate region:\n\nCalculate the suspected coordinates of the body (only based on the x/y-position and size of the face region, i.e. not learned).\nExtract the feature maps of the face region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).\nExtract the feature maps of the body region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).\nL2-normalize each feature map.\nConcatenate the (RoI-pooled and normalized) feature maps of the face (at multiple scales) with each other (creates one tensor).\nConcatenate the (RoI-pooled and normalized) feature maps of the body (at multiple scales) with each other (creates another tensor).\nApply a 1x1 convolution to the face tensor.\nApply a 1x1 convolution to the body tensor.\nApply two fully connected layers to the face tensor, creating a vector.\nApply two fully connected layers to the body tensor, creating a vector.\nConcatenate both vectors.\nBased on that vector, make a classification of whether it is really a face.\nBased on that vector, make a regression of the face's final bounding box coordinates and dimensions.\n\n\n\n Receive face candidate regions from MS-RPN. Do per candidate region:\n\nCalculate the suspected coordinates of the body (only based on the x/y-position and size of the face region, i.e. not learned).\nExtract the feature maps of the face region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).\nExtract the feature maps of the body region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).\nL2-normalize each feature map.\nConcatenate the (RoI-pooled and normalized) feature maps of the face (at multiple scales) with each other (creates one tensor).\nConcatenate the (RoI-pooled and normalized) feature maps of the body (at multiple scales) with each other (creates another tensor).\nApply a 1x1 convolution to the face tensor.\nApply a 1x1 convolution to the body tensor.\nApply two fully connected layers to the face tensor, creating a vector.\nApply two fully connected layers to the body tensor, creating a vector.\nConcatenate both vectors.\nBased on that vector, make a classification of whether it is really a face.\nBased on that vector, make a regression of the face's final bounding box coordinates and dimensions.\n\n Calculate the suspected coordinates of the body (only based on the x/y-position and size of the face region, i.e. not learned). Extract the feature maps of the face region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width). Extract the feature maps of the body region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width). L2-normalize each feature map. Concatenate the (RoI-pooled and normalized) feature maps of the face (at multiple scales) with each other (creates one tensor). Concatenate the (RoI-pooled and normalized) feature maps of the body (at multiple scales) with each other (creates another tensor). Apply a 1x1 convolution to the face tensor. Apply a 1x1 convolution to the body tensor. Apply two fully connected layers to the face tensor, creating a vector. Apply two fully connected layers to the body tensor, creating a vector. Concatenate both vectors. Based on that vector, make a classification of whether it is really a face. Based on that vector, make a regression of the face's final bounding box coordinates and dimensions. Note: They use in both networks the multi-scale approach in order to be able to find small or tiny faces. Otherwise, after pooling these small faces would be hard or impossible to detect. \nResults\n\nAdding context to the classification (i.e. the body regions) empirically improves the results.\nTheir model achieves the highest recall rate on FDDB compared to other models. However, it has lower recall if only very few false positives are accepted.\nFDDB ROC curves (theirs is bold red):\n\n\n\n\nExample results on FDDB:\n\n\n\n\n\n Adding context to the classification (i.e. the body regions) empirically improves the results. Their model achieves the highest recall rate on FDDB compared to other models. However, it has lower recall if only very few false positives are accepted. FDDB ROC curves (theirs is bold red):\n\n\n\n  Example results on FDDB:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1606.05413v1"
    },
    "54": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Conditional_Image_Generation_with_PixelCNN_Decoders.md",
        "transcript": "\nWhat\n\nPixelRNN\n\nPixelRNNs generate new images pixel by pixel (and row by row) via LSTMs (or other RNNs).\nEach pixel is therefore conditioned on the previously generated pixels.\nTraining of PixelRNNs is slow due to the RNN-architecture (hard to parallelize).\nPreviously PixelCNNs have been suggested, which use masked convolutions during training (instead of RNNs), but their image quality was worse.\n\n\nThey suggest changes to PixelCNNs that improve the quality of the generated images (while still keeping them faster than RNNs).\n\n PixelRNN\n\nPixelRNNs generate new images pixel by pixel (and row by row) via LSTMs (or other RNNs).\nEach pixel is therefore conditioned on the previously generated pixels.\nTraining of PixelRNNs is slow due to the RNN-architecture (hard to parallelize).\nPreviously PixelCNNs have been suggested, which use masked convolutions during training (instead of RNNs), but their image quality was worse.\n\n PixelRNNs generate new images pixel by pixel (and row by row) via LSTMs (or other RNNs). Each pixel is therefore conditioned on the previously generated pixels. Training of PixelRNNs is slow due to the RNN-architecture (hard to parallelize). Previously PixelCNNs have been suggested, which use masked convolutions during training (instead of RNNs), but their image quality was worse. They suggest changes to PixelCNNs that improve the quality of the generated images (while still keeping them faster than RNNs). \nHow\n\nPixelRNNs split up the distribution p(image) into many conditional probabilities, one per pixel, each conditioned on all previous pixels: p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1).\nPixelCNNs implement that using convolutions, which are faster to train than RNNs.\n\nThese convolutions uses masked filters, i.e. the center weight and also all weights right and/or below the center pixel are 0 (because they are current/future values and we only want to condition on the past).\nIn most generative models, several layers are stacked, ultimately ending in three float values per pixel (RGB images, one value for grayscale images). PixelRNNs (including this implementation) traditionally end in a softmax over 255 values per pixel and channel (so 3*255 per RGB pixel).\nThe following image shows the application of such a convolution with the softmax output (left) and the mask for a filter (right):\n\n\n\n\n\n\nBlind spot\n\nUsing the mask on each convolutional filter effectively converts them into non-squared shapes (the green values in the image).\nAdvantage: Using such non-squared convolutions prevents future values from leaking into present values.\nDisadvantage: Using such non-squared convolutions creates blind spots, i.e. for each pixel, some past values (diagonally top-right from it) cannot influence the value of that pixel.\n\n\n\n\nThey combine horizontal (1xN) and vertical (Nx1) convolutions to prevent that.\n\n\nGated convolutions\n\nPixelRNNs via LSTMs so far created visually better images than PixelCNNs.\nThey assume that one advantage of LSTMs is, that they (also) have multiplicative gates, while stacked convolutional layers only operate with summations.\nThey alleviate that problem by adding gates to their convolutions:\n\nEquation: output image = tanh(weights_1 * image) <element-wise product> sigmoid(weights_2 * image)\n\n* is the convolutional operator.\ntanh(weights_1 * image) is a classical convolution with tanh activation function.\nsigmoid(weights_2 * image) are the gate values (0 = gate closed, 1 = gate open).\nweights_1 and weights_2 are learned.\n\n\n\n\n\n\nConditional PixelCNNs\n\nWhen generating images, they do not only want to condition the previous values, but also on a laten vector h that describes the image to generate.\nThe new image distribution becomes: p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1, h).\nTo implement that, they simply modify the previously mentioned gated convolution, adding h to it:\n\nEquation: output image = tanh(weights_1 * image + weights_2 . h) <element-wise product> sigmoid(weights_3 * image + weights_4 . h)\n\n. denotes here the matrix-vector multiplication.\n\n\n\n\n\n\nPixelCNN Autoencoder\n\nThe decoder in a standard autoencoder can be replaced by a PixelCNN, creating a PixelCNN-Autoencoder.\n\n\n\n PixelRNNs split up the distribution p(image) into many conditional probabilities, one per pixel, each conditioned on all previous pixels: p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1). PixelCNNs implement that using convolutions, which are faster to train than RNNs.\n\nThese convolutions uses masked filters, i.e. the center weight and also all weights right and/or below the center pixel are 0 (because they are current/future values and we only want to condition on the past).\nIn most generative models, several layers are stacked, ultimately ending in three float values per pixel (RGB images, one value for grayscale images). PixelRNNs (including this implementation) traditionally end in a softmax over 255 values per pixel and channel (so 3*255 per RGB pixel).\nThe following image shows the application of such a convolution with the softmax output (left) and the mask for a filter (right):\n\n\n\n\n\n These convolutions uses masked filters, i.e. the center weight and also all weights right and/or below the center pixel are 0 (because they are current/future values and we only want to condition on the past). In most generative models, several layers are stacked, ultimately ending in three float values per pixel (RGB images, one value for grayscale images). PixelRNNs (including this implementation) traditionally end in a softmax over 255 values per pixel and channel (so 3*255 per RGB pixel). The following image shows the application of such a convolution with the softmax output (left) and the mask for a filter (right):\n\n\n\n  Blind spot\n\nUsing the mask on each convolutional filter effectively converts them into non-squared shapes (the green values in the image).\nAdvantage: Using such non-squared convolutions prevents future values from leaking into present values.\nDisadvantage: Using such non-squared convolutions creates blind spots, i.e. for each pixel, some past values (diagonally top-right from it) cannot influence the value of that pixel.\n\n\n\n\nThey combine horizontal (1xN) and vertical (Nx1) convolutions to prevent that.\n\n Using the mask on each convolutional filter effectively converts them into non-squared shapes (the green values in the image). Advantage: Using such non-squared convolutions prevents future values from leaking into present values. Disadvantage: Using such non-squared convolutions creates blind spots, i.e. for each pixel, some past values (diagonally top-right from it) cannot influence the value of that pixel.\n\n\n\n  They combine horizontal (1xN) and vertical (Nx1) convolutions to prevent that. Gated convolutions\n\nPixelRNNs via LSTMs so far created visually better images than PixelCNNs.\nThey assume that one advantage of LSTMs is, that they (also) have multiplicative gates, while stacked convolutional layers only operate with summations.\nThey alleviate that problem by adding gates to their convolutions:\n\nEquation: output image = tanh(weights_1 * image) <element-wise product> sigmoid(weights_2 * image)\n\n* is the convolutional operator.\ntanh(weights_1 * image) is a classical convolution with tanh activation function.\nsigmoid(weights_2 * image) are the gate values (0 = gate closed, 1 = gate open).\nweights_1 and weights_2 are learned.\n\n\n\n\n\n PixelRNNs via LSTMs so far created visually better images than PixelCNNs. They assume that one advantage of LSTMs is, that they (also) have multiplicative gates, while stacked convolutional layers only operate with summations. They alleviate that problem by adding gates to their convolutions:\n\nEquation: output image = tanh(weights_1 * image) <element-wise product> sigmoid(weights_2 * image)\n\n* is the convolutional operator.\ntanh(weights_1 * image) is a classical convolution with tanh activation function.\nsigmoid(weights_2 * image) are the gate values (0 = gate closed, 1 = gate open).\nweights_1 and weights_2 are learned.\n\n\n\n Equation: output image = tanh(weights_1 * image) <element-wise product> sigmoid(weights_2 * image)\n\n* is the convolutional operator.\ntanh(weights_1 * image) is a classical convolution with tanh activation function.\nsigmoid(weights_2 * image) are the gate values (0 = gate closed, 1 = gate open).\nweights_1 and weights_2 are learned.\n\n * is the convolutional operator. tanh(weights_1 * image) is a classical convolution with tanh activation function. sigmoid(weights_2 * image) are the gate values (0 = gate closed, 1 = gate open). weights_1 and weights_2 are learned. Conditional PixelCNNs\n\nWhen generating images, they do not only want to condition the previous values, but also on a laten vector h that describes the image to generate.\nThe new image distribution becomes: p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1, h).\nTo implement that, they simply modify the previously mentioned gated convolution, adding h to it:\n\nEquation: output image = tanh(weights_1 * image + weights_2 . h) <element-wise product> sigmoid(weights_3 * image + weights_4 . h)\n\n. denotes here the matrix-vector multiplication.\n\n\n\n\n\n When generating images, they do not only want to condition the previous values, but also on a laten vector h that describes the image to generate. The new image distribution becomes: p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1, h). To implement that, they simply modify the previously mentioned gated convolution, adding h to it:\n\nEquation: output image = tanh(weights_1 * image + weights_2 . h) <element-wise product> sigmoid(weights_3 * image + weights_4 . h)\n\n. denotes here the matrix-vector multiplication.\n\n\n\n Equation: output image = tanh(weights_1 * image + weights_2 . h) <element-wise product> sigmoid(weights_3 * image + weights_4 . h)\n\n. denotes here the matrix-vector multiplication.\n\n . denotes here the matrix-vector multiplication. PixelCNN Autoencoder\n\nThe decoder in a standard autoencoder can be replaced by a PixelCNN, creating a PixelCNN-Autoencoder.\n\n The decoder in a standard autoencoder can be replaced by a PixelCNN, creating a PixelCNN-Autoencoder. \nResults\n\nThey achieve similar NLL-results as PixelRNN on CIFAR-10 and ImageNet, while training about twice as fast.\n\nHere, \"fast\" means that they used 32 GPUs for 60 hours.\n\n\nUsing Conditional PixelCNNs on ImageNet (i.e. adding class information to each convolution) did not improve the NLL-score, but it did improve the image quality.\n\n\n\n\nThey use a different neural network to create embeddings of human faces. Then they generate new faces based on these embeddings via PixelCNN.\n\n\n\n\nTheir PixelCNN-Autoencoder generates significantly sharper (i.e. less blurry) images than a \"normal\" autoencoder.\n\n They achieve similar NLL-results as PixelRNN on CIFAR-10 and ImageNet, while training about twice as fast.\n\nHere, \"fast\" means that they used 32 GPUs for 60 hours.\n\n Here, \"fast\" means that they used 32 GPUs for 60 hours. Using Conditional PixelCNNs on ImageNet (i.e. adding class information to each convolution) did not improve the NLL-score, but it did improve the image quality.\n\n\n\n  They use a different neural network to create embeddings of human faces. Then they generate new faces based on these embeddings via PixelCNN.\n\n\n\n  Their PixelCNN-Autoencoder generates significantly sharper (i.e. less blurry) images than a \"normal\" autoencoder. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1606.05328"
    },
    "55": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/InfoGAN.md",
        "transcript": "\nWhat\n\nUsually GANs transform a noise vector z into images. z might be sampled from a normal or uniform distribution.\nThe effect of this is, that the components in z are deeply entangled.\n\nChanging single components has hardly any influence on the generated images. One has to change multiple components to affect the image.\nThe components end up not being interpretable. Ideally one would like to have meaningful components, e.g. for human faces one that controls the hair length and a categorical one that controls the eye color.\n\n\nThey suggest a change to GANs based on Mutual Information, which leads to interpretable components.\n\nE.g. for MNIST a component that controls the stroke thickness and a categorical component that controls the digit identity (1, 2, 3, ...).\nThese components are learned in a (mostly) unsupervised fashion.\n\n\n\n Usually GANs transform a noise vector z into images. z might be sampled from a normal or uniform distribution. The effect of this is, that the components in z are deeply entangled.\n\nChanging single components has hardly any influence on the generated images. One has to change multiple components to affect the image.\nThe components end up not being interpretable. Ideally one would like to have meaningful components, e.g. for human faces one that controls the hair length and a categorical one that controls the eye color.\n\n Changing single components has hardly any influence on the generated images. One has to change multiple components to affect the image. The components end up not being interpretable. Ideally one would like to have meaningful components, e.g. for human faces one that controls the hair length and a categorical one that controls the eye color. They suggest a change to GANs based on Mutual Information, which leads to interpretable components.\n\nE.g. for MNIST a component that controls the stroke thickness and a categorical component that controls the digit identity (1, 2, 3, ...).\nThese components are learned in a (mostly) unsupervised fashion.\n\n E.g. for MNIST a component that controls the stroke thickness and a categorical component that controls the digit identity (1, 2, 3, ...). These components are learned in a (mostly) unsupervised fashion. \nHow\n\nThe latent code c\n\n\"Normal\" GANs parameterize the generator as G(z), i.e. G receives a noise vector and transforms it into an image.\nThis is changed to G(z, c), i.e. G now receives a noise vector z and a latent code c and transforms both into an image.\nc can contain multiple variables following different distributions, e.g. in MNIST a categorical variable for the digit identity and a gaussian one for the stroke thickness.\n\n\nMutual Information\n\nIf using a latent code via G(z, c), nothing forces the generator to actually use c. It can easily ignore it and just deteriorate to G(z).\nTo prevent that, they force G to generate images x in a way that c must be recoverable. So, if you have an image x you must be able to reliable tell which latent code c it has, which means that G must use c in a meaningful way.\nThis relationship can be expressed with mutual information, i.e. the mutual information between x and c must be high.\n\nThe mutual information between two variables X and Y is defined as I(X; Y) = entropy(X) - entropy(X|Y) = entropy(Y) - entropy(Y|X).\nIf the mutual information between X and Y is high, then knowing Y helps you to decently predict the value of X (and the other way round).\nIf the mutual information between X and Y is low, then knowing Y doesn't tell you much about the value of X (and the other way round).\n\n\nThe new GAN loss becomes old loss - lambda * I(G(z, c); c), i.e. the higher the mutual information, the lower the result of the loss function.\n\n\nVariational Mutual Information Maximization\n\nIn order to minimize I(G(z, c); c), one has to know the distribution P(c|x) (from image to latent code), which however is unknown.\nSo instead they create Q(c|x), which is an approximation of P(c|x).\nI(G(z, c); c) is then computed using a lower bound maximization, similar to the one in variational autoencoders (called \"Variational Information Maximization\", hence the name \"InfoGAN\").\nBasic equation: LowerBoundOfMutualInformation(G, Q) = E[log Q(c|x)] + H(c) <= I(G(z, c); c)\n\nc is the latent code.\nx is the generated image.\nH(c) is the entropy of the latent codes (constant throughout the optimization).\nOptimization w.r.t. Q is done directly.\nOptimization w.r.t. G is done via the reparameterization trick.\n\n\nIf Q(c|x) approximates P(c|x) perfectly, the lower bound becomes the mutual information (\"the lower bound becomes tight\").\nIn practice, Q(c|x) is implemented as a neural network. Both Q and D have to process the generated images, which means that they can share many convolutional layers, significantly reducing the extra cost of training Q.\n\n\n\n The latent code c\n\n\"Normal\" GANs parameterize the generator as G(z), i.e. G receives a noise vector and transforms it into an image.\nThis is changed to G(z, c), i.e. G now receives a noise vector z and a latent code c and transforms both into an image.\nc can contain multiple variables following different distributions, e.g. in MNIST a categorical variable for the digit identity and a gaussian one for the stroke thickness.\n\n \"Normal\" GANs parameterize the generator as G(z), i.e. G receives a noise vector and transforms it into an image. This is changed to G(z, c), i.e. G now receives a noise vector z and a latent code c and transforms both into an image. c can contain multiple variables following different distributions, e.g. in MNIST a categorical variable for the digit identity and a gaussian one for the stroke thickness. Mutual Information\n\nIf using a latent code via G(z, c), nothing forces the generator to actually use c. It can easily ignore it and just deteriorate to G(z).\nTo prevent that, they force G to generate images x in a way that c must be recoverable. So, if you have an image x you must be able to reliable tell which latent code c it has, which means that G must use c in a meaningful way.\nThis relationship can be expressed with mutual information, i.e. the mutual information between x and c must be high.\n\nThe mutual information between two variables X and Y is defined as I(X; Y) = entropy(X) - entropy(X|Y) = entropy(Y) - entropy(Y|X).\nIf the mutual information between X and Y is high, then knowing Y helps you to decently predict the value of X (and the other way round).\nIf the mutual information between X and Y is low, then knowing Y doesn't tell you much about the value of X (and the other way round).\n\n\nThe new GAN loss becomes old loss - lambda * I(G(z, c); c), i.e. the higher the mutual information, the lower the result of the loss function.\n\n If using a latent code via G(z, c), nothing forces the generator to actually use c. It can easily ignore it and just deteriorate to G(z). To prevent that, they force G to generate images x in a way that c must be recoverable. So, if you have an image x you must be able to reliable tell which latent code c it has, which means that G must use c in a meaningful way. This relationship can be expressed with mutual information, i.e. the mutual information between x and c must be high.\n\nThe mutual information between two variables X and Y is defined as I(X; Y) = entropy(X) - entropy(X|Y) = entropy(Y) - entropy(Y|X).\nIf the mutual information between X and Y is high, then knowing Y helps you to decently predict the value of X (and the other way round).\nIf the mutual information between X and Y is low, then knowing Y doesn't tell you much about the value of X (and the other way round).\n\n The mutual information between two variables X and Y is defined as I(X; Y) = entropy(X) - entropy(X|Y) = entropy(Y) - entropy(Y|X). If the mutual information between X and Y is high, then knowing Y helps you to decently predict the value of X (and the other way round). If the mutual information between X and Y is low, then knowing Y doesn't tell you much about the value of X (and the other way round). The new GAN loss becomes old loss - lambda * I(G(z, c); c), i.e. the higher the mutual information, the lower the result of the loss function. Variational Mutual Information Maximization\n\nIn order to minimize I(G(z, c); c), one has to know the distribution P(c|x) (from image to latent code), which however is unknown.\nSo instead they create Q(c|x), which is an approximation of P(c|x).\nI(G(z, c); c) is then computed using a lower bound maximization, similar to the one in variational autoencoders (called \"Variational Information Maximization\", hence the name \"InfoGAN\").\nBasic equation: LowerBoundOfMutualInformation(G, Q) = E[log Q(c|x)] + H(c) <= I(G(z, c); c)\n\nc is the latent code.\nx is the generated image.\nH(c) is the entropy of the latent codes (constant throughout the optimization).\nOptimization w.r.t. Q is done directly.\nOptimization w.r.t. G is done via the reparameterization trick.\n\n\nIf Q(c|x) approximates P(c|x) perfectly, the lower bound becomes the mutual information (\"the lower bound becomes tight\").\nIn practice, Q(c|x) is implemented as a neural network. Both Q and D have to process the generated images, which means that they can share many convolutional layers, significantly reducing the extra cost of training Q.\n\n In order to minimize I(G(z, c); c), one has to know the distribution P(c|x) (from image to latent code), which however is unknown. So instead they create Q(c|x), which is an approximation of P(c|x). I(G(z, c); c) is then computed using a lower bound maximization, similar to the one in variational autoencoders (called \"Variational Information Maximization\", hence the name \"InfoGAN\"). Basic equation: LowerBoundOfMutualInformation(G, Q) = E[log Q(c|x)] + H(c) <= I(G(z, c); c)\n\nc is the latent code.\nx is the generated image.\nH(c) is the entropy of the latent codes (constant throughout the optimization).\nOptimization w.r.t. Q is done directly.\nOptimization w.r.t. G is done via the reparameterization trick.\n\n c is the latent code. x is the generated image. H(c) is the entropy of the latent codes (constant throughout the optimization). Optimization w.r.t. Q is done directly. Optimization w.r.t. G is done via the reparameterization trick. If Q(c|x) approximates P(c|x) perfectly, the lower bound becomes the mutual information (\"the lower bound becomes tight\"). In practice, Q(c|x) is implemented as a neural network. Both Q and D have to process the generated images, which means that they can share many convolutional layers, significantly reducing the extra cost of training Q. \nResults\n\nMNIST\n\nThey use for c one categorical variable (10 values) and two continuous ones (uniform between -1 and +1).\nInfoGAN learns to associate the categorical one with the digit identity and the continuous ones with rotation and width.\nApplying Q(c|x) to an image and then classifying only on the categorical variable (i.e. fully unsupervised) yields 95% accuracy.\nSampling new images with exaggerated continuous variables in the range [-2,+2] yields sound images (i.e. the network generalizes well).\n\n\n\n3D face images\n\nInfoGAN learns to represent the faces via pose, elevation, lighting.\nThey used five uniform variables for c. (So two of them apparently weren't associated with anything sensible? They are not mentioned.)\n\n\n3D chair images\n\nInfoGAN learns to represent the chairs via identity (categorical) and rotation or width (apparently they did two experiments).\nThey used one categorical variable (four values) and one continuous variable (uniform [-1, +1]).\n\n\nSVHN\n\nInfoGAN learns to represent lighting and to spot the center digit.\nThey used four categorical variables (10 values each) and two continuous variables (uniform [-1, +1]). (Again, a few variables were apparently not associated with anything sensible?)\n\n\nCelebA\n\nInfoGAN learns to represent pose, presence of sunglasses (not perfectly), hair style and emotion (in the sense of \"smiling or not smiling\").\nThey used 10 categorical variables (10 values each). (Again, a few variables were apparently not associated with anything sensible?)\n\n\n\n\n MNIST\n\nThey use for c one categorical variable (10 values) and two continuous ones (uniform between -1 and +1).\nInfoGAN learns to associate the categorical one with the digit identity and the continuous ones with rotation and width.\nApplying Q(c|x) to an image and then classifying only on the categorical variable (i.e. fully unsupervised) yields 95% accuracy.\nSampling new images with exaggerated continuous variables in the range [-2,+2] yields sound images (i.e. the network generalizes well).\n\n\n They use for c one categorical variable (10 values) and two continuous ones (uniform between -1 and +1). InfoGAN learns to associate the categorical one with the digit identity and the continuous ones with rotation and width. Applying Q(c|x) to an image and then classifying only on the categorical variable (i.e. fully unsupervised) yields 95% accuracy. Sampling new images with exaggerated continuous variables in the range [-2,+2] yields sound images (i.e. the network generalizes well).  3D face images\n\nInfoGAN learns to represent the faces via pose, elevation, lighting.\nThey used five uniform variables for c. (So two of them apparently weren't associated with anything sensible? They are not mentioned.)\n\n InfoGAN learns to represent the faces via pose, elevation, lighting. They used five uniform variables for c. (So two of them apparently weren't associated with anything sensible? They are not mentioned.) 3D chair images\n\nInfoGAN learns to represent the chairs via identity (categorical) and rotation or width (apparently they did two experiments).\nThey used one categorical variable (four values) and one continuous variable (uniform [-1, +1]).\n\n InfoGAN learns to represent the chairs via identity (categorical) and rotation or width (apparently they did two experiments). They used one categorical variable (four values) and one continuous variable (uniform [-1, +1]). SVHN\n\nInfoGAN learns to represent lighting and to spot the center digit.\nThey used four categorical variables (10 values each) and two continuous variables (uniform [-1, +1]). (Again, a few variables were apparently not associated with anything sensible?)\n\n InfoGAN learns to represent lighting and to spot the center digit. They used four categorical variables (10 values each) and two continuous variables (uniform [-1, +1]). (Again, a few variables were apparently not associated with anything sensible?) CelebA\n\nInfoGAN learns to represent pose, presence of sunglasses (not perfectly), hair style and emotion (in the sense of \"smiling or not smiling\").\nThey used 10 categorical variables (10 values each). (Again, a few variables were apparently not associated with anything sensible?)\n\n\n InfoGAN learns to represent pose, presence of sunglasses (not perfectly), hair style and emotion (in the sense of \"smiling or not smiling\"). They used 10 categorical variables (10 values each). (Again, a few variables were apparently not associated with anything sensible?)  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.03657"
    },
    "56": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Improved_Techniques_for_Training_GANs.md",
        "transcript": "\nWhat\n\nThey suggest some small changes to the GAN training scheme that lead to visually improved results.\nThey suggest a new scoring method to compare the results of different GAN models with each other.\n\n They suggest some small changes to the GAN training scheme that lead to visually improved results. They suggest a new scoring method to compare the results of different GAN models with each other. \nHow\n\nFeature Matching\n\nUsually G would be trained to mislead D as often as possible, i.e. to maximize D's output.\nNow they train G to minimize the feature distance between real and fake images. I.e. they do:\n\nPick a layer l from D.\nForward real images through D and extract the features from layer l.\nForward fake images through D and extract the features from layer l.\nCompute the squared euclidean distance between the layers and backpropagate.\n\n\n\n\nMinibatch discrimination\n\nThey allow D to look at multiple images in the same minibatch.\nThat is, they feed the features (of each image) extracted by an intermediate layer of D through a linear operation, resulting in a matrix per image.\nThey then compute the L1-distances between these matrices.\nThey then let D make its judgement (fake/real image) based on the features extracted from the image and these distances.\nThey add this mechanism so that the diversity of images generated by G increases (which should also prevent collapses).\n\n\nHistorical averaging\n\nThey add a penalty term that punishes weights which are rather far away from their historical average values.\nI.e. the cost is distance(current parameters, average of parameters over the last t batches).\nThey argue that this can help the network to find equilibria that normal gradient descent would not find.\n\n\nOne-sided label smoothing\n\nUsually one would use the labels 0 (image is fake) and 1 (image is real).\nUsing smoother labels (0.1 and 0.9) seems to make networks more resistent to adversarial examples.\n\u00a0 \u00a0* So they smooth the labels of real images (apparently to 0.9?).\nSmoothing the labels of fake images would lead to (mathematical) problems in some cases, so they keep these at 0.\n\n\nVirtual Batch Normalization (VBN)\n\nUsually BN normalizes each example with respect to the other examples in the same batch.\nThey instead normalize each example with respect to the examples in a reference batch, which was picked once at the start of the training.\nVBN is intended to reduce the dependence of each example on the other examples in the batch.\nVBN is computationally expensive, because it requires forwarding of two minibatches.\nThey use VBN for their G.\n\n\nInception Scoring\n\nThey introduce a new scoring method for GAN results.\nTheir method is based on feeding the generated images through another network, here they use Inception.\nFor an image x and predicted classes y (softmax-output of Inception):\n\nThey argue that they want p(y|x) to have low entropy, i.e. the model should be rather certain of seeing a class (or few classes) in the image.\nThey argue that they want p(y) to have high entropy, i.e. the predicted classes (and therefore image contents) should have high diversity. (This seems like something that is quite a bit dependend on the used dataset?)\nThey combine both measurements to the final score of exp(KL(p(y|x) || p(y))) = exp( <sum over images> p(y|xi) * (log(p(y|xi)) - log(p(y))) ).\n\np(y) can be approximated as the mean of the softmax-outputs over many examples.\n\n\nRelevant python code that they use (where part seems to be of shape (batch size, number of classes), i.e. the softmax outputs): kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0))); kl = np.mean(np.sum(kl, 1)); scores.append(np.exp(kl));\n\n\nThey average this score over 50,000 generated images.\n\n\nSemi-supervised Learning\n\nFor a dataset with K classes they extend D by K outputs (leading to K+1 outputs total).\nThey then optimize two loss functions jointly:\n\nUnsupervised loss: The classic GAN loss, i.e. D has to predict the fake/real output correctly. (The other outputs seem to not influence this loss.)\nSupervised loss: D must correctly predict the image's class label, if it happens to be a real image and if it was annotated with a class.\n\n\nThey note that training G with feature matching produces the best results for semi-supervised classification.\nThey note that training G with minibatch discrimination produces significantly worse results for semi-supervised classification. (But visually the samples look better.)\nThey note that using semi-supervised learning overall results in higher image quality than not using it. They speculate that this has to do with the class labels containing information about image statistics that are important to humans.\n\n\n\n Feature Matching\n\nUsually G would be trained to mislead D as often as possible, i.e. to maximize D's output.\nNow they train G to minimize the feature distance between real and fake images. I.e. they do:\n\nPick a layer l from D.\nForward real images through D and extract the features from layer l.\nForward fake images through D and extract the features from layer l.\nCompute the squared euclidean distance between the layers and backpropagate.\n\n\n\n Usually G would be trained to mislead D as often as possible, i.e. to maximize D's output. Now they train G to minimize the feature distance between real and fake images. I.e. they do:\n\nPick a layer l from D.\nForward real images through D and extract the features from layer l.\nForward fake images through D and extract the features from layer l.\nCompute the squared euclidean distance between the layers and backpropagate.\n\n Pick a layer l from D. Forward real images through D and extract the features from layer l. Forward fake images through D and extract the features from layer l. Compute the squared euclidean distance between the layers and backpropagate. Minibatch discrimination\n\nThey allow D to look at multiple images in the same minibatch.\nThat is, they feed the features (of each image) extracted by an intermediate layer of D through a linear operation, resulting in a matrix per image.\nThey then compute the L1-distances between these matrices.\nThey then let D make its judgement (fake/real image) based on the features extracted from the image and these distances.\nThey add this mechanism so that the diversity of images generated by G increases (which should also prevent collapses).\n\n They allow D to look at multiple images in the same minibatch. That is, they feed the features (of each image) extracted by an intermediate layer of D through a linear operation, resulting in a matrix per image. They then compute the L1-distances between these matrices. They then let D make its judgement (fake/real image) based on the features extracted from the image and these distances. They add this mechanism so that the diversity of images generated by G increases (which should also prevent collapses). Historical averaging\n\nThey add a penalty term that punishes weights which are rather far away from their historical average values.\nI.e. the cost is distance(current parameters, average of parameters over the last t batches).\nThey argue that this can help the network to find equilibria that normal gradient descent would not find.\n\n They add a penalty term that punishes weights which are rather far away from their historical average values. I.e. the cost is distance(current parameters, average of parameters over the last t batches). They argue that this can help the network to find equilibria that normal gradient descent would not find. One-sided label smoothing\n\nUsually one would use the labels 0 (image is fake) and 1 (image is real).\nUsing smoother labels (0.1 and 0.9) seems to make networks more resistent to adversarial examples.\n\u00a0 \u00a0* So they smooth the labels of real images (apparently to 0.9?).\nSmoothing the labels of fake images would lead to (mathematical) problems in some cases, so they keep these at 0.\n\n Usually one would use the labels 0 (image is fake) and 1 (image is real). Using smoother labels (0.1 and 0.9) seems to make networks more resistent to adversarial examples.\n\u00a0 \u00a0* So they smooth the labels of real images (apparently to 0.9?). Smoothing the labels of fake images would lead to (mathematical) problems in some cases, so they keep these at 0. Virtual Batch Normalization (VBN)\n\nUsually BN normalizes each example with respect to the other examples in the same batch.\nThey instead normalize each example with respect to the examples in a reference batch, which was picked once at the start of the training.\nVBN is intended to reduce the dependence of each example on the other examples in the batch.\nVBN is computationally expensive, because it requires forwarding of two minibatches.\nThey use VBN for their G.\n\n Usually BN normalizes each example with respect to the other examples in the same batch. They instead normalize each example with respect to the examples in a reference batch, which was picked once at the start of the training. VBN is intended to reduce the dependence of each example on the other examples in the batch. VBN is computationally expensive, because it requires forwarding of two minibatches. They use VBN for their G. Inception Scoring\n\nThey introduce a new scoring method for GAN results.\nTheir method is based on feeding the generated images through another network, here they use Inception.\nFor an image x and predicted classes y (softmax-output of Inception):\n\nThey argue that they want p(y|x) to have low entropy, i.e. the model should be rather certain of seeing a class (or few classes) in the image.\nThey argue that they want p(y) to have high entropy, i.e. the predicted classes (and therefore image contents) should have high diversity. (This seems like something that is quite a bit dependend on the used dataset?)\nThey combine both measurements to the final score of exp(KL(p(y|x) || p(y))) = exp( <sum over images> p(y|xi) * (log(p(y|xi)) - log(p(y))) ).\n\np(y) can be approximated as the mean of the softmax-outputs over many examples.\n\n\nRelevant python code that they use (where part seems to be of shape (batch size, number of classes), i.e. the softmax outputs): kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0))); kl = np.mean(np.sum(kl, 1)); scores.append(np.exp(kl));\n\n\nThey average this score over 50,000 generated images.\n\n They introduce a new scoring method for GAN results. Their method is based on feeding the generated images through another network, here they use Inception. For an image x and predicted classes y (softmax-output of Inception):\n\nThey argue that they want p(y|x) to have low entropy, i.e. the model should be rather certain of seeing a class (or few classes) in the image.\nThey argue that they want p(y) to have high entropy, i.e. the predicted classes (and therefore image contents) should have high diversity. (This seems like something that is quite a bit dependend on the used dataset?)\nThey combine both measurements to the final score of exp(KL(p(y|x) || p(y))) = exp( <sum over images> p(y|xi) * (log(p(y|xi)) - log(p(y))) ).\n\np(y) can be approximated as the mean of the softmax-outputs over many examples.\n\n\nRelevant python code that they use (where part seems to be of shape (batch size, number of classes), i.e. the softmax outputs): kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0))); kl = np.mean(np.sum(kl, 1)); scores.append(np.exp(kl));\n\n They argue that they want p(y|x) to have low entropy, i.e. the model should be rather certain of seeing a class (or few classes) in the image. They argue that they want p(y) to have high entropy, i.e. the predicted classes (and therefore image contents) should have high diversity. (This seems like something that is quite a bit dependend on the used dataset?) They combine both measurements to the final score of exp(KL(p(y|x) || p(y))) = exp( <sum over images> p(y|xi) * (log(p(y|xi)) - log(p(y))) ).\n\np(y) can be approximated as the mean of the softmax-outputs over many examples.\n\n p(y) can be approximated as the mean of the softmax-outputs over many examples. Relevant python code that they use (where part seems to be of shape (batch size, number of classes), i.e. the softmax outputs): kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0))); kl = np.mean(np.sum(kl, 1)); scores.append(np.exp(kl)); They average this score over 50,000 generated images. Semi-supervised Learning\n\nFor a dataset with K classes they extend D by K outputs (leading to K+1 outputs total).\nThey then optimize two loss functions jointly:\n\nUnsupervised loss: The classic GAN loss, i.e. D has to predict the fake/real output correctly. (The other outputs seem to not influence this loss.)\nSupervised loss: D must correctly predict the image's class label, if it happens to be a real image and if it was annotated with a class.\n\n\nThey note that training G with feature matching produces the best results for semi-supervised classification.\nThey note that training G with minibatch discrimination produces significantly worse results for semi-supervised classification. (But visually the samples look better.)\nThey note that using semi-supervised learning overall results in higher image quality than not using it. They speculate that this has to do with the class labels containing information about image statistics that are important to humans.\n\n For a dataset with K classes they extend D by K outputs (leading to K+1 outputs total). They then optimize two loss functions jointly:\n\nUnsupervised loss: The classic GAN loss, i.e. D has to predict the fake/real output correctly. (The other outputs seem to not influence this loss.)\nSupervised loss: D must correctly predict the image's class label, if it happens to be a real image and if it was annotated with a class.\n\n Unsupervised loss: The classic GAN loss, i.e. D has to predict the fake/real output correctly. (The other outputs seem to not influence this loss.) Supervised loss: D must correctly predict the image's class label, if it happens to be a real image and if it was annotated with a class. They note that training G with feature matching produces the best results for semi-supervised classification. They note that training G with minibatch discrimination produces significantly worse results for semi-supervised classification. (But visually the samples look better.) They note that using semi-supervised learning overall results in higher image quality than not using it. They speculate that this has to do with the class labels containing information about image statistics that are important to humans. \nResults\n\nMNIST\n\nThey use weight normalization and white noise in D.\nSamples of high visual quality when using minibatch discrimination with semi-supervised learning.\nVery good results in semi-supervised learning when using feature matching.\nUsing feature matching decreases visual quality of generated images, but improves results of semi-supervised learning.\n\n\nCIFAR-10\n\nD: 9-layer CNN with dropout, weight normalization.\nG: 4-layer CNN with batch normalization (so no VBN?).\nVisually very good generated samples when using minibatch discrimination with semi-supervised learning. (Probably new record quality.)\n\nNote: No comparison with nearest neighbours from the dataset.\n\n\nWhen using feature matching the results are visually not as good.\nAgain, very good results in semi-supervised learning when using feature matching.\n\n\nSVHN\n\nSame setup as in CIFAR-10 and similar results.\n\n\nImageNet\n\nThey tried to generate 128x128 images and compared to DCGAN.\nThey improved from \"total garbage\" to \"garbage\" (they now hit some textures, but structure is still wildly off).\n\n\n\n MNIST\n\nThey use weight normalization and white noise in D.\nSamples of high visual quality when using minibatch discrimination with semi-supervised learning.\nVery good results in semi-supervised learning when using feature matching.\nUsing feature matching decreases visual quality of generated images, but improves results of semi-supervised learning.\n\n They use weight normalization and white noise in D. Samples of high visual quality when using minibatch discrimination with semi-supervised learning. Very good results in semi-supervised learning when using feature matching. Using feature matching decreases visual quality of generated images, but improves results of semi-supervised learning. CIFAR-10\n\nD: 9-layer CNN with dropout, weight normalization.\nG: 4-layer CNN with batch normalization (so no VBN?).\nVisually very good generated samples when using minibatch discrimination with semi-supervised learning. (Probably new record quality.)\n\nNote: No comparison with nearest neighbours from the dataset.\n\n\nWhen using feature matching the results are visually not as good.\nAgain, very good results in semi-supervised learning when using feature matching.\n\n D: 9-layer CNN with dropout, weight normalization. G: 4-layer CNN with batch normalization (so no VBN?). Visually very good generated samples when using minibatch discrimination with semi-supervised learning. (Probably new record quality.)\n\nNote: No comparison with nearest neighbours from the dataset.\n\n Note: No comparison with nearest neighbours from the dataset. When using feature matching the results are visually not as good. Again, very good results in semi-supervised learning when using feature matching. SVHN\n\nSame setup as in CIFAR-10 and similar results.\n\n Same setup as in CIFAR-10 and similar results. ImageNet\n\nThey tried to generate 128x128 images and compared to DCGAN.\nThey improved from \"total garbage\" to \"garbage\" (they now hit some textures, but structure is still wildly off).\n\n They tried to generate 128x128 images and compared to DCGAN. They improved from \"total garbage\" to \"garbage\" (they now hit some textures, but structure is still wildly off). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.03498"
    },
    "57": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Synthesizing_the_preferred_inputs_for_neurons_in_neural_networks_via_deep_generator_networks.md",
        "transcript": "\nWhat\n\nThey suggest a new method to generate images which maximize the activation of a specific neuron in a (trained) target network (abbreviated with \"DNN\").\n\nE.g. if your DNN contains a neuron that is active whenever there is a car in an image, the method should generate images containing cars.\nSuch methods can be used to investigate what exactly a network has learned.\n\n\nThere are plenty of methods like this one. They usually differ from each other by using different natural image priors.\n\nA natural image prior is a restriction on the generated images.\nSuch a prior pushes the generated images towards realistic looking ones.\nWithout such a prior it is easy to generate images that lead to high activations of specific neurons, but don't look realistic at all (e.g. they might look psychodelic or like white noise).\n\nThat's because the space of possible images is extremely high-dimensional and can therefore hardly be covered reliably by a single network. Note also that training datasets usually only show a very limited subset of all possible images.\n\n\nTheir work introduces a new natural image prior.\n\n\n\n They suggest a new method to generate images which maximize the activation of a specific neuron in a (trained) target network (abbreviated with \"DNN\").\n\nE.g. if your DNN contains a neuron that is active whenever there is a car in an image, the method should generate images containing cars.\nSuch methods can be used to investigate what exactly a network has learned.\n\n E.g. if your DNN contains a neuron that is active whenever there is a car in an image, the method should generate images containing cars. Such methods can be used to investigate what exactly a network has learned. There are plenty of methods like this one. They usually differ from each other by using different natural image priors.\n\nA natural image prior is a restriction on the generated images.\nSuch a prior pushes the generated images towards realistic looking ones.\nWithout such a prior it is easy to generate images that lead to high activations of specific neurons, but don't look realistic at all (e.g. they might look psychodelic or like white noise).\n\nThat's because the space of possible images is extremely high-dimensional and can therefore hardly be covered reliably by a single network. Note also that training datasets usually only show a very limited subset of all possible images.\n\n\nTheir work introduces a new natural image prior.\n\n A natural image prior is a restriction on the generated images. Such a prior pushes the generated images towards realistic looking ones. Without such a prior it is easy to generate images that lead to high activations of specific neurons, but don't look realistic at all (e.g. they might look psychodelic or like white noise).\n\nThat's because the space of possible images is extremely high-dimensional and can therefore hardly be covered reliably by a single network. Note also that training datasets usually only show a very limited subset of all possible images.\n\n That's because the space of possible images is extremely high-dimensional and can therefore hardly be covered reliably by a single network. Note also that training datasets usually only show a very limited subset of all possible images. Their work introduces a new natural image prior. \nHow\n\nUsually, if one wants to generate images that lead to high activations, the basic/naive method is to:\n\nStart with a noise image,\nFeed that image through DNN,\nCompute an error that is high if the activation of the specified neuron is low (analogous for high activation),\nBackpropagate the error through DNN,\nChange the noise image according to the gradient,\nRepeat.\n\n\nSo, the noise image is basically treated like weights in the network.\nTheir alternative method is based on a Generator network G.\n\nThat G is trained according to the method described in Generating Images with Perceptual Similarity Metrics based on Deep Networks.\nVery rough outline of that method:\n\nFirst, a pretrained network E is given (they picked CaffeNet, which is a variation of AlexNet).\nG then has to learn to inverse E, i.e. G receives per image the features extracted by a specific layer in E (e.g. the last fully connected layer before the output) and has to generate (recreate) the image from these features.\n\n\n\n\nTheir modified steps are:\n\n(New step) Start with a noise vector,\n(New step) Feed that vector through G resulting in an image,\n(Same) Feed that image through DNN,\n(Same) Compute an error that is low if the activation of the specified neuron is high (analogous for low activations),\n(Same) Backpropagate the error through DNN,\n(Modified) Change the noise vector according to the gradient,\n(Same) Repeat.\n\n\nVisualization of their architecture:\n\n\n\n\nAdditionally they do:\n\nApply an L2 norm to the noise vector, which adds pressure to each component to take low values. They say that this improved the results.\nClip each component of the noise vector to a range [0, a], which improved the results significantly.\n\nThe range starts at 0, because the network (E) inverted by their Generator (G) is based on ReLUs.\na is derived from test images fed through E and set to 3 standard diviations of the mean activation of that component (recall that the \"noise\" vector mirrors a specific layer in E).\nThey argue that this clipping is similar to a prior on the noise vector components. That prior reflects likely values of the layer in E that is used for the noise vector.\n\n\n\n\n\n Usually, if one wants to generate images that lead to high activations, the basic/naive method is to:\n\nStart with a noise image,\nFeed that image through DNN,\nCompute an error that is high if the activation of the specified neuron is low (analogous for high activation),\nBackpropagate the error through DNN,\nChange the noise image according to the gradient,\nRepeat.\n\n Start with a noise image, Feed that image through DNN, Compute an error that is high if the activation of the specified neuron is low (analogous for high activation), Backpropagate the error through DNN, Change the noise image according to the gradient, Repeat. So, the noise image is basically treated like weights in the network. Their alternative method is based on a Generator network G.\n\nThat G is trained according to the method described in Generating Images with Perceptual Similarity Metrics based on Deep Networks.\nVery rough outline of that method:\n\nFirst, a pretrained network E is given (they picked CaffeNet, which is a variation of AlexNet).\nG then has to learn to inverse E, i.e. G receives per image the features extracted by a specific layer in E (e.g. the last fully connected layer before the output) and has to generate (recreate) the image from these features.\n\n\n\n That G is trained according to the method described in Generating Images with Perceptual Similarity Metrics based on Deep Networks. Very rough outline of that method:\n\nFirst, a pretrained network E is given (they picked CaffeNet, which is a variation of AlexNet).\nG then has to learn to inverse E, i.e. G receives per image the features extracted by a specific layer in E (e.g. the last fully connected layer before the output) and has to generate (recreate) the image from these features.\n\n First, a pretrained network E is given (they picked CaffeNet, which is a variation of AlexNet). G then has to learn to inverse E, i.e. G receives per image the features extracted by a specific layer in E (e.g. the last fully connected layer before the output) and has to generate (recreate) the image from these features. Their modified steps are:\n\n(New step) Start with a noise vector,\n(New step) Feed that vector through G resulting in an image,\n(Same) Feed that image through DNN,\n(Same) Compute an error that is low if the activation of the specified neuron is high (analogous for low activations),\n(Same) Backpropagate the error through DNN,\n(Modified) Change the noise vector according to the gradient,\n(Same) Repeat.\n\n (New step) Start with a noise vector, (New step) Feed that vector through G resulting in an image, (Same) Feed that image through DNN, (Same) Compute an error that is low if the activation of the specified neuron is high (analogous for low activations), (Same) Backpropagate the error through DNN, (Modified) Change the noise vector according to the gradient, (Same) Repeat. Visualization of their architecture:\n\n\n\n  Additionally they do:\n\nApply an L2 norm to the noise vector, which adds pressure to each component to take low values. They say that this improved the results.\nClip each component of the noise vector to a range [0, a], which improved the results significantly.\n\nThe range starts at 0, because the network (E) inverted by their Generator (G) is based on ReLUs.\na is derived from test images fed through E and set to 3 standard diviations of the mean activation of that component (recall that the \"noise\" vector mirrors a specific layer in E).\nThey argue that this clipping is similar to a prior on the noise vector components. That prior reflects likely values of the layer in E that is used for the noise vector.\n\n\n\n Apply an L2 norm to the noise vector, which adds pressure to each component to take low values. They say that this improved the results. Clip each component of the noise vector to a range [0, a], which improved the results significantly.\n\nThe range starts at 0, because the network (E) inverted by their Generator (G) is based on ReLUs.\na is derived from test images fed through E and set to 3 standard diviations of the mean activation of that component (recall that the \"noise\" vector mirrors a specific layer in E).\nThey argue that this clipping is similar to a prior on the noise vector components. That prior reflects likely values of the layer in E that is used for the noise vector.\n\n The range starts at 0, because the network (E) inverted by their Generator (G) is based on ReLUs. a is derived from test images fed through E and set to 3 standard diviations of the mean activation of that component (recall that the \"noise\" vector mirrors a specific layer in E). They argue that this clipping is similar to a prior on the noise vector components. That prior reflects likely values of the layer in E that is used for the noise vector. \nResults\n\nExamples of generated images:\n\n\n\n\nEarly vs. late layers\n\nFor G they have to pick a specific layer from E that G has to invert. They found that using \"later\" layers (e.g. the fully connected layers at the end) produced images with more reasonable overall structure than using \"early\" layers (e.g. first convolutional layers). Early layers led to repeating structures.\n\n\nDatasets and architectures\n\nBoth G and DNN have to be trained on datasets.\nThey found that these networks can actually be trained on different datasets, the results will still look good.\nHowever, they found that the architectures of DNN and E should be similar to create the best looking images (though this might also be down to depth of the tested networks).\n\n\nVerification that the prior can generate any image\n\nThey tested whether the generated images really show what the DNN-neurons prefer and not what the Generator/prior prefers.\nTo do that, they retrained DNNs on images that were both directly from the dataset as well as images that were somehow modified.\nThose modifications were:\n\nTreated RGB images as if they were BGR (creating images with weird colors).\nCopy-pasted areas in the images around (creating mosaics).\nBlurred the images (with gaussian blur).\n\n\nThe DNNs were then trained to classify the \"normal\" images into 1000 classes and the modified images into 1000 other classes (2000 total).\nSo at the end there were (in the same DNN) neurons reacting strongly to specific classes of unmodified images and other neurons that reacted strongly to specific classes of modified images.\nWhen generating images to maximize activations of specific neurons, the Generator was able to create both modified and unmodified images. Though it seemed to have some trouble with blurring.\nThat shows that the generated images probably indeed show what the DNN has learned and not just what G has learned.\n\n\nUncanonical images\n\nThe method can sometimes generate uncanonical images (e.g. instead of a full dog just blobs of texture).\nThey found that this seems to be mostly the case when the dataset images have uncanonical pose, i.e. are very diverse/multi-modal.\n\n\n\n Examples of generated images:\n\n\n\n  Early vs. late layers\n\nFor G they have to pick a specific layer from E that G has to invert. They found that using \"later\" layers (e.g. the fully connected layers at the end) produced images with more reasonable overall structure than using \"early\" layers (e.g. first convolutional layers). Early layers led to repeating structures.\n\n For G they have to pick a specific layer from E that G has to invert. They found that using \"later\" layers (e.g. the fully connected layers at the end) produced images with more reasonable overall structure than using \"early\" layers (e.g. first convolutional layers). Early layers led to repeating structures. Datasets and architectures\n\nBoth G and DNN have to be trained on datasets.\nThey found that these networks can actually be trained on different datasets, the results will still look good.\nHowever, they found that the architectures of DNN and E should be similar to create the best looking images (though this might also be down to depth of the tested networks).\n\n Both G and DNN have to be trained on datasets. They found that these networks can actually be trained on different datasets, the results will still look good. However, they found that the architectures of DNN and E should be similar to create the best looking images (though this might also be down to depth of the tested networks). Verification that the prior can generate any image\n\nThey tested whether the generated images really show what the DNN-neurons prefer and not what the Generator/prior prefers.\nTo do that, they retrained DNNs on images that were both directly from the dataset as well as images that were somehow modified.\nThose modifications were:\n\nTreated RGB images as if they were BGR (creating images with weird colors).\nCopy-pasted areas in the images around (creating mosaics).\nBlurred the images (with gaussian blur).\n\n\nThe DNNs were then trained to classify the \"normal\" images into 1000 classes and the modified images into 1000 other classes (2000 total).\nSo at the end there were (in the same DNN) neurons reacting strongly to specific classes of unmodified images and other neurons that reacted strongly to specific classes of modified images.\nWhen generating images to maximize activations of specific neurons, the Generator was able to create both modified and unmodified images. Though it seemed to have some trouble with blurring.\nThat shows that the generated images probably indeed show what the DNN has learned and not just what G has learned.\n\n They tested whether the generated images really show what the DNN-neurons prefer and not what the Generator/prior prefers. To do that, they retrained DNNs on images that were both directly from the dataset as well as images that were somehow modified. Those modifications were:\n\nTreated RGB images as if they were BGR (creating images with weird colors).\nCopy-pasted areas in the images around (creating mosaics).\nBlurred the images (with gaussian blur).\n\n Treated RGB images as if they were BGR (creating images with weird colors). Copy-pasted areas in the images around (creating mosaics). Blurred the images (with gaussian blur). The DNNs were then trained to classify the \"normal\" images into 1000 classes and the modified images into 1000 other classes (2000 total). So at the end there were (in the same DNN) neurons reacting strongly to specific classes of unmodified images and other neurons that reacted strongly to specific classes of modified images. When generating images to maximize activations of specific neurons, the Generator was able to create both modified and unmodified images. Though it seemed to have some trouble with blurring. That shows that the generated images probably indeed show what the DNN has learned and not just what G has learned. Uncanonical images\n\nThe method can sometimes generate uncanonical images (e.g. instead of a full dog just blobs of texture).\nThey found that this seems to be mostly the case when the dataset images have uncanonical pose, i.e. are very diverse/multi-modal.\n\n The method can sometimes generate uncanonical images (e.g. instead of a full dog just blobs of texture). They found that this seems to be mostly the case when the dataset images have uncanonical pose, i.e. are very diverse/multi-modal. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1605.09304v3"
    },
    "58": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/FractalNet_Ultra-Deep_Networks_without_Residuals.md",
        "transcript": "\nWhat\n\nThey describe an architecture for deep CNNs that contains short and long paths. (Short = few convolutions between input and output, long = many convolutions between input and output)\nThey achieve comparable accuracy to residual networks, without using residuals.\n\n They describe an architecture for deep CNNs that contains short and long paths. (Short = few convolutions between input and output, long = many convolutions between input and output) They achieve comparable accuracy to residual networks, without using residuals. \nHow\n\nBasic principle:\n\nThey start with two branches. The left branch contains one convolutional layer, the right branch contains a subnetwork.\nThat subnetwork again contains a left branch (one convolutional layer) and a right branch (a subnetwork).\nThis creates a recursion.\nAt the last step of the recursion they simply insert two convolutional layers as the subnetwork.\nEach pair of branches (left and right) is merged using a pair-wise mean. (Result: One of the branches can be skipped or removed and the result after the merge will still be sound.)\nTheir recursive expansion rule (left) and architecture (middle and right) visualized:\n\n\n\nBlocks:\n\nEach of the recursively generated networks is one block.\nThey chain five blocks in total to create the network that they use for their experiments.\nAfter each block they add a max pooling layer.\nTheir first block uses 64 filters per convolutional layer, the second one 128, followed by 256, 512 and again 512.\n\n\nDrop-path:\n\nThey randomly dropout whole convolutional layers between merge-layers.\nThey define two methods for that:\n\nLocal drop-path: Drops each input to each merge layer with a fixed probability, but at least one always survives. (See image, first three examples.)\nGlobal drop-path: Drops convolutional layers so that only a single columns (and thereby path) in the whole network survives. (See image, right.)\n\n\nVisualization:\n\n\n\n\n Basic principle:\n\nThey start with two branches. The left branch contains one convolutional layer, the right branch contains a subnetwork.\nThat subnetwork again contains a left branch (one convolutional layer) and a right branch (a subnetwork).\nThis creates a recursion.\nAt the last step of the recursion they simply insert two convolutional layers as the subnetwork.\nEach pair of branches (left and right) is merged using a pair-wise mean. (Result: One of the branches can be skipped or removed and the result after the merge will still be sound.)\nTheir recursive expansion rule (left) and architecture (middle and right) visualized:\n\n\n They start with two branches. The left branch contains one convolutional layer, the right branch contains a subnetwork. That subnetwork again contains a left branch (one convolutional layer) and a right branch (a subnetwork). This creates a recursion. At the last step of the recursion they simply insert two convolutional layers as the subnetwork. Each pair of branches (left and right) is merged using a pair-wise mean. (Result: One of the branches can be skipped or removed and the result after the merge will still be sound.) Their recursive expansion rule (left) and architecture (middle and right) visualized:\n Blocks:\n\nEach of the recursively generated networks is one block.\nThey chain five blocks in total to create the network that they use for their experiments.\nAfter each block they add a max pooling layer.\nTheir first block uses 64 filters per convolutional layer, the second one 128, followed by 256, 512 and again 512.\n\n Each of the recursively generated networks is one block. They chain five blocks in total to create the network that they use for their experiments. After each block they add a max pooling layer. Their first block uses 64 filters per convolutional layer, the second one 128, followed by 256, 512 and again 512. Drop-path:\n\nThey randomly dropout whole convolutional layers between merge-layers.\nThey define two methods for that:\n\nLocal drop-path: Drops each input to each merge layer with a fixed probability, but at least one always survives. (See image, first three examples.)\nGlobal drop-path: Drops convolutional layers so that only a single columns (and thereby path) in the whole network survives. (See image, right.)\n\n\nVisualization:\n\n\n They randomly dropout whole convolutional layers between merge-layers. They define two methods for that:\n\nLocal drop-path: Drops each input to each merge layer with a fixed probability, but at least one always survives. (See image, first three examples.)\nGlobal drop-path: Drops convolutional layers so that only a single columns (and thereby path) in the whole network survives. (See image, right.)\n\n Local drop-path: Drops each input to each merge layer with a fixed probability, but at least one always survives. (See image, first three examples.) Global drop-path: Drops convolutional layers so that only a single columns (and thereby path) in the whole network survives. (See image, right.) Visualization:\n \nResults\n\nThey test on CIFAR-10, CIFAR-100 and SVHN with no or mild (crops, flips) augmentation.\nThey add dropout at the start of each block (probabilities: 0%, 10%, 20%, 30%, 40%).\nThey use for 50% of the batches local drop-path at 15% and for the other 50% global drop-path.\nThey achieve comparable accuracy to ResNets (a bit behind them actually).\n\nNote: The best ResNet that they compare to is \"ResNet with Identity Mappings\". They don't compare to Wide ResNets, even though they perform best.\n\n\nIf they use image augmentations, dropout and drop-path don't seem to provide much benefit (only small improvement).\nIf they extract the deepest column and test on that one alone, they achieve nearly the same performance as with the whole network.\n\nThey derive from that, that their fractal architecture is actually only really used to help that deepest column to learn anything. (Without shorter paths it would just learn nothing due to vanishing gradients.)\n\n\n\n They test on CIFAR-10, CIFAR-100 and SVHN with no or mild (crops, flips) augmentation. They add dropout at the start of each block (probabilities: 0%, 10%, 20%, 30%, 40%). They use for 50% of the batches local drop-path at 15% and for the other 50% global drop-path. They achieve comparable accuracy to ResNets (a bit behind them actually).\n\nNote: The best ResNet that they compare to is \"ResNet with Identity Mappings\". They don't compare to Wide ResNets, even though they perform best.\n\n Note: The best ResNet that they compare to is \"ResNet with Identity Mappings\". They don't compare to Wide ResNets, even though they perform best. If they use image augmentations, dropout and drop-path don't seem to provide much benefit (only small improvement). If they extract the deepest column and test on that one alone, they achieve nearly the same performance as with the whole network.\n\nThey derive from that, that their fractal architecture is actually only really used to help that deepest column to learn anything. (Without shorter paths it would just learn nothing due to vanishing gradients.)\n\n They derive from that, that their fractal architecture is actually only really used to help that deepest column to learn anything. (Without shorter paths it would just learn nothing due to vanishing gradients.) \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1605.07648v1"
    },
    "59": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/PlaNet.md",
        "transcript": "\nWhat\n\nThey describe a convolutional network that takes in photos and returns where (on the planet) these photos were likely made.\nThe output is a distribution over locations around the world (so not just one single location). This can be useful in the case of ambiguous images.\n\n They describe a convolutional network that takes in photos and returns where (on the planet) these photos were likely made. The output is a distribution over locations around the world (so not just one single location). This can be useful in the case of ambiguous images. \nHow\n\nBasic architecture\n\nThey simply use the Inception architecture for their model.\nThey have 97M parameters.\n\n\nGrid\n\nThe network uses a grid of cells over the planet.\nFor each photo and every grid cell it returns the likelihood that the photo was made within the region covered by the cell (simple softmax layer).\nThe naive way would be to use a regular grid around the planet (i.e. a grid in which all cells have the same size).\n\nPossible disadvantages:\n\nIn places where lots of photos are taken you still have the same grid cell size as in places where barely any photos are taken.\nMaps are often distorted towards the poles (countries are represented much larger than they really are). This will likely affect the grid cells too.\n\n\n\n\nThey instead use an adaptive grid pattern based on S2 cells.\n\nS2 cells interpret the planet as a sphere and project a cube onto it.\nThe 6 sides of the cube are then partitioned using quad trees, creating the grid cells.\nThey don't use the same depth for all quad trees. Instead they subdivide them only if their leafs contain enough photos (based on their dataset of geolocated images).\nThey remove some cells for which their dataset does not contain enough images, e.g. cells on oceans. (They also remove these images from the dataset. They don't say how many images are affected by this.)\nThey end up with roughly 26k cells, some of them reaching the street level of major cities.\nVisualization of their cells:\n\n\n\n\n\nTraining\n\nFor each example photo that they feed into the network, they set the correct grid cell to 1.0 and all other grid cells to 0.0.\nThey train on a dataset of 126M images with Exif geolocation information. The images were collected from all over the web.\nThey used Adagrad.\nThey trained on 200 CPUs for 2.5 months.\n\n\nAlbum network\n\nFor photo albums they develop variations of their network.\nThey do that because albums often contain images that are very hard to geolocate on their own, but much easier if the other images of the album are seen.\nThey use LSTMs for their album network.\nThe simplest one just iterates over every photo, applies their previously described model to it and extracts the last layer (before output) from that model. These vectors (one per image) are then fed into an LSTM, which is trained to predict (again) the grid cell location per image.\nMore complicated versions use multiple passes or are bidirectional LSTMs (to use the information from the last images to classify the first ones in the album).\n\n\n\n Basic architecture\n\nThey simply use the Inception architecture for their model.\nThey have 97M parameters.\n\n They simply use the Inception architecture for their model. They have 97M parameters. Grid\n\nThe network uses a grid of cells over the planet.\nFor each photo and every grid cell it returns the likelihood that the photo was made within the region covered by the cell (simple softmax layer).\nThe naive way would be to use a regular grid around the planet (i.e. a grid in which all cells have the same size).\n\nPossible disadvantages:\n\nIn places where lots of photos are taken you still have the same grid cell size as in places where barely any photos are taken.\nMaps are often distorted towards the poles (countries are represented much larger than they really are). This will likely affect the grid cells too.\n\n\n\n\nThey instead use an adaptive grid pattern based on S2 cells.\n\nS2 cells interpret the planet as a sphere and project a cube onto it.\nThe 6 sides of the cube are then partitioned using quad trees, creating the grid cells.\nThey don't use the same depth for all quad trees. Instead they subdivide them only if their leafs contain enough photos (based on their dataset of geolocated images).\nThey remove some cells for which their dataset does not contain enough images, e.g. cells on oceans. (They also remove these images from the dataset. They don't say how many images are affected by this.)\nThey end up with roughly 26k cells, some of them reaching the street level of major cities.\nVisualization of their cells:\n\n\n\n\n The network uses a grid of cells over the planet. For each photo and every grid cell it returns the likelihood that the photo was made within the region covered by the cell (simple softmax layer). The naive way would be to use a regular grid around the planet (i.e. a grid in which all cells have the same size).\n\nPossible disadvantages:\n\nIn places where lots of photos are taken you still have the same grid cell size as in places where barely any photos are taken.\nMaps are often distorted towards the poles (countries are represented much larger than they really are). This will likely affect the grid cells too.\n\n\n\n Possible disadvantages:\n\nIn places where lots of photos are taken you still have the same grid cell size as in places where barely any photos are taken.\nMaps are often distorted towards the poles (countries are represented much larger than they really are). This will likely affect the grid cells too.\n\n In places where lots of photos are taken you still have the same grid cell size as in places where barely any photos are taken. Maps are often distorted towards the poles (countries are represented much larger than they really are). This will likely affect the grid cells too. They instead use an adaptive grid pattern based on S2 cells.\n\nS2 cells interpret the planet as a sphere and project a cube onto it.\nThe 6 sides of the cube are then partitioned using quad trees, creating the grid cells.\nThey don't use the same depth for all quad trees. Instead they subdivide them only if their leafs contain enough photos (based on their dataset of geolocated images).\nThey remove some cells for which their dataset does not contain enough images, e.g. cells on oceans. (They also remove these images from the dataset. They don't say how many images are affected by this.)\nThey end up with roughly 26k cells, some of them reaching the street level of major cities.\nVisualization of their cells:\n\n\n S2 cells interpret the planet as a sphere and project a cube onto it. The 6 sides of the cube are then partitioned using quad trees, creating the grid cells. They don't use the same depth for all quad trees. Instead they subdivide them only if their leafs contain enough photos (based on their dataset of geolocated images). They remove some cells for which their dataset does not contain enough images, e.g. cells on oceans. (They also remove these images from the dataset. They don't say how many images are affected by this.) They end up with roughly 26k cells, some of them reaching the street level of major cities. Visualization of their cells:\n Training\n\nFor each example photo that they feed into the network, they set the correct grid cell to 1.0 and all other grid cells to 0.0.\nThey train on a dataset of 126M images with Exif geolocation information. The images were collected from all over the web.\nThey used Adagrad.\nThey trained on 200 CPUs for 2.5 months.\n\n For each example photo that they feed into the network, they set the correct grid cell to 1.0 and all other grid cells to 0.0. They train on a dataset of 126M images with Exif geolocation information. The images were collected from all over the web. They used Adagrad. They trained on 200 CPUs for 2.5 months. Album network\n\nFor photo albums they develop variations of their network.\nThey do that because albums often contain images that are very hard to geolocate on their own, but much easier if the other images of the album are seen.\nThey use LSTMs for their album network.\nThe simplest one just iterates over every photo, applies their previously described model to it and extracts the last layer (before output) from that model. These vectors (one per image) are then fed into an LSTM, which is trained to predict (again) the grid cell location per image.\nMore complicated versions use multiple passes or are bidirectional LSTMs (to use the information from the last images to classify the first ones in the album).\n\n For photo albums they develop variations of their network. They do that because albums often contain images that are very hard to geolocate on their own, but much easier if the other images of the album are seen. They use LSTMs for their album network. The simplest one just iterates over every photo, applies their previously described model to it and extracts the last layer (before output) from that model. These vectors (one per image) are then fed into an LSTM, which is trained to predict (again) the grid cell location per image. More complicated versions use multiple passes or are bidirectional LSTMs (to use the information from the last images to classify the first ones in the album). \nResults\n\nThey beat previous models (based on hand-engineered features or nearest neighbour methods) by a significant margin.\nIn a small experiment they can beat experienced humans in geoguessr.com.\nBased on a dataset of 2.3M photos from Flickr, their method correctly predicts the country where the photo was made in 30% of all cases (top-1; top-5: about 50%). City-level accuracy is about 10% (top-1; top-5: about 18%).\nExample predictions (using in coarser grid with 354 cells):\n\nUsing the LSTM-technique for albums significantly improves prediction accuracy for these images.\n\n They beat previous models (based on hand-engineered features or nearest neighbour methods) by a significant margin. In a small experiment they can beat experienced humans in geoguessr.com. Based on a dataset of 2.3M photos from Flickr, their method correctly predicts the country where the photo was made in 30% of all cases (top-1; top-5: about 50%). City-level accuracy is about 10% (top-1; top-5: about 18%). Example predictions (using in coarser grid with 354 cells):\n Using the LSTM-technique for albums significantly improves prediction accuracy for these images. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1602.05314"
    },
    "60": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Adam.md",
        "transcript": "\nWhat\n\nThey suggest a new stochastic optimization method, similar to the existing SGD, Adagrad or RMSProp.\n\nStochastic optimization methods have to find parameters that minimize/maximize a stochastic function.\nA function is stochastic (non-deterministic), if the same set of parameters can generate different results. E.g. the loss of different mini-batches can differ, even when the parameters remain unchanged. Even for the same mini-batch the results can change due to e.g. dropout.\n\n\nTheir method tends to converge faster to optimal parameters than the existing competitors.\nTheir method can deal with non-stationary distributions (similar to e.g. SGD, Adadelta, RMSProp).\nTheir method can deal with very sparse or noisy gradients (similar to e.g. Adagrad).\n\n They suggest a new stochastic optimization method, similar to the existing SGD, Adagrad or RMSProp.\n\nStochastic optimization methods have to find parameters that minimize/maximize a stochastic function.\nA function is stochastic (non-deterministic), if the same set of parameters can generate different results. E.g. the loss of different mini-batches can differ, even when the parameters remain unchanged. Even for the same mini-batch the results can change due to e.g. dropout.\n\n Stochastic optimization methods have to find parameters that minimize/maximize a stochastic function. A function is stochastic (non-deterministic), if the same set of parameters can generate different results. E.g. the loss of different mini-batches can differ, even when the parameters remain unchanged. Even for the same mini-batch the results can change due to e.g. dropout. Their method tends to converge faster to optimal parameters than the existing competitors. Their method can deal with non-stationary distributions (similar to e.g. SGD, Adadelta, RMSProp). Their method can deal with very sparse or noisy gradients (similar to e.g. Adagrad). \nHow\n\nBasic principle\n\nStandard SGD just updates the parameters based on parameters = parameters - learningRate * gradient.\nAdam operates similar to that, but adds more \"cleverness\" to the rule.\nIt assumes that the gradient values have means and variances and tries to estimate these values.\n\nRecall here that the function to optimize is stochastic, so there is some randomness in the gradients.\nThe mean is also called \"the first moment\".\nThe variance is also called \"the second (raw) moment\".\n\n\nThen an update rule very similar to SGD would be parameters = parameters - learningRate * means.\nThey instead use the update rule parameters = parameters - learningRate * means/sqrt(variances).\n\nThey call means/sqrt(variances) a 'Signal to Noise Ratio'.\nBasically, if the variance of a specific parameter's gradient is high, it is pretty unclear how it should be changend. So we choose a small step size in the update rule via learningRate * mean/sqrt(highValue).\nIf the variance is low, it is easier to predict how far to \"move\", so we choose a larger step size via learningRate * mean/sqrt(lowValue).\n\n\n\n\nExponential moving averages\n\nIn order to approximate the mean and variance values you could simply save the last T gradients and then average the values.\nThat however is a pretty bad idea, because it can lead to high memory demands (e.g. for millions of parameters in CNNs).\nA simple average also has the disadvantage, that it would completely ignore all gradients before T and weight all of the last T gradients identically. In reality, you might want to give more weight to the last couple of gradients.\nInstead, they use an exponential moving average, which fixes both problems and simply updates the average at every timestep via the formula avg = alpha * avg + (1 - alpha) * avg.\nLet the gradient at timestep (batch) t be g, then we can approximate the mean and variance values using:\n\nmean = beta1 * mean + (1 - beta1) * g\nvariance = beta2 * variance + (1 - beta2) * g^2.\nbeta1 and beta2 are hyperparameters of the algorithm. Good values for them seem to be beta1=0.9 and beta2=0.999.\nAt the start of the algorithm, mean and variance are initialized to zero-vectors.\n\n\n\n\nBias correction\n\nInitializing the mean and variance vectors to zero is an easy and logical step, but has the disadvantage that bias is introduced.\nE.g. at the first timestep, the mean of the gradient would be mean = beta1 * 0 + (1 - beta1) * g, with beta1=0.9 then: mean = 0.9 * g. So 0.9g, not g. Both the mean and the variance are biased (towards 0).\nThis seems pretty harmless, but it can be shown that it lowers the convergence speed of the algorithm by quite a bit.\nSo to fix this pretty they perform bias-corrections of the mean and the variance:\n\ncorrectedMean = mean / (1-beta1^t) (where t is the timestep).\ncorrectedVariance = variance / (1-beta2^t).\nBoth formulas are applied at every timestep after the exponential moving averages (they do not influence the next timestep).\n\n\n\n\n\n Basic principle\n\nStandard SGD just updates the parameters based on parameters = parameters - learningRate * gradient.\nAdam operates similar to that, but adds more \"cleverness\" to the rule.\nIt assumes that the gradient values have means and variances and tries to estimate these values.\n\nRecall here that the function to optimize is stochastic, so there is some randomness in the gradients.\nThe mean is also called \"the first moment\".\nThe variance is also called \"the second (raw) moment\".\n\n\nThen an update rule very similar to SGD would be parameters = parameters - learningRate * means.\nThey instead use the update rule parameters = parameters - learningRate * means/sqrt(variances).\n\nThey call means/sqrt(variances) a 'Signal to Noise Ratio'.\nBasically, if the variance of a specific parameter's gradient is high, it is pretty unclear how it should be changend. So we choose a small step size in the update rule via learningRate * mean/sqrt(highValue).\nIf the variance is low, it is easier to predict how far to \"move\", so we choose a larger step size via learningRate * mean/sqrt(lowValue).\n\n\n\n Standard SGD just updates the parameters based on parameters = parameters - learningRate * gradient. Adam operates similar to that, but adds more \"cleverness\" to the rule. It assumes that the gradient values have means and variances and tries to estimate these values.\n\nRecall here that the function to optimize is stochastic, so there is some randomness in the gradients.\nThe mean is also called \"the first moment\".\nThe variance is also called \"the second (raw) moment\".\n\n Recall here that the function to optimize is stochastic, so there is some randomness in the gradients. The mean is also called \"the first moment\". The variance is also called \"the second (raw) moment\". Then an update rule very similar to SGD would be parameters = parameters - learningRate * means. They instead use the update rule parameters = parameters - learningRate * means/sqrt(variances).\n\nThey call means/sqrt(variances) a 'Signal to Noise Ratio'.\nBasically, if the variance of a specific parameter's gradient is high, it is pretty unclear how it should be changend. So we choose a small step size in the update rule via learningRate * mean/sqrt(highValue).\nIf the variance is low, it is easier to predict how far to \"move\", so we choose a larger step size via learningRate * mean/sqrt(lowValue).\n\n They call means/sqrt(variances) a 'Signal to Noise Ratio'. Basically, if the variance of a specific parameter's gradient is high, it is pretty unclear how it should be changend. So we choose a small step size in the update rule via learningRate * mean/sqrt(highValue). If the variance is low, it is easier to predict how far to \"move\", so we choose a larger step size via learningRate * mean/sqrt(lowValue). Exponential moving averages\n\nIn order to approximate the mean and variance values you could simply save the last T gradients and then average the values.\nThat however is a pretty bad idea, because it can lead to high memory demands (e.g. for millions of parameters in CNNs).\nA simple average also has the disadvantage, that it would completely ignore all gradients before T and weight all of the last T gradients identically. In reality, you might want to give more weight to the last couple of gradients.\nInstead, they use an exponential moving average, which fixes both problems and simply updates the average at every timestep via the formula avg = alpha * avg + (1 - alpha) * avg.\nLet the gradient at timestep (batch) t be g, then we can approximate the mean and variance values using:\n\nmean = beta1 * mean + (1 - beta1) * g\nvariance = beta2 * variance + (1 - beta2) * g^2.\nbeta1 and beta2 are hyperparameters of the algorithm. Good values for them seem to be beta1=0.9 and beta2=0.999.\nAt the start of the algorithm, mean and variance are initialized to zero-vectors.\n\n\n\n In order to approximate the mean and variance values you could simply save the last T gradients and then average the values. That however is a pretty bad idea, because it can lead to high memory demands (e.g. for millions of parameters in CNNs). A simple average also has the disadvantage, that it would completely ignore all gradients before T and weight all of the last T gradients identically. In reality, you might want to give more weight to the last couple of gradients. Instead, they use an exponential moving average, which fixes both problems and simply updates the average at every timestep via the formula avg = alpha * avg + (1 - alpha) * avg. Let the gradient at timestep (batch) t be g, then we can approximate the mean and variance values using:\n\nmean = beta1 * mean + (1 - beta1) * g\nvariance = beta2 * variance + (1 - beta2) * g^2.\nbeta1 and beta2 are hyperparameters of the algorithm. Good values for them seem to be beta1=0.9 and beta2=0.999.\nAt the start of the algorithm, mean and variance are initialized to zero-vectors.\n\n mean = beta1 * mean + (1 - beta1) * g variance = beta2 * variance + (1 - beta2) * g^2. beta1 and beta2 are hyperparameters of the algorithm. Good values for them seem to be beta1=0.9 and beta2=0.999. At the start of the algorithm, mean and variance are initialized to zero-vectors. Bias correction\n\nInitializing the mean and variance vectors to zero is an easy and logical step, but has the disadvantage that bias is introduced.\nE.g. at the first timestep, the mean of the gradient would be mean = beta1 * 0 + (1 - beta1) * g, with beta1=0.9 then: mean = 0.9 * g. So 0.9g, not g. Both the mean and the variance are biased (towards 0).\nThis seems pretty harmless, but it can be shown that it lowers the convergence speed of the algorithm by quite a bit.\nSo to fix this pretty they perform bias-corrections of the mean and the variance:\n\ncorrectedMean = mean / (1-beta1^t) (where t is the timestep).\ncorrectedVariance = variance / (1-beta2^t).\nBoth formulas are applied at every timestep after the exponential moving averages (they do not influence the next timestep).\n\n\n\n Initializing the mean and variance vectors to zero is an easy and logical step, but has the disadvantage that bias is introduced. E.g. at the first timestep, the mean of the gradient would be mean = beta1 * 0 + (1 - beta1) * g, with beta1=0.9 then: mean = 0.9 * g. So 0.9g, not g. Both the mean and the variance are biased (towards 0). This seems pretty harmless, but it can be shown that it lowers the convergence speed of the algorithm by quite a bit. So to fix this pretty they perform bias-corrections of the mean and the variance:\n\ncorrectedMean = mean / (1-beta1^t) (where t is the timestep).\ncorrectedVariance = variance / (1-beta2^t).\nBoth formulas are applied at every timestep after the exponential moving averages (they do not influence the next timestep).\n\n correctedMean = mean / (1-beta1^t) (where t is the timestep). correctedVariance = variance / (1-beta2^t). Both formulas are applied at every timestep after the exponential moving averages (they do not influence the next timestep). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1412.6980"
    },
    "61": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Generating_Images_with_Recurrent_Adversarial_Networks.md",
        "transcript": "\nWhat\n\nThey describe a new architecture for GANs.\nThe architecture is based on letting the Generator (G) create images in multiple steps, similar to DRAW.\nThey also briefly suggest a method to compare the quality of the results of different generators with each other.\n\n They describe a new architecture for GANs. The architecture is based on letting the Generator (G) create images in multiple steps, similar to DRAW. They also briefly suggest a method to compare the quality of the results of different generators with each other. \nHow\n\nIn a classic GAN one samples a noise vector z, feeds that into a Generator (G), which then generates an image x, which is then fed through the Discriminator (D) to estimate its quality.\nTheir method operates in basically the same way, but internally G is changed to generate images in multiple time steps.\nOutline of how their G operates:\n\nTime step 0:\n\nInput: Empty image delta C-1, randomly sampled z.\nFeed delta C-1 through a number of downsampling convolutions to create a tensor. (Not very useful here, as the image is empty. More useful in later timesteps.)\nFeed z through a number of upsampling convolutions to create a tensor (similar to DCGAN).\nConcat the output of the previous two steps.\nFeed that concatenation through a few more convolutions.\nOutput: delta C0 (changes to apply to the empty starting canvas).\n\n\nTime step 1 (and later):\n\nInput: Previous change delta C0, randomly sampled z (can be the same as in step 0).\nFeed delta C0 through a number of downsampling convolutions to create a tensor.\nFeed z through a number of upsampling convolutions to create a tensor (similar to DCGAN).\nConcat the output of the previous two steps.\nFeed that concatenation through a few more convolutions.\nOutput: delta C1 (changes to apply to the empty starting canvas).\n\n\nAt the end, after all timesteps have been performed:\n\nCreate final output image by summing all the changes, i.e. delta C0 + delta C1 + ..., which basically means empty start canvas + changes from time step 0 + changes from time step 1 + ....\n\n\n\n\nTheir architecture as an image:\n\n\n\n\nComparison measure\n\nThey suggest a new method to compare GAN results with each other.\nThey suggest to train pairs of G and D, e.g. for two pairs (G1, D1), (G2, D2). Then they let the pairs compete with each other.\nTo estimate the quality of D they suggest r_test = errorRate(D1, testset) / errorRate(D2, testset). (\"Which D is better at spotting that the test set images are real images?\")\nTo estimate the quality of the generated samples they suggest r_sample = errorRate(D1, images by G2) / errorRate(D2, images by G1). (\"Which G is better at fooling an unknown D, i.e. possibly better at generating life-like images?\")\nThey suggest to estimate which G is better using r_sample and then to estimate how valid that result is using r_test.\n\n\n\n In a classic GAN one samples a noise vector z, feeds that into a Generator (G), which then generates an image x, which is then fed through the Discriminator (D) to estimate its quality. Their method operates in basically the same way, but internally G is changed to generate images in multiple time steps. Outline of how their G operates:\n\nTime step 0:\n\nInput: Empty image delta C-1, randomly sampled z.\nFeed delta C-1 through a number of downsampling convolutions to create a tensor. (Not very useful here, as the image is empty. More useful in later timesteps.)\nFeed z through a number of upsampling convolutions to create a tensor (similar to DCGAN).\nConcat the output of the previous two steps.\nFeed that concatenation through a few more convolutions.\nOutput: delta C0 (changes to apply to the empty starting canvas).\n\n\nTime step 1 (and later):\n\nInput: Previous change delta C0, randomly sampled z (can be the same as in step 0).\nFeed delta C0 through a number of downsampling convolutions to create a tensor.\nFeed z through a number of upsampling convolutions to create a tensor (similar to DCGAN).\nConcat the output of the previous two steps.\nFeed that concatenation through a few more convolutions.\nOutput: delta C1 (changes to apply to the empty starting canvas).\n\n\nAt the end, after all timesteps have been performed:\n\nCreate final output image by summing all the changes, i.e. delta C0 + delta C1 + ..., which basically means empty start canvas + changes from time step 0 + changes from time step 1 + ....\n\n\n\n Time step 0:\n\nInput: Empty image delta C-1, randomly sampled z.\nFeed delta C-1 through a number of downsampling convolutions to create a tensor. (Not very useful here, as the image is empty. More useful in later timesteps.)\nFeed z through a number of upsampling convolutions to create a tensor (similar to DCGAN).\nConcat the output of the previous two steps.\nFeed that concatenation through a few more convolutions.\nOutput: delta C0 (changes to apply to the empty starting canvas).\n\n Input: Empty image delta C-1, randomly sampled z. Feed delta C-1 through a number of downsampling convolutions to create a tensor. (Not very useful here, as the image is empty. More useful in later timesteps.) Feed z through a number of upsampling convolutions to create a tensor (similar to DCGAN). Concat the output of the previous two steps. Feed that concatenation through a few more convolutions. Output: delta C0 (changes to apply to the empty starting canvas). Time step 1 (and later):\n\nInput: Previous change delta C0, randomly sampled z (can be the same as in step 0).\nFeed delta C0 through a number of downsampling convolutions to create a tensor.\nFeed z through a number of upsampling convolutions to create a tensor (similar to DCGAN).\nConcat the output of the previous two steps.\nFeed that concatenation through a few more convolutions.\nOutput: delta C1 (changes to apply to the empty starting canvas).\n\n Input: Previous change delta C0, randomly sampled z (can be the same as in step 0). Feed delta C0 through a number of downsampling convolutions to create a tensor. Feed z through a number of upsampling convolutions to create a tensor (similar to DCGAN). Concat the output of the previous two steps. Feed that concatenation through a few more convolutions. Output: delta C1 (changes to apply to the empty starting canvas). At the end, after all timesteps have been performed:\n\nCreate final output image by summing all the changes, i.e. delta C0 + delta C1 + ..., which basically means empty start canvas + changes from time step 0 + changes from time step 1 + ....\n\n Create final output image by summing all the changes, i.e. delta C0 + delta C1 + ..., which basically means empty start canvas + changes from time step 0 + changes from time step 1 + .... Their architecture as an image:\n\n\n\n  Comparison measure\n\nThey suggest a new method to compare GAN results with each other.\nThey suggest to train pairs of G and D, e.g. for two pairs (G1, D1), (G2, D2). Then they let the pairs compete with each other.\nTo estimate the quality of D they suggest r_test = errorRate(D1, testset) / errorRate(D2, testset). (\"Which D is better at spotting that the test set images are real images?\")\nTo estimate the quality of the generated samples they suggest r_sample = errorRate(D1, images by G2) / errorRate(D2, images by G1). (\"Which G is better at fooling an unknown D, i.e. possibly better at generating life-like images?\")\nThey suggest to estimate which G is better using r_sample and then to estimate how valid that result is using r_test.\n\n They suggest a new method to compare GAN results with each other. They suggest to train pairs of G and D, e.g. for two pairs (G1, D1), (G2, D2). Then they let the pairs compete with each other. To estimate the quality of D they suggest r_test = errorRate(D1, testset) / errorRate(D2, testset). (\"Which D is better at spotting that the test set images are real images?\") To estimate the quality of the generated samples they suggest r_sample = errorRate(D1, images by G2) / errorRate(D2, images by G1). (\"Which G is better at fooling an unknown D, i.e. possibly better at generating life-like images?\") They suggest to estimate which G is better using r_sample and then to estimate how valid that result is using r_test. \nResults\n\nGenerated images of churches, with timesteps 1 to 5:\n\n\n\n\nOverfitting\n\nThey saw no indication of overfitting in the sense of memorizing images from the training dataset.\nThey however saw some indication of G just interpolating between some good images and of G reusing small image patches in different images.\n\n\nRandomness of noise vector z:\n\nSampling the noise vector once seems to be better than resampling it at every timestep.\nResampling it at every time step often led to very similar looking output images.\n\n\n\n Generated images of churches, with timesteps 1 to 5:\n\n\n\n  Overfitting\n\nThey saw no indication of overfitting in the sense of memorizing images from the training dataset.\nThey however saw some indication of G just interpolating between some good images and of G reusing small image patches in different images.\n\n They saw no indication of overfitting in the sense of memorizing images from the training dataset. They however saw some indication of G just interpolating between some good images and of G reusing small image patches in different images. Randomness of noise vector z:\n\nSampling the noise vector once seems to be better than resampling it at every timestep.\nResampling it at every time step often led to very similar looking output images.\n\n Sampling the noise vector once seems to be better than resampling it at every timestep. Resampling it at every time step often led to very similar looking output images. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1602.05110v4"
    },
    "62": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Adversarially_Learned_Inference.md",
        "transcript": "\nWhat\n\nThey suggest a new architecture for GANs.\nTheir architecture adds another Generator for a reverse branch (from images to noise vector z).\nTheir architecture takes some ideas from VAEs/variational neural nets.\nOverall they can improve on the previous state of the art (DCGAN).\n\n They suggest a new architecture for GANs. Their architecture adds another Generator for a reverse branch (from images to noise vector z). Their architecture takes some ideas from VAEs/variational neural nets. Overall they can improve on the previous state of the art (DCGAN). \nHow\n\nArchitecture\n\nUsually, in GANs one feeds a noise vector z into a Generator (G), which then generates an image (x) from that noise.\nThey add a reverse branch (G2), in which another Generator takes a real image (x) and generates a noise vector z from that.\n\nThe noise vector can now be viewed as a latent space vector.\n\n\nInstead of letting G2 generate discrete values for z (as it is usually done), they instead take the approach commonly used VAEs and use continuous variables instead.\n\nThat is, if z represents N latent variables, they let G2 generate N means and N variances of gaussian distributions, with each distribution representing one value of z.\nSo the model could e.g. represent something along the lines of \"this face looks a lot like a female, but with very low probability could also be male\".\n\n\n\n\nTraining\n\nThe Discriminator (D) is now trained on pairs of either (real image, generated latent space vector) or (generated image, randomly sampled latent space vector) and has to tell them apart from each other.\nBoth Generators are trained to maximally confuse D.\n\nG1 (from z to x) confuses D maximally, if it generates new images that (a) look real and (b) fit well to the latent variables in z (e.g. if z says \"image contains a cat\", then the image should contain a cat).\nG2 (from x to z) confuses D maximally, if it generates good latent variables z that fit to the image x.\n\n\nContinuous variables\n\nThe variables in z follow gaussian distributions, which makes the training more complicated, as you can't trivially backpropagate through gaussians.\nWhen training G1 (from z to x) the situation is easy: You draw a random z-vector following a gaussian distribution (N(0, I)). (This is basically the same as in \"normal\" GANs. They just often use uniform distributions instead.)\nWhen training G2 (from x to z) the situation is a bit harder.\n\nHere we need to use the reparameterization trick here.\nThat roughly means, that G2 predicts the means and variances of the gaussian variables in z and then we draw a sample of z according to exactly these means and variances.\nThat sample gives us discrete values for our backpropagation.\nIf we do that sampling often enough, we get a good approximation of the true gradient (of the continuous variables). (Monte Carlo approximation.)\n\n\n\n\n\n\n\n Architecture\n\nUsually, in GANs one feeds a noise vector z into a Generator (G), which then generates an image (x) from that noise.\nThey add a reverse branch (G2), in which another Generator takes a real image (x) and generates a noise vector z from that.\n\nThe noise vector can now be viewed as a latent space vector.\n\n\nInstead of letting G2 generate discrete values for z (as it is usually done), they instead take the approach commonly used VAEs and use continuous variables instead.\n\nThat is, if z represents N latent variables, they let G2 generate N means and N variances of gaussian distributions, with each distribution representing one value of z.\nSo the model could e.g. represent something along the lines of \"this face looks a lot like a female, but with very low probability could also be male\".\n\n\n\n Usually, in GANs one feeds a noise vector z into a Generator (G), which then generates an image (x) from that noise. They add a reverse branch (G2), in which another Generator takes a real image (x) and generates a noise vector z from that.\n\nThe noise vector can now be viewed as a latent space vector.\n\n The noise vector can now be viewed as a latent space vector. Instead of letting G2 generate discrete values for z (as it is usually done), they instead take the approach commonly used VAEs and use continuous variables instead.\n\nThat is, if z represents N latent variables, they let G2 generate N means and N variances of gaussian distributions, with each distribution representing one value of z.\nSo the model could e.g. represent something along the lines of \"this face looks a lot like a female, but with very low probability could also be male\".\n\n That is, if z represents N latent variables, they let G2 generate N means and N variances of gaussian distributions, with each distribution representing one value of z. So the model could e.g. represent something along the lines of \"this face looks a lot like a female, but with very low probability could also be male\". Training\n\nThe Discriminator (D) is now trained on pairs of either (real image, generated latent space vector) or (generated image, randomly sampled latent space vector) and has to tell them apart from each other.\nBoth Generators are trained to maximally confuse D.\n\nG1 (from z to x) confuses D maximally, if it generates new images that (a) look real and (b) fit well to the latent variables in z (e.g. if z says \"image contains a cat\", then the image should contain a cat).\nG2 (from x to z) confuses D maximally, if it generates good latent variables z that fit to the image x.\n\n\nContinuous variables\n\nThe variables in z follow gaussian distributions, which makes the training more complicated, as you can't trivially backpropagate through gaussians.\nWhen training G1 (from z to x) the situation is easy: You draw a random z-vector following a gaussian distribution (N(0, I)). (This is basically the same as in \"normal\" GANs. They just often use uniform distributions instead.)\nWhen training G2 (from x to z) the situation is a bit harder.\n\nHere we need to use the reparameterization trick here.\nThat roughly means, that G2 predicts the means and variances of the gaussian variables in z and then we draw a sample of z according to exactly these means and variances.\nThat sample gives us discrete values for our backpropagation.\nIf we do that sampling often enough, we get a good approximation of the true gradient (of the continuous variables). (Monte Carlo approximation.)\n\n\n\n\n\n The Discriminator (D) is now trained on pairs of either (real image, generated latent space vector) or (generated image, randomly sampled latent space vector) and has to tell them apart from each other. Both Generators are trained to maximally confuse D.\n\nG1 (from z to x) confuses D maximally, if it generates new images that (a) look real and (b) fit well to the latent variables in z (e.g. if z says \"image contains a cat\", then the image should contain a cat).\nG2 (from x to z) confuses D maximally, if it generates good latent variables z that fit to the image x.\n\n G1 (from z to x) confuses D maximally, if it generates new images that (a) look real and (b) fit well to the latent variables in z (e.g. if z says \"image contains a cat\", then the image should contain a cat). G2 (from x to z) confuses D maximally, if it generates good latent variables z that fit to the image x. Continuous variables\n\nThe variables in z follow gaussian distributions, which makes the training more complicated, as you can't trivially backpropagate through gaussians.\nWhen training G1 (from z to x) the situation is easy: You draw a random z-vector following a gaussian distribution (N(0, I)). (This is basically the same as in \"normal\" GANs. They just often use uniform distributions instead.)\nWhen training G2 (from x to z) the situation is a bit harder.\n\nHere we need to use the reparameterization trick here.\nThat roughly means, that G2 predicts the means and variances of the gaussian variables in z and then we draw a sample of z according to exactly these means and variances.\nThat sample gives us discrete values for our backpropagation.\nIf we do that sampling often enough, we get a good approximation of the true gradient (of the continuous variables). (Monte Carlo approximation.)\n\n\n\n The variables in z follow gaussian distributions, which makes the training more complicated, as you can't trivially backpropagate through gaussians. When training G1 (from z to x) the situation is easy: You draw a random z-vector following a gaussian distribution (N(0, I)). (This is basically the same as in \"normal\" GANs. They just often use uniform distributions instead.) When training G2 (from x to z) the situation is a bit harder.\n\nHere we need to use the reparameterization trick here.\nThat roughly means, that G2 predicts the means and variances of the gaussian variables in z and then we draw a sample of z according to exactly these means and variances.\nThat sample gives us discrete values for our backpropagation.\nIf we do that sampling often enough, we get a good approximation of the true gradient (of the continuous variables). (Monte Carlo approximation.)\n\n Here we need to use the reparameterization trick here. That roughly means, that G2 predicts the means and variances of the gaussian variables in z and then we draw a sample of z according to exactly these means and variances. That sample gives us discrete values for our backpropagation. If we do that sampling often enough, we get a good approximation of the true gradient (of the continuous variables). (Monte Carlo approximation.) \nResults\n\nImages generated based on Celeb-A dataset:\n\n\n\n\nLeft column per pair: Real image, right column per pair: reconstruction (x -> z via G2, then z -> x via G1)\n\n\n\n\nReconstructions of SVHN, notice how the digits often stay the same, while the font changes:\n\n\n\n\nCIFAR-10 samples, still lots of errors, but some quite correct:\n\n\n\n\n\n Images generated based on Celeb-A dataset:\n\n\n\n  Left column per pair: Real image, right column per pair: reconstruction (x -> z via G2, then z -> x via G1)\n\n\n\n  Reconstructions of SVHN, notice how the digits often stay the same, while the font changes:\n\n\n\n  CIFAR-10 samples, still lots of errors, but some quite correct:\n\n\n\n  \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1606.00704"
    },
    "63": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Resnet_in_Resnet.md",
        "transcript": "\nWhat\n\nThey describe an architecture that merges classical convolutional networks and residual networks.\nThe architecture can (theoretically) learn anything that a classical convolutional network or a residual network can learn, as it contains both of them.\nThe architecture can (theoretically) learn how many convolutional layers it should use per residual block (up to the amount of convolutional layers in the whole network).\n\n They describe an architecture that merges classical convolutional networks and residual networks. The architecture can (theoretically) learn anything that a classical convolutional network or a residual network can learn, as it contains both of them. The architecture can (theoretically) learn how many convolutional layers it should use per residual block (up to the amount of convolutional layers in the whole network). \nHow\n\nJust like residual networks, they have \"blocks\". Each block contains convolutional layers.\nEach block contains residual units and non-residual units.\nThey have two \"streams\" of data in their network (just matrices generated by each block):\n\nResidual stream: The residual blocks write to this stream (i.e. it's their output).\nTransient stream: The non-residual blocks write to this stream.\n\n\nResidual and non-residual layers receive both streams as input, but only write to their stream as output.\nTheir architecture visualized:\n\nBecause of this architecture, their model can learn the number of layers per residual block (though BN and ReLU might cause problems here?):\n\nThe easiest way to implement this should be along the lines of the following (some of the visualized convolutions can be merged):\n\nInput of size CxHxW (both streams, each C/2 planes)\n\nConcat\n\nResidual block: Apply C/2 convolutions to the C input planes, with shortcut addition afterwards.\nTransient block: Apply C/2 convolutions to the C input planes.\n\n\nApply BN\nApply ReLU\n\n\nOutput of size CxHxW.\n\n\nThe whole operation can also be implemented with just a single convolutional layer, but then one has to make sure that some weights stay at zero.\n\n Just like residual networks, they have \"blocks\". Each block contains convolutional layers. Each block contains residual units and non-residual units. They have two \"streams\" of data in their network (just matrices generated by each block):\n\nResidual stream: The residual blocks write to this stream (i.e. it's their output).\nTransient stream: The non-residual blocks write to this stream.\n\n Residual stream: The residual blocks write to this stream (i.e. it's their output). Transient stream: The non-residual blocks write to this stream. Residual and non-residual layers receive both streams as input, but only write to their stream as output. Their architecture visualized:\n Because of this architecture, their model can learn the number of layers per residual block (though BN and ReLU might cause problems here?):\n The easiest way to implement this should be along the lines of the following (some of the visualized convolutions can be merged):\n\nInput of size CxHxW (both streams, each C/2 planes)\n\nConcat\n\nResidual block: Apply C/2 convolutions to the C input planes, with shortcut addition afterwards.\nTransient block: Apply C/2 convolutions to the C input planes.\n\n\nApply BN\nApply ReLU\n\n\nOutput of size CxHxW.\n\n Input of size CxHxW (both streams, each C/2 planes)\n\nConcat\n\nResidual block: Apply C/2 convolutions to the C input planes, with shortcut addition afterwards.\nTransient block: Apply C/2 convolutions to the C input planes.\n\n\nApply BN\nApply ReLU\n\n Concat\n\nResidual block: Apply C/2 convolutions to the C input planes, with shortcut addition afterwards.\nTransient block: Apply C/2 convolutions to the C input planes.\n\n Residual block: Apply C/2 convolutions to the C input planes, with shortcut addition afterwards. Transient block: Apply C/2 convolutions to the C input planes. Apply BN Apply ReLU Output of size CxHxW. The whole operation can also be implemented with just a single convolutional layer, but then one has to make sure that some weights stay at zero. \nResults\n\nThey test on CIFAR-10 and CIFAR-100.\nThey search for optimal hyperparameters (learning rate, optimizer, L2 penalty, initialization method, type of shortcut connection in residual blocks) using a grid search.\nTheir model improves upon a wide ResNet and an equivalent non-residual CNN by a good margin (CIFAR-10: 0.5-1%, CIFAR-100: 1-2%).\n\n They test on CIFAR-10 and CIFAR-100. They search for optimal hyperparameters (learning rate, optimizer, L2 penalty, initialization method, type of shortcut connection in residual blocks) using a grid search. Their model improves upon a wide ResNet and an equivalent non-residual CNN by a good margin (CIFAR-10: 0.5-1%, CIFAR-100: 1-2%). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1603.08029"
    },
    "64": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Rank_Ordered_Autoencoders.md",
        "transcript": "\nWhat\n\nAutoencoders typically have some additional criterion that pushes them towards learning meaningful representations.\n\nE.g. L1-Penalty on the code layer (z), Dropout on z, Noise on z.\nOften, representations with sparse activations are considered meaningful (so that each activation reflects are clear concept).\n\n\nThis paper introduces another technique that leads to sparsity.\nThey use a rank ordering on z.\nThe first (according to the ranking) activations have to do most of the reconstruction work of the data (i.e. image).\n\n Autoencoders typically have some additional criterion that pushes them towards learning meaningful representations.\n\nE.g. L1-Penalty on the code layer (z), Dropout on z, Noise on z.\nOften, representations with sparse activations are considered meaningful (so that each activation reflects are clear concept).\n\n E.g. L1-Penalty on the code layer (z), Dropout on z, Noise on z. Often, representations with sparse activations are considered meaningful (so that each activation reflects are clear concept). This paper introduces another technique that leads to sparsity. They use a rank ordering on z. The first (according to the ranking) activations have to do most of the reconstruction work of the data (i.e. image). \nHow\n\nBasic architecture:\n\nThey use an Autoencoder architecture: Input -> Encoder -> z -> Decoder -> Output.\nTheir encoder and decoder seem to be empty, i.e. z is the only hidden layer in the network.\nTheir output is not just one image (or whatever is encoded), instead they generate one for every unit in layer z.\nThen they order these outputs based on the activation of the units in z (rank ordering), i.e. the output of the unit with the highest activation is placed in the first position, the output of the unit with the 2nd highest activation gets the 2nd position and so on.\nThey then generate the final output image based on a cumulative sum. So for three reconstructed output images I1, I2, I3 (rank ordered that way) they would compute final image = I1 + (I1+I2) + (I1+I2+I3).\nThey then compute the error based on that reconstruction (reconstruction - input image) and backpropagate it.\n\n\nCumulative sum:\n\nUsing the cumulative sum puts most optimization pressure on units with high activation, as they have the largest influence on the reconstruction error.\nThe cumulative sum is best optimized by letting few units have high activations and generate most of the output (correctly). All the other units have ideally low to zero activations and low or no influence on the output. (Though if the output generated by the first units is wrong, you should then end up with an extremely high cumulative error sum...)\n\nSo their z coding should end up with few but high activations, i.e. it should become very sparse.\n\n\nThe cumulative generates an individual error per output, while an ordinary sum generates the same error for every output. They argue that this \"blurs\" the error less.\n\n\nTo avoid blow ups in their network they use TReLUs, which saturate below 0 and above 1, i.e. min(1, max(0, input)).\nThey use a custom derivative function for the TReLUs, which is dependent on both the input value of the unit and its gradient. Basically, if the input is >1 (saturated) and the error is high, then the derivative pushes the weight down, so that the input gets into the unsaturated regime. Similarly for input values <0 (pushed up). If the input value is between 0 and 1 and/or the error is low, then nothing is changed.\nThey argue that the algorithmic complexity of the rank ordering should be low, due to sorts being O(n log(n)), where n is the number of hidden units in z.\n\n Basic architecture:\n\nThey use an Autoencoder architecture: Input -> Encoder -> z -> Decoder -> Output.\nTheir encoder and decoder seem to be empty, i.e. z is the only hidden layer in the network.\nTheir output is not just one image (or whatever is encoded), instead they generate one for every unit in layer z.\nThen they order these outputs based on the activation of the units in z (rank ordering), i.e. the output of the unit with the highest activation is placed in the first position, the output of the unit with the 2nd highest activation gets the 2nd position and so on.\nThey then generate the final output image based on a cumulative sum. So for three reconstructed output images I1, I2, I3 (rank ordered that way) they would compute final image = I1 + (I1+I2) + (I1+I2+I3).\nThey then compute the error based on that reconstruction (reconstruction - input image) and backpropagate it.\n\n They use an Autoencoder architecture: Input -> Encoder -> z -> Decoder -> Output. Their encoder and decoder seem to be empty, i.e. z is the only hidden layer in the network. Their output is not just one image (or whatever is encoded), instead they generate one for every unit in layer z. Then they order these outputs based on the activation of the units in z (rank ordering), i.e. the output of the unit with the highest activation is placed in the first position, the output of the unit with the 2nd highest activation gets the 2nd position and so on. They then generate the final output image based on a cumulative sum. So for three reconstructed output images I1, I2, I3 (rank ordered that way) they would compute final image = I1 + (I1+I2) + (I1+I2+I3). They then compute the error based on that reconstruction (reconstruction - input image) and backpropagate it. Cumulative sum:\n\nUsing the cumulative sum puts most optimization pressure on units with high activation, as they have the largest influence on the reconstruction error.\nThe cumulative sum is best optimized by letting few units have high activations and generate most of the output (correctly). All the other units have ideally low to zero activations and low or no influence on the output. (Though if the output generated by the first units is wrong, you should then end up with an extremely high cumulative error sum...)\n\nSo their z coding should end up with few but high activations, i.e. it should become very sparse.\n\n\nThe cumulative generates an individual error per output, while an ordinary sum generates the same error for every output. They argue that this \"blurs\" the error less.\n\n Using the cumulative sum puts most optimization pressure on units with high activation, as they have the largest influence on the reconstruction error. The cumulative sum is best optimized by letting few units have high activations and generate most of the output (correctly). All the other units have ideally low to zero activations and low or no influence on the output. (Though if the output generated by the first units is wrong, you should then end up with an extremely high cumulative error sum...)\n\nSo their z coding should end up with few but high activations, i.e. it should become very sparse.\n\n So their z coding should end up with few but high activations, i.e. it should become very sparse. The cumulative generates an individual error per output, while an ordinary sum generates the same error for every output. They argue that this \"blurs\" the error less. To avoid blow ups in their network they use TReLUs, which saturate below 0 and above 1, i.e. min(1, max(0, input)). They use a custom derivative function for the TReLUs, which is dependent on both the input value of the unit and its gradient. Basically, if the input is >1 (saturated) and the error is high, then the derivative pushes the weight down, so that the input gets into the unsaturated regime. Similarly for input values <0 (pushed up). If the input value is between 0 and 1 and/or the error is low, then nothing is changed. They argue that the algorithmic complexity of the rank ordering should be low, due to sorts being O(n log(n)), where n is the number of hidden units in z. \nResults\n\nThey autoencode 7x7 patches from CIFAR-10.\nThey get very sparse activations.\nTraining and test loss develop identically, i.e. no overfitting.\n\n They autoencode 7x7 patches from CIFAR-10. They get very sparse activations. Training and test loss develop identically, i.e. no overfitting. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1605.01749"
    },
    "65": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Wide_Residual_Networks.md",
        "transcript": "\nWhat\n\nThe authors start with a standard ResNet architecture (i.e. residual network has suggested in \"Identity Mappings in Deep Residual Networks\").\n\nTheir residual block:\n\nSeveral residual blocks of 16 filters per conv-layer, followed by 32 and then 64 filters per conv-layer.\n\n\nThey empirically try to answer the following questions:\n\nHow many residual blocks are optimal? (Depth)\nHow many filters should be used per convolutional layer? (Width)\nHow many convolutional layers should be used per residual block?\nDoes Dropout between the convolutional layers help?\n\n\n\n The authors start with a standard ResNet architecture (i.e. residual network has suggested in \"Identity Mappings in Deep Residual Networks\").\n\nTheir residual block:\n\nSeveral residual blocks of 16 filters per conv-layer, followed by 32 and then 64 filters per conv-layer.\n\n Their residual block:\n Several residual blocks of 16 filters per conv-layer, followed by 32 and then 64 filters per conv-layer. They empirically try to answer the following questions:\n\nHow many residual blocks are optimal? (Depth)\nHow many filters should be used per convolutional layer? (Width)\nHow many convolutional layers should be used per residual block?\nDoes Dropout between the convolutional layers help?\n\n How many residual blocks are optimal? (Depth) How many filters should be used per convolutional layer? (Width) How many convolutional layers should be used per residual block? Does Dropout between the convolutional layers help? \nResults\n\nLayers per block and kernel sizes:\n\nUsing 2 convolutional layers per residual block seems to perform best:\n\nUsing 3x3 kernel sizes for both layers seems to perform best.\nHowever, using 3 layers with kernel sizes 3x3, 1x1, 3x3 and then using less residual blocks performs nearly as good and decreases the required time per batch.\n\n\nWidth and depth:\n\nIncreasing the width considerably improves the test error.\nThey achieve the best results (on CIFAR-10) when decreasing the depth to 28 convolutional layers, with each having 10 times their normal width (i.e. 16*10 filters, 32*10 and 64*10):\n\nThey argue that their results show no evidence that would support the common theory that thin and deep networks somehow regularized better than wide and shallow(er) networks.\n\n\nDropout:\n\nThey use dropout with p=0.3 (CIFAR) and p=0.4 (SVHN).\nOn CIFAR-10 dropout doesn't seem to consistently improve test error.\nOn CIFAR-100 and SVHN dropout seems to lead to improvements that are either small (wide and shallower net, i.e. depth=28, width multiplier=10) or significant (ResNet-50).\n\nThey also observed oscillations in error (both train and test) during the training. Adding dropout decreased these oscillations.\n\n\nComputational efficiency:\n\nApplying few big convolutions is much more efficient on GPUs than applying many small ones sequentially.\nTheir network with the best test error is 1.6 times faster than ResNet-1001, despite having about 3 times more parameters.\n\n\n\n Layers per block and kernel sizes:\n\nUsing 2 convolutional layers per residual block seems to perform best:\n\nUsing 3x3 kernel sizes for both layers seems to perform best.\nHowever, using 3 layers with kernel sizes 3x3, 1x1, 3x3 and then using less residual blocks performs nearly as good and decreases the required time per batch.\n\n Using 2 convolutional layers per residual block seems to perform best:\n Using 3x3 kernel sizes for both layers seems to perform best. However, using 3 layers with kernel sizes 3x3, 1x1, 3x3 and then using less residual blocks performs nearly as good and decreases the required time per batch. Width and depth:\n\nIncreasing the width considerably improves the test error.\nThey achieve the best results (on CIFAR-10) when decreasing the depth to 28 convolutional layers, with each having 10 times their normal width (i.e. 16*10 filters, 32*10 and 64*10):\n\nThey argue that their results show no evidence that would support the common theory that thin and deep networks somehow regularized better than wide and shallow(er) networks.\n\n Increasing the width considerably improves the test error. They achieve the best results (on CIFAR-10) when decreasing the depth to 28 convolutional layers, with each having 10 times their normal width (i.e. 16*10 filters, 32*10 and 64*10):\n They argue that their results show no evidence that would support the common theory that thin and deep networks somehow regularized better than wide and shallow(er) networks. Dropout:\n\nThey use dropout with p=0.3 (CIFAR) and p=0.4 (SVHN).\nOn CIFAR-10 dropout doesn't seem to consistently improve test error.\nOn CIFAR-100 and SVHN dropout seems to lead to improvements that are either small (wide and shallower net, i.e. depth=28, width multiplier=10) or significant (ResNet-50).\n\nThey also observed oscillations in error (both train and test) during the training. Adding dropout decreased these oscillations.\n\n They use dropout with p=0.3 (CIFAR) and p=0.4 (SVHN). On CIFAR-10 dropout doesn't seem to consistently improve test error. On CIFAR-100 and SVHN dropout seems to lead to improvements that are either small (wide and shallower net, i.e. depth=28, width multiplier=10) or significant (ResNet-50).\n They also observed oscillations in error (both train and test) during the training. Adding dropout decreased these oscillations. Computational efficiency:\n\nApplying few big convolutions is much more efficient on GPUs than applying many small ones sequentially.\nTheir network with the best test error is 1.6 times faster than ResNet-1001, despite having about 3 times more parameters.\n\n Applying few big convolutions is much more efficient on GPUs than applying many small ones sequentially. Their network with the best test error is 1.6 times faster than ResNet-1001, despite having about 3 times more parameters. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1605.07146v1"
    },
    "66": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Identity_Mappings_in_Deep_Residual_Networks.md",
        "transcript": "\nWhat\n\nThe authors reevaluate the original residual design of neural networks.\nThey compare various architectures of residual units and actually find one that works quite a bit better.\n\n The authors reevaluate the original residual design of neural networks. They compare various architectures of residual units and actually find one that works quite a bit better. \nHow\n\nThe new variation starts the transformation branch of each residual unit with BN and a ReLU.\nIt removes BN and ReLU after the last convolution.\nAs a result, the information from previous layers can flow completely unaltered through the shortcut branch of each residual unit.\nThe image below shows some variations (of the position of BN and ReLU) that they tested. The new and better design is on the right:\n\nThey also tried various alternative designs for the shortcut connections. However, all of these designs performed worse than the original one. Only one (d) came close under certain conditions. Therefore, the recommendation is to stick with the old/original design.\n\n\n The new variation starts the transformation branch of each residual unit with BN and a ReLU. It removes BN and ReLU after the last convolution. As a result, the information from previous layers can flow completely unaltered through the shortcut branch of each residual unit. The image below shows some variations (of the position of BN and ReLU) that they tested. The new and better design is on the right:\n They also tried various alternative designs for the shortcut connections. However, all of these designs performed worse than the original one. Only one (d) came close under certain conditions. Therefore, the recommendation is to stick with the old/original design.\n \nResults\n\nSignificantly faster training for very deep residual networks (1001 layers).\nBetter regularization due to the placement of BN.\nCIFAR-10 and CIFAR-100 results, old vs. new design:\n\n\n Significantly faster training for very deep residual networks (1001 layers). Better regularization due to the placement of BN. CIFAR-10 and CIFAR-100 results, old vs. new design:\n \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1603.05027v2"
    },
    "67": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Swapout.md",
        "transcript": "\nWhat\n\nThey describe a regularization method similar to dropout and stochastic depth.\nThe method could be viewed as a merge of the two techniques (dropout, stochastic depth).\nThe method seems to regularize better than any of the two alone.\n\n They describe a regularization method similar to dropout and stochastic depth. The method could be viewed as a merge of the two techniques (dropout, stochastic depth). The method seems to regularize better than any of the two alone. \nHow\n\nLet x be the input to a layer. That layer produces an output. The output can be:\n\nFeed forward (\"classic\") network: F(x).\nResidual network: x + F(x).\n\n\nThe standard dropout-like methods do the following:\n\nDropout in feed forward networks: Sometimes 0, sometimes F(x). Decided per unit.\nDropout in residual networks (rarely used): Sometimes 0, sometimes x + F(x). Decided per unit.\nStochastic depth (only in residual networks): Sometimes x, sometimes x + F(x). Decided per layer.\nSkip forward (only in residual networks): Sometimes x, sometimes x + F(x). Decided per unit.\nSwapout (any network): Sometimes 0, sometimes F(x), sometimes x, sometimes x + F(x). Decided per unit.\n\n\nSwapout can be represented using the formula y = theta_1 * x + theta_2 * F(x).\n\n* is the element-wise product.\ntheta_1 and theta_2 are tensors following bernoulli distributions, i.e. their values are all exactly 0 or exactly 1.\nSetting the values of theta_1 and theta_2 per unit in the right way leads to the values 0 (both 0), x (1, 0), F(x) (0, 1) or x + F(x) (1, 1).\n\n\nDeterministic and Stochastic Inference\n\nIdeally, when using a dropout-like technique you would like to get rid of its stochastic effects during prediction, so that you can predict values with exactly one forward pass through the network (instead of having to average over many passes).\nFor Swapout it can be mathematically shown that you can't calculate a deterministic version of it that performs equally to the stochastic one (averaging over many forward passes).\nThis is even more the case when using Batch Normalization in a network. (Actually also when not using Swapout, but instead Dropout + BN.)\nSo for best results you should use the stochastic method (averaging over many forward passes).\n\n\n\n Let x be the input to a layer. That layer produces an output. The output can be:\n\nFeed forward (\"classic\") network: F(x).\nResidual network: x + F(x).\n\n Feed forward (\"classic\") network: F(x). Residual network: x + F(x). The standard dropout-like methods do the following:\n\nDropout in feed forward networks: Sometimes 0, sometimes F(x). Decided per unit.\nDropout in residual networks (rarely used): Sometimes 0, sometimes x + F(x). Decided per unit.\nStochastic depth (only in residual networks): Sometimes x, sometimes x + F(x). Decided per layer.\nSkip forward (only in residual networks): Sometimes x, sometimes x + F(x). Decided per unit.\nSwapout (any network): Sometimes 0, sometimes F(x), sometimes x, sometimes x + F(x). Decided per unit.\n\n Dropout in feed forward networks: Sometimes 0, sometimes F(x). Decided per unit. Dropout in residual networks (rarely used): Sometimes 0, sometimes x + F(x). Decided per unit. Stochastic depth (only in residual networks): Sometimes x, sometimes x + F(x). Decided per layer. Skip forward (only in residual networks): Sometimes x, sometimes x + F(x). Decided per unit. Swapout (any network): Sometimes 0, sometimes F(x), sometimes x, sometimes x + F(x). Decided per unit. Swapout can be represented using the formula y = theta_1 * x + theta_2 * F(x).\n\n* is the element-wise product.\ntheta_1 and theta_2 are tensors following bernoulli distributions, i.e. their values are all exactly 0 or exactly 1.\nSetting the values of theta_1 and theta_2 per unit in the right way leads to the values 0 (both 0), x (1, 0), F(x) (0, 1) or x + F(x) (1, 1).\n\n * is the element-wise product. theta_1 and theta_2 are tensors following bernoulli distributions, i.e. their values are all exactly 0 or exactly 1. Setting the values of theta_1 and theta_2 per unit in the right way leads to the values 0 (both 0), x (1, 0), F(x) (0, 1) or x + F(x) (1, 1). Deterministic and Stochastic Inference\n\nIdeally, when using a dropout-like technique you would like to get rid of its stochastic effects during prediction, so that you can predict values with exactly one forward pass through the network (instead of having to average over many passes).\nFor Swapout it can be mathematically shown that you can't calculate a deterministic version of it that performs equally to the stochastic one (averaging over many forward passes).\nThis is even more the case when using Batch Normalization in a network. (Actually also when not using Swapout, but instead Dropout + BN.)\nSo for best results you should use the stochastic method (averaging over many forward passes).\n\n Ideally, when using a dropout-like technique you would like to get rid of its stochastic effects during prediction, so that you can predict values with exactly one forward pass through the network (instead of having to average over many passes). For Swapout it can be mathematically shown that you can't calculate a deterministic version of it that performs equally to the stochastic one (averaging over many forward passes). This is even more the case when using Batch Normalization in a network. (Actually also when not using Swapout, but instead Dropout + BN.) So for best results you should use the stochastic method (averaging over many forward passes). \nResults\n\nThey compare various dropout-like methods, including Swapout, applied to residual networks. (On CIFAR-10 and CIFAR-100.)\nGeneral performance:\n\nResults with Swapout are better than with the other methods.\nAccording to their results, the ranking of methods is roughly: Swapout > Dropout > Stochastic Depth > Skip Forward > None.\n\n\nStochastic vs deterministic method:\n\nThe stochastic method of swapout (average over N forward passes) performs significantly better than the deterministic one.\nUsing about 15-30 forward passes seems to yield good results.\n\n\nOptimal parameter choice:\n\nPreviously the Swapout-formula y = theta_1 * x + theta_2 * F(x) was mentioned.\ntheta_1 and theta_2 are generated via Bernoulli distributions which have parameters p_1 and p_2.\nIf using fixed values for p_1 and p_2 throughout the network, it seems to be best to either set both of them to 0.5 or to set p_1 to >0.5 and p_2 to <0.5 (preference towards y = x).\nIt's best however to start both at 1.0 (always y = x + F(x)) and to then linearly decay them to both 0.5 towards the end of the network, i.e. to apply less noise to the early layers. (This is similar to the results in the Stochastic Depth paper.)\n\n\nThin vs. wide residual networks:\n\nThe standard residual networks that they compared to used a (16, 32, 64) pattern for their layers, i.e. they started with layers of each having 16 convolutional filters, followed by some layers with each having 32 filters, followed by some layers with 64 filters.\nThey tried instead a (32, 64, 128) pattern, i.e. they doubled the amount of filters.\nThen they reduced the number of layers from 100 down to 20.\nTheir wider residual network performed significantly better than the deep and thin counterpart. However, their parameter count also increased by about 4 times.\nIncreasing the pattern again to (64, 128, 256) and increasing the number of layers from 20 to 32 leads to another performance improvement, beating a 1000-layer network of pattern (16, 32, 64). (Parameter count is then 27 times the original value.)\n\n\n\n They compare various dropout-like methods, including Swapout, applied to residual networks. (On CIFAR-10 and CIFAR-100.) General performance:\n\nResults with Swapout are better than with the other methods.\nAccording to their results, the ranking of methods is roughly: Swapout > Dropout > Stochastic Depth > Skip Forward > None.\n\n Results with Swapout are better than with the other methods. According to their results, the ranking of methods is roughly: Swapout > Dropout > Stochastic Depth > Skip Forward > None. Stochastic vs deterministic method:\n\nThe stochastic method of swapout (average over N forward passes) performs significantly better than the deterministic one.\nUsing about 15-30 forward passes seems to yield good results.\n\n The stochastic method of swapout (average over N forward passes) performs significantly better than the deterministic one. Using about 15-30 forward passes seems to yield good results. Optimal parameter choice:\n\nPreviously the Swapout-formula y = theta_1 * x + theta_2 * F(x) was mentioned.\ntheta_1 and theta_2 are generated via Bernoulli distributions which have parameters p_1 and p_2.\nIf using fixed values for p_1 and p_2 throughout the network, it seems to be best to either set both of them to 0.5 or to set p_1 to >0.5 and p_2 to <0.5 (preference towards y = x).\nIt's best however to start both at 1.0 (always y = x + F(x)) and to then linearly decay them to both 0.5 towards the end of the network, i.e. to apply less noise to the early layers. (This is similar to the results in the Stochastic Depth paper.)\n\n Previously the Swapout-formula y = theta_1 * x + theta_2 * F(x) was mentioned. theta_1 and theta_2 are generated via Bernoulli distributions which have parameters p_1 and p_2. If using fixed values for p_1 and p_2 throughout the network, it seems to be best to either set both of them to 0.5 or to set p_1 to >0.5 and p_2 to <0.5 (preference towards y = x). It's best however to start both at 1.0 (always y = x + F(x)) and to then linearly decay them to both 0.5 towards the end of the network, i.e. to apply less noise to the early layers. (This is similar to the results in the Stochastic Depth paper.) Thin vs. wide residual networks:\n\nThe standard residual networks that they compared to used a (16, 32, 64) pattern for their layers, i.e. they started with layers of each having 16 convolutional filters, followed by some layers with each having 32 filters, followed by some layers with 64 filters.\nThey tried instead a (32, 64, 128) pattern, i.e. they doubled the amount of filters.\nThen they reduced the number of layers from 100 down to 20.\nTheir wider residual network performed significantly better than the deep and thin counterpart. However, their parameter count also increased by about 4 times.\nIncreasing the pattern again to (64, 128, 256) and increasing the number of layers from 20 to 32 leads to another performance improvement, beating a 1000-layer network of pattern (16, 32, 64). (Parameter count is then 27 times the original value.)\n\n The standard residual networks that they compared to used a (16, 32, 64) pattern for their layers, i.e. they started with layers of each having 16 convolutional filters, followed by some layers with each having 32 filters, followed by some layers with 64 filters. They tried instead a (32, 64, 128) pattern, i.e. they doubled the amount of filters. Then they reduced the number of layers from 100 down to 20. Their wider residual network performed significantly better than the deep and thin counterpart. However, their parameter count also increased by about 4 times. Increasing the pattern again to (64, 128, 256) and increasing the number of layers from 20 to 32 leads to another performance improvement, beating a 1000-layer network of pattern (16, 32, 64). (Parameter count is then 27 times the original value.) \nComments\n\nStochastic depth works layer-wise, while Swapout works unit-wise. When a layer in Stochastic Depth is dropped, its whole forward- and backward-pass don't have to be calculated. That saves time. Swapout is not going to save time.\nThey argue that dropout+BN would also profit from using stochastic inference instead of deterministic inference, just like Swapout does. However, they don't mention using it for dropout in their comparison, only for Swapout.\nThey show that linear decay for their parameters (less dropping on early layers, more on later ones) significantly improves the results of Swapout. However, they don't mention testing the same thing for dropout. Maybe dropout would also profit from it?\nFor the above two points: Dropout's test error is at 5.87, Swapout's test error is at 5.68. So the difference is already quite small, making any disadvantage for dropout significant.\n\n Stochastic depth works layer-wise, while Swapout works unit-wise. When a layer in Stochastic Depth is dropped, its whole forward- and backward-pass don't have to be calculated. That saves time. Swapout is not going to save time. They argue that dropout+BN would also profit from using stochastic inference instead of deterministic inference, just like Swapout does. However, they don't mention using it for dropout in their comparison, only for Swapout. They show that linear decay for their parameters (less dropping on early layers, more on later ones) significantly improves the results of Swapout. However, they don't mention testing the same thing for dropout. Maybe dropout would also profit from it? For the above two points: Dropout's test error is at 5.87, Swapout's test error is at 5.68. So the difference is already quite small, making any disadvantage for dropout significant. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1605.06465v1"
    },
    "68": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Multi-Scale_Context_Aggregation_by_Dilated_Convolutions.md",
        "transcript": "\nWhat\n\nThey describe a variation of convolutions that have a differently structured receptive field.\nThey argue that their variation works better for dense prediction, i.e. for predicting values for every pixel in an image (e.g. coloring, segmentation, upscaling).\n\n They describe a variation of convolutions that have a differently structured receptive field. They argue that their variation works better for dense prediction, i.e. for predicting values for every pixel in an image (e.g. coloring, segmentation, upscaling). \nHow\n\nOne can image the input into a convolutional layer as a 3d-grid. Each cell is a \"pixel\" generated by a filter.\nNormal convolutions compute their output per cell as a weighted sum of the input cells in a dense area. I.e. all input cells are right next to each other.\nIn dilated convolutions, the cells are not right next to each other. E.g. 2-dilated convolutions skip 1 cell between each input cell, 3-dilated convolutions skip 2 cells etc. (Similar to striding.)\nNormal convolutions are simply 1-dilated convolutions (skipping 0 cells).\nOne can use a 1-dilated convolution and then a 2-dilated convolution. The receptive field of the second convolution will then be 7x7 instead of the usual 5x5 due to the spacing.\nIncreasing the dilation factor by 2 per layer (1, 2, 4, 8, ...) leads to an exponential increase in the receptive field size, while every cell in the receptive field will still be part in the computation of at least one convolution.\nThey had problems with badly performing networks, which they fixed using an identity initialization for the weights. (Sounds like just using resdiual connections would have been easier.)\n\n One can image the input into a convolutional layer as a 3d-grid. Each cell is a \"pixel\" generated by a filter. Normal convolutions compute their output per cell as a weighted sum of the input cells in a dense area. I.e. all input cells are right next to each other. In dilated convolutions, the cells are not right next to each other. E.g. 2-dilated convolutions skip 1 cell between each input cell, 3-dilated convolutions skip 2 cells etc. (Similar to striding.) Normal convolutions are simply 1-dilated convolutions (skipping 0 cells). One can use a 1-dilated convolution and then a 2-dilated convolution. The receptive field of the second convolution will then be 7x7 instead of the usual 5x5 due to the spacing. Increasing the dilation factor by 2 per layer (1, 2, 4, 8, ...) leads to an exponential increase in the receptive field size, while every cell in the receptive field will still be part in the computation of at least one convolution. They had problems with badly performing networks, which they fixed using an identity initialization for the weights. (Sounds like just using resdiual connections would have been easier.) Results\n\nThey took a VGG net, removed the pooling layers and replaced the convolutions with dilated ones (weights can be kept).\nThey then used the network to segment images.\nTheir results were significantly better than previous methods.\nThey also added another network with more dilated convolutions in front of the VGG one, again improving the results.\n\n They took a VGG net, removed the pooling layers and replaced the convolutions with dilated ones (weights can be kept). They then used the network to segment images. Their results were significantly better than previous methods. They also added another network with more dilated convolutions in front of the VGG one, again improving the results. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.07122"
    },
    "69": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Texture_Synthesis_Through_CNNs_and_Spectrum_Constraints.md",
        "transcript": "\nWhat\n\nThe well known method of Artistic Style Transfer can be used to generate new texture images (from an existing example) by skipping the content loss and only using the style loss.\nThe method however can have problems with large scale structures and quasi-periodic patterns.\nThey add a new loss based on the spectrum of the images (synthesized image and style image), which decreases these problems and handles especially periodic patterns well.\n\n The well known method of Artistic Style Transfer can be used to generate new texture images (from an existing example) by skipping the content loss and only using the style loss. The method however can have problems with large scale structures and quasi-periodic patterns. They add a new loss based on the spectrum of the images (synthesized image and style image), which decreases these problems and handles especially periodic patterns well. \nHow\n\nEverything is handled in the same way as in the Artistic Style Transfer paper (without content loss).\nOn top of that they add their spectrum loss:\n\nThe loss is based on a squared distance, i.e. 1/2 d(I_s, I_t)^2.\n\nI_s is the last synthesized image.\nI_t is the texture example.\n\n\nd(I_s, I_t) then does the following:\n\nIt assumes that I_t is an example for a space of target images.\nWithin that set it finds the image I_p which is most similar to I_s. That is done using a projection via Fourier Transformations. (See formula 5 in the paper.)\nThe returned distance is then I_s - I_p.\n\n\n\n\n\n Everything is handled in the same way as in the Artistic Style Transfer paper (without content loss). On top of that they add their spectrum loss:\n\nThe loss is based on a squared distance, i.e. 1/2 d(I_s, I_t)^2.\n\nI_s is the last synthesized image.\nI_t is the texture example.\n\n\nd(I_s, I_t) then does the following:\n\nIt assumes that I_t is an example for a space of target images.\nWithin that set it finds the image I_p which is most similar to I_s. That is done using a projection via Fourier Transformations. (See formula 5 in the paper.)\nThe returned distance is then I_s - I_p.\n\n\n\n The loss is based on a squared distance, i.e. 1/2 d(I_s, I_t)^2.\n\nI_s is the last synthesized image.\nI_t is the texture example.\n\n I_s is the last synthesized image. I_t is the texture example. d(I_s, I_t) then does the following:\n\nIt assumes that I_t is an example for a space of target images.\nWithin that set it finds the image I_p which is most similar to I_s. That is done using a projection via Fourier Transformations. (See formula 5 in the paper.)\nThe returned distance is then I_s - I_p.\n\n It assumes that I_t is an example for a space of target images. Within that set it finds the image I_p which is most similar to I_s. That is done using a projection via Fourier Transformations. (See formula 5 in the paper.) The returned distance is then I_s - I_p. \nResults\n\nEqual quality for textures without quasi-periodic structures.\nSignificantly better quality for textures with quasi-periodic structures.\n\n Equal quality for textures without quasi-periodic structures. Significantly better quality for textures with quasi-periodic structures. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1605.01141v3"
    },
    "70": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Markovian_GANs.md",
        "transcript": "\nSee also\n\nVideo explanation by authors\n\n Video explanation by authors \nWhat\n\nThey describe a method that can be used for two problems:\n\n(1) Choose a style image and apply that style to other images.\n(2) Choose an example texture image and create new texture images that look similar.\n\n\nIn contrast to previous methods their method can be applied very fast to images (style transfer) or noise (texture creation). However, per style/texture a single (expensive) initial training session is still necessary.\nTheir method builds upon their previous paper \"Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis\".\n\n They describe a method that can be used for two problems:\n\n(1) Choose a style image and apply that style to other images.\n(2) Choose an example texture image and create new texture images that look similar.\n\n (1) Choose a style image and apply that style to other images. (2) Choose an example texture image and create new texture images that look similar. In contrast to previous methods their method can be applied very fast to images (style transfer) or noise (texture creation). However, per style/texture a single (expensive) initial training session is still necessary. Their method builds upon their previous paper \"Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis\". \nHow\n\nRough overview of their previous method:\n\nTransfer styles using three losses:\n\nContent loss: MSE between VGG representations.\nRegularization loss: Sum of x-gradient and y-gradients (encouraging smooth areas).\nMRF-based style loss: Sample k x k patches from VGG representations of content image and style image. For each patch from content image find the nearest neighbor (based on normalized cross correlation) from style patches. Loss is then the sum of squared errors of euclidean distances between content patches and their nearest neighbors.\n\n\nGeneration of new images is done by starting with noise and then iteratively applying changes that minimize the loss function.\n\n\nThey introduce mostly two major changes:\n\n(a) Get rid of the costly nearest neighbor search for the MRF loss. Instead, use a discriminator-network that receives a patch and rates how real that patch looks.\n\nThis discriminator-network is costly to train, but that only has to be done once (per style/texture).\n\n\n(b) Get rid of the slow, iterative generation of images. Instead, start with the content image (style transfer) or noise image (texture generation) and feed that through a single generator-network to create the output image (with transfered style or generated texture).\n\nThis generator-network is costly to train, but that only has to be done once (per style/texture).\n\n\n\n\nMDANs\n\nThey implement change (a) to the standard architecture and call that an \"MDAN\" (Markovian Deconvolutional Adversarial Networks).\nSo the architecture of the MDAN is:\n\nInput: Image (RGB pixels)\nBranch 1: Markovian Patch Quality Rater (aka Discriminator)\n\nStarts by feeding the image through VGG19 until layer relu3_1. (Note: VGG weights are fixed/not trained.)\nThen extracts k x k patches from the generated representations.\nFeeds each patch through a shallow ConvNet (convolution with BN then fully connected layer).\nTraining loss is a hinge loss, i.e. max margin between classes +1 (real looking patch) and -1 (fake looking patch). (Could also take a single sigmoid output, but they argue that hinge loss isn't as likely to saturate.)\nThis branch will be trained continuously while synthesizing a new image.\n\n\nBranch 2: Content Estimation/Guidance\n\nNote: This branch is only used for style transfer, i.e if using an content image and not for texture generation.\nStarts by feeding the currently synthesized image through VGG19 until layer relu5_1. (Note: VGG weights are fixed/not trained.)\nAlso feeds the content image through VGG19 until layer relu5_1.\nThen uses a MSE loss between both representations (so similar to a MSE on RGB pixels that is often used in autoencoders).\nNothing in this branch needs to trained, the loss only affects the synthesizing of the image.\n\n\n\n\n\n\nMGANs\n\nThe MGAN is like the MDAN, but additionally implements change (b), i.e. they add a generator that takes an image and stylizes it.\nThe generator's architecture is:\n\nInput: Image (RGB pixels) or noise (for texture synthesis)\nOutput: Image (RGB pixels) (stylized input image or generated texture)\nThe generator takes the image (pixels) and feeds that through VGG19 until layer relu4_1.\nSimilar to the DCGAN generator, they then apply a few fractionally strided convolutions (with BN and LeakyReLUs) to that, ending in a Tanh output. (Fractionally strided convolutions increase the height/width of the images, here to compensate the VGG pooling layers.)\nThe output after the Tanh is the output image (RGB pixels).\n\n\nThey train the generator with pairs of (input image, stylized image or texture). These pairs can be gathered by first running the MDAN alone on several images. (With significant augmentation a few dozen pairs already seem to be enough.)\nOne of two possible loss functions can then be used:\n\nSimple standard choice: MSE on the euclidean distance between expected output pixels and generated output pixels. Can cause blurriness.\nBetter choice: MSE on a higher VGG representation. Simply feed the generated output pixels through VGG19 until relu4_1 and the reuse the already generated (see above) VGG-representation of the input image. This is very similar to the pixel-wise comparison, but tends to cause less blurriness.\n\n\nNote: For some reason the authors call their generator a VAE, but don't mention any typical VAE technique, so it's not described like one here.\n\n\nThey use Adam to train their networks.\nFor texture generation they use Perlin Noise instead of simple white noise. In Perlin Noise, lower frequency components dominate more than higher frequency components. White noise didn't work well with the VGG representations in the generator (activations were close to zero).\n\n Rough overview of their previous method:\n\nTransfer styles using three losses:\n\nContent loss: MSE between VGG representations.\nRegularization loss: Sum of x-gradient and y-gradients (encouraging smooth areas).\nMRF-based style loss: Sample k x k patches from VGG representations of content image and style image. For each patch from content image find the nearest neighbor (based on normalized cross correlation) from style patches. Loss is then the sum of squared errors of euclidean distances between content patches and their nearest neighbors.\n\n\nGeneration of new images is done by starting with noise and then iteratively applying changes that minimize the loss function.\n\n Transfer styles using three losses:\n\nContent loss: MSE between VGG representations.\nRegularization loss: Sum of x-gradient and y-gradients (encouraging smooth areas).\nMRF-based style loss: Sample k x k patches from VGG representations of content image and style image. For each patch from content image find the nearest neighbor (based on normalized cross correlation) from style patches. Loss is then the sum of squared errors of euclidean distances between content patches and their nearest neighbors.\n\n Content loss: MSE between VGG representations. Regularization loss: Sum of x-gradient and y-gradients (encouraging smooth areas). MRF-based style loss: Sample k x k patches from VGG representations of content image and style image. For each patch from content image find the nearest neighbor (based on normalized cross correlation) from style patches. Loss is then the sum of squared errors of euclidean distances between content patches and their nearest neighbors. Generation of new images is done by starting with noise and then iteratively applying changes that minimize the loss function. They introduce mostly two major changes:\n\n(a) Get rid of the costly nearest neighbor search for the MRF loss. Instead, use a discriminator-network that receives a patch and rates how real that patch looks.\n\nThis discriminator-network is costly to train, but that only has to be done once (per style/texture).\n\n\n(b) Get rid of the slow, iterative generation of images. Instead, start with the content image (style transfer) or noise image (texture generation) and feed that through a single generator-network to create the output image (with transfered style or generated texture).\n\nThis generator-network is costly to train, but that only has to be done once (per style/texture).\n\n\n\n (a) Get rid of the costly nearest neighbor search for the MRF loss. Instead, use a discriminator-network that receives a patch and rates how real that patch looks.\n\nThis discriminator-network is costly to train, but that only has to be done once (per style/texture).\n\n This discriminator-network is costly to train, but that only has to be done once (per style/texture). (b) Get rid of the slow, iterative generation of images. Instead, start with the content image (style transfer) or noise image (texture generation) and feed that through a single generator-network to create the output image (with transfered style or generated texture).\n\nThis generator-network is costly to train, but that only has to be done once (per style/texture).\n\n This generator-network is costly to train, but that only has to be done once (per style/texture). MDANs\n\nThey implement change (a) to the standard architecture and call that an \"MDAN\" (Markovian Deconvolutional Adversarial Networks).\nSo the architecture of the MDAN is:\n\nInput: Image (RGB pixels)\nBranch 1: Markovian Patch Quality Rater (aka Discriminator)\n\nStarts by feeding the image through VGG19 until layer relu3_1. (Note: VGG weights are fixed/not trained.)\nThen extracts k x k patches from the generated representations.\nFeeds each patch through a shallow ConvNet (convolution with BN then fully connected layer).\nTraining loss is a hinge loss, i.e. max margin between classes +1 (real looking patch) and -1 (fake looking patch). (Could also take a single sigmoid output, but they argue that hinge loss isn't as likely to saturate.)\nThis branch will be trained continuously while synthesizing a new image.\n\n\nBranch 2: Content Estimation/Guidance\n\nNote: This branch is only used for style transfer, i.e if using an content image and not for texture generation.\nStarts by feeding the currently synthesized image through VGG19 until layer relu5_1. (Note: VGG weights are fixed/not trained.)\nAlso feeds the content image through VGG19 until layer relu5_1.\nThen uses a MSE loss between both representations (so similar to a MSE on RGB pixels that is often used in autoencoders).\nNothing in this branch needs to trained, the loss only affects the synthesizing of the image.\n\n\n\n\n\n They implement change (a) to the standard architecture and call that an \"MDAN\" (Markovian Deconvolutional Adversarial Networks). So the architecture of the MDAN is:\n\nInput: Image (RGB pixels)\nBranch 1: Markovian Patch Quality Rater (aka Discriminator)\n\nStarts by feeding the image through VGG19 until layer relu3_1. (Note: VGG weights are fixed/not trained.)\nThen extracts k x k patches from the generated representations.\nFeeds each patch through a shallow ConvNet (convolution with BN then fully connected layer).\nTraining loss is a hinge loss, i.e. max margin between classes +1 (real looking patch) and -1 (fake looking patch). (Could also take a single sigmoid output, but they argue that hinge loss isn't as likely to saturate.)\nThis branch will be trained continuously while synthesizing a new image.\n\n\nBranch 2: Content Estimation/Guidance\n\nNote: This branch is only used for style transfer, i.e if using an content image and not for texture generation.\nStarts by feeding the currently synthesized image through VGG19 until layer relu5_1. (Note: VGG weights are fixed/not trained.)\nAlso feeds the content image through VGG19 until layer relu5_1.\nThen uses a MSE loss between both representations (so similar to a MSE on RGB pixels that is often used in autoencoders).\nNothing in this branch needs to trained, the loss only affects the synthesizing of the image.\n\n\n\n Input: Image (RGB pixels) Branch 1: Markovian Patch Quality Rater (aka Discriminator)\n\nStarts by feeding the image through VGG19 until layer relu3_1. (Note: VGG weights are fixed/not trained.)\nThen extracts k x k patches from the generated representations.\nFeeds each patch through a shallow ConvNet (convolution with BN then fully connected layer).\nTraining loss is a hinge loss, i.e. max margin between classes +1 (real looking patch) and -1 (fake looking patch). (Could also take a single sigmoid output, but they argue that hinge loss isn't as likely to saturate.)\nThis branch will be trained continuously while synthesizing a new image.\n\n Starts by feeding the image through VGG19 until layer relu3_1. (Note: VGG weights are fixed/not trained.) Then extracts k x k patches from the generated representations. Feeds each patch through a shallow ConvNet (convolution with BN then fully connected layer). Training loss is a hinge loss, i.e. max margin between classes +1 (real looking patch) and -1 (fake looking patch). (Could also take a single sigmoid output, but they argue that hinge loss isn't as likely to saturate.) This branch will be trained continuously while synthesizing a new image. Branch 2: Content Estimation/Guidance\n\nNote: This branch is only used for style transfer, i.e if using an content image and not for texture generation.\nStarts by feeding the currently synthesized image through VGG19 until layer relu5_1. (Note: VGG weights are fixed/not trained.)\nAlso feeds the content image through VGG19 until layer relu5_1.\nThen uses a MSE loss between both representations (so similar to a MSE on RGB pixels that is often used in autoencoders).\nNothing in this branch needs to trained, the loss only affects the synthesizing of the image.\n\n Note: This branch is only used for style transfer, i.e if using an content image and not for texture generation. Starts by feeding the currently synthesized image through VGG19 until layer relu5_1. (Note: VGG weights are fixed/not trained.) Also feeds the content image through VGG19 until layer relu5_1. Then uses a MSE loss between both representations (so similar to a MSE on RGB pixels that is often used in autoencoders). Nothing in this branch needs to trained, the loss only affects the synthesizing of the image. MGANs\n\nThe MGAN is like the MDAN, but additionally implements change (b), i.e. they add a generator that takes an image and stylizes it.\nThe generator's architecture is:\n\nInput: Image (RGB pixels) or noise (for texture synthesis)\nOutput: Image (RGB pixels) (stylized input image or generated texture)\nThe generator takes the image (pixels) and feeds that through VGG19 until layer relu4_1.\nSimilar to the DCGAN generator, they then apply a few fractionally strided convolutions (with BN and LeakyReLUs) to that, ending in a Tanh output. (Fractionally strided convolutions increase the height/width of the images, here to compensate the VGG pooling layers.)\nThe output after the Tanh is the output image (RGB pixels).\n\n\nThey train the generator with pairs of (input image, stylized image or texture). These pairs can be gathered by first running the MDAN alone on several images. (With significant augmentation a few dozen pairs already seem to be enough.)\nOne of two possible loss functions can then be used:\n\nSimple standard choice: MSE on the euclidean distance between expected output pixels and generated output pixels. Can cause blurriness.\nBetter choice: MSE on a higher VGG representation. Simply feed the generated output pixels through VGG19 until relu4_1 and the reuse the already generated (see above) VGG-representation of the input image. This is very similar to the pixel-wise comparison, but tends to cause less blurriness.\n\n\nNote: For some reason the authors call their generator a VAE, but don't mention any typical VAE technique, so it's not described like one here.\n\n The MGAN is like the MDAN, but additionally implements change (b), i.e. they add a generator that takes an image and stylizes it. The generator's architecture is:\n\nInput: Image (RGB pixels) or noise (for texture synthesis)\nOutput: Image (RGB pixels) (stylized input image or generated texture)\nThe generator takes the image (pixels) and feeds that through VGG19 until layer relu4_1.\nSimilar to the DCGAN generator, they then apply a few fractionally strided convolutions (with BN and LeakyReLUs) to that, ending in a Tanh output. (Fractionally strided convolutions increase the height/width of the images, here to compensate the VGG pooling layers.)\nThe output after the Tanh is the output image (RGB pixels).\n\n Input: Image (RGB pixels) or noise (for texture synthesis) Output: Image (RGB pixels) (stylized input image or generated texture) The generator takes the image (pixels) and feeds that through VGG19 until layer relu4_1. Similar to the DCGAN generator, they then apply a few fractionally strided convolutions (with BN and LeakyReLUs) to that, ending in a Tanh output. (Fractionally strided convolutions increase the height/width of the images, here to compensate the VGG pooling layers.) The output after the Tanh is the output image (RGB pixels). They train the generator with pairs of (input image, stylized image or texture). These pairs can be gathered by first running the MDAN alone on several images. (With significant augmentation a few dozen pairs already seem to be enough.) One of two possible loss functions can then be used:\n\nSimple standard choice: MSE on the euclidean distance between expected output pixels and generated output pixels. Can cause blurriness.\nBetter choice: MSE on a higher VGG representation. Simply feed the generated output pixels through VGG19 until relu4_1 and the reuse the already generated (see above) VGG-representation of the input image. This is very similar to the pixel-wise comparison, but tends to cause less blurriness.\n\n Simple standard choice: MSE on the euclidean distance between expected output pixels and generated output pixels. Can cause blurriness. Better choice: MSE on a higher VGG representation. Simply feed the generated output pixels through VGG19 until relu4_1 and the reuse the already generated (see above) VGG-representation of the input image. This is very similar to the pixel-wise comparison, but tends to cause less blurriness. Note: For some reason the authors call their generator a VAE, but don't mention any typical VAE technique, so it's not described like one here. They use Adam to train their networks. For texture generation they use Perlin Noise instead of simple white noise. In Perlin Noise, lower frequency components dominate more than higher frequency components. White noise didn't work well with the VGG representations in the generator (activations were close to zero). \nResults\n\nSimilar quality like previous methods, but much faster (compared to most methods).\nFor the Markovian Patch Quality Rater (MDAN branch 1):\n\nThey found that the weights of this branch can be used as initialization for other training sessions (e.g. other texture styles), leading to a decrease in required iterations/epochs.\nUsing VGG for feature extraction seems to be crucial. Training from scratch generated in worse results.\nUsing larger patch sizes preserves more structure of the structure of the style image/texture. Smaller patches leads to more flexibility in generated patterns.\nThey found that using more than 3 convolutional layers or more than 64 filters per layer provided no visible benefit in quality.\n\n\n\n Similar quality like previous methods, but much faster (compared to most methods). For the Markovian Patch Quality Rater (MDAN branch 1):\n\nThey found that the weights of this branch can be used as initialization for other training sessions (e.g. other texture styles), leading to a decrease in required iterations/epochs.\nUsing VGG for feature extraction seems to be crucial. Training from scratch generated in worse results.\nUsing larger patch sizes preserves more structure of the structure of the style image/texture. Smaller patches leads to more flexibility in generated patterns.\nThey found that using more than 3 convolutional layers or more than 64 filters per layer provided no visible benefit in quality.\n\n They found that the weights of this branch can be used as initialization for other training sessions (e.g. other texture styles), leading to a decrease in required iterations/epochs. Using VGG for feature extraction seems to be crucial. Training from scratch generated in worse results. Using larger patch sizes preserves more structure of the structure of the style image/texture. Smaller patches leads to more flexibility in generated patterns. They found that using more than 3 convolutional layers or more than 64 filters per layer provided no visible benefit in quality. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1604.04382"
    },
    "71": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Neural_Doodle.md",
        "transcript": "\nWhat\n\nThey describe a method to transfer image styles based on semantic classes.\nThis allows to:\n\n(1) Transfer styles between images more accurately than with previous models. E.g. so that the background of an image does not receive the style of skin/hair/clothes/... seen in the style image. Skin in the synthesized image should receive the style of skin from the style image. Same for hair, clothes, etc.\n(2) Turn simple doodles into artwork by treating the simplified areas in the doodle as semantic classes and annotating an artwork with these same semantic classes. (E.g. \"this blob should receive the style from these trees.\")\n\n\n\n They describe a method to transfer image styles based on semantic classes. This allows to:\n\n(1) Transfer styles between images more accurately than with previous models. E.g. so that the background of an image does not receive the style of skin/hair/clothes/... seen in the style image. Skin in the synthesized image should receive the style of skin from the style image. Same for hair, clothes, etc.\n(2) Turn simple doodles into artwork by treating the simplified areas in the doodle as semantic classes and annotating an artwork with these same semantic classes. (E.g. \"this blob should receive the style from these trees.\")\n\n (1) Transfer styles between images more accurately than with previous models. E.g. so that the background of an image does not receive the style of skin/hair/clothes/... seen in the style image. Skin in the synthesized image should receive the style of skin from the style image. Same for hair, clothes, etc. (2) Turn simple doodles into artwork by treating the simplified areas in the doodle as semantic classes and annotating an artwork with these same semantic classes. (E.g. \"this blob should receive the style from these trees.\") \nHow\n\nTheir method is based on Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis.\nThey use the same content loss and mostly the same MRF-based style loss. (Apparently they don't use the regularization loss.)\nThey change the input of the MRF-based style loss.\n\nUsually that input would only be the activations of a VGG-layer (for the synthesized image or the style source image).\nThey add a semantic map with weighting gamma to the activation, i.e. <representation of image> = <activation of specific layer for that image> || gamma * <semantic map>.\nThe semantic map has N channels with 1s in a channel where a specific class is located (e.g. skin).\nThe semantic map has to be created by the user for both the content image and the style image.\nAs usually for the MRF loss, patches are then sampled from the representations. The semantic maps then influence the distance measure. I.e. patches are more likely to be sampled from the same semantic class.\nHigher gamma values make it more likely to sample from the same semantic class (because the distance from patches from different classes gets larger).\n\n\nOne can create a small doodle with few colors, then use the colors as the semantic map. Then add a semantic map to an artwork and run the algorithm to transform the doodle into an artwork.\n\n Their method is based on Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis. They use the same content loss and mostly the same MRF-based style loss. (Apparently they don't use the regularization loss.) They change the input of the MRF-based style loss.\n\nUsually that input would only be the activations of a VGG-layer (for the synthesized image or the style source image).\nThey add a semantic map with weighting gamma to the activation, i.e. <representation of image> = <activation of specific layer for that image> || gamma * <semantic map>.\nThe semantic map has N channels with 1s in a channel where a specific class is located (e.g. skin).\nThe semantic map has to be created by the user for both the content image and the style image.\nAs usually for the MRF loss, patches are then sampled from the representations. The semantic maps then influence the distance measure. I.e. patches are more likely to be sampled from the same semantic class.\nHigher gamma values make it more likely to sample from the same semantic class (because the distance from patches from different classes gets larger).\n\n Usually that input would only be the activations of a VGG-layer (for the synthesized image or the style source image). They add a semantic map with weighting gamma to the activation, i.e. <representation of image> = <activation of specific layer for that image> || gamma * <semantic map>. The semantic map has N channels with 1s in a channel where a specific class is located (e.g. skin). The semantic map has to be created by the user for both the content image and the style image. As usually for the MRF loss, patches are then sampled from the representations. The semantic maps then influence the distance measure. I.e. patches are more likely to be sampled from the same semantic class. Higher gamma values make it more likely to sample from the same semantic class (because the distance from patches from different classes gets larger). One can create a small doodle with few colors, then use the colors as the semantic map. Then add a semantic map to an artwork and run the algorithm to transform the doodle into an artwork. \nResults\n\nMore control over the transfered styles than previously.\nLess sensitive to the style weighting, because of the additional gamma hyperparameter.\nEasy transformation from doodle to artwork.\n\n More control over the transfered styles than previously. Less sensitive to the style weighting, because of the additional gamma hyperparameter. Easy transformation from doodle to artwork. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1603.01768"
    },
    "72": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Combining_MRFs_and_CNNs_for_Image_Synthesis.md",
        "transcript": "\nWhat\n\nThey describe a method that applies the style of a source image to a target image.\n\nExample: Let a normal photo look like a van Gogh painting.\nExample: Let a normal car look more like a specific luxury car.\n\n\nTheir method builds upon the well known artistic style paper and uses a new MRF prior.\nThe prior leads to locally more plausible patterns (e.g. less artifacts).\n\n They describe a method that applies the style of a source image to a target image.\n\nExample: Let a normal photo look like a van Gogh painting.\nExample: Let a normal car look more like a specific luxury car.\n\n Example: Let a normal photo look like a van Gogh painting. Example: Let a normal car look more like a specific luxury car. Their method builds upon the well known artistic style paper and uses a new MRF prior. The prior leads to locally more plausible patterns (e.g. less artifacts). \nHow\n\nThey reuse the content loss from the artistic style paper.\n\nThe content loss was calculated by feed the source and target image through a network (here: VGG19) and then estimating the squared error of the euclidean distance between one or more hidden layer activations.\nThey use layer relu4_2 for the distance measurement.\n\n\nThey replace the original style loss with a MRF based style loss.\n\nStep 1: Extract from the source image k x k sized overlapping patches.\nStep 2: Perform step (1) analogously for the target image.\nStep 3: Feed the source image patches through a pretrained network (here: VGG19) and select the representations r_s from specific hidden layers (here: relu3_1, relu4_1).\nStep 4: Perform step (3) analogously for the target image. (Result: r_t)\nStep 5: For each patch of r_s find the best matching patch in r_t (based on normalized cross correlation).\nStep 6: Calculate the sum of squared errors (based on euclidean distances) of each patch in r_s and its best match (according to step 5).\n\n\nThey add a regularizer loss.\n\nThe loss encourages smooth transitions in the synthesized image (i.e. few edges, corners).\nIt is based on the raw pixel values of the last synthesized image.\nFor each pixel in the synthesized image, they calculate the squared x-gradient and the squared y-gradient and then add both.\nThey use the sum of all those values as their loss (i.e. regularizer loss = <sum over all pixels> x-gradient^2 + y-gradient^2).\n\n\nTheir whole optimization problem is then roughly image = argmin_image MRF-style-loss + alpha1 * content-loss + alpha2 * regularizer-loss.\nIn practice, they start their synthesis with a low resolution image and then progressively increase the resolution (each time performing some iterations of optimization).\nIn practice, they sample patches from the style image under several different rotations and scalings.\n\n They reuse the content loss from the artistic style paper.\n\nThe content loss was calculated by feed the source and target image through a network (here: VGG19) and then estimating the squared error of the euclidean distance between one or more hidden layer activations.\nThey use layer relu4_2 for the distance measurement.\n\n The content loss was calculated by feed the source and target image through a network (here: VGG19) and then estimating the squared error of the euclidean distance between one or more hidden layer activations. They use layer relu4_2 for the distance measurement. They replace the original style loss with a MRF based style loss.\n\nStep 1: Extract from the source image k x k sized overlapping patches.\nStep 2: Perform step (1) analogously for the target image.\nStep 3: Feed the source image patches through a pretrained network (here: VGG19) and select the representations r_s from specific hidden layers (here: relu3_1, relu4_1).\nStep 4: Perform step (3) analogously for the target image. (Result: r_t)\nStep 5: For each patch of r_s find the best matching patch in r_t (based on normalized cross correlation).\nStep 6: Calculate the sum of squared errors (based on euclidean distances) of each patch in r_s and its best match (according to step 5).\n\n Step 1: Extract from the source image k x k sized overlapping patches. Step 2: Perform step (1) analogously for the target image. Step 3: Feed the source image patches through a pretrained network (here: VGG19) and select the representations r_s from specific hidden layers (here: relu3_1, relu4_1). Step 4: Perform step (3) analogously for the target image. (Result: r_t) Step 5: For each patch of r_s find the best matching patch in r_t (based on normalized cross correlation). Step 6: Calculate the sum of squared errors (based on euclidean distances) of each patch in r_s and its best match (according to step 5). They add a regularizer loss.\n\nThe loss encourages smooth transitions in the synthesized image (i.e. few edges, corners).\nIt is based on the raw pixel values of the last synthesized image.\nFor each pixel in the synthesized image, they calculate the squared x-gradient and the squared y-gradient and then add both.\nThey use the sum of all those values as their loss (i.e. regularizer loss = <sum over all pixels> x-gradient^2 + y-gradient^2).\n\n The loss encourages smooth transitions in the synthesized image (i.e. few edges, corners). It is based on the raw pixel values of the last synthesized image. For each pixel in the synthesized image, they calculate the squared x-gradient and the squared y-gradient and then add both. They use the sum of all those values as their loss (i.e. regularizer loss = <sum over all pixels> x-gradient^2 + y-gradient^2). Their whole optimization problem is then roughly image = argmin_image MRF-style-loss + alpha1 * content-loss + alpha2 * regularizer-loss. In practice, they start their synthesis with a low resolution image and then progressively increase the resolution (each time performing some iterations of optimization). In practice, they sample patches from the style image under several different rotations and scalings. \nResults\n\nIn comparison to the original artistic style paper:\n\nLess artifacts.\nTheir method tends to preserve style better, but content worse.\nCan handle photorealistic style transfer better, so long as the images are similar enough. If no good matches between patches can be found, their method performs worse.\n\n\n\n In comparison to the original artistic style paper:\n\nLess artifacts.\nTheir method tends to preserve style better, but content worse.\nCan handle photorealistic style transfer better, so long as the images are similar enough. If no good matches between patches can be found, their method performs worse.\n\n Less artifacts. Their method tends to preserve style better, but content worse. Can handle photorealistic style transfer better, so long as the images are similar enough. If no good matches between patches can be found, their method performs worse. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1601.04589"
    },
    "73": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Accurate_Image_Super-Resolution.md",
        "transcript": "\nWhat\n\nThey describe a model that upscales low resolution images to their high resolution equivalents (\"Single Image Super Resolution\").\nTheir model uses a deeper architecture than previous models and has a residual component.\n\n They describe a model that upscales low resolution images to their high resolution equivalents (\"Single Image Super Resolution\"). Their model uses a deeper architecture than previous models and has a residual component. \nHow\n\nTheir model is a fully convolutional neural network.\nInput of the model: The image to upscale, already upscaled to the desired size (but still blurry).\nOutput of the model: The upscaled image (without the blurriness).\nThey use 20 layers of padded 3x3 convolutions with size 64xHxW with ReLU activations. (No pooling.)\nThey have a residual component, i.e. the model only learns and outputs the change that has to be applied/added to the blurry input image (instead of outputting the full image). That change is applied to the blurry input image before using the loss function on it. (Note that this is a bit different from the currently used \"residual learning\".)\nThey use a MSE between the \"correct\" upscaling and the generated upscaled image (input image + residual).\nThey use SGD starting with a learning rate of 0.1 and decay it 3 times by a factor of 10.\nThey use weight decay of 0.0001.\nDuring training they use a special gradient clipping adapted to the learning rate. Usually gradient clipping restricts the gradient values to [-t, t] (t is a hyperparameter). Their gradient clipping restricts the values to [-t/lr, t/lr] (where lr is the learning rate).\nThey argue that their special gradient clipping allows the use of significantly higher learning rates.\nThey train their model on multiple scales, e.g. 2x, 3x, 4x upscaling. (Not really clear how. They probably feed their upscaled image again into the network or something like that?)\n\n Their model is a fully convolutional neural network. Input of the model: The image to upscale, already upscaled to the desired size (but still blurry). Output of the model: The upscaled image (without the blurriness). They use 20 layers of padded 3x3 convolutions with size 64xHxW with ReLU activations. (No pooling.) They have a residual component, i.e. the model only learns and outputs the change that has to be applied/added to the blurry input image (instead of outputting the full image). That change is applied to the blurry input image before using the loss function on it. (Note that this is a bit different from the currently used \"residual learning\".) They use a MSE between the \"correct\" upscaling and the generated upscaled image (input image + residual). They use SGD starting with a learning rate of 0.1 and decay it 3 times by a factor of 10. They use weight decay of 0.0001. During training they use a special gradient clipping adapted to the learning rate. Usually gradient clipping restricts the gradient values to [-t, t] (t is a hyperparameter). Their gradient clipping restricts the values to [-t/lr, t/lr] (where lr is the learning rate). They argue that their special gradient clipping allows the use of significantly higher learning rates. They train their model on multiple scales, e.g. 2x, 3x, 4x upscaling. (Not really clear how. They probably feed their upscaled image again into the network or something like that?) \nResults\n\nHigher accuracy upscaling than all previous methods.\nCan handle well upscaling factors above 2x.\nResidual network learns significantly faster than non-residual network.\n\n Higher accuracy upscaling than all previous methods. Can handle well upscaling factors above 2x. Residual network learns significantly faster than non-residual network. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1511.04587"
    },
    "74": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Joint_Training_of_a_ConvNet_and_a_PGM_for_HPE.md",
        "transcript": "\nWhat\n\nThey describe a model for human pose estimation, i.e. one that finds the joints (\"skeleton\") of a person in an image.\nThey argue that part of their model resembles a Markov Random Field (but in reality its implemented as just one big neural network).\n\n They describe a model for human pose estimation, i.e. one that finds the joints (\"skeleton\") of a person in an image. They argue that part of their model resembles a Markov Random Field (but in reality its implemented as just one big neural network). \nHow\n\nThey have two components in their network:\n\nPart-Detector:\n\nFinds candidate locations for human joints in an image.\nPretty standard ConvNet. A few convolutional layers with pooling and ReLUs.\nThey use two branches: A fine and a coarse one. Both branches have practically the same architecture (convolutions, pooling etc.). The coarse one however receives the image downscaled by a factor of 2 (half width/height) and upscales it by a factor of 2 at the end of the branch.\nAt the end they merge the results of both branches with more convolutions.\nThe output of this model are 4 heatmaps (one per joint? unclear), each having lower resolution than the original image.\n\n\nSpatial-Model:\n\nTakes the results of the part detector and tries to remove all detections that were false positives.\nThey derive their architecture from a fully connected Markov Random Field which would be solved with one step of belief propagation.\nThey use large convolutions (128x128) to resemble the \"fully connected\" part.\nThey initialize the weights of the convolutions with joint positions gathered from the training set.\nThe convolutions are followed by log(), element-wise additions and exp() to resemble an energy function.\nThe end result are the input heatmaps, but cleaned up.\n\n\n\n\n\n They have two components in their network:\n\nPart-Detector:\n\nFinds candidate locations for human joints in an image.\nPretty standard ConvNet. A few convolutional layers with pooling and ReLUs.\nThey use two branches: A fine and a coarse one. Both branches have practically the same architecture (convolutions, pooling etc.). The coarse one however receives the image downscaled by a factor of 2 (half width/height) and upscales it by a factor of 2 at the end of the branch.\nAt the end they merge the results of both branches with more convolutions.\nThe output of this model are 4 heatmaps (one per joint? unclear), each having lower resolution than the original image.\n\n\nSpatial-Model:\n\nTakes the results of the part detector and tries to remove all detections that were false positives.\nThey derive their architecture from a fully connected Markov Random Field which would be solved with one step of belief propagation.\nThey use large convolutions (128x128) to resemble the \"fully connected\" part.\nThey initialize the weights of the convolutions with joint positions gathered from the training set.\nThe convolutions are followed by log(), element-wise additions and exp() to resemble an energy function.\nThe end result are the input heatmaps, but cleaned up.\n\n\n\n Part-Detector:\n\nFinds candidate locations for human joints in an image.\nPretty standard ConvNet. A few convolutional layers with pooling and ReLUs.\nThey use two branches: A fine and a coarse one. Both branches have practically the same architecture (convolutions, pooling etc.). The coarse one however receives the image downscaled by a factor of 2 (half width/height) and upscales it by a factor of 2 at the end of the branch.\nAt the end they merge the results of both branches with more convolutions.\nThe output of this model are 4 heatmaps (one per joint? unclear), each having lower resolution than the original image.\n\n Finds candidate locations for human joints in an image. Pretty standard ConvNet. A few convolutional layers with pooling and ReLUs. They use two branches: A fine and a coarse one. Both branches have practically the same architecture (convolutions, pooling etc.). The coarse one however receives the image downscaled by a factor of 2 (half width/height) and upscales it by a factor of 2 at the end of the branch. At the end they merge the results of both branches with more convolutions. The output of this model are 4 heatmaps (one per joint? unclear), each having lower resolution than the original image. Spatial-Model:\n\nTakes the results of the part detector and tries to remove all detections that were false positives.\nThey derive their architecture from a fully connected Markov Random Field which would be solved with one step of belief propagation.\nThey use large convolutions (128x128) to resemble the \"fully connected\" part.\nThey initialize the weights of the convolutions with joint positions gathered from the training set.\nThe convolutions are followed by log(), element-wise additions and exp() to resemble an energy function.\nThe end result are the input heatmaps, but cleaned up.\n\n Takes the results of the part detector and tries to remove all detections that were false positives. They derive their architecture from a fully connected Markov Random Field which would be solved with one step of belief propagation. They use large convolutions (128x128) to resemble the \"fully connected\" part. They initialize the weights of the convolutions with joint positions gathered from the training set. The convolutions are followed by log(), element-wise additions and exp() to resemble an energy function. The end result are the input heatmaps, but cleaned up. \nResults\n\nBeats all previous models (with and without spatial model).\nAccuracy seems to be around 90% (with enough (16px) tolerance in pixel distance from ground truth).\nAdding the spatial model adds a few percentage points of accuracy.\nUsing two branches instead of one (in the part detector) adds a bit of accuracy. Adding a third branch adds a tiny bit more.\n\n Beats all previous models (with and without spatial model). Accuracy seems to be around 90% (with enough (16px) tolerance in pixel distance from ground truth). Adding the spatial model adds a few percentage points of accuracy. Using two branches instead of one (in the part detector) adds a bit of accuracy. Adding a third branch adds a tiny bit more. \n(1) Introduction\n\nHuman Pose Estimation (HPE) from RGB images is difficult due to the high dimensionality of the input.\nApproaches:\n\nDeformable-part models: Traditionally based on hand-crafted features.\nDeep-learning based disciminative models: Recently outperformed other models. However, it is hard to incorporate priors (e.g. possible joint- inter-connectivity) into the model.\n\n\nThey combine:\n\nA part-detector (ConvNet, utilizes multi-resolution feature representation with overlapping receptive fields)\nPart-based Spatial-Model (approximates loopy belief propagation)\n\n\nThey backpropagate through the spatial model and then the part-detector.\n\n Human Pose Estimation (HPE) from RGB images is difficult due to the high dimensionality of the input. Approaches:\n\nDeformable-part models: Traditionally based on hand-crafted features.\nDeep-learning based disciminative models: Recently outperformed other models. However, it is hard to incorporate priors (e.g. possible joint- inter-connectivity) into the model.\n\n Deformable-part models: Traditionally based on hand-crafted features. Deep-learning based disciminative models: Recently outperformed other models. However, it is hard to incorporate priors (e.g. possible joint- inter-connectivity) into the model. They combine:\n\nA part-detector (ConvNet, utilizes multi-resolution feature representation with overlapping receptive fields)\nPart-based Spatial-Model (approximates loopy belief propagation)\n\n A part-detector (ConvNet, utilizes multi-resolution feature representation with overlapping receptive fields) Part-based Spatial-Model (approximates loopy belief propagation) They backpropagate through the spatial model and then the part-detector. \n(3) Model\n\n(3.1) Convolutional Network Part-Detector\n\nThis model locates possible positions of human key joints in the image (\"part detector\").\nInput: RGB image.\nOutput: 4 heatmaps, one per key joint (per pixel: likelihood).\nThey use a fully convolutional network.\nThey argue that applying convolutions to every pixel is similar to moving a sliding window over the image.\nThey use two receptive field sizes for their \"sliding window\": A large but coarse/blurry one, a small but fine one.\nTo implement that, they use two branches. Both branches are mostly identical (convolutions, poolings, ReLU). They simply feed a downscaled (half width/height) version of the input image into the coarser branch. At the end they upscale the coarser branch once and then merge both branches.\nAfter the merge they apply 9x9 convolutions and then 1x1 convolutions to get it down to 4xHxW (H=60, W=90 where expected input was H=320, W=240).\n\n\n(3.2) Higher-level Spatial-Model\n\nThis model takes the detected joint positions (heatmaps) and tries to remove those that are probably false positives.\nIt is a ConvNet, which tries to emulate (1) a Markov Random Field and (2) solving that MRF approximately via one step of belief propagation.\nThe raw MRF formula would be something like <likelihood of joint A per px> = normalize( <product over joint v from joints V> <probability of joint A per px given a> * <probability of joint v at px?> + someBiasTerm).\nThey treat the probabilities as energies and remove from the formula the partition function (normalize) for various reasons (e.g. because they are only interested in the maximum value anyways).\nThey use exp() in combination with log() to replace the product with a sum.\nThey apply SoftPlus and ReLU so that the energies are always positive (and therefore play well with log).\nApparently <probability of joint v at px?> are the input heatmaps of the part detector.\nApparently <probability of joint A per px given a> is implemented as the weights of a convolution.\nApparently someBiasTerm is implemented as the bias of a convolution.\nThe convolutions that they use are large (128x128) to emulate a fully connected graph.\nThey initialize the convolution weights based on histograms gathered from the dataset (empirical distribution of joint displacements).\n\n\n(3.3) Unified Models\n\nThey combine the part-based model and the spatial model to a single one.\nThey first train only the part-based model, then only the spatial model, then both.\n\n\n\n (3.1) Convolutional Network Part-Detector\n\nThis model locates possible positions of human key joints in the image (\"part detector\").\nInput: RGB image.\nOutput: 4 heatmaps, one per key joint (per pixel: likelihood).\nThey use a fully convolutional network.\nThey argue that applying convolutions to every pixel is similar to moving a sliding window over the image.\nThey use two receptive field sizes for their \"sliding window\": A large but coarse/blurry one, a small but fine one.\nTo implement that, they use two branches. Both branches are mostly identical (convolutions, poolings, ReLU). They simply feed a downscaled (half width/height) version of the input image into the coarser branch. At the end they upscale the coarser branch once and then merge both branches.\nAfter the merge they apply 9x9 convolutions and then 1x1 convolutions to get it down to 4xHxW (H=60, W=90 where expected input was H=320, W=240).\n\n This model locates possible positions of human key joints in the image (\"part detector\"). Input: RGB image. Output: 4 heatmaps, one per key joint (per pixel: likelihood). They use a fully convolutional network. They argue that applying convolutions to every pixel is similar to moving a sliding window over the image. They use two receptive field sizes for their \"sliding window\": A large but coarse/blurry one, a small but fine one. To implement that, they use two branches. Both branches are mostly identical (convolutions, poolings, ReLU). They simply feed a downscaled (half width/height) version of the input image into the coarser branch. At the end they upscale the coarser branch once and then merge both branches. After the merge they apply 9x9 convolutions and then 1x1 convolutions to get it down to 4xHxW (H=60, W=90 where expected input was H=320, W=240). (3.2) Higher-level Spatial-Model\n\nThis model takes the detected joint positions (heatmaps) and tries to remove those that are probably false positives.\nIt is a ConvNet, which tries to emulate (1) a Markov Random Field and (2) solving that MRF approximately via one step of belief propagation.\nThe raw MRF formula would be something like <likelihood of joint A per px> = normalize( <product over joint v from joints V> <probability of joint A per px given a> * <probability of joint v at px?> + someBiasTerm).\nThey treat the probabilities as energies and remove from the formula the partition function (normalize) for various reasons (e.g. because they are only interested in the maximum value anyways).\nThey use exp() in combination with log() to replace the product with a sum.\nThey apply SoftPlus and ReLU so that the energies are always positive (and therefore play well with log).\nApparently <probability of joint v at px?> are the input heatmaps of the part detector.\nApparently <probability of joint A per px given a> is implemented as the weights of a convolution.\nApparently someBiasTerm is implemented as the bias of a convolution.\nThe convolutions that they use are large (128x128) to emulate a fully connected graph.\nThey initialize the convolution weights based on histograms gathered from the dataset (empirical distribution of joint displacements).\n\n This model takes the detected joint positions (heatmaps) and tries to remove those that are probably false positives. It is a ConvNet, which tries to emulate (1) a Markov Random Field and (2) solving that MRF approximately via one step of belief propagation. The raw MRF formula would be something like <likelihood of joint A per px> = normalize( <product over joint v from joints V> <probability of joint A per px given a> * <probability of joint v at px?> + someBiasTerm). They treat the probabilities as energies and remove from the formula the partition function (normalize) for various reasons (e.g. because they are only interested in the maximum value anyways). They use exp() in combination with log() to replace the product with a sum. They apply SoftPlus and ReLU so that the energies are always positive (and therefore play well with log). Apparently <probability of joint v at px?> are the input heatmaps of the part detector. Apparently <probability of joint A per px given a> is implemented as the weights of a convolution. Apparently someBiasTerm is implemented as the bias of a convolution. The convolutions that they use are large (128x128) to emulate a fully connected graph. They initialize the convolution weights based on histograms gathered from the dataset (empirical distribution of joint displacements). (3.3) Unified Models\n\nThey combine the part-based model and the spatial model to a single one.\nThey first train only the part-based model, then only the spatial model, then both.\n\n They combine the part-based model and the spatial model to a single one. They first train only the part-based model, then only the spatial model, then both. \n(4) Results\n\nUsed datasets: FLIC (4k training images, 1k test, mostly front-facing and standing poses), FLIC-plus (17k, 1k ?), extended-LSP (10k, 1k).\nFLIC contains images showing multiple persons with only one being annotated. So for FLIC they add a heatmap of the annotated body torso to the input (i.e. the part-detector does not have to search for the person any more).\nThe evaluation metric roughly measures, how often predicted joint positions are within a certain radius of the true joint positions.\nTheir model performs significantly better than competing models (on both FLIC and LSP).\nAccuracy seems to be at around 80%-95% per joint (when choosing high enough evaluation tolerance, i.e. 10px+).\nAdding the spatial model to the part detector increases the accuracy by around 10-15 percentage points.\nTraining the part detector and the spatial model jointly adds ~3 percentage points accuracy over training them separately.\nAdding the second filter bank (coarser branch in the part detector) adds around 5 percentage points accuracy. Adding a third filter bank adds a tiny bit more accuracy.\n\n Used datasets: FLIC (4k training images, 1k test, mostly front-facing and standing poses), FLIC-plus (17k, 1k ?), extended-LSP (10k, 1k). FLIC contains images showing multiple persons with only one being annotated. So for FLIC they add a heatmap of the annotated body torso to the input (i.e. the part-detector does not have to search for the person any more). The evaluation metric roughly measures, how often predicted joint positions are within a certain radius of the true joint positions. Their model performs significantly better than competing models (on both FLIC and LSP). Accuracy seems to be at around 80%-95% per joint (when choosing high enough evaluation tolerance, i.e. 10px+). Adding the spatial model to the part detector increases the accuracy by around 10-15 percentage points. Training the part detector and the spatial model jointly adds ~3 percentage points accuracy over training them separately. Adding the second filter bank (coarser branch in the part detector) adds around 5 percentage points accuracy. Adding a third filter bank adds a tiny bit more accuracy. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1406.2984"
    },
    "75": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Hierarchical_Deep_Reinforcement_Learning.md",
        "transcript": "\nWhat\n\nThey present a hierarchical method for reinforcement learning.\nThe method combines \"long\"-term goals with short-term action choices.\n\n They present a hierarchical method for reinforcement learning. The method combines \"long\"-term goals with short-term action choices. \nHow\n\nThey have two components:\n\nMeta-Controller:\n\nResponsible for the \"long\"-term goals.\nIs trained to pick goals (based on the current state) that maximize (extrinsic) rewards, just like you would usually optimize to maximize rewards by picking good actions.\nThe Meta-Controller only picks goals when the Controller terminates or achieved the goal.\n\n\nController:\n\nReceives the current state and the current goal.\nHas to pick a reward maximizing action based on those, just as the agent would usually do (only the goal is added here).\nThe reward is intrinsic. It comes from the Critic. The Critic gives reward whenever the current goal is reached.\n\n\n\n\nFor Montezuma's Revenge:\n\nA goal is to reach a specific object.\nThe goal is encoded via a bitmask (as big as the game screen). The mask contains 1s wherever the object is.\nThey hand-extract the location of a few specific objects.\nSo basically:\n\nThe Meta-Controller picks the next object to reach via a Q-value function.\nIt receives extrinsic reward when objects have been reached in a specific sequence.\nThe Controller picks actions that lead to reaching the object based on a Q-value function. It iterates action-choosing until it terminates or reached the goal-object.\nThe Critic awards intrinsic reward to the Controller whenever the goal-object was reached.\n\n\nThey use CNNs for the Meta-Controller and the Controller, similar in architecture to the Atari-DQN paper (shallow CNNs).\nThey use two replay memories, one for the Meta-Controller (size 40k) and one for the Controller (size 1M).\nBoth follow an epsilon-greedy policy (for picking goals/actions). Epsilon starts at 1.0 and is annealed down to 0.1.\nThey use a discount factor / gamma of 0.9.\nThey train with SGD.\n\n\n\n They have two components:\n\nMeta-Controller:\n\nResponsible for the \"long\"-term goals.\nIs trained to pick goals (based on the current state) that maximize (extrinsic) rewards, just like you would usually optimize to maximize rewards by picking good actions.\nThe Meta-Controller only picks goals when the Controller terminates or achieved the goal.\n\n\nController:\n\nReceives the current state and the current goal.\nHas to pick a reward maximizing action based on those, just as the agent would usually do (only the goal is added here).\nThe reward is intrinsic. It comes from the Critic. The Critic gives reward whenever the current goal is reached.\n\n\n\n Meta-Controller:\n\nResponsible for the \"long\"-term goals.\nIs trained to pick goals (based on the current state) that maximize (extrinsic) rewards, just like you would usually optimize to maximize rewards by picking good actions.\nThe Meta-Controller only picks goals when the Controller terminates or achieved the goal.\n\n Responsible for the \"long\"-term goals. Is trained to pick goals (based on the current state) that maximize (extrinsic) rewards, just like you would usually optimize to maximize rewards by picking good actions. The Meta-Controller only picks goals when the Controller terminates or achieved the goal. Controller:\n\nReceives the current state and the current goal.\nHas to pick a reward maximizing action based on those, just as the agent would usually do (only the goal is added here).\nThe reward is intrinsic. It comes from the Critic. The Critic gives reward whenever the current goal is reached.\n\n Receives the current state and the current goal. Has to pick a reward maximizing action based on those, just as the agent would usually do (only the goal is added here). The reward is intrinsic. It comes from the Critic. The Critic gives reward whenever the current goal is reached. For Montezuma's Revenge:\n\nA goal is to reach a specific object.\nThe goal is encoded via a bitmask (as big as the game screen). The mask contains 1s wherever the object is.\nThey hand-extract the location of a few specific objects.\nSo basically:\n\nThe Meta-Controller picks the next object to reach via a Q-value function.\nIt receives extrinsic reward when objects have been reached in a specific sequence.\nThe Controller picks actions that lead to reaching the object based on a Q-value function. It iterates action-choosing until it terminates or reached the goal-object.\nThe Critic awards intrinsic reward to the Controller whenever the goal-object was reached.\n\n\nThey use CNNs for the Meta-Controller and the Controller, similar in architecture to the Atari-DQN paper (shallow CNNs).\nThey use two replay memories, one for the Meta-Controller (size 40k) and one for the Controller (size 1M).\nBoth follow an epsilon-greedy policy (for picking goals/actions). Epsilon starts at 1.0 and is annealed down to 0.1.\nThey use a discount factor / gamma of 0.9.\nThey train with SGD.\n\n A goal is to reach a specific object. The goal is encoded via a bitmask (as big as the game screen). The mask contains 1s wherever the object is. They hand-extract the location of a few specific objects. So basically:\n\nThe Meta-Controller picks the next object to reach via a Q-value function.\nIt receives extrinsic reward when objects have been reached in a specific sequence.\nThe Controller picks actions that lead to reaching the object based on a Q-value function. It iterates action-choosing until it terminates or reached the goal-object.\nThe Critic awards intrinsic reward to the Controller whenever the goal-object was reached.\n\n The Meta-Controller picks the next object to reach via a Q-value function. It receives extrinsic reward when objects have been reached in a specific sequence. The Controller picks actions that lead to reaching the object based on a Q-value function. It iterates action-choosing until it terminates or reached the goal-object. The Critic awards intrinsic reward to the Controller whenever the goal-object was reached. They use CNNs for the Meta-Controller and the Controller, similar in architecture to the Atari-DQN paper (shallow CNNs). They use two replay memories, one for the Meta-Controller (size 40k) and one for the Controller (size 1M). Both follow an epsilon-greedy policy (for picking goals/actions). Epsilon starts at 1.0 and is annealed down to 0.1. They use a discount factor / gamma of 0.9. They train with SGD. \nResults\n\nLearns to play Montezuma's Revenge.\nLearns to act well in a more abstract MDP with delayed rewards and where simple Q-learning failed.\n\n Learns to play Montezuma's Revenge. Learns to act well in a more abstract MDP with delayed rewards and where simple Q-learning failed. \n(1) Introduction\n\nBasic problem: Learn goal directed behaviour from sparse feedbacks.\nChallenges:\n\nExplore state space efficiently\nCreate multiple levels of spatio-temporal abstractions\n\n\nTheir method: Combines deep reinforcement learning with hierarchical value functions.\nTheir agent is motivated to solve specific intrinsic goals.\nGoals are defined in the space of entities and relations, which constraints the search space.\nThey define their value function as V(s, g) where s is the state and g is a goal.\nFirst, their agent learns to solve intrinsically generated goals. Then it learns to chain these goals together.\nTheir model has two hiearchy levels:\n\nMeta-Controller: Selects the current goal based on the current state.\nController: Takes state s and goal g, then selects a good action based on s and g. The controller operates until g is achieved, then the meta-controller picks the next goal.\n\n\nMeta-Controller gets extrinsic rewards, controller gets intrinsic rewards.\nThey use SGD to optimize the whole system (with respect to reward maximization).\n\n Basic problem: Learn goal directed behaviour from sparse feedbacks. Challenges:\n\nExplore state space efficiently\nCreate multiple levels of spatio-temporal abstractions\n\n Explore state space efficiently Create multiple levels of spatio-temporal abstractions Their method: Combines deep reinforcement learning with hierarchical value functions. Their agent is motivated to solve specific intrinsic goals. Goals are defined in the space of entities and relations, which constraints the search space. They define their value function as V(s, g) where s is the state and g is a goal. First, their agent learns to solve intrinsically generated goals. Then it learns to chain these goals together. Their model has two hiearchy levels:\n\nMeta-Controller: Selects the current goal based on the current state.\nController: Takes state s and goal g, then selects a good action based on s and g. The controller operates until g is achieved, then the meta-controller picks the next goal.\n\n Meta-Controller: Selects the current goal based on the current state. Controller: Takes state s and goal g, then selects a good action based on s and g. The controller operates until g is achieved, then the meta-controller picks the next goal. Meta-Controller gets extrinsic rewards, controller gets intrinsic rewards. They use SGD to optimize the whole system (with respect to reward maximization). \n(3) Model\n\nBasic setting: Action a out of all actions A, state s out of S, transition function T(s,a)->s', reward by state F(s)->R.\nepsilon-greedy is good for local exploration, but it's not good at exploring very different areas of the state space.\nThey use intrinsically motivated goals to better explore the state space.\nSequences of goals are arranged to maximize the received extrinsic reward.\nThe agent learns one policy per goal.\nMeta-Controller: Receives current state, chooses goal.\nController: Receives current state and current goal, chooses action. Keeps choosing actions until goal is achieved or a terminal state is reached. Has the optimization target of maximizing cumulative reward.\nCritic: Checks if current goal is achieved and if so provides intrinsic reward.\nThey use deep Q learning to train their model.\nThere are two Q-value functions. One for the controller and one for the meta-controller.\nBoth formulas are extended by the last chosen goal g.\nThe Q-value function of the meta-controller does not depend on the chosen action.\nThe Q-value function of the controller receives only intrinsic direct reward, not extrinsic direct reward.\nBoth Q-value functions are reprsented with DQNs.\nBoth are optimized to minimize MSE losses.\nThey use separate replay memories for the controller and meta-controller.\nA memory is added for the meta-controller whenever the controller terminates.\nEach new goal is picked by the meta-controller epsilon-greedy (based on the current state).\nThe controller picks actions epsilon-greedy (based on the current state and goal).\nBoth epsilons are annealed down.\n\n Basic setting: Action a out of all actions A, state s out of S, transition function T(s,a)->s', reward by state F(s)->R. epsilon-greedy is good for local exploration, but it's not good at exploring very different areas of the state space. They use intrinsically motivated goals to better explore the state space. Sequences of goals are arranged to maximize the received extrinsic reward. The agent learns one policy per goal. Meta-Controller: Receives current state, chooses goal. Controller: Receives current state and current goal, chooses action. Keeps choosing actions until goal is achieved or a terminal state is reached. Has the optimization target of maximizing cumulative reward. Critic: Checks if current goal is achieved and if so provides intrinsic reward. They use deep Q learning to train their model. There are two Q-value functions. One for the controller and one for the meta-controller. Both formulas are extended by the last chosen goal g. The Q-value function of the meta-controller does not depend on the chosen action. The Q-value function of the controller receives only intrinsic direct reward, not extrinsic direct reward. Both Q-value functions are reprsented with DQNs. Both are optimized to minimize MSE losses. They use separate replay memories for the controller and meta-controller. A memory is added for the meta-controller whenever the controller terminates. Each new goal is picked by the meta-controller epsilon-greedy (based on the current state). The controller picks actions epsilon-greedy (based on the current state and goal). Both epsilons are annealed down. \n(4) Experiments\n\n(4.1) Discrete MDP with delayed rewards\n\nBasic MDP setting, following roughly: Several states (s1 to s6) organized in a chain. The agent can move left or right. It gets high reward if it moves to state s6 and then back to s1, otherwise it gets small reward per reached state.\nThey use their hierarchical method, but without neural nets.\nBaseline is Q-learning without a hierarchy/intrinsic rewards.\nTheir method performs significantly better than the baseline.\n\n\n(4.2) ATARI game with delayed rewards\n\nThey play Montezuma's Revenge with their method, because that game has very delayed rewards.\nThey use CNNs for the controller and meta-controller (architecture similar to the Atari-DQN paper).\nThe critic reacts to (entity1, relation, entity2) relationships. The entities are just objects visible in the game. The relation is (apparently ?) always \"reached\", i.e. whether object1 arrived at object2.\nThey extract the objects manually, i.e. assume the existance of a perfect unsupervised object detector.\nThey encode the goals apparently not as vectors, but instead just use a bitmask (game screen heightand width), which has 1s at the pixels that show the object.\nReplay memory sizes: 1M for controller, 50k for meta-controller.\ngamma=0.99\nThey first only train the controller (i.e. meta-controller completely random) and only then train both jointly.\nTheir method successfully learns to perform actions which lead to rewards with long delays.\nIt starts with easier goals and then learns harder goals.\n\n\n\n (4.1) Discrete MDP with delayed rewards\n\nBasic MDP setting, following roughly: Several states (s1 to s6) organized in a chain. The agent can move left or right. It gets high reward if it moves to state s6 and then back to s1, otherwise it gets small reward per reached state.\nThey use their hierarchical method, but without neural nets.\nBaseline is Q-learning without a hierarchy/intrinsic rewards.\nTheir method performs significantly better than the baseline.\n\n Basic MDP setting, following roughly: Several states (s1 to s6) organized in a chain. The agent can move left or right. It gets high reward if it moves to state s6 and then back to s1, otherwise it gets small reward per reached state. They use their hierarchical method, but without neural nets. Baseline is Q-learning without a hierarchy/intrinsic rewards. Their method performs significantly better than the baseline. (4.2) ATARI game with delayed rewards\n\nThey play Montezuma's Revenge with their method, because that game has very delayed rewards.\nThey use CNNs for the controller and meta-controller (architecture similar to the Atari-DQN paper).\nThe critic reacts to (entity1, relation, entity2) relationships. The entities are just objects visible in the game. The relation is (apparently ?) always \"reached\", i.e. whether object1 arrived at object2.\nThey extract the objects manually, i.e. assume the existance of a perfect unsupervised object detector.\nThey encode the goals apparently not as vectors, but instead just use a bitmask (game screen heightand width), which has 1s at the pixels that show the object.\nReplay memory sizes: 1M for controller, 50k for meta-controller.\ngamma=0.99\nThey first only train the controller (i.e. meta-controller completely random) and only then train both jointly.\nTheir method successfully learns to perform actions which lead to rewards with long delays.\nIt starts with easier goals and then learns harder goals.\n\n They play Montezuma's Revenge with their method, because that game has very delayed rewards. They use CNNs for the controller and meta-controller (architecture similar to the Atari-DQN paper). The critic reacts to (entity1, relation, entity2) relationships. The entities are just objects visible in the game. The relation is (apparently ?) always \"reached\", i.e. whether object1 arrived at object2. They extract the objects manually, i.e. assume the existance of a perfect unsupervised object detector. They encode the goals apparently not as vectors, but instead just use a bitmask (game screen heightand width), which has 1s at the pixels that show the object. Replay memory sizes: 1M for controller, 50k for meta-controller. gamma=0.99 They first only train the controller (i.e. meta-controller completely random) and only then train both jointly. Their method successfully learns to perform actions which lead to rewards with long delays. It starts with easier goals and then learns harder goals. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.06057"
    },
    "76": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Let_there_be_Color.md",
        "transcript": "\nWhat\n\nThey present a model which adds color to grayscale images (e.g. to old black and white images).\nIt works best with 224x224 images, but can handle other sizes too.\n\n They present a model which adds color to grayscale images (e.g. to old black and white images). It works best with 224x224 images, but can handle other sizes too. \nHow\n\nTheir model has three feature extraction components:\n\nLow level features:\n\nReceives 1xHxW images and outputs 512xH/8xW/8 matrices.\nUses 6 convolutional layers (3x3, strided, ReLU) for that.\n\n\nGlobal features:\n\nReceives the low level features and converts them to 256 dimensional vectors.\nUses 4 convolutional layers (3x3, strided, ReLU) and 3 fully connected layers (1024 -> 512 -> 256; ReLU) for that.\n\n\nMid-level features:\n\nReceives the low level features and converts them to 256xH/8xW/8 matrices.\nUses 2 convolutional layers (3x3, ReLU) for that.\n\n\n\n\nThe global and mid-level features are then merged with a Fusion Layer.\n\nThe Fusion Layer is basically an extended convolutional layer.\nIt takes the mid-level features (256xH/8xW/8) and the global features (256) as input and outputs a matrix of shape 256xH/8xW/8.\nIt mostly operates like a normal convolutional layer on the mid-level features. However, its weight matrix is extended to also include weights for the global features (which will be added at every pixel).\nSo they use something like fusion at pixel u,v = sigmoid(bias + weights * [global features, mid-level features at pixel u,v]) - and that with 256 different weight matrices and biases for 256 filters.\n\n\nAfter the Fusion Layer they use another network to create the coloring:\n\nThis network receives 256xH/8xW/8 matrices (merge of global and mid-level features) and generates 2xHxW outputs (color in L*a*b* color space).\nIt uses a few convolutional layers combined with layers that do nearest neighbour upsampling.\n\n\nThe loss for the colorization network is a MSE based on the true coloring.\nThey train the global feature extraction also on the true class labels of the used images.\nTheir model can handle any sized image. If the image doesn't have a size of 224x224, it must be resized to 224x224 for the gobal feature extraction. The mid-level feature extraction only uses convolutions, therefore it can work with any image size.\n\n Their model has three feature extraction components:\n\nLow level features:\n\nReceives 1xHxW images and outputs 512xH/8xW/8 matrices.\nUses 6 convolutional layers (3x3, strided, ReLU) for that.\n\n\nGlobal features:\n\nReceives the low level features and converts them to 256 dimensional vectors.\nUses 4 convolutional layers (3x3, strided, ReLU) and 3 fully connected layers (1024 -> 512 -> 256; ReLU) for that.\n\n\nMid-level features:\n\nReceives the low level features and converts them to 256xH/8xW/8 matrices.\nUses 2 convolutional layers (3x3, ReLU) for that.\n\n\n\n Low level features:\n\nReceives 1xHxW images and outputs 512xH/8xW/8 matrices.\nUses 6 convolutional layers (3x3, strided, ReLU) for that.\n\n Receives 1xHxW images and outputs 512xH/8xW/8 matrices. Uses 6 convolutional layers (3x3, strided, ReLU) for that. Global features:\n\nReceives the low level features and converts them to 256 dimensional vectors.\nUses 4 convolutional layers (3x3, strided, ReLU) and 3 fully connected layers (1024 -> 512 -> 256; ReLU) for that.\n\n Receives the low level features and converts them to 256 dimensional vectors. Uses 4 convolutional layers (3x3, strided, ReLU) and 3 fully connected layers (1024 -> 512 -> 256; ReLU) for that. Mid-level features:\n\nReceives the low level features and converts them to 256xH/8xW/8 matrices.\nUses 2 convolutional layers (3x3, ReLU) for that.\n\n Receives the low level features and converts them to 256xH/8xW/8 matrices. Uses 2 convolutional layers (3x3, ReLU) for that. The global and mid-level features are then merged with a Fusion Layer.\n\nThe Fusion Layer is basically an extended convolutional layer.\nIt takes the mid-level features (256xH/8xW/8) and the global features (256) as input and outputs a matrix of shape 256xH/8xW/8.\nIt mostly operates like a normal convolutional layer on the mid-level features. However, its weight matrix is extended to also include weights for the global features (which will be added at every pixel).\nSo they use something like fusion at pixel u,v = sigmoid(bias + weights * [global features, mid-level features at pixel u,v]) - and that with 256 different weight matrices and biases for 256 filters.\n\n The Fusion Layer is basically an extended convolutional layer. It takes the mid-level features (256xH/8xW/8) and the global features (256) as input and outputs a matrix of shape 256xH/8xW/8. It mostly operates like a normal convolutional layer on the mid-level features. However, its weight matrix is extended to also include weights for the global features (which will be added at every pixel). So they use something like fusion at pixel u,v = sigmoid(bias + weights * [global features, mid-level features at pixel u,v]) - and that with 256 different weight matrices and biases for 256 filters. After the Fusion Layer they use another network to create the coloring:\n\nThis network receives 256xH/8xW/8 matrices (merge of global and mid-level features) and generates 2xHxW outputs (color in L*a*b* color space).\nIt uses a few convolutional layers combined with layers that do nearest neighbour upsampling.\n\n This network receives 256xH/8xW/8 matrices (merge of global and mid-level features) and generates 2xHxW outputs (color in L*a*b* color space). It uses a few convolutional layers combined with layers that do nearest neighbour upsampling. The loss for the colorization network is a MSE based on the true coloring. They train the global feature extraction also on the true class labels of the used images. Their model can handle any sized image. If the image doesn't have a size of 224x224, it must be resized to 224x224 for the gobal feature extraction. The mid-level feature extraction only uses convolutions, therefore it can work with any image size. \nResults\n\nThe training set that they use is the \"Places scene dataset\".\nAfter cleanup the dataset contains 2.3M training images (205 different classes) and 19k validation images.\nUsers rate images colored by their method in 92.6% of all cases as real-looking (ground truth: 97.2%).\nIf they exclude global features from their method, they only achieve 70% real-looking images.\nThey can also extract the global features from image A and then use them on image B. That transfers the style from A to B. But it only works well on semantically similar images.\n\n The training set that they use is the \"Places scene dataset\". After cleanup the dataset contains 2.3M training images (205 different classes) and 19k validation images. Users rate images colored by their method in 92.6% of all cases as real-looking (ground truth: 97.2%). If they exclude global features from their method, they only achieve 70% real-looking images. They can also extract the global features from image A and then use them on image B. That transfers the style from A to B. But it only works well on semantically similar images. \n(1) Introduction\n\nThey use a CNN to color images.\nTheir network extracts global priors and local features from grayscale images.\nGlobal priors:\n\nExtracted from the whole image (e.g. time of day, indoor or outdoors, ...).\nThey use class labels of images to train those. (Not needed during test.)\n\n\nLocal features: Extracted from small patches (e.g. texture).\nThey don't generate a full RGB image, instead they generate the chrominance map using the CIE L*a*b* colorspace.\nComponents of the model:\n\nLow level features network: Generated first.\nMid level features network: Generated based on the low level features.\nGlobal features network: Generated based on the low level features.\nColorization network: Receives mid level and global features, which were merged in a fusion layer.\n\n\nTheir network can process images of arbitrary size.\nGlobal features can be generated based on another image to change the style of colorization, e.g. to change the seasonal colors from spring to summer.\n\n They use a CNN to color images. Their network extracts global priors and local features from grayscale images. Global priors:\n\nExtracted from the whole image (e.g. time of day, indoor or outdoors, ...).\nThey use class labels of images to train those. (Not needed during test.)\n\n Extracted from the whole image (e.g. time of day, indoor or outdoors, ...). They use class labels of images to train those. (Not needed during test.) Local features: Extracted from small patches (e.g. texture). They don't generate a full RGB image, instead they generate the chrominance map using the CIE L*a*b* colorspace. Components of the model:\n\nLow level features network: Generated first.\nMid level features network: Generated based on the low level features.\nGlobal features network: Generated based on the low level features.\nColorization network: Receives mid level and global features, which were merged in a fusion layer.\n\n Low level features network: Generated first. Mid level features network: Generated based on the low level features. Global features network: Generated based on the low level features. Colorization network: Receives mid level and global features, which were merged in a fusion layer. Their network can process images of arbitrary size. Global features can be generated based on another image to change the style of colorization, e.g. to change the seasonal colors from spring to summer. \n(3) Joint Global and Local Model\n\n\n\nThey mostly use ReLUs.\n(3.1) Deep Networks\n\n\n\n\n\n(3.2) Fusing Global and Local Features for Colorization\n\nGlobal features are used as priors for local features.\n(3.2.1) Shared Low-Level Features\n\nThe low level features are which's (low level) features are fed into the networks of both the global and the medium level features extractors.\nThey generate them from the input image using a ConvNet with 6 layers (3x3, 1x1 padding, strided/no pooling, ends in 512xH/8xW/8).\n\n\n(3.2.2) Global Image Features\n\nThey process the low level features via another network into global features.\nThat network has 4 conv-layers (3x3, 2 strided layers, all 512 filters), followed by 3 fully connected layers (1024, 512, 256).\nInput size (of low level features) is expected to be 224x224.\n\n\n(3.2.3) Mid-Level Features\n\nTakes the low level features (512xH/8xW/8) and uses 2 conv layers (3x3) to transform them to 256xH/8xW/8.\n\n\n(3.2.4) Fusing Global and Local Features\n\nThe Fusion Layer is basically an extended convolutional layer.\nIt takes the mid-level features (256xH/8xW/8) and the global features (256) as input and outputs a matrix of shape 256xH/8xW/8.\nIt mostly operates like a normal convolutional layer on the mid-level features. However, its weight matrix is extended to also include weights for the global features (which will be added at every pixel).\nSo they use something like fusion at pixel u,v = sigmoid(bias + weights * [global features, mid-level features at pixel u,v]) - and that with 256 different weight matrices and biases for 256 filters.\n\n\n(3.2.5) Colorization Network\n\nThe colorization network receives the 256xH/8xW/8 matrix from the fusion layer and transforms it to the 2xHxW chrominance map.\nIt basically uses two upsampling blocks, each starting with a nearest neighbour upsampling layer, followed by 2 3x3 convs.\nThe last layer uses a sigmoid activation.\nThe network ends in a MSE.\n\n\n\n\n(3.3) Colorization with Classification\n\nTo make training more effective, they train parts of the global features network via image class labels.\nI.e. they take the output of the 2nd fully connected layer (at the end of the global network), add one small hidden layer after it, followed by a sigmoid output layer (size equals number of class labels).\nThey train that with cross entropy. So their global loss becomes something like L = MSE(color accuracy) + alpha*CrossEntropy(class labels accuracy).\n\n\n(3.4) Optimization and Learning\n\nLow level feature extraction uses only convs, so they can be extracted from any image size.\nGlobal feature extraction uses fc layers, so they can only be extracted from 224x224 images.\nIf an image has a size unequal to 224x224, it must be (1) resized to 224x224, fed through low level feature extraction, then fed through the global feature extraction and (2) separately (without resize) fed through the low level feature extraction and then fed through the mid-level feature extraction.\nHowever, they only trained on 224x224 images (for efficiency).\nAugmentation: 224x224 crops from 256x256 images; random horizontal flips.\nThey use Adadelta, because they don't want to set learning rates. (Why not adagrad/adam/...?)\n\n\n\n \n They mostly use ReLUs. (3.1) Deep Networks\n\n\n\n\n \n (3.2) Fusing Global and Local Features for Colorization\n\nGlobal features are used as priors for local features.\n(3.2.1) Shared Low-Level Features\n\nThe low level features are which's (low level) features are fed into the networks of both the global and the medium level features extractors.\nThey generate them from the input image using a ConvNet with 6 layers (3x3, 1x1 padding, strided/no pooling, ends in 512xH/8xW/8).\n\n\n(3.2.2) Global Image Features\n\nThey process the low level features via another network into global features.\nThat network has 4 conv-layers (3x3, 2 strided layers, all 512 filters), followed by 3 fully connected layers (1024, 512, 256).\nInput size (of low level features) is expected to be 224x224.\n\n\n(3.2.3) Mid-Level Features\n\nTakes the low level features (512xH/8xW/8) and uses 2 conv layers (3x3) to transform them to 256xH/8xW/8.\n\n\n(3.2.4) Fusing Global and Local Features\n\nThe Fusion Layer is basically an extended convolutional layer.\nIt takes the mid-level features (256xH/8xW/8) and the global features (256) as input and outputs a matrix of shape 256xH/8xW/8.\nIt mostly operates like a normal convolutional layer on the mid-level features. However, its weight matrix is extended to also include weights for the global features (which will be added at every pixel).\nSo they use something like fusion at pixel u,v = sigmoid(bias + weights * [global features, mid-level features at pixel u,v]) - and that with 256 different weight matrices and biases for 256 filters.\n\n\n(3.2.5) Colorization Network\n\nThe colorization network receives the 256xH/8xW/8 matrix from the fusion layer and transforms it to the 2xHxW chrominance map.\nIt basically uses two upsampling blocks, each starting with a nearest neighbour upsampling layer, followed by 2 3x3 convs.\nThe last layer uses a sigmoid activation.\nThe network ends in a MSE.\n\n\n\n Global features are used as priors for local features. (3.2.1) Shared Low-Level Features\n\nThe low level features are which's (low level) features are fed into the networks of both the global and the medium level features extractors.\nThey generate them from the input image using a ConvNet with 6 layers (3x3, 1x1 padding, strided/no pooling, ends in 512xH/8xW/8).\n\n The low level features are which's (low level) features are fed into the networks of both the global and the medium level features extractors. They generate them from the input image using a ConvNet with 6 layers (3x3, 1x1 padding, strided/no pooling, ends in 512xH/8xW/8). (3.2.2) Global Image Features\n\nThey process the low level features via another network into global features.\nThat network has 4 conv-layers (3x3, 2 strided layers, all 512 filters), followed by 3 fully connected layers (1024, 512, 256).\nInput size (of low level features) is expected to be 224x224.\n\n They process the low level features via another network into global features. That network has 4 conv-layers (3x3, 2 strided layers, all 512 filters), followed by 3 fully connected layers (1024, 512, 256). Input size (of low level features) is expected to be 224x224. (3.2.3) Mid-Level Features\n\nTakes the low level features (512xH/8xW/8) and uses 2 conv layers (3x3) to transform them to 256xH/8xW/8.\n\n Takes the low level features (512xH/8xW/8) and uses 2 conv layers (3x3) to transform them to 256xH/8xW/8. (3.2.4) Fusing Global and Local Features\n\nThe Fusion Layer is basically an extended convolutional layer.\nIt takes the mid-level features (256xH/8xW/8) and the global features (256) as input and outputs a matrix of shape 256xH/8xW/8.\nIt mostly operates like a normal convolutional layer on the mid-level features. However, its weight matrix is extended to also include weights for the global features (which will be added at every pixel).\nSo they use something like fusion at pixel u,v = sigmoid(bias + weights * [global features, mid-level features at pixel u,v]) - and that with 256 different weight matrices and biases for 256 filters.\n\n The Fusion Layer is basically an extended convolutional layer. It takes the mid-level features (256xH/8xW/8) and the global features (256) as input and outputs a matrix of shape 256xH/8xW/8. It mostly operates like a normal convolutional layer on the mid-level features. However, its weight matrix is extended to also include weights for the global features (which will be added at every pixel). So they use something like fusion at pixel u,v = sigmoid(bias + weights * [global features, mid-level features at pixel u,v]) - and that with 256 different weight matrices and biases for 256 filters. (3.2.5) Colorization Network\n\nThe colorization network receives the 256xH/8xW/8 matrix from the fusion layer and transforms it to the 2xHxW chrominance map.\nIt basically uses two upsampling blocks, each starting with a nearest neighbour upsampling layer, followed by 2 3x3 convs.\nThe last layer uses a sigmoid activation.\nThe network ends in a MSE.\n\n The colorization network receives the 256xH/8xW/8 matrix from the fusion layer and transforms it to the 2xHxW chrominance map. It basically uses two upsampling blocks, each starting with a nearest neighbour upsampling layer, followed by 2 3x3 convs. The last layer uses a sigmoid activation. The network ends in a MSE. (3.3) Colorization with Classification\n\nTo make training more effective, they train parts of the global features network via image class labels.\nI.e. they take the output of the 2nd fully connected layer (at the end of the global network), add one small hidden layer after it, followed by a sigmoid output layer (size equals number of class labels).\nThey train that with cross entropy. So their global loss becomes something like L = MSE(color accuracy) + alpha*CrossEntropy(class labels accuracy).\n\n To make training more effective, they train parts of the global features network via image class labels. I.e. they take the output of the 2nd fully connected layer (at the end of the global network), add one small hidden layer after it, followed by a sigmoid output layer (size equals number of class labels). They train that with cross entropy. So their global loss becomes something like L = MSE(color accuracy) + alpha*CrossEntropy(class labels accuracy). (3.4) Optimization and Learning\n\nLow level feature extraction uses only convs, so they can be extracted from any image size.\nGlobal feature extraction uses fc layers, so they can only be extracted from 224x224 images.\nIf an image has a size unequal to 224x224, it must be (1) resized to 224x224, fed through low level feature extraction, then fed through the global feature extraction and (2) separately (without resize) fed through the low level feature extraction and then fed through the mid-level feature extraction.\nHowever, they only trained on 224x224 images (for efficiency).\nAugmentation: 224x224 crops from 256x256 images; random horizontal flips.\nThey use Adadelta, because they don't want to set learning rates. (Why not adagrad/adam/...?)\n\n Low level feature extraction uses only convs, so they can be extracted from any image size. Global feature extraction uses fc layers, so they can only be extracted from 224x224 images. If an image has a size unequal to 224x224, it must be (1) resized to 224x224, fed through low level feature extraction, then fed through the global feature extraction and (2) separately (without resize) fed through the low level feature extraction and then fed through the mid-level feature extraction. However, they only trained on 224x224 images (for efficiency). Augmentation: 224x224 crops from 256x256 images; random horizontal flips. They use Adadelta, because they don't want to set learning rates. (Why not adagrad/adam/...?) \n(4) Experimental Results and Discussion\n\nThey set the alpha in their loss to 1/300.\nThey use the \"Places scene dataset\". They filter images with low color variance (including grayscale images). They end up with 2.3M training images and 19k validation images. They have 205 classes.\nBatch size: 128.\nThey train for about 11 epochs.\n(4.1) Colorization results\n\nGood looking colorization results on the Places scene dataset.\n\n\n(4.2) Comparison with State of the Art\n\nTheir method succeeds where other methods fail.\nTheir method can handle very different kinds of images.\n\n\n(4.3) User study\n\nWhen rated by users, 92.6% think that their coloring is real (ground truth: 97.2%).\nNote: Users were told to only look briefly at the images.\n\n\n(4.4) Importance of Global Features\n\nTheir model without global features only achieves 70% user rating.\nThere are too many ambiguities on the local level.\n\n\n(4.5) Style Transfer through Global Features\n\nThey can perform style transfer by extracting the global features of image B and using them for image A.\n\n\n(4.6) Colorizing the past\n\nTheir model performs well on old images despite the artifacts commonly found on those.\n\n\n(4.7) Classification Results\n\nTheir method achieves nearly as high classification accuracy as VGG (see classification loss for global features).\n\n\n(4.8) Comparison of Color Spaces\n\nL*a*b* color space performs slightly better than RGB and YUV, so they picked that color space.\n\n\n(4.9) Computation Time\n\nOne image is usually processed within seconds.\nCPU takes roughly 5x longer.\n\n\n(4.10) Limitations and Discussion\n\nTheir approach is data driven, i.e. can only deal well with types of images that appeared in the dataset.\nStyle transfer works only really well for semantically similar images.\nStyle transfer cannot necessarily transfer specific colors, because the whole model only sees the grayscale version of the image.\nTheir model tends to strongly prefer the most common color for objects (e.g. grass always green).\n\n\n\n They set the alpha in their loss to 1/300. They use the \"Places scene dataset\". They filter images with low color variance (including grayscale images). They end up with 2.3M training images and 19k validation images. They have 205 classes. Batch size: 128. They train for about 11 epochs. (4.1) Colorization results\n\nGood looking colorization results on the Places scene dataset.\n\n Good looking colorization results on the Places scene dataset. (4.2) Comparison with State of the Art\n\nTheir method succeeds where other methods fail.\nTheir method can handle very different kinds of images.\n\n Their method succeeds where other methods fail. Their method can handle very different kinds of images. (4.3) User study\n\nWhen rated by users, 92.6% think that their coloring is real (ground truth: 97.2%).\nNote: Users were told to only look briefly at the images.\n\n When rated by users, 92.6% think that their coloring is real (ground truth: 97.2%). Note: Users were told to only look briefly at the images. (4.4) Importance of Global Features\n\nTheir model without global features only achieves 70% user rating.\nThere are too many ambiguities on the local level.\n\n Their model without global features only achieves 70% user rating. There are too many ambiguities on the local level. (4.5) Style Transfer through Global Features\n\nThey can perform style transfer by extracting the global features of image B and using them for image A.\n\n They can perform style transfer by extracting the global features of image B and using them for image A. (4.6) Colorizing the past\n\nTheir model performs well on old images despite the artifacts commonly found on those.\n\n Their model performs well on old images despite the artifacts commonly found on those. (4.7) Classification Results\n\nTheir method achieves nearly as high classification accuracy as VGG (see classification loss for global features).\n\n Their method achieves nearly as high classification accuracy as VGG (see classification loss for global features). (4.8) Comparison of Color Spaces\n\nL*a*b* color space performs slightly better than RGB and YUV, so they picked that color space.\n\n L*a*b* color space performs slightly better than RGB and YUV, so they picked that color space. (4.9) Computation Time\n\nOne image is usually processed within seconds.\nCPU takes roughly 5x longer.\n\n One image is usually processed within seconds. CPU takes roughly 5x longer. (4.10) Limitations and Discussion\n\nTheir approach is data driven, i.e. can only deal well with types of images that appeared in the dataset.\nStyle transfer works only really well for semantically similar images.\nStyle transfer cannot necessarily transfer specific colors, because the whole model only sees the grayscale version of the image.\nTheir model tends to strongly prefer the most common color for objects (e.g. grass always green).\n\n Their approach is data driven, i.e. can only deal well with types of images that appeared in the dataset. Style transfer works only really well for semantically similar images. Style transfer cannot necessarily transfer specific colors, because the whole model only sees the grayscale version of the image. Their model tends to strongly prefer the most common color for objects (e.g. grass always green). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/"
    },
    "77": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Artistic_Style_Transfer_for_Videos.md",
        "transcript": "\nWhat\n\nThe paper describes a method to transfer the style (e.g. choice of colors, structure of brush strokes) of an image to a whole video.\nThe method is designed so that the transfered style is consistent over many frames.\nExamples for such consistency:\n\nNo flickering of style between frames. So the next frame has always roughly the same style in the same locations.\nNo artefacts at the boundaries of objects, even if they are moving.\nIf an area gets occluded and then unoccluded a few frames later, the style of that area is still the same as before the occlusion.\n\n\n\n The paper describes a method to transfer the style (e.g. choice of colors, structure of brush strokes) of an image to a whole video. The method is designed so that the transfered style is consistent over many frames. Examples for such consistency:\n\nNo flickering of style between frames. So the next frame has always roughly the same style in the same locations.\nNo artefacts at the boundaries of objects, even if they are moving.\nIf an area gets occluded and then unoccluded a few frames later, the style of that area is still the same as before the occlusion.\n\n No flickering of style between frames. So the next frame has always roughly the same style in the same locations. No artefacts at the boundaries of objects, even if they are moving. If an area gets occluded and then unoccluded a few frames later, the style of that area is still the same as before the occlusion. \nHow\n\nAssume that we have a frame to stylize x and an image from which to extract the style a.\nThe basic process is the same as in the original Artistic Style Transfer paper, they just add a bit on top of that.\nThey start with a gaussian noise image x' and change it gradually so that a loss function gets minimized.\nThe loss function has the following components:\n\nContent loss (old, same as in the Artistic Style Transfer paper)\n\nThis loss makes sure that the content in the generated/stylized image still matches the content of the original image.\nx and x' are fed forward through a pretrained network (VGG in their case).\nThen the generated representations of the intermediate layers of the network are extracted/read.\nOne or more layers are picked and the difference between those layers for x and x' is measured via a MSE.\nE.g. if we used only the representations of the layer conv5 then we would get something like (conv5(x) - conv5(x'))^2 per example. (Where conv5() also executes all previous layers.)\n\n\nStyle loss (old)\n\nThis loss makes sure that the style of the generated/stylized image matches the style source a.\nx' and a are fed forward through a pretrained network (VGG in their case).\nThen the generated representations of the intermediate layers of the network are extracted/read.\nOne or more layers are picked and the Gram Matrices of those layers are calculated.\nThen the difference between those matrices is measured via a MSE.\n\n\nTemporal loss (new)\n\nThis loss enforces consistency in style between a pair of frames.\nThe main sources of inconsistency are boundaries of moving objects and areas that get unonccluded.\nThey use the optical flow to detect motion.\n\nApplying an optical flow method to two frames (i, i+1) returns per pixel the movement of that pixel, i.e. if the pixel at (x=1, y=2) moved to (x=2, y=4) the optical flow at that pixel would be (u=1, v=2).\nThe optical flow can be split into the forward flow (here fw) and the backward flow (here bw). The forward flow is the flow from frame i to i+1 (as described in the previous point). The backward flow is the flow from frame i+1 to i (reverse direction in time).\n\n\nBoundaries\n\nAt boundaries of objects the derivative of the flow is high, i.e. the flow \"suddenly\" changes significantly from one pixel to the other.\nSo to detect boundaries they use (per pixel) roughly the equation gradient(u)^2 + gradient(v)^2 > length((u,v)).\n\n\nOcclusions and disocclusions\n\nIf a pixel does not get occluded/disoccluded between frames, the optical flow method should be able to correctly estimate the motion of that pixel between the frames. The forward and backward flows then should be roughly equal, just in opposing directions.\nIf a pixel does get occluded/disoccluded between frames, it will not be visible in one the two frames and therefore the optical flow method cannot reliably estimate the motion for that pixel. It is then expected that the forward and backward flow are unequal.\nTo measure that effect they roughly use (per pixel) a formula matching length(fw + bw)^2 > length(fw)^2 + length(bw)^2.\n\n\nMask c\n\nThey create a mask c with the size of the frame.\nFor every pixel they estimate whether the boundary-equation or the disocclusion-equation is true.\nIf either of them is true, they add a 0 to the mask, otherwise a 1. So the mask is 1 wherever there is no disocclusion or motion boundary.\n\n\nCombination\n\nThe final temporal loss is the mean (over all pixels) of c*(x-w)^2.\n\nx is the frame to stylize.\nw is the previous stylized frame (frame i-1), warped according to the optical flow between frame i-1 and i.\nc is the mask value at the pixel.\n\n\nBy using the difference x-w they ensure that the difference in styles between two frames is low.\nBy adding c they ensure the style-consistency only at pixels that probably should have a consistent style.\n\n\n\n\nLong-term loss (new)\n\nThis loss enforces consistency in style between pairs of frames that are longer apart from each other.\nIt is a simple extension of the temporal (short-term) loss.\nThe temporal loss was computed for frames (i-1, i). The long-term loss is the sum of the temporal losses for the frame pairs {(i-4,i), (i-2,i), (i-1,i)}.\nThe c mask is recomputed for every pair and 1 if there are no boundaries/disocclusions detected, but only if there is not a 1 for the same pixel in a later mask. The additional condition is intended to associate pixels with their closest neighbours in time to minimize possible errors.\nNote that the long-term loss can completely replace the temporal loss as the latter one is contained in the former one.\n\n\n\n\nMulti-pass approach (new)\n\nThey had problems with contrast around the boundaries of the frames.\nTo combat that, they use a multi-pass method in which they seem to calculate the optical flow in multiple forward and backward passes? (Not very clear here what they do and why it would help.)\n\n\nInitialization with previous frame (new)\n\nInstead of starting at a gaussian noise image every time, they instead use the previous stylized frame.\nThat immediately leads to more similarity between the frames.\n\n\n\n Assume that we have a frame to stylize x and an image from which to extract the style a. The basic process is the same as in the original Artistic Style Transfer paper, they just add a bit on top of that. They start with a gaussian noise image x' and change it gradually so that a loss function gets minimized. The loss function has the following components:\n\nContent loss (old, same as in the Artistic Style Transfer paper)\n\nThis loss makes sure that the content in the generated/stylized image still matches the content of the original image.\nx and x' are fed forward through a pretrained network (VGG in their case).\nThen the generated representations of the intermediate layers of the network are extracted/read.\nOne or more layers are picked and the difference between those layers for x and x' is measured via a MSE.\nE.g. if we used only the representations of the layer conv5 then we would get something like (conv5(x) - conv5(x'))^2 per example. (Where conv5() also executes all previous layers.)\n\n\nStyle loss (old)\n\nThis loss makes sure that the style of the generated/stylized image matches the style source a.\nx' and a are fed forward through a pretrained network (VGG in their case).\nThen the generated representations of the intermediate layers of the network are extracted/read.\nOne or more layers are picked and the Gram Matrices of those layers are calculated.\nThen the difference between those matrices is measured via a MSE.\n\n\nTemporal loss (new)\n\nThis loss enforces consistency in style between a pair of frames.\nThe main sources of inconsistency are boundaries of moving objects and areas that get unonccluded.\nThey use the optical flow to detect motion.\n\nApplying an optical flow method to two frames (i, i+1) returns per pixel the movement of that pixel, i.e. if the pixel at (x=1, y=2) moved to (x=2, y=4) the optical flow at that pixel would be (u=1, v=2).\nThe optical flow can be split into the forward flow (here fw) and the backward flow (here bw). The forward flow is the flow from frame i to i+1 (as described in the previous point). The backward flow is the flow from frame i+1 to i (reverse direction in time).\n\n\nBoundaries\n\nAt boundaries of objects the derivative of the flow is high, i.e. the flow \"suddenly\" changes significantly from one pixel to the other.\nSo to detect boundaries they use (per pixel) roughly the equation gradient(u)^2 + gradient(v)^2 > length((u,v)).\n\n\nOcclusions and disocclusions\n\nIf a pixel does not get occluded/disoccluded between frames, the optical flow method should be able to correctly estimate the motion of that pixel between the frames. The forward and backward flows then should be roughly equal, just in opposing directions.\nIf a pixel does get occluded/disoccluded between frames, it will not be visible in one the two frames and therefore the optical flow method cannot reliably estimate the motion for that pixel. It is then expected that the forward and backward flow are unequal.\nTo measure that effect they roughly use (per pixel) a formula matching length(fw + bw)^2 > length(fw)^2 + length(bw)^2.\n\n\nMask c\n\nThey create a mask c with the size of the frame.\nFor every pixel they estimate whether the boundary-equation or the disocclusion-equation is true.\nIf either of them is true, they add a 0 to the mask, otherwise a 1. So the mask is 1 wherever there is no disocclusion or motion boundary.\n\n\nCombination\n\nThe final temporal loss is the mean (over all pixels) of c*(x-w)^2.\n\nx is the frame to stylize.\nw is the previous stylized frame (frame i-1), warped according to the optical flow between frame i-1 and i.\nc is the mask value at the pixel.\n\n\nBy using the difference x-w they ensure that the difference in styles between two frames is low.\nBy adding c they ensure the style-consistency only at pixels that probably should have a consistent style.\n\n\n\n\nLong-term loss (new)\n\nThis loss enforces consistency in style between pairs of frames that are longer apart from each other.\nIt is a simple extension of the temporal (short-term) loss.\nThe temporal loss was computed for frames (i-1, i). The long-term loss is the sum of the temporal losses for the frame pairs {(i-4,i), (i-2,i), (i-1,i)}.\nThe c mask is recomputed for every pair and 1 if there are no boundaries/disocclusions detected, but only if there is not a 1 for the same pixel in a later mask. The additional condition is intended to associate pixels with their closest neighbours in time to minimize possible errors.\nNote that the long-term loss can completely replace the temporal loss as the latter one is contained in the former one.\n\n\n\n Content loss (old, same as in the Artistic Style Transfer paper)\n\nThis loss makes sure that the content in the generated/stylized image still matches the content of the original image.\nx and x' are fed forward through a pretrained network (VGG in their case).\nThen the generated representations of the intermediate layers of the network are extracted/read.\nOne or more layers are picked and the difference between those layers for x and x' is measured via a MSE.\nE.g. if we used only the representations of the layer conv5 then we would get something like (conv5(x) - conv5(x'))^2 per example. (Where conv5() also executes all previous layers.)\n\n This loss makes sure that the content in the generated/stylized image still matches the content of the original image. x and x' are fed forward through a pretrained network (VGG in their case). Then the generated representations of the intermediate layers of the network are extracted/read. One or more layers are picked and the difference between those layers for x and x' is measured via a MSE. E.g. if we used only the representations of the layer conv5 then we would get something like (conv5(x) - conv5(x'))^2 per example. (Where conv5() also executes all previous layers.) Style loss (old)\n\nThis loss makes sure that the style of the generated/stylized image matches the style source a.\nx' and a are fed forward through a pretrained network (VGG in their case).\nThen the generated representations of the intermediate layers of the network are extracted/read.\nOne or more layers are picked and the Gram Matrices of those layers are calculated.\nThen the difference between those matrices is measured via a MSE.\n\n This loss makes sure that the style of the generated/stylized image matches the style source a. x' and a are fed forward through a pretrained network (VGG in their case). Then the generated representations of the intermediate layers of the network are extracted/read. One or more layers are picked and the Gram Matrices of those layers are calculated. Then the difference between those matrices is measured via a MSE. Temporal loss (new)\n\nThis loss enforces consistency in style between a pair of frames.\nThe main sources of inconsistency are boundaries of moving objects and areas that get unonccluded.\nThey use the optical flow to detect motion.\n\nApplying an optical flow method to two frames (i, i+1) returns per pixel the movement of that pixel, i.e. if the pixel at (x=1, y=2) moved to (x=2, y=4) the optical flow at that pixel would be (u=1, v=2).\nThe optical flow can be split into the forward flow (here fw) and the backward flow (here bw). The forward flow is the flow from frame i to i+1 (as described in the previous point). The backward flow is the flow from frame i+1 to i (reverse direction in time).\n\n\nBoundaries\n\nAt boundaries of objects the derivative of the flow is high, i.e. the flow \"suddenly\" changes significantly from one pixel to the other.\nSo to detect boundaries they use (per pixel) roughly the equation gradient(u)^2 + gradient(v)^2 > length((u,v)).\n\n\nOcclusions and disocclusions\n\nIf a pixel does not get occluded/disoccluded between frames, the optical flow method should be able to correctly estimate the motion of that pixel between the frames. The forward and backward flows then should be roughly equal, just in opposing directions.\nIf a pixel does get occluded/disoccluded between frames, it will not be visible in one the two frames and therefore the optical flow method cannot reliably estimate the motion for that pixel. It is then expected that the forward and backward flow are unequal.\nTo measure that effect they roughly use (per pixel) a formula matching length(fw + bw)^2 > length(fw)^2 + length(bw)^2.\n\n\nMask c\n\nThey create a mask c with the size of the frame.\nFor every pixel they estimate whether the boundary-equation or the disocclusion-equation is true.\nIf either of them is true, they add a 0 to the mask, otherwise a 1. So the mask is 1 wherever there is no disocclusion or motion boundary.\n\n\nCombination\n\nThe final temporal loss is the mean (over all pixels) of c*(x-w)^2.\n\nx is the frame to stylize.\nw is the previous stylized frame (frame i-1), warped according to the optical flow between frame i-1 and i.\nc is the mask value at the pixel.\n\n\nBy using the difference x-w they ensure that the difference in styles between two frames is low.\nBy adding c they ensure the style-consistency only at pixels that probably should have a consistent style.\n\n\n\n This loss enforces consistency in style between a pair of frames. The main sources of inconsistency are boundaries of moving objects and areas that get unonccluded. They use the optical flow to detect motion.\n\nApplying an optical flow method to two frames (i, i+1) returns per pixel the movement of that pixel, i.e. if the pixel at (x=1, y=2) moved to (x=2, y=4) the optical flow at that pixel would be (u=1, v=2).\nThe optical flow can be split into the forward flow (here fw) and the backward flow (here bw). The forward flow is the flow from frame i to i+1 (as described in the previous point). The backward flow is the flow from frame i+1 to i (reverse direction in time).\n\n Applying an optical flow method to two frames (i, i+1) returns per pixel the movement of that pixel, i.e. if the pixel at (x=1, y=2) moved to (x=2, y=4) the optical flow at that pixel would be (u=1, v=2). The optical flow can be split into the forward flow (here fw) and the backward flow (here bw). The forward flow is the flow from frame i to i+1 (as described in the previous point). The backward flow is the flow from frame i+1 to i (reverse direction in time). Boundaries\n\nAt boundaries of objects the derivative of the flow is high, i.e. the flow \"suddenly\" changes significantly from one pixel to the other.\nSo to detect boundaries they use (per pixel) roughly the equation gradient(u)^2 + gradient(v)^2 > length((u,v)).\n\n At boundaries of objects the derivative of the flow is high, i.e. the flow \"suddenly\" changes significantly from one pixel to the other. So to detect boundaries they use (per pixel) roughly the equation gradient(u)^2 + gradient(v)^2 > length((u,v)). Occlusions and disocclusions\n\nIf a pixel does not get occluded/disoccluded between frames, the optical flow method should be able to correctly estimate the motion of that pixel between the frames. The forward and backward flows then should be roughly equal, just in opposing directions.\nIf a pixel does get occluded/disoccluded between frames, it will not be visible in one the two frames and therefore the optical flow method cannot reliably estimate the motion for that pixel. It is then expected that the forward and backward flow are unequal.\nTo measure that effect they roughly use (per pixel) a formula matching length(fw + bw)^2 > length(fw)^2 + length(bw)^2.\n\n If a pixel does not get occluded/disoccluded between frames, the optical flow method should be able to correctly estimate the motion of that pixel between the frames. The forward and backward flows then should be roughly equal, just in opposing directions. If a pixel does get occluded/disoccluded between frames, it will not be visible in one the two frames and therefore the optical flow method cannot reliably estimate the motion for that pixel. It is then expected that the forward and backward flow are unequal. To measure that effect they roughly use (per pixel) a formula matching length(fw + bw)^2 > length(fw)^2 + length(bw)^2. Mask c\n\nThey create a mask c with the size of the frame.\nFor every pixel they estimate whether the boundary-equation or the disocclusion-equation is true.\nIf either of them is true, they add a 0 to the mask, otherwise a 1. So the mask is 1 wherever there is no disocclusion or motion boundary.\n\n They create a mask c with the size of the frame. For every pixel they estimate whether the boundary-equation or the disocclusion-equation is true. If either of them is true, they add a 0 to the mask, otherwise a 1. So the mask is 1 wherever there is no disocclusion or motion boundary. Combination\n\nThe final temporal loss is the mean (over all pixels) of c*(x-w)^2.\n\nx is the frame to stylize.\nw is the previous stylized frame (frame i-1), warped according to the optical flow between frame i-1 and i.\nc is the mask value at the pixel.\n\n\nBy using the difference x-w they ensure that the difference in styles between two frames is low.\nBy adding c they ensure the style-consistency only at pixels that probably should have a consistent style.\n\n The final temporal loss is the mean (over all pixels) of c*(x-w)^2.\n\nx is the frame to stylize.\nw is the previous stylized frame (frame i-1), warped according to the optical flow between frame i-1 and i.\nc is the mask value at the pixel.\n\n x is the frame to stylize. w is the previous stylized frame (frame i-1), warped according to the optical flow between frame i-1 and i. c is the mask value at the pixel. By using the difference x-w they ensure that the difference in styles between two frames is low. By adding c they ensure the style-consistency only at pixels that probably should have a consistent style. Long-term loss (new)\n\nThis loss enforces consistency in style between pairs of frames that are longer apart from each other.\nIt is a simple extension of the temporal (short-term) loss.\nThe temporal loss was computed for frames (i-1, i). The long-term loss is the sum of the temporal losses for the frame pairs {(i-4,i), (i-2,i), (i-1,i)}.\nThe c mask is recomputed for every pair and 1 if there are no boundaries/disocclusions detected, but only if there is not a 1 for the same pixel in a later mask. The additional condition is intended to associate pixels with their closest neighbours in time to minimize possible errors.\nNote that the long-term loss can completely replace the temporal loss as the latter one is contained in the former one.\n\n This loss enforces consistency in style between pairs of frames that are longer apart from each other. It is a simple extension of the temporal (short-term) loss. The temporal loss was computed for frames (i-1, i). The long-term loss is the sum of the temporal losses for the frame pairs {(i-4,i), (i-2,i), (i-1,i)}. The c mask is recomputed for every pair and 1 if there are no boundaries/disocclusions detected, but only if there is not a 1 for the same pixel in a later mask. The additional condition is intended to associate pixels with their closest neighbours in time to minimize possible errors. Note that the long-term loss can completely replace the temporal loss as the latter one is contained in the former one. Multi-pass approach (new)\n\nThey had problems with contrast around the boundaries of the frames.\nTo combat that, they use a multi-pass method in which they seem to calculate the optical flow in multiple forward and backward passes? (Not very clear here what they do and why it would help.)\n\n They had problems with contrast around the boundaries of the frames. To combat that, they use a multi-pass method in which they seem to calculate the optical flow in multiple forward and backward passes? (Not very clear here what they do and why it would help.) Initialization with previous frame (new)\n\nInstead of starting at a gaussian noise image every time, they instead use the previous stylized frame.\nThat immediately leads to more similarity between the frames.\n\n Instead of starting at a gaussian noise image every time, they instead use the previous stylized frame. That immediately leads to more similarity between the frames. \nResults\n\nVideo\n\n Video \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1604.08610"
    },
    "78": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Playing_Atari_with_Deep_Reinforcement_Learning.md",
        "transcript": "\nWhat\n\nThey use an implementation of Q-learning (i.e. reinforcement learning) with CNNs to automatically play Atari games.\nThe algorithm receives the raw pixels as its input and has to choose buttons to press as its output. No hand-engineered features are used. So the model \"sees\" the game and \"uses\" the controller, just like a human player would.\nThe model achieves good results on various games, beating all previous techniques and sometimes even surpassing human players.\n\n They use an implementation of Q-learning (i.e. reinforcement learning) with CNNs to automatically play Atari games. The algorithm receives the raw pixels as its input and has to choose buttons to press as its output. No hand-engineered features are used. So the model \"sees\" the game and \"uses\" the controller, just like a human player would. The model achieves good results on various games, beating all previous techniques and sometimes even surpassing human players. \nHow\n\nDeep Q Learning\n\nThis is yet another explanation of deep Q learning, see also this blog post for longer explanation.\nWhile playing, sequences of the form (state1, action1, reward, state2) are generated.\n\nstate1 is the current game state. The agent only sees the pixels of that state. (Example: Screen shows enemy.)\naction1 is an action that the agent chooses. (Example: Shoot!)\nreward is the direct reward received for picking action1 in state1. (Example: +1 for a kill.)\nstate2 is the next game state, after the action was chosen in state1. (Example: Screen shows dead enemy.)\n\n\nOne can pick actions at random for some time to generate lots of such tuples. That leads to a replay memory.\nDirect reward\n\nAfter playing randomly for some time, one can train a model to predict the direct reward given a screen (we don't want to use the whole state, just the pixels) and an action, i.e. Q(screen, action) -> direct reward.\nThat function would need a forward pass for each possible action that we could take. So for e.g. 8 buttons that would be 8 forward passes. To make things more efficient, we can let the model directly predict the direct reward for each available action, e.g. for 3 buttons Q(screen) -> (direct reward of action1, direct reward of action2, direct reward of action3).\nWe can then sample examples from our replay memory. The input per example is the screen. The output is the reward as a tuple. E.g. if we picked button 1 of 3 in one example and received a reward of +1 then our output/label for that example would be (1, 0, 0).\nWe can then train the model by playing completely randomly for some time, then sample some batches and train using a mean squared error. Then play a bit less randomly, i.e. start to use the action which the network thinks would generate the highest reward. Then train again, and so on.\n\n\nIndirect reward\n\nDoing the previous steps, the model will learn to anticipate the direct reward correctly. However, we also want it to predict indirect rewards. Otherwise, the model e.g. would never learn to shoot rockets at enemies, because the reward from killing an enemy would come many frames later.\nTo learn the indirect reward, one simply adds the reward value of highest reward action according to Q(state2) to the direct reward.\nI.e. if we have a tuple (state1, action1, reward, state2), we would not add (state1, action1, reward) to the replay memory, but instead (state1, action1, reward + highestReward(Q(screen2))). (Where highestReward() returns the reward of the action with the highest reward according to Q().)\nBy training to predict reward + highestReward(Q(screen2)) the network learns to anticipate the direct reward and the indirect reward. It takes a leap of faith to accept that this will ever converge to a good solution, but it does.\nWe then add gamma to the equation: reward + gamma*highestReward(Q(screen2)). gamma may be set to 0.9. It is a discount factor that devalues future states, e.g. because the world is not deterministic and therefore we can't exactly predict what's going to happen. Note that Q will automatically learn to stack it, e.g. state3 will be discounted to gamma^2 at state1.\n\n\n\n\nThis paper\n\nThey use the mentioned Deep Q Learning to train their model Q.\nThey use a k-th frame technique, i.e. they let the model decide upon an action at (here) every 4th frame.\nQ is implemented via a neural net. It receives 84x84x4 grayscale pixels that show the game and projects that onto the rewards of 4 to 18 actions.\nThe input is HxWx4 because they actually feed the last 4 frames into the network, instead of just 1 frame. So the network knows more about what things are moving how.\nThe network architecture is:\n\n84x84x4 (input)\n16 convs, 8x8, stride 4, ReLU\n32 convs, 4x4, stride 2, ReLU\n256 fully connected neurons, ReLU\n<N_actions> fully connected neurons, linear\n\n\nThey use a replay memory of 1 million frames.\n\n\n\n Deep Q Learning\n\nThis is yet another explanation of deep Q learning, see also this blog post for longer explanation.\nWhile playing, sequences of the form (state1, action1, reward, state2) are generated.\n\nstate1 is the current game state. The agent only sees the pixels of that state. (Example: Screen shows enemy.)\naction1 is an action that the agent chooses. (Example: Shoot!)\nreward is the direct reward received for picking action1 in state1. (Example: +1 for a kill.)\nstate2 is the next game state, after the action was chosen in state1. (Example: Screen shows dead enemy.)\n\n\nOne can pick actions at random for some time to generate lots of such tuples. That leads to a replay memory.\nDirect reward\n\nAfter playing randomly for some time, one can train a model to predict the direct reward given a screen (we don't want to use the whole state, just the pixels) and an action, i.e. Q(screen, action) -> direct reward.\nThat function would need a forward pass for each possible action that we could take. So for e.g. 8 buttons that would be 8 forward passes. To make things more efficient, we can let the model directly predict the direct reward for each available action, e.g. for 3 buttons Q(screen) -> (direct reward of action1, direct reward of action2, direct reward of action3).\nWe can then sample examples from our replay memory. The input per example is the screen. The output is the reward as a tuple. E.g. if we picked button 1 of 3 in one example and received a reward of +1 then our output/label for that example would be (1, 0, 0).\nWe can then train the model by playing completely randomly for some time, then sample some batches and train using a mean squared error. Then play a bit less randomly, i.e. start to use the action which the network thinks would generate the highest reward. Then train again, and so on.\n\n\nIndirect reward\n\nDoing the previous steps, the model will learn to anticipate the direct reward correctly. However, we also want it to predict indirect rewards. Otherwise, the model e.g. would never learn to shoot rockets at enemies, because the reward from killing an enemy would come many frames later.\nTo learn the indirect reward, one simply adds the reward value of highest reward action according to Q(state2) to the direct reward.\nI.e. if we have a tuple (state1, action1, reward, state2), we would not add (state1, action1, reward) to the replay memory, but instead (state1, action1, reward + highestReward(Q(screen2))). (Where highestReward() returns the reward of the action with the highest reward according to Q().)\nBy training to predict reward + highestReward(Q(screen2)) the network learns to anticipate the direct reward and the indirect reward. It takes a leap of faith to accept that this will ever converge to a good solution, but it does.\nWe then add gamma to the equation: reward + gamma*highestReward(Q(screen2)). gamma may be set to 0.9. It is a discount factor that devalues future states, e.g. because the world is not deterministic and therefore we can't exactly predict what's going to happen. Note that Q will automatically learn to stack it, e.g. state3 will be discounted to gamma^2 at state1.\n\n\n\n This is yet another explanation of deep Q learning, see also this blog post for longer explanation. While playing, sequences of the form (state1, action1, reward, state2) are generated.\n\nstate1 is the current game state. The agent only sees the pixels of that state. (Example: Screen shows enemy.)\naction1 is an action that the agent chooses. (Example: Shoot!)\nreward is the direct reward received for picking action1 in state1. (Example: +1 for a kill.)\nstate2 is the next game state, after the action was chosen in state1. (Example: Screen shows dead enemy.)\n\n state1 is the current game state. The agent only sees the pixels of that state. (Example: Screen shows enemy.) action1 is an action that the agent chooses. (Example: Shoot!) reward is the direct reward received for picking action1 in state1. (Example: +1 for a kill.) state2 is the next game state, after the action was chosen in state1. (Example: Screen shows dead enemy.) One can pick actions at random for some time to generate lots of such tuples. That leads to a replay memory. Direct reward\n\nAfter playing randomly for some time, one can train a model to predict the direct reward given a screen (we don't want to use the whole state, just the pixels) and an action, i.e. Q(screen, action) -> direct reward.\nThat function would need a forward pass for each possible action that we could take. So for e.g. 8 buttons that would be 8 forward passes. To make things more efficient, we can let the model directly predict the direct reward for each available action, e.g. for 3 buttons Q(screen) -> (direct reward of action1, direct reward of action2, direct reward of action3).\nWe can then sample examples from our replay memory. The input per example is the screen. The output is the reward as a tuple. E.g. if we picked button 1 of 3 in one example and received a reward of +1 then our output/label for that example would be (1, 0, 0).\nWe can then train the model by playing completely randomly for some time, then sample some batches and train using a mean squared error. Then play a bit less randomly, i.e. start to use the action which the network thinks would generate the highest reward. Then train again, and so on.\n\n After playing randomly for some time, one can train a model to predict the direct reward given a screen (we don't want to use the whole state, just the pixels) and an action, i.e. Q(screen, action) -> direct reward. That function would need a forward pass for each possible action that we could take. So for e.g. 8 buttons that would be 8 forward passes. To make things more efficient, we can let the model directly predict the direct reward for each available action, e.g. for 3 buttons Q(screen) -> (direct reward of action1, direct reward of action2, direct reward of action3). We can then sample examples from our replay memory. The input per example is the screen. The output is the reward as a tuple. E.g. if we picked button 1 of 3 in one example and received a reward of +1 then our output/label for that example would be (1, 0, 0). We can then train the model by playing completely randomly for some time, then sample some batches and train using a mean squared error. Then play a bit less randomly, i.e. start to use the action which the network thinks would generate the highest reward. Then train again, and so on. Indirect reward\n\nDoing the previous steps, the model will learn to anticipate the direct reward correctly. However, we also want it to predict indirect rewards. Otherwise, the model e.g. would never learn to shoot rockets at enemies, because the reward from killing an enemy would come many frames later.\nTo learn the indirect reward, one simply adds the reward value of highest reward action according to Q(state2) to the direct reward.\nI.e. if we have a tuple (state1, action1, reward, state2), we would not add (state1, action1, reward) to the replay memory, but instead (state1, action1, reward + highestReward(Q(screen2))). (Where highestReward() returns the reward of the action with the highest reward according to Q().)\nBy training to predict reward + highestReward(Q(screen2)) the network learns to anticipate the direct reward and the indirect reward. It takes a leap of faith to accept that this will ever converge to a good solution, but it does.\nWe then add gamma to the equation: reward + gamma*highestReward(Q(screen2)). gamma may be set to 0.9. It is a discount factor that devalues future states, e.g. because the world is not deterministic and therefore we can't exactly predict what's going to happen. Note that Q will automatically learn to stack it, e.g. state3 will be discounted to gamma^2 at state1.\n\n Doing the previous steps, the model will learn to anticipate the direct reward correctly. However, we also want it to predict indirect rewards. Otherwise, the model e.g. would never learn to shoot rockets at enemies, because the reward from killing an enemy would come many frames later. To learn the indirect reward, one simply adds the reward value of highest reward action according to Q(state2) to the direct reward. I.e. if we have a tuple (state1, action1, reward, state2), we would not add (state1, action1, reward) to the replay memory, but instead (state1, action1, reward + highestReward(Q(screen2))). (Where highestReward() returns the reward of the action with the highest reward according to Q().) By training to predict reward + highestReward(Q(screen2)) the network learns to anticipate the direct reward and the indirect reward. It takes a leap of faith to accept that this will ever converge to a good solution, but it does. We then add gamma to the equation: reward + gamma*highestReward(Q(screen2)). gamma may be set to 0.9. It is a discount factor that devalues future states, e.g. because the world is not deterministic and therefore we can't exactly predict what's going to happen. Note that Q will automatically learn to stack it, e.g. state3 will be discounted to gamma^2 at state1. This paper\n\nThey use the mentioned Deep Q Learning to train their model Q.\nThey use a k-th frame technique, i.e. they let the model decide upon an action at (here) every 4th frame.\nQ is implemented via a neural net. It receives 84x84x4 grayscale pixels that show the game and projects that onto the rewards of 4 to 18 actions.\nThe input is HxWx4 because they actually feed the last 4 frames into the network, instead of just 1 frame. So the network knows more about what things are moving how.\nThe network architecture is:\n\n84x84x4 (input)\n16 convs, 8x8, stride 4, ReLU\n32 convs, 4x4, stride 2, ReLU\n256 fully connected neurons, ReLU\n<N_actions> fully connected neurons, linear\n\n\nThey use a replay memory of 1 million frames.\n\n They use the mentioned Deep Q Learning to train their model Q. They use a k-th frame technique, i.e. they let the model decide upon an action at (here) every 4th frame. Q is implemented via a neural net. It receives 84x84x4 grayscale pixels that show the game and projects that onto the rewards of 4 to 18 actions. The input is HxWx4 because they actually feed the last 4 frames into the network, instead of just 1 frame. So the network knows more about what things are moving how. The network architecture is:\n\n84x84x4 (input)\n16 convs, 8x8, stride 4, ReLU\n32 convs, 4x4, stride 2, ReLU\n256 fully connected neurons, ReLU\n<N_actions> fully connected neurons, linear\n\n 84x84x4 (input) 16 convs, 8x8, stride 4, ReLU 32 convs, 4x4, stride 2, ReLU 256 fully connected neurons, ReLU <N_actions> fully connected neurons, linear They use a replay memory of 1 million frames. \nResults\n\nThey ran experiments on the Atari games Beam Rider, Breakout, Enduro, Pong, Qbert, Seaquest and Space Invaders.\nSame architecture and hyperparameters for all games.\nRewards were based on score changes in the games, i.e. they used +1 (score increases) and -1 (score decreased).\nOptimizer: RMSProp, Batch Size: 32.\nTrained for 10 million examples/frames per game.\nThey had no problems with instability and their average Q value per game increased smoothly.\nTheir method beats all other state of the art methods.\nThey managed to beat a human player in games that required not so much \"long\" term strategies (the less frames the better).\nVideo (starts at 46:05).\n\n They ran experiments on the Atari games Beam Rider, Breakout, Enduro, Pong, Qbert, Seaquest and Space Invaders. Same architecture and hyperparameters for all games. Rewards were based on score changes in the games, i.e. they used +1 (score increases) and -1 (score decreased). Optimizer: RMSProp, Batch Size: 32. Trained for 10 million examples/frames per game. They had no problems with instability and their average Q value per game increased smoothly. Their method beats all other state of the art methods. They managed to beat a human player in games that required not so much \"long\" term strategies (the less frames the better). Video (starts at 46:05). \n(1) Introduction\n\nProblems when using neural nets in reinforcement learning (RL):\n\nReward signal is often sparse, noise and delayed.\nOften assumption that data samples are independent, while they are correlated in RL.\nData distribution can change when the algorithm learns new behaviours.\n\n\nThey use Q-learning with a CNN and stochastic gradient descent.\nThey use an experience replay mechanism (i.e. memory) from which they can sample previous transitions (for training).\nThey apply their method to Atari 2600 games in the Arcade Learning Environment (ALE).\nThey use only the visible pixels as input to the network, i.e. no manual feature extraction.\n\n Problems when using neural nets in reinforcement learning (RL):\n\nReward signal is often sparse, noise and delayed.\nOften assumption that data samples are independent, while they are correlated in RL.\nData distribution can change when the algorithm learns new behaviours.\n\n Reward signal is often sparse, noise and delayed. Often assumption that data samples are independent, while they are correlated in RL. Data distribution can change when the algorithm learns new behaviours. They use Q-learning with a CNN and stochastic gradient descent. They use an experience replay mechanism (i.e. memory) from which they can sample previous transitions (for training). They apply their method to Atari 2600 games in the Arcade Learning Environment (ALE). They use only the visible pixels as input to the network, i.e. no manual feature extraction. \n(2) Background\n\nblablabla, standard deep q learning explanation\n\n blablabla, standard deep q learning explanation \n(3) Related Work\n\nTD-Backgammon: \"Solved\" backgammon. Worked similarly to Q-learning and used a multi-layer perceptron.\nAttempts to copy TD-Backgammon to other games failed.\nResearch was focused on linear function approximators as there were problems with non-linear ones diverging.\nRecently again interest in using neural nets for reinforcement learning. Some attempts to fix divergence problems with gradient temporal-difference methods.\nNFQ is a very similar method (to the one in this paper), but worked on the whole batch instead of minibatches, making it slow. It also first applied dimensionality reduction via autoencoders on the images instead of training on them end-to-end.\nHyperNEAT was applied to Atari games and evolved a neural net for each game. The networks learned to exploit design flaws.\n\n TD-Backgammon: \"Solved\" backgammon. Worked similarly to Q-learning and used a multi-layer perceptron. Attempts to copy TD-Backgammon to other games failed. Research was focused on linear function approximators as there were problems with non-linear ones diverging. Recently again interest in using neural nets for reinforcement learning. Some attempts to fix divergence problems with gradient temporal-difference methods. NFQ is a very similar method (to the one in this paper), but worked on the whole batch instead of minibatches, making it slow. It also first applied dimensionality reduction via autoencoders on the images instead of training on them end-to-end. HyperNEAT was applied to Atari games and evolved a neural net for each game. The networks learned to exploit design flaws. \n(4) Deep Reinforcement Learning\n\nThey want to connect a reinforcement learning algorithm with a deep neural network, e.g. to get rid of handcrafted features.\nThe network is supposes to run on the raw RGB images.\nThey use experience replay, i.e. store tuples of (pixels, chosen action, received reward) in a memory and use that during training.\nThey use Q-learning.\nThey use an epsilon-greedy policy.\nAdvantages from using experience replay instead of learning \"live\" during game playing:\n\nExperiences can be reused many times (more efficient).\nSamples are less correlated.\nLearned parameters from one batch don't determine as much the distributions of the examples in the next batch.\n\n\nThey save the last N experiences and sample uniformly from them during training.\n(4.1) Preprocessing and Model Architecture\n\nRaw Atari images are 210x160 pixels with 128 possible colors.\nThey downsample them to 110x84 pixels and then crop the 84x84 playing area out of them.\nThey also convert the images to grayscale.\nThey use the last 4 frames as input and stack them.\nSo their network input has shape 84x84x4.\nThey use one output neuron per possible action. So they can compute the Q-value (expected reward) of each action with one forward pass.\nArchitecture: 84x84x4 (input) => 16 8x8 convs, stride 4, ReLU => 32 4x4 convs stride 2 ReLU => fc 256, ReLU => fc N actions, linear\n4 to 18 actions/outputs (depends on the game).\nAside from the outputs, the architecture is the same for all games.\n\n\n\n They want to connect a reinforcement learning algorithm with a deep neural network, e.g. to get rid of handcrafted features. The network is supposes to run on the raw RGB images. They use experience replay, i.e. store tuples of (pixels, chosen action, received reward) in a memory and use that during training. They use Q-learning. They use an epsilon-greedy policy. Advantages from using experience replay instead of learning \"live\" during game playing:\n\nExperiences can be reused many times (more efficient).\nSamples are less correlated.\nLearned parameters from one batch don't determine as much the distributions of the examples in the next batch.\n\n Experiences can be reused many times (more efficient). Samples are less correlated. Learned parameters from one batch don't determine as much the distributions of the examples in the next batch. They save the last N experiences and sample uniformly from them during training. (4.1) Preprocessing and Model Architecture\n\nRaw Atari images are 210x160 pixels with 128 possible colors.\nThey downsample them to 110x84 pixels and then crop the 84x84 playing area out of them.\nThey also convert the images to grayscale.\nThey use the last 4 frames as input and stack them.\nSo their network input has shape 84x84x4.\nThey use one output neuron per possible action. So they can compute the Q-value (expected reward) of each action with one forward pass.\nArchitecture: 84x84x4 (input) => 16 8x8 convs, stride 4, ReLU => 32 4x4 convs stride 2 ReLU => fc 256, ReLU => fc N actions, linear\n4 to 18 actions/outputs (depends on the game).\nAside from the outputs, the architecture is the same for all games.\n\n Raw Atari images are 210x160 pixels with 128 possible colors. They downsample them to 110x84 pixels and then crop the 84x84 playing area out of them. They also convert the images to grayscale. They use the last 4 frames as input and stack them. So their network input has shape 84x84x4. They use one output neuron per possible action. So they can compute the Q-value (expected reward) of each action with one forward pass. Architecture: 84x84x4 (input) => 16 8x8 convs, stride 4, ReLU => 32 4x4 convs stride 2 ReLU => fc 256, ReLU => fc N actions, linear 4 to 18 actions/outputs (depends on the game). Aside from the outputs, the architecture is the same for all games. \n(5) Experiments\n\nGames that they played: Beam Rider, Breakout, Enduro, Pong, Qbert, Seaquest, Space Invaders\nThey use the same architecture und hyperparameters for all games.\nThey give a reward of +1 whenever the in-game score increases and -1 whenever it decreases.\nThey use RMSProp.\nMini batch size was 32.\nThey train for 10 million frames/examples.\nThey initialize epsilon (in their epsilon greedy strategy) to 1.0 and decrease it linearly to 0.1 at one million frames.\nThey let the agent decide upon an action at every 4th in-game frame (3rd in space invaders).\n(5.1) Training and stability\n\nThey plot the average reward und Q-value per N games to evaluate the agent's training progress,\nThe average reward increases in a noisy way.\nThe average Q value increases smoothly.\nThey did not experience any divergence issues during their training.\n\n\n(5.2) Visualizating the Value Function\n\nThe agent learns to predict the value function accurately, even for rather long sequences (here: ~25 frames).\n\n\n(5.3) Main Evaluation\n\nThey compare to three other methods that use hand-engineered features and/or use the pixel data combined with significant prior knownledge.\nThey mostly outperform the other methods.\nThey managed to beat a human player in three games. The ones where the human won seemed to require strategies that stretched over longer time frames.\n\n\n\n Games that they played: Beam Rider, Breakout, Enduro, Pong, Qbert, Seaquest, Space Invaders They use the same architecture und hyperparameters for all games. They give a reward of +1 whenever the in-game score increases and -1 whenever it decreases. They use RMSProp. Mini batch size was 32. They train for 10 million frames/examples. They initialize epsilon (in their epsilon greedy strategy) to 1.0 and decrease it linearly to 0.1 at one million frames. They let the agent decide upon an action at every 4th in-game frame (3rd in space invaders). (5.1) Training and stability\n\nThey plot the average reward und Q-value per N games to evaluate the agent's training progress,\nThe average reward increases in a noisy way.\nThe average Q value increases smoothly.\nThey did not experience any divergence issues during their training.\n\n They plot the average reward und Q-value per N games to evaluate the agent's training progress, The average reward increases in a noisy way. The average Q value increases smoothly. They did not experience any divergence issues during their training. (5.2) Visualizating the Value Function\n\nThe agent learns to predict the value function accurately, even for rather long sequences (here: ~25 frames).\n\n The agent learns to predict the value function accurately, even for rather long sequences (here: ~25 frames). (5.3) Main Evaluation\n\nThey compare to three other methods that use hand-engineered features and/or use the pixel data combined with significant prior knownledge.\nThey mostly outperform the other methods.\nThey managed to beat a human player in three games. The ones where the human won seemed to require strategies that stretched over longer time frames.\n\n They compare to three other methods that use hand-engineered features and/or use the pixel data combined with significant prior knownledge. They mostly outperform the other methods. They managed to beat a human player in three games. The ones where the human won seemed to require strategies that stretched over longer time frames. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1312.5602"
    },
    "79": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Attend_Infer_Repeat.md",
        "transcript": "\nWhat\n\nAIR (attend, infer, repeat) is a recurrent autoencoder architecture to transform images into latent representations object by object.\nAs an autoencoder it is unsupervised.\nThe latent representation is generated in multiple time steps.\nEach time step is intended to encode information about exactly one object in the image.\nThe information encoded for each object is (mostly) a what-where information, i.e. which class the object has and where (in 2D: translation, scaling) it is shown.\nAIR has a dynamic number of time step. After encoding one object the model can decide whether it has encoded all objects or whether there is another one to encode. As a result the latent layer size is not fixed.\nAIR uses an attention mechanism during the encoding to focus on each object.\n\n AIR (attend, infer, repeat) is a recurrent autoencoder architecture to transform images into latent representations object by object. As an autoencoder it is unsupervised. The latent representation is generated in multiple time steps. Each time step is intended to encode information about exactly one object in the image. The information encoded for each object is (mostly) a what-where information, i.e. which class the object has and where (in 2D: translation, scaling) it is shown. AIR has a dynamic number of time step. After encoding one object the model can decide whether it has encoded all objects or whether there is another one to encode. As a result the latent layer size is not fixed. AIR uses an attention mechanism during the encoding to focus on each object. \nHow\n\nAt its core, AIR is a variational autoencoder.\nIt maximizes lower bounds on the error instead of using a \"classic\" reconstruction error (like MSE on the euclidean distance).\nIt has an encoder and a decoder.\nThe model uses a recurrent architecture via an LSTM.\nIt (ideally) encodes/decodes one object per time step.\nEncoder\n\nThe encoder receives the image and generates latent information for one object (what object, where it is).\nAt the second timestep it receives the image, the previous timestep's latent information and the previous timestep's hidden layer. It then generates another latent information (for another object).\nAnd so on.\n\n\nDecoder\n\nThe decoder receives latent information from the encoder (timestep by timestep) and treats it as a what-where information when reconstructing the images.\n\nIt takes the what-part and uses a \"normal\" decoder to generate an image that shows the object.\nIt takes the where-part and the generated image and feeds both into a spatial transformer, which then transforms the generated image by translating or rotating it.\n\n\n\n\nDynamic size\n\nAIR makes use of a dynamically sized latent layer. It is not necessarily limited to a fixed number of time steps.\nImplementation: Instead of just letting the encoder generate what-where information, the encoder also generates a \"present\" information, which is 0 or 1. If it is 1, the reccurence will continue with encoding and decoding another object. Otherwise it will stop.\n\n\nAttention\n\nTo add an attention mechanism, AIR first uses the LSTM's hidden layer to generate \"where\" and \"present\" information per object.\nIt stops if the \"present\" information is 0.\nOtherwise it uses the \"where\" information to focus on the object using a spatial transformer. The object is then encoded to the \"what\" information.\n\n\n\n At its core, AIR is a variational autoencoder. It maximizes lower bounds on the error instead of using a \"classic\" reconstruction error (like MSE on the euclidean distance). It has an encoder and a decoder. The model uses a recurrent architecture via an LSTM. It (ideally) encodes/decodes one object per time step. Encoder\n\nThe encoder receives the image and generates latent information for one object (what object, where it is).\nAt the second timestep it receives the image, the previous timestep's latent information and the previous timestep's hidden layer. It then generates another latent information (for another object).\nAnd so on.\n\n The encoder receives the image and generates latent information for one object (what object, where it is). At the second timestep it receives the image, the previous timestep's latent information and the previous timestep's hidden layer. It then generates another latent information (for another object). And so on. Decoder\n\nThe decoder receives latent information from the encoder (timestep by timestep) and treats it as a what-where information when reconstructing the images.\n\nIt takes the what-part and uses a \"normal\" decoder to generate an image that shows the object.\nIt takes the where-part and the generated image and feeds both into a spatial transformer, which then transforms the generated image by translating or rotating it.\n\n\n\n The decoder receives latent information from the encoder (timestep by timestep) and treats it as a what-where information when reconstructing the images.\n\nIt takes the what-part and uses a \"normal\" decoder to generate an image that shows the object.\nIt takes the where-part and the generated image and feeds both into a spatial transformer, which then transforms the generated image by translating or rotating it.\n\n It takes the what-part and uses a \"normal\" decoder to generate an image that shows the object. It takes the where-part and the generated image and feeds both into a spatial transformer, which then transforms the generated image by translating or rotating it. Dynamic size\n\nAIR makes use of a dynamically sized latent layer. It is not necessarily limited to a fixed number of time steps.\nImplementation: Instead of just letting the encoder generate what-where information, the encoder also generates a \"present\" information, which is 0 or 1. If it is 1, the reccurence will continue with encoding and decoding another object. Otherwise it will stop.\n\n AIR makes use of a dynamically sized latent layer. It is not necessarily limited to a fixed number of time steps. Implementation: Instead of just letting the encoder generate what-where information, the encoder also generates a \"present\" information, which is 0 or 1. If it is 1, the reccurence will continue with encoding and decoding another object. Otherwise it will stop. Attention\n\nTo add an attention mechanism, AIR first uses the LSTM's hidden layer to generate \"where\" and \"present\" information per object.\nIt stops if the \"present\" information is 0.\nOtherwise it uses the \"where\" information to focus on the object using a spatial transformer. The object is then encoded to the \"what\" information.\n\n To add an attention mechanism, AIR first uses the LSTM's hidden layer to generate \"where\" and \"present\" information per object. It stops if the \"present\" information is 0. Otherwise it uses the \"where\" information to focus on the object using a spatial transformer. The object is then encoded to the \"what\" information. \nResults\n\nOn a dataset of images, each containing multiple MNIST digits, AIR learns to accurately count the digits and estimate their position and scale.\nWhen AIR is trained on images of 0 to 2 digits and tested on images containing 3 digits it performs poorly.\nWhen AIR is trained on images of 0, 1 or 3 digits and tested on images containing 2 digits it performs mediocre.\nDAIR performs well on both tasks. Likely because it learns to remove each digit from the image after it has investigated it.\nWhen AIR is trained on 0 to 2 digits and a second network is trained (separately) to work with the generated latent layer (trained to sum the shown digits and rate whether they are shown in ascending order), then that second network reaches high accuracy with relatively few examples. That indicates usefulness for unsupervised learning.\nWhen AIR is trained on a dataset of handwritten characters from different alphabets, it learns to represent distinct strokes in its latent layer.\nWhen AIR is trained in combination with a renderer (inverse graphics), it is able to accurately recover latent parameters of rendered objects - better than supervised networks. That indicates usefulness for robots which have to interact with objects.\n\n On a dataset of images, each containing multiple MNIST digits, AIR learns to accurately count the digits and estimate their position and scale. When AIR is trained on images of 0 to 2 digits and tested on images containing 3 digits it performs poorly. When AIR is trained on images of 0, 1 or 3 digits and tested on images containing 2 digits it performs mediocre. DAIR performs well on both tasks. Likely because it learns to remove each digit from the image after it has investigated it. When AIR is trained on 0 to 2 digits and a second network is trained (separately) to work with the generated latent layer (trained to sum the shown digits and rate whether they are shown in ascending order), then that second network reaches high accuracy with relatively few examples. That indicates usefulness for unsupervised learning. When AIR is trained on a dataset of handwritten characters from different alphabets, it learns to represent distinct strokes in its latent layer. When AIR is trained in combination with a renderer (inverse graphics), it is able to accurately recover latent parameters of rendered objects - better than supervised networks. That indicates usefulness for robots which have to interact with objects. \n(1) Introduction\n\nAssumption: Images are made up of distinct objects. These objects have visual and physical properties.\nThey developed a framework for efficient inference in images (i.e. get from the image to a latent representation of the objects, i.e. inverse graphics).\nParts of the framework: High dimensional representations (e.g. object images), interpretable latent variables (e.g. for rotation) and generative processes (to combine object images with latent variables).\nContributions:\n\nA scheme for efficient variational inference in latent spaces of variable dimensionality.\n\nIdea: Treat inference as an iterative process, implemented via an RNN that looks at one object at a time and learns an appropriate number of inference steps. (Attend-Infer-Repeat, AIR)\nEnd-to-end training via amortized variational inference (continuous variables: gradient descent, discrete variables: black-box optimization).\n\n\nAIR allows to train generative models that automatically learn to decompose scenes.\nAIR allows to recover objects and their attributes from rendered 3D scenes (inverse rendering).\n\n\n\n Assumption: Images are made up of distinct objects. These objects have visual and physical properties. They developed a framework for efficient inference in images (i.e. get from the image to a latent representation of the objects, i.e. inverse graphics). Parts of the framework: High dimensional representations (e.g. object images), interpretable latent variables (e.g. for rotation) and generative processes (to combine object images with latent variables). Contributions:\n\nA scheme for efficient variational inference in latent spaces of variable dimensionality.\n\nIdea: Treat inference as an iterative process, implemented via an RNN that looks at one object at a time and learns an appropriate number of inference steps. (Attend-Infer-Repeat, AIR)\nEnd-to-end training via amortized variational inference (continuous variables: gradient descent, discrete variables: black-box optimization).\n\n\nAIR allows to train generative models that automatically learn to decompose scenes.\nAIR allows to recover objects and their attributes from rendered 3D scenes (inverse rendering).\n\n A scheme for efficient variational inference in latent spaces of variable dimensionality.\n\nIdea: Treat inference as an iterative process, implemented via an RNN that looks at one object at a time and learns an appropriate number of inference steps. (Attend-Infer-Repeat, AIR)\nEnd-to-end training via amortized variational inference (continuous variables: gradient descent, discrete variables: black-box optimization).\n\n Idea: Treat inference as an iterative process, implemented via an RNN that looks at one object at a time and learns an appropriate number of inference steps. (Attend-Infer-Repeat, AIR) End-to-end training via amortized variational inference (continuous variables: gradient descent, discrete variables: black-box optimization). AIR allows to train generative models that automatically learn to decompose scenes. AIR allows to recover objects and their attributes from rendered 3D scenes (inverse rendering). \n(2) Approach\n\nJust like in VAEs, the scene interpretation is treated with a bayesian approach.\nThere are latent variables z and images x.\nImages are generated via a probability distribution p(x|z).\nThis can be reversed via bayes rule to p(x|z) = p(x)p(z|x) / p(z), which means that p(x|z)p(z) / p(x) = p(z|x).\nThe prior p(z) must be chosen and captures assumptions about the distributions of the latent variables.\np(x|z) is the likelihood and represents the model that generates images from latent variables.\nThey assume that there can be multiple objects in an image.\nEvery object gets its own latent variables.\nA probability distribution p(x|z) then converts each object (on its own) from the latent variables to an image.\nThe number of objects follows a probability distribution p(n).\nFor the prior and likelihood they assume two scenarios:\n\n2D: Three dimensions for X, Y and scale. Additionally n dimensions for its shape.\n3D: Dimensions for X, Y, Z, rotation, object identity/category (multinomial variable). (No scale?)\n\n\nBoth 2D and 3D can be separated into latent variables for \"where\" and \"what\".\nIt is assumed that the prior latent variables are independent of each other.\n(2.1) Inference\n\nInference for their model is intractable, therefore they use an approximation q(z,n|x), which minizes KL(q(z,n|x)||p(z,n|x)), i.e. KL(approximation||real) using amortized variational approximation.\nChallenges for them:\n\nThe dimensionality of their latent variable layer is a random variable p(n) (i.e. no static size.).\nStrong symmetries.\n\n\nThey implement inference via an RNN which encodes the image object by object.\nThe encoded latent variables can be gaussians.\nThey encode the latent layer length n via a vector (instead of an integer). The vector has the form of n ones followed by one zero.\nIf the length vector is #z then they want to approximate q(z,#z|x).\nThat can apparently be decomposed into <product> q(latent variable value i, #z is still 1 at i|x, previous latent variable values) * q(has length n|z,x).\nSo instead of computing #z once, they instead compute at every time step whether there is another object in the image, which indirectly creates a chain of ones followed by a zero (the #z vector).\n\n\n(2.2) Learning\n\nThe parameters theta (p, latent variable -> image) and phi (q, image -> latent variables) are jointly optimized.\nOptimization happens by maximizing a lower bound E[log(p(x,z,n) / q(z,n|x))] called the negative free energy.\n(2.2.1) Parameters of the model theta\n\nParameters theta of log(p(x,z,n)) can easily be obtained using differentiation, so long as z and n are well approximated.\nThe differentiation of the lower bound with repsect to theta can be approximated using Monte Carlo methods.\n\n\n(2.2.2) Parameters of the inference network phi\n\nphi are the parameters of q, i.e. of the RNN that generates z and #z in i timesteps.\nAt each timestep (i.e. per object) the RNN generates three kinds of information: What (object), where (it is), whether it is present (i <= n).\nEach of these information is represented via variables. These variables can be discrete or continuous.\nWhen differentiating w.r.t. a continuous variable they use the reparameterization trick.\nWhen differentiating w.r.t. a discrete variable they use the likelihood ratio estimator.\n\n\n\n\n\n Just like in VAEs, the scene interpretation is treated with a bayesian approach. There are latent variables z and images x. Images are generated via a probability distribution p(x|z). This can be reversed via bayes rule to p(x|z) = p(x)p(z|x) / p(z), which means that p(x|z)p(z) / p(x) = p(z|x). The prior p(z) must be chosen and captures assumptions about the distributions of the latent variables. p(x|z) is the likelihood and represents the model that generates images from latent variables. They assume that there can be multiple objects in an image. Every object gets its own latent variables. A probability distribution p(x|z) then converts each object (on its own) from the latent variables to an image. The number of objects follows a probability distribution p(n). For the prior and likelihood they assume two scenarios:\n\n2D: Three dimensions for X, Y and scale. Additionally n dimensions for its shape.\n3D: Dimensions for X, Y, Z, rotation, object identity/category (multinomial variable). (No scale?)\n\n 2D: Three dimensions for X, Y and scale. Additionally n dimensions for its shape. 3D: Dimensions for X, Y, Z, rotation, object identity/category (multinomial variable). (No scale?) Both 2D and 3D can be separated into latent variables for \"where\" and \"what\". It is assumed that the prior latent variables are independent of each other. (2.1) Inference\n\nInference for their model is intractable, therefore they use an approximation q(z,n|x), which minizes KL(q(z,n|x)||p(z,n|x)), i.e. KL(approximation||real) using amortized variational approximation.\nChallenges for them:\n\nThe dimensionality of their latent variable layer is a random variable p(n) (i.e. no static size.).\nStrong symmetries.\n\n\nThey implement inference via an RNN which encodes the image object by object.\nThe encoded latent variables can be gaussians.\nThey encode the latent layer length n via a vector (instead of an integer). The vector has the form of n ones followed by one zero.\nIf the length vector is #z then they want to approximate q(z,#z|x).\nThat can apparently be decomposed into <product> q(latent variable value i, #z is still 1 at i|x, previous latent variable values) * q(has length n|z,x).\nSo instead of computing #z once, they instead compute at every time step whether there is another object in the image, which indirectly creates a chain of ones followed by a zero (the #z vector).\n\n Inference for their model is intractable, therefore they use an approximation q(z,n|x), which minizes KL(q(z,n|x)||p(z,n|x)), i.e. KL(approximation||real) using amortized variational approximation. Challenges for them:\n\nThe dimensionality of their latent variable layer is a random variable p(n) (i.e. no static size.).\nStrong symmetries.\n\n The dimensionality of their latent variable layer is a random variable p(n) (i.e. no static size.). Strong symmetries. They implement inference via an RNN which encodes the image object by object. The encoded latent variables can be gaussians. They encode the latent layer length n via a vector (instead of an integer). The vector has the form of n ones followed by one zero. If the length vector is #z then they want to approximate q(z,#z|x). That can apparently be decomposed into <product> q(latent variable value i, #z is still 1 at i|x, previous latent variable values) * q(has length n|z,x). So instead of computing #z once, they instead compute at every time step whether there is another object in the image, which indirectly creates a chain of ones followed by a zero (the #z vector). (2.2) Learning\n\nThe parameters theta (p, latent variable -> image) and phi (q, image -> latent variables) are jointly optimized.\nOptimization happens by maximizing a lower bound E[log(p(x,z,n) / q(z,n|x))] called the negative free energy.\n(2.2.1) Parameters of the model theta\n\nParameters theta of log(p(x,z,n)) can easily be obtained using differentiation, so long as z and n are well approximated.\nThe differentiation of the lower bound with repsect to theta can be approximated using Monte Carlo methods.\n\n\n(2.2.2) Parameters of the inference network phi\n\nphi are the parameters of q, i.e. of the RNN that generates z and #z in i timesteps.\nAt each timestep (i.e. per object) the RNN generates three kinds of information: What (object), where (it is), whether it is present (i <= n).\nEach of these information is represented via variables. These variables can be discrete or continuous.\nWhen differentiating w.r.t. a continuous variable they use the reparameterization trick.\nWhen differentiating w.r.t. a discrete variable they use the likelihood ratio estimator.\n\n\n\n The parameters theta (p, latent variable -> image) and phi (q, image -> latent variables) are jointly optimized. Optimization happens by maximizing a lower bound E[log(p(x,z,n) / q(z,n|x))] called the negative free energy. (2.2.1) Parameters of the model theta\n\nParameters theta of log(p(x,z,n)) can easily be obtained using differentiation, so long as z and n are well approximated.\nThe differentiation of the lower bound with repsect to theta can be approximated using Monte Carlo methods.\n\n Parameters theta of log(p(x,z,n)) can easily be obtained using differentiation, so long as z and n are well approximated. The differentiation of the lower bound with repsect to theta can be approximated using Monte Carlo methods. (2.2.2) Parameters of the inference network phi\n\nphi are the parameters of q, i.e. of the RNN that generates z and #z in i timesteps.\nAt each timestep (i.e. per object) the RNN generates three kinds of information: What (object), where (it is), whether it is present (i <= n).\nEach of these information is represented via variables. These variables can be discrete or continuous.\nWhen differentiating w.r.t. a continuous variable they use the reparameterization trick.\nWhen differentiating w.r.t. a discrete variable they use the likelihood ratio estimator.\n\n phi are the parameters of q, i.e. of the RNN that generates z and #z in i timesteps. At each timestep (i.e. per object) the RNN generates three kinds of information: What (object), where (it is), whether it is present (i <= n). Each of these information is represented via variables. These variables can be discrete or continuous. When differentiating w.r.t. a continuous variable they use the reparameterization trick. When differentiating w.r.t. a discrete variable they use the likelihood ratio estimator. \n(3) Models and Experiments\n\nThe RNN is implemented via an LSTM.\nDAIR\n\nThe \"normal\" AIR model uses at every time step the image and the RNN's hidden layer to generate the next latent information (what object, where it is and whether it is present).\nDAIR uses that latent information to change the image at every time step and then use the difference (D) image for the next time step, i.e. DAIR can remove an object from the image after it has generated latent variables for it.\n\n\n(3.1) Multi-MNIST\n\nThey generate a dataset of images containing multiple MNIST digits.\nEach image contains 0 to 2 digits.\nAIR is trained on the dataset.\nIt learns without supervision a good attention scanning policy for the images (to \"hit\" all digits), to count the digits visible in the image and to use a matching number of time steps.\nDuring training, the model seems to first learn proper reconstruction of the digits and only then to do it with as few timesteps as possible.\n(3.1.1) Strong Generalization\n\nThey test the generalization capabilities of AIR.\nExtrapolation task: They generate images with 0 to 2 digits for training, then test on images with 3 digits. The model is unable to correctly count the digits (~0% accuracy).\nInterpolation task: They generate images with 0, 1 or 3 digits for training, then test on images with 2 digits. The model performs OK-ish (~60% accuracy).\nDAIR performs in both cases well (~80% for extrapolation, ~95% accuracy for interpolation).\n\n\n(3.1.2) Representational Power\n\nThey train AIR on images containing 0, 1 or 2 digits.\nThen they train a second network. That network takes the output of the first one and computes a) the sum of the digits and b) estimates whether they are shown in ascending order.\nAccuracy for both tasks is ~95%.\nThe network reaches that accuracy significantly faster than a separately trained CNN (i.e. requires less labels / is more unsupervised).\n\n\n\n\n(3.2) Omniglot\n\nThey train AIR on the Omniglot dataset (1.6k handwritten characters from 50 alphabets).\nThey allow the model to use up to 4 timesteps.\nThe model learns to reconstruct the images in timesteps that resemble strokes.\n\n\n(3.3) 3D Scenes\n\nHere, the generator p(x|z) is a 3D renderer, only q(z|x) must be approximated.\nThe model has to learn to count the objects and to estimate per object its identity (class) and pose.\nThey use \"finite-differencing\" to get gradients through the renderer and use \"score function estimators\" to get gradients with respect to discrete variables.\nThey first test with a setup where the object count is always 1. The network learns to accurately recover the object parameters.\nA similar \"normal\" network has much more problems with recovering the parameters, especially rotation, because the conditional probabilities are multi-modal. The lower bound maximization strategy seems to work better in those cases.\nIn a second experiment with multiple complex objects, AIR also achieves high reconstruction accuracy.\n\n\n\n The RNN is implemented via an LSTM. DAIR\n\nThe \"normal\" AIR model uses at every time step the image and the RNN's hidden layer to generate the next latent information (what object, where it is and whether it is present).\nDAIR uses that latent information to change the image at every time step and then use the difference (D) image for the next time step, i.e. DAIR can remove an object from the image after it has generated latent variables for it.\n\n The \"normal\" AIR model uses at every time step the image and the RNN's hidden layer to generate the next latent information (what object, where it is and whether it is present). DAIR uses that latent information to change the image at every time step and then use the difference (D) image for the next time step, i.e. DAIR can remove an object from the image after it has generated latent variables for it. (3.1) Multi-MNIST\n\nThey generate a dataset of images containing multiple MNIST digits.\nEach image contains 0 to 2 digits.\nAIR is trained on the dataset.\nIt learns without supervision a good attention scanning policy for the images (to \"hit\" all digits), to count the digits visible in the image and to use a matching number of time steps.\nDuring training, the model seems to first learn proper reconstruction of the digits and only then to do it with as few timesteps as possible.\n(3.1.1) Strong Generalization\n\nThey test the generalization capabilities of AIR.\nExtrapolation task: They generate images with 0 to 2 digits for training, then test on images with 3 digits. The model is unable to correctly count the digits (~0% accuracy).\nInterpolation task: They generate images with 0, 1 or 3 digits for training, then test on images with 2 digits. The model performs OK-ish (~60% accuracy).\nDAIR performs in both cases well (~80% for extrapolation, ~95% accuracy for interpolation).\n\n\n(3.1.2) Representational Power\n\nThey train AIR on images containing 0, 1 or 2 digits.\nThen they train a second network. That network takes the output of the first one and computes a) the sum of the digits and b) estimates whether they are shown in ascending order.\nAccuracy for both tasks is ~95%.\nThe network reaches that accuracy significantly faster than a separately trained CNN (i.e. requires less labels / is more unsupervised).\n\n\n\n They generate a dataset of images containing multiple MNIST digits. Each image contains 0 to 2 digits. AIR is trained on the dataset. It learns without supervision a good attention scanning policy for the images (to \"hit\" all digits), to count the digits visible in the image and to use a matching number of time steps. During training, the model seems to first learn proper reconstruction of the digits and only then to do it with as few timesteps as possible. (3.1.1) Strong Generalization\n\nThey test the generalization capabilities of AIR.\nExtrapolation task: They generate images with 0 to 2 digits for training, then test on images with 3 digits. The model is unable to correctly count the digits (~0% accuracy).\nInterpolation task: They generate images with 0, 1 or 3 digits for training, then test on images with 2 digits. The model performs OK-ish (~60% accuracy).\nDAIR performs in both cases well (~80% for extrapolation, ~95% accuracy for interpolation).\n\n They test the generalization capabilities of AIR. Extrapolation task: They generate images with 0 to 2 digits for training, then test on images with 3 digits. The model is unable to correctly count the digits (~0% accuracy). Interpolation task: They generate images with 0, 1 or 3 digits for training, then test on images with 2 digits. The model performs OK-ish (~60% accuracy). DAIR performs in both cases well (~80% for extrapolation, ~95% accuracy for interpolation). (3.1.2) Representational Power\n\nThey train AIR on images containing 0, 1 or 2 digits.\nThen they train a second network. That network takes the output of the first one and computes a) the sum of the digits and b) estimates whether they are shown in ascending order.\nAccuracy for both tasks is ~95%.\nThe network reaches that accuracy significantly faster than a separately trained CNN (i.e. requires less labels / is more unsupervised).\n\n They train AIR on images containing 0, 1 or 2 digits. Then they train a second network. That network takes the output of the first one and computes a) the sum of the digits and b) estimates whether they are shown in ascending order. Accuracy for both tasks is ~95%. The network reaches that accuracy significantly faster than a separately trained CNN (i.e. requires less labels / is more unsupervised). (3.2) Omniglot\n\nThey train AIR on the Omniglot dataset (1.6k handwritten characters from 50 alphabets).\nThey allow the model to use up to 4 timesteps.\nThe model learns to reconstruct the images in timesteps that resemble strokes.\n\n They train AIR on the Omniglot dataset (1.6k handwritten characters from 50 alphabets). They allow the model to use up to 4 timesteps. The model learns to reconstruct the images in timesteps that resemble strokes. (3.3) 3D Scenes\n\nHere, the generator p(x|z) is a 3D renderer, only q(z|x) must be approximated.\nThe model has to learn to count the objects and to estimate per object its identity (class) and pose.\nThey use \"finite-differencing\" to get gradients through the renderer and use \"score function estimators\" to get gradients with respect to discrete variables.\nThey first test with a setup where the object count is always 1. The network learns to accurately recover the object parameters.\nA similar \"normal\" network has much more problems with recovering the parameters, especially rotation, because the conditional probabilities are multi-modal. The lower bound maximization strategy seems to work better in those cases.\nIn a second experiment with multiple complex objects, AIR also achieves high reconstruction accuracy.\n\n Here, the generator p(x|z) is a 3D renderer, only q(z|x) must be approximated. The model has to learn to count the objects and to estimate per object its identity (class) and pose. They use \"finite-differencing\" to get gradients through the renderer and use \"score function estimators\" to get gradients with respect to discrete variables. They first test with a setup where the object count is always 1. The network learns to accurately recover the object parameters. A similar \"normal\" network has much more problems with recovering the parameters, especially rotation, because the conditional probabilities are multi-modal. The lower bound maximization strategy seems to work better in those cases. In a second experiment with multiple complex objects, AIR also achieves high reconstruction accuracy. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1603.08575"
    },
    "80": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/SqueezeNet.md",
        "transcript": "\nWhat\n\nThe authors train a variant of AlexNet that has significantly fewer parameters than the original network, while keeping the network's accuracy stable.\nAdvantages of this:\n\nMore efficient distributed training, because less parameters have to be transferred.\nMore efficient transfer via the internet, because the model's file size is smaller.\nPossibly less memory demand in production, because fewer parameters have to be kept in memory.\n\n\n\n The authors train a variant of AlexNet that has significantly fewer parameters than the original network, while keeping the network's accuracy stable. Advantages of this:\n\nMore efficient distributed training, because less parameters have to be transferred.\nMore efficient transfer via the internet, because the model's file size is smaller.\nPossibly less memory demand in production, because fewer parameters have to be kept in memory.\n\n More efficient distributed training, because less parameters have to be transferred. More efficient transfer via the internet, because the model's file size is smaller. Possibly less memory demand in production, because fewer parameters have to be kept in memory. \nHow\n\nThey define a Fire Module. A Fire Module contains of:\n\nSqueeze Module: A 1x1 convolution that reduces the number of channels (e.g. from 128x32x32 to 64x32x32).\nExpand Module: A 1x1 convolution and a 3x3 convolution, both applied to the output of the Squeeze Module. Their results are concatenated.\n\n\nUsing many 1x1 convolutions is advantageous, because they need less parameters than 3x3s.\nThey use ReLUs, only convolutions (no fully connected layers) and Dropout (50%, before the last convolution).\nThey use late maxpooling. They argue that applying pooling late - rather than early - improves accuracy while not needing more parameters.\nThey try residual connections:\n\nOne network without any residual connections (performed the worst).\nOne network with residual connections based on identity functions, but only between layers of same dimensionality (performed the best).\nOne network with residual connections based on identity functions and other residual connections with 1x1 convs (where dimensionality changed) (performance between the other two).\n\n\nThey use pruning from Deep Compression to reduce the parameters further. Pruning simply collects the 50% of all parameters of a layer that have the lowest values and sets them to zero. That creates a sparse matrix.\n\n They define a Fire Module. A Fire Module contains of:\n\nSqueeze Module: A 1x1 convolution that reduces the number of channels (e.g. from 128x32x32 to 64x32x32).\nExpand Module: A 1x1 convolution and a 3x3 convolution, both applied to the output of the Squeeze Module. Their results are concatenated.\n\n Squeeze Module: A 1x1 convolution that reduces the number of channels (e.g. from 128x32x32 to 64x32x32). Expand Module: A 1x1 convolution and a 3x3 convolution, both applied to the output of the Squeeze Module. Their results are concatenated. Using many 1x1 convolutions is advantageous, because they need less parameters than 3x3s. They use ReLUs, only convolutions (no fully connected layers) and Dropout (50%, before the last convolution). They use late maxpooling. They argue that applying pooling late - rather than early - improves accuracy while not needing more parameters. They try residual connections:\n\nOne network without any residual connections (performed the worst).\nOne network with residual connections based on identity functions, but only between layers of same dimensionality (performed the best).\nOne network with residual connections based on identity functions and other residual connections with 1x1 convs (where dimensionality changed) (performance between the other two).\n\n One network without any residual connections (performed the worst). One network with residual connections based on identity functions, but only between layers of same dimensionality (performed the best). One network with residual connections based on identity functions and other residual connections with 1x1 convs (where dimensionality changed) (performance between the other two). They use pruning from Deep Compression to reduce the parameters further. Pruning simply collects the 50% of all parameters of a layer that have the lowest values and sets them to zero. That creates a sparse matrix. \nResults\n\n50x parameter reduction of AlexNet (1.2M parameters before pruning, 0.4M after pruning).\n510x file size reduction of AlexNet (from 250mb to 0.47mb) when combined with Deep Compression.\nTop-1 accuracy remained stable.\nPruning apparently can be used safely, even after the network parameters have already been reduced significantly.\nWhile pruning was generally safe, they found that two of their later layers reacted quite sensitive to it. Adding parameters to these (instead of removing them) actually significantly improved accuracy.\nGenerally they found 1x1 convs to react more sensitive to pruning than 3x3s. Therefore they focused pruning on 3x3 convs.\nFirst pruning a network, then re-adding the pruned weights (initialized with 0s) and then retraining for some time significantly improved accuracy.\nThe network was rather resilient to significant channel reduction in the Squeeze Modules. Reducing to 25-50% of the original channels (e.g. 128x32x32 to 64x32x32) seemed to be a good choice.\nThe network was rather resilient to removing 3x3 convs and replacing them with 1x1 convs. A ratio of 2:1 to 1:1 (1x1 to 3x3) seemed to produce good results while mostly keeping the accuracy.\nAdding some residual connections between the Fire Modules improved the accuracy.\nAdding residual connections with identity functions and also residual connections with 1x1 convs (where dimensionality changed) improved the accuracy, but not as much as using only residual connections with identity functions (i.e. it's better to keep some modules without identity functions).\n\n 50x parameter reduction of AlexNet (1.2M parameters before pruning, 0.4M after pruning). 510x file size reduction of AlexNet (from 250mb to 0.47mb) when combined with Deep Compression. Top-1 accuracy remained stable. Pruning apparently can be used safely, even after the network parameters have already been reduced significantly. While pruning was generally safe, they found that two of their later layers reacted quite sensitive to it. Adding parameters to these (instead of removing them) actually significantly improved accuracy. Generally they found 1x1 convs to react more sensitive to pruning than 3x3s. Therefore they focused pruning on 3x3 convs. First pruning a network, then re-adding the pruned weights (initialized with 0s) and then retraining for some time significantly improved accuracy. The network was rather resilient to significant channel reduction in the Squeeze Modules. Reducing to 25-50% of the original channels (e.g. 128x32x32 to 64x32x32) seemed to be a good choice. The network was rather resilient to removing 3x3 convs and replacing them with 1x1 convs. A ratio of 2:1 to 1:1 (1x1 to 3x3) seemed to produce good results while mostly keeping the accuracy. Adding some residual connections between the Fire Modules improved the accuracy. Adding residual connections with identity functions and also residual connections with 1x1 convs (where dimensionality changed) improved the accuracy, but not as much as using only residual connections with identity functions (i.e. it's better to keep some modules without identity functions). \n(1) Introduction and Motivation\n\nAdvantages from having less parameters:\n\nMore efficient distributed training, because less data (parameters) have to be transfered.\nLess data to transfer to clients, e.g. when a model used by some app is updated.\nFPGAs often have hardly any memory, i.e. a model has to be small to be executed.\n\n\nTarget here: Find a CNN architecture with less parameters than an existing one but comparable accuracy.\n\n Advantages from having less parameters:\n\nMore efficient distributed training, because less data (parameters) have to be transfered.\nLess data to transfer to clients, e.g. when a model used by some app is updated.\nFPGAs often have hardly any memory, i.e. a model has to be small to be executed.\n\n More efficient distributed training, because less data (parameters) have to be transfered. Less data to transfer to clients, e.g. when a model used by some app is updated. FPGAs often have hardly any memory, i.e. a model has to be small to be executed. Target here: Find a CNN architecture with less parameters than an existing one but comparable accuracy. \n(2) Related Work\n\n(2.1) Model Compression\n\nSVD-method: Just apply SVD to the parameters of an existing model.\nNetwork Pruning: Replace parameters below threshold with zeros (-> sparse matrix), then retrain a bit.\nAdd quantization and huffman encoding to network pruning = Deep Compression.\n\n\n(2.2) CNN Microarchitecture\n\nThe term \"CNN Microarchitecture\" refers to the \"organization and dimensions of the individual modules\" (so an Inception module would have a complex CNN microarchitecture).\n\n\n(2.3) CNN Macroarchitecture\n\nCNN Macroarchitecture = \"big picture\" / organization of many modules in a network / general characteristics of the network, like depth\nAdding connections between modules can help (e.g. residual networks)\n\n\n(2.4) Neural Network Design Space Exploration\n\nApproaches for Design Space Exporation (DSE):\n\nBayesian Optimization, Simulated Annealing, Randomized Search, Genetic Algorithms\n\n\n\n\n\n (2.1) Model Compression\n\nSVD-method: Just apply SVD to the parameters of an existing model.\nNetwork Pruning: Replace parameters below threshold with zeros (-> sparse matrix), then retrain a bit.\nAdd quantization and huffman encoding to network pruning = Deep Compression.\n\n SVD-method: Just apply SVD to the parameters of an existing model. Network Pruning: Replace parameters below threshold with zeros (-> sparse matrix), then retrain a bit. Add quantization and huffman encoding to network pruning = Deep Compression. (2.2) CNN Microarchitecture\n\nThe term \"CNN Microarchitecture\" refers to the \"organization and dimensions of the individual modules\" (so an Inception module would have a complex CNN microarchitecture).\n\n The term \"CNN Microarchitecture\" refers to the \"organization and dimensions of the individual modules\" (so an Inception module would have a complex CNN microarchitecture). (2.3) CNN Macroarchitecture\n\nCNN Macroarchitecture = \"big picture\" / organization of many modules in a network / general characteristics of the network, like depth\nAdding connections between modules can help (e.g. residual networks)\n\n CNN Macroarchitecture = \"big picture\" / organization of many modules in a network / general characteristics of the network, like depth Adding connections between modules can help (e.g. residual networks) (2.4) Neural Network Design Space Exploration\n\nApproaches for Design Space Exporation (DSE):\n\nBayesian Optimization, Simulated Annealing, Randomized Search, Genetic Algorithms\n\n\n\n Approaches for Design Space Exporation (DSE):\n\nBayesian Optimization, Simulated Annealing, Randomized Search, Genetic Algorithms\n\n Bayesian Optimization, Simulated Annealing, Randomized Search, Genetic Algorithms \n(3) SqueezeNet: preserving accuracy with few parameters\n\n(3.1) Architectural Design Strategies\n\nA conv layer with N filters applied to CxHxW input (e.g. 3x128x128 for a possible first layer) with kernel size kHxkW (e.g. 3x3) has N*C*kH*kW parameters.\nSo one way to reduce the parameters is to decrease kH and kW, e.g. from 3x3 to 1x1 (reduces parameters by a factor of 9).\nA second way is to reduce the number of channels (C), e.g. by using 1x1 convs before the 3x3 ones.\nThey think that accuracy can be improved by performing downsampling later in the network (if parameter count is kept constant).\n\n\n(3.2) The Fire Module\n\nThe Fire Module has two components:\n\nSqueeze Module:\n\nOne layer of 1x1 convs\n\n\nExpand Module:\n\nConcat the results of:\n\nOne layer of 1x1 convs\nOne layer of 3x3 convs\n\n\n\n\n\n\nThe Squeeze Module decreases the number of input channels significantly.\nThe Expand Module then increases the number of input channels again.\n\n\n(3.3) The SqueezeNet architecture\n\nOne standalone conv, then several fire modules, then a standalone conv, then global average pooling, then softmax.\nThree late max pooling laters.\nGradual increase of filter numbers.\n(3.3.1) Other SqueezeNet details\n\nReLU activations\nDropout before the last conv layer.\nNo linear layers.\n\n\n\n\n\n (3.1) Architectural Design Strategies\n\nA conv layer with N filters applied to CxHxW input (e.g. 3x128x128 for a possible first layer) with kernel size kHxkW (e.g. 3x3) has N*C*kH*kW parameters.\nSo one way to reduce the parameters is to decrease kH and kW, e.g. from 3x3 to 1x1 (reduces parameters by a factor of 9).\nA second way is to reduce the number of channels (C), e.g. by using 1x1 convs before the 3x3 ones.\nThey think that accuracy can be improved by performing downsampling later in the network (if parameter count is kept constant).\n\n A conv layer with N filters applied to CxHxW input (e.g. 3x128x128 for a possible first layer) with kernel size kHxkW (e.g. 3x3) has N*C*kH*kW parameters. So one way to reduce the parameters is to decrease kH and kW, e.g. from 3x3 to 1x1 (reduces parameters by a factor of 9). A second way is to reduce the number of channels (C), e.g. by using 1x1 convs before the 3x3 ones. They think that accuracy can be improved by performing downsampling later in the network (if parameter count is kept constant). (3.2) The Fire Module\n\nThe Fire Module has two components:\n\nSqueeze Module:\n\nOne layer of 1x1 convs\n\n\nExpand Module:\n\nConcat the results of:\n\nOne layer of 1x1 convs\nOne layer of 3x3 convs\n\n\n\n\n\n\nThe Squeeze Module decreases the number of input channels significantly.\nThe Expand Module then increases the number of input channels again.\n\n The Fire Module has two components:\n\nSqueeze Module:\n\nOne layer of 1x1 convs\n\n\nExpand Module:\n\nConcat the results of:\n\nOne layer of 1x1 convs\nOne layer of 3x3 convs\n\n\n\n\n\n Squeeze Module:\n\nOne layer of 1x1 convs\n\n One layer of 1x1 convs Expand Module:\n\nConcat the results of:\n\nOne layer of 1x1 convs\nOne layer of 3x3 convs\n\n\n\n Concat the results of:\n\nOne layer of 1x1 convs\nOne layer of 3x3 convs\n\n One layer of 1x1 convs One layer of 3x3 convs The Squeeze Module decreases the number of input channels significantly. The Expand Module then increases the number of input channels again. (3.3) The SqueezeNet architecture\n\nOne standalone conv, then several fire modules, then a standalone conv, then global average pooling, then softmax.\nThree late max pooling laters.\nGradual increase of filter numbers.\n(3.3.1) Other SqueezeNet details\n\nReLU activations\nDropout before the last conv layer.\nNo linear layers.\n\n\n\n One standalone conv, then several fire modules, then a standalone conv, then global average pooling, then softmax. Three late max pooling laters. Gradual increase of filter numbers. (3.3.1) Other SqueezeNet details\n\nReLU activations\nDropout before the last conv layer.\nNo linear layers.\n\n ReLU activations Dropout before the last conv layer. No linear layers. \n(4) Evaluation of SqueezeNet\n\nResults of competing methods:\n\nSVD: 5x compression, 56% top-1 accuracy\nPruning: 9x compression, 57.2% top-1 accuracy\nDeep Compression: 35x compression, ~57% top-1 accuracy\n\n\nSqueezeNet: 50x compression, ~57% top-1 accuracy\nSqueezeNet combines low parameter counts with Deep Compression.\nThe accuracy does not go down because of that, i.e. apparently Deep Compression can even be applied to small models without giving up on performance.\n\n Results of competing methods:\n\nSVD: 5x compression, 56% top-1 accuracy\nPruning: 9x compression, 57.2% top-1 accuracy\nDeep Compression: 35x compression, ~57% top-1 accuracy\n\n SVD: 5x compression, 56% top-1 accuracy Pruning: 9x compression, 57.2% top-1 accuracy Deep Compression: 35x compression, ~57% top-1 accuracy SqueezeNet: 50x compression, ~57% top-1 accuracy SqueezeNet combines low parameter counts with Deep Compression. The accuracy does not go down because of that, i.e. apparently Deep Compression can even be applied to small models without giving up on performance. \n(5) CNN Microarchitecture Design Space Exploration\n\n(5.1) CNN Microarchitecture metaparameters\n\nblabla we test various values for this and that parameter\n\n\n(5.2) Squeeze Ratio\n\nIn a Fire Module there is first a Squeeze Module and then an Expand Module. The Squeeze Module decreases the number of input channels to which 1x1 and 3x3 both are applied (at the same time).\nThey analyzed how far you can go down with the Sqeeze Module by training multiple networks and calculating the top-5 accuracy for each of them.\nThe accuracy by Squeeze Ratio (percentage of input channels kept in 1x1 squeeze, i.e. 50% = reduced by half, e.g. from 128 to 64):\n\n12%: ~80% top-5 accuracy\n25%: ~82% top-5 accuracy\n50%: ~85% top-5 accuracy\n75%: ~86% top-5 accuracy\n100%: ~86% top-5 accuracy\n\n\n\n\n(5.3) Trading off 1x1 and 3x3 filters\n\nSimilar to the Squeeze Ratio, they analyze the optimal ratio of 1x1 filters to 3x3 filters.\nE.g. 50% would mean that half of all filters in each Fire Module are 1x1 filters.\nResults:\n\n01%: ~76% top-5 accuracy\n12%: ~80% top-5 accuracy\n25%: ~82% top-5 accuracy\n50%: ~85% top-5 accuracy\n75%: ~85% top-5 accuracy\n99%: ~85% top-5 accuracy\n\n\n\n\n\n (5.1) CNN Microarchitecture metaparameters\n\nblabla we test various values for this and that parameter\n\n blabla we test various values for this and that parameter (5.2) Squeeze Ratio\n\nIn a Fire Module there is first a Squeeze Module and then an Expand Module. The Squeeze Module decreases the number of input channels to which 1x1 and 3x3 both are applied (at the same time).\nThey analyzed how far you can go down with the Sqeeze Module by training multiple networks and calculating the top-5 accuracy for each of them.\nThe accuracy by Squeeze Ratio (percentage of input channels kept in 1x1 squeeze, i.e. 50% = reduced by half, e.g. from 128 to 64):\n\n12%: ~80% top-5 accuracy\n25%: ~82% top-5 accuracy\n50%: ~85% top-5 accuracy\n75%: ~86% top-5 accuracy\n100%: ~86% top-5 accuracy\n\n\n\n In a Fire Module there is first a Squeeze Module and then an Expand Module. The Squeeze Module decreases the number of input channels to which 1x1 and 3x3 both are applied (at the same time). They analyzed how far you can go down with the Sqeeze Module by training multiple networks and calculating the top-5 accuracy for each of them. The accuracy by Squeeze Ratio (percentage of input channels kept in 1x1 squeeze, i.e. 50% = reduced by half, e.g. from 128 to 64):\n\n12%: ~80% top-5 accuracy\n25%: ~82% top-5 accuracy\n50%: ~85% top-5 accuracy\n75%: ~86% top-5 accuracy\n100%: ~86% top-5 accuracy\n\n 12%: ~80% top-5 accuracy 25%: ~82% top-5 accuracy 50%: ~85% top-5 accuracy 75%: ~86% top-5 accuracy 100%: ~86% top-5 accuracy (5.3) Trading off 1x1 and 3x3 filters\n\nSimilar to the Squeeze Ratio, they analyze the optimal ratio of 1x1 filters to 3x3 filters.\nE.g. 50% would mean that half of all filters in each Fire Module are 1x1 filters.\nResults:\n\n01%: ~76% top-5 accuracy\n12%: ~80% top-5 accuracy\n25%: ~82% top-5 accuracy\n50%: ~85% top-5 accuracy\n75%: ~85% top-5 accuracy\n99%: ~85% top-5 accuracy\n\n\n\n Similar to the Squeeze Ratio, they analyze the optimal ratio of 1x1 filters to 3x3 filters. E.g. 50% would mean that half of all filters in each Fire Module are 1x1 filters. Results:\n\n01%: ~76% top-5 accuracy\n12%: ~80% top-5 accuracy\n25%: ~82% top-5 accuracy\n50%: ~85% top-5 accuracy\n75%: ~85% top-5 accuracy\n99%: ~85% top-5 accuracy\n\n 01%: ~76% top-5 accuracy 12%: ~80% top-5 accuracy 25%: ~82% top-5 accuracy 50%: ~85% top-5 accuracy 75%: ~85% top-5 accuracy 99%: ~85% top-5 accuracy \n(6) CNN Macroarchitecture Design Space Exploration\n\nThey compare the following networks:\n\n(1) Without residual connections\n(2) With residual connections between modules of same dimensionality\n(3) With residual connections between all modules (except pooling layers) using 1x1 convs (instead of identity functions) where needed\n\n\nAdding residual connections (2) improved top-1 accuracy from 57.5% to 60.4% without any new parameters.\nAdding complex residual connections (3) worsed top-1 accuracy again to 58.8%, while adding new parameters.\n\n They compare the following networks:\n\n(1) Without residual connections\n(2) With residual connections between modules of same dimensionality\n(3) With residual connections between all modules (except pooling layers) using 1x1 convs (instead of identity functions) where needed\n\n (1) Without residual connections (2) With residual connections between modules of same dimensionality (3) With residual connections between all modules (except pooling layers) using 1x1 convs (instead of identity functions) where needed Adding residual connections (2) improved top-1 accuracy from 57.5% to 60.4% without any new parameters. Adding complex residual connections (3) worsed top-1 accuracy again to 58.8%, while adding new parameters. \n(7) Model Compression Design Space Exploration\n\n(7.1) Sensitivity Analysis: Where to Prune or Add parameters\n\nThey went through all layers (including each one in the Fire Modules).\nIn each layer they set the 50% smallest weights to zero (pruning) and measured the effect on the top-5 accuracy.\nIt turns out that doing that has basically no influence on the top-5 accuracy in most layers.\nTwo layers towards the end however had significant influence (accuracy went down by 5-10%).\nAdding parameters to these layers improved top-1 accuracy from 57.5% to 59.5%.\nGenerally they found 1x1 layers to be more sensitive than 3x3 layers so they pruned them less aggressively.\n\n\n(7.2) Improving Accuracy by Densifying Sparse Models\n\nThey found that first pruning a model and then retraining it again (initializing the pruned weights to 0) leads to higher accuracy.\nThey could improve top-1 accuracy by 4.3% in this way.\n\n\n\n (7.1) Sensitivity Analysis: Where to Prune or Add parameters\n\nThey went through all layers (including each one in the Fire Modules).\nIn each layer they set the 50% smallest weights to zero (pruning) and measured the effect on the top-5 accuracy.\nIt turns out that doing that has basically no influence on the top-5 accuracy in most layers.\nTwo layers towards the end however had significant influence (accuracy went down by 5-10%).\nAdding parameters to these layers improved top-1 accuracy from 57.5% to 59.5%.\nGenerally they found 1x1 layers to be more sensitive than 3x3 layers so they pruned them less aggressively.\n\n They went through all layers (including each one in the Fire Modules). In each layer they set the 50% smallest weights to zero (pruning) and measured the effect on the top-5 accuracy. It turns out that doing that has basically no influence on the top-5 accuracy in most layers. Two layers towards the end however had significant influence (accuracy went down by 5-10%). Adding parameters to these layers improved top-1 accuracy from 57.5% to 59.5%. Generally they found 1x1 layers to be more sensitive than 3x3 layers so they pruned them less aggressively. (7.2) Improving Accuracy by Densifying Sparse Models\n\nThey found that first pruning a model and then retraining it again (initializing the pruned weights to 0) leads to higher accuracy.\nThey could improve top-1 accuracy by 4.3% in this way.\n\n They found that first pruning a model and then retraining it again (initializing the pruned weights to 0) leads to higher accuracy. They could improve top-1 accuracy by 4.3% in this way. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1602.07360v3"
    },
    "81": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Noisy_Activation_Functions.md",
        "transcript": "\nWhat\n\nCertain activation functions, mainly sigmoid, tanh, hard-sigmoid and hard-tanh can saturate.\nThat means that their gradient is either flat 0 after threshold values (e.g. -1 and +1) or that it approaches zero for high/low values.\nIf there's no gradient, training becomes slow or stops completely.\nThat's a problem, because sigmoid, tanh, hard-sigmoid and hard-tanh are still often used in some models, like LSTMs, GRUs or Neural Turing Machines.\nTo fix the saturation problem, they add noise to the output of the activation functions.\nThe noise increases as the unit saturates.\nIntuitively, once the unit is saturating, it will occasionally \"test\" an activation in the non-saturating regime to see if that output performs better.\n\n Certain activation functions, mainly sigmoid, tanh, hard-sigmoid and hard-tanh can saturate. That means that their gradient is either flat 0 after threshold values (e.g. -1 and +1) or that it approaches zero for high/low values. If there's no gradient, training becomes slow or stops completely. That's a problem, because sigmoid, tanh, hard-sigmoid and hard-tanh are still often used in some models, like LSTMs, GRUs or Neural Turing Machines. To fix the saturation problem, they add noise to the output of the activation functions. The noise increases as the unit saturates. Intuitively, once the unit is saturating, it will occasionally \"test\" an activation in the non-saturating regime to see if that output performs better. \nHow\n\nThe basic formula is: phi(x,z) = alpha*h(x) + (1-alpha)u(x) + d(x)std(x)epsilon\nVariables in that formula:\n\nNon-linear part alpha*h(x):\n\nalpha: A constant hyperparameter that determines the \"direction\" of the noise and the slope. Values below 1.0 let the noise point away from the unsaturated regime. Values <=1.0 let it point towards the unsaturated regime (higher alpha = stronger noise).\nh(x): The original activation function.\n\n\nLinear part (1-alpha)u(x):\n\nu(x): First-order Taylor expansion of h(x).\n\nFor sigmoid: u(x) = 0.25x + 0.5\nFor tanh: u(x) = x\nFor hard-sigmoid: u(x) = max(min(0.25x+0.5, 1), 0)\nFor hard-tanh: u(x) = max(min(x, 1), -1)\n\n\n\n\nNoise/Stochastic part d(x)std(x)epsilon:\n\nd(x) = -sgn(x)sgn(1-alpha): Changes the \"direction\" of the noise.\nstd(x) = c(sigmoid(p*v(x))-0.5)^2 = c(sigmoid(p*(h(x)-u(x)))-0.5)^2\n\nc is a hyperparameter that controls the scale of the standard deviation of the noise.\np controls the magnitude of the noise. Due to the sigmoid(y)-0.5 this can influence the sign. p is learned.\n\n\nepsilon: A noise creating random variable. Usually either a Gaussian or the positive half of a Gaussian (i.e. z or |z|).\n\n\n\n\nThe hyperparameter c can be initialized at a high value and then gradually decreased over time. That would be comparable to simulated annealing.\nNoise could also be applied to the input, i.e. h(x) becomes h(x + noise).\n\n The basic formula is: phi(x,z) = alpha*h(x) + (1-alpha)u(x) + d(x)std(x)epsilon Variables in that formula:\n\nNon-linear part alpha*h(x):\n\nalpha: A constant hyperparameter that determines the \"direction\" of the noise and the slope. Values below 1.0 let the noise point away from the unsaturated regime. Values <=1.0 let it point towards the unsaturated regime (higher alpha = stronger noise).\nh(x): The original activation function.\n\n\nLinear part (1-alpha)u(x):\n\nu(x): First-order Taylor expansion of h(x).\n\nFor sigmoid: u(x) = 0.25x + 0.5\nFor tanh: u(x) = x\nFor hard-sigmoid: u(x) = max(min(0.25x+0.5, 1), 0)\nFor hard-tanh: u(x) = max(min(x, 1), -1)\n\n\n\n\nNoise/Stochastic part d(x)std(x)epsilon:\n\nd(x) = -sgn(x)sgn(1-alpha): Changes the \"direction\" of the noise.\nstd(x) = c(sigmoid(p*v(x))-0.5)^2 = c(sigmoid(p*(h(x)-u(x)))-0.5)^2\n\nc is a hyperparameter that controls the scale of the standard deviation of the noise.\np controls the magnitude of the noise. Due to the sigmoid(y)-0.5 this can influence the sign. p is learned.\n\n\nepsilon: A noise creating random variable. Usually either a Gaussian or the positive half of a Gaussian (i.e. z or |z|).\n\n\n\n Non-linear part alpha*h(x):\n\nalpha: A constant hyperparameter that determines the \"direction\" of the noise and the slope. Values below 1.0 let the noise point away from the unsaturated regime. Values <=1.0 let it point towards the unsaturated regime (higher alpha = stronger noise).\nh(x): The original activation function.\n\n alpha: A constant hyperparameter that determines the \"direction\" of the noise and the slope. Values below 1.0 let the noise point away from the unsaturated regime. Values <=1.0 let it point towards the unsaturated regime (higher alpha = stronger noise). h(x): The original activation function. Linear part (1-alpha)u(x):\n\nu(x): First-order Taylor expansion of h(x).\n\nFor sigmoid: u(x) = 0.25x + 0.5\nFor tanh: u(x) = x\nFor hard-sigmoid: u(x) = max(min(0.25x+0.5, 1), 0)\nFor hard-tanh: u(x) = max(min(x, 1), -1)\n\n\n\n u(x): First-order Taylor expansion of h(x).\n\nFor sigmoid: u(x) = 0.25x + 0.5\nFor tanh: u(x) = x\nFor hard-sigmoid: u(x) = max(min(0.25x+0.5, 1), 0)\nFor hard-tanh: u(x) = max(min(x, 1), -1)\n\n For sigmoid: u(x) = 0.25x + 0.5 For tanh: u(x) = x For hard-sigmoid: u(x) = max(min(0.25x+0.5, 1), 0) For hard-tanh: u(x) = max(min(x, 1), -1) Noise/Stochastic part d(x)std(x)epsilon:\n\nd(x) = -sgn(x)sgn(1-alpha): Changes the \"direction\" of the noise.\nstd(x) = c(sigmoid(p*v(x))-0.5)^2 = c(sigmoid(p*(h(x)-u(x)))-0.5)^2\n\nc is a hyperparameter that controls the scale of the standard deviation of the noise.\np controls the magnitude of the noise. Due to the sigmoid(y)-0.5 this can influence the sign. p is learned.\n\n\nepsilon: A noise creating random variable. Usually either a Gaussian or the positive half of a Gaussian (i.e. z or |z|).\n\n d(x) = -sgn(x)sgn(1-alpha): Changes the \"direction\" of the noise. std(x) = c(sigmoid(p*v(x))-0.5)^2 = c(sigmoid(p*(h(x)-u(x)))-0.5)^2\n\nc is a hyperparameter that controls the scale of the standard deviation of the noise.\np controls the magnitude of the noise. Due to the sigmoid(y)-0.5 this can influence the sign. p is learned.\n\n c is a hyperparameter that controls the scale of the standard deviation of the noise. p controls the magnitude of the noise. Due to the sigmoid(y)-0.5 this can influence the sign. p is learned. epsilon: A noise creating random variable. Usually either a Gaussian or the positive half of a Gaussian (i.e. z or |z|). The hyperparameter c can be initialized at a high value and then gradually decreased over time. That would be comparable to simulated annealing. Noise could also be applied to the input, i.e. h(x) becomes h(x + noise). \nResults\n\nThey replaced sigmoid/tanh/hard-sigmoid/hard-tanh units in various experiments (without further optimizations).\nThe experiments were:\n\nLearn to execute source code (LSTM?)\nLanguage model from Penntreebank (2-layer LSTM)\nNeural Machine Translation engine trained on Europarl (LSTM?)\nImage caption generation with soft attention trained on Flickr8k (LSTM)\nCounting unique integers in a sequence of integers (LSTM)\nAssociative recall (Neural Turing Machine)\n\n\nNoisy activations practically always led to a small or moderate improvement in resulting accuracy/NLL/BLEU.\nIn one experiment annealed noise significantly outperformed unannealed noise, even beating careful curriculum learning. (Somehow there are not more experiments about that.)\nThe Neural Turing Machine learned far faster with noisy activations and also converged to a much better solution.\n\n They replaced sigmoid/tanh/hard-sigmoid/hard-tanh units in various experiments (without further optimizations). The experiments were:\n\nLearn to execute source code (LSTM?)\nLanguage model from Penntreebank (2-layer LSTM)\nNeural Machine Translation engine trained on Europarl (LSTM?)\nImage caption generation with soft attention trained on Flickr8k (LSTM)\nCounting unique integers in a sequence of integers (LSTM)\nAssociative recall (Neural Turing Machine)\n\n Learn to execute source code (LSTM?) Language model from Penntreebank (2-layer LSTM) Neural Machine Translation engine trained on Europarl (LSTM?) Image caption generation with soft attention trained on Flickr8k (LSTM) Counting unique integers in a sequence of integers (LSTM) Associative recall (Neural Turing Machine) Noisy activations practically always led to a small or moderate improvement in resulting accuracy/NLL/BLEU. In one experiment annealed noise significantly outperformed unannealed noise, even beating careful curriculum learning. (Somehow there are not more experiments about that.) The Neural Turing Machine learned far faster with noisy activations and also converged to a much better solution. \n(1) Introduction\n\nReLU and Maxout activation functions have improved the capabilities of training deep networks.\nPreviously, tanh and sigmoid were used, which were only suited for shallow networks, because they saturate, which kills the gradient.\nThey suggest a different avenue: Use saturating nonlinearities, but inject noise when they start to saturate (and let the network learn how much noise is \"good\").\nThe noise allows to train deep networks with saturating activation functions.\nMany current architectures (LSTMs, GRUs, Neural Turing Machines, ...) require \"hard\" decisions (yes/no). But they use \"soft\" activation functions to implement those, because hard functions lack gradient.\nThe soft activation functions can still saturate (no more gradient) and don't match the nature of the binary decision problem. So it would be good to replace them with something better.\nThey instead use hard activation functions and compensate for the lack of gradient by using noise (during training).\nNetworks with hard activation functions outperform those with soft ones.\n\n ReLU and Maxout activation functions have improved the capabilities of training deep networks. Previously, tanh and sigmoid were used, which were only suited for shallow networks, because they saturate, which kills the gradient. They suggest a different avenue: Use saturating nonlinearities, but inject noise when they start to saturate (and let the network learn how much noise is \"good\"). The noise allows to train deep networks with saturating activation functions. Many current architectures (LSTMs, GRUs, Neural Turing Machines, ...) require \"hard\" decisions (yes/no). But they use \"soft\" activation functions to implement those, because hard functions lack gradient. The soft activation functions can still saturate (no more gradient) and don't match the nature of the binary decision problem. So it would be good to replace them with something better. They instead use hard activation functions and compensate for the lack of gradient by using noise (during training). Networks with hard activation functions outperform those with soft ones. \n(2) Saturating Activation Functions\n\nActivation Function = A function that maps a real value to a new real value and is differentiable almost everywhere.\nRight saturation = The gradient of an activation function becomes 0 if the input value goes towards infinity.\nLeft saturation = The gradient of an activation function becomes 0 if the input value goes towards -infinity.\nSaturation = A activation function saturates if it right-saturates and left-saturates.\nHard saturation = If there is a constant c for which for which the gradient becomes 0.\nSoft saturation = If there is no constant, i.e. the input value must become +/- infinity.\nSoft saturating activation functions can be converted to hard saturating ones by using a first-order Taylor expansion and then clipping the values to the required range (e.g. 0 to 1).\nA hard activating tanh is just f(x) = x. With clipping to [-1, 1]: max(min(f(x), 1), -1).\nThe gradient for hard activation functions is 0 above/below certain constants, which will make training significantly more challenging.\nhard-sigmoid, sigmoid and tanh are contractive mappings, hard-tanh for some reason only when it's greater than the threshold.\nThe fixed-point for tanh is 0, for the others !=0. That can have influences on the training performance.\n\n Activation Function = A function that maps a real value to a new real value and is differentiable almost everywhere. Right saturation = The gradient of an activation function becomes 0 if the input value goes towards infinity. Left saturation = The gradient of an activation function becomes 0 if the input value goes towards -infinity. Saturation = A activation function saturates if it right-saturates and left-saturates. Hard saturation = If there is a constant c for which for which the gradient becomes 0. Soft saturation = If there is no constant, i.e. the input value must become +/- infinity. Soft saturating activation functions can be converted to hard saturating ones by using a first-order Taylor expansion and then clipping the values to the required range (e.g. 0 to 1). A hard activating tanh is just f(x) = x. With clipping to [-1, 1]: max(min(f(x), 1), -1). The gradient for hard activation functions is 0 above/below certain constants, which will make training significantly more challenging. hard-sigmoid, sigmoid and tanh are contractive mappings, hard-tanh for some reason only when it's greater than the threshold. The fixed-point for tanh is 0, for the others !=0. That can have influences on the training performance. \n(3) Annealing with Noisy Activation Functions\n\nSuppose that there is an activation function like hard-sigmoid or hard-tanh with additional noise (iid, mean=0, variance=std^2).\nIf the noise's std is 0 then the activation function is the original, deterministic one.\nIf the noise's std is very high then the derivatives and gradient become high too. The noise then \"drowns\" signal and the optimizer just moves randomly through the parameter space.\nLet the signal to noise ratio be SNR = std_signal / std_noise. So if SNR is low then noise drowns the signal and exploration is random.\nBy letting SNR grow (i.e. decreaseing std_noise) we switch the model to fine tuning mode (less coarse exploration).\nThat is similar to simulated annealing, where noise is also gradually decreased to focus on better and better regions of the parameter space.\n\n Suppose that there is an activation function like hard-sigmoid or hard-tanh with additional noise (iid, mean=0, variance=std^2). If the noise's std is 0 then the activation function is the original, deterministic one. If the noise's std is very high then the derivatives and gradient become high too. The noise then \"drowns\" signal and the optimizer just moves randomly through the parameter space. Let the signal to noise ratio be SNR = std_signal / std_noise. So if SNR is low then noise drowns the signal and exploration is random. By letting SNR grow (i.e. decreaseing std_noise) we switch the model to fine tuning mode (less coarse exploration). That is similar to simulated annealing, where noise is also gradually decreased to focus on better and better regions of the parameter space. \n(4) Adding Noise when the Unit Saturate\n\nThis approach does not always add the same noise. Instead, noise is added proportinally to the saturation magnitude. More saturation, more noise.\nThat results in a clean signal in \"good\" regimes (non-saturation, strong gradients) and a noisy signal in \"bad\" regimes (saturation).\nBasic activation function with noise: phi(x, z) = h(x) + (mu + std(x)*z), where h(x) is the saturating activation function, mu is the mean of the noise, std is the standard deviation of the noise and z is a random variable.\nIdeally the noise is unbiased so that the expectation values of phi(x,z) and h(x) are the same.\nstd(x) should take higher values as h(x) enters the saturating regime.\nTo calculate how \"saturating\" a activation function is, one can v(x) = h(x) - u(x), where u(x) is the first-order Taylor expansion of h(x).\nEmpirically they found that a good choice is std(x) = c(sigmoid(p*v(x)) - 0.5)^2 where c is a hyperparameter and p is learned.\n(4.1) Derivatives in the Saturated Regime\n\nFor values below the threshold, the gradient of the noisy activation function is identical to the normal activation function.\nFor values above the threshold, the gradient of the noisy activation function is phi'(x,z) = std'(x)*z. (Assuming that z is unbiased so that mu=0.)\n\n\n(4.2) Pushing Activations towards the Linear Regime\n\nIn saturated regimes, one would like to have more of the noise point towards the unsaturated regimes than away from them (i.e. let the model try often whether the unsaturated regimes might be better).\nTo achieve this they use the formula phi(x,z) = alpha*h(x) + (1-alpha)u(x) + d(x)std(x)epsilon\n\nalpha: A constant hyperparameter that determines the \"direction\" of the noise and the slope. Values below 1.0 let the noise point away from the unsaturated regime. Values <=1.0 let it point towards the unsaturated regime (higher alpha = stronger noise).\nh(x): The original activation function.\nu(x): First-order Taylor expansion of h(x).\nd(x) = -sgn(x)sgn(1-alpha): Changes the \"direction\" of the noise.\nstd(x) = c(sigmoid(p*v(x))-0.5)^2 = c(sigmoid(p*(h(x)-u(x)))-0.5)^2 with c being a hyperparameter and p learned.\nepsilon: Either z or |z|. If z is a Gaussian, then |z| is called \"half-normal\" while just z is called \"normal\". Half-normal lets the noise only point towards one \"direction\" (towards the unsaturated regime or away from it), while normal noise lets it point in both directions (with the slope being influenced by alpha).\n\n\nThe formula can be split into three parts:\n\nalpha*h(x): Nonlinear part.\n(1-alpha)u(x): Linear part.\nd(x)std(x)epsilon: Stochastic part.\n\n\nEach of these parts resembles a path along which gradient can flow through the network.\nDuring test time the activation function is made deterministic by using its expectation value: E[phi(x,z)] = alpha*h(x) + (1-alpha)u(x) + d(x)std(x)E[epsilon].\nIf z is half-normal then E[epsilon] = sqrt(2/pi). If z is normal then E[epsilon] = 0.\n\n\n\n This approach does not always add the same noise. Instead, noise is added proportinally to the saturation magnitude. More saturation, more noise. That results in a clean signal in \"good\" regimes (non-saturation, strong gradients) and a noisy signal in \"bad\" regimes (saturation). Basic activation function with noise: phi(x, z) = h(x) + (mu + std(x)*z), where h(x) is the saturating activation function, mu is the mean of the noise, std is the standard deviation of the noise and z is a random variable. Ideally the noise is unbiased so that the expectation values of phi(x,z) and h(x) are the same. std(x) should take higher values as h(x) enters the saturating regime. To calculate how \"saturating\" a activation function is, one can v(x) = h(x) - u(x), where u(x) is the first-order Taylor expansion of h(x). Empirically they found that a good choice is std(x) = c(sigmoid(p*v(x)) - 0.5)^2 where c is a hyperparameter and p is learned. (4.1) Derivatives in the Saturated Regime\n\nFor values below the threshold, the gradient of the noisy activation function is identical to the normal activation function.\nFor values above the threshold, the gradient of the noisy activation function is phi'(x,z) = std'(x)*z. (Assuming that z is unbiased so that mu=0.)\n\n For values below the threshold, the gradient of the noisy activation function is identical to the normal activation function. For values above the threshold, the gradient of the noisy activation function is phi'(x,z) = std'(x)*z. (Assuming that z is unbiased so that mu=0.) (4.2) Pushing Activations towards the Linear Regime\n\nIn saturated regimes, one would like to have more of the noise point towards the unsaturated regimes than away from them (i.e. let the model try often whether the unsaturated regimes might be better).\nTo achieve this they use the formula phi(x,z) = alpha*h(x) + (1-alpha)u(x) + d(x)std(x)epsilon\n\nalpha: A constant hyperparameter that determines the \"direction\" of the noise and the slope. Values below 1.0 let the noise point away from the unsaturated regime. Values <=1.0 let it point towards the unsaturated regime (higher alpha = stronger noise).\nh(x): The original activation function.\nu(x): First-order Taylor expansion of h(x).\nd(x) = -sgn(x)sgn(1-alpha): Changes the \"direction\" of the noise.\nstd(x) = c(sigmoid(p*v(x))-0.5)^2 = c(sigmoid(p*(h(x)-u(x)))-0.5)^2 with c being a hyperparameter and p learned.\nepsilon: Either z or |z|. If z is a Gaussian, then |z| is called \"half-normal\" while just z is called \"normal\". Half-normal lets the noise only point towards one \"direction\" (towards the unsaturated regime or away from it), while normal noise lets it point in both directions (with the slope being influenced by alpha).\n\n\nThe formula can be split into three parts:\n\nalpha*h(x): Nonlinear part.\n(1-alpha)u(x): Linear part.\nd(x)std(x)epsilon: Stochastic part.\n\n\nEach of these parts resembles a path along which gradient can flow through the network.\nDuring test time the activation function is made deterministic by using its expectation value: E[phi(x,z)] = alpha*h(x) + (1-alpha)u(x) + d(x)std(x)E[epsilon].\nIf z is half-normal then E[epsilon] = sqrt(2/pi). If z is normal then E[epsilon] = 0.\n\n In saturated regimes, one would like to have more of the noise point towards the unsaturated regimes than away from them (i.e. let the model try often whether the unsaturated regimes might be better). To achieve this they use the formula phi(x,z) = alpha*h(x) + (1-alpha)u(x) + d(x)std(x)epsilon\n\nalpha: A constant hyperparameter that determines the \"direction\" of the noise and the slope. Values below 1.0 let the noise point away from the unsaturated regime. Values <=1.0 let it point towards the unsaturated regime (higher alpha = stronger noise).\nh(x): The original activation function.\nu(x): First-order Taylor expansion of h(x).\nd(x) = -sgn(x)sgn(1-alpha): Changes the \"direction\" of the noise.\nstd(x) = c(sigmoid(p*v(x))-0.5)^2 = c(sigmoid(p*(h(x)-u(x)))-0.5)^2 with c being a hyperparameter and p learned.\nepsilon: Either z or |z|. If z is a Gaussian, then |z| is called \"half-normal\" while just z is called \"normal\". Half-normal lets the noise only point towards one \"direction\" (towards the unsaturated regime or away from it), while normal noise lets it point in both directions (with the slope being influenced by alpha).\n\n alpha: A constant hyperparameter that determines the \"direction\" of the noise and the slope. Values below 1.0 let the noise point away from the unsaturated regime. Values <=1.0 let it point towards the unsaturated regime (higher alpha = stronger noise). h(x): The original activation function. u(x): First-order Taylor expansion of h(x). d(x) = -sgn(x)sgn(1-alpha): Changes the \"direction\" of the noise. std(x) = c(sigmoid(p*v(x))-0.5)^2 = c(sigmoid(p*(h(x)-u(x)))-0.5)^2 with c being a hyperparameter and p learned. epsilon: Either z or |z|. If z is a Gaussian, then |z| is called \"half-normal\" while just z is called \"normal\". Half-normal lets the noise only point towards one \"direction\" (towards the unsaturated regime or away from it), while normal noise lets it point in both directions (with the slope being influenced by alpha). The formula can be split into three parts:\n\nalpha*h(x): Nonlinear part.\n(1-alpha)u(x): Linear part.\nd(x)std(x)epsilon: Stochastic part.\n\n alpha*h(x): Nonlinear part. (1-alpha)u(x): Linear part. d(x)std(x)epsilon: Stochastic part. Each of these parts resembles a path along which gradient can flow through the network. During test time the activation function is made deterministic by using its expectation value: E[phi(x,z)] = alpha*h(x) + (1-alpha)u(x) + d(x)std(x)E[epsilon]. If z is half-normal then E[epsilon] = sqrt(2/pi). If z is normal then E[epsilon] = 0. \n(5) Adding Noise to Input of the Function\n\nNoise can also be added to the input of an activation function, i.e. h(x) becomes h(x + noise).\nThe noise can either always be applied or only once the input passes a threshold.\n\n Noise can also be added to the input of an activation function, i.e. h(x) becomes h(x + noise). The noise can either always be applied or only once the input passes a threshold. \n(6) Experimental Results\n\nThey applied noise only during training.\nThey used existing setups and just changed the activation functions to noisy ones. No further optimizations.\np was initialized uniformly to [-1,1].\nBasic experiment settings:\n\nNAN: Normal noise applied to the outputs.\nNAH: Half-normal noise, i.e. |z|, i.e. noise is \"directed\" towards the unsaturated or satured regime.\nNANI: Normal noise applied to the input, i.e. h(x+noise).\nNANIL: Normal noise applied to the input with learned variance.\nNANIS: Normal noise applied to the input, but only if the unit saturates (i.e. above/below thresholds).\n\n\n(6.1) Exploratory analysis\n\nA very simple MNIST network performed slightly better with noisy activations than without. But comparison was only to tanh and hard-tanh, not ReLU or similar.\nIn an experiment with a simple GRU, NANI (noisy input) and NAN (noisy output) performed practically identical. NANIS (noisy input, only when saturated) performed significantly worse.\n\n\n(6.2) Learning to Execute\n\nProblem setting: Predict the output of some lines of code.\nThey replaced sigmoids and tanhs with their noisy counterparts (NAH, i.e. half-normal noise on output). The model learned faster.\n\n\n(6.3) Penntreebank Experiments\n\nThey trained a standard 2-layer LSTM language model on Penntreebank.\nTheir model used noisy activations, as opposed to the usually non-noisy ones.\nThey could improve upon the previously best value. Normal noise and half-normal noise performed roughly the same.\n\n\n(6.4) Neural Machine Translation Experiments\n\nThey replaced all sigmoids and tanh units in the Neural Attention Model with noisy ones. Then they trained on the Europarl corpus.\nThey improved upon the previously best score.\n\n\n(6.5) Image Caption Generation Experiments\n\nThey train a network with soft attention to generate captions for the Flickr8k dataset.\nUsing noisy activation units improved the result over normal sigmoids and tanhs.\n\n\n(6.6) Experiments with Continuation\n\nThey build an LSTM and train it to predict how many unique integers there are in a sequence of random integers.\nInstead of using a constant value for hyperparameter c of the noisy activations (scale of the standard deviation of the noise), they start at c=30 and anneal down to c=0.5.\nAnnealed noise performed significantly better then unannealed noise.\nNoise applied to the output (NAN) significantly beat noise applied to the input (NANIL).\nIn a second experiment they trained a Neural Turing Machine on the associative recall task.\nAgain they used annealed noise.\nThe NTM with annealed noise learned by far faster than the one without annealed noise and converged to a perfect solution.\n\n\n\n They applied noise only during training. They used existing setups and just changed the activation functions to noisy ones. No further optimizations. p was initialized uniformly to [-1,1]. Basic experiment settings:\n\nNAN: Normal noise applied to the outputs.\nNAH: Half-normal noise, i.e. |z|, i.e. noise is \"directed\" towards the unsaturated or satured regime.\nNANI: Normal noise applied to the input, i.e. h(x+noise).\nNANIL: Normal noise applied to the input with learned variance.\nNANIS: Normal noise applied to the input, but only if the unit saturates (i.e. above/below thresholds).\n\n NAN: Normal noise applied to the outputs. NAH: Half-normal noise, i.e. |z|, i.e. noise is \"directed\" towards the unsaturated or satured regime. NANI: Normal noise applied to the input, i.e. h(x+noise). NANIL: Normal noise applied to the input with learned variance. NANIS: Normal noise applied to the input, but only if the unit saturates (i.e. above/below thresholds). (6.1) Exploratory analysis\n\nA very simple MNIST network performed slightly better with noisy activations than without. But comparison was only to tanh and hard-tanh, not ReLU or similar.\nIn an experiment with a simple GRU, NANI (noisy input) and NAN (noisy output) performed practically identical. NANIS (noisy input, only when saturated) performed significantly worse.\n\n A very simple MNIST network performed slightly better with noisy activations than without. But comparison was only to tanh and hard-tanh, not ReLU or similar. In an experiment with a simple GRU, NANI (noisy input) and NAN (noisy output) performed practically identical. NANIS (noisy input, only when saturated) performed significantly worse. (6.2) Learning to Execute\n\nProblem setting: Predict the output of some lines of code.\nThey replaced sigmoids and tanhs with their noisy counterparts (NAH, i.e. half-normal noise on output). The model learned faster.\n\n Problem setting: Predict the output of some lines of code. They replaced sigmoids and tanhs with their noisy counterparts (NAH, i.e. half-normal noise on output). The model learned faster. (6.3) Penntreebank Experiments\n\nThey trained a standard 2-layer LSTM language model on Penntreebank.\nTheir model used noisy activations, as opposed to the usually non-noisy ones.\nThey could improve upon the previously best value. Normal noise and half-normal noise performed roughly the same.\n\n They trained a standard 2-layer LSTM language model on Penntreebank. Their model used noisy activations, as opposed to the usually non-noisy ones. They could improve upon the previously best value. Normal noise and half-normal noise performed roughly the same. (6.4) Neural Machine Translation Experiments\n\nThey replaced all sigmoids and tanh units in the Neural Attention Model with noisy ones. Then they trained on the Europarl corpus.\nThey improved upon the previously best score.\n\n They replaced all sigmoids and tanh units in the Neural Attention Model with noisy ones. Then they trained on the Europarl corpus. They improved upon the previously best score. (6.5) Image Caption Generation Experiments\n\nThey train a network with soft attention to generate captions for the Flickr8k dataset.\nUsing noisy activation units improved the result over normal sigmoids and tanhs.\n\n They train a network with soft attention to generate captions for the Flickr8k dataset. Using noisy activation units improved the result over normal sigmoids and tanhs. (6.6) Experiments with Continuation\n\nThey build an LSTM and train it to predict how many unique integers there are in a sequence of random integers.\nInstead of using a constant value for hyperparameter c of the noisy activations (scale of the standard deviation of the noise), they start at c=30 and anneal down to c=0.5.\nAnnealed noise performed significantly better then unannealed noise.\nNoise applied to the output (NAN) significantly beat noise applied to the input (NANIL).\nIn a second experiment they trained a Neural Turing Machine on the associative recall task.\nAgain they used annealed noise.\nThe NTM with annealed noise learned by far faster than the one without annealed noise and converged to a perfect solution.\n\n They build an LSTM and train it to predict how many unique integers there are in a sequence of random integers. Instead of using a constant value for hyperparameter c of the noisy activations (scale of the standard deviation of the noise), they start at c=30 and anneal down to c=0.5. Annealed noise performed significantly better then unannealed noise. Noise applied to the output (NAN) significantly beat noise applied to the input (NANIL). In a second experiment they trained a Neural Turing Machine on the associative recall task. Again they used annealed noise. The NTM with annealed noise learned by far faster than the one without annealed noise and converged to a perfect solution. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1603.00391v3"
    },
    "82": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/DenseCap.md",
        "transcript": "\nWhat\n\nThey define four subtasks of image understanding:\n\nClassification: Assign a single label to a whole image.\nCaptioning: Assign a sequence of words (description) to a whole image*\nDetection: Find objects/regions in an image and assign a single label to each one.\nDense Captioning: Find objects/regions in an image and assign a sequence of words (description) to each one.\n\n\nDenseCap accomplishes the fourth task, i.e. it is a model that finds objects/regions in images and describes them with natural language.\n\n They define four subtasks of image understanding:\n\nClassification: Assign a single label to a whole image.\nCaptioning: Assign a sequence of words (description) to a whole image*\nDetection: Find objects/regions in an image and assign a single label to each one.\nDense Captioning: Find objects/regions in an image and assign a sequence of words (description) to each one.\n\n Classification: Assign a single label to a whole image. Captioning: Assign a sequence of words (description) to a whole image* Detection: Find objects/regions in an image and assign a single label to each one. Dense Captioning: Find objects/regions in an image and assign a sequence of words (description) to each one. DenseCap accomplishes the fourth task, i.e. it is a model that finds objects/regions in images and describes them with natural language. \nHow\n\nTheir model consists of four subcomponents, which run for each image in sequence:\n\n(1) Convolutional Network:\n\nBasically just VGG-16.\n\n\n(2) Localization Layer:\n\nThis layer uses a convolutional network that has mostly the same architecture as in the \"Faster R-CNN\" paper.\nThat ConvNet is applied to a grid of anchor points on the image.\nFor each anchor point, it extracts the features generated by the VGG-Net (model 1) around that point.\nIt then generates the attributes of k (default: 12) boxes using a shallow convolutional net. These attributes are (roughly): Height, width, center x, center y, confidence score.\nIt then extracts the features of these boxes from the VGG-Net output (model 1) and uses bilinear sampling to project them onto a fixed size (height, width) for the next model. The result are the final region proposals.\nBy default every image pixel is an anchor point, which results in a large number of regions. Hence, subsampling is used during training and testing.\n\n\n(3) Recognition Network:\n\nTakes a region (flattened to 1d vector) and projects it onto a vector of length 4096.\nIt uses fully connected layers to do that (ReLU, dropout).\nAdditionally, the network takes the 4096 vector and outputs new values for the region's position and confidence (for late fine tuning).\nThe 4096 vectors of all regions are combined to a matrix that is fed into the next component (RNN).\nThe intended sense of the this component seems to be to convert the \"visual\" features of each region to a more abstract, high-dimensional representation/description.\n\n\n(4) RNN Language Model:\n\nThe take each 4096 vector and apply a fully connected layer + ReLU to it.\nThen they feed it into an LSTM, followed by a START token.\nThe LSTM then generates word (as one hot vectors), which are fed back into the model for the next time step.\nThis is continued until the LSTM generates an END token.\n\n\n\n\nTheir full loss function has five components:\n\nBinary logistic loss for the confidence values generated by the localization layer.\nBinary logistic loss for the confidence values generated by the recognition layer.\nSmooth L1 loss for the region dimensions generated by the localization layer.\nSmooth L1 loss for the region dimensiosn generated by the recognition layer.\nCross-entropy at every time-step of the language model.\n\n\nThe whole model can be trained end-to-end.\n\n Their model consists of four subcomponents, which run for each image in sequence:\n\n(1) Convolutional Network:\n\nBasically just VGG-16.\n\n\n(2) Localization Layer:\n\nThis layer uses a convolutional network that has mostly the same architecture as in the \"Faster R-CNN\" paper.\nThat ConvNet is applied to a grid of anchor points on the image.\nFor each anchor point, it extracts the features generated by the VGG-Net (model 1) around that point.\nIt then generates the attributes of k (default: 12) boxes using a shallow convolutional net. These attributes are (roughly): Height, width, center x, center y, confidence score.\nIt then extracts the features of these boxes from the VGG-Net output (model 1) and uses bilinear sampling to project them onto a fixed size (height, width) for the next model. The result are the final region proposals.\nBy default every image pixel is an anchor point, which results in a large number of regions. Hence, subsampling is used during training and testing.\n\n\n(3) Recognition Network:\n\nTakes a region (flattened to 1d vector) and projects it onto a vector of length 4096.\nIt uses fully connected layers to do that (ReLU, dropout).\nAdditionally, the network takes the 4096 vector and outputs new values for the region's position and confidence (for late fine tuning).\nThe 4096 vectors of all regions are combined to a matrix that is fed into the next component (RNN).\nThe intended sense of the this component seems to be to convert the \"visual\" features of each region to a more abstract, high-dimensional representation/description.\n\n\n(4) RNN Language Model:\n\nThe take each 4096 vector and apply a fully connected layer + ReLU to it.\nThen they feed it into an LSTM, followed by a START token.\nThe LSTM then generates word (as one hot vectors), which are fed back into the model for the next time step.\nThis is continued until the LSTM generates an END token.\n\n\n\n (1) Convolutional Network:\n\nBasically just VGG-16.\n\n Basically just VGG-16. (2) Localization Layer:\n\nThis layer uses a convolutional network that has mostly the same architecture as in the \"Faster R-CNN\" paper.\nThat ConvNet is applied to a grid of anchor points on the image.\nFor each anchor point, it extracts the features generated by the VGG-Net (model 1) around that point.\nIt then generates the attributes of k (default: 12) boxes using a shallow convolutional net. These attributes are (roughly): Height, width, center x, center y, confidence score.\nIt then extracts the features of these boxes from the VGG-Net output (model 1) and uses bilinear sampling to project them onto a fixed size (height, width) for the next model. The result are the final region proposals.\nBy default every image pixel is an anchor point, which results in a large number of regions. Hence, subsampling is used during training and testing.\n\n This layer uses a convolutional network that has mostly the same architecture as in the \"Faster R-CNN\" paper. That ConvNet is applied to a grid of anchor points on the image. For each anchor point, it extracts the features generated by the VGG-Net (model 1) around that point. It then generates the attributes of k (default: 12) boxes using a shallow convolutional net. These attributes are (roughly): Height, width, center x, center y, confidence score. It then extracts the features of these boxes from the VGG-Net output (model 1) and uses bilinear sampling to project them onto a fixed size (height, width) for the next model. The result are the final region proposals. By default every image pixel is an anchor point, which results in a large number of regions. Hence, subsampling is used during training and testing. (3) Recognition Network:\n\nTakes a region (flattened to 1d vector) and projects it onto a vector of length 4096.\nIt uses fully connected layers to do that (ReLU, dropout).\nAdditionally, the network takes the 4096 vector and outputs new values for the region's position and confidence (for late fine tuning).\nThe 4096 vectors of all regions are combined to a matrix that is fed into the next component (RNN).\nThe intended sense of the this component seems to be to convert the \"visual\" features of each region to a more abstract, high-dimensional representation/description.\n\n Takes a region (flattened to 1d vector) and projects it onto a vector of length 4096. It uses fully connected layers to do that (ReLU, dropout). Additionally, the network takes the 4096 vector and outputs new values for the region's position and confidence (for late fine tuning). The 4096 vectors of all regions are combined to a matrix that is fed into the next component (RNN). The intended sense of the this component seems to be to convert the \"visual\" features of each region to a more abstract, high-dimensional representation/description. (4) RNN Language Model:\n\nThe take each 4096 vector and apply a fully connected layer + ReLU to it.\nThen they feed it into an LSTM, followed by a START token.\nThe LSTM then generates word (as one hot vectors), which are fed back into the model for the next time step.\nThis is continued until the LSTM generates an END token.\n\n The take each 4096 vector and apply a fully connected layer + ReLU to it. Then they feed it into an LSTM, followed by a START token. The LSTM then generates word (as one hot vectors), which are fed back into the model for the next time step. This is continued until the LSTM generates an END token. Their full loss function has five components:\n\nBinary logistic loss for the confidence values generated by the localization layer.\nBinary logistic loss for the confidence values generated by the recognition layer.\nSmooth L1 loss for the region dimensions generated by the localization layer.\nSmooth L1 loss for the region dimensiosn generated by the recognition layer.\nCross-entropy at every time-step of the language model.\n\n Binary logistic loss for the confidence values generated by the localization layer. Binary logistic loss for the confidence values generated by the recognition layer. Smooth L1 loss for the region dimensions generated by the localization layer. Smooth L1 loss for the region dimensiosn generated by the recognition layer. Cross-entropy at every time-step of the language model. The whole model can be trained end-to-end. \nResults\n\nThey mostly use the Visual Genome dataset.\nTheir model finds lots of good regions in images.\nTheir model generates good captions for each region. (Only short captions with simple language however.)\nThe model seems to love colors. Like 30-50% of all captions contain a color. (Probably caused by the dataset?)\nThey compare to EdgeBoxes (other method to find regions in images). Their model seems to perform better.\nTheir model requires about 240ms per image (test time).\nThe generated regions and captions enable one to search for specific objects in images using text queries.\n\n They mostly use the Visual Genome dataset. Their model finds lots of good regions in images. Their model generates good captions for each region. (Only short captions with simple language however.) The model seems to love colors. Like 30-50% of all captions contain a color. (Probably caused by the dataset?) They compare to EdgeBoxes (other method to find regions in images). Their model seems to perform better. Their model requires about 240ms per image (test time). The generated regions and captions enable one to search for specific objects in images using text queries. \n(1) Introduction\n\nThey define four subtasks of visual scene understanding:\n\nClassification: Assign a single label to a whole image\nCaptioning: Assign a sequence of words (description) to a whole image\nDetection: Find objects in an image and assign a single label to each one\nDense Captioning: Find objects in an image and assign a sequence of words (description) to each one\n\n\nThey developed a model for dense captioning.\nIt has two three important components:\n\nA convoltional network for scene understanding\nA localization layer for region level predictions. It predicts regions of interest and then uses bilinear sampling to extract the activations of these regions.\nA recurrent network as the language model\n\n\nThey evaluate the model on the large-scale Visual Genome dataset (94k images, 4.1M region captions).\n\n They define four subtasks of visual scene understanding:\n\nClassification: Assign a single label to a whole image\nCaptioning: Assign a sequence of words (description) to a whole image\nDetection: Find objects in an image and assign a single label to each one\nDense Captioning: Find objects in an image and assign a sequence of words (description) to each one\n\n Classification: Assign a single label to a whole image Captioning: Assign a sequence of words (description) to a whole image Detection: Find objects in an image and assign a single label to each one Dense Captioning: Find objects in an image and assign a sequence of words (description) to each one They developed a model for dense captioning. It has two three important components:\n\nA convoltional network for scene understanding\nA localization layer for region level predictions. It predicts regions of interest and then uses bilinear sampling to extract the activations of these regions.\nA recurrent network as the language model\n\n A convoltional network for scene understanding A localization layer for region level predictions. It predicts regions of interest and then uses bilinear sampling to extract the activations of these regions. A recurrent network as the language model They evaluate the model on the large-scale Visual Genome dataset (94k images, 4.1M region captions). \n(3) Model\n\nModel architecture\n\nConvolutional Network\n\nThey use VGG-16, but remove the last pooling layer.\nFor an image of size W, H the output is 512xW/16xH/16.\nThat output is the input into the localization layer.\n\n\nFully Convolutional Localization Layer\n\nInput to this layer: Activations from the convolutional network.\nOutput of this layer: Regions of interest, as fixed-sized representations.\n\nFor B Regions:\n\nCoordinates of the bounding boxes (matrix of shape Bx4)\nConfidence scores (vector of length B)\nFeatures (matrix of shape BxCxXxY)\n\n\n\n\nMethod: Faster R-CNN (pooling replaced by bilinear interpolation)\nThis layer is fully differentiable.\nThe localization layer predicts boxes at anchor points.\nAt each anchor point it proposes k boxes using a small convolutional network. It assigns a confidence score and coordinates (center x, center y, height, width) to each proposal.\nFor an image with size 720x540 and k=12 the model would have to predict 17,280 boxes, hence subsampling is used.\nDuring training they use minibatches with 256/2 positive and 256/2 negative region examples. A box counts as a positive example for a specific image if it has high overlap (intersection) with an annotated box for that image.\nDuring test time they use greedy non-maximum suppression (NMS) (?) to subsample the 300 most confident boxes.\nThe region proposals have varying box sizes, but the output of the localization layer (which will be fed into the RNN) is ought to have fixed sizes.\nSo they project each proposed region onto a fixed sized region. They use bilinear sampling for that projection, which is differentiable.\n\n\nRecognition network\n\nEach region is flattened to a one-dimensional vector.\nThat vector is fed through 2 fully connected layers (unknown size, ReLU, dropout), ending with a 4096 neuron layer.\nThe confidence score and box coordinates are also adjusted by the network during that process (fine tuning).\n\n\nRNN Language Model\n\nEach region is translated to a sentence.\nThe region is fed into an LSTM (after a linear layer + ReLU), followed by a special START token.\nThe LSTM outputs multiple words as one-hot-vectors, where each vector has the length V+1 (i.e. vocabulary size + END token).\nLoss function is average crossentropy between output words and target words.\nDuring test time, words are sampled until an END tag is generated.\n\n\n\n\nLoss function\n\nTheir full loss function has five components:\n\nBinary logistic loss for the confidence values generated by the localization layer.\nBinary logistic loss for the confidence values generated by the recognition layer.\nSmooth L1 loss for the region dimensions generated by the localization layer.\nSmooth L1 loss for the region dimensiosn generated by the recognition layer.\nCross-entropy at every time-step of the language model.\n\n\nThe language model term has a weight of 1.0, all other components have a weight of 0.1.\n\n\nTraining an optimization\n\nInitialization: CNN pretrained on ImageNet, all other weights from N(0, 0.01).\nSGD for the CNN (lr=?, momentum=0.9)\nAdam everywhere else (lr=1e-6, beta1=0.9, beta2=0.99)\nCNN is trained after epoch 1. CNN's first four layers are not trained.\nBatch size is 1.\nImage size is 720 on the longest side.\nThey use Torch.\n3 days of training time.\n\n\n\n Model architecture\n\nConvolutional Network\n\nThey use VGG-16, but remove the last pooling layer.\nFor an image of size W, H the output is 512xW/16xH/16.\nThat output is the input into the localization layer.\n\n\nFully Convolutional Localization Layer\n\nInput to this layer: Activations from the convolutional network.\nOutput of this layer: Regions of interest, as fixed-sized representations.\n\nFor B Regions:\n\nCoordinates of the bounding boxes (matrix of shape Bx4)\nConfidence scores (vector of length B)\nFeatures (matrix of shape BxCxXxY)\n\n\n\n\nMethod: Faster R-CNN (pooling replaced by bilinear interpolation)\nThis layer is fully differentiable.\nThe localization layer predicts boxes at anchor points.\nAt each anchor point it proposes k boxes using a small convolutional network. It assigns a confidence score and coordinates (center x, center y, height, width) to each proposal.\nFor an image with size 720x540 and k=12 the model would have to predict 17,280 boxes, hence subsampling is used.\nDuring training they use minibatches with 256/2 positive and 256/2 negative region examples. A box counts as a positive example for a specific image if it has high overlap (intersection) with an annotated box for that image.\nDuring test time they use greedy non-maximum suppression (NMS) (?) to subsample the 300 most confident boxes.\nThe region proposals have varying box sizes, but the output of the localization layer (which will be fed into the RNN) is ought to have fixed sizes.\nSo they project each proposed region onto a fixed sized region. They use bilinear sampling for that projection, which is differentiable.\n\n\nRecognition network\n\nEach region is flattened to a one-dimensional vector.\nThat vector is fed through 2 fully connected layers (unknown size, ReLU, dropout), ending with a 4096 neuron layer.\nThe confidence score and box coordinates are also adjusted by the network during that process (fine tuning).\n\n\nRNN Language Model\n\nEach region is translated to a sentence.\nThe region is fed into an LSTM (after a linear layer + ReLU), followed by a special START token.\nThe LSTM outputs multiple words as one-hot-vectors, where each vector has the length V+1 (i.e. vocabulary size + END token).\nLoss function is average crossentropy between output words and target words.\nDuring test time, words are sampled until an END tag is generated.\n\n\n\n Convolutional Network\n\nThey use VGG-16, but remove the last pooling layer.\nFor an image of size W, H the output is 512xW/16xH/16.\nThat output is the input into the localization layer.\n\n They use VGG-16, but remove the last pooling layer. For an image of size W, H the output is 512xW/16xH/16. That output is the input into the localization layer. Fully Convolutional Localization Layer\n\nInput to this layer: Activations from the convolutional network.\nOutput of this layer: Regions of interest, as fixed-sized representations.\n\nFor B Regions:\n\nCoordinates of the bounding boxes (matrix of shape Bx4)\nConfidence scores (vector of length B)\nFeatures (matrix of shape BxCxXxY)\n\n\n\n\nMethod: Faster R-CNN (pooling replaced by bilinear interpolation)\nThis layer is fully differentiable.\nThe localization layer predicts boxes at anchor points.\nAt each anchor point it proposes k boxes using a small convolutional network. It assigns a confidence score and coordinates (center x, center y, height, width) to each proposal.\nFor an image with size 720x540 and k=12 the model would have to predict 17,280 boxes, hence subsampling is used.\nDuring training they use minibatches with 256/2 positive and 256/2 negative region examples. A box counts as a positive example for a specific image if it has high overlap (intersection) with an annotated box for that image.\nDuring test time they use greedy non-maximum suppression (NMS) (?) to subsample the 300 most confident boxes.\nThe region proposals have varying box sizes, but the output of the localization layer (which will be fed into the RNN) is ought to have fixed sizes.\nSo they project each proposed region onto a fixed sized region. They use bilinear sampling for that projection, which is differentiable.\n\n Input to this layer: Activations from the convolutional network. Output of this layer: Regions of interest, as fixed-sized representations.\n\nFor B Regions:\n\nCoordinates of the bounding boxes (matrix of shape Bx4)\nConfidence scores (vector of length B)\nFeatures (matrix of shape BxCxXxY)\n\n\n\n For B Regions:\n\nCoordinates of the bounding boxes (matrix of shape Bx4)\nConfidence scores (vector of length B)\nFeatures (matrix of shape BxCxXxY)\n\n Coordinates of the bounding boxes (matrix of shape Bx4) Confidence scores (vector of length B) Features (matrix of shape BxCxXxY) Method: Faster R-CNN (pooling replaced by bilinear interpolation) This layer is fully differentiable. The localization layer predicts boxes at anchor points. At each anchor point it proposes k boxes using a small convolutional network. It assigns a confidence score and coordinates (center x, center y, height, width) to each proposal. For an image with size 720x540 and k=12 the model would have to predict 17,280 boxes, hence subsampling is used. During training they use minibatches with 256/2 positive and 256/2 negative region examples. A box counts as a positive example for a specific image if it has high overlap (intersection) with an annotated box for that image. During test time they use greedy non-maximum suppression (NMS) (?) to subsample the 300 most confident boxes. The region proposals have varying box sizes, but the output of the localization layer (which will be fed into the RNN) is ought to have fixed sizes. So they project each proposed region onto a fixed sized region. They use bilinear sampling for that projection, which is differentiable. Recognition network\n\nEach region is flattened to a one-dimensional vector.\nThat vector is fed through 2 fully connected layers (unknown size, ReLU, dropout), ending with a 4096 neuron layer.\nThe confidence score and box coordinates are also adjusted by the network during that process (fine tuning).\n\n Each region is flattened to a one-dimensional vector. That vector is fed through 2 fully connected layers (unknown size, ReLU, dropout), ending with a 4096 neuron layer. The confidence score and box coordinates are also adjusted by the network during that process (fine tuning). RNN Language Model\n\nEach region is translated to a sentence.\nThe region is fed into an LSTM (after a linear layer + ReLU), followed by a special START token.\nThe LSTM outputs multiple words as one-hot-vectors, where each vector has the length V+1 (i.e. vocabulary size + END token).\nLoss function is average crossentropy between output words and target words.\nDuring test time, words are sampled until an END tag is generated.\n\n Each region is translated to a sentence. The region is fed into an LSTM (after a linear layer + ReLU), followed by a special START token. The LSTM outputs multiple words as one-hot-vectors, where each vector has the length V+1 (i.e. vocabulary size + END token). Loss function is average crossentropy between output words and target words. During test time, words are sampled until an END tag is generated. Loss function\n\nTheir full loss function has five components:\n\nBinary logistic loss for the confidence values generated by the localization layer.\nBinary logistic loss for the confidence values generated by the recognition layer.\nSmooth L1 loss for the region dimensions generated by the localization layer.\nSmooth L1 loss for the region dimensiosn generated by the recognition layer.\nCross-entropy at every time-step of the language model.\n\n\nThe language model term has a weight of 1.0, all other components have a weight of 0.1.\n\n Their full loss function has five components:\n\nBinary logistic loss for the confidence values generated by the localization layer.\nBinary logistic loss for the confidence values generated by the recognition layer.\nSmooth L1 loss for the region dimensions generated by the localization layer.\nSmooth L1 loss for the region dimensiosn generated by the recognition layer.\nCross-entropy at every time-step of the language model.\n\n Binary logistic loss for the confidence values generated by the localization layer. Binary logistic loss for the confidence values generated by the recognition layer. Smooth L1 loss for the region dimensions generated by the localization layer. Smooth L1 loss for the region dimensiosn generated by the recognition layer. Cross-entropy at every time-step of the language model. The language model term has a weight of 1.0, all other components have a weight of 0.1. Training an optimization\n\nInitialization: CNN pretrained on ImageNet, all other weights from N(0, 0.01).\nSGD for the CNN (lr=?, momentum=0.9)\nAdam everywhere else (lr=1e-6, beta1=0.9, beta2=0.99)\nCNN is trained after epoch 1. CNN's first four layers are not trained.\nBatch size is 1.\nImage size is 720 on the longest side.\nThey use Torch.\n3 days of training time.\n\n Initialization: CNN pretrained on ImageNet, all other weights from N(0, 0.01). SGD for the CNN (lr=?, momentum=0.9) Adam everywhere else (lr=1e-6, beta1=0.9, beta2=0.99) CNN is trained after epoch 1. CNN's first four layers are not trained. Batch size is 1. Image size is 720 on the longest side. They use Torch. 3 days of training time. \n(4) Experiments\n\nThey use the Visual Genome Dataset (94k images, 4.1M regions with captions)\nTheir total vocabulary size is 10,497 words. (Rare words in captions were replaced with <UNK>.)\nThey throw away annotations with too many words as well as images with too few/too many regions.\nThey merge heavily overlapping regions to single regions with multiple captions.\nDense Captioning\n\nDense captioning task: The model receives one image and produces a set of regions, each having a caption and a confidence score.\nEvaluation metrics\n\nEvaluation of the output is non-trivial.\nThey compare predicted regions with regions from the annotation that have high overlap (above a threshold).\nThey then compare the predicted caption with the captions having similar METEOR score (above a threshold).\nInstead of setting one threshold for each comparison they use multiple thresholds. Then they calculate the Mean Average Precision using the various pairs of thresholds.\n\n\nBaseline models\n\nSources of region proposals during test time:\n\nGT: Ground truth boxes (i.e. found by humans).\nEB: EdgeBox (completely separate and pretrained system).\nRPN: Their localization and recognition networks trained separately on VG regions dataset (i.e. trained without the RNN language model).\n\n\nModels:\n\nRegion RNN model: Apparently the recognition layer and the RNN language model, trained on predefined regions. (Where do these regions come from? VG training dataset?)\nFull Image RNN model: Apparently the recognition layer and the RNN language model, trained on full images from MSCOCO instead of small regions.\nFCLN on EB: Apparently the recognition layer and the RNN language model, trained on regions generated by EdgeBox (EB) (on VG dataset?).\nFCLN: Apparently their full model (trained on VG dataset?).\n\n\n\n\nDiscrepancy between region and image level statistics\n\nWhen evaluating the models only on METEOR (language \"quality\"), the Region RNN model consistently outperforms the Full Image RNN model.\nThat's probably because the Full Image RNN model was trained on captions of whole images, while the Region RNN model was trained on captions of small regions, which tend to be a bit different from full image captions.\n\n\nRPN outperforms external region proposals\n\nGenerating region proposals via RPN basically always beats EB.\n\n\nOur model outperforms individual region description\n\nTheir full jointly trained model (FCLN) achieves the best results.\nThe full jointly trained model performs significantly better than RPN + Region RNN model (i.e. separately trained region proposal and region captioning networks).\n\n\nQualitative results\n\nFinds plenty of good regions and generates reasonable captions for them.\nSometimes finds the same region twice.\n\n\nRuntime evaluation\n\n240ms on 720x600 image with 300 region proposals.\n166ms on 720x600 image with 100 region proposals.\nRecognition of region proposals takes up most time.\nGenerating region proposals takes up the 2nd most time.\nGenerating captions for regions (RNN) takes almost no time.\n\n\n\n\nImage Retrieval using Regions and Captions\n\nThey try to search for regions based on search queries.\nThey search by letting their FCLN network or EB generate 100 region proposals per network. Then they calculate per region the probability of generating the search query as the caption. They use that probability to rank the results.\nThey pick images from the VG dataset, then pick captions within those images as search query. Then they evaluate the ranking of those images for the respective search query.\nThe results show that the model can learn to rank objects, object parts, people and actions as expected/desired.\nThe method described can also be used to detect an arbitrary number of distinct classes in images (as opposed to the usual 10 to 1000 classes), because the classes are contained in the generated captions.\n\n\n\n They use the Visual Genome Dataset (94k images, 4.1M regions with captions) Their total vocabulary size is 10,497 words. (Rare words in captions were replaced with <UNK>.) They throw away annotations with too many words as well as images with too few/too many regions. They merge heavily overlapping regions to single regions with multiple captions. Dense Captioning\n\nDense captioning task: The model receives one image and produces a set of regions, each having a caption and a confidence score.\nEvaluation metrics\n\nEvaluation of the output is non-trivial.\nThey compare predicted regions with regions from the annotation that have high overlap (above a threshold).\nThey then compare the predicted caption with the captions having similar METEOR score (above a threshold).\nInstead of setting one threshold for each comparison they use multiple thresholds. Then they calculate the Mean Average Precision using the various pairs of thresholds.\n\n\nBaseline models\n\nSources of region proposals during test time:\n\nGT: Ground truth boxes (i.e. found by humans).\nEB: EdgeBox (completely separate and pretrained system).\nRPN: Their localization and recognition networks trained separately on VG regions dataset (i.e. trained without the RNN language model).\n\n\nModels:\n\nRegion RNN model: Apparently the recognition layer and the RNN language model, trained on predefined regions. (Where do these regions come from? VG training dataset?)\nFull Image RNN model: Apparently the recognition layer and the RNN language model, trained on full images from MSCOCO instead of small regions.\nFCLN on EB: Apparently the recognition layer and the RNN language model, trained on regions generated by EdgeBox (EB) (on VG dataset?).\nFCLN: Apparently their full model (trained on VG dataset?).\n\n\n\n\nDiscrepancy between region and image level statistics\n\nWhen evaluating the models only on METEOR (language \"quality\"), the Region RNN model consistently outperforms the Full Image RNN model.\nThat's probably because the Full Image RNN model was trained on captions of whole images, while the Region RNN model was trained on captions of small regions, which tend to be a bit different from full image captions.\n\n\nRPN outperforms external region proposals\n\nGenerating region proposals via RPN basically always beats EB.\n\n\nOur model outperforms individual region description\n\nTheir full jointly trained model (FCLN) achieves the best results.\nThe full jointly trained model performs significantly better than RPN + Region RNN model (i.e. separately trained region proposal and region captioning networks).\n\n\nQualitative results\n\nFinds plenty of good regions and generates reasonable captions for them.\nSometimes finds the same region twice.\n\n\nRuntime evaluation\n\n240ms on 720x600 image with 300 region proposals.\n166ms on 720x600 image with 100 region proposals.\nRecognition of region proposals takes up most time.\nGenerating region proposals takes up the 2nd most time.\nGenerating captions for regions (RNN) takes almost no time.\n\n\n\n Dense captioning task: The model receives one image and produces a set of regions, each having a caption and a confidence score. Evaluation metrics\n\nEvaluation of the output is non-trivial.\nThey compare predicted regions with regions from the annotation that have high overlap (above a threshold).\nThey then compare the predicted caption with the captions having similar METEOR score (above a threshold).\nInstead of setting one threshold for each comparison they use multiple thresholds. Then they calculate the Mean Average Precision using the various pairs of thresholds.\n\n Evaluation of the output is non-trivial. They compare predicted regions with regions from the annotation that have high overlap (above a threshold). They then compare the predicted caption with the captions having similar METEOR score (above a threshold). Instead of setting one threshold for each comparison they use multiple thresholds. Then they calculate the Mean Average Precision using the various pairs of thresholds. Baseline models\n\nSources of region proposals during test time:\n\nGT: Ground truth boxes (i.e. found by humans).\nEB: EdgeBox (completely separate and pretrained system).\nRPN: Their localization and recognition networks trained separately on VG regions dataset (i.e. trained without the RNN language model).\n\n\nModels:\n\nRegion RNN model: Apparently the recognition layer and the RNN language model, trained on predefined regions. (Where do these regions come from? VG training dataset?)\nFull Image RNN model: Apparently the recognition layer and the RNN language model, trained on full images from MSCOCO instead of small regions.\nFCLN on EB: Apparently the recognition layer and the RNN language model, trained on regions generated by EdgeBox (EB) (on VG dataset?).\nFCLN: Apparently their full model (trained on VG dataset?).\n\n\n\n Sources of region proposals during test time:\n\nGT: Ground truth boxes (i.e. found by humans).\nEB: EdgeBox (completely separate and pretrained system).\nRPN: Their localization and recognition networks trained separately on VG regions dataset (i.e. trained without the RNN language model).\n\n GT: Ground truth boxes (i.e. found by humans). EB: EdgeBox (completely separate and pretrained system). RPN: Their localization and recognition networks trained separately on VG regions dataset (i.e. trained without the RNN language model). Models:\n\nRegion RNN model: Apparently the recognition layer and the RNN language model, trained on predefined regions. (Where do these regions come from? VG training dataset?)\nFull Image RNN model: Apparently the recognition layer and the RNN language model, trained on full images from MSCOCO instead of small regions.\nFCLN on EB: Apparently the recognition layer and the RNN language model, trained on regions generated by EdgeBox (EB) (on VG dataset?).\nFCLN: Apparently their full model (trained on VG dataset?).\n\n Region RNN model: Apparently the recognition layer and the RNN language model, trained on predefined regions. (Where do these regions come from? VG training dataset?) Full Image RNN model: Apparently the recognition layer and the RNN language model, trained on full images from MSCOCO instead of small regions. FCLN on EB: Apparently the recognition layer and the RNN language model, trained on regions generated by EdgeBox (EB) (on VG dataset?). FCLN: Apparently their full model (trained on VG dataset?). Discrepancy between region and image level statistics\n\nWhen evaluating the models only on METEOR (language \"quality\"), the Region RNN model consistently outperforms the Full Image RNN model.\nThat's probably because the Full Image RNN model was trained on captions of whole images, while the Region RNN model was trained on captions of small regions, which tend to be a bit different from full image captions.\n\n When evaluating the models only on METEOR (language \"quality\"), the Region RNN model consistently outperforms the Full Image RNN model. That's probably because the Full Image RNN model was trained on captions of whole images, while the Region RNN model was trained on captions of small regions, which tend to be a bit different from full image captions. RPN outperforms external region proposals\n\nGenerating region proposals via RPN basically always beats EB.\n\n Generating region proposals via RPN basically always beats EB. Our model outperforms individual region description\n\nTheir full jointly trained model (FCLN) achieves the best results.\nThe full jointly trained model performs significantly better than RPN + Region RNN model (i.e. separately trained region proposal and region captioning networks).\n\n Their full jointly trained model (FCLN) achieves the best results. The full jointly trained model performs significantly better than RPN + Region RNN model (i.e. separately trained region proposal and region captioning networks). Qualitative results\n\nFinds plenty of good regions and generates reasonable captions for them.\nSometimes finds the same region twice.\n\n Finds plenty of good regions and generates reasonable captions for them. Sometimes finds the same region twice. Runtime evaluation\n\n240ms on 720x600 image with 300 region proposals.\n166ms on 720x600 image with 100 region proposals.\nRecognition of region proposals takes up most time.\nGenerating region proposals takes up the 2nd most time.\nGenerating captions for regions (RNN) takes almost no time.\n\n 240ms on 720x600 image with 300 region proposals. 166ms on 720x600 image with 100 region proposals. Recognition of region proposals takes up most time. Generating region proposals takes up the 2nd most time. Generating captions for regions (RNN) takes almost no time. Image Retrieval using Regions and Captions\n\nThey try to search for regions based on search queries.\nThey search by letting their FCLN network or EB generate 100 region proposals per network. Then they calculate per region the probability of generating the search query as the caption. They use that probability to rank the results.\nThey pick images from the VG dataset, then pick captions within those images as search query. Then they evaluate the ranking of those images for the respective search query.\nThe results show that the model can learn to rank objects, object parts, people and actions as expected/desired.\nThe method described can also be used to detect an arbitrary number of distinct classes in images (as opposed to the usual 10 to 1000 classes), because the classes are contained in the generated captions.\n\n They try to search for regions based on search queries. They search by letting their FCLN network or EB generate 100 region proposals per network. Then they calculate per region the probability of generating the search query as the caption. They use that probability to rank the results. They pick images from the VG dataset, then pick captions within those images as search query. Then they evaluate the ranking of those images for the respective search query. The results show that the model can learn to rank objects, object parts, people and actions as expected/desired. The method described can also be used to detect an arbitrary number of distinct classes in images (as opposed to the usual 10 to 1000 classes), because the classes are contained in the generated captions. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1511.07571"
    },
    "83": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Deep_Networks_with_Stochastic_Depth.md",
        "transcript": "\nWhat\n\nStochastic Depth (SD) is a method for residual networks, which randomly removes/deactivates residual blocks during training.\nAs such, it is similar to dropout.\nWhile dropout removes neurons, SD removes blocks (roughly the layers of a residual network).\nOne can argue that dropout randomly changes the width of layers, while SD randomly changes the depth of the network.\nOne can argue that using dropout is similar to training an ensemble of networks with different layer widths, while using SD is similar to training an ensemble of networks with different depths.\nUsing SD has the following advantages:\n\nIt decreases the effects of vanishing gradients, because on average the network is shallower during training (per batch), thereby increasing the gradient that reaches the early blocks.\nIt increases training speed, because on average less convolutions have to be applied (due to blocks being removed).\nIt has a regularizing effect, because blocks cannot easily co-adapt any more. (Similar to dropout avoiding co-adaption of neurons.)\nIf using an increasing removal probability for later blocks: It spends more training time on the early (and thus most important) blocks than on the later blocks.\n\n\n\n Stochastic Depth (SD) is a method for residual networks, which randomly removes/deactivates residual blocks during training. As such, it is similar to dropout. While dropout removes neurons, SD removes blocks (roughly the layers of a residual network). One can argue that dropout randomly changes the width of layers, while SD randomly changes the depth of the network. One can argue that using dropout is similar to training an ensemble of networks with different layer widths, while using SD is similar to training an ensemble of networks with different depths. Using SD has the following advantages:\n\nIt decreases the effects of vanishing gradients, because on average the network is shallower during training (per batch), thereby increasing the gradient that reaches the early blocks.\nIt increases training speed, because on average less convolutions have to be applied (due to blocks being removed).\nIt has a regularizing effect, because blocks cannot easily co-adapt any more. (Similar to dropout avoiding co-adaption of neurons.)\nIf using an increasing removal probability for later blocks: It spends more training time on the early (and thus most important) blocks than on the later blocks.\n\n It decreases the effects of vanishing gradients, because on average the network is shallower during training (per batch), thereby increasing the gradient that reaches the early blocks. It increases training speed, because on average less convolutions have to be applied (due to blocks being removed). It has a regularizing effect, because blocks cannot easily co-adapt any more. (Similar to dropout avoiding co-adaption of neurons.) If using an increasing removal probability for later blocks: It spends more training time on the early (and thus most important) blocks than on the later blocks. \nHow\n\nNormal formula for a residual block (test and train):\n\noutput = ReLU(f(input) + identity(input))\nf(x) are usually one or two convolutions.\n\n\nFormula with SD (during training):\n\noutput = ReLU(b * f(input) + identity(input))\nb is either exactly 1 (block survived, i.e. is not removed) or exactly 0 (block was removed).\nb is sampled from a bernoulli random variable that has the hyperparameter p.\np is the survival probability of a block (i.e. chance to not be removed). (Note that this is the opposite of dropout, where higher values lead to more removal.)\n\n\nFormula with SD (during test):\n\noutput = ReLU(p * f(input) + input)\np is the average probability with which this residual block survives during training, i.e. the hyperparameter for the bernoulli variable.\nThe test formula has to be changed, because the network will adapt during training to blocks being missing. Activating them all at the same time can lead to overly strong signals. This is similar to dropout, where weights also have to be changed during test.\n\n\nThere are two simple schemas to set p per layer:\n\nUniform schema: Every block gets the same p hyperparameter, i.e. the last block has the same chance of survival as the first block.\nLinear decay schema: Survival probability is higher for early layers and decreases towards the end.\n\nThe formula is p = 1 - (l/L)(1-q).\nl: Number of the block for which to set p.\nL: Total number of blocks.\nq: Desired survival probability of the last block (0.5 is a good value).\n\n\n\n\nFor linear decay with q=0.5 and L blocks, on average (3/4)L blocks will be trained per minibatch.\nFor linear decay with q=0.5 the average speedup will be about 1/4 (25%). If using q=0.2 the speedup will be ~40%.\n\n Normal formula for a residual block (test and train):\n\noutput = ReLU(f(input) + identity(input))\nf(x) are usually one or two convolutions.\n\n output = ReLU(f(input) + identity(input)) f(x) are usually one or two convolutions. Formula with SD (during training):\n\noutput = ReLU(b * f(input) + identity(input))\nb is either exactly 1 (block survived, i.e. is not removed) or exactly 0 (block was removed).\nb is sampled from a bernoulli random variable that has the hyperparameter p.\np is the survival probability of a block (i.e. chance to not be removed). (Note that this is the opposite of dropout, where higher values lead to more removal.)\n\n output = ReLU(b * f(input) + identity(input)) b is either exactly 1 (block survived, i.e. is not removed) or exactly 0 (block was removed). b is sampled from a bernoulli random variable that has the hyperparameter p. p is the survival probability of a block (i.e. chance to not be removed). (Note that this is the opposite of dropout, where higher values lead to more removal.) Formula with SD (during test):\n\noutput = ReLU(p * f(input) + input)\np is the average probability with which this residual block survives during training, i.e. the hyperparameter for the bernoulli variable.\nThe test formula has to be changed, because the network will adapt during training to blocks being missing. Activating them all at the same time can lead to overly strong signals. This is similar to dropout, where weights also have to be changed during test.\n\n output = ReLU(p * f(input) + input) p is the average probability with which this residual block survives during training, i.e. the hyperparameter for the bernoulli variable. The test formula has to be changed, because the network will adapt during training to blocks being missing. Activating them all at the same time can lead to overly strong signals. This is similar to dropout, where weights also have to be changed during test. There are two simple schemas to set p per layer:\n\nUniform schema: Every block gets the same p hyperparameter, i.e. the last block has the same chance of survival as the first block.\nLinear decay schema: Survival probability is higher for early layers and decreases towards the end.\n\nThe formula is p = 1 - (l/L)(1-q).\nl: Number of the block for which to set p.\nL: Total number of blocks.\nq: Desired survival probability of the last block (0.5 is a good value).\n\n\n\n Uniform schema: Every block gets the same p hyperparameter, i.e. the last block has the same chance of survival as the first block. Linear decay schema: Survival probability is higher for early layers and decreases towards the end.\n\nThe formula is p = 1 - (l/L)(1-q).\nl: Number of the block for which to set p.\nL: Total number of blocks.\nq: Desired survival probability of the last block (0.5 is a good value).\n\n The formula is p = 1 - (l/L)(1-q). l: Number of the block for which to set p. L: Total number of blocks. q: Desired survival probability of the last block (0.5 is a good value). For linear decay with q=0.5 and L blocks, on average (3/4)L blocks will be trained per minibatch. For linear decay with q=0.5 the average speedup will be about 1/4 (25%). If using q=0.2 the speedup will be ~40%. \nResults\n\n152 layer networks with SD outperform identical networks without SD on CIFAR-10, CIFAR-100 and SVHN.\nThe improvement in test error is quite significant.\nSD seems to have a regularizing effect. Networks with SD are not overfitting where networks without SD already are.\nEven networks with >1000 layers are well trainable with SD.\nThe gradients that reach the early blocks of the networks are consistently significantly higher with SD than without SD (i.e. less vanishing gradient).\nThe linear decay schema consistently outperforms the uniform schema (in test error). The best value seems to be q=0.5, though values between 0.4 and 0.8 all seem to be good. For the uniform schema only 0.8 seems to be good.\n\n 152 layer networks with SD outperform identical networks without SD on CIFAR-10, CIFAR-100 and SVHN. The improvement in test error is quite significant. SD seems to have a regularizing effect. Networks with SD are not overfitting where networks without SD already are. Even networks with >1000 layers are well trainable with SD. The gradients that reach the early blocks of the networks are consistently significantly higher with SD than without SD (i.e. less vanishing gradient). The linear decay schema consistently outperforms the uniform schema (in test error). The best value seems to be q=0.5, though values between 0.4 and 0.8 all seem to be good. For the uniform schema only 0.8 seems to be good. \n(1) Introduction\n\nProblems of deep networks:\n\nVanishing Gradients: During backpropagation, gradients approach zero due to being repeatedly multiplied with small weights. Possible counter-measures: Careful initialization of weights, \"hidden layer supervision\" (?), batch normalization.\nDiminishing feature reuse: Aequivalent problem to vanishing gradients during forward propagation. Results of early layers are repeatedly multiplied with later layer's (randomly initialized) weights. The total result then becomes meaningless noise and doesn't have a clear/strong gradient to fix it.\nLong training time: The time of each forward-backward increases linearly with layer depth. Current 152-layer networks can take weeks to train on ImageNet.\n\n\nI.e.: Shallow networks can be trained effectively and fast, but deep networks would be much more expressive.\nDuring testing we want deep networks, during training we want shallow networks.\nThey randomly \"drop out\" (i.e. remove) complete layers during training (per minibatch), resulting in shallow networks.\nResult: Lower training time and lower test error.\nWhile dropout randomly removes width from the network, stochastic depth randomly removes depth from the networks.\nWhile dropout can be thought of as training an ensemble of networks with different depth, stochastic depth can be thought of as training an ensemble of networks with different depth.\nStochastic depth acts as a regularizer, similar to dropout and batch normalization. It allows deeper networks without overfitting (because 1000 layers clearly wasn't enough!).\n\n Problems of deep networks:\n\nVanishing Gradients: During backpropagation, gradients approach zero due to being repeatedly multiplied with small weights. Possible counter-measures: Careful initialization of weights, \"hidden layer supervision\" (?), batch normalization.\nDiminishing feature reuse: Aequivalent problem to vanishing gradients during forward propagation. Results of early layers are repeatedly multiplied with later layer's (randomly initialized) weights. The total result then becomes meaningless noise and doesn't have a clear/strong gradient to fix it.\nLong training time: The time of each forward-backward increases linearly with layer depth. Current 152-layer networks can take weeks to train on ImageNet.\n\n Vanishing Gradients: During backpropagation, gradients approach zero due to being repeatedly multiplied with small weights. Possible counter-measures: Careful initialization of weights, \"hidden layer supervision\" (?), batch normalization. Diminishing feature reuse: Aequivalent problem to vanishing gradients during forward propagation. Results of early layers are repeatedly multiplied with later layer's (randomly initialized) weights. The total result then becomes meaningless noise and doesn't have a clear/strong gradient to fix it. Long training time: The time of each forward-backward increases linearly with layer depth. Current 152-layer networks can take weeks to train on ImageNet. I.e.: Shallow networks can be trained effectively and fast, but deep networks would be much more expressive. During testing we want deep networks, during training we want shallow networks. They randomly \"drop out\" (i.e. remove) complete layers during training (per minibatch), resulting in shallow networks. Result: Lower training time and lower test error. While dropout randomly removes width from the network, stochastic depth randomly removes depth from the networks. While dropout can be thought of as training an ensemble of networks with different depth, stochastic depth can be thought of as training an ensemble of networks with different depth. Stochastic depth acts as a regularizer, similar to dropout and batch normalization. It allows deeper networks without overfitting (because 1000 layers clearly wasn't enough!). \n(2) Background\n\nSome previous methods to train deep networks: Greedy layer-wise training, careful initializations, batch normalization, highway connections, residual connections.\n\n\n\n\nDropout loses effectiveness when combined with batch normalization. Seems to have basically no benefit any more for deep residual networks with batch normalization.\n\n Some previous methods to train deep networks: Greedy layer-wise training, careful initializations, batch normalization, highway connections, residual connections. \n \n Dropout loses effectiveness when combined with batch normalization. Seems to have basically no benefit any more for deep residual networks with batch normalization. \n(3) Deep Networks with Stochastic Depth\n\nThey randomly skip entire layers during training.\nTo do that, they use residual connections. They select random layers and use only the identity function for these layers (instead of the full residual block of identity + convolutions + add).\nResNet architecture: They use standard residual connections. ReLU activations, 2 convolutional layers (conv->BN->ReLU->conv->BN->add->ReLU). They use <= 64 filters per conv layer.\nWhile the standard formula for residual connections is output = ReLU(f(input) + identity(input)), their formula is output = ReLU(b * f(input) + identity(input)) with b being either 0 (inactive/removed layer) or 1 (active layer), i.e. is a sample of a bernoulli random variable.\nThe probabilities of the bernoulli random variables are now hyperparameters, similar to dropout.\nNote that the probability here means the probability of survival, i.e. high value = more survivors.\nThe probabilities could be set uniformly, e.g. to 0.5 for each variable/layer.\nThey can also be set with a linear decay, so that the first layer has a very high probability of survival, while the last layer has a very low probability of survival.\nLinear decay formula: p = 1 - (l/L)(1-q) where l is the current layer's number, L is the total number of layers, p is the survival probability of layer l and q is the desired survival probability of the last layer (e.g. 0.5).\nThey argue that linear decay is better, as the early layer extract low level features and are therefor more important.\nThe expected number of surviving layers is simply the sum of the probabilities.\nFor linear decay with q=0.5 and L=54 (i.e. 54 residual blocks = 110 total layers) the expected number of surviving blocks is roughly (3/4)L = (3/4)54 = 40, i.e. on average 14 residual blocks will be removed per training batch.\nWith linear decay and q=0.5 the expected speedup of training is about 25%. q=0.2 leads to about 40% speedup (while in one test still achieving the test error of the same network without stochastic depth).\nDepending on the q setting, they observe significantly lower test errors. They argue that stochastic depth has a regularizing effect (training an ensemble of many networks with different depths).\nSimilar to dropout, the forward pass rule during testing must be slightly changed, because the network was trained on missing values. The residual formular during test time becomes output = ReLU(p * f(input) + input) where p is the average probability with which this residual block survives during training.\n\n They randomly skip entire layers during training. To do that, they use residual connections. They select random layers and use only the identity function for these layers (instead of the full residual block of identity + convolutions + add). ResNet architecture: They use standard residual connections. ReLU activations, 2 convolutional layers (conv->BN->ReLU->conv->BN->add->ReLU). They use <= 64 filters per conv layer. While the standard formula for residual connections is output = ReLU(f(input) + identity(input)), their formula is output = ReLU(b * f(input) + identity(input)) with b being either 0 (inactive/removed layer) or 1 (active layer), i.e. is a sample of a bernoulli random variable. The probabilities of the bernoulli random variables are now hyperparameters, similar to dropout. Note that the probability here means the probability of survival, i.e. high value = more survivors. The probabilities could be set uniformly, e.g. to 0.5 for each variable/layer. They can also be set with a linear decay, so that the first layer has a very high probability of survival, while the last layer has a very low probability of survival. Linear decay formula: p = 1 - (l/L)(1-q) where l is the current layer's number, L is the total number of layers, p is the survival probability of layer l and q is the desired survival probability of the last layer (e.g. 0.5). They argue that linear decay is better, as the early layer extract low level features and are therefor more important. The expected number of surviving layers is simply the sum of the probabilities. For linear decay with q=0.5 and L=54 (i.e. 54 residual blocks = 110 total layers) the expected number of surviving blocks is roughly (3/4)L = (3/4)54 = 40, i.e. on average 14 residual blocks will be removed per training batch. With linear decay and q=0.5 the expected speedup of training is about 25%. q=0.2 leads to about 40% speedup (while in one test still achieving the test error of the same network without stochastic depth). Depending on the q setting, they observe significantly lower test errors. They argue that stochastic depth has a regularizing effect (training an ensemble of many networks with different depths). Similar to dropout, the forward pass rule during testing must be slightly changed, because the network was trained on missing values. The residual formular during test time becomes output = ReLU(p * f(input) + input) where p is the average probability with which this residual block survives during training. \n(4) Results\n\nTheir model architecture:\n\nThree chains of 18 residual blocks each, so 3*18 blocks per model.\nNumber of filters per conv. layer: 16 (first chain), 32 (second chain), 64 (third chain)\nBetween each block they use average pooling. Then they zero-pad the new dimensions (e.g. from 16 to 32 at the end of the first chain).\n\n\nCIFAR-10:\n\nTrained with SGD (momentum=0.9, dampening=0, lr=0.1 after 1st epoch, 0.01 after epoch 250, 0.001 after epoch 375).\nWeight decay/L2 of 1e-4.\nBatch size 128.\nAugmentation: Horizontal flipping, crops (4px offset).\nThey achieve 5.23% error (compared to 6.41% in the original paper about residual networks).\n\n\nCIFAR-100:\n\nSame settings as before.\n24.58% error with stochastic depth, 27.22% without.\n\n\nSVHN:\n\nThe use both the hard and easy sub-datasets of images.\nThey preprocess to zero-mean, unit-variance.\nBatch size 128.\nLearning rate is 0.1 (start), 0.01 (after epoch 30), 0.001 (after epoch 35).\n1.75% error with stochastic depth, 2.01% error without.\nNetwork without stochastic depth starts to overfit towards the end.\n\n\nStochastic depth with linear decay and q=0.5 gives ~25% speedup.\n1202-layer CIFAR-10:\n\nThey trained a 1202-layer deep network on CIFAR-10 (previous tests: 152 layers).\nWithout stochastic depth: 6.72% test error.\nWith stochastic depth: 4.91% test error.\n\n\n\n Their model architecture:\n\nThree chains of 18 residual blocks each, so 3*18 blocks per model.\nNumber of filters per conv. layer: 16 (first chain), 32 (second chain), 64 (third chain)\nBetween each block they use average pooling. Then they zero-pad the new dimensions (e.g. from 16 to 32 at the end of the first chain).\n\n Three chains of 18 residual blocks each, so 3*18 blocks per model. Number of filters per conv. layer: 16 (first chain), 32 (second chain), 64 (third chain) Between each block they use average pooling. Then they zero-pad the new dimensions (e.g. from 16 to 32 at the end of the first chain). CIFAR-10:\n\nTrained with SGD (momentum=0.9, dampening=0, lr=0.1 after 1st epoch, 0.01 after epoch 250, 0.001 after epoch 375).\nWeight decay/L2 of 1e-4.\nBatch size 128.\nAugmentation: Horizontal flipping, crops (4px offset).\nThey achieve 5.23% error (compared to 6.41% in the original paper about residual networks).\n\n Trained with SGD (momentum=0.9, dampening=0, lr=0.1 after 1st epoch, 0.01 after epoch 250, 0.001 after epoch 375). Weight decay/L2 of 1e-4. Batch size 128. Augmentation: Horizontal flipping, crops (4px offset). They achieve 5.23% error (compared to 6.41% in the original paper about residual networks). CIFAR-100:\n\nSame settings as before.\n24.58% error with stochastic depth, 27.22% without.\n\n Same settings as before. 24.58% error with stochastic depth, 27.22% without. SVHN:\n\nThe use both the hard and easy sub-datasets of images.\nThey preprocess to zero-mean, unit-variance.\nBatch size 128.\nLearning rate is 0.1 (start), 0.01 (after epoch 30), 0.001 (after epoch 35).\n1.75% error with stochastic depth, 2.01% error without.\nNetwork without stochastic depth starts to overfit towards the end.\n\n The use both the hard and easy sub-datasets of images. They preprocess to zero-mean, unit-variance. Batch size 128. Learning rate is 0.1 (start), 0.01 (after epoch 30), 0.001 (after epoch 35). 1.75% error with stochastic depth, 2.01% error without. Network without stochastic depth starts to overfit towards the end. Stochastic depth with linear decay and q=0.5 gives ~25% speedup. 1202-layer CIFAR-10:\n\nThey trained a 1202-layer deep network on CIFAR-10 (previous tests: 152 layers).\nWithout stochastic depth: 6.72% test error.\nWith stochastic depth: 4.91% test error.\n\n They trained a 1202-layer deep network on CIFAR-10 (previous tests: 152 layers). Without stochastic depth: 6.72% test error. With stochastic depth: 4.91% test error. \n(5) Analytic experiments\n\nVanishing Gradient:\n\nThey analyzed the gradient that reaches the first layer.\nThe gradient with stochastic depth is consistently higher (throughout the epochs) than without stochastic depth.\nThe difference is very significant after decreasing the learning rate.\n\n\nHyper-parameter sensitivity:\n\nThey evaluated with test error for different choices of the survival probability q.\nLinear decay schema: Values between 0.4 and 0.8 perform best. 0.5 is suggested (nearly best value, good spedup). Even 0.2 improves the test error (compared to no stochastic depth).\nUniform schema: 0.8 performs best, other values mostly significantly worse.\nLinear decay performs consistently better than the uniform schema.\n\n\n\n Vanishing Gradient:\n\nThey analyzed the gradient that reaches the first layer.\nThe gradient with stochastic depth is consistently higher (throughout the epochs) than without stochastic depth.\nThe difference is very significant after decreasing the learning rate.\n\n They analyzed the gradient that reaches the first layer. The gradient with stochastic depth is consistently higher (throughout the epochs) than without stochastic depth. The difference is very significant after decreasing the learning rate. Hyper-parameter sensitivity:\n\nThey evaluated with test error for different choices of the survival probability q.\nLinear decay schema: Values between 0.4 and 0.8 perform best. 0.5 is suggested (nearly best value, good spedup). Even 0.2 improves the test error (compared to no stochastic depth).\nUniform schema: 0.8 performs best, other values mostly significantly worse.\nLinear decay performs consistently better than the uniform schema.\n\n They evaluated with test error for different choices of the survival probability q. Linear decay schema: Values between 0.4 and 0.8 perform best. 0.5 is suggested (nearly best value, good spedup). Even 0.2 improves the test error (compared to no stochastic depth). Uniform schema: 0.8 performs best, other values mostly significantly worse. Linear decay performs consistently better than the uniform schema. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1603.09382v1"
    },
    "84": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Deep_Generative_Image_Models_using_a_Laplacian_Pyramid_of_Adversarial_Networks.md",
        "transcript": "\nWhat\n\nThe original GAN approach used one Generator (G) to generate images and one Discriminator (D) to rate these images.\nThe laplacian pyramid GAN uses multiple pairs of G and D.\nIt starts with an ordinary GAN that generates small images (say, 4x4).\nEach following pair learns to generate plausible upscalings of the image, usually by a factor of 2. (So e.g. from 4x4 to 8x8.)\nThis scaling from coarse to fine resembles a laplacian pyramid, hence the name.\n\n The original GAN approach used one Generator (G) to generate images and one Discriminator (D) to rate these images. The laplacian pyramid GAN uses multiple pairs of G and D. It starts with an ordinary GAN that generates small images (say, 4x4). Each following pair learns to generate plausible upscalings of the image, usually by a factor of 2. (So e.g. from 4x4 to 8x8.) This scaling from coarse to fine resembles a laplacian pyramid, hence the name. \nHow\n\nThe first pair of G and D is just like an ordinary GAN.\nFor each pair afterwards, G recieves the output of the previous step, upscaled to the desired size. Due to the upscaling, the image will be blurry.\nG has to learn to generate a plausible sharpening of that blurry image.\nG outputs a difference image, not the full sharpened image.\nD recieves the upscaled/blurry image. D also recieves either the optimal difference image (for images from the training set) or G's generated difference image.\nD adds the difference image to the blurry image as its first step. Afterwards it applies convolutions to the image and ends in one sigmoid unit.\nThe training procedure is just like in the ordinary GAN setting. Each upscaling pair of G and D can be trained on its own.\nThe first G recieves a \"normal\" noise vector, just like in the ordinary GAN setting. Later Gs recieve noise as one plane, so each image has four channels: R, G, B, noise.\n\n The first pair of G and D is just like an ordinary GAN. For each pair afterwards, G recieves the output of the previous step, upscaled to the desired size. Due to the upscaling, the image will be blurry. G has to learn to generate a plausible sharpening of that blurry image. G outputs a difference image, not the full sharpened image. D recieves the upscaled/blurry image. D also recieves either the optimal difference image (for images from the training set) or G's generated difference image. D adds the difference image to the blurry image as its first step. Afterwards it applies convolutions to the image and ends in one sigmoid unit. The training procedure is just like in the ordinary GAN setting. Each upscaling pair of G and D can be trained on its own. The first G recieves a \"normal\" noise vector, just like in the ordinary GAN setting. Later Gs recieve noise as one plane, so each image has four channels: R, G, B, noise. \nResults\n\nImages are rated as looking more realistic than the ones from ordinary GANs.\nThe approximated log likelihood is significantly lower (improved) compared to ordinary GANs.\nThe generated images do however still look distorted compared to real images.\nThey also tried to add class conditional information to G and D (just a one hot vector for the desired class of the image). G and D learned successfully to adapt to that information (e.g. to only generate images that seem to show birds).\n\n Images are rated as looking more realistic than the ones from ordinary GANs. The approximated log likelihood is significantly lower (improved) compared to ordinary GANs. The generated images do however still look distorted compared to real images. They also tried to add class conditional information to G and D (just a one hot vector for the desired class of the image). G and D learned successfully to adapt to that information (e.g. to only generate images that seem to show birds). \nIntroduction\n\nInstead of just one big generative model, they build multiple ones.\nThey start with one model at a small image scale (e.g. 4x4) and then add multiple generative models that increase the image size (e.g. from 4x4 to 8x8).\nThis scaling from coarse to fine (low frequency to high frequency components) resembles a laplacian pyramid, hence the name of the paper.\n\n Instead of just one big generative model, they build multiple ones. They start with one model at a small image scale (e.g. 4x4) and then add multiple generative models that increase the image size (e.g. from 4x4 to 8x8). This scaling from coarse to fine (low frequency to high frequency components) resembles a laplacian pyramid, hence the name of the paper. \nRelated Works\n\nTypes of generative image models:\n\nNon-Parametric: Models copy patches from training set (e.g. texture synthesis, super-resolution)\nParametric: E.g. Deep Boltzmann machines or denoising auto-encoders\n\n\nNovel approaches: e.g. DRAW, diffusion-based processes, LSTMs\nThis work is based on (conditional) GANs\n\n Types of generative image models:\n\nNon-Parametric: Models copy patches from training set (e.g. texture synthesis, super-resolution)\nParametric: E.g. Deep Boltzmann machines or denoising auto-encoders\n\n Non-Parametric: Models copy patches from training set (e.g. texture synthesis, super-resolution) Parametric: E.g. Deep Boltzmann machines or denoising auto-encoders Novel approaches: e.g. DRAW, diffusion-based processes, LSTMs This work is based on (conditional) GANs \nApproach\n\nThey start with a Gaussian and a Laplacian pyramid.\nThey build the Gaussian pyramid by repeatedly decreasing the image height/width by 2: [full size image, half size image, quarter size image, ...]\nThey build a Laplacian pyramid by taking pairs of images in the gaussian pyramid, upscaling the smaller one and then taking the difference.\nIn the laplacian GAN approach, an image at scale k is created by first upscaling the image at scale k-1 and then adding a refinement to it (de-blurring). The refinement is created with a GAN that recieves the upscaled image as input.\nNote that the refinement is a difference image (between the upscaled image and the optimal upscaled image).\nThe very first (small scale) image is generated by an ordinary GAN.\nD recieves an upscaled image and a difference image. It then adds them together to create an upscaled and de-blurred image. Then D applies ordinary convolutions to the result and ends in a quality rating (sigmoid).\n\n They start with a Gaussian and a Laplacian pyramid. They build the Gaussian pyramid by repeatedly decreasing the image height/width by 2: [full size image, half size image, quarter size image, ...] They build a Laplacian pyramid by taking pairs of images in the gaussian pyramid, upscaling the smaller one and then taking the difference. In the laplacian GAN approach, an image at scale k is created by first upscaling the image at scale k-1 and then adding a refinement to it (de-blurring). The refinement is created with a GAN that recieves the upscaled image as input. Note that the refinement is a difference image (between the upscaled image and the optimal upscaled image). The very first (small scale) image is generated by an ordinary GAN. D recieves an upscaled image and a difference image. It then adds them together to create an upscaled and de-blurred image. Then D applies ordinary convolutions to the result and ends in a quality rating (sigmoid). \nModel Architecture and Training\n\nDatasets: CIFAR-10 (32x32, 100k images), STL (96x96, 100k), LSUN (64x64, 10M)\nThey use a uniform distribution of [-1, 1] for their noise vectors.\nFor the upscaling Generators they add the noise as a fourth plane (to the RGB image).\nCIFAR-10: 8->14->28 (height/width), STL: 8->16->32->64->96, LSUN: 4->8->16->32->64\nCIFAR-10: G=3 layers, D=2 layers, STL: G=3 layers, D=2 layers, LSUN: G=5 layers, D=3 layers.\n\n Datasets: CIFAR-10 (32x32, 100k images), STL (96x96, 100k), LSUN (64x64, 10M) They use a uniform distribution of [-1, 1] for their noise vectors. For the upscaling Generators they add the noise as a fourth plane (to the RGB image). CIFAR-10: 8->14->28 (height/width), STL: 8->16->32->64->96, LSUN: 4->8->16->32->64 CIFAR-10: G=3 layers, D=2 layers, STL: G=3 layers, D=2 layers, LSUN: G=5 layers, D=3 layers. \nExperiments\n\nEvaluation methods:\n\nComputation of log-likelihood on a held out image set\n\nThey use a Gaussian window based Parzen estimation to approximate the probability of an image (note: not very accurate).\nThey adapt their estimation method to the special case of the laplacian pyramid.\nTheir laplacian pyramid model seems to perform significantly better than ordinary GANs.\n\n\nSubjective evaluation of generated images\n\nTheir model seems to learn the rough structure and color correlations of images to generate.\nThey add class conditional information to G and D. G indeed learns to generate different classes of images.\nAll images still have noticeable distortions.\n\n\nSubjective evaluation of generated images by other people\n\n15 volunteers.\nThey show generated or real images in an interface for 50-2000ms. Volunteer then has to decide whether the image is fake or real.\n10k ratings were collected.\nAt 2000ms, around 50% of the generated images were considered real, ~90 of the true real ones and <10% of the images generated by an ordinary GAN.\n\n\n\n\n\n Evaluation methods:\n\nComputation of log-likelihood on a held out image set\n\nThey use a Gaussian window based Parzen estimation to approximate the probability of an image (note: not very accurate).\nThey adapt their estimation method to the special case of the laplacian pyramid.\nTheir laplacian pyramid model seems to perform significantly better than ordinary GANs.\n\n\nSubjective evaluation of generated images\n\nTheir model seems to learn the rough structure and color correlations of images to generate.\nThey add class conditional information to G and D. G indeed learns to generate different classes of images.\nAll images still have noticeable distortions.\n\n\nSubjective evaluation of generated images by other people\n\n15 volunteers.\nThey show generated or real images in an interface for 50-2000ms. Volunteer then has to decide whether the image is fake or real.\n10k ratings were collected.\nAt 2000ms, around 50% of the generated images were considered real, ~90 of the true real ones and <10% of the images generated by an ordinary GAN.\n\n\n\n Computation of log-likelihood on a held out image set\n\nThey use a Gaussian window based Parzen estimation to approximate the probability of an image (note: not very accurate).\nThey adapt their estimation method to the special case of the laplacian pyramid.\nTheir laplacian pyramid model seems to perform significantly better than ordinary GANs.\n\n They use a Gaussian window based Parzen estimation to approximate the probability of an image (note: not very accurate). They adapt their estimation method to the special case of the laplacian pyramid. Their laplacian pyramid model seems to perform significantly better than ordinary GANs. Subjective evaluation of generated images\n\nTheir model seems to learn the rough structure and color correlations of images to generate.\nThey add class conditional information to G and D. G indeed learns to generate different classes of images.\nAll images still have noticeable distortions.\n\n Their model seems to learn the rough structure and color correlations of images to generate. They add class conditional information to G and D. G indeed learns to generate different classes of images. All images still have noticeable distortions. Subjective evaluation of generated images by other people\n\n15 volunteers.\nThey show generated or real images in an interface for 50-2000ms. Volunteer then has to decide whether the image is fake or real.\n10k ratings were collected.\nAt 2000ms, around 50% of the generated images were considered real, ~90 of the true real ones and <10% of the images generated by an ordinary GAN.\n\n 15 volunteers. They show generated or real images in an interface for 50-2000ms. Volunteer then has to decide whether the image is fake or real. 10k ratings were collected. At 2000ms, around 50% of the generated images were considered real, ~90 of the true real ones and <10% of the images generated by an ordinary GAN. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1506.05751"
    },
    "85": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/DRAW_A_Recurrent_Neural_Network_for_Image_Generation.md",
        "transcript": "What\n\nDRAW = deep recurrent attentive writer\nDRAW is a recurrent autoencoder for (primarily) images that uses attention mechanisms.\nLike all autoencoders it has an encoder, a latent layer Z in the \"middle\" and a decoder.\nDue to the recurrence, there are actually multiple autoencoders, one for each timestep (the number of timesteps is fixed).\nDRAW has attention mechanisms which allow the model to decide where to look at in the input image (\"glimpses\") and where to write/draw to in the output image.\nIf the attention mechanisms are skipped, the model becomes a simple recurrent autoencoder.\nBy training the full autoencoder on a dataset and then only using the decoder, one can generate new images that look similar to the dataset images.\n\n DRAW = deep recurrent attentive writer DRAW is a recurrent autoencoder for (primarily) images that uses attention mechanisms. Like all autoencoders it has an encoder, a latent layer Z in the \"middle\" and a decoder. Due to the recurrence, there are actually multiple autoencoders, one for each timestep (the number of timesteps is fixed). DRAW has attention mechanisms which allow the model to decide where to look at in the input image (\"glimpses\") and where to write/draw to in the output image. If the attention mechanisms are skipped, the model becomes a simple recurrent autoencoder. By training the full autoencoder on a dataset and then only using the decoder, one can generate new images that look similar to the dataset images. How\n\nGeneral architecture\n\nThe encoder-decoder-pair follows the design of variational autoencoders.\nThe latent layer follows an n-dimensional gaussian distribution. The hyperparameters of that distribution (means, standard deviations) are derived from the output of the encoder using a linear transformation.\nUsing a gaussian distribution enables the use of the reparameterization trick, which can be useful for backpropagation.\nThe decoder receives a sample drawn from that gaussian distribution.\nWhile the encoder reads from the input image, the decoder writes to an image canvas (where \"write\" is an addition, not a replacement of the old values).\nThe model works in a fixed number of timesteps. At each timestep the encoder performs a read operation and the decoder a write operation.\nBoth the encoder and the decoder receive the previous output of the encoder.\n\n\nLoss functions\n\nThe loss function of the latent layer is the KL-divergence between that layer's gaussian distribution and a prior, summed over the timesteps.\nThe loss function of the decoder is the negative log likelihood of the image given the final canvas content under a bernoulli distribution.\nThe total loss, which is optimized, is the expectation of the sum of both losses (latent layer loss, decoder loss).\n\n\nAttention\n\nThe selective read attention works on image patches of varying sizes. The result size is always NxN.\nThe mechanism has the following parameters:\n\ngx: x-axis coordinate of the center of the patch\ngy: y-axis coordinate of the center of the patch\ndelta: Strides. The higher the strides value, the larger the read image patch.\nsigma: Standard deviation. The higher the sigma value, the more blurry the extracted patch will be.\ngamma: Intensity-Multiplier. Will be used on the result.\nAll of these parameters are generated using a linear transformation applied to the decoder's output.\n\n\nThe mechanism places a grid of NxN gaussians on the image. The grid is centered at (gx, gy). The gaussians are delta pixels apart from each other and have a standard deviation of sigma.\nEach gaussian is applied to the image, the center pixel is read and added to the result.\n\n\n\n General architecture\n\nThe encoder-decoder-pair follows the design of variational autoencoders.\nThe latent layer follows an n-dimensional gaussian distribution. The hyperparameters of that distribution (means, standard deviations) are derived from the output of the encoder using a linear transformation.\nUsing a gaussian distribution enables the use of the reparameterization trick, which can be useful for backpropagation.\nThe decoder receives a sample drawn from that gaussian distribution.\nWhile the encoder reads from the input image, the decoder writes to an image canvas (where \"write\" is an addition, not a replacement of the old values).\nThe model works in a fixed number of timesteps. At each timestep the encoder performs a read operation and the decoder a write operation.\nBoth the encoder and the decoder receive the previous output of the encoder.\n\n The encoder-decoder-pair follows the design of variational autoencoders. The latent layer follows an n-dimensional gaussian distribution. The hyperparameters of that distribution (means, standard deviations) are derived from the output of the encoder using a linear transformation. Using a gaussian distribution enables the use of the reparameterization trick, which can be useful for backpropagation. The decoder receives a sample drawn from that gaussian distribution. While the encoder reads from the input image, the decoder writes to an image canvas (where \"write\" is an addition, not a replacement of the old values). The model works in a fixed number of timesteps. At each timestep the encoder performs a read operation and the decoder a write operation. Both the encoder and the decoder receive the previous output of the encoder. Loss functions\n\nThe loss function of the latent layer is the KL-divergence between that layer's gaussian distribution and a prior, summed over the timesteps.\nThe loss function of the decoder is the negative log likelihood of the image given the final canvas content under a bernoulli distribution.\nThe total loss, which is optimized, is the expectation of the sum of both losses (latent layer loss, decoder loss).\n\n The loss function of the latent layer is the KL-divergence between that layer's gaussian distribution and a prior, summed over the timesteps. The loss function of the decoder is the negative log likelihood of the image given the final canvas content under a bernoulli distribution. The total loss, which is optimized, is the expectation of the sum of both losses (latent layer loss, decoder loss). Attention\n\nThe selective read attention works on image patches of varying sizes. The result size is always NxN.\nThe mechanism has the following parameters:\n\ngx: x-axis coordinate of the center of the patch\ngy: y-axis coordinate of the center of the patch\ndelta: Strides. The higher the strides value, the larger the read image patch.\nsigma: Standard deviation. The higher the sigma value, the more blurry the extracted patch will be.\ngamma: Intensity-Multiplier. Will be used on the result.\nAll of these parameters are generated using a linear transformation applied to the decoder's output.\n\n\nThe mechanism places a grid of NxN gaussians on the image. The grid is centered at (gx, gy). The gaussians are delta pixels apart from each other and have a standard deviation of sigma.\nEach gaussian is applied to the image, the center pixel is read and added to the result.\n\n The selective read attention works on image patches of varying sizes. The result size is always NxN. The mechanism has the following parameters:\n\ngx: x-axis coordinate of the center of the patch\ngy: y-axis coordinate of the center of the patch\ndelta: Strides. The higher the strides value, the larger the read image patch.\nsigma: Standard deviation. The higher the sigma value, the more blurry the extracted patch will be.\ngamma: Intensity-Multiplier. Will be used on the result.\nAll of these parameters are generated using a linear transformation applied to the decoder's output.\n\n gx: x-axis coordinate of the center of the patch gy: y-axis coordinate of the center of the patch delta: Strides. The higher the strides value, the larger the read image patch. sigma: Standard deviation. The higher the sigma value, the more blurry the extracted patch will be. gamma: Intensity-Multiplier. Will be used on the result. All of these parameters are generated using a linear transformation applied to the decoder's output. The mechanism places a grid of NxN gaussians on the image. The grid is centered at (gx, gy). The gaussians are delta pixels apart from each other and have a standard deviation of sigma. Each gaussian is applied to the image, the center pixel is read and added to the result. Results\n\nRealistic looking generated images for MNIST and SVHN.\nStructurally OK, but overall blurry images for CIFAR-10.\nResults with attention are usually significantly better than without attention.\nImage generation without attention starts with a blurry image and progressively sharpens it.\n\n Realistic looking generated images for MNIST and SVHN. Structurally OK, but overall blurry images for CIFAR-10. Results with attention are usually significantly better than without attention. Image generation without attention starts with a blurry image and progressively sharpens it. \n\nIntroduction\n\n\nThe natural way to draw an image is in a step by step way (add some lines, then add some more, etc.).\nMost generative neural networks however create the image in one step.\nThat removes the possibility of iterative self-correction, is hard to scale to large images and makes the image generation process dependent on a single latent distribution (input parameters).\nThe DRAW architecture generates images in multiple steps, allowing refinements/corrections.\nDRAW is based on varational autoencoders: An encoder compresses images to codes and a decoder generates images from codes.\nThe loss function is a variational upper bound on the log-likelihood of the data.\nDRAW uses recurrance to generate images step by step.\nThe recurrance is combined with attention via partial glimpses/foveations (i.e. the model sees only a small part of the image).\nAttention is implemented in a differentiable way in DRAW.\n\n Introduction The natural way to draw an image is in a step by step way (add some lines, then add some more, etc.). Most generative neural networks however create the image in one step. That removes the possibility of iterative self-correction, is hard to scale to large images and makes the image generation process dependent on a single latent distribution (input parameters). The DRAW architecture generates images in multiple steps, allowing refinements/corrections. DRAW is based on varational autoencoders: An encoder compresses images to codes and a decoder generates images from codes. The loss function is a variational upper bound on the log-likelihood of the data. DRAW uses recurrance to generate images step by step. The recurrance is combined with attention via partial glimpses/foveations (i.e. the model sees only a small part of the image). Attention is implemented in a differentiable way in DRAW. \n\nThe DRAW Network\n\n\nThe DRAW architecture is based on variational autoencoders:\n\nEncoder: Compresses an image to latent codes, which represent the information contained in the image.\nDecoder: Transforms the codes from the encoder to images (i.e. defines a distribution over images which is conditioned on the distribution of codes).\n\n\nDifferences to variational autoencoders:\n\nEncoder and decoder are both recurrent neural networks.\nThe encoder receives the previous output of the decoder.\nThe decoder writes several times to the image array (instead of only once).\nThe encoder has an attention mechanism. It can make a decision about the read location in the input image.\nThe decoder has an attention mechanism. It can make a decision about the write location in the output image.\n\n\n2.1 Network architecture\n\nThey use LSTMs for the encoder and decoder.\nThe encoder generates a vector.\nThe decoder generates a vector.\nThe encoder receives at each time step the image and the output of the previous decoding step.\nThe hidden layer in between encoder and decoder is a distribution Q(Zt|ht^enc), which is a diagonal gaussian.\nThe mean and standard deviation of that gaussian is derived from the encoder's output vector with a linear transformation.\nUsing a gaussian instead of a bernoulli distribution enables the use of the reparameterization trick. That trick makes it straightforward to backpropagate \"low variance stochastic gradients of the loss function through the latent distribution\".\nThe decoder writes to an image canvas. At every timestep the vector generated by the decoder is added to that canvas.\n\n\n2.2 Loss function\n\nThe main loss function is the negative log probability: -log D(x|ct), where x is the input image and ct is the final output image of the autoencoder. D is a bernoulli distribution if the image is binary (only 0s and 1s).\nThe model also uses a latent loss for the latent layer (between encoder and decoder). That is typical for VAEs. The loss is the KL-Divergence between Q(Zt|ht_enc) (Zt = latent layer, ht_enc = result of encoder) and a prior P(Zt).\nThe full loss function is the expection value of both losses added up.\n\n\n2.3 Stochastic Data Generation\n\nTo generate images, samples can be picked from the latent layer based on a prior. These samples are then fed into the decoder. That is repeated for several timesteps until the image is finished.\n\n\n\n The DRAW Network The DRAW architecture is based on variational autoencoders:\n\nEncoder: Compresses an image to latent codes, which represent the information contained in the image.\nDecoder: Transforms the codes from the encoder to images (i.e. defines a distribution over images which is conditioned on the distribution of codes).\n\n Encoder: Compresses an image to latent codes, which represent the information contained in the image. Decoder: Transforms the codes from the encoder to images (i.e. defines a distribution over images which is conditioned on the distribution of codes). Differences to variational autoencoders:\n\nEncoder and decoder are both recurrent neural networks.\nThe encoder receives the previous output of the decoder.\nThe decoder writes several times to the image array (instead of only once).\nThe encoder has an attention mechanism. It can make a decision about the read location in the input image.\nThe decoder has an attention mechanism. It can make a decision about the write location in the output image.\n\n Encoder and decoder are both recurrent neural networks. The encoder receives the previous output of the decoder. The decoder writes several times to the image array (instead of only once). The encoder has an attention mechanism. It can make a decision about the read location in the input image. The decoder has an attention mechanism. It can make a decision about the write location in the output image. 2.1 Network architecture\n\nThey use LSTMs for the encoder and decoder.\nThe encoder generates a vector.\nThe decoder generates a vector.\nThe encoder receives at each time step the image and the output of the previous decoding step.\nThe hidden layer in between encoder and decoder is a distribution Q(Zt|ht^enc), which is a diagonal gaussian.\nThe mean and standard deviation of that gaussian is derived from the encoder's output vector with a linear transformation.\nUsing a gaussian instead of a bernoulli distribution enables the use of the reparameterization trick. That trick makes it straightforward to backpropagate \"low variance stochastic gradients of the loss function through the latent distribution\".\nThe decoder writes to an image canvas. At every timestep the vector generated by the decoder is added to that canvas.\n\n They use LSTMs for the encoder and decoder. The encoder generates a vector. The decoder generates a vector. The encoder receives at each time step the image and the output of the previous decoding step. The hidden layer in between encoder and decoder is a distribution Q(Zt|ht^enc), which is a diagonal gaussian. The mean and standard deviation of that gaussian is derived from the encoder's output vector with a linear transformation. Using a gaussian instead of a bernoulli distribution enables the use of the reparameterization trick. That trick makes it straightforward to backpropagate \"low variance stochastic gradients of the loss function through the latent distribution\". The decoder writes to an image canvas. At every timestep the vector generated by the decoder is added to that canvas. 2.2 Loss function\n\nThe main loss function is the negative log probability: -log D(x|ct), where x is the input image and ct is the final output image of the autoencoder. D is a bernoulli distribution if the image is binary (only 0s and 1s).\nThe model also uses a latent loss for the latent layer (between encoder and decoder). That is typical for VAEs. The loss is the KL-Divergence between Q(Zt|ht_enc) (Zt = latent layer, ht_enc = result of encoder) and a prior P(Zt).\nThe full loss function is the expection value of both losses added up.\n\n The main loss function is the negative log probability: -log D(x|ct), where x is the input image and ct is the final output image of the autoencoder. D is a bernoulli distribution if the image is binary (only 0s and 1s). The model also uses a latent loss for the latent layer (between encoder and decoder). That is typical for VAEs. The loss is the KL-Divergence between Q(Zt|ht_enc) (Zt = latent layer, ht_enc = result of encoder) and a prior P(Zt). The full loss function is the expection value of both losses added up. 2.3 Stochastic Data Generation\n\nTo generate images, samples can be picked from the latent layer based on a prior. These samples are then fed into the decoder. That is repeated for several timesteps until the image is finished.\n\n To generate images, samples can be picked from the latent layer based on a prior. These samples are then fed into the decoder. That is repeated for several timesteps until the image is finished. \n\nRead and Write Operations\n\n\n3.1 Reading and writing without attention\n\nWithout attention, DRAW simply reads in the whole image and modifies the whole output image canvas at every timestep.\n\n\n3.2 Selective attention model\n\nThe model can decide which parts of the image to read, i.e. where to look at. These looks are called glimpses.\nEach glimpse is defined by its center (x, y), its stride (zoom level), its gaussian variance (the higher the variance, the more blurry is the result) and a scalar multiplier (that scales the intensity of the glimpse result).\nThese parameters are calculated based on the decoder output using a linear transformation.\nFor an NxN patch/glimpse N*N gaussians are created and applied to the image. The center pixel of each gaussian is then used as the respective output pixel of the glimpse.\n\n\n3.3 Reading and writing with attention\n\nMostly the same technique from (3.2) is applied to both reading and writing.\nThe glimpse parameters are generated from the decoder output in both cases. The parameters can be different (i.e. read and write at different positions).\nFor RGB the same glimpses are applied to each channel.\n\n\n\n Read and Write Operations 3.1 Reading and writing without attention\n\nWithout attention, DRAW simply reads in the whole image and modifies the whole output image canvas at every timestep.\n\n Without attention, DRAW simply reads in the whole image and modifies the whole output image canvas at every timestep. 3.2 Selective attention model\n\nThe model can decide which parts of the image to read, i.e. where to look at. These looks are called glimpses.\nEach glimpse is defined by its center (x, y), its stride (zoom level), its gaussian variance (the higher the variance, the more blurry is the result) and a scalar multiplier (that scales the intensity of the glimpse result).\nThese parameters are calculated based on the decoder output using a linear transformation.\nFor an NxN patch/glimpse N*N gaussians are created and applied to the image. The center pixel of each gaussian is then used as the respective output pixel of the glimpse.\n\n The model can decide which parts of the image to read, i.e. where to look at. These looks are called glimpses. Each glimpse is defined by its center (x, y), its stride (zoom level), its gaussian variance (the higher the variance, the more blurry is the result) and a scalar multiplier (that scales the intensity of the glimpse result). These parameters are calculated based on the decoder output using a linear transformation. For an NxN patch/glimpse N*N gaussians are created and applied to the image. The center pixel of each gaussian is then used as the respective output pixel of the glimpse. 3.3 Reading and writing with attention\n\nMostly the same technique from (3.2) is applied to both reading and writing.\nThe glimpse parameters are generated from the decoder output in both cases. The parameters can be different (i.e. read and write at different positions).\nFor RGB the same glimpses are applied to each channel.\n\n Mostly the same technique from (3.2) is applied to both reading and writing. The glimpse parameters are generated from the decoder output in both cases. The parameters can be different (i.e. read and write at different positions). For RGB the same glimpses are applied to each channel. \n\nExperimental results\n\n\nThey train on binary MNIST, cluttered MNIST, SVHN and CIFAR-10.\nThey then classfiy the images (cluttered MNIST) or generate new images (other datasets).\nThey say that these generated images are unique (to which degree?) and that they look realistic for MNIST and SVHN.\nResults on CIFAR-10 are blurry.\nThey use binary crossentropy as the loss function for binary MNIST.\nThey use crossentropy as the loss function for SVHN and CIFAR-10 (color).\nThey used Adam as their optimizer.\n4.1 Cluttered MNIST classification\n\nThey classify images of cluttered MNIST. To do that, they use an LSTM that performs N read-glimpses and then classifies via a softmax layer.\nTheir model's error rate is significantly below a previous non-differentiable attention based model.\nPerforming more glimpses seems to decrease the error rate further.\n\n\n4.2 MNIST generation\n\nThey generate binary MNIST images using only the decoder.\nDRAW without attention seems to perform similarly to previous models.\nDRAW with attention seems to perform significantly better than previous models.\nDRAW without attention progressively sharpens images.\nDRAW with attention draws lines by tracing them.\n\n\n4.3 MNIST generation with two digits\n\nThey created a dataset of 60x60 images, each of them containing two random 28x28 MNIST images.\nThey then generated new images using only the decoder.\nDRAW learned to do that.\nUsing attention, the model usually first drew one digit then the other.\n\n\n4.4 Street view house number generation\n\nThey generate SVHN images using only the decoder.\nResults look quite realistic.\n\n\n4.5 Generating CIFAR images\n\nThey generate CIFAR-10 images using only the decoder.\nResults follow roughly the structure of CIFAR-images, but look blurry.\n\n\n\n Experimental results They train on binary MNIST, cluttered MNIST, SVHN and CIFAR-10. They then classfiy the images (cluttered MNIST) or generate new images (other datasets). They say that these generated images are unique (to which degree?) and that they look realistic for MNIST and SVHN. Results on CIFAR-10 are blurry. They use binary crossentropy as the loss function for binary MNIST. They use crossentropy as the loss function for SVHN and CIFAR-10 (color). They used Adam as their optimizer. 4.1 Cluttered MNIST classification\n\nThey classify images of cluttered MNIST. To do that, they use an LSTM that performs N read-glimpses and then classifies via a softmax layer.\nTheir model's error rate is significantly below a previous non-differentiable attention based model.\nPerforming more glimpses seems to decrease the error rate further.\n\n They classify images of cluttered MNIST. To do that, they use an LSTM that performs N read-glimpses and then classifies via a softmax layer. Their model's error rate is significantly below a previous non-differentiable attention based model. Performing more glimpses seems to decrease the error rate further. 4.2 MNIST generation\n\nThey generate binary MNIST images using only the decoder.\nDRAW without attention seems to perform similarly to previous models.\nDRAW with attention seems to perform significantly better than previous models.\nDRAW without attention progressively sharpens images.\nDRAW with attention draws lines by tracing them.\n\n They generate binary MNIST images using only the decoder. DRAW without attention seems to perform similarly to previous models. DRAW with attention seems to perform significantly better than previous models. DRAW without attention progressively sharpens images. DRAW with attention draws lines by tracing them. 4.3 MNIST generation with two digits\n\nThey created a dataset of 60x60 images, each of them containing two random 28x28 MNIST images.\nThey then generated new images using only the decoder.\nDRAW learned to do that.\nUsing attention, the model usually first drew one digit then the other.\n\n They created a dataset of 60x60 images, each of them containing two random 28x28 MNIST images. They then generated new images using only the decoder. DRAW learned to do that. Using attention, the model usually first drew one digit then the other. 4.4 Street view house number generation\n\nThey generate SVHN images using only the decoder.\nResults look quite realistic.\n\n They generate SVHN images using only the decoder. Results look quite realistic. 4.5 Generating CIFAR images\n\nThey generate CIFAR-10 images using only the decoder.\nResults follow roughly the structure of CIFAR-images, but look blurry.\n\n They generate CIFAR-10 images using only the decoder. Results follow roughly the structure of CIFAR-images, but look blurry. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1502.04623"
    },
    "86": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Generating_Images_with_Perceptual_Similarity_Metrics_based_on_Deep_Networks.md",
        "transcript": "\nWhat\n\nThe authors define in this paper a special loss function (DeePSiM), mostly for autoencoders.\nUsually one would use a MSE of euclidean distance as the loss function for an autoencoder. But that loss function basically always leads to blurry reconstructed images.\nThey add two new ingredients to the loss function, which results in significantly sharper looking images.\n\n The authors define in this paper a special loss function (DeePSiM), mostly for autoencoders. Usually one would use a MSE of euclidean distance as the loss function for an autoencoder. But that loss function basically always leads to blurry reconstructed images. They add two new ingredients to the loss function, which results in significantly sharper looking images. \nHow\n\nTheir loss function has three components:\n\nEuclidean distance in image space (i.e. pixel distance between reconstructed image and original image, as usually used in autoencoders)\nEuclidean distance in feature space. Another pretrained neural net (e.g. VGG, AlexNet, ...) is used to extract features from the original and the reconstructed image. Then the euclidean distance between both vectors is measured.\nAdversarial loss, as usually used in GANs (generative adversarial networks). The autoencoder is here treated as the GAN-Generator. Then a second network, the GAN-Discriminator is introduced. They are trained in the typical GAN-fashion. The loss component for DeePSiM is the loss of the Discriminator. I.e. when reconstructing an image, the autoencoder would learn to reconstruct it in a way that lets the Discriminator believe that the image is real.\n\n\nUsing the loss in feature space alone would not be enough as that tends to lead to overpronounced high frequency components in the image (i.e. too strong edges, corners, other artefacts).\nTo decrease these high frequency components, a \"natural image prior\" is usually used. Other papers define some function by hand. This paper uses the adversarial loss for that (i.e. learns a good prior).\nInstead of training a full autoencoder (encoder + decoder) it is also possible to only train a decoder and feed features - e.g. extracted via AlexNet - into the decoder.\n\n Their loss function has three components:\n\nEuclidean distance in image space (i.e. pixel distance between reconstructed image and original image, as usually used in autoencoders)\nEuclidean distance in feature space. Another pretrained neural net (e.g. VGG, AlexNet, ...) is used to extract features from the original and the reconstructed image. Then the euclidean distance between both vectors is measured.\nAdversarial loss, as usually used in GANs (generative adversarial networks). The autoencoder is here treated as the GAN-Generator. Then a second network, the GAN-Discriminator is introduced. They are trained in the typical GAN-fashion. The loss component for DeePSiM is the loss of the Discriminator. I.e. when reconstructing an image, the autoencoder would learn to reconstruct it in a way that lets the Discriminator believe that the image is real.\n\n Euclidean distance in image space (i.e. pixel distance between reconstructed image and original image, as usually used in autoencoders) Euclidean distance in feature space. Another pretrained neural net (e.g. VGG, AlexNet, ...) is used to extract features from the original and the reconstructed image. Then the euclidean distance between both vectors is measured. Adversarial loss, as usually used in GANs (generative adversarial networks). The autoencoder is here treated as the GAN-Generator. Then a second network, the GAN-Discriminator is introduced. They are trained in the typical GAN-fashion. The loss component for DeePSiM is the loss of the Discriminator. I.e. when reconstructing an image, the autoencoder would learn to reconstruct it in a way that lets the Discriminator believe that the image is real. Using the loss in feature space alone would not be enough as that tends to lead to overpronounced high frequency components in the image (i.e. too strong edges, corners, other artefacts). To decrease these high frequency components, a \"natural image prior\" is usually used. Other papers define some function by hand. This paper uses the adversarial loss for that (i.e. learns a good prior). Instead of training a full autoencoder (encoder + decoder) it is also possible to only train a decoder and feed features - e.g. extracted via AlexNet - into the decoder. \nResults\n\nUsing the DeePSiM loss with a normal autoencoder results in sharp reconstructed images.\nUsing the DeePSiM loss with a VAE to generate ILSVRC-2012 images results in sharp images, which are locally sound, but globally don't make sense. Simple euclidean distance loss results in blurry images.\nUsing the DeePSiM loss when feeding only image space features (extracted via AlexNet) into the decoder leads to high quality reconstructions. Features from early layers will lead to more exact reconstructions.\nOne can again feed extracted features into the network, but then take the reconstructed image, extract features of that image and feed them back into the network. When using DeePSiM, even after several iterations of that process the images still remain semantically similar, while their exact appearance changes (e.g. a dog's fur color might change, counts of visible objects change).\n\n Using the DeePSiM loss with a normal autoencoder results in sharp reconstructed images. Using the DeePSiM loss with a VAE to generate ILSVRC-2012 images results in sharp images, which are locally sound, but globally don't make sense. Simple euclidean distance loss results in blurry images. Using the DeePSiM loss when feeding only image space features (extracted via AlexNet) into the decoder leads to high quality reconstructions. Features from early layers will lead to more exact reconstructions. One can again feed extracted features into the network, but then take the reconstructed image, extract features of that image and feed them back into the network. When using DeePSiM, even after several iterations of that process the images still remain semantically similar, while their exact appearance changes (e.g. a dog's fur color might change, counts of visible objects change). \n(1) Introduction\n\nUsing a MSE of euclidean distances for image generation (e.g. autoencoders) often results in blurry images.\nThey suggest a better loss function that cares about the existence of features, but not as much about their exact translation, rotation or other local statistics.\nTheir loss function is based on distances in suitable feature spaces.\nThey use ConvNets to generate those feature spaces, as these networks are sensitive towards important changes (e.g. edges) and insensitive towards unimportant changes (e.g. translation).\nHowever, naively using the ConvNet features does not yield good results, because the networks tend to project very different images onto the same feature vectors (i.e. they are contractive). That leads to artefacts in the generated images.\nInstead, they combine the feature based loss with GANs (adversarial loss). The adversarial loss decreases the negative effects of the feature loss (\"natural image prior\").\n\n Using a MSE of euclidean distances for image generation (e.g. autoencoders) often results in blurry images. They suggest a better loss function that cares about the existence of features, but not as much about their exact translation, rotation or other local statistics. Their loss function is based on distances in suitable feature spaces. They use ConvNets to generate those feature spaces, as these networks are sensitive towards important changes (e.g. edges) and insensitive towards unimportant changes (e.g. translation). However, naively using the ConvNet features does not yield good results, because the networks tend to project very different images onto the same feature vectors (i.e. they are contractive). That leads to artefacts in the generated images. Instead, they combine the feature based loss with GANs (adversarial loss). The adversarial loss decreases the negative effects of the feature loss (\"natural image prior\"). \n(3) Model\n\nA typical choice for the loss function in image generation tasks (e.g. when using an autoencoders) would be squared euclidean/L2 loss or L1 loss.\nThey suggest a new class of losses called \"DeePSiM\".\nWe have a Generator G, a Discriminator D, a feature space creator C (takes an image, outputs a feature space for that image), one (or more) input images x and one (or more) target images y. Input and target image can be identical.\nThe total DeePSiM loss is a weighted sum of three components:\n\nFeature loss: Squared euclidean distance between the feature spaces of (1) input after fed through G and (2) the target image, i.e. ||C(G(x))-C(y)||^2_2.\nAdversarial loss: A discriminator is introduced to estimate the \"fakeness\" of images generated by the generator. The losses for D and G are the standard GAN losses.\nPixel space loss: Classic squared euclidean distance (as commonly used in autoencoders). They found that this loss stabilized their adversarial training.\n\n\nThe feature loss alone would create high frequency artefacts in the generated image, which is why a second loss (\"natural image prior\") is needed. The adversarial loss fulfills that role.\nArchitectures\n\nGenerator (G):\n\nThey define different ones based on the task.\nThey all use up-convolutions, which they implement by stacking two layers: (1) a linear upsampling layer, then (2) a normal convolutional layer.\nThey use leaky ReLUs (alpha=0.3).\n\n\nComparators (C):\n\nThey use variations of AlexNet and Exemplar-CNN.\nThey extract the features from different layers, depending on the experiment.\n\n\nDiscriminator (D):\n\n5 convolutions (with some striding; 7x7 then 5x5, afterwards 3x3), into average pooling, then dropout, then 2x linear, then 2-way softmax.\n\n\n\n\nTraining details\n\nThey use Adam with learning rate 0.0002 and normal momentums (0.9 and 0.999).\nThey temporarily stop the discriminator training when it gets too good.\nBatch size was 64.\n500k to 1000k batches per training.\n\n\n\n A typical choice for the loss function in image generation tasks (e.g. when using an autoencoders) would be squared euclidean/L2 loss or L1 loss. They suggest a new class of losses called \"DeePSiM\". We have a Generator G, a Discriminator D, a feature space creator C (takes an image, outputs a feature space for that image), one (or more) input images x and one (or more) target images y. Input and target image can be identical. The total DeePSiM loss is a weighted sum of three components:\n\nFeature loss: Squared euclidean distance between the feature spaces of (1) input after fed through G and (2) the target image, i.e. ||C(G(x))-C(y)||^2_2.\nAdversarial loss: A discriminator is introduced to estimate the \"fakeness\" of images generated by the generator. The losses for D and G are the standard GAN losses.\nPixel space loss: Classic squared euclidean distance (as commonly used in autoencoders). They found that this loss stabilized their adversarial training.\n\n Feature loss: Squared euclidean distance between the feature spaces of (1) input after fed through G and (2) the target image, i.e. ||C(G(x))-C(y)||^2_2. Adversarial loss: A discriminator is introduced to estimate the \"fakeness\" of images generated by the generator. The losses for D and G are the standard GAN losses. Pixel space loss: Classic squared euclidean distance (as commonly used in autoencoders). They found that this loss stabilized their adversarial training. The feature loss alone would create high frequency artefacts in the generated image, which is why a second loss (\"natural image prior\") is needed. The adversarial loss fulfills that role. Architectures\n\nGenerator (G):\n\nThey define different ones based on the task.\nThey all use up-convolutions, which they implement by stacking two layers: (1) a linear upsampling layer, then (2) a normal convolutional layer.\nThey use leaky ReLUs (alpha=0.3).\n\n\nComparators (C):\n\nThey use variations of AlexNet and Exemplar-CNN.\nThey extract the features from different layers, depending on the experiment.\n\n\nDiscriminator (D):\n\n5 convolutions (with some striding; 7x7 then 5x5, afterwards 3x3), into average pooling, then dropout, then 2x linear, then 2-way softmax.\n\n\n\n Generator (G):\n\nThey define different ones based on the task.\nThey all use up-convolutions, which they implement by stacking two layers: (1) a linear upsampling layer, then (2) a normal convolutional layer.\nThey use leaky ReLUs (alpha=0.3).\n\n They define different ones based on the task. They all use up-convolutions, which they implement by stacking two layers: (1) a linear upsampling layer, then (2) a normal convolutional layer. They use leaky ReLUs (alpha=0.3). Comparators (C):\n\nThey use variations of AlexNet and Exemplar-CNN.\nThey extract the features from different layers, depending on the experiment.\n\n They use variations of AlexNet and Exemplar-CNN. They extract the features from different layers, depending on the experiment. Discriminator (D):\n\n5 convolutions (with some striding; 7x7 then 5x5, afterwards 3x3), into average pooling, then dropout, then 2x linear, then 2-way softmax.\n\n 5 convolutions (with some striding; 7x7 then 5x5, afterwards 3x3), into average pooling, then dropout, then 2x linear, then 2-way softmax. Training details\n\nThey use Adam with learning rate 0.0002 and normal momentums (0.9 and 0.999).\nThey temporarily stop the discriminator training when it gets too good.\nBatch size was 64.\n500k to 1000k batches per training.\n\n They use Adam with learning rate 0.0002 and normal momentums (0.9 and 0.999). They temporarily stop the discriminator training when it gets too good. Batch size was 64. 500k to 1000k batches per training. \n(4) Experiments\n\nAutoencoder\n\nSimple autoencoder with an 8x8x8 code layer between encoder and decoder (so actually more values than in the input image?!).\nEncoder has a few convolutions, decoder a few up-convolutions (linear upsampling + convolution).\nThey train on STL-10 (96x96) and take random 64x64 crops.\nUsing for C AlexNet tends to break small structural details, using Exempler-CNN breaks color details.\nThe autoencoder with their loss tends to produce less blurry images than the common L2 and L1 based losses.\nTraining an SVM on the 8x8x8 hidden layer performs significantly with their loss than L2/L1. That indicates potential for unsupervised learning.\n\n\nVariational Autoencoder\n\nThey replace part of the standard VAE loss with their DeePSiM loss (keeping the KL divergence term).\nEverything else is just like in a standard VAE.\nSamples generated by a VAE with normal loss function look very blurry. Samples generated with their loss function look crisp and have locally sound statistics, but still (globally) don't really make any sense.\n\n\nInverting AlexNet\n\nAssume the following variables:\n\nI: An image\nConvNet: A convolutional network\nF: The features extracted by a ConvNet, i.e. ConvNet(I) (feaures in all layers, not just the last one)\n\n\nThen you can invert the representation of a network in two ways:\n\n(1) An inversion that takes an F and returns roughly the I that resulted in F (it's not key here that ConvNet(reconstructed I) returns the same F again).\n(2) An inversion that takes an F and projects it to some I so that ConvNet(I) returns roughly the same F again.\n\n\nSimilar to the autoencoder cases, they define a decoder, but not encoder.\nThey feed into the decoder a feature representation of an image. The features are extracted using AlexNet (they try the features from different layers).\nThe decoder has to reconstruct the original image (i.e. inversion scenario 1). They use their DeePSiM loss during the training.\nThe images can be reonstructed quite well from the last convolutional layer in AlexNet. Chosing the later fully connected layers results in more errors (specifially in the case of the very last layer).\nThey also try their luck with the inversion scenario (2), but didn't succeed (as their loss function does not care about diversity).\nThey iteratively encode and decode the same image multiple times (probably means: image -> features via AlexNet -> decode -> reconstructed image -> features via AlexNet -> decode -> ...). They observe, that the image does not get \"destroyed\", but rather changes semantically, e.g. three apples might turn to one after several steps.\nThey interpolate between images. The interpolations are smooth.\n\n\n\n Autoencoder\n\nSimple autoencoder with an 8x8x8 code layer between encoder and decoder (so actually more values than in the input image?!).\nEncoder has a few convolutions, decoder a few up-convolutions (linear upsampling + convolution).\nThey train on STL-10 (96x96) and take random 64x64 crops.\nUsing for C AlexNet tends to break small structural details, using Exempler-CNN breaks color details.\nThe autoencoder with their loss tends to produce less blurry images than the common L2 and L1 based losses.\nTraining an SVM on the 8x8x8 hidden layer performs significantly with their loss than L2/L1. That indicates potential for unsupervised learning.\n\n Simple autoencoder with an 8x8x8 code layer between encoder and decoder (so actually more values than in the input image?!). Encoder has a few convolutions, decoder a few up-convolutions (linear upsampling + convolution). They train on STL-10 (96x96) and take random 64x64 crops. Using for C AlexNet tends to break small structural details, using Exempler-CNN breaks color details. The autoencoder with their loss tends to produce less blurry images than the common L2 and L1 based losses. Training an SVM on the 8x8x8 hidden layer performs significantly with their loss than L2/L1. That indicates potential for unsupervised learning. Variational Autoencoder\n\nThey replace part of the standard VAE loss with their DeePSiM loss (keeping the KL divergence term).\nEverything else is just like in a standard VAE.\nSamples generated by a VAE with normal loss function look very blurry. Samples generated with their loss function look crisp and have locally sound statistics, but still (globally) don't really make any sense.\n\n They replace part of the standard VAE loss with their DeePSiM loss (keeping the KL divergence term). Everything else is just like in a standard VAE. Samples generated by a VAE with normal loss function look very blurry. Samples generated with their loss function look crisp and have locally sound statistics, but still (globally) don't really make any sense. Inverting AlexNet\n\nAssume the following variables:\n\nI: An image\nConvNet: A convolutional network\nF: The features extracted by a ConvNet, i.e. ConvNet(I) (feaures in all layers, not just the last one)\n\n\nThen you can invert the representation of a network in two ways:\n\n(1) An inversion that takes an F and returns roughly the I that resulted in F (it's not key here that ConvNet(reconstructed I) returns the same F again).\n(2) An inversion that takes an F and projects it to some I so that ConvNet(I) returns roughly the same F again.\n\n\nSimilar to the autoencoder cases, they define a decoder, but not encoder.\nThey feed into the decoder a feature representation of an image. The features are extracted using AlexNet (they try the features from different layers).\nThe decoder has to reconstruct the original image (i.e. inversion scenario 1). They use their DeePSiM loss during the training.\nThe images can be reonstructed quite well from the last convolutional layer in AlexNet. Chosing the later fully connected layers results in more errors (specifially in the case of the very last layer).\nThey also try their luck with the inversion scenario (2), but didn't succeed (as their loss function does not care about diversity).\nThey iteratively encode and decode the same image multiple times (probably means: image -> features via AlexNet -> decode -> reconstructed image -> features via AlexNet -> decode -> ...). They observe, that the image does not get \"destroyed\", but rather changes semantically, e.g. three apples might turn to one after several steps.\nThey interpolate between images. The interpolations are smooth.\n\n Assume the following variables:\n\nI: An image\nConvNet: A convolutional network\nF: The features extracted by a ConvNet, i.e. ConvNet(I) (feaures in all layers, not just the last one)\n\n I: An image ConvNet: A convolutional network F: The features extracted by a ConvNet, i.e. ConvNet(I) (feaures in all layers, not just the last one) Then you can invert the representation of a network in two ways:\n\n(1) An inversion that takes an F and returns roughly the I that resulted in F (it's not key here that ConvNet(reconstructed I) returns the same F again).\n(2) An inversion that takes an F and projects it to some I so that ConvNet(I) returns roughly the same F again.\n\n (1) An inversion that takes an F and returns roughly the I that resulted in F (it's not key here that ConvNet(reconstructed I) returns the same F again). (2) An inversion that takes an F and projects it to some I so that ConvNet(I) returns roughly the same F again. Similar to the autoencoder cases, they define a decoder, but not encoder. They feed into the decoder a feature representation of an image. The features are extracted using AlexNet (they try the features from different layers). The decoder has to reconstruct the original image (i.e. inversion scenario 1). They use their DeePSiM loss during the training. The images can be reonstructed quite well from the last convolutional layer in AlexNet. Chosing the later fully connected layers results in more errors (specifially in the case of the very last layer). They also try their luck with the inversion scenario (2), but didn't succeed (as their loss function does not care about diversity). They iteratively encode and decode the same image multiple times (probably means: image -> features via AlexNet -> decode -> reconstructed image -> features via AlexNet -> decode -> ...). They observe, that the image does not get \"destroyed\", but rather changes semantically, e.g. three apples might turn to one after several steps. They interpolate between images. The interpolations are smooth. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1602.02644"
    },
    "87": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Generative_Moment_Matching_Networks.md",
        "transcript": "\nWhat\n\nGenerative Moment Matching Networks (GMMN) are generative models that use maximum mean discrepancy (MMD) for their objective function.\nMMD is a measure of how similar two datasets are (here: generated dataset and training set).\nGMMNs are similar to GANs, but they replace the Discriminator with the MMD measure, making their optimization more stable.\n\n Generative Moment Matching Networks (GMMN) are generative models that use maximum mean discrepancy (MMD) for their objective function. MMD is a measure of how similar two datasets are (here: generated dataset and training set). GMMNs are similar to GANs, but they replace the Discriminator with the MMD measure, making their optimization more stable. \nHow\n\nMMD calculates a similarity measure by comparing statistics of two datasets with each other.\nMMD is calculated based on samples from the training set and the generated dataset.\nA kernel function is applied to pairs of these samples (thus the statistics are acutally calculated in high-dimensional spaces). The authors use Gaussian kernels.\nMMD can be approximated using a small number of samples.\nMMD is differentiable and therefor can be used as a standard loss function.\nThey train two models:\n\nGMMN: Noise vector input (as in GANs), several ReLU layers into one sigmoid layer. MMD as the loss function.\nGMMN+AE: Same as GMMN, but the sigmoid output is not an image, but instead the code that gets fed into an autoencoder's (AE) decoder. The AE is trained separately on the dataset. MMD is backpropagated through the decoder and then the GMMN. I.e. the GMMN learns to produce codes that let the decoder generate good looking images.\n\n\n\n MMD calculates a similarity measure by comparing statistics of two datasets with each other. MMD is calculated based on samples from the training set and the generated dataset. A kernel function is applied to pairs of these samples (thus the statistics are acutally calculated in high-dimensional spaces). The authors use Gaussian kernels. MMD can be approximated using a small number of samples. MMD is differentiable and therefor can be used as a standard loss function. They train two models:\n\nGMMN: Noise vector input (as in GANs), several ReLU layers into one sigmoid layer. MMD as the loss function.\nGMMN+AE: Same as GMMN, but the sigmoid output is not an image, but instead the code that gets fed into an autoencoder's (AE) decoder. The AE is trained separately on the dataset. MMD is backpropagated through the decoder and then the GMMN. I.e. the GMMN learns to produce codes that let the decoder generate good looking images.\n\n GMMN: Noise vector input (as in GANs), several ReLU layers into one sigmoid layer. MMD as the loss function. GMMN+AE: Same as GMMN, but the sigmoid output is not an image, but instead the code that gets fed into an autoencoder's (AE) decoder. The AE is trained separately on the dataset. MMD is backpropagated through the decoder and then the GMMN. I.e. the GMMN learns to produce codes that let the decoder generate good looking images. Results\n\nThey tested only on MNIST and TFD (i.e. datasets that are well suited for AEs...).\nTheir GMMN achieves similar log likelihoods compared to other models.\nTheir GMMN+AE achieves better log likelihoods than other models.\nGMMN+AE produces good looking images.\nGMMN+AE produces smooth interpolations between images.\n\n They tested only on MNIST and TFD (i.e. datasets that are well suited for AEs...). Their GMMN achieves similar log likelihoods compared to other models. Their GMMN+AE achieves better log likelihoods than other models. GMMN+AE produces good looking images. GMMN+AE produces smooth interpolations between images. \n(1) Introduction\n\nSampling in GMMNs is fast.\nGMMNs are similar to GANs.\nWhile the training objective in GANs is a minimax problem, in GMMNs it is a simple loss function.\nGMMNs are based on maximum mean discrepancy. They use that (implemented via the kernel trick) as the loss function.\nGMMNs try to generate data so that the moments in the generated data are as similar as possible to the moments in the training data.\nThey combine GMMNs with autoencoders. That is, they first train an autoencoder to generate images. Then they train a GMMN to produce sound code inputs to the decoder of the autoencoder.\n\n Sampling in GMMNs is fast. GMMNs are similar to GANs. While the training objective in GANs is a minimax problem, in GMMNs it is a simple loss function. GMMNs are based on maximum mean discrepancy. They use that (implemented via the kernel trick) as the loss function. GMMNs try to generate data so that the moments in the generated data are as similar as possible to the moments in the training data. They combine GMMNs with autoencoders. That is, they first train an autoencoder to generate images. Then they train a GMMN to produce sound code inputs to the decoder of the autoencoder. \n(2) Maximum Mean Discrepancy\n\nMaximum mean discrepancy (MMD) is a frequentist estimator to tell whether two datasets X and Y come from the same probability distribution.\nMMD estimates basic statistics values (i.e. mean and higher order statistics) of both datasets and compares them with each other.\nMMD can be formulated so that examples from the datasets are only used for scalar products. Then the kernel trick can be applied.\nIt can be shown that minimizing MMD with gaussian kernels is equivalent to matching all moments between the probability distributions of the datasets.\n\n Maximum mean discrepancy (MMD) is a frequentist estimator to tell whether two datasets X and Y come from the same probability distribution. MMD estimates basic statistics values (i.e. mean and higher order statistics) of both datasets and compares them with each other. MMD can be formulated so that examples from the datasets are only used for scalar products. Then the kernel trick can be applied. It can be shown that minimizing MMD with gaussian kernels is equivalent to matching all moments between the probability distributions of the datasets. \n(4) Generative Moment Matching Networks\n\nData Space Networks\n\nJust like GANs, GMMNs start with a noise vector that has N values sampled uniformly from [-1, 1].\nThe noise vector is then fed forward through several fully connected ReLU layers.\nThe MMD is differentiable and therefor can be used for backpropagation.\n\n\nAuto-Encoder Code Sparse Networks\n\nAEs can be used to reconstruct high-dimensional data, which is a simpler task than to learn to generate new data from scratch.\nAdvantages of using the AE code space:\n\nDimensionality can be explicitly chosen.\nDisentangling factors of variation.\n\n\nThey suggest a combination of GMMN and AE. They first train an AE, then they train a GMMN to generate good codes for the AE's decoder (based on MMD loss).\nFor some reason they use greedy layer-wise pretraining with later fine-tuning for the AE, but don't explain why. (That training method is outdated?)\nThey add dropout to their AE's encoder to get a smoother code manifold.\n\n\nPractical Considerations\n\nMMD has a bandwidth parameter (as its based on RBFs). Instead of chosing a single fixed bandwidth, they instead use multiple kernels with different bandwidths (1, 5, 10, ...), apply them all and then sum the results.\nInstead of MMD^2 loss they use sqrt(MMD^2), which does not go as fast to zero as raw MMD, thereby creating stronger gradients.\nPer minibatch they generate a small number of samples und they pick a small number of samples from the training set. They then compute MMD for these samples. I.e. they don't run MMD over the whole training set as that would be computationally prohibitive.\n\n\n\n Data Space Networks\n\nJust like GANs, GMMNs start with a noise vector that has N values sampled uniformly from [-1, 1].\nThe noise vector is then fed forward through several fully connected ReLU layers.\nThe MMD is differentiable and therefor can be used for backpropagation.\n\n Just like GANs, GMMNs start with a noise vector that has N values sampled uniformly from [-1, 1]. The noise vector is then fed forward through several fully connected ReLU layers. The MMD is differentiable and therefor can be used for backpropagation. Auto-Encoder Code Sparse Networks\n\nAEs can be used to reconstruct high-dimensional data, which is a simpler task than to learn to generate new data from scratch.\nAdvantages of using the AE code space:\n\nDimensionality can be explicitly chosen.\nDisentangling factors of variation.\n\n\nThey suggest a combination of GMMN and AE. They first train an AE, then they train a GMMN to generate good codes for the AE's decoder (based on MMD loss).\nFor some reason they use greedy layer-wise pretraining with later fine-tuning for the AE, but don't explain why. (That training method is outdated?)\nThey add dropout to their AE's encoder to get a smoother code manifold.\n\n AEs can be used to reconstruct high-dimensional data, which is a simpler task than to learn to generate new data from scratch. Advantages of using the AE code space:\n\nDimensionality can be explicitly chosen.\nDisentangling factors of variation.\n\n Dimensionality can be explicitly chosen. Disentangling factors of variation. They suggest a combination of GMMN and AE. They first train an AE, then they train a GMMN to generate good codes for the AE's decoder (based on MMD loss). For some reason they use greedy layer-wise pretraining with later fine-tuning for the AE, but don't explain why. (That training method is outdated?) They add dropout to their AE's encoder to get a smoother code manifold. Practical Considerations\n\nMMD has a bandwidth parameter (as its based on RBFs). Instead of chosing a single fixed bandwidth, they instead use multiple kernels with different bandwidths (1, 5, 10, ...), apply them all and then sum the results.\nInstead of MMD^2 loss they use sqrt(MMD^2), which does not go as fast to zero as raw MMD, thereby creating stronger gradients.\nPer minibatch they generate a small number of samples und they pick a small number of samples from the training set. They then compute MMD for these samples. I.e. they don't run MMD over the whole training set as that would be computationally prohibitive.\n\n MMD has a bandwidth parameter (as its based on RBFs). Instead of chosing a single fixed bandwidth, they instead use multiple kernels with different bandwidths (1, 5, 10, ...), apply them all and then sum the results. Instead of MMD^2 loss they use sqrt(MMD^2), which does not go as fast to zero as raw MMD, thereby creating stronger gradients. Per minibatch they generate a small number of samples und they pick a small number of samples from the training set. They then compute MMD for these samples. I.e. they don't run MMD over the whole training set as that would be computationally prohibitive. \n(5) Experiments\n\nThey trained on MNIST and TFD.\nThey used an GMMN with 4 ReLU layers and autoencoders with either 2/2 (encoder, decoder) hidden sigmoid layers (MNIST) or 3/3 (TFD).\nThey used dropout on the encoder layers.\nThey used layer-wise pretraining and finetuning for the AEs.\nThey tuned most of the hyperparameters using bayesian optimization.\nThey use minibatch sizes of 1000 and compute MMD based on those (i.e. based on 2000 points total).\nTheir GMMN+AE model achieves better log likelihood values than all competitors. The raw GMMN model performs roughly on par with the competitors.\nNearest neighbor evaluation indicates that it did not just memorize the training set.\nThe model learns smooth interpolations between digits (MNIST) and faces (TFD).\n\n They trained on MNIST and TFD. They used an GMMN with 4 ReLU layers and autoencoders with either 2/2 (encoder, decoder) hidden sigmoid layers (MNIST) or 3/3 (TFD). They used dropout on the encoder layers. They used layer-wise pretraining and finetuning for the AEs. They tuned most of the hyperparameters using bayesian optimization. They use minibatch sizes of 1000 and compute MMD based on those (i.e. based on 2000 points total). Their GMMN+AE model achieves better log likelihood values than all competitors. The raw GMMN model performs roughly on par with the competitors. Nearest neighbor evaluation indicates that it did not just memorize the training set. The model learns smooth interpolations between digits (MNIST) and faces (TFD). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1502.02761"
    },
    "88": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Pixel_Recurrent_Neural_Networks.md",
        "transcript": "\nWhat\n\nThe authors describe multiple architectures that can model the distributions of images.\nThese networks can be used to generate new images or to complete existing ones.\nThe networks are mostly based on RNNs.\n\n The authors describe multiple architectures that can model the distributions of images. These networks can be used to generate new images or to complete existing ones. The networks are mostly based on RNNs. \nHow\n\nThey define three architectures:\n\nRow LSTM:\n\nPredicts a pixel value based on all previous pixels in the image.\nIt applies 1D convolutions (with kernel size 3) to the current and previous rows of the image.\nIt uses the convolution results as features to predict a pixel value.\n\n\nDiagonal BiLSTM:\n\nPredicts a pixel value based on all previous pixels in the image.\nInstead of applying convolutions in a row-wise fashion, they apply them to the diagonals towards the top left and top right of the pixel.\nDiagonal convolutions can be applied by padding the n-th row with n-1 pixels from the left (diagonal towards top left) or from the right (diagonal towards the top right), then apply a 3x1 column convolution.\n\n\nPixelCNN:\n\nApplies convolutions to the region around a pixel to predict its values.\nUses masks to zero out pixels that follow after the target pixel.\nThey use no pooling layers.\nWhile for the LSTMs each pixel is conditioned on all previous pixels, the dependency range of the CNN is bounded.\n\n\n\n\nThey use up to 12 LSTM layers.\nThey use residual connections between their LSTM layers.\nAll architectures predict pixel values as a softmax over 255 distinct values (per channel). According to the authors that leads to better results than just using one continuous output (i.e. sigmoid) per channel.\nThey also try a multi-scale approach: First, one network generates a small image. Then a second networks generates the full scale image while being conditioned on the small image.\n\n They define three architectures:\n\nRow LSTM:\n\nPredicts a pixel value based on all previous pixels in the image.\nIt applies 1D convolutions (with kernel size 3) to the current and previous rows of the image.\nIt uses the convolution results as features to predict a pixel value.\n\n\nDiagonal BiLSTM:\n\nPredicts a pixel value based on all previous pixels in the image.\nInstead of applying convolutions in a row-wise fashion, they apply them to the diagonals towards the top left and top right of the pixel.\nDiagonal convolutions can be applied by padding the n-th row with n-1 pixels from the left (diagonal towards top left) or from the right (diagonal towards the top right), then apply a 3x1 column convolution.\n\n\nPixelCNN:\n\nApplies convolutions to the region around a pixel to predict its values.\nUses masks to zero out pixels that follow after the target pixel.\nThey use no pooling layers.\nWhile for the LSTMs each pixel is conditioned on all previous pixels, the dependency range of the CNN is bounded.\n\n\n\n Row LSTM:\n\nPredicts a pixel value based on all previous pixels in the image.\nIt applies 1D convolutions (with kernel size 3) to the current and previous rows of the image.\nIt uses the convolution results as features to predict a pixel value.\n\n Predicts a pixel value based on all previous pixels in the image. It applies 1D convolutions (with kernel size 3) to the current and previous rows of the image. It uses the convolution results as features to predict a pixel value. Diagonal BiLSTM:\n\nPredicts a pixel value based on all previous pixels in the image.\nInstead of applying convolutions in a row-wise fashion, they apply them to the diagonals towards the top left and top right of the pixel.\nDiagonal convolutions can be applied by padding the n-th row with n-1 pixels from the left (diagonal towards top left) or from the right (diagonal towards the top right), then apply a 3x1 column convolution.\n\n Predicts a pixel value based on all previous pixels in the image. Instead of applying convolutions in a row-wise fashion, they apply them to the diagonals towards the top left and top right of the pixel. Diagonal convolutions can be applied by padding the n-th row with n-1 pixels from the left (diagonal towards top left) or from the right (diagonal towards the top right), then apply a 3x1 column convolution. PixelCNN:\n\nApplies convolutions to the region around a pixel to predict its values.\nUses masks to zero out pixels that follow after the target pixel.\nThey use no pooling layers.\nWhile for the LSTMs each pixel is conditioned on all previous pixels, the dependency range of the CNN is bounded.\n\n Applies convolutions to the region around a pixel to predict its values. Uses masks to zero out pixels that follow after the target pixel. They use no pooling layers. While for the LSTMs each pixel is conditioned on all previous pixels, the dependency range of the CNN is bounded. They use up to 12 LSTM layers. They use residual connections between their LSTM layers. All architectures predict pixel values as a softmax over 255 distinct values (per channel). According to the authors that leads to better results than just using one continuous output (i.e. sigmoid) per channel. They also try a multi-scale approach: First, one network generates a small image. Then a second networks generates the full scale image while being conditioned on the small image. \nResults\n\nThe softmax layers learn reasonable distributions. E.g. neighboring colors end up with similar probabilities. Values 0 and 255 tend to have higher probabilities than others, especially for the very first pixel.\nIn the 12-layer LSTM row model, residual and skip connections seem to have roughly the same effect on the network's results. Using both yields a tiny improvement over just using one of the techniques alone.\nThey achieve a slightly better result on MNIST than DRAW did.\nTheir negative log likelihood results for CIFAR-10 improve upon previous models. The diagonal BiLSTM model performs best, followed by the row LSTM model, followed by PixelCNN.\nTheir generated images for CIFAR-10 and Imagenet capture real local spatial dependencies. The multi-scale model produces better looking results. The images do not appear blurry. Overall they still look very unreal.\n\n The softmax layers learn reasonable distributions. E.g. neighboring colors end up with similar probabilities. Values 0 and 255 tend to have higher probabilities than others, especially for the very first pixel. In the 12-layer LSTM row model, residual and skip connections seem to have roughly the same effect on the network's results. Using both yields a tiny improvement over just using one of the techniques alone. They achieve a slightly better result on MNIST than DRAW did. Their negative log likelihood results for CIFAR-10 improve upon previous models. The diagonal BiLSTM model performs best, followed by the row LSTM model, followed by PixelCNN. Their generated images for CIFAR-10 and Imagenet capture real local spatial dependencies. The multi-scale model produces better looking results. The images do not appear blurry. Overall they still look very unreal. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1601.06759"
    },
    "89": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks.md",
        "transcript": "\nWhat\n\nDCGANs are just a different architecture of GANs.\nIn GANs a Generator network (G) generates images. A discriminator network (D) learns to differentiate between real images from the training set and images generated by G.\nDCGANs basically convert the laplacian pyramid technique (many pairs of G and D to progressively upscale an image) to a single pair of G and D.\n\n DCGANs are just a different architecture of GANs. In GANs a Generator network (G) generates images. A discriminator network (D) learns to differentiate between real images from the training set and images generated by G. DCGANs basically convert the laplacian pyramid technique (many pairs of G and D to progressively upscale an image) to a single pair of G and D. \nHow\n\nTheir D: Convolutional networks. No linear layers. No pooling, instead strided layers. LeakyReLUs.\nTheir G: Starts with 100d noise vector. Generates with linear layers 1024x4x4 values. Then uses fractionally strided convolutions (move by 0.5 per step) to upscale to 512x8x8. This is continued till Cx32x32 or Cx64x64. The last layer is a convolution to 3x32x32/3x64x64 (Tanh activation).\nThe fractionally strided convolutions do basically the same as the progressive upscaling in the laplacian pyramid. So it's basically one laplacian pyramid in a single network and all upscalers are trained jointly leading to higher quality images.\nThey use Adam as their optimizer. To decrease instability issues they decreased the learning rate to 0.0002 (from 0.001) and the momentum/beta1 to 0.5 (from 0.9).\n\n Their D: Convolutional networks. No linear layers. No pooling, instead strided layers. LeakyReLUs. Their G: Starts with 100d noise vector. Generates with linear layers 1024x4x4 values. Then uses fractionally strided convolutions (move by 0.5 per step) to upscale to 512x8x8. This is continued till Cx32x32 or Cx64x64. The last layer is a convolution to 3x32x32/3x64x64 (Tanh activation). The fractionally strided convolutions do basically the same as the progressive upscaling in the laplacian pyramid. So it's basically one laplacian pyramid in a single network and all upscalers are trained jointly leading to higher quality images. They use Adam as their optimizer. To decrease instability issues they decreased the learning rate to 0.0002 (from 0.001) and the momentum/beta1 to 0.5 (from 0.9). Results\n\nHigh quality images. Still with distortions and errors, but at first glance they look realistic.\nSmooth interpolations between generated images are possible (by interpolating between the noise vectors and feeding these interpolations into G).\nThe features extracted by D seem to have some potential for unsupervised learning.\nThere seems to be some potential for vector arithmetics (using the initial noise vectors) similar to the vector arithmetics with wordvectors. E.g. to generate mean with sunglasses via vector(men) + vector(sunglasses).\n\n High quality images. Still with distortions and errors, but at first glance they look realistic. Smooth interpolations between generated images are possible (by interpolating between the noise vectors and feeding these interpolations into G). The features extracted by D seem to have some potential for unsupervised learning. There seems to be some potential for vector arithmetics (using the initial noise vectors) similar to the vector arithmetics with wordvectors. E.g. to generate mean with sunglasses via vector(men) + vector(sunglasses). \nIntroduction\n\nFor unsupervised learning, they propose to use to train a GAN and then reuse the weights of D.\nGANs have traditionally been hard to train.\n\n For unsupervised learning, they propose to use to train a GAN and then reuse the weights of D. GANs have traditionally been hard to train. \nApproach and model architecture\n\nThey use for D an convnet without linear layers, withput pooling layers (only strides), LeakyReLUs and Batch Normalization.\nThey use for G ReLUs (hidden layers) and Tanh (output).\n\n They use for D an convnet without linear layers, withput pooling layers (only strides), LeakyReLUs and Batch Normalization. They use for G ReLUs (hidden layers) and Tanh (output). \nDetails of adversarial training\n\nThey trained on LSUN, Imagenet-1k and a custom dataset of faces.\nMinibatch size was 128.\nLeakyReLU alpha 0.2.\nThey used Adam with a learning rate of 0.0002 and momentum of 0.5.\nThey note that a higher momentum lead to oscillations.\n\n They trained on LSUN, Imagenet-1k and a custom dataset of faces. Minibatch size was 128. LeakyReLU alpha 0.2. They used Adam with a learning rate of 0.0002 and momentum of 0.5. They note that a higher momentum lead to oscillations. \nLSUN\n\n3M images of bedrooms.\nThey use an autoencoder based technique to filter out 0.25M near duplicate images.\n\n 3M images of bedrooms. They use an autoencoder based technique to filter out 0.25M near duplicate images. \nFaces\n\nThey downloaded 3M images of 10k people.\nThey extracted 350k faces with OpenCV.\n\n They downloaded 3M images of 10k people. They extracted 350k faces with OpenCV. \nEmpirical validation of DCGANs capabilities\n\nClassifying CIFAR-10 GANs as a feature extractor\n\nThey train a pair of G and D on Imagenet-1k.\nD's top layer has 512*4*4 features.\nThey train an SVM on these features to classify the images of CIFAR-10.\nThey achieve a score of 82.8%, better than unsupervised K-Means based methods, but worse than Exemplar CNNs.\n\n\nClassifying SVHN digits using GANs as a feature extractor\n\nThey reuse the same pipeline (D trained on CIFAR-10, SVM) for the StreetView House Numbers dataset.\nThey use 1000 SVHN images (with the features from D) to train the SVM.\nThey achieve 22.48% test error.\n\n\n\n Classifying CIFAR-10 GANs as a feature extractor\n\nThey train a pair of G and D on Imagenet-1k.\nD's top layer has 512*4*4 features.\nThey train an SVM on these features to classify the images of CIFAR-10.\nThey achieve a score of 82.8%, better than unsupervised K-Means based methods, but worse than Exemplar CNNs.\n\n They train a pair of G and D on Imagenet-1k. D's top layer has 512*4*4 features. They train an SVM on these features to classify the images of CIFAR-10. They achieve a score of 82.8%, better than unsupervised K-Means based methods, but worse than Exemplar CNNs. Classifying SVHN digits using GANs as a feature extractor\n\nThey reuse the same pipeline (D trained on CIFAR-10, SVM) for the StreetView House Numbers dataset.\nThey use 1000 SVHN images (with the features from D) to train the SVM.\nThey achieve 22.48% test error.\n\n They reuse the same pipeline (D trained on CIFAR-10, SVM) for the StreetView House Numbers dataset. They use 1000 SVHN images (with the features from D) to train the SVM. They achieve 22.48% test error. \nInvestigating and visualizing the internals of the networks\n\nWalking in the latent space\n\nThe performs walks in the latent space (= interpolate between input noise vectors and generate several images for the interpolation).\nThey argue that this might be a good way to detect overfitting/memorizations as those might lead to very sudden (not smooth) transitions.\n\n\nVisualizing the discriminator features\n\nThey use guided backpropagation to visualize what the feature maps in D have learned (i.e. to which images they react).\nThey can show that their LSUN-bedroom GAN seems to have learned in an unsupervised way what beds and windows look like.\n\n\nForgetting to draw certain objects\n\nThey manually annotated the locations of objects in some generated bedroom images.\nBased on these annotations they estimated which feature maps were mostly responsible for generating the objects.\nThey deactivated these feature maps and regenerated the images.\nThat decreased the appearance of these objects. It's however not as easy as one feature map deactivation leading to one object disappearing. They deactivated quite a lot of feature maps (200) and they objects were often still quite visible or replaced by artefacts/errors.\n\n\nVector arithmetic on face samples\n\nWordvectors can be used to perform semantic arithmetic (e.g. king - man + woman = queen).\nThe unsupervised representations seem to be useable in a similar fashion.\nE.g. they generated images via G. They then picked several images that showed men with glasses and averaged these image's noise vectors. They did with same with men without glasses and women without glasses. Then they performed on these vectors men with glasses - mean without glasses + women without glasses to get `womean with glasses\n\n\n\n Walking in the latent space\n\nThe performs walks in the latent space (= interpolate between input noise vectors and generate several images for the interpolation).\nThey argue that this might be a good way to detect overfitting/memorizations as those might lead to very sudden (not smooth) transitions.\n\n The performs walks in the latent space (= interpolate between input noise vectors and generate several images for the interpolation). They argue that this might be a good way to detect overfitting/memorizations as those might lead to very sudden (not smooth) transitions. Visualizing the discriminator features\n\nThey use guided backpropagation to visualize what the feature maps in D have learned (i.e. to which images they react).\nThey can show that their LSUN-bedroom GAN seems to have learned in an unsupervised way what beds and windows look like.\n\n They use guided backpropagation to visualize what the feature maps in D have learned (i.e. to which images they react). They can show that their LSUN-bedroom GAN seems to have learned in an unsupervised way what beds and windows look like. Forgetting to draw certain objects\n\nThey manually annotated the locations of objects in some generated bedroom images.\nBased on these annotations they estimated which feature maps were mostly responsible for generating the objects.\nThey deactivated these feature maps and regenerated the images.\nThat decreased the appearance of these objects. It's however not as easy as one feature map deactivation leading to one object disappearing. They deactivated quite a lot of feature maps (200) and they objects were often still quite visible or replaced by artefacts/errors.\n\n They manually annotated the locations of objects in some generated bedroom images. Based on these annotations they estimated which feature maps were mostly responsible for generating the objects. They deactivated these feature maps and regenerated the images. That decreased the appearance of these objects. It's however not as easy as one feature map deactivation leading to one object disappearing. They deactivated quite a lot of feature maps (200) and they objects were often still quite visible or replaced by artefacts/errors. Vector arithmetic on face samples\n\nWordvectors can be used to perform semantic arithmetic (e.g. king - man + woman = queen).\nThe unsupervised representations seem to be useable in a similar fashion.\nE.g. they generated images via G. They then picked several images that showed men with glasses and averaged these image's noise vectors. They did with same with men without glasses and women without glasses. Then they performed on these vectors men with glasses - mean without glasses + women without glasses to get `womean with glasses\n\n Wordvectors can be used to perform semantic arithmetic (e.g. king - man + woman = queen). The unsupervised representations seem to be useable in a similar fashion. E.g. they generated images via G. They then picked several images that showed men with glasses and averaged these image's noise vectors. They did with same with men without glasses and women without glasses. Then they performed on these vectors men with glasses - mean without glasses + women without glasses to get `womean with glasses \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1511.06434"
    },
    "90": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/A_Neural_Algorithm_for_Artistic_Style.md",
        "transcript": "\nWhat\n\nThe paper describes a method to separate content and style from each other in an image.\nThe style can then be transfered to a new image.\nExamples:\n\nLet a photograph look like a painting of van Gogh.\nImprove a dark beach photo by taking the style from a sunny beach photo.\n\n\n\n The paper describes a method to separate content and style from each other in an image. The style can then be transfered to a new image. Examples:\n\nLet a photograph look like a painting of van Gogh.\nImprove a dark beach photo by taking the style from a sunny beach photo.\n\n Let a photograph look like a painting of van Gogh. Improve a dark beach photo by taking the style from a sunny beach photo. \nHow\n\nThey use the pretrained 19-layer VGG net as their base network.\nThey assume that two images are provided: One with the content, one with the desired style.\nThey feed the content image through the VGG net and extract the activations of the last convolutional layer. These activations are called the content representation.\nThey feed the style image through the VGG net and extract the activations of all convolutional layers. They transform each layer to a Gram Matrix representation. These Gram Matrices are called the style representation.\nHow to calculate a Gram Matrix:\n\nTake the activations of a layer. That layer will contain some convolution filters (e.g. 128), each one having its own activations.\nConvert each filter's activations to a (1-dimensional) vector.\nPick all pairs of filters. Calculate the scalar product of both filter's vectors.\nAdd the scalar product result as an entry to a matrix of size #filters x #filters (e.g. 128x128).\nRepeat that for every pair to get the Gram Matrix.\nThe Gram Matrix roughly represents the texture of the image.\n\n\nNow you have the content representation (activations of a layer) and the style representation (Gram Matrices).\nCreate a new image of the size of the content image. Fill it with random white noise.\nFeed that image through VGG to get its content representation and style representation. (This step will be repeated many times during the image creation.)\nMake changes to the new image using gradient descent to optimize a loss function.\n\nThe loss function has two components:\n\nThe mean squared error between the new image's content representation and the previously extracted content representation.\nThe mean squared error between the new image's style representation and the previously extracted style representation.\n\n\nAdd up both components to get the total loss.\nGive both components a weight to alter for more/less style matching (at the expense of content matching).\n\n\n\n They use the pretrained 19-layer VGG net as their base network. They assume that two images are provided: One with the content, one with the desired style. They feed the content image through the VGG net and extract the activations of the last convolutional layer. These activations are called the content representation. They feed the style image through the VGG net and extract the activations of all convolutional layers. They transform each layer to a Gram Matrix representation. These Gram Matrices are called the style representation. How to calculate a Gram Matrix:\n\nTake the activations of a layer. That layer will contain some convolution filters (e.g. 128), each one having its own activations.\nConvert each filter's activations to a (1-dimensional) vector.\nPick all pairs of filters. Calculate the scalar product of both filter's vectors.\nAdd the scalar product result as an entry to a matrix of size #filters x #filters (e.g. 128x128).\nRepeat that for every pair to get the Gram Matrix.\nThe Gram Matrix roughly represents the texture of the image.\n\n Take the activations of a layer. That layer will contain some convolution filters (e.g. 128), each one having its own activations. Convert each filter's activations to a (1-dimensional) vector. Pick all pairs of filters. Calculate the scalar product of both filter's vectors. Add the scalar product result as an entry to a matrix of size #filters x #filters (e.g. 128x128). Repeat that for every pair to get the Gram Matrix. The Gram Matrix roughly represents the texture of the image. Now you have the content representation (activations of a layer) and the style representation (Gram Matrices). Create a new image of the size of the content image. Fill it with random white noise. Feed that image through VGG to get its content representation and style representation. (This step will be repeated many times during the image creation.) Make changes to the new image using gradient descent to optimize a loss function.\n\nThe loss function has two components:\n\nThe mean squared error between the new image's content representation and the previously extracted content representation.\nThe mean squared error between the new image's style representation and the previously extracted style representation.\n\n\nAdd up both components to get the total loss.\nGive both components a weight to alter for more/less style matching (at the expense of content matching).\n\n The loss function has two components:\n\nThe mean squared error between the new image's content representation and the previously extracted content representation.\nThe mean squared error between the new image's style representation and the previously extracted style representation.\n\n The mean squared error between the new image's content representation and the previously extracted content representation. The mean squared error between the new image's style representation and the previously extracted style representation. Add up both components to get the total loss. Give both components a weight to alter for more/less style matching (at the expense of content matching). \nPage 1\n\nA painted image can be decomposed in its content and its artistic style.\nHere they use a neural network to separate content and style from each other (and to apply that style to an existing image).\n\n A painted image can be decomposed in its content and its artistic style. Here they use a neural network to separate content and style from each other (and to apply that style to an existing image). \nPage 2\n\nRepresentations get more abstract as you go deeper in networks, hence they should more resemble the actual content (as opposed to the artistic style).\nThey call the feature responses in higher layers content representation.\nTo capture style information, they use a method that was originally designed to capture texture information.\nThey somehow build a feature space on top of the existing one, that is somehow dependent on correlations of features. That leads to a \"stationary\" (?) and multi-scale representation of the style.\n\n Representations get more abstract as you go deeper in networks, hence they should more resemble the actual content (as opposed to the artistic style). They call the feature responses in higher layers content representation. To capture style information, they use a method that was originally designed to capture texture information. They somehow build a feature space on top of the existing one, that is somehow dependent on correlations of features. That leads to a \"stationary\" (?) and multi-scale representation of the style. \nPage 3\n\nThey use VGG as their base CNN.\n\n They use VGG as their base CNN. \nPage 4\n\nBased on the extracted style features, they can generate a new image, which has equal activations in these style features.\nThe new image should match the style (texture, color, localized structures) of the artistic image.\nThe style features become more and more abtstract with higher layers. They call that multi-scale the style representation.\nThe key contribution of the paper is a method to separate style and content representation from each other.\nThese representations can then be used to change the style of an existing image (by changing it so that its content representation stays the same, but its style representation matches the artwork).\n\n Based on the extracted style features, they can generate a new image, which has equal activations in these style features. The new image should match the style (texture, color, localized structures) of the artistic image. The style features become more and more abtstract with higher layers. They call that multi-scale the style representation. The key contribution of the paper is a method to separate style and content representation from each other. These representations can then be used to change the style of an existing image (by changing it so that its content representation stays the same, but its style representation matches the artwork). \nPage 6\n\nThe generated images look most appealing if all features from the style representation are used. (The lower layers tend to reflect small features, the higher layers tend to reflect larger features.)\nContent and style can't be separated perfectly.\nTheir loss function has two terms, one for content matching and one for style matching.\nThe terms can be increased/decreased to match content or style more.\n\n The generated images look most appealing if all features from the style representation are used. (The lower layers tend to reflect small features, the higher layers tend to reflect larger features.) Content and style can't be separated perfectly. Their loss function has two terms, one for content matching and one for style matching. The terms can be increased/decreased to match content or style more. \nPage 8\n\nPrevious techniques work only on limited or simple domains or used non-parametric approaches (see non-photorealistic rendering).\nPreviously neural networks have been used to classify the time period of paintings (based on their style).\nThey argue that separating content from style might be useful and many other domains (other than transfering style of paintings to images).\n\n Previous techniques work only on limited or simple domains or used non-parametric approaches (see non-photorealistic rendering). Previously neural networks have been used to classify the time period of paintings (based on their style). They argue that separating content from style might be useful and many other domains (other than transfering style of paintings to images). \nPage 9\n\nThe style representation is gathered by measuring correlations between activations of neurons.\nThey argue that this is somehow similar to what \"complex cells\" in the primary visual system (V1) do.\nThey note that deep convnets seem to automatically learn to separate content from style, probably because it is helpful for style-invariant classification.\n\n The style representation is gathered by measuring correlations between activations of neurons. They argue that this is somehow similar to what \"complex cells\" in the primary visual system (V1) do. They note that deep convnets seem to automatically learn to separate content from style, probably because it is helpful for style-invariant classification. \nPage 9, Methods\n\nThey use the 19 layer VGG net as their basis.\nThey use only its convolutional layers, not the linear ones.\nThey use average pooling instead of max pooling, as that produced slightly better results.\n\n They use the 19 layer VGG net as their basis. They use only its convolutional layers, not the linear ones. They use average pooling instead of max pooling, as that produced slightly better results. \nPage 10, Methods\n\nThe information about the image that is contained in layers can be visualized. To do that, extract the features of a layer as the labels, then start with a white noise image and change it via gradient descent until the generated features have minimal distance (MSE) to the extracted features.\nThe build a style representation by calculating Gram Matrices for each layer.\n\n The information about the image that is contained in layers can be visualized. To do that, extract the features of a layer as the labels, then start with a white noise image and change it via gradient descent until the generated features have minimal distance (MSE) to the extracted features. The build a style representation by calculating Gram Matrices for each layer. \nPage 11, Methods\n\nThe Gram Matrix is generated in the following way:\n\nConvert each filter of a convolutional layer to a 1-dimensional vector.\nFor a pair of filters i, j calculate the value in the Gram Matrix by calculating the scalar product of the two vectors of the filters.\nDo that for every pair of filters, generating a matrix of size #filters x #filters. That is the Gram Matrix.\n\n\nAgain, a white noise image can be changed with gradient descent to match the style of a given image (i.e. minimize MSE between two Gram Matrices).\nThat can be extended to match the style of several layers by measuring the MSE of the Gram Matrices of each layer and giving each layer a weighting.\n\n The Gram Matrix is generated in the following way:\n\nConvert each filter of a convolutional layer to a 1-dimensional vector.\nFor a pair of filters i, j calculate the value in the Gram Matrix by calculating the scalar product of the two vectors of the filters.\nDo that for every pair of filters, generating a matrix of size #filters x #filters. That is the Gram Matrix.\n\n Convert each filter of a convolutional layer to a 1-dimensional vector. For a pair of filters i, j calculate the value in the Gram Matrix by calculating the scalar product of the two vectors of the filters. Do that for every pair of filters, generating a matrix of size #filters x #filters. That is the Gram Matrix. Again, a white noise image can be changed with gradient descent to match the style of a given image (i.e. minimize MSE between two Gram Matrices). That can be extended to match the style of several layers by measuring the MSE of the Gram Matrices of each layer and giving each layer a weighting. \nPage 12, Methods\n\nTo transfer the style of a painting to an existing image, proceed as follows:\n\nStart with a white noise image.\nOptimize that image with gradient descent so that it minimizes both the content loss (relative to the image) and the style loss (relative to the painting).\nEach distance (content, style) can be weighted to have more or less influence on the loss function.\n\n\n\n To transfer the style of a painting to an existing image, proceed as follows:\n\nStart with a white noise image.\nOptimize that image with gradient descent so that it minimizes both the content loss (relative to the image) and the style loss (relative to the painting).\nEach distance (content, style) can be weighted to have more or less influence on the loss function.\n\n Start with a white noise image. Optimize that image with gradient descent so that it minimizes both the content loss (relative to the image) and the style loss (relative to the painting). Each distance (content, style) can be weighted to have more or less influence on the loss function. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1508.06576"
    },
    "91": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Batch_Normalization.md",
        "transcript": "\nWhat is BN:\n\nBatch Normalization (BN) is a normalization method/layer for neural networks.\nUsually inputs to neural networks are normalized to either the range of [0, 1] or [-1, 1] or to mean=0 and variance=1. The latter is called Whitening.\nBN essentially performs Whitening to the intermediate layers of the networks.\n\n Batch Normalization (BN) is a normalization method/layer for neural networks. Usually inputs to neural networks are normalized to either the range of [0, 1] or [-1, 1] or to mean=0 and variance=1. The latter is called Whitening. BN essentially performs Whitening to the intermediate layers of the networks. \nHow its calculated:\n\nThe basic formula is x* = (x - E[x]) / sqrt(var(x)), where x* is the new value of a single component, E[x] is its mean within a batch and var(x) is its variance within a batch.\nBN extends that formula further to x** = gamma * x* + beta, where x** is the final normalized value. gamma and beta are learned per layer. They make sure that BN can learn the identity function, which is needed in a few cases.\nFor convolutions, every layer/filter/kernel is normalized on its own (linear layer: each neuron/node/component). That means that every generated value (\"pixel\") is treated as an example. If we have a batch size of N and the image generated by the convolution has width=P and height=Q, we would calculate the mean (E) over N*P*Q examples (same for the variance).\n\n The basic formula is x* = (x - E[x]) / sqrt(var(x)), where x* is the new value of a single component, E[x] is its mean within a batch and var(x) is its variance within a batch. BN extends that formula further to x** = gamma * x* + beta, where x** is the final normalized value. gamma and beta are learned per layer. They make sure that BN can learn the identity function, which is needed in a few cases. For convolutions, every layer/filter/kernel is normalized on its own (linear layer: each neuron/node/component). That means that every generated value (\"pixel\") is treated as an example. If we have a batch size of N and the image generated by the convolution has width=P and height=Q, we would calculate the mean (E) over N*P*Q examples (same for the variance). \nTheoretical effects:\n\nBN reduces Covariate Shift. That is the change in distribution of activation of a component. By using BN, each neuron's activation becomes (more or less) a gaussian distribution, i.e. its usually not active, sometimes a bit active, rare very active.\nCovariate Shift is undesirable, because the later layers have to keep adapting to the change of the type of distribution (instead of just to new distribution parameters, e.g. new mean and variance values for gaussian distributions).\nBN reduces effects of exploding and vanishing gradients, because every becomes roughly normal distributed. Without BN, low activations of one layer can lead to lower activations in the next layer, and then even lower ones in the next layer and so on.\n\n BN reduces Covariate Shift. That is the change in distribution of activation of a component. By using BN, each neuron's activation becomes (more or less) a gaussian distribution, i.e. its usually not active, sometimes a bit active, rare very active. Covariate Shift is undesirable, because the later layers have to keep adapting to the change of the type of distribution (instead of just to new distribution parameters, e.g. new mean and variance values for gaussian distributions). BN reduces effects of exploding and vanishing gradients, because every becomes roughly normal distributed. Without BN, low activations of one layer can lead to lower activations in the next layer, and then even lower ones in the next layer and so on. \nPractical effects:\n\nBN reduces training times. (Because of less Covariate Shift, less exploding/vanishing gradients.)\nBN reduces demand for regularization, e.g. dropout or L2 norm. (Because the means and variances are calculated over batches and therefore every normalized value depends on the current batch. I.e. the network can no longer just memorize values and their correct answers.)\nBN allows higher learning rates. (Because of less danger of exploding/vanishing gradients.)\nBN enables training with saturating nonlinearities in deep networks, e.g. sigmoid. (Because the normalization prevents them from getting stuck in saturating ranges, e.g. very high/low values for sigmoid.)\n\n BN reduces training times. (Because of less Covariate Shift, less exploding/vanishing gradients.) BN reduces demand for regularization, e.g. dropout or L2 norm. (Because the means and variances are calculated over batches and therefore every normalized value depends on the current batch. I.e. the network can no longer just memorize values and their correct answers.) BN allows higher learning rates. (Because of less danger of exploding/vanishing gradients.) BN enables training with saturating nonlinearities in deep networks, e.g. sigmoid. (Because the normalization prevents them from getting stuck in saturating ranges, e.g. very high/low values for sigmoid.) \n(2) Towards Reducing Covariate Shift\n\nBatch Normalization (BN) is a special normalization method for neural networks.\nIn neural networks, the inputs to each layer depend on the outputs of all previous layers.\nThe distributions of these outputs can change during the training. Such a change is called a covariate shift.\nIf the distributions stayed the same, it would simplify the training. Then only the parameters would have to be readjusted continuously (e.g. mean and variance for normal distributions).\nIf using sigmoid activations, it can happen that one unit saturates (very high/low values). That is undesired as it leads to vanishing gradients for all units below in the network.\nBN fixes the means and variances of layer inputs to specific values (zero mean, unit variance).\nThat accomplishes:\n\nNo more covariate shift.\nFixes problems with vanishing gradients due to saturation.\n\n\nEffects:\n\nNetworks learn faster. (As they don't have to adjust to covariate shift any more.)\nOptimizes gradient flow in the network. (As the gradient becomes less dependent on the scale of the parameters and their initial values.)\nHigher learning rates are possible. (Optimized gradient flow reduces risk of divergence.)\nSaturating nonlinearities can be safely used. (Optimized gradient flow prevents the network from getting stuck in saturated modes.)\nBN reduces the need for dropout. (As it has a regularizing effect.)\n\n\nHow BN works:\n\nBN normalizes layer inputs to zero mean and unit variance. That is called whitening.\nNaive method: Train on a batch. Update model parameters. Then normalize. Doesn't work: Leads to exploding biases while distribution parameters (mean, variance) don't change.\nA proper method has to include the current example and all previous examples in the normalization step.\nThis leads to calculating in covariance matrix and its inverse square root. That's expensive. The authors found a faster way.\n\n\n\n Batch Normalization (BN) is a special normalization method for neural networks. In neural networks, the inputs to each layer depend on the outputs of all previous layers. The distributions of these outputs can change during the training. Such a change is called a covariate shift. If the distributions stayed the same, it would simplify the training. Then only the parameters would have to be readjusted continuously (e.g. mean and variance for normal distributions). If using sigmoid activations, it can happen that one unit saturates (very high/low values). That is undesired as it leads to vanishing gradients for all units below in the network. BN fixes the means and variances of layer inputs to specific values (zero mean, unit variance). That accomplishes:\n\nNo more covariate shift.\nFixes problems with vanishing gradients due to saturation.\n\n No more covariate shift. Fixes problems with vanishing gradients due to saturation. Effects:\n\nNetworks learn faster. (As they don't have to adjust to covariate shift any more.)\nOptimizes gradient flow in the network. (As the gradient becomes less dependent on the scale of the parameters and their initial values.)\nHigher learning rates are possible. (Optimized gradient flow reduces risk of divergence.)\nSaturating nonlinearities can be safely used. (Optimized gradient flow prevents the network from getting stuck in saturated modes.)\nBN reduces the need for dropout. (As it has a regularizing effect.)\n\n Networks learn faster. (As they don't have to adjust to covariate shift any more.) Optimizes gradient flow in the network. (As the gradient becomes less dependent on the scale of the parameters and their initial values.) Higher learning rates are possible. (Optimized gradient flow reduces risk of divergence.) Saturating nonlinearities can be safely used. (Optimized gradient flow prevents the network from getting stuck in saturated modes.) BN reduces the need for dropout. (As it has a regularizing effect.) How BN works:\n\nBN normalizes layer inputs to zero mean and unit variance. That is called whitening.\nNaive method: Train on a batch. Update model parameters. Then normalize. Doesn't work: Leads to exploding biases while distribution parameters (mean, variance) don't change.\nA proper method has to include the current example and all previous examples in the normalization step.\nThis leads to calculating in covariance matrix and its inverse square root. That's expensive. The authors found a faster way.\n\n BN normalizes layer inputs to zero mean and unit variance. That is called whitening. Naive method: Train on a batch. Update model parameters. Then normalize. Doesn't work: Leads to exploding biases while distribution parameters (mean, variance) don't change. A proper method has to include the current example and all previous examples in the normalization step. This leads to calculating in covariance matrix and its inverse square root. That's expensive. The authors found a faster way. \n(3) Normalization via Mini-Batch Statistics\n\nEach feature (component) is normalized individually. (Due to cost, differentiability.)\nNormalization according to: componentNormalizedValue = (componentOldValue - E[component]) / sqrt(Var(component))\nNormalizing each component can reduce the expressitivity of nonlinearities. Hence the formula is changed so that it can also learn the identiy function.\nFull formula: newValue = gamma * componentNormalizedValue + beta (gamma and beta learned per component)\nE and Var are estimated for each mini batch.\nBN is fully differentiable. Formulas for gradients/backpropagation are at the end of chapter 3 (page 4, left).\n\n Each feature (component) is normalized individually. (Due to cost, differentiability.) Normalization according to: componentNormalizedValue = (componentOldValue - E[component]) / sqrt(Var(component)) Normalizing each component can reduce the expressitivity of nonlinearities. Hence the formula is changed so that it can also learn the identiy function. Full formula: newValue = gamma * componentNormalizedValue + beta (gamma and beta learned per component) E and Var are estimated for each mini batch. BN is fully differentiable. Formulas for gradients/backpropagation are at the end of chapter 3 (page 4, left). \n(3.1) Training and Inference with Batch-Normalized Networks\n\nDuring test time, E and Var of each component can be estimated using all examples or alternatively with moving averages estimated during training.\nDuring test time, the BN formulas can be simplified to a single linear transformation.\n\n During test time, E and Var of each component can be estimated using all examples or alternatively with moving averages estimated during training. During test time, the BN formulas can be simplified to a single linear transformation. \n(3.2) Batch-Normalized Convolutional Networks\n\nAuthors recommend to place BN layers after linear/fully-connected layers and before the ninlinearities.\nThey argue that the linear layers have a better distribution that is more likely to be similar to a gaussian.\nPlacing BN after the nonlinearity would also not eliminate covariate shift (for some reason).\nLearning a separate bias isn't necessary as BN's formula already contains a bias-like term (beta).\nFor convolutions they apply BN equally to all features on a feature map. That creates effective batch sizes of m*pq, where m is the number of examples in the batch and p q are the feature map dimensions (height, width). BN for linear layers has a batch size of m.\ngamma and beta are then learned per feature map, not per single pixel. (Linear layers: Per neuron.)\n\n Authors recommend to place BN layers after linear/fully-connected layers and before the ninlinearities. They argue that the linear layers have a better distribution that is more likely to be similar to a gaussian. Placing BN after the nonlinearity would also not eliminate covariate shift (for some reason). Learning a separate bias isn't necessary as BN's formula already contains a bias-like term (beta). For convolutions they apply BN equally to all features on a feature map. That creates effective batch sizes of m*pq, where m is the number of examples in the batch and p q are the feature map dimensions (height, width). BN for linear layers has a batch size of m. gamma and beta are then learned per feature map, not per single pixel. (Linear layers: Per neuron.) \n(3.3) Batch Normalization enables higher learning rates\n\nBN normalizes activations.\nResult: Changes to early layers don't amplify towards the end.\nBN makes it less likely to get stuck in the saturating parts of nonlinearities.\nBN makes training more resilient to parameter scales.\nUsually, large learning rates cannot be used as they tend to scale up parameters. Then any change to a parameter amplifies through the network and can lead to gradient explosions.\nWith BN gradients actually go down as parameters increase. Therefore, higher learning rates can be used.\n(something about singular values and the Jacobian)\n\n BN normalizes activations. Result: Changes to early layers don't amplify towards the end. BN makes it less likely to get stuck in the saturating parts of nonlinearities. BN makes training more resilient to parameter scales. Usually, large learning rates cannot be used as they tend to scale up parameters. Then any change to a parameter amplifies through the network and can lead to gradient explosions. With BN gradients actually go down as parameters increase. Therefore, higher learning rates can be used. (something about singular values and the Jacobian) \n(3.4) Batch Normalization regularizes the model\n\nUsually: Examples are seen on their own by the network.\nWith BN: Examples are seen in conjunction with other examples (mean, variance).\nResult: Network can't easily memorize the examples any more.\nEffect: BN has a regularizing effect. Dropout can be removed or decreased in strength.\n\n Usually: Examples are seen on their own by the network. With BN: Examples are seen in conjunction with other examples (mean, variance). Result: Network can't easily memorize the examples any more. Effect: BN has a regularizing effect. Dropout can be removed or decreased in strength. \n(4) Experiments\n \n(4.1) Activations over time\n** They tested BN on MNIST with a 100x100x10 network. (One network with BN before each nonlinearity, another network without BN for comparison.)\n** Batch Size was 60.\n** The network with BN learned faster. Activations of neurons (their means and variances over several examples) seemed to be more consistent during training.\n** Generalization of the BN network seemed to be better.\n \n(4.2) ImageNet classification\n** They applied BN to the Inception network.\n** Batch Size was 32.\n** During training they used (compared to original Inception training) a higher learning rate with more decay, no dropout, less L2, no local response normalization and less distortion/augmentation.\n** They shuffle the data during training (i.e. each batch contains different examples).\n** Depending on the learning rate, they either achieve the same accuracy (as in the non-BN network) in 14 times fewer steps (5x learning rate) or a higher accuracy in 5 times fewer steps (30x learning rate).\n** BN enables training of Inception networks with sigmoid units (still a bit lower accuracy than ReLU).\n** An ensemble of 6 Inception networks with BN achieved better accuracy than the previously best network for ImageNet.\n \n(5) Conclusion\n** BN is similar to a normalization layer suggested by G\u00fclcehre and Bengio. However, they applied it to the outputs of nonlinearities.\n** They also didn't have the beta and gamma parameters (i.e. their normalization could not learn the identity function).\n \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1502.03167"
    },
    "92": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Deep_Residual_Learning_for_Image_Recognition.md",
        "transcript": "\nWhy\n\nDeep plain/ordinary networks usually perform better than shallow networks.\nHowever, when they get too deep their performance on the training set decreases. That should never happen and is a shortcoming of current optimizers.\nIf the \"good\" insights of the early layers could be transferred through the network unaltered, while changing/improving the \"bad\" insights, that effect might disappear.\n\n Deep plain/ordinary networks usually perform better than shallow networks. However, when they get too deep their performance on the training set decreases. That should never happen and is a shortcoming of current optimizers. If the \"good\" insights of the early layers could be transferred through the network unaltered, while changing/improving the \"bad\" insights, that effect might disappear. \nWhat residual architectures are\n\nResidual architectures use identity functions to transfer results from previous layers unaltered.\nThey change these previous results based on results from convolutional layers.\nSo while a plain network might do something like output = convolution(image), a residual network will do output = image + convolution(image).\nIf the convolution resorts to just doing nothing, that will make the result a lot worse in the plain network, but not alter it at all in the residual network.\nSo in the residual network, the convolution can focus fully on learning what positive changes it has to perform, while in the plain network it first has to learn the identity function and then what positive changes it can perform.\n\n Residual architectures use identity functions to transfer results from previous layers unaltered. They change these previous results based on results from convolutional layers. So while a plain network might do something like output = convolution(image), a residual network will do output = image + convolution(image). If the convolution resorts to just doing nothing, that will make the result a lot worse in the plain network, but not alter it at all in the residual network. So in the residual network, the convolution can focus fully on learning what positive changes it has to perform, while in the plain network it first has to learn the identity function and then what positive changes it can perform. \nHow it works\n\nResidual architectures can be implemented in most frameworks. You only need something like a split layer and an element-wise addition.\nUse one branch with an identity function and one with 2 or more convolutions (1 is also possible, but seems to perform poorly). Merge them with the element-wise addition.\nRough example block (for a 64x32x32 input):\n\ninput (64x32x32) --> identity --------------> + --> output (64x32x32)\n       |                                      ^\n       ------------> conv 3x3 --> conv 3x3 ---|\n\n\nAn example block when you have to change the dimensionality (e.g. here from 64x32x32 to 128x32x32):\n\n                     to: 128x32x32\ninput (64x32x32) --> conv 1x1 -----------------------------> + --> output (128x32x32)\n       |                                                     ^\n       ------------> conv 1x1 ----> conv 3x3 --> conv 1x1 ---|\n                     to: 32x32x32                to: 128x32x32\n\n\nThe authors seem to prefer using either two 3x3 convolutions or the chain of 1x1 then 3x3 then 1x1. They use the latter one for their very deep networks.\nThe authors also tested:\n\nTo use 1x1 convolutions instead of identity functions everywhere. Performed a bit better than using 1x1 only for dimensionality changes. However, also computation and memory demands.\nTo use zero-padding for dimensionality changes (no 1x1 convs, just fill the additional dimensions with zeros). Performed only a bit worse than 1x1 convs and a lot better than plain network architectures.\n\n\nPooling can be used as in plain networks. No special architectures are necessary.\nBatch normalization can be used as usually (before nonlinearities).\n\n Residual architectures can be implemented in most frameworks. You only need something like a split layer and an element-wise addition. Use one branch with an identity function and one with 2 or more convolutions (1 is also possible, but seems to perform poorly). Merge them with the element-wise addition. Rough example block (for a 64x32x32 input): An example block when you have to change the dimensionality (e.g. here from 64x32x32 to 128x32x32): The authors seem to prefer using either two 3x3 convolutions or the chain of 1x1 then 3x3 then 1x1. They use the latter one for their very deep networks. The authors also tested:\n\nTo use 1x1 convolutions instead of identity functions everywhere. Performed a bit better than using 1x1 only for dimensionality changes. However, also computation and memory demands.\nTo use zero-padding for dimensionality changes (no 1x1 convs, just fill the additional dimensions with zeros). Performed only a bit worse than 1x1 convs and a lot better than plain network architectures.\n\n To use 1x1 convolutions instead of identity functions everywhere. Performed a bit better than using 1x1 only for dimensionality changes. However, also computation and memory demands. To use zero-padding for dimensionality changes (no 1x1 convs, just fill the additional dimensions with zeros). Performed only a bit worse than 1x1 convs and a lot better than plain network architectures. Pooling can be used as in plain networks. No special architectures are necessary. Batch normalization can be used as usually (before nonlinearities). \nResults\n\nResidual networks seem to perform generally better than similarly sized plain networks.\nThey seem to be able to achieve similar results with less computation.\nThey enable well-trainable very deep architectures with up to 1000 layers and more.\nThe activations of the residual layers are low compared to plain networks. That indicates that the residual networks indeed only learn to make \"good\" changes and default to \"if in doubt, change nothing\".\n\n Residual networks seem to perform generally better than similarly sized plain networks. They seem to be able to achieve similar results with less computation. They enable well-trainable very deep architectures with up to 1000 layers and more. The activations of the residual layers are low compared to plain networks. That indicates that the residual networks indeed only learn to make \"good\" changes and default to \"if in doubt, change nothing\". \n(1) Introduction\n\nIn classical architectures, adding more layers can cause the network to perform worse on the training set.\nThat shouldn't be the case. (E.g. a shallower could be trained and then get a few layers of identity functions on top of it to create a deep network.)\nTo combat that problem, they stack residual layers.\nA residual layer is an identity function and can learn to add something on top of that.\nSo if x is an input image and f(x) is a convolution, they do something like x + f(x) or even x + f(f(x)).\nThe classical architecture would be more like f(f(f(f(x)))).\nResidual architectures can be easily implemented in existing frameworks using skip connections with identity functions (split + merge).\nResidual architecture outperformed other in ILSVRC 2015 and COCO 2015.\n\n In classical architectures, adding more layers can cause the network to perform worse on the training set. That shouldn't be the case. (E.g. a shallower could be trained and then get a few layers of identity functions on top of it to create a deep network.) To combat that problem, they stack residual layers. A residual layer is an identity function and can learn to add something on top of that. So if x is an input image and f(x) is a convolution, they do something like x + f(x) or even x + f(f(x)). The classical architecture would be more like f(f(f(f(x)))). Residual architectures can be easily implemented in existing frameworks using skip connections with identity functions (split + merge). Residual architecture outperformed other in ILSVRC 2015 and COCO 2015. \n(3) Deep Residual Learning\n\nIf some layers have to fit a function H(x) then they should also be able to fit H(x) - x (change between x and H(x)).\nThe latter case might be easier to learn than the former one.\nThe basic structure of a residual block is y = x + F(x, W), where x is the input image, y is the output image (x + change) and F(x, W) is the residual subnetwork that estimates a good change of x (W are the subnetwork's weights).\nx and F(x, W) are added using element-wise addition.\nx and the output of F(x, W) must be have equal dimensions (channels, height, width).\nIf different dimensions are required (mainly change in number of channels) a linear projection V is applied to x: y = F(x, W) + Vx. They use a 1x1 convolution for V (without nonlinearity?).\nF(x, W) subnetworks can contain any number of layer. They suggest 2+ convolutions. Using only 1 layer seems to be useless.\nThey run some tests on a network with 34 layers and compare to a 34 layer network without residual blocks and with VGG (19 layers).\nThey say that their architecture requires only 18% of the FLOPs of VGG. (Though a lot of that probably comes from VGG's 2x4096 fully connected layers? They don't use any fully connected layers, only convolutions.)\nA critical part is the change in dimensionality (e.g. from 64 kernels to 128). They test (A) adding the new dimensions empty (padding), (B) using the mentioned linear projection with 1x1 convolutions and (C) using the same linear projection, but on all residual blocks (not only for dimensionality changes).\n(A) doesn't add parameters, (B) does (i.e. breaks the pattern of using identity functions).\nThey use batch normalization before each nonlinearity.\nOptimizer is SGD.\nThey don't use dropout.\n\n If some layers have to fit a function H(x) then they should also be able to fit H(x) - x (change between x and H(x)). The latter case might be easier to learn than the former one. The basic structure of a residual block is y = x + F(x, W), where x is the input image, y is the output image (x + change) and F(x, W) is the residual subnetwork that estimates a good change of x (W are the subnetwork's weights). x and F(x, W) are added using element-wise addition. x and the output of F(x, W) must be have equal dimensions (channels, height, width). If different dimensions are required (mainly change in number of channels) a linear projection V is applied to x: y = F(x, W) + Vx. They use a 1x1 convolution for V (without nonlinearity?). F(x, W) subnetworks can contain any number of layer. They suggest 2+ convolutions. Using only 1 layer seems to be useless. They run some tests on a network with 34 layers and compare to a 34 layer network without residual blocks and with VGG (19 layers). They say that their architecture requires only 18% of the FLOPs of VGG. (Though a lot of that probably comes from VGG's 2x4096 fully connected layers? They don't use any fully connected layers, only convolutions.) A critical part is the change in dimensionality (e.g. from 64 kernels to 128). They test (A) adding the new dimensions empty (padding), (B) using the mentioned linear projection with 1x1 convolutions and (C) using the same linear projection, but on all residual blocks (not only for dimensionality changes). (A) doesn't add parameters, (B) does (i.e. breaks the pattern of using identity functions). They use batch normalization before each nonlinearity. Optimizer is SGD. They don't use dropout. \n(4) Experiments\n\nWhen testing on ImageNet an 18 layer plain (i.e. not residual) network has lower training set error than a deep 34 layer plain network.\nThey argue that this effect does probably not come from vanishing gradients, because they (a) checked the gradient norms and they looked healthy and (b) use batch normaliaztion.\nThey guess that deep plain networks might have exponentially low convergence rates.\nFor the residual architectures its the other way round. Stacking more layers improves the results.\nThe residual networks also perform better (in error %) than plain networks with the same number of parameters and layers. (Both for training and validation set.)\nRegarding the previously mentioned handling of dimensionality changes:\n\n(A) Pad new dimensions: Performs worst. (Still far better than plain network though.)\n(B) Linear projections for dimensionality changes: Performs better than A.\n(C) Linear projections for all residual blocks: Performs better than B. (Authors think that's due to introducing new parameters.)\n\n\nThey also test on very deep residual networks with 50 to 152 layers.\nFor these deep networks their residual block has the form 1x1 conv -> 3x3 conv -> 1x1 conv (i.e. dimensionality reduction, convolution, dimensionality increase).\nThese deeper networks perform significantly better.\nIn further tests on CIFAR-10 they can observe that the activations of the convolutions in residual networks are lower than in plain networks.\nSo the residual networks default to doing nothing and only change (activate) when something needs to be changed.\nThey test a network with 1202 layers. It is still easily optimizable, but overfits the training set.\nThey also test on COCO and get significantly better results than a Faster-R-CNN+VGG implementation.\n\n When testing on ImageNet an 18 layer plain (i.e. not residual) network has lower training set error than a deep 34 layer plain network. They argue that this effect does probably not come from vanishing gradients, because they (a) checked the gradient norms and they looked healthy and (b) use batch normaliaztion. They guess that deep plain networks might have exponentially low convergence rates. For the residual architectures its the other way round. Stacking more layers improves the results. The residual networks also perform better (in error %) than plain networks with the same number of parameters and layers. (Both for training and validation set.) Regarding the previously mentioned handling of dimensionality changes:\n\n(A) Pad new dimensions: Performs worst. (Still far better than plain network though.)\n(B) Linear projections for dimensionality changes: Performs better than A.\n(C) Linear projections for all residual blocks: Performs better than B. (Authors think that's due to introducing new parameters.)\n\n (A) Pad new dimensions: Performs worst. (Still far better than plain network though.) (B) Linear projections for dimensionality changes: Performs better than A. (C) Linear projections for all residual blocks: Performs better than B. (Authors think that's due to introducing new parameters.) They also test on very deep residual networks with 50 to 152 layers. For these deep networks their residual block has the form 1x1 conv -> 3x3 conv -> 1x1 conv (i.e. dimensionality reduction, convolution, dimensionality increase). These deeper networks perform significantly better. In further tests on CIFAR-10 they can observe that the activations of the convolutions in residual networks are lower than in plain networks. So the residual networks default to doing nothing and only change (activate) when something needs to be changed. They test a network with 1202 layers. It is still easily optimizable, but overfits the training set. They also test on COCO and get significantly better results than a Faster-R-CNN+VGG implementation. \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1512.03385"
    },
    "93": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/ELUs.md",
        "transcript": "\nWhat\n\nELUs are an activation function\nThe are most similar to LeakyReLUs and PReLUs\n\n ELUs are an activation function The are most similar to LeakyReLUs and PReLUs \nHow (formula)\n\nf(x):\n\nif x >= 0: x\nelse: alpha(exp(x)-1)\n\n\nf'(x) / Derivative:\n\nif x >= 0: 1\nelse: f(x) + alpha\n\n\nalpha defines at which negative value the ELU saturates.\nE. g. alpha=1.0 means that the minimum value that the ELU can reach is -1.0\nLeakyReLUs however can go to -Infinity, ReLUs can't go below 0.\n\n f(x):\n\nif x >= 0: x\nelse: alpha(exp(x)-1)\n\n if x >= 0: x else: alpha(exp(x)-1) f'(x) / Derivative:\n\nif x >= 0: 1\nelse: f(x) + alpha\n\n if x >= 0: 1 else: f(x) + alpha alpha defines at which negative value the ELU saturates. E. g. alpha=1.0 means that the minimum value that the ELU can reach is -1.0 LeakyReLUs however can go to -Infinity, ReLUs can't go below 0. \nWhy\n\nThey derive from the unit natural gradient that a network learns faster, if the mean activation of each neuron is close to zero.\nReLUs can go above 0, but never below. So their mean activation will usually be quite a bit above 0, which should slow down learning.\nELUs, LeakyReLUs and PReLUs all have negative slopes, so their mean activations should be closer to 0.\nIn contrast to LeakyReLUs and PReLUs, ELUs saturate at a negative value (usually -1.0).\nThe authors think that is good, because it lets ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence.\nSo ELUs can measure the presence of concepts quantitatively, but the absence only qualitatively.\nThey think that this makes ELUs more robust to noise.\n\n They derive from the unit natural gradient that a network learns faster, if the mean activation of each neuron is close to zero. ReLUs can go above 0, but never below. So their mean activation will usually be quite a bit above 0, which should slow down learning. ELUs, LeakyReLUs and PReLUs all have negative slopes, so their mean activations should be closer to 0. In contrast to LeakyReLUs and PReLUs, ELUs saturate at a negative value (usually -1.0). The authors think that is good, because it lets ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence. So ELUs can measure the presence of concepts quantitatively, but the absence only qualitatively. They think that this makes ELUs more robust to noise. \nResults\n\nIn their tests on MNIST, CIFAR-10, CIFAR-100 and ImageNet, ELUs perform (nearly always) better than ReLUs and LeakyReLUs.\nHowever, they don't test PReLUs at all and use an alpha of 0.1 for LeakyReLUs (even though 0.33 is afaik standard) and don't test LeakyReLUs on ImageNet (only ReLUs).\n\n In their tests on MNIST, CIFAR-10, CIFAR-100 and ImageNet, ELUs perform (nearly always) better than ReLUs and LeakyReLUs. However, they don't test PReLUs at all and use an alpha of 0.1 for LeakyReLUs (even though 0.33 is afaik standard) and don't test LeakyReLUs on ImageNet (only ReLUs). \nIntroduction\n\nCurrently popular choice: ReLUs\nReLU: max(0, x)\nReLUs are sparse and avoid the vanishing gradient problem, because their derivate is 1 when they are active.\nReLUs have a mean activation larger than zero.\nNon-zero mean activation causes a bias shift in the next layer, especially if multiple of them are correlated.\nThe natural gradient (?) corrects for the bias shift by adjusting the weight update.\nHaving less bias shift would bring the standard gradient closer to the natural gradient, which would lead to faster learning.\nSuggested solutions:\n\nCentering activation functions at zero, which would keep the off-diagonal entries of the Fisher information matrix small.\nBatch Normalization\nProjected Natural Gradient Descent (implicitly whitens the activations)\n\n\nThese solutions have the problem, that they might end up taking away previous learning steps, which would slow down learning unnecessarily.\nChosing a good activation function would be a better solution.\nPreviously, tanh was prefered over sigmoid for that reason (pushed mean towards zero).\nRecent new activation functions:\n\nLeakyReLUs: x if x > 0, else alpha*x\nPReLUs: Like LeakyReLUs, but alpha is learned\nRReLUs: Slope of part < 0 is sampled randomly\n\n\nSuch activation functions with non-zero slopes for negative values seemed to improve results.\nThe deactivation state of such units is not very robust to noise, can get very negative.\nThey suggest an activation function that can return negative values, but quickly saturates (for negative values, not for positive ones).\nSo the model can make a quantitative assessment for positive statements (there is an amount X of A in the image), but only a qualitative negative one (something indicates that B is not in the image).\nThey argue that this makes their activation function more robust to noise.\nTheir activation function still has activations with a mean close to zero.\n\n Currently popular choice: ReLUs ReLU: max(0, x) ReLUs are sparse and avoid the vanishing gradient problem, because their derivate is 1 when they are active. ReLUs have a mean activation larger than zero. Non-zero mean activation causes a bias shift in the next layer, especially if multiple of them are correlated. The natural gradient (?) corrects for the bias shift by adjusting the weight update. Having less bias shift would bring the standard gradient closer to the natural gradient, which would lead to faster learning. Suggested solutions:\n\nCentering activation functions at zero, which would keep the off-diagonal entries of the Fisher information matrix small.\nBatch Normalization\nProjected Natural Gradient Descent (implicitly whitens the activations)\n\n Centering activation functions at zero, which would keep the off-diagonal entries of the Fisher information matrix small. Batch Normalization Projected Natural Gradient Descent (implicitly whitens the activations) These solutions have the problem, that they might end up taking away previous learning steps, which would slow down learning unnecessarily. Chosing a good activation function would be a better solution. Previously, tanh was prefered over sigmoid for that reason (pushed mean towards zero). Recent new activation functions:\n\nLeakyReLUs: x if x > 0, else alpha*x\nPReLUs: Like LeakyReLUs, but alpha is learned\nRReLUs: Slope of part < 0 is sampled randomly\n\n LeakyReLUs: x if x > 0, else alpha*x PReLUs: Like LeakyReLUs, but alpha is learned RReLUs: Slope of part < 0 is sampled randomly Such activation functions with non-zero slopes for negative values seemed to improve results. The deactivation state of such units is not very robust to noise, can get very negative. They suggest an activation function that can return negative values, but quickly saturates (for negative values, not for positive ones). So the model can make a quantitative assessment for positive statements (there is an amount X of A in the image), but only a qualitative negative one (something indicates that B is not in the image). They argue that this makes their activation function more robust to noise. Their activation function still has activations with a mean close to zero. \nZero Mean Activations Speed Up Learning\n\nNatural Gradient = Update direction which corrects the gradient direction with the Fisher Information Matrix\nHessian-Free Optimization techniques use an extended Gauss-Newton approximation of Hessians and therefore can be interpreted as versions of natural gradient descent.\nComputing the Fisher matrix is too expensive for neural networks.\nMethods to approximate the Fisher matrix or to perform natural gradient descent have been developed.\nNatural gradient = inverse(FisherMatrix) * gradientOfWeights\nLots of formulas. Apparently first explaining how the natural gradient descent works, then proofing that natural gradient descent can deal well with non-zero-mean activations.\nNatural gradient descent auto-corrects bias shift (i.e. non-zero-mean activations).\nIf that auto-correction does not exist, oscillations (?) can occur, which slow down learning.\nTwo ways to push means towards zero:\n\nUnit zero mean normalization (e.g. Batch Normalization)\nActivation functions with negative parts\n\n\n\n Natural Gradient = Update direction which corrects the gradient direction with the Fisher Information Matrix Hessian-Free Optimization techniques use an extended Gauss-Newton approximation of Hessians and therefore can be interpreted as versions of natural gradient descent. Computing the Fisher matrix is too expensive for neural networks. Methods to approximate the Fisher matrix or to perform natural gradient descent have been developed. Natural gradient = inverse(FisherMatrix) * gradientOfWeights Lots of formulas. Apparently first explaining how the natural gradient descent works, then proofing that natural gradient descent can deal well with non-zero-mean activations. Natural gradient descent auto-corrects bias shift (i.e. non-zero-mean activations). If that auto-correction does not exist, oscillations (?) can occur, which slow down learning. Two ways to push means towards zero:\n\nUnit zero mean normalization (e.g. Batch Normalization)\nActivation functions with negative parts\n\n Unit zero mean normalization (e.g. Batch Normalization) Activation functions with negative parts \nExponential Linear Units (ELUs)\n\nFormula\n\nf(x):\n\nif x >= 0: x\nelse: alpha(exp(x)-1)\n\n\nf'(x) / Derivative:\n\nif x >= 0: 1\nelse: f(x) + alpha\n\n\nalpha defines at which negative value the ELU saturates.\nalpha=0.5 => minimum value is -0.5 (?)\n\n\nELUs avoid the vanishing gradient problem, because their positive part is the identity function (like e.g. ReLUs)\nThe negative values of ELUs push the mean activation towards zero.\nMean activations closer to zero resemble more the natural gradient, therefore they should speed up learning.\nELUs are more noise robust than PReLUs and LeakyReLUs, because their negative values saturate and thus should create a small gradient.\n\"ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence\"\n\n Formula\n\nf(x):\n\nif x >= 0: x\nelse: alpha(exp(x)-1)\n\n\nf'(x) / Derivative:\n\nif x >= 0: 1\nelse: f(x) + alpha\n\n\nalpha defines at which negative value the ELU saturates.\nalpha=0.5 => minimum value is -0.5 (?)\n\n f(x):\n\nif x >= 0: x\nelse: alpha(exp(x)-1)\n\n if x >= 0: x else: alpha(exp(x)-1) f'(x) / Derivative:\n\nif x >= 0: 1\nelse: f(x) + alpha\n\n if x >= 0: 1 else: f(x) + alpha alpha defines at which negative value the ELU saturates. alpha=0.5 => minimum value is -0.5 (?) ELUs avoid the vanishing gradient problem, because their positive part is the identity function (like e.g. ReLUs) The negative values of ELUs push the mean activation towards zero. Mean activations closer to zero resemble more the natural gradient, therefore they should speed up learning. ELUs are more noise robust than PReLUs and LeakyReLUs, because their negative values saturate and thus should create a small gradient. \"ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence\" \nExperiments Using ELUs\n\nThey compare ELUs to ReLUs and LeakyReLUs, but not to PReLUs (no explanation why).\nThey seem to use a negative slope of 0.1 for LeakyReLUs, even though 0.33 is standard afaik.\nThey use an alpha of 1.0 for their ELUs (i.e. minimum value is -1.0).\nMNIST classification:\n\nELUs achieved lower mean activations than ReLU/LeakyReLU\nELUs achieved lower cross entropy loss than ReLU/LeakyReLU (and also seemed to learn faster)\nThey used 5 hidden layers of 256 units each (no explanation why so many)\n(No convolutions)\n\n\nMNIST Autoencoder:\n\nELUs performed consistently best (at different learning rates)\nUsually ELU > LeakyReLU > ReLU\nLeakyReLUs not far off, so if they had used a 0.33 value maybe these would have won\n\n\nCIFAR-100 classification:\n\nConvolutional network, 11 conv layers\nLeakyReLUs performed better during the first ~50 epochs, ReLUs mostly on par with ELUs\nLeakyReLUs about on par for epochs 50-100\nELUs win in the end (the learning rates used might not be optimal for ELUs, were designed for ReLUs)\n\n\nCIFER-100, CIFAR-10 (big convnet):\n\n6.55% error on CIFAR-10, 24.28% on CIFAR-100\nNo comparison with ReLUs and LeakyReLUs for same architecture\n\n\nImageNet\n\nBig convnet with spatial pyramid pooling (?) before the fully connected layers\nNetwork with ELUs performed better than ReLU network (better score at end, faster learning)\nNetworks were still learning at the end, they didn't run till convergence\nNo comparison to LeakyReLUs\n\n\n\n They compare ELUs to ReLUs and LeakyReLUs, but not to PReLUs (no explanation why). They seem to use a negative slope of 0.1 for LeakyReLUs, even though 0.33 is standard afaik. They use an alpha of 1.0 for their ELUs (i.e. minimum value is -1.0). MNIST classification:\n\nELUs achieved lower mean activations than ReLU/LeakyReLU\nELUs achieved lower cross entropy loss than ReLU/LeakyReLU (and also seemed to learn faster)\nThey used 5 hidden layers of 256 units each (no explanation why so many)\n(No convolutions)\n\n ELUs achieved lower mean activations than ReLU/LeakyReLU ELUs achieved lower cross entropy loss than ReLU/LeakyReLU (and also seemed to learn faster) They used 5 hidden layers of 256 units each (no explanation why so many) (No convolutions) MNIST Autoencoder:\n\nELUs performed consistently best (at different learning rates)\nUsually ELU > LeakyReLU > ReLU\nLeakyReLUs not far off, so if they had used a 0.33 value maybe these would have won\n\n ELUs performed consistently best (at different learning rates) Usually ELU > LeakyReLU > ReLU LeakyReLUs not far off, so if they had used a 0.33 value maybe these would have won CIFAR-100 classification:\n\nConvolutional network, 11 conv layers\nLeakyReLUs performed better during the first ~50 epochs, ReLUs mostly on par with ELUs\nLeakyReLUs about on par for epochs 50-100\nELUs win in the end (the learning rates used might not be optimal for ELUs, were designed for ReLUs)\n\n Convolutional network, 11 conv layers LeakyReLUs performed better during the first ~50 epochs, ReLUs mostly on par with ELUs LeakyReLUs about on par for epochs 50-100 ELUs win in the end (the learning rates used might not be optimal for ELUs, were designed for ReLUs) CIFER-100, CIFAR-10 (big convnet):\n\n6.55% error on CIFAR-10, 24.28% on CIFAR-100\nNo comparison with ReLUs and LeakyReLUs for same architecture\n\n 6.55% error on CIFAR-10, 24.28% on CIFAR-100 No comparison with ReLUs and LeakyReLUs for same architecture ImageNet\n\nBig convnet with spatial pyramid pooling (?) before the fully connected layers\nNetwork with ELUs performed better than ReLU network (better score at end, faster learning)\nNetworks were still learning at the end, they didn't run till convergence\nNo comparison to LeakyReLUs\n\n Big convnet with spatial pyramid pooling (?) before the fully connected layers Network with ELUs performed better than ReLU network (better score at end, faster learning) Networks were still learning at the end, they didn't run till convergence No comparison to LeakyReLUs \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1511.07289"
    },
    "94": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Fractional_Max_Pooling.md",
        "transcript": "\nWhat and why\n\nTraditionally neural nets use max pooling with 2x2 grids (2MP).\n2MP reduces the image dimensions by a factor of 2.\nAn alternative would be to use pooling schemes that reduce by factors other than two, e.g. 1 < factor < 2.\nPooling by a factor of sqrt(2) would allow twice as many pooling layers as 2MP, resulting in \"softer\" image size reduction throughout the network.\nFractional Max Pooling (FMP) is such a method to perform max pooling by factors other than 2.\n\n Traditionally neural nets use max pooling with 2x2 grids (2MP). 2MP reduces the image dimensions by a factor of 2. An alternative would be to use pooling schemes that reduce by factors other than two, e.g. 1 < factor < 2. Pooling by a factor of sqrt(2) would allow twice as many pooling layers as 2MP, resulting in \"softer\" image size reduction throughout the network. Fractional Max Pooling (FMP) is such a method to perform max pooling by factors other than 2. \nHow\n\nIn 2MP you move a 2x2 grid always by 2 pixels.\nImagine that these step sizes follow a sequence, i.e. for 2MP: 2222222...\nIf you mix in just a single 1 you get a pooling factor of <2.\nBy chosing the right amount of 1s vs. 2s you can pool by any factor between 1 and 2.\nThe sequences of 1s and 2s can be generated in fully random order or in pseudorandom order, where pseudorandom basically means \"predictable sub patterns\" (e.g. 211211211211211...).\nFMP can happen disjoint or overlapping. Disjoint means 2x2 grids, overlapping means 3x3.\n\n In 2MP you move a 2x2 grid always by 2 pixels. Imagine that these step sizes follow a sequence, i.e. for 2MP: 2222222... If you mix in just a single 1 you get a pooling factor of <2. By chosing the right amount of 1s vs. 2s you can pool by any factor between 1 and 2. The sequences of 1s and 2s can be generated in fully random order or in pseudorandom order, where pseudorandom basically means \"predictable sub patterns\" (e.g. 211211211211211...). FMP can happen disjoint or overlapping. Disjoint means 2x2 grids, overlapping means 3x3. \nResults\n\nFMP seems to perform generally better than 2MP.\nBetter results on various tests, including CIFAR-10 and CIFAR-100 (often quite significant improvement).\nBest configuration seems to be random sequences with overlapping regions.\nResults are especially better if each test is repeated multiple times per image (as the random sequence generation creates randomness, similar to dropout). First 5-10 repetitions seem to be most valuable, but even 100+ give some improvement.\nAn FMP-factor of sqrt(2) was usually used.\n\n FMP seems to perform generally better than 2MP. Better results on various tests, including CIFAR-10 and CIFAR-100 (often quite significant improvement). Best configuration seems to be random sequences with overlapping regions. Results are especially better if each test is repeated multiple times per image (as the random sequence generation creates randomness, similar to dropout). First 5-10 repetitions seem to be most valuable, but even 100+ give some improvement. An FMP-factor of sqrt(2) was usually used. \n(1) Convolutional neural networks\n\nAdvantages of 2x2 max pooling (2MP): fast; a bit invariant to translations and distortions; quick reduction of image sizes\nDisadvantages: \"disjoint nature of pooling regions\" can limit generalization (i.e. that they don't overlap?); reduction of image sizes can be too quick\nAlternatives to 2MP: 3x3 pooling with stride 2, stochastic 2x2 pooling\nAll suggested alternatives to 2MP also reduce sizes by a factor of 2\nAuthor wants to have reduction by sqrt(2) as that would enable to use twice as many pooling layers\nFractional Max Pooling = Pooling that reduces image sizes by a factor of 1 < alpha < 2\nFMP introduces randomness into pooling (by the choice of pooling regions)\nSettings of FMP:\n\nPooling Factor alpha in range [1, 2] (1 = no change in image sizes, 2 = image sizes get halfed)\nChoice of Pooling-Regions: Random or pseudorandom. Random is stronger (?). Random+Dropout can result in underfitting.\nDisjoint or overlapping pooling regions. Results for overlapping are better.\n\n\n\n Advantages of 2x2 max pooling (2MP): fast; a bit invariant to translations and distortions; quick reduction of image sizes Disadvantages: \"disjoint nature of pooling regions\" can limit generalization (i.e. that they don't overlap?); reduction of image sizes can be too quick Alternatives to 2MP: 3x3 pooling with stride 2, stochastic 2x2 pooling All suggested alternatives to 2MP also reduce sizes by a factor of 2 Author wants to have reduction by sqrt(2) as that would enable to use twice as many pooling layers Fractional Max Pooling = Pooling that reduces image sizes by a factor of 1 < alpha < 2 FMP introduces randomness into pooling (by the choice of pooling regions) Settings of FMP:\n\nPooling Factor alpha in range [1, 2] (1 = no change in image sizes, 2 = image sizes get halfed)\nChoice of Pooling-Regions: Random or pseudorandom. Random is stronger (?). Random+Dropout can result in underfitting.\nDisjoint or overlapping pooling regions. Results for overlapping are better.\n\n Pooling Factor alpha in range [1, 2] (1 = no change in image sizes, 2 = image sizes get halfed) Choice of Pooling-Regions: Random or pseudorandom. Random is stronger (?). Random+Dropout can result in underfitting. Disjoint or overlapping pooling regions. Results for overlapping are better. \n(2) Fractional max-pooling\n\nFor traditional 2MP, every grid's top left coordinate is at (2i-1, 2j-1) and it's bottom right coordinate at (2i, 2j) (i=col, j=row).\nIt will reduce the original size N to 1/2N, i.e. 2N_in = N_out.\nPaper analyzes 1 < alpha < 2, but alpha > 2 is also possible.\nGrid top left positions can be described by sequences of integers, e.g. (only column): 1, 3, 5, ...\nDisjoint 2x2 pooling might be 1, 3, 5, ... while overlapping would have the same sequence with a larger 3x3 grid.\nThe increment of the sequences can be random or pseudorandom for alphas < 2.\nFor 2x2 FMP you can represent any alpha with a \"good\" sequence of increments that all have values 1 or 2, e.g. 2111121122111121...\nIn the case of random FMP, the optimal fraction of 1s and 2s is calculated. Then a random permutation of a sequence of 1s and 2s is generated.\nIn the case of pseudorandom FMP, the 1s and 2s follow a pattern that leads to the correct alpha, e.g. 112112121121211212...\nRandom FMP creates varying distortions of the input image. Pseudorandom FMP is a faithful downscaling.\n\n For traditional 2MP, every grid's top left coordinate is at (2i-1, 2j-1) and it's bottom right coordinate at (2i, 2j) (i=col, j=row). It will reduce the original size N to 1/2N, i.e. 2N_in = N_out. Paper analyzes 1 < alpha < 2, but alpha > 2 is also possible. Grid top left positions can be described by sequences of integers, e.g. (only column): 1, 3, 5, ... Disjoint 2x2 pooling might be 1, 3, 5, ... while overlapping would have the same sequence with a larger 3x3 grid. The increment of the sequences can be random or pseudorandom for alphas < 2. For 2x2 FMP you can represent any alpha with a \"good\" sequence of increments that all have values 1 or 2, e.g. 2111121122111121... In the case of random FMP, the optimal fraction of 1s and 2s is calculated. Then a random permutation of a sequence of 1s and 2s is generated. In the case of pseudorandom FMP, the 1s and 2s follow a pattern that leads to the correct alpha, e.g. 112112121121211212... Random FMP creates varying distortions of the input image. Pseudorandom FMP is a faithful downscaling. \n(3) Implementation\n\nIn their tests they use a convnet starting with 10 convolutions, then 20, then 30, ...\nThey add FMP with an alpha of sqrt(2) after every conv layer.\nThey calculate the desired output size, then go backwards through their network to the input. They multiply the size of the image by sqrt(2) with every FMP layer and add a flat 1 for every conv layer. The result is the required image size. They pad the images to that size.\nThey use dropout, with increasing strength from 0% to 50% towards the output.\nThey use LeakyReLUs.\nEvery time they apply an FMP layer, they generate a new sequence of 1s and 2s. That indirectly makes the network an ensemble of similar networks.\nThe output of the network can be averaged over several forward passes (for the same image). The result then becomes more accurate (especially up to >=6 forward passes).\n\n In their tests they use a convnet starting with 10 convolutions, then 20, then 30, ... They add FMP with an alpha of sqrt(2) after every conv layer. They calculate the desired output size, then go backwards through their network to the input. They multiply the size of the image by sqrt(2) with every FMP layer and add a flat 1 for every conv layer. The result is the required image size. They pad the images to that size. They use dropout, with increasing strength from 0% to 50% towards the output. They use LeakyReLUs. Every time they apply an FMP layer, they generate a new sequence of 1s and 2s. That indirectly makes the network an ensemble of similar networks. The output of the network can be averaged over several forward passes (for the same image). The result then becomes more accurate (especially up to >=6 forward passes). \n(4) Results\n\nTested on MNIST and CIFAR-100\nArchitectures (somehow different from (3)?):\n\nMNIST: 36x36 img -> 6 times (32 conv (3x3?) -> FMP alpha=sqrt(2)) -> ? -> ? -> output\nCIFAR-100: 94x94 img -> 12 times (64 conv (3x3?) -> FMP alpha=2^(1/3)) -> ? -> ? -> output\n\n\nOverlapping pooling regions seemed to perform better than disjoint regions.\nRandom FMP seemed to perform better than pseudorandom FMP.\nOther tests:\n\n\"The Online Handwritten Assamese Characters Dataset\": FMP performed better than 2MP (though their network architecture seemed to have significantly more parameters\n\"CASIA-OLHWDB1.1 database\": FMP performed better than 2MP (again, seemed to have more parameters)\nCIFAR-10: FMP performed better than current best network (especially with many tests per image)\n\n\n\n Tested on MNIST and CIFAR-100 Architectures (somehow different from (3)?):\n\nMNIST: 36x36 img -> 6 times (32 conv (3x3?) -> FMP alpha=sqrt(2)) -> ? -> ? -> output\nCIFAR-100: 94x94 img -> 12 times (64 conv (3x3?) -> FMP alpha=2^(1/3)) -> ? -> ? -> output\n\n MNIST: 36x36 img -> 6 times (32 conv (3x3?) -> FMP alpha=sqrt(2)) -> ? -> ? -> output CIFAR-100: 94x94 img -> 12 times (64 conv (3x3?) -> FMP alpha=2^(1/3)) -> ? -> ? -> output Overlapping pooling regions seemed to perform better than disjoint regions. Random FMP seemed to perform better than pseudorandom FMP. Other tests:\n\n\"The Online Handwritten Assamese Characters Dataset\": FMP performed better than 2MP (though their network architecture seemed to have significantly more parameters\n\"CASIA-OLHWDB1.1 database\": FMP performed better than 2MP (again, seemed to have more parameters)\nCIFAR-10: FMP performed better than current best network (especially with many tests per image)\n\n \"The Online Handwritten Assamese Characters Dataset\": FMP performed better than 2MP (though their network architecture seemed to have significantly more parameters \"CASIA-OLHWDB1.1 database\": FMP performed better than 2MP (again, seemed to have more parameters) CIFAR-10: FMP performed better than current best network (especially with many tests per image) \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1412.6071"
    },
    "95": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Generative_Adversarial_Networks.md",
        "transcript": "\nWhat\n\nGANs are based on adversarial training.\nAdversarial training is a basic technique to train generative models (so here primarily models that create new images).\nIn an adversarial training one model (G, Generator) generates things (e.g. images). Another model (D, discriminator) sees real things (e.g. real images) as well as fake things (e.g. images from G) and has to learn how to differentiate the two.\nNeural Networks are models that can be trained in an adversarial way (and are the only models discussed here).\n\n GANs are based on adversarial training. Adversarial training is a basic technique to train generative models (so here primarily models that create new images). In an adversarial training one model (G, Generator) generates things (e.g. images). Another model (D, discriminator) sees real things (e.g. real images) as well as fake things (e.g. images from G) and has to learn how to differentiate the two. Neural Networks are models that can be trained in an adversarial way (and are the only models discussed here). \nHow\n\nG is a simple neural net (e.g. just one fully connected hidden layer). It takes a vector as input (e.g. 100 dimensions) and produces an image as output.\nD is a simple neural net (e.g. just one fully connected hidden layer). It takes an image as input and produces a quality rating as output (0-1, so sigmoid).\nYou need a training set of things to be generated, e.g. images of human faces.\nLet the batch size be B.\nG is trained the following way:\n\nCreate B vectors of 100 random values each, e.g. sampled uniformly from [-1, +1]. (Number of values per components depends on the chosen input size of G.)\nFeed forward the vectors through G to create new images.\nFeed forward the images through D to create ratings.\nUse a cross entropy loss on these ratings. All of these (fake) images should be viewed as label=0 by D. If D gives them label=1, the error will be low (G did a good job).\nPerform a backward pass of the errors through D (without training D). That generates gradients/errors per image and pixel.\nPerform a backward pass of these errors through G to train G.\n\n\nD is trained the following way:\n\nCreate B/2 images using G (again, B/2 random vectors, feed forward through G).\nChose B/2 images from the training set. Real images get label=1.\nMerge the fake and real images to one batch. Fake images get label=0.\nFeed forward the batch through D.\nMeasure the error using cross entropy.\nPerform a backward pass with the error through D.\n\n\nTrain G for one batch, then D for one (or more) batches. Sometimes D can be too slow to catch up with D, then you need more iterations of D per batch of G.\n\n G is a simple neural net (e.g. just one fully connected hidden layer). It takes a vector as input (e.g. 100 dimensions) and produces an image as output. D is a simple neural net (e.g. just one fully connected hidden layer). It takes an image as input and produces a quality rating as output (0-1, so sigmoid). You need a training set of things to be generated, e.g. images of human faces. Let the batch size be B. G is trained the following way:\n\nCreate B vectors of 100 random values each, e.g. sampled uniformly from [-1, +1]. (Number of values per components depends on the chosen input size of G.)\nFeed forward the vectors through G to create new images.\nFeed forward the images through D to create ratings.\nUse a cross entropy loss on these ratings. All of these (fake) images should be viewed as label=0 by D. If D gives them label=1, the error will be low (G did a good job).\nPerform a backward pass of the errors through D (without training D). That generates gradients/errors per image and pixel.\nPerform a backward pass of these errors through G to train G.\n\n Create B vectors of 100 random values each, e.g. sampled uniformly from [-1, +1]. (Number of values per components depends on the chosen input size of G.) Feed forward the vectors through G to create new images. Feed forward the images through D to create ratings. Use a cross entropy loss on these ratings. All of these (fake) images should be viewed as label=0 by D. If D gives them label=1, the error will be low (G did a good job). Perform a backward pass of the errors through D (without training D). That generates gradients/errors per image and pixel. Perform a backward pass of these errors through G to train G. D is trained the following way:\n\nCreate B/2 images using G (again, B/2 random vectors, feed forward through G).\nChose B/2 images from the training set. Real images get label=1.\nMerge the fake and real images to one batch. Fake images get label=0.\nFeed forward the batch through D.\nMeasure the error using cross entropy.\nPerform a backward pass with the error through D.\n\n Create B/2 images using G (again, B/2 random vectors, feed forward through G). Chose B/2 images from the training set. Real images get label=1. Merge the fake and real images to one batch. Fake images get label=0. Feed forward the batch through D. Measure the error using cross entropy. Perform a backward pass with the error through D. Train G for one batch, then D for one (or more) batches. Sometimes D can be too slow to catch up with D, then you need more iterations of D per batch of G. \nResults\n\nGood looking images MNIST-numbers and human faces. (Grayscale, rather homogeneous datasets.)\nNot so good looking images of CIFAR-10. (Color, rather heterogeneous datasets.)\n\n Good looking images MNIST-numbers and human faces. (Grayscale, rather homogeneous datasets.) Not so good looking images of CIFAR-10. (Color, rather heterogeneous datasets.) \nIntroduction\n\nDiscriminative models performed well so far, generative models not so much.\nTheir suggested new architecture involves a generator and a discriminator.\nThe generator learns to create content (e.g. images), the discriminator learns to differentiate between real content and generated content.\nAnalogy: Generator produces counterfeit art, discriminator's job is to judge whether a piece of art is a counterfeit.\nThis principle could be used with many techniques, but they use neural nets (MLPs) for both the generator as well as the discriminator.\n\n Discriminative models performed well so far, generative models not so much. Their suggested new architecture involves a generator and a discriminator. The generator learns to create content (e.g. images), the discriminator learns to differentiate between real content and generated content. Analogy: Generator produces counterfeit art, discriminator's job is to judge whether a piece of art is a counterfeit. This principle could be used with many techniques, but they use neural nets (MLPs) for both the generator as well as the discriminator. \nAdversarial Nets\n\nThey have a Generator G (simple neural net)\n\nG takes a random vector as input (e.g. vector of 100 random values between -1 and +1).\nG creates an image as output.\n\n\nThey have a Discriminator D (simple neural net)\n\nD takes an image as input (can be real or generated by G).\nD creates a rating as output (quality, i.e. a value between 0 and 1, where 0 means \"probably fake\").\n\n\nOutputs from G are fed into D. The result can then be backpropagated through D and then G. G is trained to maximize log(D(image)), so to create a high value of D(image).\nD is trained to produce only 1s for images from G.\nBoth are trained simultaneously, i.e. one batch for G, then one batch for D, then one batch for G...\nD can also be trained multiple times in a row. That allows it to catch up with G.\n\n They have a Generator G (simple neural net)\n\nG takes a random vector as input (e.g. vector of 100 random values between -1 and +1).\nG creates an image as output.\n\n G takes a random vector as input (e.g. vector of 100 random values between -1 and +1). G creates an image as output. They have a Discriminator D (simple neural net)\n\nD takes an image as input (can be real or generated by G).\nD creates a rating as output (quality, i.e. a value between 0 and 1, where 0 means \"probably fake\").\n\n D takes an image as input (can be real or generated by G). D creates a rating as output (quality, i.e. a value between 0 and 1, where 0 means \"probably fake\"). Outputs from G are fed into D. The result can then be backpropagated through D and then G. G is trained to maximize log(D(image)), so to create a high value of D(image). D is trained to produce only 1s for images from G. Both are trained simultaneously, i.e. one batch for G, then one batch for D, then one batch for G... D can also be trained multiple times in a row. That allows it to catch up with G. \nTheoretical Results\n\nLet\n\npd(x): Probability that image x appears in the training set.\npg(x): Probability that image x appears in the images generated by G.\n\n\nIf G is now fixed then the best possible D classifies according to: D(x) = pd(x) / (pd(x) + pg(x))\nIt is proofable that there is only one global optimum for GANs, which is reached when G perfectly replicates the training set probability distribution. (Assuming unlimited capacity of the models and unlimited training time.)\nIt is proofable that G and D will converge to the global optimum, so long as D gets enough steps per training iteration to model the distribution generated by G. (Again, assuming unlimited capacity/time.)\nNote that these things are proofed for the general principle for GANs. Implementing GANs with neural nets can then introduce problems typical for neural nets (e.g. getting stuck in saddle points).\n\n Let\n\npd(x): Probability that image x appears in the training set.\npg(x): Probability that image x appears in the images generated by G.\n\n pd(x): Probability that image x appears in the training set. pg(x): Probability that image x appears in the images generated by G. If G is now fixed then the best possible D classifies according to: D(x) = pd(x) / (pd(x) + pg(x)) It is proofable that there is only one global optimum for GANs, which is reached when G perfectly replicates the training set probability distribution. (Assuming unlimited capacity of the models and unlimited training time.) It is proofable that G and D will converge to the global optimum, so long as D gets enough steps per training iteration to model the distribution generated by G. (Again, assuming unlimited capacity/time.) Note that these things are proofed for the general principle for GANs. Implementing GANs with neural nets can then introduce problems typical for neural nets (e.g. getting stuck in saddle points). \nExperiments\n\nThey tested on MNIST, Toronto Face Database (TFD) and CIFAR-10.\nThey used MLPs for G and D.\nG contained ReLUs and Sigmoids.\nD contained Maxouts.\nD had Dropout, G didn't.\nThey use a Parzen Window Estimate aka KDE (sigma obtained via cross validation) to estimate the quality of their images.\nThey note that KDE is not really a great technique for such high dimensional spaces, but its the only one known.\nResults on MNIST and TDF are great. (Note: both grayscale)\nCIFAR-10 seems to match more the texture but not really the structure.\nNoise is noticeable in CIFAR-10 (a bit in TFD too). Comes from MLPs (no convolutions).\nTheir KDE score for MNIST and TFD is competitive or better than other approaches.\n\n They tested on MNIST, Toronto Face Database (TFD) and CIFAR-10. They used MLPs for G and D. G contained ReLUs and Sigmoids. D contained Maxouts. D had Dropout, G didn't. They use a Parzen Window Estimate aka KDE (sigma obtained via cross validation) to estimate the quality of their images. They note that KDE is not really a great technique for such high dimensional spaces, but its the only one known. Results on MNIST and TDF are great. (Note: both grayscale) CIFAR-10 seems to match more the texture but not really the structure. Noise is noticeable in CIFAR-10 (a bit in TFD too). Comes from MLPs (no convolutions). Their KDE score for MNIST and TFD is competitive or better than other approaches. \nAdvantages and Disadvantages\n\nAdvantages\n\nNo Markov Chains, only backprob\nInference-free training\nWide variety of functions can be incorporated into the model (?)\nGenerator never sees any real example. It only gets gradients. (Prevents overfitting?)\nCan represent a wide variety of distributions, including sharp ones (Markov chains only work with blurry images).\n\n\nDisadvantages\nNo explicit representation of the distribution modeled by G (?)\nD and G must be well synchronized during training\n\nIf G is trained to much (i.e. D can't catch up), it can collapse many components of the random input vectors to the same output (\"Helvetica\")\n\n\n\n Advantages\n\nNo Markov Chains, only backprob\nInference-free training\nWide variety of functions can be incorporated into the model (?)\nGenerator never sees any real example. It only gets gradients. (Prevents overfitting?)\nCan represent a wide variety of distributions, including sharp ones (Markov chains only work with blurry images).\n\n No Markov Chains, only backprob Inference-free training Wide variety of functions can be incorporated into the model (?) Generator never sees any real example. It only gets gradients. (Prevents overfitting?) Can represent a wide variety of distributions, including sharp ones (Markov chains only work with blurry images). Disadvantages No explicit representation of the distribution modeled by G (?) D and G must be well synchronized during training\n\nIf G is trained to much (i.e. D can't catch up), it can collapse many components of the random input vectors to the same output (\"Helvetica\")\n\n If G is trained to much (i.e. D can't catch up), it can collapse many components of the random input vectors to the same output (\"Helvetica\") \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1406.2661"
    },
    "96": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Inception_v4.md",
        "transcript": "\nOverview\n\nInception v4 is like Inception v3, but\n\nSlimmed down, i.e. some parts were simplified\nOne new version with residual connections (Inception-ResNet-v2), one without (Inception-v4)\n\n\nThey didn't observe an improved error rate when using residual connections.\nThey did however oberserve that using residual connections decreased their training times.\nThey had to scale down the results of their residual modules (multiply them by a constant ~0.1). Otherwise their networks would die (only produce 0s).\nResults on ILSVRC 2012 (val set, 144 crops/image):\n\nTop-1 Error:\n\nInception-v4: 17.7%\nInception-ResNet-v2: 17.8%\n\n\nTop-5 Error (ILSVRC 2012 val set, 144 crops/image):\n\nInception-v4: 3.8%\nInception-ResNet-v2: 3.7%\n\n\n\n\n\n Inception v4 is like Inception v3, but\n\nSlimmed down, i.e. some parts were simplified\nOne new version with residual connections (Inception-ResNet-v2), one without (Inception-v4)\n\n Slimmed down, i.e. some parts were simplified One new version with residual connections (Inception-ResNet-v2), one without (Inception-v4) They didn't observe an improved error rate when using residual connections. They did however oberserve that using residual connections decreased their training times. They had to scale down the results of their residual modules (multiply them by a constant ~0.1). Otherwise their networks would die (only produce 0s). Results on ILSVRC 2012 (val set, 144 crops/image):\n\nTop-1 Error:\n\nInception-v4: 17.7%\nInception-ResNet-v2: 17.8%\n\n\nTop-5 Error (ILSVRC 2012 val set, 144 crops/image):\n\nInception-v4: 3.8%\nInception-ResNet-v2: 3.7%\n\n\n\n Top-1 Error:\n\nInception-v4: 17.7%\nInception-ResNet-v2: 17.8%\n\n Inception-v4: 17.7% Inception-ResNet-v2: 17.8% Top-5 Error (ILSVRC 2012 val set, 144 crops/image):\n\nInception-v4: 3.8%\nInception-ResNet-v2: 3.7%\n\n Inception-v4: 3.8% Inception-ResNet-v2: 3.7% \nArchitecture\n\nBasic structure of Inception-ResNet-v2 (layers, dimensions):\n\nImage -> Stem -> 5x Module A -> Reduction-A -> 10x Module B -> Reduction B -> 5x Module C -> AveragePooling -> Droput 20% -> Linear, Softmax\n299x299x3 -> 35x35x256 -> 35x35x256 -> 17x17x896 -> 17x17x896 -> 8x8x1792 -> 8x8x1792 -> 1792 -> 1792 -> 1000\n\n\nModules A, B, C are very similar.\nThey contain 2 (B, C) or 3 (A) branches.\nEach branch starts with a 1x1 convolution on the input.\nAll branches merge into one 1x1 convolution (which is then added to the original input, as usually in residual architectures).\nModule A uses 3x3 convolutions, B 7x1 and 1x7, C 3x1 and 1x3.\nThe reduction modules also contain multiple branches. One has max pooling (3x3 stride 2), the other branches end in convolutions with stride 2.\n\n Basic structure of Inception-ResNet-v2 (layers, dimensions):\n\nImage -> Stem -> 5x Module A -> Reduction-A -> 10x Module B -> Reduction B -> 5x Module C -> AveragePooling -> Droput 20% -> Linear, Softmax\n299x299x3 -> 35x35x256 -> 35x35x256 -> 17x17x896 -> 17x17x896 -> 8x8x1792 -> 8x8x1792 -> 1792 -> 1792 -> 1000\n\n Image -> Stem -> 5x Module A -> Reduction-A -> 10x Module B -> Reduction B -> 5x Module C -> AveragePooling -> Droput 20% -> Linear, Softmax 299x299x3 -> 35x35x256 -> 35x35x256 -> 17x17x896 -> 17x17x896 -> 8x8x1792 -> 8x8x1792 -> 1792 -> 1792 -> 1000 Modules A, B, C are very similar. They contain 2 (B, C) or 3 (A) branches. Each branch starts with a 1x1 convolution on the input. All branches merge into one 1x1 convolution (which is then added to the original input, as usually in residual architectures). Module A uses 3x3 convolutions, B 7x1 and 1x7, C 3x1 and 1x3. The reduction modules also contain multiple branches. One has max pooling (3x3 stride 2), the other branches end in convolutions with stride 2. \nIntroduction, Related Work\n\nInception v3 was adapted to run on DistBelief. Inception v4 is designed for TensorFlow, which gets rid of some constraints and allows a simplified architecture.\nAuthors don't think that residual connections are inherently needed to train deep nets, but they do speed up the training.\nHistory:\n\nInception v1 - Introduced inception blocks\nInception v2 - Added Batch Normalization\nInception v3 - Factorized the inception blocks further (more submodules)\nInception v4 - Adds residual connections\n\n\n\n Inception v3 was adapted to run on DistBelief. Inception v4 is designed for TensorFlow, which gets rid of some constraints and allows a simplified architecture. Authors don't think that residual connections are inherently needed to train deep nets, but they do speed up the training. History:\n\nInception v1 - Introduced inception blocks\nInception v2 - Added Batch Normalization\nInception v3 - Factorized the inception blocks further (more submodules)\nInception v4 - Adds residual connections\n\n Inception v1 - Introduced inception blocks Inception v2 - Added Batch Normalization Inception v3 - Factorized the inception blocks further (more submodules) Inception v4 - Adds residual connections \nArchitectural Choices\n\nPrevious architectures were constrained due to memory problems. TensorFlow got rid of that problem.\nPrevious architectures were carefully/conservatively extended. Architectures ended up being quite complicated. This version slims down everything.\nThey had problems with residual networks dieing when they contained more than 1000 filters (per inception module apparently?). They could fix that by multiplying the results of the residual subnetwork (before the element-wise addition) with a constant factor of ~0.1.\n\n Previous architectures were constrained due to memory problems. TensorFlow got rid of that problem. Previous architectures were carefully/conservatively extended. Architectures ended up being quite complicated. This version slims down everything. They had problems with residual networks dieing when they contained more than 1000 filters (per inception module apparently?). They could fix that by multiplying the results of the residual subnetwork (before the element-wise addition) with a constant factor of ~0.1. \nTraining methodology\n\nKepler GPUs, TensorFlow, RMSProb (SGD+Momentum apprently performed worse)\n\n Kepler GPUs, TensorFlow, RMSProb (SGD+Momentum apprently performed worse) \nExperimental Results\n\nTheir residual version of Inception v4 (\"Inception-ResNet-v2\") seemed to learn faster than the non-residual version.\nThey both peaked out at almost the same value.\nTop-1 Error (ILSVRC 2012 val set, 144 crops/image):\n\nInception-v4: 17.7%\nInception-ResNet-v2: 17.8%\n\n\nTop-5 Error (ILSVRC 2012 val set, 144 crops/image):\n\nInception-v4: 3.8%\nInception-ResNet-v2: 3.7%\n\n\n\n Their residual version of Inception v4 (\"Inception-ResNet-v2\") seemed to learn faster than the non-residual version. They both peaked out at almost the same value. Top-1 Error (ILSVRC 2012 val set, 144 crops/image):\n\nInception-v4: 17.7%\nInception-ResNet-v2: 17.8%\n\n Inception-v4: 17.7% Inception-ResNet-v2: 17.8% Top-5 Error (ILSVRC 2012 val set, 144 crops/image):\n\nInception-v4: 3.8%\nInception-ResNet-v2: 3.7%\n\n Inception-v4: 3.8% Inception-ResNet-v2: 3.7% \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1602.07261v1"
    },
    "97": {
        "sourceUrl": "https://github.com//aleju/papers/blob/master/neural-nets/Weight_Normalization.md",
        "transcript": "\nWhat it is\n\nWeight Normalization (WN) is a normalization technique, similar to Batch Normalization (BN).\nIt normalizes each layer's weights.\n\n Weight Normalization (WN) is a normalization technique, similar to Batch Normalization (BN). It normalizes each layer's weights. \nDifferences to BN\n\nWN normalizes based on each weight vector's orientation and magnitude. BN normalizes based on each weight's mean and variance in a batch.\nWN works on each example on its own. BN works on whole batches.\nWN is more deterministic than BN (due to not working an batches).\nWN is better suited for noisy environment (RNNs, LSTMs, reinforcement learning, generative models). (Due to being more deterministic.)\nWN is computationally simpler than BN.\n\n WN normalizes based on each weight vector's orientation and magnitude. BN normalizes based on each weight's mean and variance in a batch. WN works on each example on its own. BN works on whole batches. WN is more deterministic than BN (due to not working an batches). WN is better suited for noisy environment (RNNs, LSTMs, reinforcement learning, generative models). (Due to being more deterministic.) WN is computationally simpler than BN. \nHow its done\n\nWN is a module added on top of a linear or convolutional layer.\nIf that layer's weights are w then WN learns two parameters g (scalar) and v (vector, identical dimension to w) so that w = gv / ||v|| is fullfilled (||v|| = euclidean norm of v).\ng is the magnitude of the weights, v are their orientation.\nv is initialized to zero mean and a standard deviation of 0.05.\nFor networks without recursions (i.e. not RNN/LSTM/GRU):\n\nRight after initialization, they feed a single batch through the network.\nFor each neuron/weight, they calculate the mean and standard deviation after the WN layer.\nThey then adjust the bias to -mean/stdDev and g to 1/stdDev.\nThat makes the network start with each feature being roughly zero-mean and unit-variance.\nThe same method can also be applied to networks without WN.\n\n\n\n WN is a module added on top of a linear or convolutional layer. If that layer's weights are w then WN learns two parameters g (scalar) and v (vector, identical dimension to w) so that w = gv / ||v|| is fullfilled (||v|| = euclidean norm of v). g is the magnitude of the weights, v are their orientation. v is initialized to zero mean and a standard deviation of 0.05. For networks without recursions (i.e. not RNN/LSTM/GRU):\n\nRight after initialization, they feed a single batch through the network.\nFor each neuron/weight, they calculate the mean and standard deviation after the WN layer.\nThey then adjust the bias to -mean/stdDev and g to 1/stdDev.\nThat makes the network start with each feature being roughly zero-mean and unit-variance.\nThe same method can also be applied to networks without WN.\n\n Right after initialization, they feed a single batch through the network. For each neuron/weight, they calculate the mean and standard deviation after the WN layer. They then adjust the bias to -mean/stdDev and g to 1/stdDev. That makes the network start with each feature being roughly zero-mean and unit-variance. The same method can also be applied to networks without WN. \nResults:\n\nThey define BN-MEAN as a variant of BN which only normalizes to zero-mean (not unit-variance).\nCIFAR-10 image classification (no data augmentation, some dropout, some white noise):\n\nWN, BN, BN-MEAN all learn similarly fast. Network without normalization learns slower, but catches up towards the end.\nBN learns \"more\" per example, but is about 16% slower (time-wise) than WN.\nWN reaches about same test error as no normalization (both ~8.4%), BN achieves better results (~8.0%).\nWN + BN-MEAN achieves best results with 7.31%.\nOptimizer: Adam\n\n\nConvolutional VAE on MNIST and CIFAR-10:\n\nWN learns more per example und plateaus at better values than network without normalization. (BN was not tested.)\nOptimizer: Adamax\n\n\nDRAW on MNIST (heavy on LSTMs):\n\nWN learns significantly more example than network without normalization.\nAlso ends up with better results. (Normal network might catch up though if run longer.)\n\n\nDeep Reinforcement Learning (Space Invaders):\n\nWN seemed to overall acquire a bit more reward per epoch than network without normalization. Variance (in acquired reward) however also grew.\nResults not as clear as in DRAW.\nOptimizer: Adamax\n\n\n\n They define BN-MEAN as a variant of BN which only normalizes to zero-mean (not unit-variance). CIFAR-10 image classification (no data augmentation, some dropout, some white noise):\n\nWN, BN, BN-MEAN all learn similarly fast. Network without normalization learns slower, but catches up towards the end.\nBN learns \"more\" per example, but is about 16% slower (time-wise) than WN.\nWN reaches about same test error as no normalization (both ~8.4%), BN achieves better results (~8.0%).\nWN + BN-MEAN achieves best results with 7.31%.\nOptimizer: Adam\n\n WN, BN, BN-MEAN all learn similarly fast. Network without normalization learns slower, but catches up towards the end. BN learns \"more\" per example, but is about 16% slower (time-wise) than WN. WN reaches about same test error as no normalization (both ~8.4%), BN achieves better results (~8.0%). WN + BN-MEAN achieves best results with 7.31%. Optimizer: Adam Convolutional VAE on MNIST and CIFAR-10:\n\nWN learns more per example und plateaus at better values than network without normalization. (BN was not tested.)\nOptimizer: Adamax\n\n WN learns more per example und plateaus at better values than network without normalization. (BN was not tested.) Optimizer: Adamax DRAW on MNIST (heavy on LSTMs):\n\nWN learns significantly more example than network without normalization.\nAlso ends up with better results. (Normal network might catch up though if run longer.)\n\n WN learns significantly more example than network without normalization. Also ends up with better results. (Normal network might catch up though if run longer.) Deep Reinforcement Learning (Space Invaders):\n\nWN seemed to overall acquire a bit more reward per epoch than network without normalization. Variance (in acquired reward) however also grew.\nResults not as clear as in DRAW.\nOptimizer: Adamax\n\n WN seemed to overall acquire a bit more reward per epoch than network without normalization. Variance (in acquired reward) however also grew. Results not as clear as in DRAW. Optimizer: Adamax \nExtensions\n\nThey argue that initializing g to exp(cs) (c constant, s learned) might be better, but they didn't get better test results with that.\nDue to some gradient effects, ||v|| currently grows monotonically with every weight update. (Not necessarily when using optimizers that use separate learning rates per parameters.)\nThat grow effect leads the network to be more robust to different learning rates.\nSetting a small hard limit/constraint for ||v|| can lead to better test set performance (parameter updates are larger, introducing more noise).\n\n They argue that initializing g to exp(cs) (c constant, s learned) might be better, but they didn't get better test results with that. Due to some gradient effects, ||v|| currently grows monotonically with every weight update. (Not necessarily when using optimizers that use separate learning rates per parameters.) That grow effect leads the network to be more robust to different learning rates. Setting a small hard limit/constraint for ||v|| can lead to better test set performance (parameter updates are larger, introducing more noise). \u00a9 2021 GitHub, Inc. Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1602.07868"
    }
}