{
    "0": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2018/09/13/addressing-challenges-in-deep-rl.html",
        "trancript": "Addressing Function Approximation Error in Actor-Critic Methods & Discriminator-Actor-Critic- Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nAddressing Function Approximation Error in Actor-Critic Methods & Discriminator-Actor-Critic- Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning\nSep 13, 2018\nFujimoto et al., 2018\nand\nKostrikov et al., 2018\nSummary\nI discuss two recent related papers in the Deep RL literature in this post. The first paper, by Fujimoto et al., introduces techniques for reducing bias and variance in a popular actor-critic method, Deep Deterministic Policy Gradient (DDPG). The second paper, by Kostrikov et al., makes a similar contribution by evaluating and addressing bias and variance in inverse RL. Both of these papers take widely used Deep RL algorithms, empirically and theoretically demonstrate specific weaknesses, and suggest reasonable improvements. These are valuable studies that help develop a better understanding of Deep RL.\nAddressing Function Approx. Error in AC Methods\nIf you are unfamiliar with DDPG, you can check out\nmy blog post\non the algorithm. The most important thing to know is that the success of the whole algorithm relies on having a critic network that can accurately estimate $Q$-values. The only signal the actor network gets in its gradient to help it achieve higher rewards comes from the gradient of the critic wrt the actions selected by the actor. If the critic gradient is biased, then the actor will fail to learn anything!\nIn Section 4, the authors begin by empirically demonstrating the overestimation bias present in the critic network (action-value estimator) in DDPG. They show that the overestimation bias essentially stems from the fact that DPG algorithms have to approximate both the policy and the value functions, and the approximate policy is maximized in the gradient direction provided by the approximate value function (rather than the true value function). Then, inspired by Double Q-Learning, they introduce a technique they call \u201cclipped Double Q-Learning in AC\u201d for achieving the same idea. Basically, the critic target becomes\n\\(y_1 = r + \\gamma \\min_{i=1,2} Q_{\\theta'_i} (s', \\pi_{\\theta_1}(s')).\\)\nThis requires introducing a second critic. The min makes it so that it is possible to underestimate Q-values, but this is preferable to overestimation.\nThen, to help with variance reduction, they suggest:\nDelay updating the actor network until the critic network has almost converged\nAdd some noise to the actions selected by the actor network when updating the critic to help regularize the critic, reminiscent of Expected SARSA\nTheir experimental results on MuJoco (they call their algorithm TD3) suggest these improvements are very effective, outperforming PPO, TRPO, ACKTR, and others.\nDiscriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning\nEDIT: The title of the paper was previously \u201cAddressing Sample Inefficiency and Reward Bias in Inverse RL\u201d\nSeemingly inspired by the former, this paper recently came out exploring inverse RL\u2014specifically, generative adversarial imitation learning (GAIL) and behavioral cloning. In GAIL, the discriminator learns to distinguish between transitions sampled from an expert and those from a trained policy. The policy is rewarded for confusing the discrminiator. However, GAIL is typically quite sample inefficient.\nOne way the authors suggest to help with the sample inefficiency of GAIL is by using off-policy RL instead of on-policy RL. They modify the GAIL objective to be \n\\(\\max_D \\mathcal{E}_\\mathscr{R} [\\log(D(s,a))] + \\mathcal{E}_{\\pi_E}[\\log(1 - D(s,a))] - \\lambda H(\\pi).\\)\nBasically, $\\pi_E$ is the expert policy, from which trajectories are sampled, and $\\mathscr{R}$ is the replay buffer, from which trajectories are sampled from ~all previous policies. They ignore the importance sampling term in practice. Since TD3 is technically a deterministic policy gradient algorithm, I\u2019m assuming one way to implement this importance sampling would be to have the actor output the mean of a multivariate Gaussian\u2014this Gaussian could then be used to define the entropy term of the policy and the importance sampling ratio. This is fairly common for continuous control tasks like MuJoco\u2026the authors note that the importance sampling wasn\u2019t used in practice, however.\nThey further analyzed different reward functions for GAIL, and show that certain GAIL reward functions can actually\ninhibit\nlearning depending on the particular MDP (e.g., if the environment has a survival bonus or penalty). To create a more robust reward function that will learn the expert policies,\nthey suggest explicitly learning rewards for absorbing states of the MDP\n. They implement this by adding an indicator to these particular states so that the GAIL discriminator can identify whether reaching an absorbing state is desirable from the perspective of the expert. In the\nOpenReview thread\n, one reviewer makes sure to point out that the problems with inverse RL algorithms highlighted in the paper are due to incorrect implementations of the MDP, rather than shortcomings of the algorithms themselves (\nsee this comment in particular\n).\nVery interestingly, they used VR to generate expert trajectories of gripping blocks with a Kuka arm. This environment has a per-step penalty, and the normal GAIL reward fails to learn the expert policy due to the the learning inhibition caused by the reward function bias. The proposed method learns to imitate the expert quickly due to its added reward for absorbing states.\nIt would be great to investigate the effects of using off-policy samples in the objective more carefully (why exactly does importance sampling not matter? The absorbing state reward stuff being so useful is surprising, and should be helpful in future applications where GAIL is used for inverse RL.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1809.02925v2"
    },
    "1": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2017/05/31/recurrent-environment-simulators.html",
        "trancript": "Recurrent Environment Simulators\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nRecurrent Environment Simulators\nMay 31, 2017\nChiappa, et al., 2017\nSummary\nThis paper extends the results on action-conditional video prediction from\nOh, et al., 2015\n. The motivation behind this line of research is to investigate whether training RL agents to learn how their actions affect the environment reduces the amount of time they spend in exploration. The authors outline the following main challenges:\nThe properties of generalization and sensitivity to model structure of these methods are poorly understood\nAccurate prediction for long time periods into the future is hard\nModels that predict the high-dim image directly each time an action is taken are inefficient\nThe general approach is to use a Conv-RNN to take an observation from the environment and produce a high-dim state representation. This high-dim state representation can be combined with the action taken by the agent to predict how the environment will change. In other words, the goal is to learn a parametric model that approximates the state-action transition.\nOne of the main contributions of this work is fusing the action with the hidden state representation when predicting the next hidden state representation in time. In previous work, the action was used instead to directly predict the next image. Why? Authors suggest it could \u201cenable the model to incorporate action information more effectively\u201d.\nUsing observations directly from the environment to predict the next hidden state representation forces the agent to make predictions only when the next frame is available, i.e., 1-step prediction. Rolling forward the agent\u2019s own predictions in the future allows it to predict many time steps ahead in a more efficient manner.\nPrediction-dependent transitions\nare ones of the form \\(s_t = f(s_{t-1}, a_{t-1}, C(\\mathbb{I}(\\hat x_{t-1}, x_{t-1})))\\) where $s_t$ is the hidden state representation.\nThere\u2019s a trade-off between short- and long-term accuracy and prediction- vs observation-dependent transitions. When prediction-dependent transitions are mostly used, the agent learns better global dynamics of the game and can do better with long-term accuracy. When observation-dependent transitions are mixed with prediction-dependent, the agent pays more attention to details that provide it with better short-term accuracy.\nThey evaluated their model on Breakout, Freeway, and Pong by replacing the real game with the learned environment simulator. A human would \u201cplay\u201d the game, and the learned simulator would return the next frame given the action input by the human.\nRelated works\nAction-conditional Video Prediction\nUNREAL\nLearning to Act By Predicting the Future\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1507.08750"
    },
    "2": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/12/20/learning-to-navigate-in-complex-envs.html",
        "trancript": "Learning To Navigate in Complex Environments\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nLearning To Navigate in Complex Environments\nDec 20, 2016\nMirowski, et al., 2016\nSummary\nA primary goal of this work is to incorporate the learning of complex navigation into the RL problem.\nAuxiliary tasks are used to augment the loss to provide denser training signals. The first auxiliary task is reconstruction of a low-dimensional depth map. The second task is self-supervised; the agent is trained to predict if the current location has been previously visited within a local trajectory.\n\u201cThe agent is trained by applying a weighted sum of the gradients coming from A3C, the gradients from depth prediction, and ther gradients from the loop closure\u201d\n\u201cIn particular if the prediction loss shares representation with the policy, it could help build useful features for RL much faster, bootstrapping learning\u201d\n\u00bb This is interesting, and a bit confusing. It seems like this is generalizing from the observation that including depth prediction in the loss was superior to directly using it as an input. I\u2019m not sure if this is always true, since it seems hard to quantify/evaluate this.\nIt\u2019s pretty cool that they\u2019re incorporating aspects of SLAM- penalizing loop closures for efficient exploration.\nThe authors test a couple different variations on a 3D maze navigation task. They use dynamic mazes and multiple goals to increase the difficulty. The A3C-variant with both auxiliary tasks performed the best on the most difficult tasks. The auxiliary tasks were shown to improve data efficiency.\nTakeaways\nEngineering auxiliary tasks into the loss is an interesting direction. This won\u2019t scale on its own for more complex tasks beyond navigation, but can potentially be used in conjunction with transfer learning.\nHow does this compare with SOTA SLAM-and-RRT motion-planners? I would like to see this implemented on some robots!\nCool, but still relies on RL methods based on data-inefficient and high-variance algorithms (A3C)\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/pdf/1611.03673v2.pdf"
    },
    "3": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/10/08/unifying-count-based-exploration-and-intrinsic-motivation.html",
        "trancript": "Unifying Count-Based Exploration and Intrinsic Motivation\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nUnifying Count-Based Exploration and Intrinsic Motivation\nOct 8, 2016\nBellemare, et al., 2016\nSummary\nThis paper presents a novel RL exploration bonus based on an adaptation of count-based exploration for high-dimensional spaces. The main contribution is the derivation of the relationships between\nprediction gain\n(PG), a quantity called the\npseudo-count\n, and the well-known\ninformation gain\nfrom the intrinsic RL literature. The overall presentation is clear and precise, especially when it comes to defining notation and explaining the core concepts. The authors use results from prior work on applying count-based exploration in complex domains, so reading this paper without having that background is a bit challenging (see the bibliography for recent papers by Bellemare and Veness on this topic). These results will help make previous count-based exploration methods, such as those devised around the idea of\noptimism in the face of uncertainty\n, viable for high-dimensional problems.\nQuick Definitions\nThis paper presents the following definitions and notation, along with theoretical results about their properties, in a rich manner. I will simply present definitions necessary for understanding the main contribution here; please see the paper for further details.\n$ \\mathcal{X} := $ finite or countable alphabet, with sub-sequences of length $n$ denoted by $x_{1:n} \\in \\mathcal{X}^n $\nA\nsequential density model\nover $\\mathcal{X}$ is a mapping from finite sub-sequences to probalitiy distributions over $\\mathcal{X}$. Denote these distributions by $\\rho_{n}(x) := \\rho(x ; x_{1:n})$\nThe\nrecoding probability\nof a symbol $x$ is denoted by $\\rho_{n}^{\\prime} := \\rho(x ; x_{1:n}x)$. That is, the probability assigned to $x$ by our sequential density model after observing a new occurence of $x$\n$ \\hat{N}(x) := $ the pseudo-count function\n$ \\hat{n} := $ the pseudo-count total\nMain Result\nThe following equations summarize the main results presented in this paper.\nThe authors relate the pseudo-count function and the pseudo-count total as follows.\n\\[\\begin{equation}\n\\rho_{n}(x) = \\frac{\\hat{N}(x)}{\\hat{n}} \\qquad\\text{and}\\qquad\n\\rho_{n}^{\\prime} = \\frac{\\hat{N}(x) + 1}{\\hat{n} + 1}\n\\end{equation}\\]\nFrom (1), we can write:\n\\[\\begin{equation}\n\\hat{N}(x) = \\frac{\n\t\\rho_{n}(x)(1 - \\rho_{n}^{\\prime}(x))\n}{\n\t\\rho_{n}^{\\prime}(x) - \\rho_{n}(x)\n}\n\n\\end{equation}\\]\nBy choosing an appropriate sequential density model, such as a graphical model. See the appendix of the paper for the description of the CTS model used by the authors in the experimentation; this model treats a 42 x 42 processed image as a factorizable observation, computing the probability of the image $x$ as the product of the probabilities of each (i, j) pixel. These pixel probabilities are computed via the neighbors of that pixel. See section 5.2 as well for a discussion on using directed graphical models as sequential density models.\nThe prediction gain is defined as\n\\[\\begin{equation}\nPG_{n}(x) := \\log \\rho_{n}^{\\prime}(x) - \\log \\rho_{n}(x)\n\\end{equation}\\]\nFor the information gain (IG) defined as the change in posterior w.r.t. the KL-divergence within a mixture model $\\xi$ defined over a class $\\mathcal{M}$ of sequential density models, we have:\n\\[\\begin{equation}\nIG_{n}(x) \\le PG_{n}(x) \\le \\hat{N}(x)^{-1}\n\\end{equation}\\]\nThe reward bonus proposed in this paper is as follows:\n\\[\\begin{equation}\nR_{n}^{+}(x, a) := \\beta(\\hat{N}(x) + 0.01)^{-1/2} ,\n\\end{equation}\\]\nwith $\\beta$ = 0.05 selected from a short parameter sweep and the small added constant for numerical stability. The authors compared different forms of this bonus, using $\\hat{N}(x)^{-1}$ and $PG_{n}(x)$.\nNotes\nThey implemented their exploration as a reward bonus for Double-DQN and\nA3C\n, and were able to show significant improvements on many Atari tasks. Most notably, they achieved the highest score to date on Montezuma\u2019s Revenge. I am very intrigued by this result; without using some form of memory such as an LSTM to \u201cremember\u201d long-term behaviors, their agent was able to explore efficiently enough to learn how to achieve the hierarchical sub-goals required to visit most of the rooms and achieve high scores. One of the main benefits of\nprediction gain\nand\npseudo-counts\nis that the agent is able to recognize and adjust its behaviors efficiently to salient events, which clearly plays a major role in solving games like Montezuma\u2019s Revenge. A convolutional neural net used to represent the value function for an Atari game must have a very large learning capacity, which is normally under-utilized when it comes to \u201chard\u201d games due to the inefficiency of $\\epsilon$-greedy exploration. For example, Montezuma\u2019s Revenge has 23 total rooms the agent is able to visit; the authors showed that the agent tends to only visit about 2 of these rooms without the reward bonus. After 100 million frames of training on Montezuma\u2019s Revenge with the suggested bonus, the agent had visited about 15 of these rooms! The CNN that uses a reward bonus must be learning representations from within all or most of the rooms, and encoding them in its hidden layers. Even though Montezuma\u2019s Revenge is partially observable, the agent is able to \u201cremember\u201d how to do the sub-tasks just by eventually stumbling upon the sparse rewards and cleverly updating its Q-values by means of the bonus. I would like to see what representations were encoded by the hidden layers of the network after training. Perhaps using a recurrent network and/or memory with attention + the reward bonus would help improve the learning speed, so that the sequential nature of the sub-tasks within the game can be encoded more readily.\nIt would be nice to see this approach compared with\nVIME\n. VIME computes the amount of information gained about the dynamics model due to the agent taking an action and seeing a certain following state. The authors show that the results should be similar, as maximizing the information gain also maximizes a lower bound on the inverse of the pseudo count.\nThe authors mention that other sequential density models with more sophistication could be used, as opposed to the simple CTS model. There clearly is a trade-off, however, such that more complex sequential density models would increase the time and space complexity significantly. For example,\nOh, et al., 2015\ndesigned a recurrent convolutional neural network architecture to predict future frames from a video sequence of an Atari game. They estimated the visitation frequency of a predicted frame by an empirical distribution over the contents of the replay memory; the count was computed using a gaussian kernel that provided a distance metric between frames. This was used as an exploration bonus.\nThe authors presented the appropriate metrics and figures to convince the reader of the effectiveness of their solution. They tested it widely on many (~60) Atari games to prove its widespread impact. (On a related note, it makes the difficulty for researchers that do not have access to the computational resources that DeepMind does to carry out such extensive experimentation all the more apparent!).\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1507.08750"
    },
    "4": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/09/10/hierarchical-drl-intrinsic-motivation.html",
        "trancript": "Hierarchical Deep Reinforcement Learning- Integrating Temporal Abstraction and Intrinsic Motivation\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nHierarchical Deep Reinforcement Learning- Integrating Temporal Abstraction and Intrinsic Motivation\nSep 10, 2016\nKulkarni, et al., 2016\nSummary\nThe first two sections of this paper provide a decent overview of recent advances in the hierarchical and intrinsic RL literature. The authors propose a learning framework for achieving complex goals in the face of sparse rewards. Drawing from\nSingh, et al.\n, a separation between intrinsic and extrinsic learning is derived, such that external reward signals are generated from the environment and internal reward signals are generated by an internal critic.\nThere is a meta-controller that learns an approximation of the optimal\ngoal policy\n$\\pi_{g} (g | s )$ and a controller that learns an approximation of the optimal\naction policy\n$\\pi_{a g} (a|g,s)$. The meta-controller operates on a slower time-scale than the controller; it is concerned with selecting the optimal goal for the controller to work towards. The internal critic provides incremental feedback for the controller, and the meta-controller receives external feedback from the environment.\nEach controller is represented as a Deep Q-Network, and the usual DRL learning tricks are applied.\nThe framework is tested in two settings, a stochastic decision making problem and the Atari game Montezuma\u2019s Revenge. The hierarchical agent significantly outperforms DQN on Montezuma\u2019s Revenge (DQN gets 0 points).\nMy Notes\nThe main contribution of this paper is the use of deep Q-networks for hierarchical/intrinsically motivated RL. However, the theory of intrinsically motivated RL and hierarchical RL with sub-policies for learning incremental behaviors under sparse extrinsic feedback is not novel; hence, the overall impact of the paper is substantially reduced.\nDQN is used as a baseline in the results/figures; but the reason for using such a questionable baseline is unclear. The authors even mention that Gorila DQN achieves a better average reward of 4.16; it would be better to see a more recent DRL algorithm used as a baseline.\nThe set of goals, the intrinsic critic, and the external reward all still need to be hand-crafted for every learning problem- not a flaw of the research, since the motivation for this work was to handle sparse rewards.\nThe algorithm relies doubly on epsilon-greedy exploration; an epsilon parameter is annealed for both policies. Even with the exploration decay schedule, the asymptotic variance of the total reward for the Montezuma experiment is pretty large.\nTo solve Montezuma\u2019s Revenge, they implemented a \u201ccustom object detector\u201d to identify objects in the game such as the ladder and key. However, the authors only mention it in passing. Also, the details of the internal critic for the Montezuma agent are unclear. It appears that they defined certain relations such as \u201cagent reaches ladder\u201d and \u201cagent reaches key\u201d and used these for intrinsic rewards, but it is unclear exactly how they went about doing it. They claim that their method does not require explicit encoding of the relations between objects as well.\nOverall, the idea of using hierarchical DQN to learn a temporal abstraction is a promising one, and it should be explored more. Unsupervised discovery of sub-tasks/goals is still an open problem in RL as well.\nAn interesting avenue to look into as mentioned by the authors is the use of\nevolutionary methods to search the space of reward functions\n.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://pemami4911.github.io/paper-summaries/deep-rl/2016/01/22/incentivizing-exploraton-in-rl.html"
    },
    "5": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/09/04/VIME.html",
        "trancript": "Variational Information Maximizing Exploration\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nVariational Information Maximizing Exploration\nSep 4, 2016\nHouthooft, et al., 2016 ~ OpenAI\nSummary\nThe authors present a solution to the problem of exploring the state-action space of a continuous control task efficiently. Their solution stems from prior work on curiousity-driven exploration, which makes use of key ideas taken from information theory. The general idea is to have the agent select actions that maximize the information gain about the agent\u2019s internal belief of the dynamics of the model. They use variational inference (VI) to measure information gain and Bayesian neural networks to represent the agent\u2019s belief of the environment\u2019s dynamics. The algorithm presented in Section 2.5 is referred to as VIME.\nVIME\nVIME uses VI to compute the posterior probability of the parameters of the environment dynamics. \nThat is, for the model $ p(s_{t+1}|s_{t}, a_{t}; \\theta) $, parameterized by the random variable $\\Theta$ with values $\\theta \\in \\Theta$, $p(\\theta | \\mathcal{D}) \\approx q(\\theta; \\phi)$. In this setting, $q(\\theta; \\phi)$ is represented as factorized distribution, as is common in VI. In particular, they use a Bayesian Neural Network parameterized by a fully factorized Gaussian distrubtion.\nKey point:\nThe agent is encouraged to take actions that lead to states that are maximally informative about the dynamics model.\nLetting the history of the agent up to time step $t$ be $\\xi_{t} = \\{s_{0}, a_{0}, s_{1}, a_{1}, \u2026, s_{t}\\} $, we can derive the mutual information of the dynamics model before and after taking action $a_{t}$ as:\n\\[\\begin{equation}\n\nI(S_{t+1}; \\Theta | \\xi_{t}, a_{t}) = \\mathbb{E}_{s_{t+1} \\sim P(.| \\xi_{t}, a_{t})} \\big [ D_{KL} [p(\\theta|\\xi_{t}, a_{t}) \\hspace{2pt} || \\hspace{2pt} p(\\theta|\\xi_{t})] \\big]\n\n\\end{equation}\\]\nUsing the variational distribution $q$, we can approximate our posterior distribution by minimizing $ D_{KL} [q(\\theta|\\phi) \\hspace{2pt} || \\hspace{2pt} p(\\theta| \\mathcal{D})] $. This is done through the maximization of the variational lower bound $L[q]$:\n\\[\\begin{equation}\n\nL[q(\\theta; \\phi), \\mathcal{D}] = \\mathbb{E}_{\\theta \\sim q(.;\\phi)} \\big[ \\log p (\\mathcal{D} | \\theta ) \\big] -  D_{KL} [q(\\theta|\\phi) \\hspace{2pt} || \\hspace{2pt} p(\\theta)]\n\n\\end{equation}\\]\nA clear derivation of the variational lower bound is available on\nwikipedia\n. Note that in this case, the log evidence term $ \\log p(\\mathcal{D}) $ is computed as an expectation over the model parameters $\\theta$ w.r.t. the variational distribution $q$.\nThe variational distribution is then used to compute a \u201cbonus\u201d for the external reward function as follows:\n\\[\\begin{equation}\n\nr'(s_{t}, a_{t}, s_{t+1}) = r(s_{t}, a_{t}) + \\eta D_{KL} [ q(\\theta; \\phi_{t+1}) \\hspace{2pt} || \\hspace{2pt} q(\\theta; \\phi_{t}) ]\n\\end{equation}\\]\nThe KL-divergence between the new approximate posterior and the old approximate posterior thereby represents the information gained by having taken $a_{t}$, ended up in state $s_{t+1}$. The hyperparameter $\\eta$ controls the amount of incentive to explore (the curiosity). See Section 2.4 of the paper for an analogy drawn to model compression (the intuition is that the most informative state-action sequence up to time $t$ is the one with the shortest description length). Section 2.5 contains implementation details and an outline of the overall algorithm. One of the key points of Section 2.5 is that the use of a fully factorized Gaussian allows the KL-divergence in Eq. 3 to be computedly simply.\nBroader Impact on the RL Community\nThe authors conducted experiments with VIME to show that, when augmenting an RL algorithm, there are significant improvements in the face of sparse reward signals. They specifically tested it with Trust-Region Policy Optimization (TRPO), REINFORCE, and ERWR. As a baseline, they used these algorithms sans VIME.\nThey included a neat figure of the state space exploration in MountainCar of TRPO with and without VIME. With VIME, the exploration is much more diffused, whereas with naive Gaussian exploration it\u2019s condensed and ball-shaped around the origin.\nIn my opinion, this is one of the major avenues for further research in RL. We should be focused on developing strategies for learning with sparse or nonexistant reward signals. This is extremely important for bringing RL into new problem domains. As can be seen so far, information theory offers a promising starting point.\nSimilar work by\nMohamed and Rezende\nin 2015 presented an algorithm inspired by intrinsically motivated RL that focused on the notion of\nempowerment\n. This is a broad term that attempts to formalize the notion of having \u201cinternal drives\u201d that agents can experience to learn about their environment in an unsupervised fashion. They developed a stochastic variational information maximization algorithm. The formulation as presented in this paper is useful when explicit external reward functions are not available to an agent. The computed empowerment can be used in a closed-loop planner such as Q-learning; agents can then learn information-maximizing behaviors this way. The paper contains some cool examples of this. A major distinction with VIME, however, is that empowerment doesn\u2019t necessarily favor exploration- as stated by Mohamed and Rezende, agents are only \u2018curious\u2019 about parts of its environment that can be reached within its internal planning horizon.\nDespite the significance of the recent work in the intersection of VI and intrinsically motivated RL, these methods are non-trivial and hence will most likely catch on slower.\nNotes\nBayesian RL\nand\nPAC-MDP\nBoltzmann exploration requires a training time exponential in the number of states in order to solve an n-chain MDP..!\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/pdf/1605.09674v2.pdf"
    },
    "6": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/08/16/Deep-exploration.html",
        "trancript": "Deep Exploration via Bootstrapped DQN\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nDeep Exploration via Bootstrapped DQN\nAug 16, 2016\nOsband, et al., 2016\nSummary\nThis paper presents a novel approach to replace the classic\nepsilon-greedy\nexploration strategy. The main idea is to encourage\ndeep\nexploration by creating a new Deep Q-Learning architecture that supports selecting actions from randomized Q-functions that are trained on\nbootstrapped\ndata. This is a quick look at the proposed architecture.\nBootstrapped DQN architecture\nEach\nhead\nrepresents a different Q-function that is trained on a subset of the data. The shared network learns a joint feature representation across all the data; it can be thought of as a data-dependent dropout. For DRL, samples stored in a replay buffer contain a flag marking which of the K Q-functions it came from.\nThe algorithm speeds up learning compared to other exploration tactics for DRL since it encourages\ndeep\nexploration; at the beginning of each episode, a different Q-function is randomly sampled from a uniform distribution and it is used until the end of that episode.\nAnother key component of the Bootstrapped DQN algorithm is the bootstrap mask. The mask decides, for each Q-value function, whether or not it should train upon the experience generated at step t. Each individual experience is given a randomly sampled mask m ~ M, where M is Bernoulli, Poission, etc. Then, when training the network on a minibatch sampled from the replay buffer, the mask m decides whether or not a specific Q-value function is to be trained upon that experience. The authors show that the effect of this on the learning process is akin to dropout. This heuristic, plus the randomized Q-value functions, help Bootstrapped DQN deal with learning from noisy data and exploring complex state/action spaces efficiently.\nStrengths\nThe authors based their idea on sound statistical principles and conducted numerous experiments to back up their claims. Their results show that Bootstrapped DQN can learn faster (but not necessarily with higher long-term rewards) than state-of-the-art DQN. The authors also compare their work with Stadie, Levine, and Abeel\u2019s paper on Incentivizing Exploration in RL. See my previous post for\ndetails\n. The authors show that Bootstrapped DQN outperforms Stadie\u2019s methods, as Stadie\u2019s methods attempt the more ambitious task of learning a model of the task dynamics and using how well the agent has learned said model to inform the exploration.\nWeaknesses\nThe paper is a bit hard to follow at times, and you have to go all the way to the appendix to get a good understanding of how the entire algorithm comes together and works. The step-by-step algorithm description could be more complete (there are steps of the training process left out, albeit they are not unique to Bootstrap DQN) and should not be hidden down in the appendix. This should probably be in Section 3. The MDP examples in Section 5 were not explained well; it feels like it doesn\u2019t contribute too much to the overall impact of the paper.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/pdf/1602.04621v3.pdf"
    },
    "7": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/08/02/A3C.html",
        "trancript": "Asynchronous Methods for Deep Reinforcement Learning\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nAsynchronous Methods for Deep Reinforcement Learning\nAug 2, 2016\nMnih, et al., 2016\nSummary\nThe authors presented a number of asynchronous DRL algorithms with the intention of developing RL agents that can be trained on CPUs with multithreading. \nThe algorithms used multiple threads to run copies of the environment and generate uncorrelated sequences of training samples. \nParameters were then sent to a shared parameter server at regular intervals. Because this promotes non-stationarity for the sequences of SARSA tuples, experience replay is not necessarily needed.\nThe implementations of RMSProp and Momentum SGD used by the authors employed a Hogwild!-inpsired lock free scheme for maximum efficiency. \nA3C is the \u201cbest\u201d agent that was presented in this paper. It is an asynchronous advantage actor-critic algorithm. It maintains an approximation of the\npolicy, an estimate of the value function, and computes an \u201cadvantage\u201d function and a variance-reducing baseline\nDegris, et al. 2012\n. An entropy regularization term was also used to discourage premature convergence.\nNotes\nImplemented 1-step Q-learning, 1-step SARSA, n-step Q-learning, and Advantage Actor-Critic\nA benefit is that learning is stabilized without having to use experience replay\nReduction in training time that is roughly linear in the number of parallel actor-learners.\nIs this good? Seems like we can do better.\nThe RL algorithms used are fairly data-inefficient.\nNot really a noticeable difference in performance between n-step Q-learning and 1-step, except on TORCS.\nA3C uses n-step lookahead as well\nEvidence\nState-of-the-art results were obtained on some of the Atari games (ALE). An LSTM-based A3C agent was tested with Deepmind\u2019s Labyrinth environment. They also tested on the TORCS car racing environment and MuJoCo, the continuous-space physics simulation engine.\nStrengths\nPresenting the algorithms in pseudocode is very helpful to the reader. The authors went into implementation details, which is also helpful for those who wish to check the results for themselves.\nFuture Directions\nHow can we reduce/control the asymptotic variance of these temporal difference methods? Also, of the actor-critic method? (Need to research this topic more).\nDueling Networks for the state value and advantage functions\nReducing over-estimation bias of Q-values (Double DQN, etc)\nTry comparing with async Recurrent-DDPG? A3C seems similar to DDPG- difference is with the deterministic vs. stochastic gradient update.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://icml.cc/2012/papers/268.pdf"
    },
    "8": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/03/23/nfq.html",
        "trancript": "Neural Fitted Q Iteration\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nNeural Fitted Q Iteration\nMar 23, 2016\nRiedmiller, M. 2005\nSummary\nThe key insight here is that neural networks react globally to local weight changes, which results in unwanted behavior. For a neural network that is representing the Q-value function, the influence of a weight change for a new datapoint must be constrained by presenting previous knowledge in the form or prior experiences. The proposed algorithm is a special case of experience replay.\nIn principle, classical Q-learning can be directly implemented in a neural network. An MSE can be used to calculate a loss between the expected Q-value and the generated Q-value. Vanilla application of this transformation requires tens of thousands of training examples due to the problem stated above.\nNFQ attempts to address this by doing off-line learning considering an entire set of transition experiences. RPROP can be used for updating the weights of the neural network, which is an advanced supervised learning technique.\nStrengths\nNFQ is very flexible. One variant is to incrementally add new experiences to D, the set of transition experiences. This is useful for cases where a reasonable set of experiences can not be collected by controlling the system with purely random actions. It seems like you would want to do a combination of both this, and pre-training on sample paths.\nMethods\nFor the mountain car setup, the authors used an MLP with 3 input neurons (2 state and 1 action), 2 layers of 5 hidden neurons each and 1 output neuron, all with sigmoidal activation functions.\nInteresting related works\nRPROP and Batch Learning, both my Riedmiller\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://ml.informatik.uni-freiburg.de/_media/publications/rieecml05.pdf"
    },
    "9": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/03/04/AlphaGo.html",
        "trancript": "Mastering the Game of Go with Deep Neural Networks and Tree Search\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nMastering the Game of Go with Deep Neural Networks and Tree Search\nMar 4, 2016\nSilver, et al., 2016\nSummary\nAn SL policy network of convolutional layers $p_{\\sigma}$ is learned in a supervised fashion directly from expert human moves. This provides immediate feedback, high-quality gradients, and is used for improving board evaluation.\nAnother \u201cfast rollout\u201d network, $p_{\\pi}$, is trained for fast action selection during rollouts from expert human moves. Rollouts are off-policy simulated trajectories used for determining future pathways in a search tree.\nAn RL policy network $p_{\\rho}$ is trained for optimizing winning games through self-play. The weights are initialized to the SL policy network\u2019s. The reward signal is generated as the end of the game (+1 for winning, -1 for losing).\nFinally, a value network $v_{\\theta}$ is trained by regression to predict the winner of games played by the RL policy network against itself.\nMethodology\nMonte Carlo Tree Search\nAlphaGo uses a form of MCTS for position evaluation. The algorithm for this mixes full rollouts with truncated rollouts, resembling \n$TD(\\gamma)$. To integrate MCTS into AlphaGo, an asynchronous policy and value MCTS (APV-MCTS) algorithm was developed.\nAsync Search Algorithm\nEach node in the tree stores the prior probability $P(s,a)$, Monte Carlo estimates of the total action value \ncalculated by a weighted combination of the rollout policy and the value network, the number of times the rollout policy and \nthe value network has evaluated the node, and the combined mean action value.\nSelection\nThe algorithm begins at the root and uses a variant of UCT to traverse the tree for time steps $t \\lt L$, where $L$ is the step at which the algorithm reaches leaf node $s_{L}$\nExpansion\nPrior probabilities $P(s_{L}, a)$ are computed by the SL policy network with a softmax temperature set to $\\beta$. When the visit count for a successor node exceeds a threshold, the successor state/node $s\u2019 = f(s,a)$ is added to the search tree. The new node is added to a queue for async GPU evaluation by the policy network.  The threshold is adjusted dynamically to ensure that the rate at which positions are added to the policy queue matches the rate at which the GPUs evaluate the policy network. Mini-batch sizes of 1 were used to minimize the end-to-end evaluation time.\nEvaluation\nThe leaf node $s_{L}$ is added to a queue for evaluation by the value network $v_{\\theta}$, unless it has previously been evaluated. Then, the fast rollout policy $p_{\\pi}$ is used to simulate the rest of the game until a terminal reward is obtained.\nBackup\nAt each in-tree step $t \\le L$, the rollout statistics are updated as if it has virtually lost $n_{vl}$ games, to discourage other threads from simulateously exploring the identical variation. The value network\u2019s output is used to update value \nstatistics in a second backward pass through each step $t \\le L$. At the end of the simulation, the rollout statistics\nare updated by replacing the virtual losses by the true outcome. The mean action value $Q(s,a)$ is a weighted average\nof the output of the value network and the rollout evaluations.\nA distributed APV-MCTS was implemented which split policy evaluation across worker GPUs and handled \nthe in-tree phase of each simulation on worker CPUs.\nRollout policy\nThis is a linear softmax policy based on fast, incrementally computed, local pattern-based features. A number of \nhandcrafted local features encode common-sense Go rules. Similar to the policy network, the weights $\\pi$ of the \nrollout policy are trained from 8 million positions from human games to maximize the log likelihood by \nstochastic gradient descent. 3 x 3 pattern matching against previous moves selected by the most probable action \nis used; this is a generalization of the \u2018last good reply\u2019 heuristic.\nSymmetries\nGo symmetry is exploited at run-time by dynamically transforming each position using 8 reflections/rotations.\nSL policy Network\nA data set of 29.4 million positions from 160,000 games was used. Each position consisted of a raw board description \n\u2018s\u2019 and the move \u2018a\u2019 selected by the human. Each position was augmented with all 8 reflections/rotatoins. \nAsynchronous SGD was used to maximize the log likelihood of the action. Step size was was initialized to 0.003 and halved\nevery 80 million steps, without momentum, and with a mini-batch size of 16. Gradients older than 100 steps were\ndiscarded.\nRL policy network\nN games were played in parallel between the current policy network that was being trained and an opponent\nthat uses parameters from a previous iteration of the network, randomly sampled from a pool of opponents. Games\nwere played out till termination, scored, then replayed to calculate the policy gradient update. \nREINFORCE was used with a baseline for variance reduction. On the first pass, the baseline was set to 0, \non the second pass, the value network was used as a baseline. 10,000 minibatches of 128 games was the training set.\nValue network trained on RL policy network\nThe Value Network $v_{\\theta}$ was trained to approximate the value function of the RL policy network. Overfitting was handled by crafting a dataset of uncorrelated self-play positions. Each game was generated by randomly sampling a time-step\n$U$, sampling the first $t = 1$ to $U-1$ moves from the SL policy network, then sampling one move \nuniformly at random from available moves, then sampling the remaining sequence of moves until the game terminates, \n$t = U + 1$ to $T$, from the RL policy network. Finally, the game was scored to determine the outcome.\nEvaluation\nEach program was given a max of 5s computation per move. The distributed AlphaGo was used against\nFan Hui and won 5 - 0 in formal matches.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://www.willamette.edu/~levenick/cs448/goNature.pdf"
    },
    "10": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/01/28/dueling-networks.html",
        "trancript": "Dueling Network Architectures for Deep Reinforcement Learning\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nDueling Network Architectures for Deep Reinforcement Learning\nJan 28, 2016\nWang, de Freitas, Lanctot, 2016\nSummary\nFor many states, it is unnecessary to estimate the action value for each action. This is a problem with methods that attempt to favor exploration over exploitation\ntoo much\n, because often times there will be a large number of actions that have little to no value for a given state.\nThe Q-Network in this novel architecture is decomposed into two separate streams; a value stream and an advantage stream. Feature learning is carried out by a number of convolutional and pooling layers. The activations of the last of these layers are sent to both separate streams. Each stream contains a number of fully-connected layers. The final layer combines the output of the two streams, and the output of the network is a set of Q values, one for each action.\nThe aggregator for the two outputs of the advantage and value streams is:\n$\\beta$ refers to the parameters specific to the value network, and the alpha refers to the parameters specific to the advantage network.\nThe advantage of the dueling network over standard Q-Networks is especially prominent when the number of actions is large. For standard Q-Networks, when the variation between actions is small, the Q-Network effectively has to learn the same value for all actions while each update only modifies the Q value of one action.\nEvidence\nThe dueling network architecture outperformed the Double-DQN results in 50 out of 57 learned Atari games.\nStrengths\nThe paper does a good job of making its main contribution (a novel neural network architecture) clear at the beginning.\nThe experimental algorithm for Dueling Networks employs other state-of-the-art advances in DRL (such as Double-DQN) which helps show the correlation between research being carried on in this field.\nInteresting related works\nIncreasing the action gap (Bellemare et al., 2016)\nNotes\nThe value function V measures the importance of being in a particular state s. The Q-function measures the importance about the value of choosing each possible action when in this state. The advantage function, $A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)$, subtracts the value of the state from the Q-function to obtain a relative measure of the importance of each action\nA deep Q-network is a non-linear function approximator for the Q function having the form $Q(s,a;\\theta)$ with parameters theta\nWe optimize the following sequence of loss functions:\nwith the target of the loss function, $y_i^{DQN}$, given by the reward signal plus the discounted maximal Q value provided by the target Q-Network\nA target Q-Network that uses parameter freezing for a certain number of iterations is used to stabilize the algorithm\nThe specific gradient is\nExperience replay is used; use prioritized experience replay instead! (Don\u2019t just sample uniformly from memory)\nTo avoid over-optimistic value estimates (van Hasselt, 2010), use Double Q-Learning. Originally, the max operator uses the same values to both select and evaluate an action. Instead, use the following target:\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1511.06581"
    },
    "11": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/01/26/prioritizing-experience-replay.html",
        "trancript": "Prioritized Experience Replay\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nPrioritized Experience Replay\nJan 26, 2016\nSchaul, Quan, Antonoglou, Silver, 2016\nSummary\nUniform sampling from replay memories is not an efficient way to learn. Rather, using a clever prioritization scheme to label the experiences in replay memory, learning can be carried out much faster and more effectively. However, certain biases are introduced by this non-uniform sampling; hence, weighted importance sampling must be employed in order to correct for this. It is shown through experimentation with the Atari Learning Environment that prioritized sampling with Double DQN significantly outperforms the previous state-of-the-art Atari results.\nEvidence\nImplemented Double DQN with main changes being the addition of prioritized experience replay sampling and importance-sampling\nTested on Atari Learning Environment\nStrengths\nLots of insight about the repercussions of this research and plenty of discussion on extensions\nNotes\nThe magnitude of the TD-error indicates how unexpected a certain transition was\nThe TD-error can be a poor estimate about the amount an agent can learn from a transition when rewards are noisy\nProblems with greedily selecting experiences:\nHigh-error transitions are replayed too frequently\nLow-error transitions are almost entirely ignored\nExpensive to update entire replay memory, so errors are only updated for transitions that are replayed\nLack of diversity leads to over-fitting\nA stochastic sampling method is introduced which finds a balance between greedy prioritization and random sampling (current method)\nTwo variants of \\(P(i) = \\frac{p^{\\alpha}_i}{\\sum_k p^{\\alpha}_k}\\) were studied, where $P$ is the probability of sampling transition $i$, $p_i > 0$ is the priority of transition $i$, and the exponent $\\alpha$ determines how much prioritization is used, with $\\alpha = 0$ the uniform case\nVariant 1: proportional prioritization, where $p_i = | \\delta_i| + \\epsilon$ is used and $\\epsilon$ is a small positive constant that prevents the edge-case of transitions not being revisited once their error is zero. $\\delta$ is the TD-error\nVariant 2: rank-based prioritization, with $p_i = \\frac{1}{rank(i)}$ where $rank(i)$ is the rank of transition $i$ when the replay memory is sorted according to $\\delta_i$\nKey insight\nThe estimation of the expected value of the total discounted reward with stochastic updates requires that the updates correspond to the same distribution as the expectation. Prioritized replay introduces a bias that changes this distribution uncontrollably. This can be corrected by using importance-sampling (IS) weights $ w_i = (\\frac{1}{N} \\frac{1}{P(i)})^{\\beta} $ that fully compensate for the non-uniform probabilities $P(i)$ if $\\beta = 1$. These weights are folded into the Q-learning update by using $w_i \\times \\delta_i$, which is normalized by $\\frac{1}{\\max_i w_i}$\nIS is annealed from $\\beta_0$ to 1, which means its affect is felt more strongly at the end of the stochastic process; this is because the unbiased nature of the updates in RL is most important near convergence\nIS also reduces the gradient magnitudes which is good for optimization; allows the algorithm to follow the curvature of highly non-linear optimization landscapes because the Taylor expansion (gradient descent) is constantly re-approximated\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/pdf/1511.05952.pdf"
    },
    "12": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/01/22/incentivizing-exploraton-in-rl.html",
        "trancript": "Incentivizing Exploration in Reinforcement Learning with Deep Predictive Models\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nIncentivizing Exploration in Reinforcement Learning with Deep Predictive Models\nJan 22, 2016\nBradly C. Stadie, Sergey Levine, Pieter Abbeel, 2015\nSummary\n\u201cOptimism in the face of uncertainty\u201d, the mantra of the Upper-Confidence Bound 1 algorithm, becomes impractical to follow when the action space is continuous. Hence, most approaches default to using epsilon-greedy exploration. This paper proposes a scalable and efficient method for assigning exploration bonuses in large RL problems with complex observations.\nA model of the task dynamics is learned to assess the novelty of a new state. As the ability to model the dynamics of a particular state-action pair improves, the \u201cunderstanding\u201d of the state is thus better and hence its novelty is lower. This circumvents the need to explicitly maintain visitation frequencies for states and state-action pairs in a table. When a state-action pair is not understood well enough to make accurate predictions, it is assumed that more knowledge is needed and hence a higher \u201cnovelty\u201d value is assigned to that reward signal.\nEvidence\nThis approach was evaluated on 14 games in the Arcade Learning Environment (ALE)\nThe reinforcement learning algorithm that was employed was DQN, and performance was evaluated against DQN with epsilon-greedy exploration, Boltzman exploration, and Thompson Sampling\nNot clear that this approach outperforms other state-of-the-art methods consistently\nStrengths\nThe paper references methods that were attempted but ultimately failed, such as learning a dynamics model that would predict raw frames (next states) for the Atari simulation\nWeaknesses\nNeed to see this method tested on other environments and scenarios\nInteresting related works\nThompson Sampling\nBoltzman exploration\nNotes\nPAC-MDP algorithms such as MBIE-EB and Bayesian algorithms such as Bayesian Exploration Bonuses manage the exploration versus exploitation tradeoff by assigning bonuses to novel states. (What are thooose). These sound similar to the UCB1 exploration strategy\nAn autoencoder was used to obtain the function sigma that encodes the state prediction model. The choice of autoencoder was for dimensionality reduction of the state space\n\u201cThe hidden layers are reduced in dimension until maximal compression occurs with 128 units\u201d\nAn MLP with 2 layers was used to predict model dynamics. The sixth layer of the auto-encoder produces the state with reduced dimensionality\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/abs/1507.00814"
    },
    "13": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/computer-vision/2018/12/26/LfD-in-the-wild.html",
        "trancript": "Learning From Demonstrations in the Wild\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nLearning From Demonstrations in the Wild\nDec 26, 2018\nBehbahani et al., 2018\nSummary\nThe motivation behind this work is to develop an automated process for learning the behaviors of road users from large amounts of unlabeled video data. A generative model (trained policy) of road user behavior could be used within a larger traffic scene understanding pipeline. In this paper, they propose Horizon GAIL, an imitation-learning algorithm based on GAIL, that stabilizes learning from demonstration (LfD) over long horizons. Expert policy demonstrations are provided by a slightly improved Deep SORT tracker, and they use PPO as the \u201cstudent\u201d RL algorithm. The Unity game engine is used to build an RL env that mimcs the scene from the real-world environment to rollout their PPO Horizon-GAIL agent. Their experiments are on 850 minutes of traffic camera data of a large roundabout. By using a curriculum where the episode horizon is extended by 1 timestep each training epoch, they demonstrated how Horizon-GAIL can match the expert policy\u2019s state/action distribution much more closely than GAIL, PS-GAIL, and behavior cloning while also improving on training stability.\nObservations\nThe ability to auto-generate the Unity env from Google Maps would be crucial to scaling this technique up. maps2sim?\nThey provided an empirical comparison of DeepSORT with ViBe\u2019s vision tracker, and showed that running the Kalman Filter in 3D space improved Deep SORT\u2019s performance in multiple multi-object tracking metrics by a few percentage points\nEach road user is modeled independently, i.e., the policy does not account for other agents in the environment explicitly. It looks like the policy used for learning vehicle and pedestrian behavior is the same, although because of the Mask R-CNN detector, they are able to differentiate between the two classes. In scenarios where the behaviors exhibited by the road users can be highly unpredictable and diverse (a busy traffic intersection with heavy pedestrian presence), perhaps a hierarchical policy could be useful that conditions on the inferred object class.\nInteresting future work might include incorporating multi-agent modeling in the RL framework for more complex traffic scenarios.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1811.03516v1"
    },
    "14": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/computer-vision/2018/06/03/tracking-occluded-objects-by-reasoning-about-containment.html",
        "trancript": "Tracking Occluded Objects and Recovering Incomplete Trajectories by Reasoning about Containment Relations and Human Actions\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nTracking Occluded Objects and Recovering Incomplete Trajectories by Reasoning about Containment Relations and Human Actions\nJun 3, 2018\nLiang, Zhu, Zhu, 2018\nSummary\nThis paper looks at tracking severely occluded objects in long video sequences.\nI like this passage:\nAlthough some recent work adopted deep neural networks\nto extract contexts for object detection and tracking, these\ndata-driven feedforward methods have well-known problems:\ni) They are black-box models that cannot be explained\nand only applicable with supervised training by fitting the\ntypical context of the object, thus difficult to generalize\nto new tasks. ii) Lacking explicit representation to handle\nocclusions, low resolution, and lighting variations\u2014there\nare millions of ways to occlude an object in a given image\n (Wang et al. 2017), making it impossible to have enough\ndata for training and testing such black box models. In this\npaper, we go beyond passive recognition by reasoning about\ntime-varying containment relations.\nThey look at\ncontainment relations\ninduced by human activity. A containment relation occurs when an object contains or holds another object, obscuring it from view. Contained objects have the same trajectories as the container.\nThey use an idea that I have thought about as well and think is powerful; rather than only relying on detections for next state hypotheses, they suggest alternative hypotheses based on occlusion reasoning. The other hypotheses come from containment relations and blocked relations; if an object is contained, it inherits the track state of its container. If an object is blocked, its track state is considered stationary (not always true!).\nThis tracking scenario is unique because they only consider human action as the source of state change (i.e., this approach doesn\u2019t apply to general ped or vehicle tracking).\nThey use state-of-the-art detection and activity recognition to carry out object tracking. They use the network flow approach for occlusion-aware data association in a sliding window. I think their algorithm has to decide between containment and blocked occlusion events via the activity recognition.\nThey note that their approach is limited by how good object detection is. I think an important direction to consider is when external forces other than people can act on the object. This requires a lot more complex modeling of what an occluded object might be doing. Also, we need better object detection!\nInteresting related works\nIn\nZhang, Li, and Nevatia 2008\n, they use an Explicit Occlusion Model (EOM) in the network flow data association model. Good potential baseline.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://pdfs.semanticscholar.org/4170/0dc1e60f5c8eaef409ef014f37c8b9e1b8cd.pdf"
    },
    "15": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/computer-vision/2017/05/14/DESIRE.html",
        "trancript": "DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nDESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents\nMay 14, 2017\nLee, et al., 2016\nSummary\nThis paper presents a framework for predicting how a scene containing multiple interacting agents will play out by leveraging a number of powerful techniques. DESIRE stands for Deep Stochastic IOC RNN Encoder-decoder framework.\nThe following are the biggest obstacles for successfully predicting future actions taken by interacting agents given a visual snapshot of the present:\nThe space of future states for each agent in a scene is hard to optimize over; even when taking the context of the scene into account, there could be many plausible outcomes that seem equally likely\nIn turn, this makes rolling out and scoring potential future trajectories computationally expensive, since it requires sampling a large number of trajectories. This isn\u2019t relevant for offline processing, but for real-time robotics applications, this is important\nThe multi-agent problem; how to infer interactions between multiple actors in a scene?\nTaking into account long-term prediction rewards, rather than just one-step prediction\nHow to define a multi-objective loss function that doesn\u2019t commit errors such as averaging over all future possibilities\nDESIRE attempts to address these challenges as follows:\nDiverse sample generation using a conditional Variational Autoencoder. This allows for differentiable efficient sampling of plausible futures\nRNN encoder-decoder neural architecture that allows for mapping trajectories represented by world coordinates in $\\mathbb{R}^2$ or $\\mathbb{R}^3$ to a high-dimensional distributed representation that can be efficiently combined with the Conditoinal VAE\nAn Inverse Optimal-Control (IOC) based Ranking and Refinement module that determines the most likely hypotheses, while incorporating scene context and interactions. The IOC module estimates a regression vector to refine each prediction sample.\nA convolutional neural network is used to carry out scene context fusion. This enables encoding features such as object velocities into the hypothesis scoring component.\nThe model is trained end-to-end with a total loss consisting of multiple auxiliary losses (e.g., reconstruction error from the output trajectory of the decoder during training). The authors evaluate its performance on the KITTI and Stanford Drone tracking datasets. They show the performance with the metric \u201cerror in meters\u201d for future time steps at intervals of 1, 2, 3, and 4 seconds.\nOverall, the proposed model seems quite complex, with many sub-components. However, the motivation behind each component is reasonable and the model is shown to perform well on the datasets. The separation of the system into a Sample Generation Module and the Ranking and Refinement Module is reminiscent of the actor-critic architecture in RL. The Ranking and Refinement Module uses unsupervised learning losses to learn to score/refine the \u201cactor\u201d, or Sample Generation Module.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.04394v1"
    },
    "16": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/computer-vision/2017/05/04/semantic-image-seg-with-deep-g-crfs.html",
        "trancript": "Fast, Exact, and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nFast, Exact, and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs\nMay 4, 2017\nChandra, Kokkinos, 2016\nSemantic Image Segmentation\nImage Example\nPut simply, the task is to cluster pixels in an image together and to assign all pixels in a cluster a meaningful label. Labels are generally \u201chigh-level\u201d concepts, such as \u201chorse\u201d, \u201cdog\u201d, \u201cboat\u201d, etc.\nHere is the VOC PASCAL 2012\ndataset used in this paper.\nDiscussion Q\u2019s\nWhy G-CRFs?\nJoint distributions are hard, so exploit factorizable (exponential-family) probability distributions and conditional independence. This results in a graph-like structure.\nIf the label assigned to each pixel is a random variable, then an conditional random field suggests that there\u2019s a dependence between a pixel\u2019s label and the label\u2019s of other nearby pixels. This seems intuitive.\nAuthor\u2019s argue that continuous Gaussian outputs are unimodal conditioned on the data, so given an image, one solution dominates the posterior. The log-likelihood is a quadratic, which allows the approximate inference task to be cast a quadratic optimization in an energy-minimization setting.\nDeep G-CRF architecture\nThe basic idea is to set the outputs of a convolutional neural network to be the energy of a segmentation hypothesis, in the form of \n\\(\\begin{equation}\nE(x) = \\frac{1}{2} x^{\\intercal}(A + \\lambda I) x - Bx\n\\end{equation}\\) \n. The network predicts what the $A$ (pairwise) and $B$ (unary) terms in the energy function are for an image. $\\lambda$ is set manually to enforce positive-definiteness. Then a Quadratic Optimization + softmax module gives the final per-class scores for each pixel in the image (See Fig. 1) by solving for $x$. Originally, $A$ is a $(P \\times L) \\times (P \\times L)$ matrix, where $L$ is the number of class labels and $P$ is number of pixels.\nShared pair-wise terms\n$A$ is no longer dependent on the number of class labels; only care about pixel interactions independent on what the label is. Now, the inference equation $(A + \\lambda I) x = B$ is reduced to a system of $L + 1$ equations for $A$ of dim $P \\times P$.\nConjugate Gradient method\nNeed to solve $x = A^{-1}b$. \nThe $A$ matrix is very sparse, since it only deals with 4, 8, or 12-connected neighborhoods. CG is the current recommended approach for solving $Ax = b$ when $A$ is sparse\nwhy?\nWhen $A$ is dense, it is recommended to factorize A and then use backsubstitution.\nsee here\nExperiments\nTry out the above + fusing information across 3 different resolutions.\nmetric - Intersection over Union\nbaselines - Multiresolution DeepLab-LargeFOV, CRF-RNN\nQO network - Baseline extended to have a binary and unary stream. QO module can be shared by all resolutions, or replicated three times for each scale\n75.5% mean IOU on VOC PASCAL 2012 for this approach. Without more details and tests of significance, hard to say whether this method is really more effective than prev SOTA. It seems to do about ~1-2% IOU better than the baselines. Also seems to be much faster.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.08358"
    },
    "17": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/computer-vision/2017/03/28/pixel-recursive-super-res.html",
        "trancript": "Pixel Recursive Super Resolution\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nPixel Recursive Super Resolution\nMar 28, 2017\nDahl, Norouzi, Shlens, 2017\nSummary\nThis paper proposes a novel neural architecture for solving the super resolution task for large factors of magnification. The problem is challenging, because the input images can be as low-res as 8 x 8; hence, there is a wide variety of high-res images that could correspond to this image. The author\u2019s neural architecture consists of an autoregressive PixelCNN network augmented with a conditioning Convolutional Neural Network; the autoregressive aspect is to allow the network to output pixels sequentially and thereby capture the conditional dependency between a pixel and its neighbors. This is in contrast to prior work that assume conditional independence between pixels, and use a MSE per-pixel loss for supervision.\nQuestions for discussion\nNot too familiar with this line of research; 32 x 32 images is still fairly \u201clow-res\u201d. Is this state-of-the-art?\nThe authors highlighted how the perceptual quality did not always correspond with negative log likelihood. Why might this be? NLL is equivalent to MLE, which is equivalent to minimizing KL divergence\u2026\nGeneral commentary\nDictionary-inspired methods that search a bank of pre-learned filters on images and selecting appropriate patches by an efficient hashing mechanism has comparable performance\nPixelCNN is a stochastic model that provides an explicit model for $ \\log p( y_i | x, y_{< i}) $. However, the auto-regressive distribution largely ignores the conditioning on the low-resoultion image without explicitly separating the network into two components, one being a \u201cconditioning\u201d network.\nHad to add an extra term to the loss, the cross-entropy between the conditioning network\u2019s predictions via a softmax over the\nK\npossible values that the\ni'th\noutput pixel can take and the ground truth labels\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1702.00783"
    },
    "18": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/computer-vision/2016/07/30/ResNets.html",
        "trancript": "Deep Residual Learning for Image Recognition\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nDeep Residual Learning for Image Recognition\nJul 30, 2016\nHe, et al., 2015\nSummary\nResidual Neural Networks (ResNets) is the current state-of-the-art Convolutional Neural Network architecture. The key difference between ResNets and\nother popular architectures is the use of \u201cshortcut-connections\u201d. Basically, it was determined that \u201cvery deep\u201d neural networks, or DNNs with a large number of stacked layers, \nwere exhibiting a degradation in accuracy not caused by overfitting. Residual Layers in a DNN attempt to learn a \u201cresidual\u201d mapping, which is the desired hidden mapping minus the input to the residual layer.\nThe input to the layer is then \u201cadded\u201d back in later; hence the name \u201cshortcut-connection\u201d. One of the motivations for doing this is that it is easy\nfor the network to learn to send the residual to 0 (and hence learn an identity mapping), which is useful if an identity mapping is optimal for the situation. Learning an \nidentity mapping is very difficult for a stack of nonlinear layers. These residual layers do not add any extra parameters.\nEvidence\nThe authors tested ResNets on ImageNet and CIFAR-10, and won first place on the ILSVRC 2015 classification task.\nStrengths\nThe idea is simple and effective. The material is presented clearly with lots of data to back it up.\nInteresting related works\nBatch Norm, Highway Networks\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/pdf/1512.03385v1.pdf"
    },
    "19": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/reinforcement-learning-theory/2018/06/22/rudder.html",
        "trancript": "RUDDER: Return Decomposition for Delayed Rewards\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nRUDDER: Return Decomposition for Delayed Rewards\nJun 22, 2018\nArjona-Medina, Gillhofer, et al., 2018\nIntro\nDelayed rewards is one of the fundamental challenges of reinforcement learning. This paper proposes an algorithm for converting an MDP with delayed rewards into a different MDP with equivalent optimal policies where the delayed reward is now converted into immediate rewards. They accomplish this with \u201creward redistribution\u201d\u2014a potential solution to the RL credit assignment problem.\nBasically, they argue that reward redistribution enables monte-carlo (MC) and temporal-difference (TD) methods to be applied under delayed rewards without high variance or biasing the Q-values. This is important, because TD methods are the most commonly used RL approach, and it is proven in this paper that with delayed rewards TD methods take time exponential in the length of the delay to remove the bias.\nSome details about RUDDER\nOne of the main obstacles that is addressed in Section 3 is proving that the new MDP with redistributed rewards has the same optimal policies as the original MDP. I found that this section (and the previous) were unecessarily hard to read, but here is my take on what the key points are:\nBecause the delayed reward MDP obeys the Markov property, the transformed MDP with equivalent optimal policies but immediate rewards needs to have an altered set of states. This altered set of states obey the Markov property for predicting the immediate rewards, but crucially, not for the delayed reward. This is accomplished by using differences between states $\\triangle (s, s\u2019)$ in the delayed reward MDP as the new set of states\nThe return of the delayed reward MDP at the end of an episode should be predicted by a function $g$ (the LSTM) that can be decomposed into a sum over the state, action pairs of the episode\nIt should hold that the partial sums that all together sum to $g$, $\\sum_{\\tau = 0}^{t} h(a_{\\tau}, \\triangle(s_{\\tau}, s_{\\tau+1}))$, equal the Q-values at time $t$ for the reward redistribution to be optimal\nThis approach requires strong exploration that can actually uncover the delayed reward. If a robot gets a +1 for doing the dishes correctly and +0 reward otherwise, it may\nnever\neven see the +1 in the first place!\nTo help with the above, they use a \u201clessons replay buffer\u201d to store episodes where the delayed reward was observed, to sample from when computing gradients for improved data efficiency\nAn LSTM is used for the return prediction, and techniques for credit assignment in deep learning like layer-wise relevance propagation and integrated gradients are used for \u201cbackwards analysis\u201d to accomplish the reward redistribution. This gets pretty complex, and the appendix goes into detail of how they modified the LSTM cell for this (something called a \u201cmonotonic LSTM\u201d)\nThey have to introduce other auxiliary tasks to encourage the reward prediction LSTM network to learn the optimal reward redistribution and get around the Markov property problem mentioned earlier. These aux tasks are to predict the q-values, predict the reward in the next 10 steps, and predict the reward accumulated from time 0 so far\nExperimental results\nThe authors note that RUDDER is aimed at RL problems with (i) delayed rewards (ii) no distractions due to other rewards or changing characteristics of the environment (iii) no skills to be learned to receive the delayed reward. (iii) seems to imply that RUDDER could be combined with hierarchical RL. (ii) suggests that when there are sporadic rewards throughout an episode, the improvements from RUDDER might be minimized. Hence, the environments they experiment with all have heavily delayed rewards observed only at the end of the episode, and their results are naturally impressive. I understand the difficulty (high computational/$$$ cost) in evaluating on all 50-something Atari games, but it would be really nice to see how it fares on other games that aren\u2019t perfectly set up for RUDDER.\nThey also note that human demonstrations could be used to fill the lessons buffer, which might further improve recent\nimpressive imitation learning results\n.\nGrid world\nThey compute an optimal policy for a simple grid world with a delayed reward MDP. Then, they compute the bias and variance from the mean square error (MSE) of the Q-values for MC and TD estimators with policy evaluation. They demonstrate the exponential dependency on the reward delay length for the MC and TD methods by varying the delay. RUDDER shows favorable reductions in the bias and variance of estimated vs. optimal Q-values at all delay lengths.\nCharge-discharge\nSimple 2-state and 2-action MDP with episodes of variable length and delayed reward obtained only at the end of the episode. RUDDER augmenting a Q-learning agent is much more data efficient than pure Q-learning, MC agent, and MCTS.\nAtari evaluation\nThe most impressive results of the paper are the scores for the Venture and Bowling Atari games. They set new SOTA on Venture.\nThey augment\nPPO\nwith the return prediction network paired with integrated gradients for backwards analysis. The lessons buffer is used to train both the return prediction network and A2C (for PPO). I\u2019m not sure about that because I thought PPO was an on-policy algorithm- using episodes from the lessons buffer would bias the policy gradient? They use the difference between two consecutive frames for $\\triangle(s, s\u2019)$ plus the current frame (for seeing static objects) as input.\nNN architectures for RUDDER augmenting PPO\nVisual depiction of the reward redistribution\nThey show that RUDDER is much more data efficient than APE-X, DDQN, Noisy-DQN, Rainbow, and other SOTA Deep RL approaches on these two games.\nVideo\n.\nClosing thoughts\nI think this is an exciting RL paper that will have a major impact. Mainly because I can see lots of future research refining the ideas from this paper and then applying it to a variety of domains, especially in robotics. This paper could use polishing (some parts are not easy to read) and some more experiments showing how it performs on MDPs with both delayed rewards and also some other \u201cdistractor\u201d rewards. I think this setting is most common, as it\u2019s usually possible to do a little reward hacking to augment sparse/delayed reward MDPs. In the appendix, they go into the engineering details on how they got this to work, and it seems like that process + how the hyperparameters/auxiliary tasks/choice of method for backwards analysis will need to change for different environments will need to get cleaned up before making this approach widely useful.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1806.07857v1"
    },
    "20": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/reinforcement-learning-theory/2018/03/01/horde.html",
        "trancript": "Horde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nHorde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction\nMar 1, 2018\nSutton, et al. 2011\nSummary\nA key idea pushed in this paper is that a value function represents semantic knowledge. Indeed, they state,\n\u201cA value function asks a question\u2013what will the cumulative reward be?\u2013and an approximate value function provides an answer to that question\u201d\n. Accordingly, they introduce\nGeneralized Value Functions\n(GVFs), a construct to expand the knowledge that value functions can encapsulate to make them capable of representing knowledge about the world.\nA GVF is parameterized with four functions, a policy, pseudo-reward function, pseudo-terminal reward function, and pseudo-termination function, called\nquestion functions\n.\nThey introduce\nHorde\n, an architecture for learning 1 or more approximate GVFs in parallel, where each \u201cdemon\u201d of the Horde is responsible for learning a piece of knowledge that contributes to the whole. Approximate GVFs can be learned off-policy.\nThe paper uses GQ($\\lambda$) to train each demon, and hence a feature vector $\\phi$, behavior policy $b$, and eligibilty trace function $\\lambda$ must be specified; these are collectively called\nanswer functions\n, since they are used to numerically find the value of approximate GVFs (answering the \u201cquestion\u201d).\nThey show that a physical robot with many sensors is able to learn to predict how many steps it can go before needing to stop before hitting a wall (via 1 \u201cpredictive\u201d demon, i.e., a demon that seeks to accurately \u201cpredict\u201d the cumulative return by learning the approximate GVF for a given policy). The robot also uses 8 control demons to learn to separately maximize returns for \u201cmaxing out\u201d 8 different sensors. Finally, they trained 1 control demon to learn a light-seeking behavior.\nRecently,\nBarreto, et. al 2017\ndeveloped the ideas of successor features (SF), a value function representation that decouples environment dynamics from the reward function. They use it to show transfer between tasks. They discuss how their method is a special case of a GVF, but that it provides a method for \u201cselecting\u201d pseudo-rewards.\nThis paper differs from the\noptions framework\nin that options essentially define a hierarchy of policy abstractions. The authors note that GVFs could be combined with this approach, however.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/horde1.pdf"
    },
    "21": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/reinforcement-learning-theory/2017/08/29/deep-symbolic-rl.html",
        "trancript": "Towards Deep Symbolic Reinforcement Learning\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nTowards Deep Symbolic Reinforcement Learning\nAug 29, 2017\nGarnelo, et al., 2017\nSummary\nIt is desirable for an agent to symbolically reason about its environment, in order to expediate the process of learning optimal behaviors. However, \u201cclassic\u201d symbolic AI suffers from the\nsymbol grounding problem\n; symbolic elements have traditionally been hand-crafted, and hence, are brittle. On the other hand, Deep Learning can be used to automatically learn ~optimal \u201csymbols\u201d, upon which a reinforcement learning agent could learn behaviors motivated by these learned symbols. By forcing a Deep RL agent to operate in a symbolic domain, the decisions made by the agent are naturally more interpretable.\nThe aspects of AI that this work focuses on are closely related to those proposed in the manifesto written by\nLake et al.\n. There are also nods to\nDouglas Hofstadter\u2019s\nwork on analogy as being the main driving force behind intelligence, as well as\nMarcus Hutter\u2019s\nUniversal Artificial Intelligence work.\nConceptual abstraction - agents can use abstractions to make analogies and hence learn optimal behaviors faster\nCompositional structure - probabilistic first-order logic provides the underlying framework for a representation medium that is compositional\nCommon sense priors - priors from simple object tracking! Persistence, kinematics (constant velocity models), etc.\nCausal reasoning - the proposed architecture attempts to discover causal structure in the environment to accelerate learning by explicitly maintaining sets of causal rules\nA Q-learning agent is designed for their toy example. The state space consists of the different interactions between the learned symbolic abstractions of the environment. A tracking process is carried out separately from the agent for keeping track of the different symbolic abstractions.\nMain result: on random toy problem instances, DQN is not able to better than chance\u2026 Hypothesis: DQN relies on the fact that you should be able to internally learn a model for $p(s_{t+1}|s_{t},a_{t})$ after going through a lot of examples. When this distribution is non-stationary, it can\u2019t! However, the proposed architecture doesn\u2019t seem to care about this. It instead directly is learning about object interactions.\nMethodology\nTheir environment is a simple B&W grid upon which shapes (O\u2019s, X\u2019s, and +\u2019s) can be randomly positioned.\nLow-level symbol generation: Uses a convolutional autoencoder to do simple dimensionality reduction and learn relevant features. Then, they do a form of unsupervised clustering by finding the maximally activated feature corresponding to each pixel, then thresholding these values. Once they have a sparse list of salient pixels, they form a spectrum with the activations across all features, and define a distance metric on the spectra via the sum of squared distances. This allows them to cluster pixels (objects) into certain categories.\nRepresentation building: notions of spatial proximity, type-transitions between frames (including birth and death), neighborhood (number of neighbors), relative distances and positions\nReinforcement Learning: Agent is the \u2018+\u2019, separate Q for interactions between different pairs of object types. State space describes different possible relations between two objects types. Seek to learn how to interact via (U, D, L, R) actions.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://pemami4911.github.io/paper-summaries/general-ai/2016/05/13/learning-to-think.html"
    },
    "22": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/reinforcement-learning-theory/2016/12/30/linear-least-squares-td.html",
        "trancript": "Linear Least-Squares Algorithms for Temporal Difference Learning\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nLinear Least-Squares Algorithms for Temporal Difference Learning\nDec 30, 2016\nBradtke, Barto, 1996\nSummary\nA Brief Description of TD($\\lambda$)\nIn TD($\\lambda$), a parameterized linear function approximator is used to represent the value function. \nThe parameter update for vector $\\theta_{t}$ is:\n\\[\\begin{equation}\n\n\\theta_{t+1} = \\theta_{t} + \\alpha_{\\eta(x_{t})} \\big [ R_{t} + \\gamma V_{t}(x_{t+1} - V_{t} \\big ] \\sum_{k=1}^{t} \\lambda^{t-k} \\nabla_{\\theta_{t}} V_{t}(x_k)\n\n\\end{equation}\\]\n$\\alpha_{\\eta(x_{t})}$ is the step-size parameter, and $\\eta(x_{t})$ is the number of transitions from state $x_{t}$ up to time step $t$.\nIt is important that $V_{t}$ be linear in the parameter vector $\\theta_{t}$- and if $\\lambda \\neq 0$- $\\nabla_{\\theta_{t}} V_{t}(x_k)$ does not depend on $\\theta_{t}$. This allows for an efficient, recursive algorithm to be derived to compute the sum at the end of Equation (1).\nEpisodic TD-Lambda algorithm. The parameter vector is updated only at the end of the episode\nDaya and Sejnowski (1994) proved parameter convergence with probability 1 under these conditions for TD($\\lambda$) applied to absorbing Markov chains in a episodic-setting (i.e., offline).\nBradtke (1994) introduced a normalized version of TD(0) (called NTD(0)) to solve instabilities in the learning algorithm caused by the size of the input vectors $\\phi_{x}$\nLeast-Squares Temporal-Difference Learning\nThe general problem consists of a system with inputs $\\hat \\omega_{t} = \\omega_{t} + \\zeta_{t}$ where $\\zeta_{t}$ is the input observation noise at time $t$. For a linear function approximator $\\Psi$, we have\n\\[\\begin{align}\n\n\\psi_{t} &= \\Psi(\\hat \\omega_{t} - \\zeta_{t}) + \\eta_{t} \\nonumber \\\\\n         &= \\hat \\omega_{t}^{\\intercal} \\theta^{*} - \\zeta_{t}^{\\intercal} \\theta^{*} + \\eta_{t}\n\n\\end{align}\\]\nThe quadratic objective function to minimize over all parameter vectors is\n\\[\\begin{equation}\n\nJ_{t} = \\frac{1}{t} \\sum_{k=1}^{t} [ \\psi_{k} - \\omega_{k}^{\\intercal} \\theta_{t} ]^{2}.\n\n\\end{equation}\\]\nand the $t^{th}$ estimate of $\\theta^{*}$ is\n\\[\\begin{equation}\n\\DeclareMathOperator*{\\argmin}{\\arg\\!\\min}\n\n\\hat \\theta^{*}_{t} = \\argmin_{\\theta_{t}} J_{t} \n\\end{equation}\\]\nWe introduce an\ninstrumental variable\n$\\rho_{t}$, which is a vector that is correlated with the true input vectors, $\\omega_{t}$, but that is uncorrelated with the observation noise, $\\zeta_{t}$. Solving the linear-least squares equation by taking the gradient of the quadratic objective function and setting it to 0 results in this parameter estimate:\n\\[\\begin{equation}\n\n\\hat \\theta^{*}_{t} = \\bigg [ \\frac{1}{t} \\sum_{k=1}^{t} \\rho_{k}\\hat\\omega_{k}^{\\intercal} \\bigg ]^{-1} \\bigg [ \\frac{1}{t} \\sum_{k=1}^{t} \\rho_{k} \\psi_{k} \\bigg].\n\n\\end{equation}\\]\nThe correlation matrix $Cor(\\rho, \\omega)$ must be nonsingular and finite, the correlation matrix $Cor(\\rho, \\zeta)$ = 0, and the output observation noise $\\eta_{t}$ must be uncorrelated with the instrumental variable for $\\theta_{t}$ to converge with probability 1 to $\\theta^{*}$\nWe arrive at the LSTD algorithm based on the following assumptions (Lemma 4 in the paper)\nIf x and y are states s.t. $P(x,y) \\gt 0$\n$\\zeta_{xy} = \\gamma \\sum_{z \\in X} P(x,z) \\phi_{z} - \\gamma \\phi_{y}$\n$\\eta_{xy} = R(x,y) - \\bar r_{x}$\n$\\rho_{x} = \\phi_{x}$\nThen $Cor(\\rho, \\eta) = 0$ and $Cor(\\rho, \\zeta) = 0$.\nIn other words, we can use $\\phi_{x}$ as the instrumental variable to re-write Equation (5) as:\n\\[\\begin{equation}\n\n\\hat \\theta^{*}_{t} = \\big[ \\frac{1}{t} \\sum_{k=1}^{t} \\phi_{k} (\\phi_{k} - \\gamma \\phi_{k+1})^{\\intercal}\\big]^{-1} \\big [ \\frac{1}{t} \\sum_{k=1}^{t} \\phi_{k} r_{k} \\big]\n\n\\end{equation}\\]\nwhere $\\hat\\omega_{k} = \\phi_{k} - \\gamma \\phi_{k+1}$ and $\\psi_{k} = r_{k}$.\nUnder some non-too-restrictive conditions, the authors provided proofs of convergence of LSTD on absorbing and ergodic Markov Chains.\nMainly, for absorbing MCs, we need\nThe starting probabilities are such that there are no inaccessible states\n$R(x,y) = 0$ whenever $x,y \\in T$, the set of absorbing states\nThe set of feature vectors {$\\phi_{x} | x \\in X$} representing non-absorbing states must be linearly independent\n$\\phi_{x} = 0$ for all $x \\in T$\nEach $\\phi_{x}$ is of dimension $N = |X|$\n$0 \\leq \\gamma \\lt 1$\nThen $\\theta^{*}$ is finite and the LSTD algorithm converges to it with probability 1 as the number of trials (and state transitions) approaches infinity.\nThere are even less restrictions for convergence on ergodic MCs. The proof of convergence for absorbing MCs is straight-forward.\nConvergence of LSTD on absorving Markov Chains\nAs $t$ grows, the sampled transitions approaches the true transition probability distribution $P$. We also have that each state is visited with probability $\\pi_{x}$ (both with probability 1).\n$$\n\\begin{align}\n\n\\theta_{LSTD} &= \\lim_{t\\to\\infty}\\theta_{t} \\nonumber \\\\\n\n&= \\lim_{t\\to\\infty} \\big[ \\frac{1}{t} \\sum_{k=1}^{t} \\phi_{k} (\\phi_{k} - \\gamma \\phi_{k+1})^{\\intercal}\\big]^{-1} \\big [ \\frac{1}{t} \\sum_{k=1}^{t} \\phi_{k} r_{k} \\big] \\nonumber \\\\\n\n&= \\bigg[ \\lim_{t\\to\\infty} \\frac{1}{t} \\sum_{k=1}^{t} \\phi_{k} (\\phi_{k} - \\gamma \\phi_{k+1})^{\\intercal} \\bigg]^{-1} \\bigg [ \\lim_{t\\to\\infty} \\frac{1}{t} \\sum_{k=1}^{t} \\phi_{k} r_{k} \\bigg] \\nonumber \\\\\n\n&= \\bigg[ \\sum_{x} \\pi_{x} \\sum_{y} P(x,y) \\phi_{x} (\\phi_{x} - \\gamma \\phi_{y})^{\\intercal} \\bigg ]^{-1} \\bigg [ \\sum_{x} \\pi_{x} \\phi_{x} \\sum_{y} P(x,y) R(x,y) \\bigg] \\nonumber \\\\\n\n&= \\bigg[ \\sum_{x} \\pi_{x} \\sum_{y} P(x,y) \\phi_{x} (\\phi_{x} - \\gamma \\phi_{y})^{\\intercal} \\bigg ]^{-1} \\bigg [ \\sum_{x} \\pi_{x} \\bar R(x,y) \\phi_{x} \\bigg] \\nonumber \\\\\n\n&= \\big [ \\Phi^{\\intercal} \\Pi (I - \\gamma P) \\Phi \\big ]^{-1} \\big [ \\Phi^{\\intercal} \\Pi \\bar R \\big ]\n\n\\end{align}\n$$\n\n$\\Phi$ is a matrix where each row is $\\phi_{x}$, and $\\Pi$ is the matrix diag($\\pi$), $\\pi$ being the proportion of time spent in each state in the limit.\n\nWe know the inverse of $\\big [ \\Phi^{\\intercal} \\Pi (I - \\gamma P) \\Phi \\big ]$ exists since each of $\\Phi$, $\\Pi$, and $(I - \\gamma P)$ are full rank by our assumptions.\n\nHence,  \n\n$$\n\\begin{align}\n\n\\bar R_{x} &= V(x) - \\gamma \\sum_{y \\in X} P(x,y) V(y) \\nonumber \\\\ \n\n&= \\phi_{x}^{\\intercal}\\theta^{*} - \\gamma \\sum_{y \\in X} P(x,y) \\phi_{y}^{\\intercal}\\theta^{*} \\nonumber \\\\\n\n&= (\\phi_{x}^{\\intercal} - \\gamma \\sum_{y \\in X} P(x,y) \\phi_{y}^{\\intercal})\\theta^{*} \\nonumber \\\\\n\n&= (I - \\gamma P) \\Phi \\theta^{*}\n\n\\end{align}\n$$\n\nSubstituting this into Equation (7) we have \n\n$$\n\\begin{align}\n\n\\theta_{LSTD} &= \\big [ \\Phi^{\\intercal} \\Pi (I - \\gamma P) \\Phi \\big ]^{-1} \\big [ \\Phi^{\\intercal} \\Pi (I - \\gamma P) \\Phi \\big ] \\theta^{*} \\nonumber \\\\\n\n&= \\theta^{*}\n\n\\end{align}\n$$\nThus, $\\theta_{LSTD}$ converges to $\\theta^{*}$ with probability 1.\nThe complexity of LSTD is $O(m^{3})$, where $m$ is the state dimension, since it requires a matrix inversion at every step.\nThe authors present Recursive LSTD, which is an $O(m^{2})$ version of the LSTD algorithm. It amounts to the TD(0) learning rule, except the scalar step-size parameter is replaced by a gain matrix. See Section 5.4 for the details.\nNote that when $rank(\\Phi) = n \\lt m$, TD($\\lambda$), NTD($\\lambda$), and RLSTD converge to some $\\theta$ depending on the order that states are visited in. LSTD does not converge since the matrix inversion can no longer be computed.\nTakeaways\nRLSTD (and LSTD algorithms in general) extract more information from each episode. As shown in this paper, even on simple randomly generated Markov Chains, RLSTD produces parameter estimates with significantly lower variance and faster convergence rates than TD($\\lambda$) and NTD($\\lambda$). Also, the LSTD algorithms don\u2019t have tricky hyperparameters to tune, whereas the TD($\\lambda$) algorithms do.\nIn the experiments in this paper, RLSTD was not sensitive to the initial guess of $\\theta_{t}$. It is unclear whether this is always true. On the other hand, TD($\\lambda$) is sensitive to the initial guess (obviously, bad guesses = bad performance).\nStandard LSTD algorithms require linear function approximators and fixed size feature vectors. These are usually not realistic for high-dimensional spaces and value functions on those spaces. The value function may be nonlinear, and obtaining a feature vector may be difficult or impossible. There is a major need for TD algorithms with nonlinear function approximators and automatic feature extraction that explicitly attempt to keep the variance of the TD-error small.\nSome future reading I need to do is on kernel-based LSTD algorithms, that replace the fixed-size feature vector with a kernel.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1016&context=cs_faculty_pubs"
    },
    "23": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/reinforcement-learning-theory/2016/12/13/memory-transformations.html",
        "trancript": "Memory Transformations Enhances Reinforcement Learning in Dynamic Environments\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nMemory Transformations Enhances Reinforcement Learning in Dynamic Environments\nDec 13, 2016\nSantoro, Frankland, Richards, 2016\nSummary\nThe research question the authors answered was whether by shifting from an episodic to a \u201cschematic\u201d, or gist-like, memory system, a reinforcement learning agent could learn to achieve its goals in a dynamic environment. The authors focused on 2D navigation tasks where the reward locations constantly changed, such that new reward locations were correlated in the short-term but where independent and sampled from a stable distribution in the long-term. I found it interesting that the authors claimed the real world is like this, and consequentially they staked a lot of the significance of their work on this fact.\nThe main conclusion they came to was that given the existence of a stable long-term distribution for reward location (or whatever random variable the agent is concerned with estimating a distribution for), the optimal strategy for an agent is to shift from utilizing episodic to schematic memories slowly.\nThe authors implemented their agent using a novel neural network architecture that consisted of, in general, an episodic memory system, a schematic memory system, a critic to generate a TD-error.\nThe episodic memory system was:\na spatial encoder which took in the (x,y)-pair of the current location of the agent,\nan autoencoder implemented as a 3-layer recurrent network\na network of place field units\nThe output of the spatial encoder fed into the autoencoder, and the output of this fed into the place cells. \u201cRetrieving\u201d memory from the place cells was implemented as a fully-connected sigmoid layer.\nThe use of place field units was quite interesting; the idea behind this was to learn to associate activation patterns of place cells with specific locations within the environment where rewards were recently found.\nThe schematic memory was implemented as a Restricted Boltzman Machine. The first layer was a direct projection of the place cells from the episodic network. The ultimate goal of the RBM was to learn a general statistical model of the reward locations. It was trained in an offline manner (i.e., while the agent was \u201cat rest\u201d between trials) by using random activity in the spatial encoder, and propagating that through to the RBM. This was curious, but apparently since they also had added a TD-prediction error to the episodic system via a critic, this was more biologically plausible than iid sampling from the episodic memory.\nThe agent has a parameter that controls how much it mixes its episodic and schematic memories; the resultant \u201cmixed\u201d memory then influences action-selection.\nFuture Directions\nHow would this compare with an LSTM- literally, a Long Short-Term Memory neural network? Can an LSTM learn to adapt to environments with with both short-term and long-term statistical patterns like this?\nWe\u2019re seeing a shift towards more complex RL environments that this could be applied to; for example, 3D navigation tasks where there are multiple goals that could potentially move over time.\nPerhaps this could also be applied to modeling of the dynamic behavior of other agents in a multi-agent setting?\nCool use of unsupervised learning to enhance RL!\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://www.jneurosci.org/content/36/48/12228.full"
    },
    "24": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/reinforcement-learning-theory/2016/09/16/optimal-control-of-pomdp.html",
        "trancript": "The Optimal Control of Partially Observable Markov Processes Over a Finite Horizon\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nThe Optimal Control of Partially Observable Markov Processes Over a Finite Horizon\nSep 16, 2016\nSondik, et al., 1973\nSummary\nThis is the paper that revolutionized Partially Observable Markov Decision Processes (POMDP) research, coming out of Stanford and Xerox Palo Alto Research Center.\nThey proved that, if there is a finite horizon for the control problem, the optimal value function is piecewise-linear and convex.\nI will present the proof and discuss possible connections with future work in Deep Reinforcement Learning.\nAssumptions\nThe underlying Markov process is a discrete-time, finite-state process.\nThe number of possible outputs at each observation is finite.\nThe reward output at each time step is not directly observed by the agent, but the extension to full observability is straightforward.\nThe time horizon $T$ is finite.\nNotation\n$a \\in A(n) := $ the set of actions available at time $n$\n$p^{a}_{ij} := $ the state transition probability of moving from $i$ to $j$ when taking action $a$\n$r_{j \\theta}^{a} := $ the probability of observing output $\\theta$ if the new internal state of the process is $j$ under action $a$\n$w_{ij\\theta}^{a} := $ the immediate reward accrued if the process transitions from states $i$ to $j$ under $a$ and observes $\\theta$\n$\\pi = [ \\pi_{1} \\pi_{2} \u2026 \\pi_{n} ] := $ where $\\pi_{i}$ is the probability that the current internal state of the system is $i$. $\\pi$ is itself a discrete-time continuous-state Markov process. From a Bayesian perspective, $\\pi$ is analagous to the current belief of the true hidden state.\nSome POMDP Ramp-Up\nIf $\\pi_{j}^{\\prime}$ is the updated belief that the internal state is $j$, then it can be computed as\n\\[\\begin{equation}\n\n\\pi_{j}^{\\prime} = \\frac{\\sum_{i} \\pi_{i} p_{ij}^{a} r_{j \\theta}^{a}} {\\sum_{i,j}  \\pi_{i} p_{ij}^{a} r_{j \\theta}^{a}} .\n\\end{equation}\\]\nThis is the belief update, which follows simply by applying Bayes rule. The key thing to note is that $\\pi$ is a sufficient statistic for computing $\\pi^{\\prime}$ (i.e., all of the information necessary to compute $\\pi^{\\prime}$ is contained in $\\pi$). We can refer to (1) as\n\\[\\begin{equation}\n\\pi^{\\prime} = T(\\pi | a, \\theta).\n\\end{equation}\\]\nIf the expected immediate reward during the next time step starting in state $i$ under action $a$ is defined as\n\\[\\begin{equation}\nq_{i}^{a} = \\sum_{j, \\theta} p^{a}_{ij} r_{j \\theta}^{a} w_{ij\\theta}^{a},\n\\end{equation}\\]\nthen the value function can be defined recursively as\n\\[\\begin{equation}\nV_{n}(\\pi) = \\max_{a \\in A(n)} \\big [ \\sum_{i} \\pi_{i} q_{i}^{a} + \\sum_{i,j,\\theta} \\pi_{i}  p^{a}_{ij} r_{j \\theta}^{a} V_{n-1} [ T(\\pi|a,\\theta)] \\big]\n\\end{equation}\\]\nand is computed in a forward-backwards approach, as is common with Dynamic Programming algorithms. This equation can be interpreted as the maximum\nexpected\nreward that the system can accrue until the end of the current process if the current belief state is $\\pi$ and there are $n$ time steps remaining before the process terminates. Notice that by linearity of expectation, the value function is broken down into the sum of the expected immediate reward over the belief state (LHS of the sum), and the expected future utility over the belief state (RHS of the sum).\nSince this is only valid for $ n \\ge 1$, we can say that\n\\[\\begin{equation}\nV_{0}(\\pi) = \\sum_{i} \\pi_{i} q_{i}^{0} = \\pi \\cdot q^{0}.\n\\end{equation}\\]\nWhere $q_{i}^{0}$ is the expected value of terminating the process in state $i$.\nProof that $V_{n}(\\pi)$ is Piecewise Linear and Convex\nWe want to show the following:\n\\[\\begin{equation}\n V_{n}(\\pi) = \\max_{k} \\big [ \\sum_{i = 1}^{N} \\alpha_{i}^{k} (n) \\pi_{i} \\big ]\n\\end{equation}\\]\nfor some set of vectors $\\alpha^{k}(n) = [\\alpha_{1}^{k}(n), \\alpha_{2}^{k}(n), \\ldots, \\alpha_{N}^{k}(n)], k = 1, 2, \\ldots$. To make the notation clear, note that a single $\\alpha$-vector has $N$ components (one for each of the possible states the process could be in), and the $\\alpha$-vector has a unique index $k$ referring to the region of the belief space that it partitions (more on this later). The index $k$ effectively enumerates the vectors as well.\nWe proceed by induction. (5) demonstrates that $V_{n}(\\pi)$ has the desired form for $n=0$. Proceeding with the inductive hypothesis that $V_{n-1}(\\pi)$ is of the form in (6), we shall show that this implies $V_{n}(\\pi)$ is of the same form.\nLet\u2019s substitute (1), the belief update, into the term $V_{n-1} [ T(\\pi|a,\\theta)]$ from (4) such that\n\\[\\begin{equation}\n\nV_{n-1}[ T(\\pi|a,\\theta)] = \\max_{k} \\big [ \\sum_{j} \\alpha_{j}^{k} (n - 1) \\frac{ \\sum_{i} \\pi_{i} p_{ij}^{a} r_{j \\theta}^{a}} {\\sum_{i,j}  \\pi_{i} p_{ij}^{a} r_{j \\theta}^{a}} \\big ]\n\n\\end{equation}\\]\nIt is important to see that $V_{n-1}( \\cdot )$ is piecewise linear and convex, and that the belief\nsimplex\ncan be divided into a finite set of convex regions separated by linear hyperplanes such that $V_{n-1}( \\pi ) = \\pi \\cdot \\alpha^{k} (n - 1)$ within a region for a single index $k$. Let\u2019s define $l(\\pi, a, \\theta)$ as the corresponding $\\alpha$-vector index $k$ for the region in the belief simplex containing $T(\\pi|a,\\theta)$, the transformed belief state.\nThe belief-state update $T(\\pi|a,\\theta)$ for a simple POMDP, from Sondik, et al.\nNotice in this image how the belief simplex, represented as a triangle where each vertex is one of the possible states, is partitioned into 4 regions by the set of $\\alpha$-vectors at time-step $n - 1$. There are two possible measurements which transforms the previous belief $\\pi$ to two possible new locations on the belief simplex.\nUsing $l(\\pi, a, \\theta)$ and (7), we can substitute into (4) to yield\n\\[\\begin{equation}\n V_{n}(\\pi) = \\max_{a \\in A(n)} \\big [ \\sum_{i} \\pi_{i} [q_{i}^{a} + \\sum_{\\theta, j} p_{ij}^{a} r_{j \\theta}^{a} \\alpha_{j}^{l(\\pi, a, \\theta)} (n - 1) ] \\big ].\n\n\\end{equation}\\]\nTo demonstrate (8) is piecewise linear and convex, we simply need to show that the bracketed quantity is piecewise linear and convex, since a maximum of a set of piecewise linear convex functions is itself piecewise linear and convex. First, note that for each $a$ and $\\theta$, $l(\\pi, a, \\theta)$ is a finitely valued function of $\\pi$. This, plus the fact that $V_{n-1}( \\cdot )$ is assumed to be convex and the continuity of $T(\\pi|a,\\theta)$, implies that $l(\\pi, a, \\theta)$ partitions the belief simplex into a finite number of regions such that $l$ is constant in that region, as in the figure above. Holding $a$ constant in $l(\\pi, a, \\theta)$, we can take the union of all different partitions defined by varying $\\theta$ to create a new region; the inner bracketed quantity is then constant over each of the partitions within this region; we are taking the expectation over all possible measurements, essentially. The result is that the outer bracketed quantity in (8) is piecewise linear over the belief simplex, with each different action $a$ contributing $K^{M}$ linear functions. $K$ is the number of $\\alpha$-vectors at time step $n-1$ and and $M$ is the number of different possible measurements. While every $\\alpha^{l(\\pi, a, \\theta) = k}(n - 1)$, $k = 1, 2, \\ldots$ contributes new linear functions to (8), the majority of these regions (indexed by $k$) are\ndominated\nby the subset of vectors that are maximal. This will be important later on. By taking a maximum over $k$ in (7), we also ensure the convexity of the outer bracketed quantity in (8). Therefore, (8) is in the desired form of (6), and the proof is complete.\nDiscussion\nUnsurprisingly, no one actually directly uses the above algorithm to do Value-Iteration to solve Reinforcement Learning problems; the time and space complexity of computing the value fuction by finding each $\\alpha$-vector is mind-boggling. Starting from way back, most POMDP solvers would regularly\nprune\nthe set of $\\alpha$-vectors so that only ones which made up the maximal set (and were not dominated) were kept. By dominated, I simply mean that the hyperplanes corresponding to the dominated $\\alpha$-vectors produce utility values for any given belief that are strictly less than the maximal set of $\\alpha$-vectors.\nHowever, over the years, many improvements to this approach have been made. Chiefly, the idea of\nPoint-Based Value Iteration\n(PBVI) formalized the intuitions that the set of reachable belief-states for an agent is much smaller than the actual size of the belief simplex. In fact, for most problems, huge subspaces of the belief space are never reached. Many popular POMDP solvers that compute approximations to the optimal value function came from this idea, (e.g.,\nSARSOP\n). From there, scaling up POMDPs to larger finite-world problems by using a tree-search approach to explore the belief simplex led to the complete abandonment of explicitly computing $\\alpha$-vectors. Rather, many of these solvers opted to use Q-learning to avoid the value-function entirely, favoring the fast convergence times for bigger problems (and disregarding the deterioration in quality of solutions). There have been other approaches as well that fall into the camp of policy-iteration methods that perform well on certain problems.\nApplication to Deep Reinforcement Learning\nAlgorithms like DQN that assume the state is fully observable tend to work well when the state really is fully observable. Unsurprisingly, when the true state of the system is partially observable, the performance degrades significantly. When you apply DQN to environments where states are partially observable, it\u2019s like you\u2019re using a representative set of particles for modeling your current belief state distribution, as in PBVI, except the particle set size is fixed at 1.\nMany people studying RL will agree that there is a long way to go until our agents can exhibit high-level cognitive functionality. The common consensus is that the most difficult tasks are going to be partially observable, and will require some form of memory and clever exploration motivated by intrinsic desires to handle sparse rewards.\nSo, is there any way to utilize our deep learning arsenal with traditional POMDP methods, which provide guarantees on convergence to the optimal value-function in partially observable environments?\nFor example, one could imagine reducing the dimensionality of the belief simplex sufficiently while simultaneously encoding the important information into some clever representation. Perhaps this could be done with a spatio-temporal approach in the general strain of\nOh, et al., 2015\n,\nPCA\n, or an autoencoder. Another key component would be a learned model that could predict the corresponding set of $\\alpha$-vectors for each action at the current time step; then, you could construct your value-function out of your predicted hyperplanes and encoded belief state. There would be all kinds of interesting geometry to exploit, given that the probability of being in many of the hidden states should be close to 0, which would collapse various dimensions of most of the hyperplanes. Additionally, you will want to learn a sparse set of linear hyperplanes, sort of how SVMs produce a sparse set of support vectors, since the only ones you care about are the maximal, dominating ones. The advantages of taking this route are abundant, one being that we would be able to get rid of many of the false assumptions that must be made when approximating POMDPs as MDPs, while at the same time preserving the scalability that deep learning has brought to RL.\nChallenges\nOne of the difficulties of this approach is that you would need to come up with a way to approximate the belief-update distribution. By using this distribution, that implies that this approach is inherently model-based as opposed to model-free, which appears to be another direction that some RL-practitioners seem to be gravitating back to.\nAnother difficulty will be the representation of the belief state. Generally, a representative set of sampled particles that can be filtered through the system dynamics via Monte-Carlo methods is used. This would be good to avoid, since this adds unwieldy computational costs.\nNote: There is some prior work on learning $\\alpha$-vectors to obtain an approximation of the value function. Chiefly, a set of basis functions is supplied such that the $\\alpha$-vectors can be factored into a linear combination of the basis functions and some weights.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/pdf/1507.08750v2.pdf"
    },
    "25": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/reinforcement-learning-theory/2016/08/29/intrinsically-motivated-rl.html",
        "trancript": "Intrinsically Motivated Reinforcement Learning\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nIntrinsically Motivated Reinforcement Learning\nAug 29, 2016\nSingh, et al., 2004\nSummary\nIn the search for more general AI agents, we must eventually abandon the practice of handcrafting reward functions in RL. As of right now, state-of-the-art RL agents still require a \u201cgood\u201d reward function that helps the agent learn complex behaviors and, by extension, an approximation of the optimal policy. For example, consider the popular\npendulum\ntask. The goal of the agent is to swing up and balance the pendulum. If the reward function is simply a +1 for having balanced the pendulum at the end of the episode (achieved the goal), or a -1 if the agent failed to balance the pendulum (failed to achieve the goal), the agent would never learn anything. With positive rewards arriving so incredibly infrequently, the agent would not have any motivation to explore different swing up behaviors, since every action it tries would seem equally bad. Hence, a more complex reward function is necessary that encourages the agent to develop intermediate \u201csub-behaviors\u201d which allow it to achieve its goal.\nOne solution to this problem as presented by the authors is focusing on the acquisition of\nskills\n, or\noptions\n, that provide the agent with the ability to use composition to carry out heirarchical planning-tasks.\nThe authors claim that agents should have a sophisticated internal motivational system that should not have to be redesigned for different problems.\nOptions are closed-loop \u201cmini\u201d-policies for taking action over a period of time. The authors present a learning framework based on\nsemi-Markov Decision Processes\n, which are used for adding temporal abstractions to RL. This framework utilizes the saliency of certain events that occur in the environment to generate intrinsic reward signals that stimulate curiosity within the agent. Eventually, the agent will \u201close interest\u201d in the event, as it loses its novelty (a.k.a. boredom), but retain knowledge of the interaction. Extrinsic reward signals are present and are generated by accomplishing goals. The authors demonstrated their framework on a small world experiment, where an agent in a grid-world was able to interact with a few objects. Some of the options it could learn involved turning a light switch on and off, kicking a ball, or making a bell ring.\nNotes\nThe policies of many options are updated simultaneously during an agent\u2019s interaction with the environment. If an option\ncould have\nproduced the current action in the current state, its policy can be updated as well.\nIn general, options have to be provided by the system designer. The state that could lead to the execution of an option, the option\u2019s terminating condition, and the reward function that evaluates the option\u2019s performance is required. It is desirable to automate the discovery of new options.\nThis paper is a good starting point for looking into the area of RL that deals with the reward-function problem. The concept of developing a motivational system that is shared across tasks is one that has not really been explored/is not prevalent today.\nGoing back to my example of the pendulum task- an RL agent would require an internal motivational system that\nunderstood physics\nin order to explore the environment effectively and receive a reward from the salient event of balancing the pendulum. If the agent could understand from prior experiences what\nbalancing\nmeans (i.e., the agent utilizes a learned model of physics to generate some type of understanding of the physical properties of an object in a\nbalanced\nstate), then the agent could motivate itself to select specific sequences of actions that bring the pendulum closer to a\nbalanced\nstate. Therefore, the system designer could use a much simpler and less informative reward signal without having to do almost any hand-crafting.\nAn interesting experiment to test this would be to train an RL agent on a number of different tasks that involve balancing, and then to use transfer learning (sharing network weights?) to have the agent solve the pendulum task.\nOr, to use a form of (differentiable) memory to store an approximately optimal set of sequences of actions related to balancing objects on a number of tasks. The idea is to capture the learned \u201cskills\u201d and transfer them to new tasks. The agent could use these experiences to accelerate learning on a novel balancing task. If the memory is associative, you could also store multiple physical skills beyond just balancing. From a practical standpoint, the prior experience would need to influence the Q-values of a given state and action through some sort of \u201cbonus\u201d.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2005_724.pdf"
    },
    "26": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/reinforcement-learning-theory/2016/08/11/Coop-Inverse-RL.html",
        "trancript": "Cooperative Inverse Reinforcement Learning\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nCooperative Inverse Reinforcement Learning\nAug 11, 2016\nHadfield-Menell, et al., 2016\nSummary\nIn the future, AI and people will work together, and hence we must concern ourselves with ensuring that the AI will have interests aligned with our own. \nThe authors suggest that it is in our best interests to find a solution to the \u201cvalue-alignment problem\u201d. As recently pointed out by Ian Goodfellow,\nthis may not always be a good idea\n.\nCooperative Inverse Reinforcement Learning (CIRL) is a formulation of a cooperative, partial information game between a human and a robot. Both share a reward \nfunction, but the robot does not initially know what it is. One of the key departures from classical Inverse Reinforcement Learning\nis that the \u201cteacher\u201d, which in this case is the human, is not assumed to act optimally. Rather, it is shown that sub-optimal actions\non the part of the human can result in the robot learning a better reward function. The structure of the CIRL formulation is such that it should encourage the \nhuman to not attempt to teach by demonstration in a way that greedily maximizes immediate reward. Rather, the human learns how to \u201cbest respond\u201d to the robot.\nFurther Notes\nCIRL can be formulated as a dec-POMDP, and reduced to a single-agent POMDP. The authors solved a 2D navigation task with CIRL to demonstrate the inferiority of having the human follow a \u201cdemonstration-by-expert\u201d policy as opposed to a \u201cbest-response\u201d policy.\nIn the experiments, the authors used regret as a performance measure for learning the reward function with respect to a fully-observed setting where the robot knows the ground truth of the hidden reward function. Another performance measure used is\nthe KL-divergence between the max-entropy trajectory distributions induced by the estimate of the reward parameters and the ground truth parameters. Finally,\nthe L2-norm is used as a measure between the vector of rewards defined by the estimate of the reward parameters and the ground truth parameters.\nComments\nI believe that we\u2019ll see some work coming out of OpenAI following this line of research in the near future (where you have AI and humans learning to colloborate safely). \nIt is also interesting to note that the experiments conducted in this paper are on a grid-world and far from being applied in a real world setting (until we get much better POMDP solvers, that is).\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/pdf/1606.03137v2.pdf"
    },
    "27": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/generative-adversarial-networks/2018/03/26/progressive-growing-gans.html",
        "trancript": "Progressive Growing of GANs for Improved Quality, Stability, and Variation\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nProgressive Growing of GANs for Improved Quality, Stability, and Variation\nMar 26, 2018\nKarras, et al., 2018\nSummary\nThe basic idea is to introduce a curriculum into the GAN training procedure. One starts by training the generator to produce 4 x 4 images, progressively adding layers to increase the resolution. In the paper, they generated high-quality 1024 x 1024 samples from CelebA, LSUN, and CIFAR-10.\nThis is a nice applied paper where the core idea is quite simple and explained clearly. They describe all of the challenges hidden under the surface of training large-scale GANs and tell the reader how they tackled them. Lots of good deep learning voodoo in this paper.\nThey found that the progressive scheme helps the GAN converege to much better optimum (image quality is amazing) and reduces total training time by about a factor of 2.\nThey mainly use the\nWGAN-GP\nloss. \nRecall that the WGAN loss is \\(\\min_G \\max_D \\mathbb{E}_{x \\sim \\mathbb{P}_r} [ D(x) ] - \\mathbb{E}_{\\hat{x} \\sim \\mathbb{P}_g} [D(\\hat{x}) ]\\)\nThe main change made in WGAN-GP is the addition of a gradient penalty term to take care of the 1-Lipschitz constraint. Previous, hard weight clipping within some [-c, c] was used. The new loss looks like \\(\\min_G \\max_D \\mathbb{E}_{x \\sim \\mathbb{P}_r} [ D(x) ] - \\mathbb{E}_{\\hat{x} \\sim \\mathbb{P}_g} [D(\\hat{x}) ] + \\color{blue}{\\lambda \\mathbb{E}_{x' \\sim \\mathbb{P}_{x'}} [ \\|( \\nabla_{x'} D(x')\\|_2 - 1)^2]}\\), and $\\lambda$ is set to 10.\nDefinition:\nInception score\nis an evaluation metric for GANs where generated samples are fed into an Inception model trained on ImageNet. Images with meaningful objects are supposed to have low label entropy, but the entropy across images should be high (high variation).\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf"
    },
    "28": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/generative-adversarial-networks/2017/02/12/gans-irl-ebm.html",
        "trancript": "A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nA Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models\nFeb 12, 2017\nFinn, Christiano, Abbeel, Levine, 2016\nSummary\nIn generative modeling, there is a trade-off between maximum-likelihood approaches that produce a moment-matching distribution that try to \u201ccover\u201d all the modes of the unknown data distribution as opposed to approaches that estimate the unknown data distribution by \u201cfilling in\u201d as many modes as possible. The latter effectively allows one to produce more realistic samples but with lower diversity, while the former leads to a solution with probability mass in parts of the space that have negligible probability under the true distribution.\nHence, the authors make the claim that for imitation learning and generative modeling, having both a \u201cdiscriminator\u201d as well as a \u201cgenerator\u201d encourages mode-seeking behavior. They show a connection between IRL and GANs by theoretically motivating an optimal IRL discriminator that learns the cost function, and an optimal IRL generator that is able to generate high-quality trajectories. It is assumed that one can sample from the generator density, i.e., that it is computable and can be held fixed while training the discriminator. The discriminator is modeled as a Boltzmann distribution with a parameterized energy function.\nSince energy-based models are a more general form of the Maximum Entropy IRL problem, the authors also were able to show a direct connection between GANs and EBMs.\nNo experiments are provided in this paper, so the efficacy of using GANs for IRL remain to be seen.\nGuided Cost Learning Demo:\nNotes\nIn Section 2.3.2. Guided Cost Learning, we have the following importance sampling formulation of the cost function, where the data is modeled as a Boltzmann distribution:\n\\[\\begin{align}\n\\mathbb{L}_{cost} (\\theta) &= \\mathbb{E}_{\\tau \\sim p} \\big[ - \\log p_{\\theta} (\\tau) \\big] = \\mathbb{E}_{\\tau \\sim p} \\big[ c_{\\theta} (\\tau) \\big] + \\log Z \\nonumber \\\\\n\n&= \\mathbb{E}_{\\tau \\sim p} \\big[ c_{\\theta} (\\tau) \\big] + \\log \\bigg ( \\int \\frac{\\exp(-c_{\\theta})}{q(\\tau)} q(\\tau) d\\tau \\bigg) \\nonumber \\\\\n\n&= \\mathbb{E}_{\\tau \\sim p} \\big[ c_{\\theta} (\\tau) \\big] + \\log \\bigg ( \\mathbb{E}_{\\tau \\sim q} \\bigg [\\frac{\\exp(-c_{\\theta}(\\tau))}{q(\\tau)} \\bigg ]\\bigg). \\nonumber\n\n\\end{align}\\]\nWe want our biased distribution $q(\\tau) \\propto | \\exp(-c_{\\theta})(\\tau)) | = \\exp(-c_{\\theta}(\\tau))$. This is the optimal importance sampling distribution that produces the importance sampling estimate of some function of a random variable $f(X)$ with minimal variance. It can be shown that $q*(x) \\propto f(x) * p(x)$.\nImportance sampling estimates suffer from high variance if the sampling distribution $q$ is biased. In Guided Cost Learning, to ensure $q$ samples from all trajectories $\\tau$ with high values of $\\exp(-c_{\\theta}(\\tau))$, the demonstration data samples (low cost as result of IRL objective) are mixed with the generated samples from $q$. Hence, $q$ is replaced with $\\mu = \\frac{1}{2}p + \\frac{1}{2}q$ in the cost function.\nIn Section 3, the author\u2019s theoretical argument for comparing GANs and IRL begins by assuming that the discriminator can be written as\n\\[D_{\\theta}(\\tau) = \\frac{\\frac{1}{Z} \\exp (-c_{\\theta}(\\tau))}{\\frac{1}{Z} \\exp (-c_{\\theta}(\\tau)) + q(\\tau)}\\]\nTo see that is similar to the standard sigmoid binary classification loss, recall the identity\n\\[f(x) = \\frac{1}{1 + \\exp(-x)} = \\frac{\\exp(x)}{1 + \\exp(x)}.\\]\nLet $\\log Z$ be the bias of the sigmoid and notice that $\\log q(\\tau)$ is subtracted from the input.\nThe author\u2019s argument continues by showing that this specific form of a GAN optimizes the same thing that MaximumEnt IRL does (pg. 6).\nQuestions\nBeing able to compute the generator\u2019s density and evaluate it cheaply enables this method, since you can then realistically learn an unbiased estimate of the partition function. What happens when the partition function remains biased?\nIn Guided Cost Learning, what form does $q(\\tau)$ take? Since it attaches a probability to a trajectory, it should have the same form as the demonstration distribution $p$\u2026i.e., the input is a sequence of ($x_i$, $u_i$) and the output is a probability.\nThe GAN training procedure minimizes the\nJensen-Shannon divergence\n, which works sort of like the reverse-KL divergence. However, GANs don\u2019t try to fit as many modes of the data distribution as the model is able to- see\nSection 3.2.5\nof Goodfellow\u2019s 2016 NIPS tutorial. In fact, this is in part a symptom of the mode collapse problem that GANs have. So, does training Guided Cost Learning/EBMs with GANs make them susceptible to this problem? The authors don\u2019t\nreally\ndiscuss this, but it may only become apparent in practice. The motivation for the mixed sampling distribution for the importance sampling formulation seems to be realted to this.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1611.03852"
    },
    "29": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/generative-adversarial-networks/2017/02/01/infoGAN.html",
        "trancript": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nInfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\nFeb 1, 2017\nChen, et al., 2016\nSummary\nInfoGAN is an extension to Generative Adversarial Networks that learns disentangled representations of the latent variables within the generator network. The authors employ the variational information maximization framework to optimize a lower bound on a mutual information criterion. This MI is between a small subset of the latent variables and the output of the generator. The authors argue that this encourages these latent variables to become \u201cdisentangled\u201d.\nIn turn, this allows the GANs to learn, in a completely unsupervised manner, interesting data representations such as stylistic factors on the MNIST dataset. The change to the traditional GAN architecture is minimal, since this is simply a regularized MI term added to the minimax function.\nQuestions\nWhy don\u2019t they use the reverse KL in Eq. 4? This is what is normally used in Variational Bayes (why?). The reverse KL, KL(Q || P), is minimized when Q places no probability mass where P has no probability mass..\nWhat justifies moving $f(x,y)$ into the third integral in Eq. 7?\nWhat other potential applications of GANs are there besides generating realistic samples from $p_{data}$. How can it be used for general density estimation? (This seems like a\nbig\nquestion)\nAre the features learned by the conv nets any different between InfoGAN and GAN? (What is the potential importance of this?)\nDoes InfoGAN produce sharper/more realistic images than GANs? Seems like the answer would be no, but hard to quantify this\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/pdf/1606.03657v1.pdf"
    },
    "30": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/natural-language-processing/2018/06/22/z-forcing.html",
        "trancript": "Z-Forcing: Training Stochastic Recurrent Networks\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nZ-Forcing: Training Stochastic Recurrent Networks\nJun 22, 2018\nGoyal, et al., 2017\nIntro\nA new training procedure for recurrent VAEs is proposed. Recall that for VAEs, we model a joint distribution over observations $x$ and latent variables $z$, and assume that $z$ is involved in the generation of $x$. This distribution is parameterized by $\\theta$. Maximizing the marginal log-likelihood $p_{\\theta}(x)$ wrt $\\theta$ is intractable bc it requires integrating over $z$. Instead, introduce a variational distribution $q_{\\phi}(z|x)$ and maximize a lower bound on the marginal log-likelihood\u2013the ELBO.\nStochastic recurrent networks\nWhen applying VAEs to sequences, it has been proposed to use recurrent networks for the recognition network (aka inference network aka variation posterior) and the generation network (aka decoder aka conditional probability of the next observation given previous observations and latents). These probabilistic models can be autoregressive (in this paper, they use LSTMs with MLPs for predicting the parameters of Gaussian distributions). It is common to model these conditional distributions with Gaussians for continuous variables or categoricals for discrete variables.\nUsually, the prior over latent variables is also learned with a parametric model.\nIf I\u2019m not mistaken, learning the parameters of these parametric models with a training data set, and the using them at test time for fast inference is referred to as\namortized variational inference\n, which appears to have\ncorrelaries in our cognition\n.\nZ-forcing\nStrong autoregressive decoders overpower the latent variables $z$, preventing the CPD from learning complex multi-modal distributions. To mitigate this, they introduce an auxiliary cost to the training objective. An extra parametric model is introduced, $p_{\\eta}(b | z)$, that \u201cforces\u201d the latents to be predictive of the hidden states $b$ of the \u201cbackward network\u201d (the inference network).\nExperiments\nThey validate the approach on speech modeling (TIMIT, Blizzard) and language modeling. The metric is average LL. On Seqeuential MNIST, z-forcing is competitive with \u201cdeeper\u201d recurrent generative models like PixelRNN.\nSome fun language modeling results interpolating the latent space\nTakeaways\nIt\u2019s always a consideration as to whether increasing the complexity of an approach (adding an extra network and auxiliary cost) is worth the effort vs. simpler approaches that can get almost the same performance. The results on TIMIT and Blizzard are pretty convincing. The authors also suggest incorporating the auxiliary loss with PixelRNN/CNN in future work.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/7248-z-forcing-training-stochastic-recurrent-networks"
    },
    "31": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/natural-language-processing/2017/04/05/distributed-representations-of-words-and-phrases.html",
        "trancript": "Distributed Representations of Words and Phrases and their Compositionality\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nDistributed Representations of Words and Phrases and their Compositionality\nApr 5, 2017\nMikolov, et al., 2013\nSkip-gram model\nObjective is to find word representations that are useful for predicting the surrounding words in a sentence or a document. Given a sequence of words $w_1, w_2, \u2026, w_T$, the Skip-gram model aims to max the average log probability\n\\[\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\le j \\le c, j \\neq 0} \\log p(w_{t+j} | w_t)\\]\nwhere\nc\nis the size of the training context. Larger\nc\nresults in more training examples and thus can lead to a higher accuracy at the expense of increased training time. The probability $p(w_O | w_I )$ is represented with a softmax.\nHeirarchical Softmax\nInstead of evaluated\nW\noutput nodes of a neural network to get the probability distribution, where\nW\nis the size of the target dictionary, only need to evaluate about $\\log_2 (W)$ nodes.\nThe idea is to represent the output layer as a binary tree with\nW\nleaves and, for each node, explicitly represents the relative probabilities of its child nodes. Then the probability $p(w_O | w_I )$ can be defined by the product of probabilities of a path down the tree from the root. The root here is the first word in the sequence. The individual probabilities are outputs of a sigmoid, scaled by +1 or -1 if the current word\nw\n\u2019s probability matches that of its child.\nNegative Sampling\nA simplified form of something called Noice Constrastive Estimation (NCE). NCE aims to learn a model that is able to differentiate data from noise by means of logistic regression. The negative sampling objective simplifies this because for the Skip-gram model, only the high-quality vector representation is needed. The task becomes to distinguish the target word from draws from a noise distribution using logistic regression over\nk\nnegative samples for each data sample.\nConclusion\nThe authors used a few other tricks, like sub-sampling frequent words such as \u201cin\u201d, \u201cthe\u201d, \u201ca\u201d. Also, they used unigrams and bigrams to identify phrases during training. This approach can be applied to massive monolingual corpuses to quickly learn high-quality vector representations of words.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
    },
    "32": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/natural-language-processing/2017/01/26/neural-probabilistic-language.html",
        "trancript": "A Neural Probabilistic Language Model\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nA Neural Probabilistic Language Model\nJan 26, 2017\nBengio, et al., 2003\nSummary\nassociate with each word in the vocabulary a distributed\nword feature vector\n(real valued vector in $\\mathbb{R}^n$)\nexpress the joint probability function of word sequences in terms of the feature vectors of these words in the sequence\nlearn simultaneously the word feature vectors and the parameters of that probability function\nFor discrete random variables, learning a joint probability distribution is hard because a small change in one of the variables could cause a large change in the value of the function to be estimated. Instead, transforming the discrete random variables into a vector space in $\\mathbb{R}^n$ allows the use of neural nets or GMMs which are smooth approximators. Additionally, the notion of a \u201cnearby\u201d word within the continuous vector space representation is now defined more clearly.\nWords are mapped into a matrix $C$ of size ($|V| \\times m$) for a vocabulary size $|V|$ and embedding dim $m$. The feature vectors (columns of $C$) are learned simultanesouly with the parameters of the neural network. The input to the time-lagged neural network (RNN) is the concatenated vector of word representations. The objective is to maximize the log-likehood of a given sequence of out-of-sample words. This essentially is the encoder in Neural Machine Translation.\nThings of interest\nThe curse of dimensionality in modeling joint probability of sequences of words in a language is a major stumbling block\nOne can reduce the difficulty by using the fact that temporally closer words in the word sequence are statistically more dependent $\\rightarrow$\nn-gram\nmodels\nIt was noted by the authors that n-gram models and the neural models made different \u201cerrors\u201d, so an averaging model of the two showed improvements overall\nIt is suggested by the authors as well to train multiple smaller networks on partitions of the training data to speed things up\nLearning word embeddings is very parallelizable- specifically, the computation of the output layer of the neural model was found to take up roughly 99.7% of the computation (since you\u2019re computing a likelihood of a sequence out of the entire vocabulary)\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
    },
    "33": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-learning-theory/2020/03/22/what-can-neural-nets-reason-about.html",
        "trancript": "What Can Neural Networks Reason About?\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nWhat Can Neural Networks Reason About?\nMar 22, 2020\nXu, Li, Zhang, Du, Kawarabayashi, Jegelka, 2020\nSummary\nThis work proposes a theory they call \u201calgorithmic alignment\u201d to explain why some classes of neural net architectures generalize much better than others on certain reasoning problems. They use PAC learning to derive sample complexity bounds that show that the number of samples needed to achieve a desired amount of generalization increases when certain subproblems are \u201chard\u201d to learn.\nFor example, they empirically show that DeepSets can easily learn summary statistics for a set of numbers, like \u201cmax\u201d or \u201cmin\u201d. Since a single MLP has to learn the aggregation function (an easy subproblem) plus the for loop over all elements (a harder subproblem), their theory suggests that the number of samples required to acheive good generalization at test time is much higher for the MLP, which their experiments confirm. Interestingly, they explain why graph neural nets (GNNs) align well with dynamic programming (DP) problems (because of their iterative message-passing style updates), but then also explain why they do not align well with NP-Hard problems. They provide further experimental evidence on a shortest-paths DP problem and Subset-Sum NP-Hard problem to verify this.\nWhat About Transformers?\nThe paper doesn\u2019t discuss Transformers, so as a simple exercise, I thought about how it fits into their framework.\nTransformers map readily onto fully connected GNNs\n, which suggests that they should \u201calign algorithmically\u201d with DP problems but not NP-Hard problems. Note that for a set of $K$ objects, a multi-head attention Transformer/GNN performs an $O(K^2 d)$ operation. This highlights one limitation of Transformers; they can easily reason about object-object relations, but will struggle to generalize when faced with higher-order relations such as object-object-object. Relational reasoning over $k$-partite graphs, $k > 2$, shows up in certain NP-Hard problems like\nMultidimensional Assignment\n. It seems like algorithm alignment will certainly be useful for future research on designing neural net architectures for NP-Hard problems.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=rJxbJeHFPS"
    },
    "34": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-learning-theory/2018/04/07/convexified-cnns.html",
        "trancript": "Convexified Convolutional Neural Networks\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nConvexified Convolutional Neural Networks\nApr 7, 2018\nZhang, Liang, Wainwright, 2016\nSummary\nIn this paper, the authors proposed a method for convexifying convolutional neural networks to train them without backpropagation. Furthermore, this relaxation to the convex setting allows for theoretical proofs of bounds on the generalization error.\nSuccinctly, they propose to use RKHS and the kernel trick to lift the data into a high-dimensional space that is expressive enough to capture certain nonlinear activation functions. Hence, on experiments on MNIST and CIFAR-10, they show that they can outperform smaller CNNs by \u201cconvexifying\u201d them.\nThey note that their method doesn\u2019t work with max pooling or very deep CNNs with lots of bells and whistles.\nThis is a thought-provoking paper. I like how the authors pursued a theoretically interesting question, even though there isn\u2019t much practical use yet for this. I don\u2019t have personal experience writing theory papers, but I imagine that this is a good(?) representation of how they often go in ML. The research is driven by an interesting theoretical question, not a practical application that needs solving/SOTA results.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1609.01000"
    },
    "35": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-learning-theory/2017/11/19/geometric-deep-learning.html",
        "trancript": "Geometric deep learning: going beyond Euclidean data\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nGeometric deep learning: going beyond Euclidean data\nNov 19, 2017\nBronstein, 2017\nNotes\nThis paper surveys progress on adapting deep learning techniques to non-Euclidean data and suggests future directions. One of the strengths (and weaknesses) of deep learning\u2013specifically exploited by convolutional neural networks\u2013is that the data is assumed to exhibit translation invariance/equivariance and invariance to local deformations. Hence, long-range dependencies can be learned with multi-scale, hierarchical techniques where spatial resolution is reduced. However, this means that any information about the data that can\u2019t be learned when spatial resolution is reduced can get lost (I believe that residual networks aim to address this by the skip connections that are able to learn an identity operation; also, in computer vision, multi-scale versions of the data are often fed to CNNs). Key areas where this assumption about the data appears to be true is computer vision and speech recognition.\nSome quick background\nThe\nLaplacian\n, a self-adjoint (symmetric) positive semi-definite operator, which is defined for smooth manifolds and graphs in this paper, can be thought of as the difference between the local average of a function around a point and the value of the function at the point itself. It\u2019s generally defined as $\\triangle = -\\text{div} \\nabla$. When discretizing a continuous, smooth manifold with a\nmesh\n, note that the graph Laplacian might not converge to the continuous Laplacian operator with increasing sampling density. To be consistent, need to create a triangular mesh, i.e., represent the manifold as a polyhedral surface.\nSpectral methods\nFourier analysis on non-Euclidean domains is possible by considering the eigendecomposition of the Laplacian operator. A possible transformation of the Convolution Theorem to functions on manifolds and graphs is discussed, but is noted as not being shift-invariant.\nThe Spectral CNN can be defined by introducing a spectral convolutional layer acting on the vertices of the graph and using filters in the frequency domain and the eigenvectors of the Laplacian. However, the spectral filter coefficients will be dependent on the particular eigenvectors (basis) - domain dependency == bad for generalization!\nThe non-Euclidean analogy of pooling is\ngraph coarsening\n- only a fraction of the vertices of the graph are retained. Strided convolutions can be generalized to the spectral construction by only keeping the low-frequency components - must recompute the graph Laplacian after applying the nonlinearity in the spatial domain, however.\nPerforming matrix multiplications on the eigendecomposition of the Laplacian is expensive!\nSpectrum-free Methods\nA polynomial of the Laplacian acts as a polynomial on the eigenvalues\n. ChebNet (Defferrard et al.) and Graph Convolutional Networks (Kipf et al.) boil down to applying simple filters acting on the r- or 1-hop neighborhood of the graph in the spatial domain.\nSome examples of generalizations of CNNs that define weighting functions for a locally Euclidean coordinate system around a point on a manifold are the\nGeodesic CNN\nAnisotropic CNN\nMixture Model network (MoNet)\nWhat problems are being solved with these methods?\nRanking and community detection on social networks\nRecommender systems\n3D geometric data in Computer Vision/Graphics\nShape classification\nFeature correspondence for 3D shapes\nBehavior of N-particle systems (particle physics, LHC)\nMolecule design\nMedical imaging\nOpen Problems\nGeneralization\nspectral analogues of convolution learned on one graph cannot be readily applied to other ones (domain dependency). Spatial methods generalize across different domains, but come with their own subtleties\nTime-varying domains\nDirected graphs\nnon-symmetric Laplacian that do not have orthogonal eigendecompositions for interpretable spectral-domain constructions\nSynthesis problems\ngenerative models\nComputation\nextending deep learning frameworks for non-Euclidean data\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1611.08097"
    },
    "36": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-learning-theory/2017/08/07/deep-hessian-free.html",
        "trancript": "Deep Learning via Hessian-Free Optimization\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nDeep Learning via Hessian-Free Optimization\nAug 7, 2017\nJames Martens, 2010\nSummary\nThis paper introduces a fairly complex optimization algorithm for deep nets that uses approximate 2nd-order gradient information\nIn Hessian-Free optimization, you can directly approximate a Hessian-vector product $Hv$ with the method of finite-differences; this only costs 1 more gradient evaluation\nlinear conjugate gradient algorithm allows one to solve for the optimal search direction in $O(N)$ iterations ($N$ is the number of parameters) with only matrix-vector products\nNewton\u2019s method is scale invariant, e.g., for a new parameterization $\\hat{\\theta} = A \\theta$ for some invertible matrix $A$, the optimal search direction is now $\\hat{p} = A p$ where $p$ is the original optimal search direction. Gradient descent is not (need proof!) - so many bad things about GD, but it\u2019s so easy to implement..\nConsiderations when applying this technique\nNeed to use an adaptive damping parameters $\\lambda$ beause the relative scale of $B = H(\\theta)$ is changing and $H(\\theta)$ must remain positive semidefinite. Recommended heuristic is given in Section 4.1\nGauss-Newton matrix $G$ can produce better search directions than $H$, see\nthis blog post\nfor a summary\nCompute gradient on entire dataset, but use minibatches to compute Hessian-vector products. SGD requires 10\u2019s of thousands of iterations versus ~200 for HF\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf"
    },
    "37": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/deep-learning-theory/2017/03/01/batch-renorm.html",
        "trancript": "Batch Renormalization-Towards Reducing Minibatch Dependence in Batch-Normalized Models\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nBatch Renormalization-Towards Reducing Minibatch Dependence in Batch-Normalized Models\nMar 1, 2017\nIoffe, 2017\nSummary\nBatchNorm\nBatch Renormalization is a follow-up to the 2015 paper\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n. The original motivation for BatchNorm came from the fact that the distribution of the inputs to each layer in a deep network changes throughout training as the parameters change. Since this slows down training, the authors reasoned that normalization of these distributions should allow for the use of higher learning rates and increased insensitivity to initializations. BatchNorm achieves the same accuracy as previous networks but with signifcantly fewer training steps.\nBatchNorm is an added \u201clayer\u201d to deep networks placed after the output of the layer transformation (e.g., the convolution operation) but before the nonlinearity (e.g., a ReLu layer). During training, the sample estimates for the mean and variance of the layer\u2019s outputs are generated from mini-batch statistics. A moving average estimate is maintained as well, and is used during inference.\nSome key points about BatchNorm:\nNormalization cannot be interleaved with gradient descent optimization. This is because the gradient descent optimization wouldn\u2019t be taking into account the fact that normalization takes place. BatchNorm requires backpropagation to compute derivatives for the normalization w.r.t. minibatch statistics; otherwise, model parameters explode without the loss decreasing.\nFull whitening of each layer\u2019s inputs is costly and not everywhere differentiable, so each scalar feature is whitened independently\nTo prevent BatchNorm from changing what the network can represent, parameters are introduced that allow the BatchNorm layer to represent the identity transform. These parameters are also optimized with SGD\nBatchRenorm\nBatchNorm came with pros and cons. It is less effective when the training minibatches are small or do not consist of independent samples. Small minibatches mean that the sample estimates of the mean and variance during training are less accurate. These inaccuracies are compounded with depth. For non-iid minibatches at train-time, BatchNorm will tend to overfit to the specific distribution of the examples; BatchRenorm aims to break up the dependence between similar samples.\nTherefore, the goal of BatchRenorm is to provide the model with the capacity to overcome differences in activitations between training and inference.\nEssentially\nparameters $r$ and $d$ are introduced that relate the output of the normalizations between train time (computed with minibatch statistics) and inference (computed with population statistics for entire training set). If $\\mu$ is an estimate of the mean of some particular node $x$, and $\\sigma$ is an estimate of its standard deviation perhaps computed as a moving average over the last several minibatches, we have \\(\\frac{x_i - \\mu}{\\sigma} = \\frac{x_i - \\mu_B}{\\sigma_B} r + d\\text{, where } r = \\frac{\\sigma_B}{\\sigma}\\text{,  }d = \\frac{\\mu_B - \\mu}{\\sigma}\\)\n$r$ and $d$ are held constant during backprop\nThis transform is identity in expectation, and BatchNorm is $r = 1$, $d = 0$.\nThis allows the layers to observe the \u201ccorrect\u201d activiations that would be seen during inference. In practice, you start with BatchNorm for a few thousand training steps, then switch to BatchRenorm.\nQuestions for discussion\nWhat is the\ncost\nof BatchRenorm? Slightly more model complexity because of the added $r$ and $d$, and slightly more complex backprop equations? Hyper-parameter search needed for $r_max$ and $d_max$?\nDoesn\u2019t seem to always be necessary to use BatchRenorm - in real world problems, however, training data probably won\u2019t be \u201cnice\u201d and iid\nBatchNorm for Recurrent Networks\n?\nThis was used in Deep Reinforcement Learning to stabilize policy gradient methods!\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1702.03275"
    },
    "38": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/agi/2017/05/14/interaction-networks-for-learning-about-objects-relations-physics.html",
        "trancript": "Interaction Networks for Learning about Objects, Relations, and Physics\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nInteraction Networks for Learning about Objects, Relations, and Physics\nMay 14, 2017\nBattaglia, et al., 2016\nSummary\nThis ambitious paper proposes a deep learning framework for modeling the physical interactions between objects in an environment. The authors present Interaction Networks (IN), which explicitly separate the processes of learning object dynamics and relations between objects.\nIN is designed to work on graphs where objects are nodes and edges are relations between objects. This is to make it scalable to different environments.\nThe architecture is simple; two MLPs learn representations over object dynamics and relations between objects, respectively. The input to the system is the state decomposed into the objects and their physical relations (gravitational attraciton, collisions, springs), as well as external effects (gravity). Object states can be further decomposed into position and velocity. In general, the output is the velocity of the objects at the subsequent time step.\nThe system is evaluated on interesting physical reasoning tasks, such as n-bodies interacting, bouncing balls colliding, and string/mass systems. IN showed significantly lower MSE when predicting future states of objects in the scenes compared to simple baselines.\nA custom physics engine was used to generate trajectories for training data. All training objectives and test measures used MSE between the model\u2019s predictions and the ground truth target.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.00222"
    },
    "39": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/agi/2016/05/13/learning-to-think.html",
        "trancript": "Building Machines That Learn and Think Like People\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nBuilding Machines That Learn and Think Like People\nMay 13, 2016\nLake, et al., 2016\nSummary\nThis article presents a roadmap for future progress in developing machines that can learn and \u201cthink\u201d like humans. The authors outline a number of ideas that they believe are crucial to making progress in artificial intelligence. A short-term goal of the authors is to improve today\u2019s machine learning techniques so that complex concepts can be learned quickly with only small amounts of data. Currently, our algorithms require ungainly amounts of data and computation to perform at acceptable levels.\nThe authors argue that a marriage of two distinct research areas in artificial intelligence, that of statistical pattern recognition and of model-building, will produce a promising avenue for new advances. Specifically, the authors suggest that learning as a form of explaining observed data by the construction of causal models of the world can be accomplished by deep neural networks.\nThe first causal model that is discussed is dubbed\nintuitive physics\n. That is, a machine with an internal model of the physics of its environment is able to learn more quickly than without. This is because the machine will not need to re-learn basic physics principles when attempting to learn new tasks that require it.\nThe second causal model is\nintuitive psychology\n. A learning agent that assumes that other agents in its environment are rational and have their own goals and beliefs can try to infer what these may be. Having knowledge of the intentions of other agents in the environment allow the learning agent to achieve its own goals in a much more efficient way.\nCompositionality\nCompositionality is the classic idea that new representations can be constructed through the combination of primitive elements. See\nLake et. all 2015\nfor a discussion on the use of compositionality to generate novel handwritten characters. This is central to training agents to learn models of complex symbolic concepts.\nLearning-to-Learn\nLearning-to-learn is closely related to the concept of \u201ctransfer-learning\u201d and \u201crepresentation-learning\u201d in machine learning. Machines that learn like humans will need to be able to learn rich, informative priors and models of the world that can be applied to a variety of tasks. Systems should be able to learn to do new tasks as flexibly and rapidly as humans do. The authors suggest compositionality and the use of causal models to augment the current deep-learning research in transfer-learning.\nThinking fast\nThere are still a lot of challenges concerning intractability of certain methods and the amount of time necessary for doing optimization over high-dimensional potentially non-convex parameter spaces. Approximate inference is an area of research that aims to address these challenges. Monte Carlo methods are promising for many problems, but runs into problems when the hypothesis space is vast. Deep neural networks may potentially be used for performing probabilistic inference in a generative model or a probabilistic program.\nDangers of model-free RL\nThere is evidence that the brain uses a form of model-free reinforcement learning to accomplish certain tasks. However, there is also evidence that the brain has a model-based learning system. Shifting between model-free and model-based learning methods can benefit from taking advantage of the causal-model/compositionality discussed above. Current model-free methods, as seen in DQN, Google DeepMind\u2019s Atari agent, are severely limited when it comes to generalizing beyond a certain amount. For example, the DQN agent would need significant re-training to be able to play a game whose screen was locked to a different resolution, and it would fail to play a game that required a working memory of game states/frames from the past (i.e. the agent would be missing causal information that coudl explain what is happening at the current state of the game).\nStrengths\nThe authors address potential counter-arguments in Section 5. They are: \u201cComparing the learning speeds of humans and neural networks on specific tasks is not meaningful, because humans have extensive prior experience\u201d, \u201cBiological plausibility suggests theories of intelligence should start with networks\u201d, \u201cLanguage is essential for human intelligence. Why is it not more prominent here?\u201d\nWeaknesses\nThere isn\u2019t a clear definition of \u201cintelligence\u201d stated in the article. The closest is the authors\u2019 statement that the goal of those who wish to create machines that learn like humans (e.g. display human-level intelligence) should be to devise ways to carry out learning from far less data and to generalize in richer and more flexible ways.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/pdf/1604.00289v2.pdf"
    },
    "40": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/agi/2016/01/18/review-btom.html",
        "trancript": "Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nBayesian Theory of Mind: Modeling Joint Belief-Desire Attribution\nJan 18, 2016\nBaker, et al., 2011\nIntroduction\nQ: What is the \u2018Theory of Mind\u2019?\nA: \u201cThe human capacity for reasoning about agents\u2019 mental states such as beliefs and desires\u201d\nThe inference problem of determining the mental states and desires that caused some behavior is fundamentally ill-posed (i.e. there are many combinations of beliefs and desires that could explain the same behavior).\nOne approach to this is to employ the \u2018principal of rational action\u2019, which is the theory that agents are expected to choose actions that maximize their expected utility\nBayesian Theory of Mind (BToM)\nAn agent\u2019s planning and inference about the world is modeled as a Partially Observable Markov Decision Process. This model includes a representation of the agent\u2019s desires (utility function) and the agent\u2019s own subjective beliefs about the environment (probability distribution)\nAn observer of an agent\u2019s behavior in an environment attempts to attribute the beliefs and desires that caused the agent to generate such behavior.\nJoint belief and desire inference is carried out with a form of belief filtering; a Dynamic Bayes Net (DBN) is employed to model the agent\u2019s desires, observations, states, beliefs, and actions over time.\nConclusions\nAn experiment was conducted where a sample of test subjects were asked to infer the beliefs and desires of an agent acting in a simulated environment. The results of this study was compared to BToM. BToM performed very closely to the humans at the task of belief and desire inference. By modifying BToM and running the experiment with the alternative models, it was determined that it is necessary to perform joint inference on both the agent\u2019s beliefs and desires, and to explicitly model the agent\u2019s observational process. It was also necessary to represent the agent\u2019s initial uncertainty over both the beliefs and desires.\nThe idea of modeling agents in an environment with POMDPs shows significant promise. Due to computational difficulties, this approach is quite restricted. With recent advances in Deep Reinforcement Learning, however, multi-agent POMDPs could see a resurgence in popularity.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://mindmodeling.org/cogsci2011/papers/0583/paper0583.pdf"
    },
    "41": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/general-ml/2017/09/14/LLE.html",
        "trancript": "Nonlinear Dimensionality Reduction by Locally Linear Embedding\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nNonlinear Dimensionality Reduction by Locally Linear Embedding\nSep 14, 2017\nRoweis, Saul, 2000\nSummary\nISOMAP and MDS require estimates of pairwise distances between data points. LLE gets around this by \u201cthinking\u201d globally but fitting locally. Essentially, each data point should hypothetically be representable by a locally linear patch. Therefore, LLE seeks $W$ such that\n\\[E(W) = \\sum_i \\| X_i - \\sum_j W_{ij} X_j \\|^{2}\\]\nis minimized. Hence, a data point should be reconstructed by its neighbors; the problem is solved via least squares. Note that the weights are invariant to affine transformations and translations. Assuming that $W$ should be preserved in a lower dimensional representation of the data, LLE seeks to solve\n\\[\\Phi (Y) = \\sum_i \\| Y_i - \\sum_j W_{ij} Y_j \\|^{2}\\]\nThe optimal coordinates $Y$ can be found by solving a sparse $n \\times n$ eigenvalue problem.\nBecause of the simple construction and use of simple linear algebra, LLE has better theoretical properties than other algorithms like autoencoders\nIt also has less hyperparameters\nDoesn\u2019t need to be rerun when new dimensions are added to the embedding space (old ones do not change)\nDoes LLE work on spheres? It seems like it would run into the same problem if the sphere didn\u2019t have a hole taken out of it\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://www.robots.ox.ac.uk/~az/lectures/ml/lle.pdf"
    },
    "42": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/general-ml/2017/08/31/ISOMAP.html",
        "trancript": "A Global Geometric Framework for Nonlinear Dimensionality Reduction\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nA Global Geometric Framework for Nonlinear Dimensionality Reduction\nAug 31, 2017\nTenenbaum, et al. 2000\nSummary\nIsomap, seemingly named for \u201cIsometric mapping\u201d, seeks to provide a solution to the problem of non-linear dimensionality reduction. The method is especially suitable for high-dimensional manifolds that exhibit non-Euclidean geometry, such that the Euclidean distance between data points returns distances that are not actually realistic for the underlying low-dimensional manifold. The intuition for this approach lies in the use of the all-pairs shortest path algorithm to improve upon Multi-dimensional scaling. Under general conditions on the density and curvature of the points, a geodesic distance can be estimated between far away points on the high-dimensional manifold via the all-pairs shortest path that converges to the true distance in the limit. Then, similar to MDS, Isomap attempts to find coordinate vectors for a low-dimensional space within which the distances between points are preserved as much as possible. This essentially results in the selection of the largest p eigenvectors of the matrix of estimated distances on the high-dimensional manifold (transformed to inner products). To make the algorithm work, the first step consists of clustering the data points either using k-NN or $\\epsilon$-balls. Edges are placed between all points clustered together, to form the graph upon which all-pairs shortest path is run.\nIn this paper, the authors present examples of applying Isomap to a dataset of faces, MNIST, and the \u201cswiss roll\u201d dataset. Interestingly, they are able to map the faces dataset to a 3-D space, capturing left-right poses, up-down poses, and variations in ambient lighting. They show that PCA and MDS converge (the residual loss goes to 0) but they are unable to recover the true dimensionality of the low-dimensional manifold. This seems to be troublesome, because if one naively applies PCA to a dataset and the residual loss goes to 0, it appears then that the user of this algorithm will mistakenly believe they have recovered the true low-dimensional manifold. It would be interesting to then run a classifier on this low-dimensional representation produced by PCA, and then check the performance against the same classifier using the low-dimensional representation learned by Isomap. I imagine that the Isomap classifier will have slightly better performance.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://www.jstor.org/stable/pdf/3081721.pdf"
    },
    "43": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/general-ml/2017/06/21/mcmc-rev.html",
        "trancript": "The Markov Chain Monte Carlo Revolution\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nThe Markov Chain Monte Carlo Revolution\nJun 21, 2017\nPersi Diaconis, 2009\nThe Fundamental Theorem of Markov Chains\nFrom any starting state $x$, the $n^{th}$ step of a run of the MC has chance close to $\\pi (y)$ of being at $y$ if $n$ is large. The MC must be connected, i.e., in the limit, the kernel $K$/proposal distribution/Markov transition matrix has no zero-probability transitions.\nMetropolis Algorithm\nBased on \u201cproposal\u201d and \u201cacceptance\u201d\nThe acceptance ratio is to ensure that the fraction of time spent in each state is proportional to $\\pi(x)$ for $x \\in \\chi$\nIn this algorithm, the normalization constants of the stationary distributions cancel out!\nIn Equation 2.3, if the acceptance ratio is $< 1$, you are multiplying the probabilities $J(x,y)$ and $A(x,y)$ together. This generates the success probability $J(x,y)A(x,y)$ for transitioning x -> y. You want to accept transitions that move to states that are reversible (and hence move you closer to the true stationary distribution), and stay away from states that are not. The algorithm hence allows the Markov Chain to stay in the same place with some probability if the acceptance ratio is low.\nThis algorithm produces a reversible Markov chain:\n\\[\\pi(x) K(x,y) = \\pi(y) K(y, x).\\]\nSince $\\pi K = \\pi$ (the stationary distribution is unchanged by the operation of the kernel $K$), $\\pi$ is a left eigenvector of $K$ with eigenvalue 1. The basic result on convergence to the stationary distribution can be found by taking the spectral decomposition of $K$.\nGibbs sampler\nExample\nHere\u2019s a neat example of the Metropolis Hastings algorithm for sampling from a boltzmann distribution\n. Remember- low-energy states have high boltzmann probability!\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://www.ams.org/journals/bull/2009-46-02/S0273-0979-08-01238-X/home.html"
    },
    "44": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/general-ml/2017/04/20/topology-data.html",
        "trancript": "Topology and Data\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nTopology and Data\nApr 20, 2017\nGunnar Carlsson, 2009\nDefinitions\nHomology\nis a general way of associating a sequence of algebraic objects such as abelian groups or modules to other mathematical objects such as topological spaces. Homology itself was developed as a way to analyse and classify manifolds according to their cycles \u2013 closed loops (or more generally submanifolds) that can be drawn on a given n dimensional manifold but not continuously deformed into each other\n1\n.\nConnectivity information\nis information concerning the loops and higher dimensional analogues in a space.\nFunctoriality\nis the notion that invariants should be related not just to objects being studied, but also to the maps between those objects.\nTwo mathematical objects are said to be\nhomotopic\nif one can be continuously deformed into the other.\nThe\nk-th Betti number\ncorresponds to an informal notion of the number of independent k-dimensional surfaces in a k-dimensional vector space.\nA\nsimplicial complex structure\non a space is an expression of the space as a union of points, intervals, triangles, and higher dimensional analogues.\nSummary\nThe homology of simplicial complexes is algorithmically computable; hence, it is desirable to construct a simplicial complex which computes the homology of an underlying space X. One way to do this is to produce a homotopy equivalence from X to the simplicial complex.\nConsider the case where there is a large amount of potentially noisy high-dimensional data. A question that can be asked is whether it is possible to infer the Betti numbers of the underlying space from the noisy data. The Cech complex\n2\non a metric space, which is a covering by balls of a fixed radius $\\epsilon$, is useful because it can be shown to be homotopy equivalent to the underlying Reimannian manifold. However, the data cannot be assumed to always lie on a manifold. The philosophy of selecting all values of $\\epsilon$ at once to compute a summary of homological information is known as\npersistence\n.\nInteresting points\n\u201cTopology is exactly that branch of mathematics which deals with qualitative geometric information.\u201d\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://www.ams.org/images/carlsson-notes.pdf"
    },
    "45": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/general-ml/2016/08/23/ML-for-genomics.html",
        "trancript": "Machine Learning in Genomic Medicine: A Review of Computational Problems and Data Sets\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nMachine Learning in Genomic Medicine: A Review of Computational Problems and Data Sets\nAug 23, 2016\nLeung, Michael K. K., 2016\nSummary\nMajor points made by the article:\n\u201cOur view is that to make genomic medicine a reality, we must develop computer systems that can accurately interpret the text of the genome just as the machinery inside the cell does\u201d.\n\u201cProtein-coding exons are the most understood regions in the genome (re: \u201cstart\u201d and \u201cstop\u201d codons\u201d).\nA long standing open problem is predicting whether a mutation will disrupt the stability or structure of the final protein molecule\n\u201cPredicting\nphenotypes\n(e.g., traits and disease risks) from biomarkers such as the genome is, in principle, a supervised machine learning problem\u201d. The correct approach is not so simple; the computational model should be trained to predict measurable intermediate cell variables, also known as molecular phenotypes, and then these variables can be linked to phenotypes.\nAlternative Splicing (AS)\nis the selection and ligation of specific exons during post-transcriptional modification.\nOn average, each protein-coding gene has approximately four transcripts (# of ways of selecting and combining available exons). We would like to be able to predict splicing by discovering the instructions that control splicing\nComputational Model of Splicing\nBy accurately modeling splicing and AS computationally, researchers have been able to predict how it is affected by variations in the genome, and then to assess whether a mutation in the genome affects disease risk.\nComputational Model of Protein-DNA and Protein-RNA binding\n\u201cAccurate models of protein-sequence binding are essential for interpreting the genome and for predicting the effects of mutations\u2026Biologists have developed high-throughput experiments that measure the sequence specificity of individual proteins.\u201d\nExample computational model: inputs = genomic sequence, outputs is a binding score. One would like to predict the \u201cmotifs\u201d, or patterns, that a particular protein binds to.\nSpecific Discussion Related to Deep Learning\nDeep Learning has been used to improve predictive performance-\nsee Feedforward NNs for AS patterns\n.\nCNNs have been used to improve predictive performance for\nbinding specificity\n.\nCellular processes are highly stochastic and hence the genotype of an individual may not be sufficient to completely determine their phenotype\nMeasuring hundreds of thousands of cell variable measurements per patient for a small group of people potentially gives a better chance at deciphering the genomic instructions of the cell. More data for a model to learn from.\nNecessary to use \u201clarge-scale machine learning\u201d\nRNNs can be useful for the following\ngenome annotation\nModelling of cell variable dynamics through time\nCreating a sequential state model of protein binding based on RNNs or LSTMs\nImputation of epigenomic tracks - seq2seq\nMachine Learning models need to be more interpret-able for genomics!\nNotes\nSince this is my first foray into computational biology, I\u2019m going to keep track of a lot of terminology here:\n1. Protein-coding genes describe how to build large molecules made from amino-acid chains (human genome contains ~20,000)\n2. Non-coding genes describe how to build small molecules made from ribonucleic acid (RNA) chains (human genome contains ~25,000)\n3. Information structures making up alternating regions on a typical gene are known as Introns and Exons \n4. Protein-sequence binding is the binding of proteins to nucleotide sequences\n5. Position-Frequency Matrix - \"workhorse of binding site modeling\"\nStrengths\nExcellent paper for Machine Learning researchers to get a first look at diving into genomics.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://www.psi.toronto.edu/publications/2015/Machine%20Learning%20in%20Genomic%20Medicine-%20A%20Review%20of%20Computational%20Problems%20and%20Data%20Sets.pdf"
    },
    "46": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/general-ml/2016/01/29/multipolicy-decision-making.html",
        "trancript": "Multipolicy Decision-Making for Autonomous Driving via Changepoint-based Behavior Prediction\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nMultipolicy Decision-Making for Autonomous Driving via Changepoint-based Behavior Prediction\nJan 29, 2016\nGalceran, Cunningham, Eustice, Olson, 2015\nSummary\nThis paper presents an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies. Only a finite set of\na priori\nknown policies are considered. Bayesian changepoint detection is used to estimate which policy a given vehicle was executing at each point in its history of actions, then infer the likelihood of each potential intention of the vehicle.\nA statistical test is proposed based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes.\nEvidence\nAnomaly detection was explored by recording three trajectories corresponding to two bikes and a bus. The bikes crossed an intersection from a sidewalk, while the bus made a significantly wide turn. System was able to detect these trajectories as anomalous (Not within the set of known policies)\nEvaluated in simulated driving environment\nNotes\nBayesian Changepoint detection infers the points in the history of observations where the underlying policy that generated the observations changed. Then, the likelihood of all available policies for the target car given the distribution over the car\u2019s potential policies at the current timestep can be computed (sounds like HMM).\nThe CHAMP algorithm infers the maximum\na posteriori\nset of times at which changepoints between policies have occurred, yielding a set of segments. Given a segment from time\ns\nto\nt\nand a policy\npi\n, CHAMP approx the log of the policy-evidence for that segment via the (Bae)yesian information criterion (BIC)\nViterbi path is found for the most likely sequence of latent policies\nFor decision-making, a set of samples are drawn from the distribution over policies of other cars where each sample assigns a policy to each nearby vehicle, excluding the ego car. For each policy available to the ego car (not all policies are available in every scenario e.g. intersection handling policy is not applicable when driving on a highway), and for each sample s, the process is rolled out forward in time until the decision horizon. This yields a set of simulated trajectories. The reward is evaluated for each element of the set of simulated trajectories and the maximal policy for the ego vehicle is chosen. This repeats continuously in a receding horizon manner.\nReward function\ndistance to the goal at the end of the evaluation horizon\nminimum distance to obstacles to evaluate safety\nlane choice bias to add a preference for the right lane\nmaximum yaw rate and longitudinal jerk to measure passenger comfort\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "http://www.roboticsproceedings.org/rss11/p43.pdf"
    },
    "47": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/general-ml/2016/01/29/intention-aware.html",
        "trancript": "Intention-Aware Risk Estimation: Field Results\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nIntention-Aware Risk Estimation: Field Results\nJan 29, 2016\nLefevre, Vasquez, Laugier, Ibanez-Guzman, 2015\nSummary\nThe proposed contribution is a framework for assessing risk by estimating the intentions of drivers and detecting conflicts between them. Traffic rules are explicitly represented in the model in order to reason about what the drivers are expected to do.\nBayesian programming is used to generate the risk probabilities. Particle filtering is used to approximately solve the inference problem of finding the risk based on the probability that a driver does not intend to stop at an intersection when he is expected to.\nEvidence\nThe algorithm was tested on a T-shaped intersection with two passenger vehicles equipped with Vehicle-to-Vehicle communication modems that shared their pose and speed information at a rate of 10 Hz. The test vehicles were\nnot\nequipped with autonomous emergency braking functions, instead an auditory and visual warning were triggered whenever the algorithm detected a dangerous situation.\nExperimentation involved a priority vehicle and obstacle vehicle. Evaluation for the performance of the risk assessment algorithms was based on:\nThe rate of false alarms\nThe rate of missed detections\nThe collision prediction horizon\nOut of the 90 dangerous trials, 60 were performed with the warning system running on the priority vehicle, and 30 on the obstacle vehicle. In the 20 non-dangerous trials, there were no false alarms. For every one of the 90 dangerous tests, the system was able to issue a warning early enough for the driver to avoid collision by breaking.\nStrengths\nNo need for lengthy training\nThe proposed algorithm is generic and could be implemented for various driving scenarios\nNo trajectory rollouts\nWeaknesses\nSpeed of the vehicles during experimentatin was not reported\nNo collisions during experimentation since only real vehicles were used\nEvaluation of the robustness of the algorithm is left in question due to minimal variablility in experimentation scenarios\nNotes\nThe risk of a situation is computed based on the probability that intention and expectation do not match, given measurements of the state\na Markov State Space Model is used (this appears to be very similar to the dynamic Bayes net) to propagate the system variables for the bayesian computation\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://5d4cfa3b-a-62cb3a1a-s-sites.googlegroups.com/site/stlefevre/Lefevre_ARSO_15.pdf?attachauth=ANoY7co6MtWJ5P60cdRkTvURboXJZ6O2hEVInCYPLqHZU_kuFkCuu1JRNd2p4aLm5aT4BSzIfz8LMN3S4TBZarCcir48nbbXrxHo5qTaS1Fwzkm1AEp_faxVlca3P7vTOF0WXUvptIjkdQFrFKHkOz3CIFqHLJF96Q59rTqD14uvPfbGn1XOF9ta3W3eTC0SDCrNJFqTQ7hJ62eMvVhcwiwCvcGOMf155w%3D%3D&attredirects=0"
    },
    "48": {
        "sourceUrl": "https://pemami4911.github.io/paper-summaries/general-ml/2016/01/21/how-to-write-a-great-research-paper.html",
        "trancript": "How To Write A Great Research Paper (Professor Simon Peyton Jones)\nPatrick Emami\nUpdates on my machine learning research, summaries of papers, and blog posts\nBlog\n/\nCS Dojo\n/\nAbout Me\n/\nPaper Summaries\nHow To Write A Great Research Paper (Professor Simon Peyton Jones)\nJan 21, 2016\nA 30 minute\nlecture\non how to write a\ngreat\nresearch paper\nWhat I Learned\nOnce you have an idea, start writing right away\nIdentify what your contributions will be at the beginning of the writing process. This list can be updated as you go\nBe engaging and grab the reader in\nSave related works for the end\nUse plenty of examples as evidence\nThe majority of the paper should be the details (results/analysis)\nA reviewer should come away knowing exactly what the 1 main idea/contribution was\nGive credit instead of tearing down other similar works and identify your weaknesses in the paper\nAlways listen carefully to criticisms\nHave 1-2 people read each draft of your paper, being sure to have them tell you where they got lost\nAs a researcher, either you won\u2019t understand something and you will feel stupid and like a worm, or you will understand something and think it\u2019s too trivial and hence still feel like a worm; it turns out that the life of a PhD student is quite depressing.\nPatrick Emami\nPowered By\nGravity\nMade with\non\n{ { Jekyll } }\npemami4911\npatrickomid\nUpdates on my machine learning research, summaries of papers, and blog posts",
        "sourceType": "blog",
        "linkToPaper": "https://www.youtube.com/watch?v=g3dkRsTqdDA"
    }
}