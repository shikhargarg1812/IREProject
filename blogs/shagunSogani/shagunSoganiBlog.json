{
    "0": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Deep-Neural-Networks-for-YouTube-Recommendations",
        "transcript": "Deep Neural Networks for YouTube Recommendations \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nDeep Neural Networks for YouTube Recommendations\n2016\n\u2022\nMachine Learning\n\u2022\nRecommender Systems\n\u2022\nACM\n\u2022\nEngineering\n\u2022\nLatency\n\u2022\nML\n\u2022\nRanking\n\u2022\nRecommender\n\u2022\nScale\n\u2022\nSystems\n22 Mar 2021\nIntroduction\nThe paper describes YouTube\u2019s deep learning-based recommendation system.\nLink to the paper\nChallenges\nScale - Very large number of users and videos.\nFreshness - Very large number of videos uploaded every hour. The recommendation system should take these new videos into account as well.\nNoise - User satisfaction needs to be modeled from noisy implicit feedback signal as the explicit signal is very sparse.\nSystem Overview\nTwo neural networks: one for candidate generation and another one for ranking.\nMetrics\nOffline metrics like precision, recall, ranking loss\nA/B testing via live experiments\nCandidate Generation\nInput: events from a user\u2019s YouTube activity history.\nOutput: small subset (hundreds) of videos.\nApproach:\nRecommendation is modeled as extreme multiclass classification.\nPredict the video (from a corpus) that a user will watch at a given time.\nThe neural network\u2019s task is to learn useful user embeddings, given the user\u2019s context and history.\nFor each positive class (relevant video), negative classes (non-relevant videos) are sampled from the video corpus.\nModel Architecture\nA feedforward network with input as user embeddings and context embeddings (watch history).\nWatch history is a variable-length sequence of video ids, where each video id is mapped to an embedding.\nThe sequence of video ids is mapped to a sequence of embeddings, and this sequence is averaged to obtain fixed-sized embedding.\nAdditional signals like demographic features and search query embeddings can be added along with the context embeddings.\nThe age of a video is also used as a feature during training to account for the freshness of the content. This feature is set to zero (or slightly negative) during inference.\nOther Insights\nTraining examples are generated from all YouTube watches, including the watches from the videos embedded on other sites, to surface new content.\nGenerating the same number of training examples per user is important to avoid a small set of active users from dominating the model training.\nPredicting a user\u2019s next watch leads to better results than predicting a randomly held-out watch. This can be attributed to the general consumption pattern of videos (e.g., episodes are usually watched in order).\nRanking\nInput: list of candidate videos to rank from.\nOutput: score for each video.\nApproach\nA feedforward network (similar to candidate generation model) trained using logistic regression loss.\nFeature representation\nDifferent types of features: categorical vs. continuous, univalent vs. multivalent, describes video vs. describes user or context.\nImportant signals include user\u2019s interaction with the video (or similar videos), which source/channel added the video to the candidate set.\nEmbeddings are shared across features. For example, the representation for a video id remains the same, irrespective of whether it is being used for representing the \u201cvideo to recommend\u201d or the \u201clast seen video.\u201d\nFeature normalization and transformations like exponents (square or square root) for continuous variables improve the performance.\nTo model the expected watch time, the logistic regression loss is weighted by the observed watch time. For example, if a video was watched, its weight is given by the observed watch time, and if the video was not watched, its weight is set to 1.\nIn practice, this means that the logistic regression model learns odds that approximate the expected watch time of the video.\nRelated Posts\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nAd Click Prediction - a View from the Trenches\n01 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://research.google/pubs/pub45530/"
    },
    "1": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/The-Tail-at-Scale",
        "transcript": "The Tail at Scale \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nThe Tail at Scale\n2013\n\u2022\nDistributed Systems\n\u2022\nACM\n\u2022\nEngineering\n\u2022\nLatency\n\u2022\nScale\n\u2022\nSystems\n15 Mar 2021\nIntroduction\nThe paper presents some causes for (temporary) high-latency episodes in large-scale online systems and techniques to mitigate their impact so that the tail of latency distribution remains short.\nLink to the paper\nWhy does variability in response time exist\nShared resources between processes on the same node\nBackground processes (daemons) could use cause a momentary spike in resource usage.\nProcesses running on different nodes may contend for global resources like shared file systems.\nMaintenance activities like disk compaction or garbage collection.\nOthers like queueing, power limits, or energy management.\nIn the case of large-scale systems, the component-level variability is further amplified.\nReducing Component Variability\nUse differentiated service classes to prioritize user requests over non-interactive requests.\nReduce head-of-line blocking by breaking long-running requests into smaller requests.\nSynchronize maintenance jobs across nodes to minimize the window for high latency.\nCaching generally does not help to address tail latency.\nAdapting to Latency Variability\nTwo categories of adaptation approaches\nWithin Request Short-Term Adaptations\nThese approaches are more relevant for services that perform many read queries on loosely consistent datasets.\nHedged Request\nSend the request to multiple replicas, and once one of the replicas returns the result, cancel the other requests.\nIn practice, start by sending the request to only one replica. Send the secondary requests if the first request is outstanding for more than $95^{th}$ percentile of expected latency.\nThis introduces an additional $5\\%$ load while substantially shortening the latency tail.\nThis approach work because often, the cause of latency is not the query itself but other factors like overloaded nodes.\nTied Request\nHedged request approach makes a tradeoff regarding how long to wait before initiating requests to other replicas. The sooner the request is made, the lower should be the latency in serving the request, but more will be the overall load in the system.\nThe load in the system can be reduced by \u201ctieing\u201d requests (sent to different replicas) so that as soon as one replica starts processing the request, it can notify the other replicas, which could drop the request or deprioritize it.\nIn practice, \u201ctieing\u201d requests means that each replica has the identity of other replicas which may execute the request.\nNote that there is a short window (of the average network message delay) when multiple replicas could start executing the request. This can be mitigated if the client (issuing the requests) introduces a delay to twice the average network message delay.\nSubmit the request to the least loaded replica\nThis is less effective for reasons like the load on a replica can change after the request is made but before it is executed.\nCross-Request Long-Term Adaptations\nThese approaches are more relevant for situations where different services have different throughput.\nMicro-partitions\nGenerate more paritions than the number of nodes.\nThe partitions can be dynamically assigned to machines to ensure proper load balancing.\nIn case of machine failure, many nodes can be used to quickly re-create the micro-partitions instead of waiting on one machine to read one single large partition.\nSelective Replication\nWith micro-partitioning, replicas for micro-partitions can be created ahead of time to achieve good load balancing.\nLatency induced probation\nIn some cases, removing a slow node can improve the overall latency of the system. The probated node can be re-incorporated when its latency improves.\nLarge Information Retrieval Systems\nIn such systems, speed can be more critical than the quality of the result.\nThe system should return a \u201cgood enough\u201d result that is available with low latency instead of waiting for the \u201cbest result\u201d that is available with high latency.\nIn some cases, a request could trigger an unexpected code path or cause some other exception that could slow down the entire system.\nIn such cases, the\ncanary request\ntechnique can be used where the system sends the request initially to only 1 or 2 nodes. The request is sent over to the other nodes only after receiving a successful response from the initial nodes.\nRequests that update state are easier to handle for several reasons:\nThe scale of latency-critical modifications is generally small.\nThe update can be performed asynchronously after responding to the user.\nQuorum-based approaches (often used for ensuring consistent updates) are inherently tail-tolerant.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nAd Click Prediction - a View from the Trenches\n01 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://research.google/pubs/pub40801/"
    },
    "2": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Practical-Lessons-from-Predicting-Clicks-on-Ads-at-Facebook",
        "transcript": "Practical Lessons from Predicting Clicks on Ads at Facebook \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nPractical Lessons from Predicting Clicks on Ads at Facebook\n2014\n\u2022\nClick-Through Rate\n\u2022\nData Mining\n\u2022\nEmpirical Advice\n\u2022\nKDD 2014\n\u2022\nMachine Learning\n\u2022\nAds\n\u2022\nCTR\n\u2022\nEngineering\n\u2022\nKDD\n\u2022\nML\n\u2022\nScale\n\u2022\nSystems\n08 Mar 2021\nIntroduction\nThe paper describes several design choices for developing a model for predicting user response (clicks) on ads.\nLink to the paper\nExperimental Setup\nThe model is trained/evaluated on offline data.\nEvaluation metrics:\nNormalized Cross-Entropy (or Normalized Entropy, NE)\nDefined as the predictive log-loss per impression, divided by the entropy of the background CTR (click-through rate).\nBackground CTR is the average empirical CTR of the training data.\nLower normalized cross-entropy is better.\nThe normalization term is important to make the metric insensitive to the background CTR. Otherwise, the log loss can easily be made low when background CTR is close to 0 or 1.\nNE can also be written as $RIG - 1$, where $RIG$ is the Relative Information Gain.\nCalibration\nRatio of average estimated CTR and empirical CTR.\nArea-Under-ROC (AUC) is a good metric for measuring ranking quality (among ads). However, it is\nnot used\nas a metric to avoid over-delivery or under-delivery of ads.\nImplementation Details\nFeature Transformation\nA given add impression, $e$, is transformed into a $n-$dimensional vector, $x$, where the $i^{th}$ index denotes the value of the $i^{th}$ categorical feature.\nContinous features are binned, and the bin index is used as a categorical feature, thus applying a non-linear transformation to the features.\nCategorical features that are tuple-like (i.e., have a tuple of values) can be converted into new categorical features by taking a cartesian product.\nBoosted decision trees can be used to implement the previous two transformations in one go.\nEach tree is used as a categorical feature that takes the value of the index of the leaf node than an ad maps to.\nThe paper used the Gradient Boosting Machine with the $L_2-$TreeBoost algorithm.\nUsing the tree feature transformation improves the Normalized Cross-Entropy by $3.4\\%$.\nModel\nLogistic Regression (LR) or Bayesian online learning scheme for probit regression (BOPR) algorithms are used for training a linear classifier model.\nWhile both LR and BOPR models provide similar performance, the LR model is half the BOPR model\u2019s size and faster for performing training/inference.\nRole of Data Freshness\nWhen a model is trained on the data from a particular day and evaluated on data from the subsequent days, the model\u2019s performance degrades as the delay between training and test set increases.\nThis highlights the importance of the freshness of the training data.\nOne straightforward approach can be to train the model every day.\nAlternatively, the linear classifier can be trained using online learning, while the boosted decision tree can still be trained daily.\nDifferent choices for setting the learning rate (for online training of linear classifier) are compared, and the\nper-coordinate learning rate\nis found to perform best in practice.\nGenerating Real-Time Training Data\nAn \u201conline joiner\u201d system is used to generate real-time training data for the linear classifier.\nThe challenging part is, while there are data points with a \u201cpositive\u201d label (i.e., the user clicked on the ad), there are no datapoints with a \u201cnegative\u201d label (since there is no \u201cno-click\u201d button that the user can click).\nAn impression is considered to have the \u201cno-click\u201d label if the user does not click on the ad within a (long) time window of seeing the ad.\nToo short a time window could mislabel some impressions, while too long a time window will delay the real-time training data.\nThe online joiner performs a distributed stream-to-stream join on the stream of ad impressions and stream of ad clicks using a HashQueue.\nA HashQueue:\ncomprises of a First-In-First-Out queue as a buffer window and a hash map for fast random access to label impressions.\nsupports three operations on key-value pairs: enqueue, dequeue, and lookup.\nMemory and Latency\nIncreasing the number of boosting trees shows diminishing returns, and most of the improvements come from the first 500 trees.\nTop 10 features account for half of the total feature importance, while the last 300 features add less than 1% feature importance.\nFeatures in the boosting model can be broadly classified as contextual or historical.\nHistorical feature provides much more explanatory power than the contextual features through contextual features are helpful to handle the cold start problem.\nModels trained with just the contextual features rely more heavily on data freshness than models trained with just the historical features.\nUniform subsampling and negative downsampling techniques are used to limit the amount of training data.\nIn the case of negative downsampling, the model needs to be re-calibrated as well.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nAd Click Prediction - a View from the Trenches\n01 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/"
    },
    "3": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Ad-Click-Prediction-a-View-from-the-Trenches",
        "transcript": "Ad Click Prediction - a View from the Trenches \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nAd Click Prediction - a View from the Trenches\n2013\n\u2022\nClick-Through Rate\n\u2022\nData Mining\n\u2022\nEmpirical Advice\n\u2022\nKDD 2013\n\u2022\nMachine Learning\n\u2022\nAds\n\u2022\nCTR\n\u2022\nEngineering\n\u2022\nKDD\n\u2022\nML\n\u2022\nScale\n\u2022\nSystems\n01 Mar 2021\nIntroduction\nThe paper presents case studies from the experience of deploying an ad click-through rate (CTR) prediction model at Google.\nThe paper focuses on themes related to memory footprint, performance analysis, calibration, confidence in the predictions, and feature engineering.\nLink to the paper\nSystem Overview\nFeatures (corresponding to a given ad) include search query and the metadata in the ad. The features are very sparse.\nSingle layer, regularized Logistic Regression model is trained with Online Gradient Descent (same as Stochastic Gradient Descent, but in the online setting).\nFrom a memory perspective, it is important to minimize the size of the final model.\nAdding just the L1 penalty is not sufficient to produce weights that are precisely equal to 0.\n\u201cFollow The (Proximally) Regularized Leader\u201d algorithm or FTRL-Proximal algorithm\nis used to learn sparse models without losing on the accuracy.\nUsing per-coordinate learning rates improves the performance at the cost of memory as both the sum of gradients and the sum of the square of gradients are tracked for each feature.\nIn practice, some of the cost can be alleviated by approximating that all the events containing a given feature have the same probability.\nIn such a case, the sum of the square of gradients can be approximated using the counts of positive and negative events alone.\nSome memory overhead can be reduced based on the following observation: the vast majority of features are extremely rare. Hence, it is not necessary to track the statistics for such rare features.\nHowever, in an online setting, it is not known upfront as to which features will be sparse.\nThe paper proposes to use probabilistic feature inclusion - a feature is added to the model with probability $p$. Once it is added, the feature is not removed.\nAn alternative approach is to use a rolling set of counting Bloom filters to check if a feature has appeared at least $n$ times in training. Bloom filters are probabilistic data structures and can return false positives.\nMemory can also be saved by using fewer bits for encoding weights.\nMost of the weight coefficients lie in the range $(-2, 2)$, and a $16-$ bit encoding is used in place of $32$ or $64$ bit encoding.\nThis quantization approach needs to account for roundoff problems. The fix is easy to implement.\nWhen training many models with similar hyperparameters, per-model learning rate counters can be replaced by statistics shared by all the models, thus reducing memory footprint.\nA Single Value Structure is used to reduce the memory footprint when evaluating a very large set of model variants that differ only in addition/removal of a small subset of features.\nAll the models, that use a feature, share a single value structure corresponding to the feature. This reduces the memory overhead by order of magnitude.\nDuring the update, each model computes the weight updates corresponding to all the features that it is using. The updated weight is averaged across all the models and used to update the single value structure.\nSince CTR datasets are generally highly imbalanced, the training data (for the negative class) can be subsampled to reduce the amount of data to train over. The loss component (corresponding to negative class) can be appropriately scaled up.\nMetrics\nOffline metrics like AucLoss (1 - AUC), Log Loss, Squared Error\nOnline loss is computed on the new training data (new incoming traffic)\tbefore training on it.\nThe confidence in the model\u2019s prediction is estimated using a heuristic called\nuncertainty score\n. It can be measured using the dot product of the feature and the vector of learning rates.\nThe idea is that the learning rates already maintain a notion of uncertainty.\nFeatures for which the learning rate is high are the features for which uncertainty is also high.\nCalibrating Predictions\nThe calibration can be improved by applying correction functions $\\tau_d(p)$ where $p$ is the predicted CTR, and $d$ is an element of a partition of the training data.\n$\\tau$ can be modeled as $\\gamma^{\\kappa}$ where $\\gamma$ and $\\kappa$ are learned using Poisson regression.\nUnsuccessful Experiments\nAggressive feature hashing was tried to reduce the memory overhead. However, it leads to a significant loss in performance.\nUsing dropout did not help, probably because the features are sparse.\nUsing feature bagging hurt the AucLoss.\nFeature vector normalization did not improve performance, probably because of per-coordinate learning rates and regularization.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://research.google/pubs/pub41159/"
    },
    "4": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Anatomy-of-Catastrophic-Forgetting-Hidden-Representations-and-Task-Semantics",
        "transcript": "Anatomy of Catastrophic Forgetting - Hidden Representations and Task Semantics \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nAnatomy of Catastrophic Forgetting - Hidden Representations and Task Semantics\n2020\n\u2022\nCatastrophic Forgetting\n\u2022\nContinual Learning\n\u2022\nICLR 2021\n\u2022\nLifelong Learning\n\u2022\nReplay Buffer\n\u2022\nRepresentation Analysis\n\u2022\nAI\n\u2022\nCL\n\u2022\nICLR\n\u2022\nLL\n22 Feb 2021\nIntroduction\nThe paper studies the effect of catastrophic forgetting on representations in neural networks.\nLink to the paper\nSetup\nTechniques:\nRepresentational Similarity Measures\nLayer Freezing\nLayer Reset\nDatasets\nSplit CIFAR-10\nCIFAR-10 dataset is split into\nm\n(=2) tasks, where each task is a\nn\nway classification task.\nThe underlying network has a shared trunk with\nm\nheads, one head per task.\nSplit CIFAR-100 Distribution Shift\nEach task requires distinguishing between\nn\nCIFAR-100\nsuperclasses\nwith training/test data corresponding to a\nsubset\nof constituent classes.\nNetwork Architecture\nVGG, ResNet and DenseNet\nQuestions\nAre all representations (throughout the network) equally responsible for forgetting?\nHigher\nlayer (layers closer to the output) are the primary source of catastrophic forgetting.\nCentral Kernel Alignment (CKA)\ntechnique is used to compare the similarity between the layer representations, before and after training on the second task.\nHigher layer representations change significantly when training over two tasks while the lower layer representations remain stable.\nWhen finetuning on the second task, freezing the lower layers has only a minor effect on the accuracy of the second task.\nIn\nlayer reset\nexperiments, after training on the second task, the weights of some of the layers are reset to their values after training on the first task.\nResetting the weights of higher layers leads to significant improvement in the performance on the first task.\nDo common approaches for countering catastrophic forgetting work by stabilizing the higher layers?\nYes - both\nEWC\nand replay-based approaches counter catastrophic forgetting work by stabilizing the higher layers.\nThis is demonstrated by showing that as the quadratic penalty for EWC (or fraction of data from replay buffer) increases (to reduce catastrophic forgetting), the representations for higher layers change less during the second task.\nWhen training over a sequence of tasks, are similar tasks more likely to be forgotten than different tasks?\nSetup I\nTraining over a sequence of two binary classification tasks.\nTask 1: Two related classes (say\nship\nand\ntruck\n)\nTask 2: Two related classes, which may or may not be related to the classes for Task 1. For example, the classes could be\ncat\nand\nhorse\n(not related to classes of the first task)\nplane\nand\ncar\n(related to the classes of the first task)\nTraining over semantically similar tasks (here\nplane\nand\ncar\n) leads to less forgetting.\nSetup II\nTraining over a sequence of two classification tasks.\nTask 1: Four classes where the classes can be grouped into two groups (say\ndeer\n,\ndog\n,\nship\nand\ntruck\n)\nTask 2: Two related classes, which may be related to group 1 or group 2. For example, the classes could be two animals or two objects.\nAfter training on the second task, classes (from Task 1), which are in the different group as classes from Task 2, are forgotten less.\nConclusion\nTask representational similarity is a function of both underlying data and optimization procedure.\nForgetting is most severe for task representations of intermediate similarity.\nRepresentational similarity is necessary but not a sufficient condition for forgetting.\nHow does catastrophic forgetting change as the task similarity changes?\nIf the model learns different representations for dissimilar tasks, increasing dissimilarity can help to avoid forgetting.\nWhen training the two-task, two-class (per task) CIFAR-10 setup with an \u201cothers\u201d class (classes not already used in the setup), the forgetting is reduced.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2007.07400"
    },
    "5": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/When-Do-Curricula-Work",
        "transcript": "When Do Curricula Work? \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nWhen Do Curricula Work?\n2020\n\u2022\nCurriculum Learning\n\u2022\nICLR 2021\n\u2022\nAI\n\u2022\nEmpirical\n\u2022\nICLR\n15 Feb 2021\nIntroduction\nThe paper systematically investigates when does curriculum learning help.\nLink to the paper\nImplicit Curricula\nImplicit curricula refers to the order in which a network learns data points when trained using stochastic gradient descent, with iid sampling of data.\nWhen training, let us say that the model makes a correct prediction for a given datapoint in the $i^{th}$ epoch (and correct prediction in all the subsequent epochs). The $i^{th}$ epoch is referred to as the\nlearned iteration\nof the datapoint  (iteration in which the datapoint was learned).\nThe paper studied multiple models (VGG, ResNet, WideResNet, DenseNet, and EfficientNet) with different optimizers (Adam and SGD with momentum).\nThe resulting implicit curricula are broadly consistent within the model families, making the following discussion less dependent on the model architecture.\nExplicit Curricula\nWhen defining an explicit curriculum, three important components stand out.\nScoring Function\nMaps a data point to a numerical score of\ndifficulty\n.\nChoices:\nLoss function for a model\nlearned iteration\nEstimated c-score - It captures a given model\u2019s consistency to correctly predict a given datapoint\u2019s label when trained on an iid dataset (not containing the datapoint).\nThe three scoring functions are computed for two models on the CIFAR dataset.\nThe resulting six scores have a high Spearman Rank correlation. Hence for the rest of the discussion, only the c-score is used.\nPacing Function\nThis function, denoted by $g(t)$, controls the size of the training dataset at step $t$.\nAt step $t$, the model would be trained on the first $g(t)$ examples (as per the ordering).\nChoices: logarithmic, exponential, step, linear, quadratic, and root.\nOrder\nOrder in which the data points are picked:\nCurriculum\n- Ordering points from lowest score to highest and training on the easiest data points first.\nAnti Curriculum\n- Ordering points from highest score to lowest and training on the hardest data points first.\nRandom\n- Randomly selecting the data points to train on.\nObservations\nThe paper performed a hyperparameter sweep over 180 pacing functions and three orderings for three random seeds over the CIFAR10 and CIFAR100 datasets. For both the datasets, the best performance is obtained with random ordering, indicating that curricula did not give any benefits.\nHowever, the curriculum is useful when the number of training iterations is small.\nIt also helps with noisy data training (which is simulated by randomly permuting the labels).\nThe observations for the smaller CIFAR10/100 dataset generalize to slightly larger datasets like FOOD101 and FOOD101N.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2012.03107"
    },
    "6": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Continual-learning-with-hypernetworks",
        "transcript": "Continual learning with hypernetworks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nContinual learning with hypernetworks\n2019\n\u2022\nContinual Learning\n\u2022\nICLR 2020\n\u2022\nLifelong Learning\n\u2022\nAI\n\u2022\nCL\n\u2022\nHyperNetwork\n\u2022\nICLR\n\u2022\nLL\n08 Feb 2021\nIntroduction\nThe paper proposes the use of task-conditioned\nHyperNetworks\nfor lifelong learning / continual learning setups.\nThe idea is, the HyperNetwork would only need to remember the task-conditioned weights and not the input-output mapping for all the data points.\nLink to the paper\nAuthor\u2019s Implementation\nTerminology\n$f$ denotes the network for the given $t^{th}$ task.\n$h$ denotes the HyperNetwork that generates the weights for $f$.\n$\\Theta_{h}$ denotes the parameters of $h$.\n$e^{t}$ denotes the input task-embedding for the $t^{th}$ task.\nApproach\nWhen training on the $t^{th}$ task, the HyperNetworks generates the weights for the network $f$.\nThe current task loss is computed using the generated weights, and the candidate weight update ($\\Delta \\Theta_{h}$) is computed for $h$.\nThe actual parameter change is computed by the following expression:\n$L_{total} = L{task}(\\Theta_{h}, e^{T}, X^{T}, Y^{T}) + \\frac{\\beta_{output}}{T-1} \\sum_{t=1}^{T-1} | f_{h}(e^{t}, \\Theta_{h}^*) - f_{h}(e^{(t)}, \\Theta_{h} + \\Delta \\Theta_{h} ))|^2$\n$L_{task}$ is the loss for the current task.\n$(X^{T}, Y^{T})$ denotes the training datapoints for the $T^{th}$ task.\n$\\beta_{output}$ is a hyperparameter to control the regularizer\u2019s strength.\n$\\Theta_{h}^*$ denotes the optimal parameters after training on the $T-1$ tasks.\n$\\Theta_{h} + \\Delta \\Theta_{h}$ denotes the one-step update on the current $h$ model.\nIn practice, the task encoding $e^{t}$ is chunked into smaller vectors, and these vectors are fed as input to the HyperNetwork.\nThis enables the HyperNetwork to produce weights iteratively, instead of all at once, thus helping to scale to larger models.\nThe paper also considers the problem of inferring the task embedding from a given input pattern.\nSpecifically, the paper uses task-dependent uncertainty, where the task embedding with the least predictive uncertainty is chosen as the task embedding for the given unknown task. This approach is referred to as HNET+ENT.\nThe paper also considers using HyperNetworks to learn the weights for a task-specific generative model. This generative model will be used to generate pseudo samples for rehearsal-based approaches. The paper considers two cases:\nHNET+R where the replay model (i.e., the generative model) is parameterized using a HyperNetwork.\nHNET+TIR, where an auxiliary task inference classifier is used to predict the task identity.\nExperiments\nThree setups are considered\nCL1 - Task identity is given to the model.\nCL2 - Task identity is not given, but task-specific heads are used.\nCL3 - Task identity needs to be explicitly inferred.\nOn the permuted MNIST task, the proposed approach outperforms baselines like Synaptic Intelligence and Online EWC, and the performance gap is more significant for larger task sequences.\nForward knowledge transfer is observed with the CIFAR datasets.\nOne potential limitation (which is more of a limitation of HyperNetworks) is that HyperNetworks may be harder to scale for larger models like ResNet50 or transformers, thus limiting their usefulness for lifelong learning use cases.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.00695"
    },
    "7": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Zero-shot-Learning-by-Generating-Task-specific-Adapters",
        "transcript": "Zero-shot Learning by Generating Task-specific Adapters \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nZero-shot Learning by Generating Task-specific Adapters\n2021\n\u2022\nNatural Language Processing\n\u2022\nText-to-Text Transformer\n\u2022\nZero-Shot\n\u2022\nZero Shot Generalization\n\u2022\nAdapter\n\u2022\nAI\n\u2022\nHyperNetwork\n\u2022\nNLP\n\u2022\nTransformer\n01 Feb 2021\nIntroduction\nThe paper introduces HYPTER - a framework for zero-shot learning (ZSL) in text-to-text transformer models by training a\nHyp\nerNetwork\nto generate task-specific\nadap\nter\ns\nfrom task descriptions.\nThe focus is on\nin-task\nzero-shot learning (e.g., learning to predict an unseen class or relation) and not on\ncross-task\nlearning (e.g., training on sentiment analysis and evaluating on question-answering task).\nLink to the paper\nTerminology\nTask\n- a NLP task, like classification or question answering.\nSub-task\nA class/relation/question within a task.\nDenotes by a tuple $(d, D)$ where $d$ is the language description while $D$ represents the subtask\u2019s dataset.\nSetup\nDevelop ZSL approach for transfer to new subtasks within a task, using the task description available for each subtask.\nApproach\nHYPTER has two main parts:\nMain network\nA pretrained text-to-text network\nInstantiated as a BERT-Base/Large\nHyperNetwork\nGenerates the weights for adapter networks that will be plugged into the main network.\nHyperNetwork has two parts:\nEncoder\nEncodes the task description\nInstantiated as a RoBERTa-Base model\nDecoder\nDecodes the encoding into weights for multiple adapters (in parallel)\nInstantiated as a Feedforward Network\nThe model trains in two phases:\nMain network is trained on all the data by concatenating the task description with the input.\nAdapters are trained by sampling a task from the train set while keeping the main network frozen.\nExperiments\nWhile the idea is very promising and interesting, the evaluation felt quite limited. It uses just two datasets\nZero-shot learning from Task Descriptions\nand\nZero-shot Relation Extraction\nand shows some improvements over the baseline of directly finetuning with task descriptions as the prompt.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2101.00420"
    },
    "8": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/HyperNetworks",
        "transcript": "HyperNetworks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nHyperNetworks\n2016\n\u2022\nICLR 2017\n\u2022\nAI\n\u2022\nHyperNetwork\n\u2022\nICLR\n25 Jan 2021\nIntroduction\nThe paper explores HyperNetworks. The idea is to use one network (HyperNetwork) to generate the weights for another network.\nLink to the paper\nAuthor\u2019s implementation\nApproach\nStatic HyperNetworks - HyperNetworks for CNNs\nConsider a $D$ layer CNN where the parameters for the $j^{th}$ layer are stored in a matrix $K^j$ of the shape $N_{in}f_{size} \\times N_{out}f_{size}$.\nThe HyperNetwork is implemented as a two-layer linear network where the input is a layer embedding $z^j$, and the output is $K^j$.\nThe first layer (of the HyperNetwork) maps the input to $N_{in}$ different outputs using $N_{in}$ weight matrices.\nThe second layer maps the different $N_{in}$ inputs to $K_{i}$ using a shared matrix. The resulting $N_{in}$ (number of) $K_{i}$ matrices are concatenated to obtain $K^j$.\nAs a side note, HyperNetworks have much fewer params than the network for which it produces weights.\nIn a general case, the kernel dimensions (across layers) are not of the same size but integer multiples of some basic sizes. In that case, the HyperNetwork can generate kernels for the basic size, which can be concatenated to form larger kernels. This would require additional input embeddings but not require a change in the architecture of HyperNetwork.\nDynamic HyperNetworks - HyperNetworks for RNNs\nHyperRNNs/HyperLSTMs denote HyperNetworks that generates weights for RNNs/LSTMs.\nHyperRNNs implement a form of relaxed weight sharing - an alternative to the full weight sharing of the traditional RNNs.\nAt any timestamp $t$, the input to the HyperRNN is the concatenated vector $x_{t}$ (input to the RNN at time $t$) and the hidden state $h_{t-1}$ of the RNN. The output is the weight for the main RNN at timestep $t$.\nIn practice, a\nweight scaling vector\n$d$ is used to reduce the memory footprint, which would otherwise be $dim$ times the memory of a standard RNN. $dim$ is the dimensionality of the embedding vector $z_j$.\nExperiments\nHyperNetworks are used to train standard CNNs for MNIST and ResNets for CIFAR 10. In these experiments, HyperNetworks slightly underperform the best performing models but uses much fewer parameters.\nHyperLSTMs trained on the Penn Treebank dataset and Hutter Prize Wikipedia dataset outperform the stacked LSTMs and perform similar to layer-norm LSTMs. Interestingly, using HyperLSTMs with layer-norm improves performance over HyperLSTMs.\nGiven the similar performance of HyperLSTMs and layer-norm LSTMs, the paper conducted an ablation study to understand if HyperLSTMs learned a weight adjustment policy similar to the statistics-based approach used by layer-norm LSTMs.\nHowever, the analysis of the histogram of the hidden states suggests that using layer-norm reduces the saturation effect while in HyperLSTMs, the cell is saturated most of the time. This indicates that the two models are learning different policies.\nHyperLSTMs are also evaluated for handwriting sequence generation by training in the IAM online handwriting dataset.\nWhile HyperLSTMs are quite effective on this task, combining them with layer-norm degrades the performance.\nOn the WMT\u201914 En-to-Fr machine translation task, HyperLSTMs outperform LSTM based approaches.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1609.09106"
    },
    "9": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Energy-based-Models-for-Continual-Learning",
        "transcript": "Energy-based Models for Continual Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nEnergy-based Models for Continual Learning\n2020\n\u2022\nCatastrophic Forgetting\n\u2022\nContinual Learning\n\u2022\nEnergy-Based Models\n\u2022\nLifelong Learning\n\u2022\nReplay Buffer\n\u2022\nAI\n\u2022\nCL\n\u2022\nEBM\n\u2022\nLL\n18 Jan 2021\nIntroduction\nThe paper proposes to use Energy-based Models (EBMs) for Continual Learning.\nIn classification tasks, the standard approach uses a cross-entropy objective function along with a normalized probability distribution.\nHowever, cross-entropy reduces all negative classes\u2019 likelihood when updating the model for a given sample, potentially leading to catastrophic forgetting.\nClassification can be seen as learning an EBM across separate classes.\nDuring an update, the energy for a pair of samples and its ground truth class decreases while the energy corresponding to the pairs of sample and negative classes increases.\nUnlike the cross-entropy loss, EBMs allow choosing the negative classes to update.\nLink to the paper\nApplications of EBMs for Continual Learning\nEBMs can be used for class-incremental learning without requiring a replay-buffer or generative model for replay.\nEBMs can be used for continual learning in setups without task boundaries, i.e., setups where the data distribution can change without a clear separation between tasks.\nEBMs\nBoltzman distribution is used to define the conditional likelihood of label $y$, given an input $x$. ie, $p(y|x) = \\frac{exp(E(x, y))}{Z(x)}$ where $Z(x) = \\sum_{y \\in Y}(-E(x, y))$. Here $E$ is the learnt energy function that maps an input-label pair to a scalar energy value.\nDuring training, the contrastive divergence loss is used.\nDuring inference, the class, for which the input-class pair has the least energy, is selected as the predicted class.\nEBMs for Continual Learning\nSelection of Negative Samples\nThe paper considers several strategies for the selection of negative samples:\none negative class per sample. The negative class is sampled from the current batch of data. This selection approach performs best.\nall the negative classes in a batch are used for creating the negative samples.\nall the classes seen so far in training are used as the negative samples. This approach works the worst in practice.\nGiven the flexibility of sampling the negative classes, EBMs can be used in the boundary-agnostic setups (where the data distribution can change smoothly without an explicit task boundary).\nEnergy Network\nEBMs take both the sample and the class as the input. The class can be treated as an attention filter to select the most relevant information between the sample and the class.\nIn theory, EBMs can train for any number of classes without knowing the number of classes beforehand. This is an advantage over the softmax-based approaches, where adding new classes requires changing the size of the softmax output layer.\nInference\nDuring inference, all the classes seen so far are evaluated via the energy function. The class, which corresponds to the least energy sample-class pair, is returned as the selected class.\nExperiments\nDatasets\nSplit MNIST\nPermuted MNIST\nCIFAR-10\nCIFAR-100\nResults in Boundary-Aware Setting\nThe paper outperforms the standard continual learning approaches that neither uses a replay-buffer nor a generative model.\nAdditionally, the paper shows that for the same number of parameters, the effective capacity of EMB models is higher than the effective capacity of standard classification models.\nThe paper also shows that standard classification models tend to assign a high probability to new classes for both old and new data. EBMs assign the probability more uniformly (and correctly) across the classes.\nIn an ablation study, the paper shows that both label conditioning and contrastive divergence loss help in improving the performance of EBMs.\nResults in Boundary-Agnostic Setting\nThe performance gains in the boundary-agnostic setting are even more significant than the improvements in the boundary-aware setting.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2011.12216"
    },
    "10": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/GPipe-Easy-Scaling-with-Micro-Batch-Pipeline-Parallelism",
        "transcript": "GPipe - Easy Scaling with Micro-Batch Pipeline Parallelism \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nGPipe - Easy Scaling with Micro-Batch Pipeline Parallelism\n2018\n\u2022\nDistributed Computing\n\u2022\nModel Parallelism\n\u2022\nNeurIPS 2019\n\u2022\nAI\n\u2022\nEngineering\n\u2022\nNeurIPS\n\u2022\nScale\n\u2022\nSystems\n11 Jan 2021\nIntroduction\nThe paper introduces GPipe, a pipeline parallelism library for scaling networks that can be expressed as a sequence of layers.\nLink to the paper\nDesign\nConsider training a deep neural network with\nL\nlayers using\nK\naccelerators (say GPUs).\nEach of the\ni\nth\nlayer has its\nforward\nfunction\nf\ni\n,\nbackward\nfunction\nb\ni\n, weights\nw\ni\nand a cost\nc\ni\n(say the memory footprint or computational time).\nGPipe partitions this network into\nK\ncells and places the\ni\nth\ncell on the\ni\nth\naccelerator. Output from the\ni\nth\naccelerator is passed to the\ni+1\nth\naccelerator as input.\nDuring the forward pass, the input batch (of size\nN\n) is divided into\nM\nequal micro-batches. These micro-batches are pipelined through the\nK\naccelerators one after another.\nDuring the backward pass, gradients are computed for each micro-batch. The gradients are accumulated and applied at the end of each minibatch.\nIn batch normalization, the statistics are computed over each micro-batch (used during training) and mini-batch (used during evaluation).\nMicro-batching improves over the naive mode parallelism approach by reducing the underutilization of resources (due to the network\u2019s sequential dependencies).\nPerformance Optimization\nGPipe supports re-materialization (or checkpointing), i.e., during the forward pass, only the output activations (at partition boundaries) are stored.\nDuring backward pass, the forward function is recomputed at each accelerator. This trades off the memory requirement with increased time.\nOne potential downside is that partitioning can introduce some idle time per accelerator (referred to as the bubble overhead). However, with a sufficiently large number of micro-batches ( more than 4 times the number of partitions), the bubble overhead is negligible.\nPerformance Analysis\nTwo different types of model architectures are compared: AmoebaNet convolutional model and Transformer sequence-to-sequence model.\nFor AmoebaNet, the size of the largest trainable model (on a single 8GB Cloud TPU v2) increases from 82M to 318M. Further, a 1.8 billion parameter model can be trained on 8 accelerators (25x improvement in size using GPipe).\nFor transformers, GPipe scales the model size to 83.9 B parameters with 128 partitions (298x improvement in size compared to a single accelerator).\nSince the computation is evenly distributed across transformer layers, the training throughput scales almost linearly with the number of devices.\nQuantitative experiments on ImageNet and multilingual machine translation show that models can be effectively trained using GPipe.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1811.06965"
    },
    "11": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Compositional-Explanations-of-Neurons",
        "transcript": "Compositional Explanations of Neurons \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nCompositional Explanations of Neurons\n2020\n\u2022\nNatural Language Inference\n\u2022\nNeurIPS 2020\n\u2022\nAI\n\u2022\nCompositionality\n\u2022\nExplainability\n\u2022\nInterpretability\n\u2022\nNeurIPS\n\u2022\nNLI\n04 Jan 2021\nIntroduction\nThe paper describes a method to explain/interpret the representations learned by individual neurons in deep neural networks.\nThe explanations are generated by searching for logical forms defined by a set of composition operators (like OR, AND, NOT) over primitive concepts (like water).\nLink to the paper\nGenerating compositional explanations\nGiven a neural network\nf\n, the goal is to explain a neuron\u2019s behavior (of this network) in human-understandable terms.\nPrevious work\nbuilds on the idea that a good explanation is a description that identifies the inputs for which the neuron activates.\nGiven a set of pre-defined atomic concepts $c \\in C$ and a similarity measure $\\delta(n, c)$ where $n$ represents the activation of the $n^{th}$ neuron, the explanation, for the $n^{th}$ neuron, is the concept most similar to $n$.\nFor images, a concept could be represented as an image segmentation map. For example, the water concept can be represented by the segments of the images that show water.\nThe similarity can be measured by first thresholding the neuron activations (to get a neuron mask) and then computing the IoU score (or Jaccard Similarity) between the neuron mask and the concept.\nOne limitation of this approach is that the explanations are restricted to pre-defined concepts.\nThe paper expands the set of candidate concepts by considering the logical forms of the atomics concepts.\nIn theory, the search space would explode exponentially. In practice, it is restricted to explanations with at most $N$ atomics concepts, and beam search is performed (instead of exhaustive search).\nSetup\nImage Classification Setup\nNeurons from the final 512-unit convolutional layer of a ResNet-18 trained on the\nPlaces365 dataset\n.\nProbing for concepts from\nADE20k scenes dataset\nwith atomic concepts defined by annotations in the\nBroden dataset\nNLI Setup\nBiLSTM baseline followed by MLP layers trained on\nStanford Natural Language Inference (SNLI) corpus\n.\nProbing the penultimate hidden layer (of the MLP component) for sentence-level explanations.\nConcepts are created using the 2000 most common words in the validation split of the SNLI dataset.\nAdditional concepts are created based on the lexical overlap between premise and hypothesis.\nDo neurons learn compositional concepts\nImage Classification Setup\nAs $N$ increases, the mean IoU increases (i.e., the explanation quality increases) though the returns become diminishing beyond $N=10$.\nManual inspection of 128 neurons and their length 10 explanations show that 69% neurons learned some meaningful combination of concepts, while 31% learned some unrelated concepts.\nThe meaningful combination of concepts include:\nperceptual abstraction that is also lexically coherent (e.g., \u201cskyscraper OR lighthouse OR water tower\u201d).\nperceptual abstraction that is not lexically coherent (e.g., \u201ccradle OR autobus OR fire escape\u201d).\nspecialized abstraction of the form L1 AND NOT L2 (e.g. (water OR river) AND NOT blue).\nNLI Setup\nAs $N$ increases, the mean IoU increases (as in the image classification setup) though the IoU keeps increasing past $N=30$.\nMany neurons correspond to lexical features. For example, some neurons are gender-sensitive or activate for verbs like sitting, eating or sleeping. Some neurons are activated when the lexical overlap between premise and hypothesis is high.\nDo interpretable neurons contribute to model accuracy?\nIn image classification setup, the more interpretable the neuron is, the more accurate is the model (when the neuron is active).\nHowever, the opposite trend is seen in NLI models. i.e., the more interpretable neurons are less accurate.\nKey takeaway - interpretability (as measured by the paper) is not correlated with performance. Given a concept space, the identified behaviors may be correlated or anti-correlated with the model\u2019s performance.\nTargeting explanations to change model behavior\nThe idea is to construct examples that activate (or inhibit) certain neurons, causing a change in the model\u2019s predictions.\nThese adversarial examples are referred to as \u201ccopy-paste\u201d adversarial examples.\nFor example, the neuron corresponding to \u201c(water OR river) AND (NOT blue)\u201d is a major contributor for detecting \u201cswimming hole\u201d classes. An adversarial example is created by making the water blue. This prompts the model to predict \u201cgrotto\u201d instead of \u201cswimming hole.\u201d\nSimilarly, in the NLI model, a neuron detects the word \u201cnobody\u201d in the hypothesis as highly indicative of contradiction. An adversarial example can be created by adding the word \u201cnobody\u201d to the hypothesis, prompting the model to predict contradiction while the true label should be neutral.\nThese observations support the hypothesis that one can use explanations to create adversarial examples.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2006.14032"
    },
    "12": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Design-patterns-for-container-based-distributed-systems",
        "transcript": "Design patterns for container-based distributed systems \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nDesign patterns for container-based distributed systems\n2016\n\u2022\nDesign Pattern\n\u2022\nDistributed Systems\n\u2022\nSoftware Engineering\n\u2022\nContainer\n\u2022\nEngineering\n\u2022\nScale\n\u2022\nSystems\n\u2022\nUSENIX\n21 Dec 2020\nIntroduction\nThe paper describes three design patterns for container-based distributed systems: single-container pattern, single-node pattern, and multi-node pattern.\nLink to the paper\nSingle-container management patterns\nTraditionally, containers have exposed three functions: run, pause and stop.\nA richer API can be implemented to provide fine-grained control to system developers and operators.\nSimilarly, much more application information (including monitoring metrics) can be exposed.\nThe container interface can be used to define a contract for a complex lifecycle. For example, instead of arbitrarily shutting down the container, the system could signal that it will be terminated, giving the container some time to perform some cleanup/follow-up actions.\nSingle-node, multi-container pattern\nSidecar pattern\nMultiple containers extend and enhance the main container.\nFor example, a web-server serves from the local disk (main container) while a side container updates the data.\nBenefits:\nindependent development, deployment, and scaling of containers\npossibility of combining different type of containers\nfailure containment boundary, i.e., one failing container, need not bring down the entire system.\nAmbassador pattern\nProxy communication to and from the main container with the ambassador hiding the complexities of communication with a distributed (multi-shard system) that may be written in a different language.\nAdapter pattern\nStandardize output and interfaces across the containers to provide a simple, homogenized view to external applications.\nA common example is using a single tool for collecting/processing metrics from multiple applications.\nThis is different from the adapter pattern, which aims to provide a simplified view of the external world to an application.\nMulti-node application patterns\nLeader election pattern\nIn a sharded (or replication-based) system, the system may have to elect a leader (or multiple leaders) among the replicas (or shards).\nInstead of using a leader election library, a leader election container can be used (that communicates with other containers over, say, HTTP). This removes the restriction of using a leader election library compatible with the containers (e.g., using the same language).\nWork queue pattern\nA work coordinator container can queue different containers, each of which may have a different implementation or dependencies, thus removing the restriction that all the works use the same runtime.\nScatter/gather pattern\nAn external client sends a request to a root container.\nThis container fans out the request to many containers that may perform the computation in parallel.\nThe root container gathers these parallel computations\u2019 results and aggregates them into a response to the external client.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/burns"
    },
    "13": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Cassandra-a-decentralized-structured-storage-system",
        "transcript": "Cassandra - a decentralized structured storage system \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nCassandra - a decentralized structured storage system\n2010\n\u2022\nBig Data\n\u2022\nDistributed Systems\n\u2022\nSoftware Engineering\n\u2022\nApache\n\u2022\nCAP\n\u2022\nData\n\u2022\nDatabase\n\u2022\nDBMS\n\u2022\nEngineering\n\u2022\nScale\n\u2022\nSoftware\n\u2022\nSystems\n14 Dec 2020\nIntroduction\nCassandra is a distributed storage system that runs over cheap commodity servers and handles high write throughput while maintaining low latency for read operations.\nAt the time of writing, it was used to support the search for Facebook Inbox.\nLink to the paper\nLink to the implementation\nData Model\nA table is a distributed multidimensional map.\nThe key is a string (generally 16-36 bytes long), while the value is a structured object.\nEvery operation under a single row key is atomic per replica.\nColumns are grouped together into sets called column families.\nThere are two types of columns families:\nSimple families.\nSuper column families: visualized as a column family within a column family.\nColumns can be sorted by name or time (used to display results in time sorted order).\nThe API supports insert, get and delete operations.\nSystem Architecture\nHandling Requests\nAny read/write request gets routed to any node in the cluster. The node determines the replicas for a given key and routes the request.\nFor write query, the system waits for a quorum of replicas to acknowledge the writes\u2019 completion.\nFor read query, the system either routes the requests to the closest replica (might fetch stale results) or routes the requests to all replicas and waits for a quorum of responses.\nPartitioning\nCassandra partitions data across the cluster using consistent hashing with an order-preserving hash function.\nThe hash function\u2019s output range is treated as a fixed circular ring, and each node is assigned a random position on the ring.\nAn incoming request specifies a key used to route requests.\nOne benefit of this approach is that the addition/removal of a node only affects its immediate neighbors.\nHowever, randomly assigning nodes leads to non-uniform data and load distribution.\nCassandra uses the load information and moves lightly loaded nodes to reduce the load on other nodes.\nReplication\nEach data item is replicated at N hosts, where N is the per-instance replication factor.\nCassandra supports the following replication policies: Rack Unaware, Rack Aware (within a datacenter), and Datacenter Aware.\nFor \u201cRack Aware\u201d and \u201cDatacenter Aware\u201d strategies, Zookeeper elects a leader among the nodes and holds metadata about which range a node is responsible for.\nIn case of node failure and network partitions, the quorum requirements are relaxed.\nMembership\nCluster membership is based on Scuttlebutt, a very efficient anti-entropy Gossip based mechanism.\nCassandra uses a modified version of $\\phi$ Accrual Failure Detector for detecting failures, which provides the suspicion level (of failure) for each node.\nBootstrapping\nA node, starting for the first time, chooses a random position in the ring.\nThis information is persisted on the local disk, on Zookeeper, and gossiped around the cluster (so any node can route any query in the cluster).\nDuring bootstrapping, the newly joined node reads a list of contact points (within the cluster) using a configuration file.\nLocal Persistence\nGenerally, a write operation involves a write into a commit log (for durability and recoverability), followed by a write into the in-memory data structures.\nA read operation starts with querying the in-memory data and then looks into the filesystem.\nRead queries on the filesystem use bloom filters.\nColumn indices are maintained to make it faster to look up relevant columns.\nImplementation Details\nComponents implemented in Java.\nSystem control messages use UDP while messages for replication and request routing uses TCP.\nA new commit log is rolled out after the older one exceeds 128MB of size.\nAll the data is indexed using a primary key.\nData on the disk is chunked into sequences of blocks. Each block contains at most 128 keys and is demarcated by a block index.\nWhen the data is written to the disk, a block index is generated and maintained in the memory for faster access.\nA compaction process is performed to merge multiple files (on disk) into one file.\nPractical Experience\nData from MySQL servers is added to Cassandra using MapReduce processes.\nAlthough Cassandra is a completely decentralized system, adding some coordination (via Zookeeper) is helpful.\nFor Inbox Search, a per-user index is maintained for all the messages.\nFor \u201cterm search\u201d, the key is the userid, and the words in the message become the super column.\nFor searching all the messages ever sent/received by a user, the key is the userid, and the recipient ids are the super columns.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://dl.acm.org/doi/10.1145/1773912.1773922"
    },
    "14": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/CAP-twelve-years-later-How-the-rules-have-changed",
        "transcript": "CAP twelve years later - How the rules have changed \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nCAP twelve years later - How the rules have changed\n2012\n\u2022\nBig Data\n\u2022\nDistributed Systems\n\u2022\nACID\n\u2022\nBASE\n\u2022\nCAP\n\u2022\nDatabase\n\u2022\nDBMS\n\u2022\nEngineering\n\u2022\nIEEE\n\u2022\nLatency\n\u2022\nScale\n\u2022\nSystems\n07 Dec 2020\nIntroduction\nThe CAP theorem states that any system sharing data over the network can only have at most two (out of three) desirable properties:\nconsistency (C), i.e., a single, up-to-date copy of the data;\nhigh availability (A) of that data (for updates); and\ntolerance to network partitions (P).\nThis \u201c2 of 3\u201d formulation is misleading as it oversimplifies the interplay between properties.\nLink to the paper\nACID vs. BASE\nACID is a design philosophy that focuses on consistency as reflected in the traditional relational databases.\nThe four properties in ACID are:\nAtomicity (A), i.e., the operations are atomic, and either the entire operation succeeds or none of it succeeds.\nConsistency (C), i.e., a transaction preserves all the rules. Note that the consistency in CAP is a subset of consistency in ACID.\nIsolation (I), i.e., transactions occur in isolation and do not affect each other.\nDurability (D), i.e., the transactions are durable irrespective of system failure.\nBASE is an alternate design philosophy that focuses on availability as reflected in the NoSQL databases.\nThe four properties in BASE are:\nBasic Availability (BA), i.e., the database appears to work most of the time.\nSoft state (S), i.e., the system\u2019s state can change over time as it becomes eventually consistent.\nEventual consistency (E), i.e., the system will eventually become consistent over time.\nCAP confusion\nGenerally, partitionability is seen as a must-have, thus reducing the choice to be between availability and consistency.\nThis view is somewhat misleading because the choice between C, A, and P is not binary but granular.\nThe choice between C and A can occur at various granularity levels, and different components (of a larger system) can prioritize different aspects.\nSimilarly, the CAP theorem generally ignores latency even though it is closely related to partitionability. For example, failing to achieve consistency within a time-bound (i.e., latency) implies a partition.\nIn general, there is no global notion of partition - some subset of nodes may experience a partition, and others may not.\nOnce a partition is detected, the system can then choose between C and A.\nManaging Partitions\nThree-step process for managing partitions:\nDetect the start of a partition.\nEnter an explicit partition mode that may limit some operations.\nPossible strategies:\nReduce availability by limiting some operations.\nRecord extra information that can be used during partition recovery.\nThe strategy depends on the invariants that the system should maintain.\nFor example, if the invariant is that the keys (in a table) should be unique, the system could allow duplicate keys for some time and perform a de-duplication step during partition recovery.\nA counterexample is a monetary transaction (e.g., charging a credit card). In such cases, the system could disable the operation and record it for performing later. Sometimes this \u201cunavailability\u201d is not visible to the user.\nHistory of operations (over replicas across different partitions) can be tracked using version vectors of the form (node, logical time). The system can easily recreate the order in which they were executed (or mark them as being concurrent).\nInitiate partition recovery when communication is restored and make the state across the partitions consistent.\nOne common approach is to revert to the state when the partition was detected and apply the operations consistently across all the replicas.\nThis may require some extra effort to merge conflicts.\nOne workaround can be to constrain the use of certain operations so that the system does not encounter merge conflicts during recovery.\nSometimes, certain invariants may be violated when the system is in the partition mode and needs to be fixed during recovery.\nThe key takeaway is that when partitions exist, the choice between availability and consistency is not binary, and both can be optimized for.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://ieeexplore.ieee.org/abstract/document/6133253"
    },
    "15": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Consistency-Tradeoffs-in-Modern-Distributed-Database-System-Design",
        "transcript": "Consistency Tradeoffs in Modern Distributed Database System Design \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nConsistency Tradeoffs in Modern Distributed Database System Design\n2012\n\u2022\nBig Data\n\u2022\nDistributed Systems\n\u2022\nSoftware Engineering\n\u2022\nCAP\n\u2022\nData\n\u2022\nDatabase\n\u2022\nDBMS\n\u2022\nEngineering\n\u2022\nIEEE\n\u2022\nLatency\n\u2022\nScale\n\u2022\nSoftware\n\u2022\nSystems\n30 Nov 2020\nIntroduction\nCAP theorem has been influential in the design decisions for distributed databases.\nHowever, designers incorrectly assume that the CAP theorem \u201calways\u201d imposes restrictions in terms of the tradeoff between availability and consistency. In contrast, the tradeoff is applicable only in the case of partitions.\nCAP theorem led to the development of highly available systems with reduced consistency models (and reduced ACID guarantees).\nAnother tradeoff - between latency and consistency - has also been influential for database design.\nThe paper unifies CAP and latency-consistency tradeoffs into a single formulation called PACELC.\nNote that some of the observations, especially ones about the databases, may be outdated now (the paper was written in 2012). However, the core message is still relevant.\nLink to the paper\nLatency-Consistency Tradeoff\nLow latency (or high availability) means that the system must replicate data.\nIn case of an update query, three possibilities arise:\nThe system can choose to send data updates to all the replicas at once. This leads to two possibilities:\nA replica can receive the update queries in an arbitrary order, thus breaking consistency with other replicas.\nAlternatively, the replicas could use some protocol to agree on the order of updates. However, this can introduce latency.\nThe update queries can be first sent to a master replica.\nThe master replica can apply the updates and send them to the other replicas using one of the following strategies:\nSynchronous replication where the master waits for all the updates to be applied to a replica(s). However, this approach introduces latency.\nAsynchronous replication where the master assumes the update to be complete before it completes. In this case, the latency-consistency tradeoff depends on how read queries are handled:\nThe system can send all read queries to the master. In this case, there are no consistency issues, but additional latency is introduced because all the read queries go to the same replica, thus potentially overloading it.\nAlternatively, the read query can be served from any replica. While this improves read latency, the results can be inconsistent now.\nUse a mix of Synchronous and Asynchronous replication - i.e., some of the write queries are Synchronous, and others are Asynchronous. In this case, the latency-consistency tradeoff depends on how read queries are handled:\nIf the read is routed to at least one replica that has been Synchrnously updated, the consistency can be preserved, with additional latency for discovering the updated replica, etc.\nIf the read query can not be routed to an updated replica (maybe because none of the replicas is updated), then either latency suffers or inconsistent read can be performed.\nThe update query is first sent to an arbitrary replica.\nThis is the same as the previous case, with the query going to an arbitrary replica instead of the master replica, and suffers from the same latency issues as the last case.\nIn a nutshell, the tradeoff between latency and consistency  is always present, irrespective of network failure.\nThis contrasts with the CAP theorem, which imposes the tradeoff between availability and consistency only in the case of a network partition.\nPACELC\nIf there is a partition (P), how does the system tradeoff availability (A) and consistency (C); else (E), when the system is running without failures, how does the system tradeoff latency (L) and consistency (C)?\nThe latency-consistency tradeoff (ELC) is relevant only when the data is replicated.\nDefault versions of Dynamo, Cassandra, and Riak were PA/EL systems, i.e., if a partition occurs, availability is prioritized. In the absence of partition, lower latency is prioritized.\nFully ACID systems (VoltDB, H-Store, and Megastore) and others like BigTable and HB are PC/EC, i.e., they prioritize consistency and give up availability and latency.\nMongoDB can be classified as a PA/EC system, while PNUTS is a PC/EL system.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://www.cs.umd.edu/~abadi/papers/abadi-pacelc.pdf"
    },
    "16": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Exploring-Simple-Siamese-Representation-Learning",
        "transcript": "Exploring Simple Siamese Representation Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nExploring Simple Siamese Representation Learning\n2020\n\u2022\nSelf Supervised\n\u2022\nAI\n\u2022\nCV\n\u2022\nImageNet\n\u2022\nSiamese\n\u2022\nSSL\n\u2022\nUnsupervised\n23 Nov 2020\nIntroduction\nThe paper shows that Siamese networks can be used for unsupervised learning with images without needing techniques like negative sample pairs, large batch training, or momentum encoders. The training mechanism is referred to as the SimSiam method.\nLink to the paper\nMethod\nGiven an input image\nx\n, create two augmented views\nx1\nand\nx2\n.\nThese views are processed by an encoder network\nf\n.\nOne of the views (say\nx1\n) is processed by the encoder\nf\nas well as a predictor MLP\nh\nto obtain a projection\np1\nie\np1 = h(f(x1))\n.\nThe second view (\nx2\n) is processed only by the encoder\nf\nto obtain an encoding\nz2\ni.e.,\nz2 = f(x2)\n.\nNegative cosine similarity is minimized between\np1\nand\nz2\nwith the catch that the resulting gradients are not used to update the encoder via\nz2\n. I.e., Loss =\nD(p1, stopgrad(z2))\nwhere\nD\nis the negative cosine similarity and\nstopgrad\nis an operation that stops the flow of gradients.\nIn practice, both\np1, z2\nand\np2, z1\npairs are used for computing the loss. ie  Loss =\n0.5 * (D(p1, stopgrad(z2)) + D(p2, stopgrad(z1)))\n.\nImplementation Details\nEncoder uses batch norm in all the layers (including output) while projection MLP uses batch norm only in the hidden layers.\nSGD optimizer with learning rate as\n0.05 * batchsize / 256\n, cosine learning rate decay schedule and SGD momentum = 0.9.\nUnsupervised pretraining on the ImageNet dataset followed by training a supervised linear classifier on the frozen representations.\nResults\nStop-gradient operation is necessary to avoid a degenerate solution. Without stop-gradient, the model maps all inputs to a constant\nz\n.\nIf the projection layer is removed, the method does not work (because of the loss\u2019s symmetric nature). If the loss is also made asymmetric, the method still does not work without the projection layer. However, asymmetric loss + projection layer works.\nKeeping the projection layer fixed (i.e., not updating during training) avoids collapse but leads to poor validation performance.\nTraining the projection layer with a constant learning rate works better in practice, likely because the projection layer needs to keep adapting before the encoder layer is sufficiently trained.\nThe method works well across different batch sizes.\nRemoving batch norm layers from all the layers in all the networks does not lead to collapse, though the model\u2019s performance degrades on the validation dataset. Adding batch norm to the hidden layers alone is sufficient.\nAdding batch norm to the encoder\u2019s output further improves the performance but adding batch norm to all the layers of all the networks makes the training unstable, with the loss oscillating.\nOverall, while batch norm helps to improve performance, it is not sufficient to avoid collapse.\nThe setup does not collapse when the cross-entropy loss replaces the cosine loss.\nWhat is SimSiam solving?\nGiven that the stop-gradient operation seems to be the critical ingredient for avoiding collapse, the paper hypothesizes that SimSiam is solving a different optimization problem.\nThe hypothesis is that SimSiam is implementing an Expectation-Maximisation (EM) algorithm with two sets of variables and two underlying sub-problems.\nThe paper performs several experiments to test this hypothesis. For example, they consider\nk\nSGD steps for the first problem before performing an update for the second problem, showing that the alternating optimization is a valid formulation, of which SimSiam is a particular case.\nComparison to other methods\nSimSiam achieves the highest accuracy among SimCLR, MoCo, BYOL, and SwAV for training under 100 epochs. However, it lags behind other methods when trained longer.\nSimSiam\u2019s representations are transferable beyond the ImageNet tasks.\nAdding projection layer and stop-gradient operator to SimCLR does not improve its performance.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2011.10566"
    },
    "17": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Data-Management-for-Internet-Scale-Single-Sign-On",
        "transcript": "Data Management for Internet-Scale Single-Sign-On \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nData Management for Internet-Scale Single-Sign-On\n2006\n\u2022\nBig Data\n\u2022\nDistributed Systems\n\u2022\nKey Value\n\u2022\nSoftware Engineering\n\u2022\nData\n\u2022\nDatabase\n\u2022\nDBMS\n\u2022\nEngineering\n\u2022\nScale\n\u2022\nSoftware\n\u2022\nSSO\n\u2022\nSystems\n\u2022\nUSENIX\n16 Nov 2020\nIntroduction\nThe paper describes the architecture of an erstwhile single-sign-on (SSO) service used by Google, called Google Accounts (2006).\nNote that some of the metrics and design decisions may be outdated now (the paper was written in 2006). However, the core message is still relevant.\nLink to the paper\nOperational Constraints\nSSO\u2019s availability affects the availability of all applications that require user sign-in.\nGenerally, systems can achieve high availability by sacrificing consistency, but given the nature of SSO (matching username/passwords), providing an inconsistent view is not a good option, and single-copy consistency is a usability requirement.\nBerkeley DB\nBerkeley DB is an embedded, high-performance, scalable, transactional storage system for key-value data and provides both keyed and sequential lookup.\nIt provides a primary copy replication model with a single writer (called master) and multiple read-only replicas.\nAll writes are sent to the master, which first applies the changes and then propagates them to the replicas.\nThe master and the replicas have identical logs, and in case of master failure, a new master is elected from the replicas.\nSome synchronization may be needed between the replicas in case, e.g., the master dies in between a transaction.\nSSO Architecture\nSSO service maps usernames to user account data and services to service-specific data.\nThe SSO database is partitioned into shards, where each shard is a replicated Berkeley DB (having 5 to 15 replicas).\nEach replica stores the data in a B+-link tree data structure.\nConsistent reads must go to the master, while non-master replicas can serve \u201c stale\u201d reads.\nIn the case of larger replication groups (say 15 replicas), only a subset of replicas can become master (\u201celectable replicas\u201d).\nIn general, replicas are spread geographically to handle machine-failure, network-failure, and data center-failure.\nReplicas in a share are kept close to reduce the communication latency, which affects the time to commit a write operation or electing a new master.\nSome of the shards implement ID-map, i.e., map of username to userid and userid to shards.\nDatabase Integration\nBerkeley DB leaves decisions regarding quorums, leases, etc., up to the application.\nQuorums\nSSO chooses a quorum protocol that guarantees that updates are never lost.\nFor the write queries, the master waits for a positive acknowledgment from a majority of the replicas, including itself, before marking the query as completed.\nWhen selecting a new leader, SSO requires a majority of replicas to agree. Moreover, Berkeley DB elections always choose a replica with the latest log entry during an election, thus guaranteeing that the new master\u2019s log will include all the previous master\u2019s updates.\nLeases\nThe master holds a\nmaster lease\nwhen responding to read queries and refreshes this lease periodically by communicating with a majority of replicas.\nThe lease guarantees that the master is not returning stale data if a partition or failure causes the master to lose its mastership, i.e., holding the lease guarantees that the master is still the master.\nMoreover, elections can not be completed within the lease timeout interval.\nReplica Group Membership\nSSO maintains a replica configuration containing the logical (DNS) name and IP address of each replica.\nIn case of any changes to the configuration, the changes are specified in a file that the master reads periodically.\nIf the configuration changes, the master initiates a configuration change and update the database.\nNon-master replicas can get the new configuration from the database.\nA new replica or a replica that lost state (say due to a failure) starts as a non-voting replica and can not participate in an election till it has caught up with the master as of the time the replica joined (again).\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://www.usenix.org/legacy/event/worlds06/tech/prelim_papers/perl/perl.pdf"
    },
    "18": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Searching-for-Build-Debt-Experiences-Managing-Technical-Debt-at-Google",
        "transcript": "Searching for Build Debt - Experiences Managing Technical Debt at Google \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nSearching for Build Debt - Experiences Managing Technical Debt at Google\n2012\n\u2022\nBuild System\n\u2022\nSoftware Engineering\n\u2022\nTechnical Debt\n\u2022\nEngineering\n\u2022\nIEEE\n\u2022\nSoftware\n\u2022\nSystems\n09 Nov 2020\nIntroduction\nThe paper describes the efforts to control and repay the technical debt in the build system at Google (called the Build Debt).\nGuiding Principles:\nAutomate techniques to analyze and fix issues that contribute to technical debt.\nMake it easier to do the right thing as developers can incur technical debt unknowingly.\nMake it hard to do the wrong thing, e.g., by building stricter checks into the build process.\nNote that some of the metrics and design decisions may be outdated now (the paper was written in 2012). However, the core message is still relevant.\nLink to the paper\nGoogle\u2019s Build System Debt\nBUILD files encapsulate the specifications for building software.\nGenerally, these files are maintained manually, and the dependencies may not be up-to-date over time.\nIn extreme cases, some of the build targets are not built for months. Such targets are called zombie targets.\nOriginally, any project could depend on any other project\u2019s internal details, thus creating (sometimes unwanted) couplings.\nIf the lower-level project did not intend to expose some internal details, the unwanted couplings introduce technical debt and make it harder to modify the lower-level project.\nOne form of technical debt is the visibility debt or the cost of back-fitting visibility rules onto the existing build specifications to re-establish the appropriate encapsulations.\nAnother example of technical debt is dead code that can confuse the developers looking for useful APIs.\nDependency Debt\nOver-declared\nor\nunderutilized\ndependencies can slow the build and testing of systems.\nUnder-declared\ndependencies can make the build process brittle and make it difficult to remove\nover-declared\ndependencies.\nPotential solutions for\nover-declared\ndependencies include:\nSetting aside some dedicated time for fixing build rules. But this approach is not automated, and potential breakages make it harder for developers to do the right thing.\nAutomatically add all the\nunder-declared\ndependencies to the BUILD files. The system can raise an error if a direct dependency is missing, making it harder to do the wrong thing.\nAutomation can be applied for finding/reporting the over-declared dependencies as well.\nPotential solutions for\nunderutilized\ndependencies include:\nWhile it is challenging to automate fixing\nunderutilized\ndependencies, automating the discovery of such dependencies is still useful.\nHighlighting dependencies with high cost and low removal effort could incentivize developers to clean up their projects.\nZombie Targets\nZombie targets can be identified by query the results of build and test runs.\nA target is marked as \u201cdead\u201d if the attempts to build it have failed for at least 90 days. Until then, build errors are considered to be transient.\nA zombie target can be eliminated by deleting its definition from the BUILD and deleting the source files, which are reachable only via the zombie target.\nVisibility Debt\nOriginally, the default visibility of all the targets was public, leading to unintended dependencies.\nThe visibility of all the existing builds was set to\nlegacy_public\n, and the default visibility was changed to private.\nThis encouraged developers to explicitly consider if they wanted other projects to depend on their project.\nDead Flags\nGoogle developed its command-line parsing utilities and defined a set of recognized command-line flags for libraries and binaries.\nOvertime, the number of flags grew to half a million, and many of these flags are not useful anymore (i.e., dead).\nThese dead flags can it hard to understand and refactor code.\nExisting flags are analyzed to check which ones have always been set to the same value and replaced by those contents, clearing about 150 thousand flags.\nRemoving dead flags also helps to clean up dead/unreachable code.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37755.pdf"
    },
    "19": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/One-Solution-is-Not-All-You-Need-Few-Shot-Extrapolation-via-Structured-MaxEnt-RL",
        "transcript": "One Solution is Not All You Need - Few-Shot Extrapolation via Structured MaxEnt RL \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nOne Solution is Not All You Need - Few-Shot Extrapolation via Structured MaxEnt RL\n2020\n\u2022\nDeep Reinforcement Learning\n\u2022\nLatent Variable\n\u2022\nNeurIPS 2020\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nDRL\n\u2022\nGeneralization\n\u2022\nNeurIPS\n\u2022\nRL\n02 Nov 2020\nIntroduction\nKey idea: Practicing and remembering diverse solutions to a task can lead to robustness to that task\u2019s variations.\nThe paper proposes a framework to implement this idea - train multiple policies such that they are\ncollectively\nrobust to a new distribution over environments while using a single training environment.\nLink to the paper\nSetup\nDuring training, the agent has access to only one MDP.\nDuring the evaluation, the agent encounters a new MDP which has the same state and action space but may have a different reward and transition function.\nThe agent is allowed some interactions (say\nk\n) with the test MDP and is then evaluated on the test MDP. The setup is referred to as\nfew-shot robustness\n.\nStructured Maximum Entropy Reinforcement Learning (SMERL)\nRepresent a set of policies using a latent variable policy (i.e., a policy conditioned on a latent variable\nz\n).\nThis has two benefits: (i) Multiple policies can be represented by the same object, and (ii) diverse behaviors can be learned by encouraging the trajectories, corresponding to different\nz\nto be different, while being able to solve the task.\nA diversity-inducing objective is used to encourage the agent to learn different trajectories for different\nz\n.\nSpecifically, the mutual information between\np(Z)\nand marginal trajectory distribution for the latent variable policy is maximized, subject to the constraint that each policy achieves close to optimal returns in the train MDP.\nThe mutual information between\np(Z)\nand marginal trajectory distribution for the latent variable policy is lower bounded by the sum of mutual information terms over individual states (appearing in the trajectory).\nAn unsupervised reward function is defined using the mutual information between states and latent variables.\n\\(r(s, a) = log(q_{\\phi})(z\\|s) - log(p(z))\\) where \\(q_{\\phi}\\) is a learned discriminator.\nThis unsupervised reward is optimized for only when the policy achieves close to an optimal return, i.e., the environment return is close to the optimal return. Otherwise, the agent optimizes only for the environment return.\nImplementation\nSMERL is implemented using SAC with a latent variable maximum entropy policy.\nThe set of latent variables is a fixed discrete set \\(Z\\) and \\(p(z)\\) is set to be a uniform distribution over this set.\nAt the start of an episode, a \\(z\\) is sampled and used throughout the episode.\nDiscriminator \\(q_{\\phi}(z\\|s)\\) is trained to infer \\(z\\) from the visited states.\nA baseline SAC agent is trained beforehand to evaluate if the current training policy achieves close to optimal environment return.\nDuring the evaluation, the policy corresponding to each latent variable is executed in the test MDP, and the policy with the maximum return is returned.\nTheoretical Analysis\nGiven an MDP \\(M\\) and \\(\\epsilon>0\\), the MDP robustness set is defined as the set of all MDPs \\(M'\\) where the optimal policy of \\(M'\\) produces the same trajectory distribution in \\(M'\\) as \\(M\\). Moreover, on the training MDP \\(M\\), the optimal policies (corresponding to \\(M\\) and \\(M'\\)) obtain similar returns.\nThe paper shows that SMERL generalizes to MDPs belong to the robustness set.\nIt also provides a simplified view of the optimization objective and shows how it naturally leads to a trajectory-centric mutual information objective.\nExperiments\nEnvironments\n2D navigation environments with point mass.\nMujoco Environments: HalfCheetah-Goal, Walker2d-Velocity, Hopper-Velocity.\nOn the 2D navigation environment, the paper shows that SMERL learns to use different trajectories to reach the goal.\nOn the Mujoco setup, the evaluation shows that SMERL generally outperforms the best-performing baseline or is close to the best-performing baseline on different tasks.\nGenerally, higher train performance does not correlate with higher test performance, and there is no single policy that performs the best across all the tasks. Thus, it should be beneficial to learn multiple diverse policies that can be selected from during testing.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2010.14484"
    },
    "20": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Learning-Explanations-That-Are-Hard-To-Vary",
        "transcript": "Learning Explanations That Are Hard To Vary \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nLearning Explanations That Are Hard To Vary\n2020\n\u2022\nAI\n\u2022\nInvariance\n19 Oct 2020\nIntroduction\nThe paper builds on the principle \u201cgood explanations are hard to vary\u201d to propose that\ninvariant mechanisms\ncan be identified by finding explanations (say model parameters) that are hard to vary across examples.\nLink to the paper\nLink to the code\nSetup\nCollection of\nd\ndifferent datasets (from different environments). Each dataset is a collection of input-target tuples.\nObjective is to learn a function\nf\n(also called\nmechanism\n) to map the input to the target (for all the environments).\nThe standard approach is to pool the loss for examples corresponding to the different environments and perform gradient updates on this average-pooled loss.\nIn this standard gradient-based setup, the model may not learn invariances due to the following reasons:\nModel learned the spurious features first, and now the training loss is too small.\nThe pooled loss is generally computed by summing (or averaging) the loss corresponding to individual examples. Thus the gradient for each example is calculated independently. Each sample can be thought of as a dataset of size 1, for which all the features are relevant.\nGradient descent with averaging (of gradients across the environments) greedily maximizes for the learning speed and not invariance.\nPerforming arithmetic mean can be seen as performing an OR operation (i.e., the sum can be high if any one of the constituents is high), whereas performing geometric mean can be seen as performing an AND operation (i.e., the product can be high only if all the constituents are high).\nInvariant Learning Consistency(ILC)\nGiven an algorithm \\(A\\), let \\(\\theta_{A}^{*}\\) denote the set of convergence points of \\(A\\) when trained on all the environments.\nEach convergence point is associated with a consistency score.\nIntuitively, given a convergence point and an environment\ne\n, find the set of parameters equivalent to the convergence point (in terms of loss) with respect to\ne\n. Let\u2019s call this set as\nS\n.\nEvaluate the points in this set for all the remaining environments. For the given convergence point, an environment\ne\u2019\nis consistent with\ne\nif the maximum difference in the loss for two environments is small, for all points belonging to\nS\n.\nThis idea is used to define the invariant learning consistency score for algorithm \\(A\\), which measures the expected consistency of the converged points (on the pooled data) across all the environments.\nThe paper shows that the converged points\u2019 consistency is linked to the Hessians\u2019 geometric mean and that for the convex quadratic case, using the elementwise geometric mean of gradients improves consistency.\nHowever, there are some practical challenges:\nGeometric mean is defined only when all signs are consistent. This issue can potentially be handled by treating different signs as 0.\nThere is very little flexibility in \u201cpartial\u201d agreement, and even a single zero gradient component can stop optimization for that component. This can probably be handled by not masking if many environments have a gradient for that component.\nGeometric component needs to be computed in the log-domain (for numerical scalability), but that can be computationally more expensive.\nWhen using adaptive optimizers like Adam, the exact magnitude of geometric mean will be ignored because of rescaling for the local curvature adaptation.\nSome of these challenges can be handled using average gradients when the geometric mean would be 0 and masking out components based on the sign.\nAND-mask\nThe ideas from the previous section can be used to develop a practical algorithm called AND-mask.\nZero-out gradients that have inconsistent signs across some threshold number (hyper-parameter) of environments.\nIn the presence of purely random gradient patterns, the AND-mask decreases the signals\u2019 strength exponentially fast.\nExperiments\nSynthetic Memorization Dataset\nThis is a binary classification task with two kind of features: (i) \u201cmeaningful\u201d features that are shared across environments but harder for the model to learn and (ii) \u201cshortcut\u201d features that are easy to learn but not shared across environments.\nWhile the dataset may look simple, it is difficult to find the invariant mechanism because the \u201cshortcut\u201d features allow for a simple, linear decision boundary, with a large margin that is fast to learn, has perfect accuracy, robust to input noise, and no iid generalization gap.\nBaselines:\nMLPs trained with regularizers like dropout, L1, L2, and batch norm.\nDomain Adversarial Neural Networks (DANN)\nInvariant Risk Minimization (IRM)\nIn terms of results, AND-mask with L1/L2 regularizers gives the best results.\nEmpirically, the paper shows that the signal from the \u201cmeaningful\u201d features is present when the gradients are averaged, but their magnitude is much smaller than the signal from the \u201cshortcut\u201d features.\nExperiments on CIFAR-10\nA ResNet model is trained on the CIFAR-10 dataset with random labels, with and without the AND-mask.\nThe model with the AND-mask did not memorize the data, whereas the model without the AND-mask did. As sanity, the paper ensured that both the models generalize well when trained with the original labels.\nNote that for this experiment, every example was treated to have come from its own environment.\nBehavioral Cloning on CoinRun\nTrain an expert policy using PPO for 400M steps on the full distribution of levels.\nGenerate a dataset of state-action pairs. Training data consists of 1000 states from each of the 64 levels, while the test data comes from 2000 levels.\nA ResNet18 model is used as an imitation learning policy.\nThe exact implementation of the AND-mask is a little more involved, but the key takeaway is that model trained with AND-mask identifies invariant mechanisms across different levels.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2009.00329"
    },
    "21": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Remembering-for-the-Right-Reasons-Explanations-Reduce-Catastrophic-Forgetting",
        "transcript": "Remembering for the Right Reasons - Explanations Reduce Catastrophic Forgetting \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nRemembering for the Right Reasons - Explanations Reduce Catastrophic Forgetting\n2020\n\u2022\nCatastrophic Forgetting\n\u2022\nContinual Learning\n\u2022\nLifelong Learning\n\u2022\nReplay Buffer\n\u2022\nAI\n\u2022\nCL\n\u2022\nLL\n12 Oct 2020\nIntroduction\nThe paper hypothesizes that catastrophic forgetting can happen if the model can not rely on \u201creasoning\u201d used for an old datapoint. If that is the case, catastrophic forgetting may be alleviated when the model \u201cremembers\u201d why it made a prediction previously.\nThe paper presents a simple instantiation of this hypothesis, in the form of a technique called Remembering for the Right Reasons (RRR).\nThe idea is to store model explanations, along with previous examples in the replay buffer. During replay, an additional\nexplanation loss\nis used, along with the regular replay loss.\nLink to the paper\nLink to the code\nSetup\nThe model is trained over a sequence of data distributions in the class-incremental learning setup. A single-head architecture is used so that the task ID is not required during inference.\nAlong with the standard replay buffer (\\(M^{rep}\\)) for the raw input examples (from different tasks), another replay buffer (\\(M^{RRR}\\)) is maintained for storing the \u201cexplanations\u201d (in the form of saliency maps), corresponding to examples in \\(M^{rep}\\).\nRRR is implemented as an L1 loss on the error between the saliency map generated after training on the current task and the saliency map in \\(M^{RRR}\\).\nSaliency maps need to be generated while the model is training. This requirement rules out black-box saliency methods, which can be used only after training.\nThe gradient-based white-box explainability techniques that are used include:\nVanilla backpropagation - Perform a forward pass through the model and take the gradient of the given output class with respect to the input.\nBackpropagation with SmoothGrad - Saliency maps generated using Vanilla backpropagation can be visually noisy. These maps can be improved by adding pixel-wise Gaussian noise to\nn\ncopies of the image and averaging the resulting gradients. The paper used\nn=40\n.\nGradient-weighted Class Activation Mapping (Grad-CAM) - Uses gradients to determine the importance of feature map activations on a given prediction.\nRRR can be easily used with memory and regularization based approaches.\nThe paper combined RRR with the following standard Class Incremental Learning (CIL) models:\niTAML : An incremental task-agnostic meta-learning approach\nEnd-to-end incremental learning (EEIL)\nLarge scale incremental learning (BiC)\nTOpology-Preserving knowledge InCrementer (TOPIC)\niCaRL: Incremental Classifier and Representation Learning\nElastic Weight Consolidation\nLearning without forgetting\nExperiments\nFew-Shiot Class Incremental Learning\nC-way K-shot class incremental learning with C classes and K training samples per class and b base classes to learn as the first task.\nCaltech-UCSD Birds dataset with 100 base classes and remaining 100 classes divided into ten tasks, with three samples per class. The test set is not changed.\nIn teems of saliency maps., Grad-CAM is better than Vanilla Backpropagation, which in turn is comparable to SmoothGrad. The same trend is seen in terms of memory overhead, with Grad-CAM having the least memory overhead.\nAdding the RRR loss improves the performance of all the baselines.\nStandard Class Incremental Learning\nCIFAR100 and ImageNet100 with a memory budget of 2000 samples.\nAdding the RRR loss improves all the baselines\u2019 performance, and the gains for ImageNet100 are more significant than the gains for CIFAR100.\nHow often does the model remember its decision for the right reason?\nThe paper uses the Pointing Game (PG) experiment, which uses the ground truth image segmentation to define the true object region.\nIf the maximum attention location (in the predicted saliency map) falls inside the objects, it is considered a\nhit\n, else a\nmiss\n. A\nhit\non a previous example is considered a proxy for the model remembering its decision for the right reason.\nThe precision and recall are reported for the\nhit\nmetric. Using RRR increases both precision (i.e., less often the model makes the correct decision without looking at the right evidence) and recall (i.e., less frequently does the model makes an incorrect decision, despite looking at the proper evidence).\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2010.01528"
    },
    "22": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/A-Foliated-View-of-Transfer-Learning",
        "transcript": "A Foliated View of Transfer Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nA Foliated View of Transfer Learning\n2020\n\u2022\nTransfer Learning\n\u2022\nAI\n\u2022\nGeometry\n\u2022\nTheory\n28 Sep 2020\nIntroduction\nThe paper presents a formalism for transfer learning, offers a definition of relatedness between tasks, and proposes foliations as a mathematical framework to represent the relationship between tasks.\nLink to the paper\nSummary\nThe term\nrepresentation\ndenotes a mechanism for\ndescribing\nand\nrealizing\nabstract objects, thus allowing manipulation and reasoning about the objects. This description goes beyond the usual meaning (in deep learning), where\nrepresentation\ndenotes some useful information about data.\nRelatedness\ndescribes\nwhat\nchanges between tasks. Consider a set of transformations (or functions) that convert one task to another. A\nrelationship\nbetween two tasks is an element of this transformation set.\nGiven a transformation set, one can define a\nset of related tasks\n, which is the set of all the tasks that can be transformed into each other using the functions from the given transformation set. This set of tasks is an equivalence class, and the transformation set is the equivalence relationship.\nGiven two related tasks\nt1\nand\nt2\n, denote the corresponding models (trained on those tasks) as\nm1\nand\nm2\n. One can assume that\nm1\nand\nm2\nare related in the same way as\nt1\nand\nt2\n(equivariance).\nNow, given a set of transformations, one can partition the space of continuous functions into non-overlapping spaces, which describe a set of related tasks. These spaces are referred to as the\nparallel spaces\nor\ntransfer spaces\n.\nThe parallel space represents a lower dimension than the original space. So knowing which parallel space a model lies on can make it easier to find it. This is the primary motivation behind transfer learning - knowing the relationship between tasks can make it easier to find a solution to new tasks.\nAnother way of partitioning the set of transformations is to use tessellation (e.g., Voronoi diagrams). Tasks in the same partition are similar to each other as compared to a task from another partition.\nTwo tasks are defined as\nsimilar\nif the distance between them (under some distance metric) is small.\nSimilarity is a\ngeometric\nnotion, while relatedness is a\ntransformative\nnotion. Parallelized space is to relatedness what tessellation is to similarity.\nThe distinction between similarity and relatedness is quite nuanced, and the authors provide several examples to differentiate between them.\nSimilarity can only be measured in terms of a reference element (similar to what). For example, when one finetunes a pre-trained model on a new task, one assumes that the model\u2019s pretraining task is similar to the current task.\nGiven a set (say\nT\n), a\nquantity\n(a function that maps elemenets of\nT\nto a\nk\ndimensional vector) is said to be\ninvariant\nwith respect to a transformation\np\n(defined on\nT\n) if\nq(f) = q(p(f))\nie the value of\nf\n(belonging to\nT\n) does not change if\nf\nis transformed by\np\n.\nIf one assumes that the set of transformations is a group, specifically a Lie group whose action on the set of tasks is locally free and regular, then one can define a parallel partitioning of the space of tasks and the space of models.\nOne can develop a hierarchial categorization scheme for the set of all considered tasks using the invariant quantities.\nOne can consider the space of tasks and models to be smooth manifolds as manifolds naturally give a notion of representation and transformations between them.\nA manifold is a topological space that can be locally mapped to a Euclidean space using coordinate charts. One can define regular foliation by choosing charts that satisfy certain conditions. In that case, the manifold has immersed, connected, non-intersecting submanifolds called leaves.\nThe charts (that satisfies those conditions) give a set of rectified coordinates, where the notions of \u201cwhich leaf a point is on\u201d and \u201cwhere on the leaf it is\u201d are clearly separated.\nThus, foliation can provide the theoretical tools to work with parallel spaces.\nHow can the foliations be incorporated into theory and solutions for transfer learning is left aa future work.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2008.00546"
    },
    "23": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Harvest,-Yield,-and-Scalable-Tolerant-Systems",
        "transcript": "Harvest, Yield, and Scalable Tolerant Systems \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nHarvest, Yield, and Scalable Tolerant Systems\n1999\n\u2022\nDistributed Systems\n\u2022\nOperating Systems\n\u2022\nCAP\n\u2022\nOS\n\u2022\nScale\n21 Sep 2020\nIntroduction\nA classic paper that looks into strategies for scaling large systems that can tolerate graceful degradation.\nLink to the paper\nCAP Theorem\nCAP refers to strong\nC\nonsistency, high\nA\nvailability, and\nP\nartitionability.\nStrong consistency refers to single copy ACID consistency.\nHigh availability means any consumer can access the data anytime. Generally, this is achieved by adding one or more data replicas.\nPartitionability means that the system can survive a partition between the different replicas.\nStrong CAP theorem states that any system can have only two out of three properties.\nWeak CAP theorem says that stronger are the guarantees about any two properties, weaker are the third property\u2019s guarantees.\nHarvest, Yield, and CAP Theorem\nAssume that the clients are making a request to a server.\nThere are two quantities of interest here:\nYield - the probability of completing a request.\nHarvest - completeness of answer to a query.\nIn the presence of faults, a tradeoff can is made between yield and harvest. This tradeoff applies to both read and update queries.\nTwo strategies for scaling systems\nTrading Harvest for Yield\nIn a hundred node cluster (without replication), a single-node failure reduces harvest by 1 %, and in the case of multi-node failure, the harvest degrades linearly.\nThe probability of losing high-priority data can be reduced by replicating it. However, replicating all the data would not n guarantee 100% harvest and yield despite significant costs.\nApplication Decomposition and Orthogonal Mechanisms\nDecompose a large application into subcomponents so that each component can be provisioned separately. Strong consistency can only be applied only on the components that need it, instead of the application as a whole.\nFurther, failure of one or more components need not cause the application to fail as a whole.\nDecomposition also provides the opportunity to use orthogonal mechanisms, i.e., mechanisms independent of other mechanisms with no runtime interface.\nComposition of orthogonal subsystems improves the robustness of runtime interactions by\nlocally\ncontaining the errors. For example, the orthogonal components can be restarted /replaced independently without affecting other running components.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://dl.acm.org/doi/10.5555/822076.822436"
    },
    "24": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/MONet-Unsupervised-Scene-Decomposition-and-Representation",
        "transcript": "MONet - Unsupervised Scene Decomposition and Representation \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nMONet - Unsupervised Scene Decomposition and Representation\n2019\n\u2022\nObject-Oriented Learning\n\u2022\nAI\n\u2022\nAttention\n\u2022\nCV\n\u2022\nUnsupervised\n14 Sep 2020\nIntroduction\nThe paper introduces Multi-Object Network (MONet) architecture that learns a modular representation of images by spatially decomposing scenes into\nobjects\nand learning a representation for these\nobjects\n.\nLink to the paper\nArchitecture\nTwo components:\nAttention Module: generates spatial masks corresponding to the\nobjects\nin the scene.\nVAE: learn representation for each\nobject\n.\nVAE components:\nEncoder: It takes as input the image and the attention mask generated by the attention module and produce the parameters for distribution over latent variable\nz\n.\nDecoder: It takes as input the latent variable\nz\nand attempts to reproduce the image.\nThe decoder loss term is weighted by mask, i.e., the decoder tries to reproduce only those parts of the image that the attention mask focuses on.\nThe attention mechanism is auto-regressive with an ongoing state (called a scope) that tracks which parts of the image are not yet attended over.\nIn the last step, no attention mask is computed, and the previous scope is used as-is. This ensures that all the masks sum to 1.\nThe VAE also models the attention mask over the components, i.e., the probability that the pixels belong to a particular component.\nMotivation\nA model could efficiently process compositional visual scenes if it can exploit some recurring structures in the scene.\nThe paper validates this hypothesis by showing that an autoencoder performs better if it can build up the scenes compositionally, processing one mask at a time (these masks are ground-truth spatial masks) rather than processing the scene at once.\nResults\nVAE encoder parameterizes a diagonal Gaussian latent posterior with a spatial broadcast decoder that encourages the VAE to learn disentangled features.\nMONet with seven slots is trained on\nObjects Room\ndataset with 1-3 objects.\nIt learns to generate different attention mask for different objects.\nCombining the reconstructed components using the corresponding attention masks produces good quality reconstruction for the entire scene.\nSince it is an autoregressive model, MONet can be evaluated for more slots. The model generalizes to novel scene configurations (not seen during training).\nOn the Multi-dSprites dataset (modification of the dSprites dataset), the model (post-training) distinguishes individual sprites and background.\nOn the CLEVER data (2-10 objects per image), the model generates good image segmentation and reconstructions and can distinguish between overlapping shapes.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1901.11390"
    },
    "25": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Revisiting-Fundamentals-of-Experience-Replay",
        "transcript": "Revisiting Fundamentals of Experience Replay \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nRevisiting Fundamentals of Experience Replay\n2020\n\u2022\nDeep Reinforcement Learning\n\u2022\nICML 2020\n\u2022\nOff policy RL\n\u2022\nReinforcement Learning\n\u2022\nReplay Buffer\n\u2022\nAI\n\u2022\nDRL\n\u2022\nEmpirical\n\u2022\nICML\n\u2022\nRL\n07 Sep 2020\nIntroduction\nThe paper presents an extensive study of the effects of experience replay in Q-learning based methods.\nIt focuses explicitly on the replay capacity and replay ratio (ratio of learning updates to experience collected).\nLink to the paper\nSetup\nReplay capacity is defined as the total number of transitions stored in the replay buffer.\nAge of a transition (stored in the replay buffer) is defined as the number of gradient steps taken by the agent since the transition was stored.\nMore is the replay capacity, more will be the age of the oldest transition (also referred to as the age of the oldest policy).\nMore is the replay capacity, more will be the degree of \u201coff-policyness\u201d of the transitions in the buffer (with everything else held constant).\nReplay ratio is the number of gradient updates per environment transition. This ratio can be used as a proxy for how often the agent uses old data (vs. collecting new data) and is related to off-policyness.\nIn\nDQN paper\n, the replay ratio is set to be 0.25.\nFor experiments, a subset  (of 14 games) is selected from Atari ALE (Arcade Learning Environment) with sticky actions.\nEach experiment is repeated with three seeds.\nRainbow is used as the base algorithm.\nTotal number of gradient updates and batch size (per gradient update) are fixed for all the experiments.\nRainbow used replay capacity of 1M and oldest policy of age 250K.\nIn experiments, replay capacity varies from 0.1M to 10M ( 5 values), and the age of the oldest policy varies from 25K to 25M (4 values).\nObservations\nWith the age of the oldest policy fixed, performance improves with higher replay capacity, probably due to increased state-action coverage.\nWith fixed replay capacity, reducing the oldest policy\u2019s age improves performance, probably due to the reduced off-policyness of the data in the replay buffer.\nHowever, in some specific instances (with sparse reward, hard exploration setup), performance can drop when reducing the oldest policy\u2019s age.\nIncreasing replay capacity, while keeping the replay ratio fixed, provides varying improvements and depends on the particular values of replacy capacity and replay ratio.\nThe paper reports the effect of these choices for DQN as well.\nUnlike Rainbow, DQN does not improve with larger replay capacity, irrespective of whether the replay ratio or age of the oldest policy is kept fixed.\nGiven that the Rainbow agent is a DQN agent with additional components, the paper explores which of these components leads to an improvement in Rainbow\u2019s performance as replay capacity increases.\nAdditive Experiments\nFour new DQN variants are created by adding each of Rainbow\u2019s four components to the base DQN agent.\nDQN with n-step returns is the only variant that benefits by increased replay capacity.\nThe usefulness of n-step returns is further validated by verifying that Rainbow agent without n-step returns does not benefit by increased replay capacity. While Rainbow agent without any other component benefits by the increased capacity.\nPrioritized Experience Replay does not significantly affect the performance with increased replay capacity.\nThe observation that n-step returns are critical for taking advantage of larger replay sizes is surprising because the uncorrected n-step returns are theoretically not suitable for off-policy learning.\nThe paper tests the limits of increasing replay capacity (with n-step returns) by performing experiments in the offline-RL setup, the agent collects a dataset of about 200M frames. These frames are used to train another agent.\nEven in this extreme setup, n-step returns improve the learning agent\u2019s performance.\nWhy do n-step returns help?\nHypothesis 1: n-step returns help to counter the increased off-policyness produced by a larger replay buffer.\nThis hypothesis does not seem to hold as keeping the oldest policy fixed or using the same contrastive factor as an n-step update does not improve the 1-step update\u2019s performance.\nHypothesis 2: Increasing the replay buffer\u2019s capacity may reduce the variance of the n-step returns.\nThis hypothesis is evaluated by training on environments with lesser variance or by turning off the sticky actions in the atari domain.\nWhile the hypothesis does explain the gains by using n-step returns to some extent, n-step gains are observed even in environments with low variance.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2007.06700"
    },
    "26": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Deep-Reinforcement-Learning-and-the-Deadly-Triad",
        "transcript": "Deep Reinforcement Learning and the Deadly Triad \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nDeep Reinforcement Learning and the Deadly Triad\n2018\n\u2022\nDeep Reinforcement Learning\n\u2022\nEmpirical Advice\n\u2022\nOff policy RL\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nDRL\n\u2022\nEmpirical\n\u2022\nRL\n31 Aug 2020\nIntroduction\nThe paper investigates the practical impact of the deadly triad (function approximation, bootstrapping, and off-policy learning) in deep Q-networks (trained with experience replay).\nThe deadly triad is called so because when all the three components are combined, TD learning can diverge, and value estimates can become unbounded.\nHowever, in practice, the component of the deadly triad has been combined successfully. An example is training DQN agents to play Atari.\nLink to the paper\nSetup\nThe effect of each component of the triad can be regulated with some design choices:\nBootstrapping - by controlling the number of steps before bootstrapping.\nFunction approximation - by controlling the size of the neural network.\nOff-policy learning - by controlling how data points are sampled from the replay buffer (i.e., using different prioritization approaches)\nThe problem is studied in two contexts: toy example and Atari 2600 games.\nThe paper makes several hypotheses about how different components may interact in the triad and evaluate these hypotheses by training DQN with different hyperparameters:\nNumber of steps before bootstrapping - 1, 3, 10\nFour levels of prioritization (for sampling data from the replay buffer)\nBootstrap target - Q-learning, target Q-learning, inverse double Q-learning, and double Q-learning\nNetwork sizes-small, medium, large and extra-large.\nEach experiment was run with three different seeds.\nThe paper formulates a series of hypotheses and designs experiments to support/reject the hypotheses.\nHypothesis 1: Combining Q learning with conventional deep RL function spaces does not commonly lead to divergence\nRewards are clipped between -1 and 1, and the discount factor is set to 0.99. Hence, the maximum absolute action value is bound to smaller than 100. This upper bound is used soft-divergence in the value estimates.\nThe paper reports that while soft-divergence does occur, the values do not become unbounded, thus supporting the hypothesis.\nHypothesis 2: There is less divergence when correcting for overestimation bias or when bootstrapping on separate networks.\nOne manifestation of bootstrapping on separate networks is target-Q learning. While using separate networks helps on Atari, it does not entirely solve the problem on the toy setup.\nOne manifestation of correcting for the overestimation bias is using double Q-learning.\nIn the standard form, double Q-learning benefits by bootstrapping on a separate network. To isolate the gains by using each component independently, an inverse double Q-learning update is used that does not use a separate target-network for bootstrapping.\nExperimentally, Q-learning is the most unstable while target Q-learning and double Q-learning are the most stable. This observation supports the hypothesis.\nHypothesis 3: Longer multi-step returns will diverge easily\nThis hypothesis is intuitive as the dependence on bootstrapping is reduced with multi-step returns.\nExperimental results support this hypothesis.\nHypothesis 4: Larger, more capacity networks will diverge less easily.\nThis hypothesis is based on the assumption that more flexible value function approximations may behave more like the tabular case.\nIn practice, smaller networks show fewer instances of instability than the larger networks.\nThe hypothesis is not supported by the experiments.\nHypothesis 5: Stronger prioritization of updates will diverge more easily.\nThis hypothesis is supported by the experiments for all the four updates.\nEffect of the deadly triad on the agent\u2019s performance\nGenerally, soft-divergence correlates with poor control performance.\nFor example, longer multi-step returns lead to fewer instances of instabilities and better performance.\nThe trend is more interesting in terms of network capacity. Large networks tend to diverge more but also perform the best.\nWhile action-value estimates can grow to large values, they can recover to plausible values as training progresses.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1812.02648"
    },
    "27": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Alpha-Net-Adaptation-with-Composition-in-Classifier-Space",
        "transcript": "Alpha Net--Adaptation with Composition in Classifier Space \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nAlpha Net--Adaptation with Composition in Classifier Space\n2020\n\u2022\nLong-tailed Dataset\n\u2022\nTransfer Learning\n\u2022\nAI\n\u2022\nClassifier\n\u2022\nCompositionality\n24 Aug 2020\nIntroduction\nCommon transfer learning method focuses on transferring knowledge in the model feature space.\nIn contrast, the paper argues that the learned knowledge is more concisely captured in the \u201cclassifier space\u201d as the classifier is fitted for all the samples for a given class, while the feature representation is specific to each sample.\nBuilding on this intuition, the paper proposes to combine strong classifiers (trained on large datasets) with weak classifiers (trained on smaller datasets) to improve the weak classifiers\u2019 performance.\nLink to the paper\nHigh-Level Idea\nGiven $n$ classifiers, $C_1, \u2026, C_n$, trained with a large amount of data and a weak classifier $a$ trained for a class with few samples.\nFind the nearest neighbors of $a$.\nTrain a new classifier by linearly combining $a$ with its nearest classifiers.\nThe coefficients (for linearly combining the classifiers) are learned using another classifier called as AlphaNet.\nIn theory, this approach can be used with any set of classifiers.\nSetup\nA long-tailed dataset is one where some classes (referred to as the tail classes) have very few examples\u2014for example, ImageNet-LT and Places-LT.\nSplit the long-tailed dataset into two splits - \u201cbase\u201d classes with $B$ (number of) classes and \u201cfew\u201d classes with $F$ (number of) classes.\nTotal number of classes $N = B + F$.\nStart with a pre-trained model, with classifiers $w_j$ and biases $b_j$ for $j \\in (1, N)$.\nFor a given target class $j$, find its top $k$ nearest neighbor classifiers and concatenate their output.\nFor each \u201cfew\u201d class, learn a feedforward network that takes the concatenated representation (of classifiers) as the input and returns a vector of $k \\alpha$ values.\nThese $\\alpha$ values are interpreted as the classifier\u2019s strength (or confidence) in its nearest neighbors.\nThe (normalized) alpha values are used for defining the weight and bias for the classifier for the given \u201cfew\u201d class.\nThe collection of all the \u201cfew\u201d classifiers is referred to as the AlphaNet.\nThe paper outlines a degenerate case, where the confidence in the prediction of all the strong classifiers goes to 0. The paper proposes to counter this case by clamping the $\\alpha$ values.\nThe entire setup is trained end-to-end using cross-entropy loss on AlphaNet.\nResults\nGiven the proposed approach\u2019s flexibility, it is used to combine the state-of-the-art models on ImageNet-LT, namely retraining classifiers on class-balanced samples and training models with weight normalization. The combined setup outperforms the individual models.\nOne interesting observation is that it is useful to include the weak classifiers, along with the strong classifiers, as AlphaNet adjusts the position of weak classifiers towards the appropriate strong classifier.\nWhile the idea is described in the context of long-tail data distribution, the idea is useful in the general context of non-stationary data distribution. One instantiation could be lifelong class incremental learning where the model encounters new data classes during training. For some time duration (till sufficient data points are seen), the newly seen classes are the \u201cfew\u201d classes. This approach can help with faster adaptation when the model is yet to see sufficient examples for the unseen classes.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2008.07073"
    },
    "28": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer",
        "transcript": "Outrageously Large Neural Networks--The Sparsely-Gated Mixture-of-Experts Layer \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nOutrageously Large Neural Networks--The Sparsely-Gated Mixture-of-Experts Layer\n2017\n\u2022\nConditional Computation\n\u2022\nDistributed Computing\n\u2022\nICLR 2017\n\u2022\nMixture of Experts\n\u2022\nAI\n\u2022\nGating\n\u2022\nICLR\n14 Aug 2020\nIntroduction\nConditional computation is a technique to increase a model\u2019s capacity (without a proportional increase in computation) by activating parts of the network on a per example basis.\nThe paper describes (and address) the computational and algorithmic challenges in conditional computation. It introduces a sparsely-gated Mixture-of-Experts layer (MoE) with 1000s of feed-forward sub-networks.\nLink to the paper\nPractical Challenges\nGPUs are fast at matrix arithmetic but slow at branching.\nLarge batch sizes amortizes the cost of updates. Conditional computation reduces the effective batch size for different components of the model.\nNetwork bandwidth can be a bottleneck with the network demand overshadowing the computational demand.\nAdditional losses may be needed to achieve the desired level of sparsity.\nConditional computation is most useful for large datasets.\nArchitecture\nn\nExpert Networks - $E_1$, \u2026, $E_n$.\nGating Network $G$ to select a sparse combination of experts.\nOutput of the MoE module is the weighted sum of predictions of experts (weighted by the output of the gate).\nIf the gating network\u2019s output is sparse, then some of the experts\u2019 value does not have to be computed.\nIn theory, one could use a hierarchical mixture of experts where a mixture of experts is trained at each level.\nChoices for the Gating Network\nSoftmax Gating\nNoisy top-k gating - Add tunable Gaussian noise to the output of softmax gating and retain only the top-k values. A second trainable weight matrix controls the amount of noise per component.\nAddressing Performance Challenge\nShrinking Batch Problem\nIf the MoE selects\nk\nout of\nn\nexperts, the effective batch size reduces by a factor of\nk\n/\nn\n.\nThis reduction in batch size is accounted for by combining data parallelism (for standard layers and gasting networks) and model parallelism (for experts in MoE). Thus, with\nd\ndevices, the batch size changes by a factor of (\nk\nx\nd\n) /\nn\n.\nFor hierarchical MoE, the primary gating network uses data parallelism while secondary MoEs use model parallelism.\nThe paper considers LSTM models where the MoE is applied once the previous layer has finished. This increases the batch size (for the current MoE layer) by a factor equal to the number of unrolling timesteps.\nNetwork Bandwith limitations can be overcome by ensuring that the ratio of computation (of each expert) to the input and output size is greater than (or equal to) the ratio of computational to network capacity.\nComputational efficiency can be improved by using larger hidden layers (or more hidden layers).\nBalancing Expert Utilization\nImportance of an expert (relative to a batch of training examples) is defined as the batchwise sum of the expert\u2019s goal values.\nAn additional loss, called importance loss, is added to encourage the experts to have equal importance.\nThe importance loss is defined as the square of the coefficient of variation (of a set of importance values) multiplied by a (hand-tuned) scaling factor $w_{importance}$.\nIn practice, an additional loss called $L_{load}$ might be needed to ensure that the different experts get equal load (along with equal importance).\nExperiments\nDatasets\nBillon Word Language modeling Benchmark\n100 Billion word Google News Corpus\nMachine Translation datasets\nSingle Language Pairs - WMT\u201914 En to Fr (36M sentence pairs) and En to De (5M sentence pairs).\nMultilingual Machine Translation - large combine dataset of twelve language pairs.\nIn all the setups, the proposed MoE models achieve significantly better results than the baseline models, at a lower computational cost.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1701.06538"
    },
    "29": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Gradient-Surgery-for-Multi-Task-Learning",
        "transcript": "Gradient Surgery for Multi-Task Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nGradient Surgery for Multi-Task Learning\n2019\n\u2022\nGradient Manipulation\n\u2022\nMulti Task\n\u2022\nAI\n06 Aug 2020\nThe paper hypothesizes that main optimization challenges in multi-task learning arise because of negative interference between different tasks\u2019 gradients.\nIt hypothesizes that negative interference happens when:\nThe gradients are conflicting (i.e., have a negative cosine similarity).\nThe gradients coincide with high positive curvature.\nThe difference in gradient magnitude is quite large.\nThe paper proses to work around this problem by performing \u201cgradient surgery.\u201d\nIf two gradients are conflicting, modify the gradients by projecting each onto the other\u2019s normal plane.\nThis modification is equivalent to removing the conflicting component of the gradient.\nThis approach is referred to as\nprojecting conflicting gradients\n(PCGrad).\nLink to the paper\nTheoretical Analysis\nThe paper proves the local conditions under which PCGrad improves multi-task gradient descent in the two-task setup.\nThe conditions are:\nAngle between the task gradients is not too small.\nDifference in the magnitude of the gradients is sufficiently large.\nCurvature of the multi-task gradient is large.\nLarge enough learning rate.\nExperimental Setup\nMulti-task supervised learning\nMutliMNIST, Multi-task CIFAR100, NYUv2.\nFor Multi-task CIFAR-100, PCGrad is used with the shared parameters of the routing networks.\nFor NYUv2, PCGrad is combined with MTAN.\nIn all the cases, using PCGrad improves the performance.\nMulti-task Reinforcement Learning\nMeta-World Benchmark\nPCGrad + SAC outperforms all other baselines.\nIn the context of SAC, the paper suggests learning temperature $\\alpha$ on a per-task basis.\nGoal-conditioned Reinforcement Learning\nGoal-conditioned robotic pushing task with a Sawyer robot.\nPCGrad + SAC outperforms vanilla SAC.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2001.06782"
    },
    "30": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/GradNorm-Gradient-Normalization-for-Adaptive-Loss-Balancing-in-Deep-Multitask-Networks",
        "transcript": "GradNorm--Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nGradNorm--Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks\n2017\n\u2022\nGradient Manipulation\n\u2022\nGradient Normalization\n\u2022\nICML 2018\n\u2022\nMulti Task\n\u2022\nAI\n\u2022\nICML\n30 Jul 2020\nIntroduction\nThe paper proposes GradNorm, a gradient normalization algorithm that improves multi-task training by dynamically tuning the magnitude of gradients corresponding to different tasks.\nLink to the paper\nMotivation\nDuring multi-task training, some tasks can dominate the training, at the expense of others.\nIt is common to define the multi-task loss as a linearly weighted combination of the individual task losses.\nThe paper proposes two changes to this setup:\nAdapt weight-coefficients, assigned to each loss term, at each training step.\nDirectly modify the gradient magnitudes, corresponding to different tasks, so that all the tasks are learning at similar rates.\nProposed GradNorm algorithm is similar to BatchNorm, but it performs normalization across tasks, not data batches.\nAlgorithm\nGradient norm at timestep $t$, for the $i^{th}$ task, is computed as the product between average gradient norm (across all tasks at timestep $t$) and $r_i(t) ^ {\\alpha}$.\n$r_i$ is the relative inverse training rate of task $i$. It is defined as the ratio between the loss ratio of task $i$ and the average loss ratio (across all the tasks).\n$\\alpha$ is a hyperparameter.\nThis computed per-task gradient norm is treated as the target value for actual gradient norms.\nAn additional $L_1$ loss is incorporated between the actual and the target gradient norms, summed over all the tasks, and optimizes the weight-coefficients only.\nAfter every step, the weight-coefficients are renormalized to decouple the gradient normalization from the global learning rate.\nNote that all the gradient norm computations are performed only for the layers on which GradNorm is applied. Generally, GradNorm is used with only the last shared layer of weights (to save on computational costs).\nExperiments\nTwo variants of NYUv2 dataset \u2013 NYUv2+seg (small dataset) and NYUv2+kpts (big dataset).\nBoth regression and classification setups were used.\nModels:\nSegNet with a symmetric VGG16 encoder/decoder\nFCN with modified ResNet-50 as the encoder and shallow ResNet as the decoder.\nStandard pixel-wise losses for each task.\nResults\nGradNorm with $\\alpha=1.5$ outperforms the equal-weight baseline and either surpasses or matches the best performance of single networks for each task.\nAlmost any value of 0 < $\\alpha$ < 3 improves the network\u2019s performance over an equal weight baseline.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.02257"
    },
    "31": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/TASKNORM-Rethinking-Batch-Normalization-for-Meta-Learning",
        "transcript": "TaskNorm--Rethinking Batch Normalization for Meta-Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nTaskNorm--Rethinking Batch Normalization for Meta-Learning\n2020\n\u2022\nBatch Normalisation\n\u2022\nICML 2020'\n\u2022\nMeta Learning\n\u2022\nAI\n\u2022\nBatchNorm\n\u2022\nBN\n\u2022\nICML\n\u2022\nMAML\n\u2022\nNormalization\n23 Jul 2020\nIntroduction\nMeta-learning techniques are shown to benefit from the use of deep neural networks.\nBatchNorm is a commonly used component when training deep networks, especially for vision tasks.\nHowever, BatchNorm and meta-learning make contradictory assumptions, and their combination may not work well in practice.\nThe paper proposes TaskNorm, a normalization method that is designed explicitly for meta-learning.\nLink to the paper\nSetup\nStandard meta-learning setup with $k$ tasks, each task with its own context and target set.\nTwo sets of parameters are considered during meta-learning - (i) global parameters, and (ii) task-specific parameters.\nMeta-learning setup can be viewed as an inference task, where the task-specific parameters are inferred using a context set and some additional (trainable) parameters.\nNormalization layers are commonly used to accelerate the training of neural networks. The general approach is to use normalization moments (statistics) along with some learned parameters.\nBatchNorm is a well-known and widely used normalization approach. It relies on the implicit assumption that the dataset comprises of iid samples from some underlying distribution.\nHowever, in meta-learning, data points are assumed to be iid only within a specific task.\nThis leaves open the question of what moments to use during meta-train and meta-test time.\nVariants of BatchNorm\nConventional BatchNorm (CBN)\nCompute moments at meta train time and use during meta test time.\nThis is equivalent to lumping the moments with the global parameters. I.e., the running moments are shared globally, while the data is iid only locally.\nUsing CBN with MAML leads to poor results.\nMoreover, meta-learning setup can some times require the use of a very small batch size. (e.g., 1-shot learning) In those cases, the computed statistics are likely to be inaccurate.\nTransductive BatchNorm (TBN)\nUse context/target set statistics at both meta-train and meta-test time.\nThis is the default BatchNorm mode used in MAML.\nInstance-based normalization\nMoments are computed separately for each instance.\nThis mode corresponds to treating the statistics as local at the observation level.\nThese methods provide only limited improvement in performance, and can sometimes have a large overhead.\nTask Normalization (Proposed)\nThe normalization statistics are local at the task level, and statistics for a given data point should only depend on the context set\u2019s data point. It should not depend on the other elements of the target set.\nMeta-Batch Normalisation (METABN) is a precursor to TaskNorm where the context set alone is used to compute the normalization statistics for both the context and the target set (during both meta-test and meta-train time).\nMETABN does not perform well when used with small context sets.\nTaskNorm overcomes this limitation by using a set of non-transductive, secondary moments (computed from the input being normalized).\nWhen the context is small, using additional moments will help to improve the moment estimates.\nIn the general case, a trainable blending factor, $\\alpha$, is used to combine the two sets of moments.\nWhile the computational cost of TaskNorm is slightly more than CBN, it converges faster than CBN in practice.\nNormalization mechanism in Reptile can be interpreted as a particular case of TaskNorm.\nExperiments\nSmall scale few-shot classification experiments\nOmniglot and imin ImageNet dataset\nFirst order MAML, with different kinds of normalization schemes.\nTransductive BatchNorm performs the best.\nAmong non-transductive approaches, TaskNorm using Instance Normalisation augmentation performs the best.\nSimilar trend holds for the speed of convergence as well.\nLarge scale few-shot classification experiments\nMetaDataset dataset\nCNAPs model\nThe context set\u2019s size varies across tasks in this setup and can be as small as 5.\nTaskNorm with Instance Normalisation ranks first in 10 (out of 13) datasets and is also the fastest to train.\nWhile Instance-based methods (Instance Normalisation and Layer Normalisation) are the slowest to converge, they still outperform the running average based methods (conventional BatchNorm).\nThe results demonstrate that designing meta-learning specific normalization methods can significantly improve performance and that Transductive BatchNorm may not always be the optimal choice.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2003.03284"
    },
    "32": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Averaging-Weights-leads-to-Wider-Optima-and-Better-Generalization",
        "transcript": "Averaging Weights leads to Wider Optima and Better Generalization \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nAveraging Weights leads to Wider Optima and Better Generalization\n2018\n\u2022\nStochastic Gradient Descent\n\u2022\nUAI 2018\n\u2022\nAI\n\u2022\nGeneralization\n\u2022\nSGD\n\u2022\nSWA\n\u2022\nUAI\n16 Jul 2020\nIntroduction\nThe paper proposes Stochastic Weight Averaging (SWA) procedure for improving the generalization performance of models trained with SGD (with cyclic or constant learning rate).\nSpecifically, the model is checkpointed at several points along the training trajectory, and these checkpoints are averaged (in the parameter space) to obtain a single model.\nLink to the paper\nIdea\n\u201cStochastic\u201d in the name refers to the idea that with cyclical or constant learning rate, SGD proposals are approximately sampled from a neural network\u2019s loss surface and are hence stochastic.\nSWA uses a learning rate schedule that allows exploration in the weight space.\nSGD with cyclical and constant learning rates explore points (model instances) at the periphery of high-performing networks.\nWith different initializations, SGD will find different points (of low training loss) on this boundary, but will not move inside it.\nAveraging the points provide a mechanism to move inside this periphery.\nThe train and the test error surfaces, while being similar, are not perfectly aligned. Hence, averaging several models (along the optimization trajectory) could lead to a more robust model.\nAlgorithm\nGiven a model $w$ and some training budget $B$, train the model in the conventional way for approx 75% of the budget.\nStarting from that point, continue training with the remaining budget, with a constant or cyclical learning rate.\nFor fixed learning rate, checkpoint models at each epoch. For cyclical learning rate, checkpoint the model at the lowest learning rate in the cycle.\nAverage all the models to get the SWA model.\nIf the model has Batch Normalization layers, run an additional pass to compute the SWA model\u2019s running mean and standard deviation.\nThe computational and space complexity of computing the SWA model is relatively low.\nThe paper highlights the ensembling like the effect of SWA by showing that if the model checkpoints ($w_i$) are generated by training with Fast Geometric Ensembling (FGE), the difference between averaging the weights and averaging the predictions is of the order $O(\\Delta)$ where $\\Delta = max ||w_i - w_{SA}||$.\nNote that SWA does not have the overhead of an extra-forward pass during inference.\nExperiments\nDatasets: CIFAR10, CIFAR100, ImageNet\nModels: VGG16, WideResNet, 164-layer preactivation ResNet, ShakeShake, Pyramid Net.\nBaselines: Conventional SGD, Exponentially decaying average with SGD and FGE.\nIn all the CIFAR experiments, SWA consistently outperforms SGD in one budget and consistently improves with training.\nSWA also achieves performance comparable to FGE, despite FGE being an ensemble method.\nOn ImageNet, SWA is run on a pre-trained model, and it improves performance in all the cases.\nAn ablation experiment (on CIFAR-100) shows that it is possible to train a network (with SWA) using a fixed learning rate. In that setup, using SWA improves performance by 16%.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1803.05407"
    },
    "33": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Decentralized-Reinforcement-Learning-Global-Decision-Making-via-Local-Economic-Transactions",
        "transcript": "Decentralized Reinforcement Learning -- Global Decision-Making via Local Economic Transactions \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nDecentralized Reinforcement Learning -- Global Decision-Making via Local Economic Transactions\n2020\n\u2022\nCredit Assignment\n\u2022\nDecentralized Reinforcement Learning\n\u2022\nICML 2020\n\u2022\nHierarchical Reinforcement Learning\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nEconomics\n\u2022\nHRL\n\u2022\nICML\n\u2022\nRL\n09 Jul 2020\nIntroduction\nThe paper explores the connections between the concepts of a single agent vs. society of agents.\nA society of agents can be modeled as a single agent while a single agent can be modeled as a society of components (or sub-agents).\nThe paper focuses on mechanisms for training a society of self-interested agents to solve a given task \u2013 as if the system was a single task.\nLink to the paper\nContributions\nSocietal-decision making\nframework relates the local optimization problem of a single agent with the global optimization problem of a society of agents.\nCloned Vickrey Society\nis proposed as a mechanism to guarantee that an agent\u2019s dominant strategy equilibrium coincides with the group\u2019s optimal policy.\nA class of\ndecentralized RL algorithms\nthat optimize the MDP object of the society as a whole, as a consequence of individual agents optimizing their objectives.\nEmpirical evaluation of Cloned Vickrey Society using any implementation called\nCredit Conserving Vickery\n.\nTerminology\nEnvironment\n- a tuple that specifies an input space, an output space, and parameters for determining an objective.\nA standard RL setup can be mapped to\nenvironment\nby mapping state space to input space, action space to output space and reward function, transition function, and discount factors to the parameters specifying the objective.\nAgent\n- a function that maps input space to output space.\nObjective\n- a functional that maps an agent to a real number.\nIn\nauction environments\n, the input space is a single auction item (say\ns\n), and the output space is bidding space\nB\n.\nThere are\nN\nagents who compete by bidding for an item\ns\nusing their bidding policy.\n$b$ is a vector of bids produced by the agents.\n$v_s$ is a vector of agent\u2019s valuations of item\ns\n.\nThe $i^{th}$ agent\u2019s utility is given as $v_s^i \\times X^i(b) - P^i(b)$. Here, $X^i(b)$ is the portion of $s$ allocated to $i^{th}$ agent and $P^i(b)$ is the price that $i^{th}$ agent is willing to pay.\nDesign Choices\nEach agent is independently maximizing its utility.\nIn certain conditions (i.e., if the auction is dominant strategy incentive compatible), it is optimal for each agent to bid its valuation.\nThese conditions are satisfied by the Vickery auction where $P^i(b)$ is set to be the second-highest bid and $X^i(b) = 1$ if the $i^{th}$ agent wins (and 0 otherwise).\nA\nsociety\nis a set of agents where each agent is a tuple of bidding policy $\\psi$ and a transformation function.\nThe environment is modeled at two levels - (i) global environment (referred to as the global MDP) and local environment (referred to as local auction).\nEach state $s$ in the global MDP is an auction item in a different auction. The winner (of local auction at $s$) transforms $s$ into some other state $s\u2019$.\nIf these transformations are modeled as actions, then the proposed framework can be interpreted as a decentralized reinforcement learning framework.\nMotivated by the design of market economy (where economic transactions determine wealth distribution), the paper proposes that, for an agent, the valuation of winning an auction is the revenue it can receive in the auction at the next timestep by selling the transformed state.\nA global MDP that adhere to this design is referred to as the Market MDP.\nThere is a catch in the design of the market MDP - the winning agent, at time $t-1$, gets the amount that the highest bidder is willing to pay at time $t+1$. But the winner at time $t+1$ only paid the second-highest bid. Hence, the credit is not conserved.\nThis inconsistency can be fixed by introducing \u201cduplicate\u201d (or cloned) agents, and the society is called the Cloned Vickery Society.\nThe Cloned Vickrey Auction mechanism is compared against alternate bidding mechanisms like\nfirst price auction\n(where winner pays the bid they proposed), solitary version of Vickrey auction (no cloning), and\nEnvironment Reward\nwhere only environment reward is used, and there is no price term.\nIt is empirically shown that Cloned Vickrey Auction learns bids that are most close to their actual valuations. Moreover, solitary version leads bids which are more spread out than the ones learned by cloned version. This highlights the importance of competitive pressure to learn bid values.\nThree different implementations of Cloned Vickrey Auction are considered:\nBucket Brigade (BB) - winner at timestep $t$ receives the highest bid at time step $t+1$, and the subsequent winner pays the highest bid. This case satisfies Credit Conservation and Bellman Optimality.\nVickrey (V) - winner at timestep $t$ receives the highest bid at time step $t+1$, and the subsequent winner pays the second-highest bid. This case satisfies Truthful Dominant Strategy and Bellman Optimality.\nCredit Conserving Vickrey (CCV) - winner at timestep $t$ receives the second-highest bid at time step $t+1$, and the subsequent winner pays the second-highest bid. This case satisfies Truthful Dominant Strategy and Credit Conservation.\nCCV implementation provides bid values closest to the optimal Q-values.\nIn one experiment, the paper explores the use of the proposed approach for selecting between sub-policies. It shows that CVV is more sample efficient for pretraining sub-policies and adapting them to transfer tasks.\nIn another experiment, the task is to transform MNIST images by composing two out of 6 affine transformations. The transformed images are fed to a pretrained classifier that predicts a label. The agent gets a reward of 1 if the classifier makes correct prediction and 0 otherwise. CCV implementation obtains a mean reward of 0.933, thus highlighting the effectiveness of the CCV model.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2007.02382"
    },
    "34": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/When-to-use-parametric-models-in-reinforcement-learning",
        "transcript": "When to use parametric models in reinforcement learning? \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nWhen to use parametric models in reinforcement learning?\n2019\n\u2022\nDeep Reinforcement Learning\n\u2022\nModel-Based\n\u2022\nModel-Free\n\u2022\nNeurips 2019\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nDRL\n\u2022\nNeurips\n\u2022\nPlanning\n\u2022\nRL\n02 Jul 2020\nIntroduction\nThe paper compares replay-based approaches with model-based approaches in Reinforcement Learning (RL).\nIt hypothesizes that if the parametric model is only used for generation transitions for the update rule, then under certain conditions, replay-based approaches will be as good as model-based approaches.\nLink to the paper\nTerminology\nPlanning: Any algorithm that uses additional computations (but not additional experience) to improve its performance.\nLearning: Any algorithm that uses additional experience to improve its performance.\nIn some cases, a replay buffer can be seen as a model. For example, querying using state-action pair (from the replay buffer) is similar to querying the (expected) next-state and reward from a model. In general, the model will be more flexible as any arbitrary state-action pair can be used for querying.\nComputation Properties\nParametric models require more computation than sampling from a replay buffer. In contrast, the cost of maintaining a replay buffer scales linearly with their capacity.\nParametric models are useful for planning multiple-steps into the future while it is much harder to do so with a replay buffer (even more so with pixel observations).\nAn imperfect model maybe be more suitable for selecting actions (instead of updating the policy) because the chosen action, when executed in the environment, will lead to transitions that would improve the model.\nWhen planning with an imperfect model, it is better to plan backward, as the update is applied on an imaginary state (which would not be encountered if the model is poor).\nIf the model is accurate, forward and backward planning is equivalent. This distinction between forward and backward updates does not apply to replay buffers.\nFailure to learn\nWhen using a replay buffer and (i) uniformly replaying transitions, (ii) from a buffer containing only full episodes, and (iii) using TD updates, then the algorithm is stable.\nWhen using a replay buffer and (i) uniformly replaying transitions, (ii) generating transitions using a model, and (iii) using TD updates, then the algorithm can diverge.\nThis case can be fixed by:\nRepeatedly interating over the model and sampling transitions\nto\nand\nfrom\nthe state model generates (not a satisfactory solution).\nUsing multiple-step returns (this can increase the variance).\nUse algorithms specifically for stable off-policy learning (not a definitive solution).\nModel-based algorithms at scale\nThe paper compares against SimPLe (model-based) with Rainbow DQN (replay-based).\nThe paper shows that when using a similar number of real interactions, Rainbow DQN needs fewer replay samples than model samples in SimPLe, making it more efficient (computation-wise).\nChanges to Rainbow DQN:\nIncrease number of steps, for bootstrapping, from 3 to 20.\nReduce the number of steps, before sampling starts from the replay buffer, from 20K to 1600.\nWith these changes, Rainbow DQN outperforms SimPLe in 17 out of 26 games.\nConclusion\nWhen using a parametric model in a replay-like setting (sampling observed states from the past), model-based learning can be unstable (in theory). Using a replay buffer is likely a better strategy under the state sampling distribution.\nParametric models are likely more useful when:\nplanning backward for credit assignment - even if the model is in-accurate, backward planning will only update fictional states.\nplanning forward for behavior - the resulting plan is only used to collect real\nexperience\nin the environment (and not directly update the policy).\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.05243"
    },
    "35": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Network-Randomization-A-Simple-Technique-for-Generalization-in-Deep-Reinforcement-Learning",
        "transcript": "Network Randomization - A Simple Technique for Generalization in Deep Reinforcement Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nNetwork Randomization - A Simple Technique for Generalization in Deep Reinforcement Learning\n2019\n\u2022\nDeep Reinforcement Learning\n\u2022\nICLR 2020\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nDRL\n\u2022\nGeneralization\n\u2022\nICLR\n\u2022\nRL\n25 Jun 2020\nIntroduction\nThe paper proposed a Technique for improving the generalization ability of RL agents when evaluated on an unseen environment (which is similar to the training environment).\nLink to the paper\nLink to the code\nApproach\nThe key idea is to learn features that are invariant across environments by using a randomized CNN (\nf\n) that randomly perturbs the inputs.\nThe policy is trained using the randomized observations obtained using\nf\n.\nInvariant features are learned using a feature matching (FM) loss that matches the feature representation of the original and randomized observations.\nThe random network\u2019s parameters are initialized as $\\alpha I + (1 - \\alpha) N(0, \\sqrt\\frac{2}{n_{in} + n_{out}})$ where $\\alpha \\in [0, 1]$, $N$ denotes the Gaussian Distribution and $n_{in}, n_{out}$ denote the number of input and output channels respectively.\nXavier Normal distribution is used for randomization to maintain the variance between the input and the randomized input.\nf\nis randomized per iteration.\nDuring inference, the expected action is computed by approximating over\nM\nsamples (i.e., randomizing the input\nM\ntimes).\nEnvironments\n2D CoinRun, 3D DeepMind Lab, 3D Robotics Control Task\nThe evaluation environments consist of different styles of backgrounds, objects, and floors.\nBaselines\nRegularization methods: Dropout, L2 regularization, Batch Normalization\nDataset Augmentation methods: Cutout, Gray out, Inversion, Color Jitter\nResults\nOn CoinRun, the proposed approaches significantly outperforms the other baselines during evaluation. The performance improvement saturates around 10\nM\nsamples.\nCycle consistency is used to measure the similarity between two trajectories. The proposed method improves the cycle consistency as compared to the vanilla PPO baseline. It also produces sharper activation maps in the evaluation environments.\nFor the large-scale experiments, when evaluated on 500 levels of CoinRun, the proposed method improves the success rates from 39.8% to 58.7%.\nOn DeepMind Lab and Surreal robotics control tasks, the proposed method leads to agents that generalize better on the unseen environments (during evaluation).\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=HJgcvJBFvB"
    },
    "36": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/On-the-Difficulty-of-Warm-Starting-Neural-Network-Training",
        "transcript": "On the Difficulty of Warm-Starting Neural Network Training \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nOn the Difficulty of Warm-Starting Neural Network Training\n2019\n\u2022\nIncremental Learning\n\u2022\nOnline Learning\n\u2022\nTransfer Learning\n\u2022\nAI\n\u2022\nEmpirical\n18 Jun 2020\nIntroduction\nThe paper considers learning scenarios where the training data is available incrementally (and not at once).\nFor example, in some applications, new data is available periodically (e.g., latest news articles come out every day).\nThe paper highlights that, in such scenarios, the conventional wisdom of \u201cwarm start\u201d does not apply.\nWhen new data is available, it is better to train a new model from scratch than to update the model trained on previously available data.\nWhile the two setups lead to similar training performance, the randomly initialized model has a much better generalization performance.\nLink to the paper\nBasic Batch Updating\nCreate two random, equally-sized partitions of the training data.\nTrain the model till convergence on the first half of the data. Then train the model on the entire dataset.\nModels: ResNet18, MLPs, Logisitic Regression (LR)\nDataset: CIFAR10, CIFAR100, SVHN\nOptimizers: Adam, SGD\nWarm starting hurts generalization in all the cases.\nThe effect is more pronounced in the case of ResNets and MLPs (compared to LR) and harder CIFAR 10 dataset (as compared to SVHN dataset).\nOnline Learning\nPassive Online Learning\nThe model is given access to k new learning examples at each iteration.\nA warm started model reuses the previously initialized model and trains (till convergence) on the new batch of k items.\nA \u201crandomly initialized\u201d model is trained on all the examples (seen so far) from scratch.\nDataset: CIFAR10\nModel: ResNet18\nAs more training data becomes available, the generalization gap between the two setups increases, and warmup starts hurting generalization.\nActive Online Learning\nIn this setup, the learner is trained to sample k new examples to add to the training dataset (using margin-based sampling).\nLike the previous setup, warmup strategy still hurts generalization.\nTransfer Learning\nTrain a Resnet18 model on the CIFAR10 dataset and use this model to warm start training on the SVHN dataset.\nWhen a small percentage of the SVHN dataset is used, the setup resembles pretraining / transfer learning and performs better than training from scratch.\nAs the percentage of the SVHN dataset increases, the warmup approach starts underperforming.\nOvercoming warm start problem\nResNet18 model on CIFAR10 dataset\nWhen performing a hyper-parameter sweep over the learning rate and batch size, it is possible to train warm start models to reach the same generalization performance as training from scratch.\nThough, in that case, there are no computational savings as the warm-started models take about the same time (to converge) as the randomly initialized model.\nThe increased training time indicates that the warm started model probably needs to forget the knowledge from previous training rounds.\nWarm start Resnet models, that generalize well, have a low correlation to their initialization stage (measured via Pearson correlation coefficient between the model weights).\nGeneralization is damaged even when using a model trained on incomplete data for only a few epochs.\nFor warm start models, the gradient (corresponding to the \u201cnew\u201d data) is higher than that for randomly initialized models. This hints that regularisation may help to close the generalization gap. But in practice, regularization helps both the warmup and randomly initialized model.\nWarm starting only a few layers also does not close the gap.\nAdding some noise to the warm started model (with the motivation of having a partially random initialization) does help somewhat but also increases the training time.\nMotivating the problem as an instance of catastrophic forgetting, the authors use the EWC algorithm but report that using EWC hurts model performance.\nThe paper does not propose a solution to the problem but provides a thorough analysis of the problem setup, which is quite useful for understanding the phenomenon itself.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1910.08475"
    },
    "37": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Supervised-Contrastive-Learning",
        "transcript": "Supervised Contrastive Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nSupervised Contrastive Learning\n2020\n\u2022\nContrastive Learning\n\u2022\nAI\n\u2022\nContrastive\n\u2022\nImageNet\n30 Apr 2020\nIntroduction\nThe paper builds on the prior work on self-supervised contrastive learning and extends it for the supervised learning case where many positive examples are available for each anchor.\nLink to the paper\nApproach\nThe representation learning framework has the following components:\nData Augmentation Module\nThis module transforms the input example. The paper considers the following strategies:\nRandom crop, followed by resizing\nAuto Augment\n- A method to search for data augmentation strategies.\nRand Augment\n- Randomly sampling a sequence of data augmentations, with repetition\nSimAugment - Sequentially apply random color distortion and Gaussian blurring, followed by probabilistic sparse image wrap.\nEncoder Network\n* This module maps the input to a latent representation.\n\n* The same network is used to encode both the anchor and the sample.\n\n* The representation vector is normalized to lie on the unit hypersphere.\nProjection Network\n* This module maps the normalized representation to another representation, on which the contrastive loss is computed.\n\n* This network is only used for training the supervised contrastive loss.\nLoss function\n* The paper extends the standard contrastive loss formulation to handle multiple positive examples.\n\n* The main effect is that the modified loss accounts for all the same-class pairs (from within the sampled batch as well as the augmented batch).\n\n* The paper shows that the gradient (corresponding to the modified loss) causes the learning to focus more on hard examples. \"Hard\" cases are the ones where contrasting the anchor benefits the encoder more.\n\n* The proposed loss can also be seen as a generalization of the triplet loss.\nExperiments\nDataset - ImageNet\nModels - ResNet50, ResNet200\nThe network is \u201cpretrained\u201d using supervised contrastive loss.\nAfter pre-training, the projection network is removed, and a linear classifier is added.\nThis classifier is trained with the CE loss while the rest of the network is kept fixed.\nResults\nUsing supervised contrastive loss improves over all the baseline models and data augmentation approaches.\nThe resulting classifier is more robust to image corruptions, as shown by the mean Corruption Error (mCE) metric on the ImageNet-C dataset.\nThe model is more stable to the choice oh hyperparameter values (like optimizers, data augmentation, and learning rates).\nTraining Details\nSupervised Contrastive loss is trained for 700 epochs during pre-training.\nEach step is about 50% more expensive than performing CE.\nThe dense classifier layer can be trained in as few as ten epochs.\nThe temperature value is set to 0.07. Using a lower temperature is better than using a higher temperature.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2004.11362"
    },
    "38": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/CURL-Contrastive-Unsupervised-Representations-for-Reinforcement-Learning",
        "transcript": "CURL - Contrastive Unsupervised Representations for Reinforcement Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nCURL - Contrastive Unsupervised Representations for Reinforcement Learning\n2020\n\u2022\nContrastive Learning\n\u2022\nDeep Reinforcement Learning\n\u2022\nReinforcement Learning\n\u2022\nSelf Supervised\n\u2022\nSample Efficient\n\u2022\nAI\n\u2022\nContrastive\n\u2022\nDRL\n\u2022\nRL\n\u2022\nUnsupervised\n09 Apr 2020\nIntroduction\nThe paper proposes a contrastive learning approach, called CURL, for performing off-policy control from raw pixel observations (by transforming them into high dimensional features).\nThe idea is motivated by the application of contrastive losses in computer vision. But there are additional challenges:\nThe learning agent has to perform both unsupervised and reinforcement learning.\nThe \u201cdataset\u201d for unsupervised learning is not fixed and keeps changing with the policy of the agent.\nUnlike prior work, CURL introduces fewer changes in the underlying RL pipeline and provides more significant sample efficiency gains. For example, CURL (trained on pixels) nearly matches the performance of SAC policy (trained on state-based features).\nLink to the paper\nImplementation\nCURL uses instance discrimination. Deep RL algorithms commonly use a stack of temporally consecutive frames as input to the policy. In such cases, instance discrimination is applied to all the images in the stack.\nFor generating the positive and negative samples, random crop data augmentation is used.\nBilinear inner product is used as the similarity metric as it outperforms the commonly used normalized dot product.\nFor encoding the anchors and the samples, InfoNCE is used. It learns two encoders $f_q$ and $f_k$ that transform the query (base input) and the key (positive/negative samples) into latent representations. The similarity loss is applied to these latents.\nMomentum contrast is used to update the parameters ($\\theta_k$) of the $f_k$ network. ie $\\theta_k = m \\theta_k + (1-m) \\theta_q$. $\\theta_q$ are the parameters of the $f_q$ network and are updated in the usual way, using both the contrastive loss and the RL loss.\nExperiment\nDMControl100K and Atart100K refer to the setups where the agent is trained for 100K steps on DMControl and Atari, respectively.\nMetrics:\nSample Efficiency - How many steps does the baseline need to match CURL\u2019s performance after 100K steps.\nPerformance - Ratio of episodic returns by CURL vs. the baseline after 100K steps.\nBaselines:\nDMControl\nSAC-AE\nSLAC\nPlaNet\nDreamer\nPixel SAC\nSAC trained on state-space observations\nAtari\nSimPLe\nRainbowDQN\nOTRainbow (Over Trained Rainbow)\nEfficient Rainbow\nRandom Agent\nHuman Performance\nResults\nDM Control\nCURL outperforms all pixel-based RL algorithms by a significant margin for all environments on DMControl and most environments on Atari.\nOn DMControl, it closely matches the performance of the SAC agent trained on state-space observations.\nOn Atari, it achieves better median human normalizes score (HNS) than the other baselines and close to human efficiency in three environments.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://github.com/MishaLaskin/curl"
    },
    "39": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Competitive-Training-of-Mixtures-of-Independent-Deep-Generative-Models",
        "transcript": "Competitive Training of Mixtures of Independent Deep Generative Models \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nCompetitive Training of Mixtures of Independent Deep Generative Models\n2018\n\u2022\nCausal Learning\n\u2022\nClustering\n\u2022\nGenerative Models\n\u2022\nAI\n\u2022\nCausality\n12 Mar 2020\nIntroduction\nThe paper proposes a Competitive training mechanism to train a mixture of independent generative models.\nThe idea is that this mixture of different models would divide the data distribution amongst themselves and specialize to their respective splits.\nThe training procedure is related to clustering-based methods.\nLink to the paper\nMotivation\nIn causal modeling, a common assumption is that the data is generated by a set of independent mechanisms.\nIt is not known which mechanism generates which datapoint and recovering the underlying mechanisms can be modeled as learning a structural causal generative model.\nSetup\nThe paper assumes that the support of the different generators do not overlap, i.e., the underlying data distribution is factorized into non-overlapping regions.\nThis data factorization is learned using a set of discriminators.\nIf there are $k$ generators, $k$ binary partition functions $c_i, \u2026 c_k$ are used.\nFor a given datapoint $x$, if $c_i(x) = 1$ then $c_j(x) = 0$ for all other $j$ and $x$ is assigned to $i^{th}$ generator.\nFor a fixed partition function $c_j^t$ ($t$ denotes the partition function at time $t$), minimize the sum of f-divergence between the model and the data distribution (that is assigned to it). The loss formulation is an upper bound on the f-divergence of the mixture model.\nIn the next step, the data points are re-assigned to the generative models, based on the likelihood of each data point for each model.\nThe likelihood is estimated by training a discriminator that can distinguish the generated samples from the real samples.\nIndependence as an inductive bias\nThe independence assumption may be too restrictive because the low-level features will be common across the distribution splits.\nThis \u201cviolation\u201d can be avoided by pretraining the model using a uniform random split of the dataset. In that case, the independence assumption will hold approximately after pretraining.\nAnother approach could be to share some parameters across the models.\nA \u201cload balancing\u201d approach is also used where each model always keeps training on the data points assigned to it if not enough data points are assigned to it.\nComparison to VAEs and GANs\nVAEs tend to be \u201coverly inclusive\u201d of the training distribution, i.e., they try to cover the entire support of the distribution.\nGANs are prone to mode collapse where the model focuses only on one part of the distribution.\nThe proposed method provides a middle ground where the different generative models can focus on different parts of the distribution.\nExperiments\nThe experiments seem to be limited. The paper shows that their proposed setup improves over the VAE and GAN baselines.\nFor datasets, the paper uses two-dimensional synthetic data, MNIST and CelebA\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1804.11130"
    },
    "40": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/What-Does-Classifying-More-Than-10,000-Image-Categories-Tell-Us",
        "transcript": "What Does Classifying More Than 10,000 Image Categories Tell Us? \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nWhat Does Classifying More Than 10,000 Image Categories Tell Us?\n2010\n\u2022\nECCV 2010\n\u2022\nAI\n\u2022\nCV\n\u2022\nECCV\n\u2022\nImageNet\n05 Mar 2020\nThe paper is among the first to study image classification at a large scale (10000 classes and 9 million examples).\nThis is a relatively old paper (2010). Some of the findings may not be relevant anymore. For instance, specific scaling challenges have been significantly overcome. Moreover, the paper uses approaches like SVM and KNN (popular at that time) and not use CNNs.\nOther observations of the paper are still very relevant, and it is an educating paper. For example, since ImagetNet classes are based on WordNet, the paper looks at the effect of semantic relations (tree) of categories on the performance of the training models.\nLink to the paper\nThe paper considers three variants of the ImageNet dataset - ImageNet 10K (10184 classes), ImageNet 7K (7404 classes) and ImageNet 1K (1000 classes).\nThey also consider smaller variants with randomly sampled classes or cases where the examples are sampled from one high-level category like vehicles.\nSVM and KNN models are used with features like Bag of Words, GIST descriptors, and spatial pyramid of histograms.\nObservations\nA model that performs well on the smaller dataset (with fewer classes) may not perform well on the larger dataset (with more classes).\nThere seems to be an approximate correlation between the structure of the semantic hierarchy of the labels (obtained via WordNet) and visual confusion between the categories.\nFor example, consider two high-level concepts - says artifacts and animals. The model is less likely to confuse between the classes across the high-level concepts but more likely to confuse between the classes in the respective concepts.\nFor dense categories (categories where the classes are semantically more closely related to each other), the model tends to make more mistakes (even if the number of classes is fewer).\nAccounting for the label hierarchy (in the loss function) improves the classification performance.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "http://openaccess.thecvf.com/content_cvpr_2015/papers/Jain_What_do_15000_2015_CVPR_paper.pdf"
    },
    "41": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/ELECTRA-Pre-training-Text-Encoders-as-Discriminators-Rather-Than-Generators",
        "transcript": "ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators\n2019\n\u2022\nICLR 2020\n\u2022\nNatural Language Processing\n\u2022\nAI\n\u2022\nAttention\n\u2022\nFinetuning\n\u2022\nICLR\n\u2022\nNLP\n\u2022\nPretraining\n\u2022\nTransformer\n20 Feb 2020\nIntroduction\nMasked Language Modeling (MLM) is a common technique for pre-training language-based models. The idea is to \u201ccorrupt\u201d some tokens in the input text (around 15%) by replacing them with the [MASK] token and then training the network to reconstruct (or predict) the corrupted tokens.\nSince the network learns from only about 15% of the tokens, the computational cost of training using MLM can be quite high.\nThe paper proposes to use a \u201creplaced token detection\u201d task where some tokens in the input text are replaced by other plausible tokens.\nFor each token in the modified text, the network has to predict if the token has been replaced or not.\nThe alternative token is generated using a small generator network.\nUnlike the previous MLM setup, the proposed task is defined for all the input tokens, thus utilizing the training data more efficiently.\nLink to the paper\nApproach\nThe proposed approach is called ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\nTwo neural networks - Generator (G) and Discriminator (D) are trained.\nEach network has a Transformer-based text encoder that maps a sequence of words into a sequence of vectors.\nGiven an input sequence x (of length N), k indices are chosen for replacing the tokens.\nFor each index, the generator produces a distribution over tokens. A token is sampled to replace in the original sequence. The resulting sequence is referred to as the corrupted sequence.\nGiven the corrupted sequence, the Discriminator predicts which token comes from the data distribution and which comes from the generator.\nThe generator is trained using the MLM setup, and the Discriminator is trained using the discriminative loss.\nAfter pre-training, only the Discriminator is finetuned on the downstream tasks.\nExperiments\nDatasets\nGLUE Benchmark\nStanford QA dataset\nArchitecture Choices\nSharing word embeddings between generator and Discriminator helps.\nTying all the encoder weights leads to marginal improvement but forces the generator and the Discriminator to be of the same size. Hence only embeddings are shared.\nGenerator model is kept smaller than the discriminator model as a strong generator can make the training difficult for the Discriminator.\nA two-stage training procedure was explored where only the generator is trained for n steps. Then the weights of the generator are used to initialize the Discriminator. The Discriminator is then trained for n steps while keeping the generator fixed.\nThis two-stage setup provides a nice curriculum for the Discriminator but does not outperform the joint training based setup.\nAn adversarial loss based setup is also explored but it does not work well probably because of the following reasons:\nAdverserially trained generator is not as good as the MLM generator.\nAdverserially trained generator produces a low entropy output distribution.\nResults\nBoth small and large ELECTRA models outperform baselines models like\nBERT\n,\nRoBERTa\n,\nELMo\nand\nGPT\n.\nAblations\nELECTRA-15 is a variant of ELECTRA where the Discriminator is trained on only 15% of the tokens (similar to the MLM setup). This reduces performance significantly.\nReplace MLM setup\nPerform MLM training, but instead of using [MASK], use a toke sampled from the generator.\nThis improves the performance marginally.\nAll-token MLM\nIn the MLM setup, replace the [MASK] token by the sampled tokens and train the MLM model to generate all the words.\nIn practice, the MLM model can either generate a word or copy the existing word.\nThis approach closes much of the gap between BERT and ELECTRA.\nInterestingly, ELECTRA outperforms All-token MLM BERT suggesting the ELECTRA may be benefitting from parameter efficiency since it does not have to learn a distribution over all the words.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=r1xMH1BtvB"
    },
    "42": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Gradient-based-sample-selection-for-online-continual-learning",
        "transcript": "Gradient based sample selection for online continual learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nGradient based sample selection for online continual learning\n2019\n\u2022\nCatastrophic Forgetting\n\u2022\nContinual Learning\n\u2022\nLifelong Learning\n\u2022\nNeurIPS 2019\n\u2022\nReplay Buffer\n\u2022\nAI\n\u2022\nCL\n\u2022\nLL\n\u2022\nNeurIPS\n13 Feb 2020\nIntroduction\nUse of replay buffer (and rehearsal) is a common technique for mitigating catastrophic forgetting.\nThe paper builds on this idea but focuses on the sample selection aspect ie, which data points to store in the replay buffer.\nIt formulates sample selection as a constraint minimization problem and shows that the proposed formulation is equivalent to maximizing the diversity of the samples with respect to parameter gradient.\nLink to the paper\nSetup\nSupervised learning tasks\nOnline stream of data (i.e., one or few datapoints accessed at a time).\nWhen considering the $t^{th}$ task, the objective is: minimize the loss on the current task without increasing the loss on any of the previous tasks.\nThe above constraint can be rephrased as $dot(g_t, g_i) \\gt 0 \\forall i \\in [0, t-1]$ where $g_t$ is the gradient for the $t^{th}$ task.\nThis is equivalent to saying that the current task gradient should not interfere negatively with the previous task gradient.\nApproach\nIn practice, the gradient constraint is enforced only over the examples in the minibatch (and not the full dataset).\nThe paper interprets the constraint satisfaction problem as approximating an optimal feasible region (in the gradient space) where current task performance can be improved without hurting the performance on the previous tasks.\nThe approximate region (of the shape of a polyhedral convex cone) is determined using only the examples from the replay buffer. Hence, the optimal region (defined for the entire dataset) would be contained within the approximate region.\nThe size of the approximate region can be measured in terms of the solid angle defined by the intersection between the approximate region and a unit sphere.\nThe paper argues that the approximate region can be made smaller by reducing the angle between each pair of gradients.\nThe set of points, satisfying the constraint, can be computed using the Integer Quadratic Programming (IQP).\nGiven that the problem setup is online learning, using IDP for every new data point is not feasible.\nAn in-exact, greedy alternative is suggested where a score is maintained for each example in the buffer.\nWhen a new datapoint comes in, the score is computed and used to decide if the existing datapoint in the buffer should be replaced.\nThe score is the maximal cosine similarity of the current example with a random sample in the buffer.\nResults\nBenchmarks\nDisjoint MNIST\nPermuted MNIST\nDisjoint CIFAR10\nShared head setup\nBaselines for sample selection\nRandomly select examples to keep in the buffer.\nPerform clustering - either in the feature space or in the gradient space.\nUse IQP to select the examples. This approach is not used for CIFAR10, as it is computationally costly.\nIt would be interesting if the paper had considered baselines like selecting samples which had the largest loss.\nThe proposed greedy approach outperforms the other methods.\nIn an ablation experiment, the paper shows that the proposed approach works better than reservoir sampling (when the underlying data distribution is imbalanced).\nAnother experiment compares the proposed approach with\nGradient Episodic Memory\nand\niCaRL\n. For Permuted and Disjoint MNIST, the different methods perform quite similar though the proposed approach performs better on Disjoint CIFAR10.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.08671"
    },
    "43": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Your-Classifier-is-Secretly-an-Energy-Based-Model,-and-You-Should-Treat-it-Like-One",
        "transcript": "Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nYour Classifier is Secretly an Energy Based Model and You Should Treat it Like One\n2019\n\u2022\nICLR 2020\n\u2022\nAdversarial Robustness\n\u2022\nEnergy-Based Models\n\u2022\nGenerative Models\n\u2022\nHybrid Models\n\u2022\nOut of Distribution\n\u2022\nOutlier Detection\n\u2022\nOut of Distribution Detection\n\u2022\nAI\n\u2022\nAdversarial\n\u2022\nCalibration\n\u2022\nEBM\n\u2022\nICLR\n\u2022\nRobustness\n06 Feb 2020\nIntroduction\nThe paper proposed a framework for joint modeling of labels and data by interpreting a discriminative classifier\np(y|x)\nas an energy-based model\np(x, y)\n.\nJoint modeling provides benefits like improved calibration (i.e., the predictive confidence should align with the miss classification rate), robustness, and out of order distribution.\nLink to the paper\nMotivation\nConsider a standard classifier $f_{\\theta}(x)$ which produces a k-dimensional vector of logits.\n$p_{\\theta}(y | x) = softmax(f_{\\theta}(x)[y])$\nUisng concepts from energy based models, we write $p_{\\theta}(x, y) = \\frac{exp(-E_{\\theta}(x, y))}{Z_{\\theta}}$ where $E_{\\theta}(x, y) = -f_{\\theta}(x)[y]$\n$p_{\\theta}(x) = \\sum_{y}{ \\frac{exp(-E_{\\theta}(x, y))}{Z_{\\theta}}}$\n$E_{\\theta}(x) = -LogSumExp_y(f_{\\theta}(x)[y])$\nNote that in the standard discriminative setup, shiting the logits $f_{\\theta}(x)$ does not affect the model but it affects $p_{\\theta}(x)$.\nComputing $p_{\\theta}(y | x)$ using $p_{\\theta}(x, y)$ and $p_{\\theta}(x)$ gives back the same softmax parameterization as before.\nThis reinterpreted classifier is referred to as a Joint Energy-based Model (JEM).\nOptimization\nThe log-liklihood of the data can be factoized as $log p_{\\theta}(x, y) = log p_{\\theta}(x) + log p_{\\theta}(y | x)$.\nThe second factor can be trained using the standard CE loss. In contrast, the first factor can be trained using a sampler based on Stochastic Gradient Langevin Dynamics.\nResults\nHybrid Modelling\nDatasets: CIFAR10, CIFAR100, SVHN.\nMetrics: Inception Score, Frechet Inception Distance\nJEM outperforms generative, discriminative, and hybrid models on both generative and discriminative tasks.\nCalibration\nA calibrated classifier is the one where the predictive confidence aligns with the misclassification rate.\nDataset: CIFAR100\nJEM improves calibration while retaining high accuracy.\nOut of Distribution (OOD) Detection\nOne way to detect OOD samples is to learn a density model that assigns a higher likelihood to in-distribution examples and lower likelihood to out of distribution examples.\nJEM consistently assigns a higher likelihood to in-distribution examples.\nThe paper also proposes an alternate metric called\napproximate mass\nto detect OOD examples.\nThe intuition is that a point could have likelihood but be impossible to sample because its surroundings have a very low density.\nOn the other hand, the in-distribution data points would lie in a region of high probability mass.\nHence the norm of the gradient of log density could provide a useful signal to detect OOD examples.\nRobustness\nJEM is more robust to adversarial attacks as compared to discriminative classifiers.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1912.03263"
    },
    "44": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Massively-Multilingual-Neural-Machine-Translation-in-the-Wild-Findings-and-Challenges",
        "transcript": "Massively Multilingual Neural Machine Translation in the Wild - Findings and Challenges \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nMassively Multilingual Neural Machine Translation in the Wild - Findings and Challenges\n2019\n\u2022\nMulti Domain\n\u2022\nMulti Task\n\u2022\nNatural Language Processing\n\u2022\nNeural Machine Translation\n\u2022\nAI\n\u2022\nNLP\n\u2022\nNMT\n\u2022\nScale\n30 Jan 2020\nIntroduction\nThe paper proposes to build a universal neural machine translation system that can translate between any pair of languages.\nAs a concrete instance, the paper prototypes a system that handles 103 languages (25 Billion translation pairs).\nLink to the paper\nWhy universal Machine Translation\nHypothesis:\nThe learning signal from one language should benefit the quality of other languages\n1\nThis positive transfer is evident for low resource languages but tends to hurt the performance for high resource languages.\nIn practice, adding new languages reduces the effective per-task capacity of the model.\nDesiderata for Multilingual Translation Model\nMaximize the number of languages within one model.\nMaximize the positive transfer to low resource languages.\nMinimize the negative interference to high resource languages.\nPerform well ion the realistic, multi-domain settings.\nDatasets\nIn-house corpus generated by crawling and extracting parallel sentences from the web.\n102 languages, with 25 billion sentence pairs.\nCompared with the existing datasets, this dataset is much larger, spans more domains, has a good variation in the amount of data available for different language pairs, and is noisier. These factors bring additional challenges to the universal NMT setup.\nBaselines\nDedicated Bilingual models (variants of Transformers).\nMost bilingual experiments used Transformer big and a shared source-target sentence-piece model (SPE).\nFor medium and low resource languages, the Transformer Base was also considered.\nBatch size of 1 M tokes per-batch. Increasing the batch size improves model quality and speeds up convergence.\nEffect of Transfer and Interference\nThe paper compares the following two setups with the baseline:\nCombine all the datasets and train over them as if it is a single dataset.\nCombine all the datasets but upsample low resource languages so all that all the languages are equally likely to appear in the combined dataset.\nA target \u201cindex\u201d is prepended with every input sentence to indicate which language it should be translated into.\nShared encoder and decoder are used across all the language pairs.\nThe two setups use a batch size of 4M tokens.\nResults\nWhen all the languages are equally sampled, the performance on the low resource languages increases, at the cost of performance on high resource languages.\nTraining over all the data at once reverse this trend.\nCountering Interference\nTemperature based sampling strategy is used to control the ratio of samples from different language pairs.\nA balanced sampling strategy improves the performance for the high resource languages (though not as good as the multilingual baselines) while retaining the high transfer performance on the low resource languages.\nAnother reason behind the lagging performance (as compared to bilingual baselines) is the capacity of the multilingual models.\nSome open problems to consider:\nTask Scheduling - How to decide the order in which different language pairs should be trained.\nOptimization for multitask learning - How to design optimizer, loss functions, etc. that can exploit task similarity.\nUnderstanding Transfer:\nFor the low resource languages, translating multiple languages to English leads to improved performance than translating English to multiple languages.\nThis can be explained as follows: In the first case (many-to-one), the setup is that of a multi-domain model (each source language is a domain). In the second case (one-to-many), the setup is that of multitasking.\nNMT models seem to be more amenable to transfer across multiple domains than transfer across tasks (since the decoder distribution does not change much).\nIn terms of zero-shot performance, the performance for most language pairs increases as the number of languages change from 10 to 102.\nEffect of preprocessing and vocabulary\nSentence Piece Model (SPM) is used.\nTemperature sampling is used to sample vocabulary from different languages.\nUsing smaller vocabulary (and hence smaller sub-word tokens) perform better for low resource languages, probably due to improved generalization.\nLow and medium resource languages tend to perform better with higher temperatures.\nEffect of Capacity\nUsing deeper models improves performance (as compared to the wider models with the same number of parameters) on most language pairs.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1907.05019"
    },
    "45": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Observational-Overfitting-in-Reinforcement-Learning",
        "transcript": "Observational Overfitting in Reinforcement Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nObservational Overfitting in Reinforcement Learning\n2019\n\u2022\nICLR 2020\n\u2022\nDeep Reinforcement Learning\n\u2022\nEvaluating Generalization\n\u2022\nMarkov Decision Process\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nDRL\n\u2022\nEvaluation\n\u2022\nGeneralization\n\u2022\nICLR\n\u2022\nMDP\n\u2022\nRL\n23 Jan 2020\nIntroduction\nThe paper studies\nobservational overfitting\n: The phenomenon where an agent overfits to different observation spaces even though the underlying MDP remains fixed.\nUnlike other works, the \u201cbackground information\u201d (in the pixel space) is correlated with the progress of the agent (and is not just noise).\nLink to the paper\nSetup\nBase MDP $M = (S, A, R, T)$ where $S$ is the state space, $A$ is the action space, $R$ is the reward function, and $T$ is the transition dynamics.\n$M$ is parameterized using $\\theta$. In practice, it means introducing an observation function $\\phi_{\\theta}$ ie $M_{\\theta} = (M, \\phi_{\\theta})$.\nA distribution over $\\theta$ defines a distribution over the MDPs.\nThe learning agent has access to the pixel space observations and not the state space observations.\nGeneralization gap is defined as $J_{\\theta}(\\pi) - J_{\\theta^{train}}(\\pi)$ where $\\pi$ is the learning agent, $\\theta$ is the distribution over all the observation functions, $\\theta^{train}$ is the distribution over the observation functions corresponding to the training environments. $J_{\\theta}(\\pi)$ is the average reward that the agent obtains over environments sampled from $M_{\\theta}$.\n$\\phi_{\\theta}$ considers two featurs - generalizable (invariant across $\\theta$) and non-generalizable (depends on $\\theta$) ie $\\phi_{\\theta}(s) = concat(f(s), g_{\\theta}(s))$ where $f$ is the invariant function and $g$ is the non-generalizable function.\nThe problem is set up such that \u201cexplicit regularization\u201d can easily solve it. The focus is on understanding the effect of \u201cimplicit regularization\u201d.\nExperiments\nOverparameterized LQR\nLQR is used as a proxy for deep RL architectures given its advantages like enabling exact gradient descent.\nThe functions are parameterized as follows:\n$f(s) = W_c(s)$\n$g_{\\theta}(s) = W_{\\theta}(s)$\nObservation at time $t$ , $o_t$, is given as $[W_c W_{\\theta}]^{-1} s_t$.\nAction at time $t$ is given as $a_t = K o_{t}$ where $K$ is the policy matrix.\nDimensionality:\nstate $s$: $d_{state}$ 100\n$f(s)$: $d_{state}$ 100\n$g_{\\theta}(s)$: $d_{noise}$ 100\nobservation $o$: $d_{state}$ + $d_{noise}$ 1100\nIn case of training on just one environment, multiple solutions exist, and overfitting happens.\nIncreasing $d_{noise}$ increases the generalization gap.\nOverparameterizing the network decreases the generalization gap and also reduces the norm of the policy.\nProjected Gym Environments\nThe base MDP is the Gym Environment.\n$M_{\\theta}$ is generated as before.\nIncreasing both width and depth for basic MLPs improves generalization.\nGeneralization also depends on the choice of activation function, residual layers, etc.\nDeconvolutional Projections\nIn the Gym environment, the actual state is projected to a larger vector and reshaped into an 84x84 tensor (image).\nThe image from $f$ is concatenated with the image from $g$. This setup is referred to as the Gym-Deconv.\nThe relative order of performance between NatureCNN, IMPALA, and IMPALA-Large (on both CoinRun and Gym-Deconv) is the same as the order of the number of parameters they contain.\nIn an ablation, the policy is given access to only $g_{\\theta}(s)$, which makes it impossible for the model to generalize. In this test of memorization capacity, implicit regularization seems to reduce the memorization effect.\nOverparameterization in CoinRun\nThe pixel space observation in CoinRun is downsized from 64x64 to 32x32 and flattened into a vector.\nIn CoinRun, the dynamics change per level, and the noisy \u201cirrelevant\u201d features change location across the 1D input, making this setup more challenging than the previous ones.\nOverparameterization improves generalization in this scenario as well.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1912.02975"
    },
    "46": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Rapid-Learning-or-Feature-Reuse-Towards-Understanding-the-Effectiveness-of-MAML",
        "transcript": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nRapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML\n2019\n\u2022\nICLR 2020\n\u2022\nMeta Learning\n\u2022\nAI\n\u2022\nICLR\n\u2022\nMAML\n16 Jan 2020\nIntroduction\nThe paper investigated two possible reasons behind the usefulness of MAML algorithm:\nRapid Learning\n- Does MAML learn features that are amenable for rapid learning?\nFeature Reuse\n- Does the MAML initialization provide high-quality features that are useful for the unseen tasks.\nThis leads to a follow-up question: how much task-specific inner loop adaptation is needed.\nLink to the paper\nApproach\nIn a standard few-shot learning setup, the different datasets have different classes. Hence, the top-most layer (or the head) of the learning model should be different for different tasks.\nThe subsequent discussion only applies to the body of the network (ie, network minus the head).\nFreezing Layer Representations\nIn this setup, a subset (or all) of parameters are frozen (after MAML training) and are not adapted during the representation.\nEven when the entire network is frozen, the performance drops only marginally.\nThis indicates that the representation learned by the meta-initialization is good enough to be useful on the test tasks (without requiring any adaptation step).\nNote that the head of the network is still adapted during testing.\nRepresentational Similarity\nIn this setup, the paper reports the change in the latent representation (learned by the network) during the inner loop update with a fully trained model.\nCanonical Correlation Analysis (CCA) and Central Kernel Alignment (CKA) metrics are used to measure the similarity between the representations.\nThe main finding is that the representations in the body of the network are very similar before and after the inner loop updates while the representations in the head of the network are very different.\nThe above two observations indicate that feature reuse is the primary driving factor for the success of MAML.\nWhen does feature reuse happen\nThe paper considers the model at different stages of training and compares the similarity in the representation (before and after the inner loop update).\nEven early in training, the CCA similarity between the representations (before and after the inner loop update) is quite high. Similarly, freezing the layers (for the test time update), early in training, does not degrade the test time performance much. This hints that the feature reuse happens early in the learning process.\nThe ANIL (Almost No Inner Loop) Algorithm\nThe empirical evidence suggests that the success of MAML lies in the feature reuse.\nThe authors build on this observation and propose a simplification of the MAML algorithm: ANIL or Almost No Inner Loop Algorithm\nIn this algorithm, the inner loop updates are applied only to the head of the network.\nDespite being much more straightforward, the performance of ANIL is close to the performance of MAML for both few-shot image classification and RL tasks.\nRemoving most of the inner loop parameters speed up the computation by a factor of 1.7 (during training) and 4.1 (during inference).\nRemoving the Inner Loop Update\nGiven that it is possible to remove most of the parameters from the inner loop update (without affecting the performance), the next step is to check if the inner loop update can be removed entirely.\nThis leads to the NIL (No Inner Loop) algorithm, which does not involve any inner loop adaptation steps.\nAlgorithm\nA few-shot learning model is trained - either with MAML or ANIL.\nDuring testing, the head is removed.\nFor each task, the K training examples are fed to the body to obtain class representations.\nFor a given test data point, the representation of the data point is compared with the different class representations to obtain the target class.\nThe NIL algorithm performs similar to the MAML and the ANIL algorithms for the few-shot image classification task.\nNote that it is still important to use MAML/ANIL during training, even though the learned head is not used during evaluation.\nConclusion\nThe paper discusses the different classes of meta-learning approaches. It concludes with the observation that feature reuse (and not rapid adaptation) seems to be the common model of operation for both optimization-based meta-learning (e.g., MAML) and model-based meta-learning.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1909.09157"
    },
    "47": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Accurate-Large-Minibatch-SGD-Training-ImageNet-in-1-Hour",
        "transcript": "Accurate, Large Minibatch SGD - Training ImageNet in 1 Hour \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nAccurate, Large Minibatch SGD - Training ImageNet in 1 Hour\n2017\n\u2022\nDistributed Computing\n\u2022\nDistributed SGD\n\u2022\nEmpirical Advice\n\u2022\nSynchronous SGD\n\u2022\nAI\n\u2022\nImageNet\n09 Jan 2020\nIntroduction\nTraining models with large minibatches (using distributed synchronous SGD) can lead to optimization issues.\nThe paper presents techniques for training models with large batch size while matching the accuracy of small minibatch setups.\nThe paper focuses on the ImageNet dataset, but many of the proposed ideas are applicable broadly.\nLink to the paper\nLinear Scaling Rule\nWhen the minibatch size increases by a factor of\nk\n, the learning rate should also be increased by a factor of\nk\n(while keeping all other hyperparameters like weight decay fixed).\nNote that this is an empirical rule and is not expected to hold under all conditions.\nOne such condition is when the model is changing rapidly during the first few epochs. In this case, a warmup phase is introduced to stabilize the model.\nThe paper verifies that the scaling rule is applicable to batch sizes as large as 8K.\nWarmup\nThe learning rate should be gradually ramped up from a small value to a large value to allow convergence.\nBatch Normalization\nBatch normalization uses batch statistics to normalize the data. Hence, the loss corresponding to each data point (in the batch) is not independent. Thus, changing the batch size could change the underlying function being optimized.\nIn the distributed SGD setup, the per-GPU (or per-worker) batch size should be kept constant, and only one worker should compute the batch norm statistics.\nPitfalls when using distributed SGD\nWhen using weight decay, scaling the cross-entropy loss is not the same as scaling the learning rate.\nWhen using momentum, changing the learning rate could require \u201cmomentum correction.\u201d\nEnsure that the per-worker loss is normalized by the size of the total minibatch and not just by the size of minibatch that each worker sees.\nFor each epoch, uses a single random shuffling of the training data (before dividing between the workers).\nCommunication\nThe paper describes various techniques to speed up the training pipeline by reducing the communication overhead between nodes. (Each node can have one or more GPUs).\nFirst, a node sums the gradient from all the GPUs it has.\nThe gradients are shared and summed across all the nodes.\nEach node broadcasts the resulting gradient to all the GPUs it has.\nGradient Aggregation is performed in parallel with the backpropagation operator. While aggregating the gradient for one layer, the system starts computing the gradient of the next layer.\nResults\nUsing these approaches, a Resnet50 model can be trained on the ImageNet dataset in an hour (using 256 workers).\nWhen an appropriate warmup strategy is used, the training and the validation curves (for the large batch size setup) matches the corresponding curves for the small batch size setup.\nThe best performing warmup strategy is the one where training starts at a learning rate of 0.1 and linearly increases to 3.2 over five epochs.\nThe paper shows that the results are not specific to the Resnet50 model (experiments with Resnet101 model) or the use case (experiments with object detection and instance segmentation using Mask R-CNN).\nAlong with providing the empirical validation of the proposed ideas, the paper describes all the hyperparameters. It also includes the training and validation curves with the different configurations which enable others to replicate and build on this work.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.02677"
    },
    "48": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Superposition-of-many-models-into-one",
        "transcript": "Superposition of many models into one \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nSuperposition of many models into one\n2019\n\u2022\nContinual Learning\n\u2022\nLifelong Learning\n\u2022\nAI\n\u2022\nCL\n\u2022\nLL\n02 Jan 2020\nIntroduction\nThe paper proposes a technique (called Parameter Superposition or PSP) for training and storing multiple models within a single set (or instance) of parameters.\nThe different models exist in \u201csuperposition\u201d and can be retrieved dynamically given task-specific context information.\nLink to the paper\n.\nParameter Substitution\nConsider a task with input \\(x \\in R^N\\) and parameter \\(W$ \\in R^{M \\times N}\\) where the output (target or features) are given as \\(y=Wx\\).\nNow consider \\(K\\) such tasks with parameters \\(W_1, W_2, \\cdots W_K\\).\nIf each \\(W_k\\) requires only a small subspace in \\(R^N\\), then a linear transformation \\(C_k^{-1}\\) can be used such that each \\(W_kC_k^{-1}\\) occupies a mutually orthogonal subspace in \\(R^N\\).\nThe set of parameters \\(W_1, \\cdots W_K\\) can be represented by a single \\(W^{M \\times N}\\) by adding \\(W_kC_k^{-1}\\).\nThe parameter corresponding to the \\(k^{th}\\) task can be retrived (with some noise) using the context \\(C_k\\) as \\(W^{~}_k = WC_k\\)\nEven though the retrieval is noisy, the effect of noise is limited for the context vectors used in the paper.\nFinally, \\(\\widetilde(y) = \\widetilde(W)_{k}x = (WC_{k})x = W(C_{k}x)\\)\nInstead of learning \\(K\\) separate models, only \\(K\\) context vectors (along with 1 superimposed model) needs to be learned.\nThe key assumption is that \\(N\\) (in \\(x \\in R^N)\\) is large enough such that each \\(W_k\\) requires only a small subspace of \\(R^N\\).\nSince images and speech signals tend to occupy a low dimensional manifold, this requirement can be satisfied by over-parameterizing x.\nChoice of Context C\nRotational Superposition (pspRotation)\nSample rotations uniformly from the orthogonal group \\(O(M)\\).\nDownside is that if \\(M \\sim N\\), it requires storing as many parameters as learning \\(K\\) individual models (since \\(C\\) is of the size of ##M \\times M$$).\nComplex Superposition (pspComplex)\nThe design of rotational superposition can be improved by choosing \\(C_k\\) to be a diagonal matrix ie \\(C_k = diag(c_k)\\) where \\(c_k\\) is a vector of size \\(M\\).\nChoosing \\(c_k\\) to be a vector of complex numbers (of the form \\(c_{k}^{j} = e^{i\\phi_{j}(k)}\\) where \\(\\phi_{j}(k)\\) or the phase is sampled uniformly from \\([-\\pi, \\pi]\\)) leads to \\(C_k\\) being a digonal orthogonal matrix.\nPowers of a single context\nThe memory footprint can be further reduced by choosing the context vectors to be integral powers of the first context vector.\nBinary Superposition (pspBinary)\nThis is a special case of complex superposition where the context vectors are binary.\nNeural Network Superposition\nThe parameter superposition principle can be applied to all the linear layers of a network.\nFor the convolutional layers, it makes more sense to apply superposition to the convolutional kernel and not to the input image (as the dimensionality of convolutional parameters is smaller than that of inputs).\nExperiments\nFor all the experiments, the baseline is a standard supervised learning setup, unless mentioned otherwise.\nThe metric is the performance on the previous tasks when the model has been trained on the newer tasks.\nInput Interference\nThe input distribution changes over time.\nPermuted MNIST dataset is used where each permutation of the pixels corresponds to a new task.\nA new task is sampled every 1000 mini-batches.\nAs the network size increases, the performance of Parameter Superposition (psp) outperforms the baseline significantly.\npspRotation > pspComplex > pspBinary in terms of both performance and the number of additional parameters required for each new task.\nGiven that pspBinary is the easiest to implement while being comparable to more sophisticated baselines like Elastic Weight Consolidation (EWC) and Synaptic Intelligence, the paper presents most of the results with the pspBinary model.\nContinous Domain Shift\nRotating-MNIST and Rotating-FashionMNIST tasks are proposed to simulate continuous domain shift.\nIn these tasks, the input images are rotated in-plane by a small angle such that the rotation is complete after 1000 steps.\nA new context is assigned after 100 steps as per step changes in the angle would be very small.\nThe 10 context vectors used in the first 1000 steps are reused for the subsequent steps.\nRandomly changing the context vector\nThe paper considers an ablation where the context vector is randomly changed at every step (of the 1000 step cycle). This required the superposition model to store 1000 models.\nThis approach is better than the supervised learning baseline but not as good as the proposed psp* models.\nOutput Interference\nThis is the setup where the model transitions from one classification task to another.\nIncremental CIFAR dataset is used with Resnet18 as the base model.\nBaseline is a standard supervised learning model where a new classification head is used for each task (since the classes have a different meaning in each dataset). The model component before the classification layer is shared across the tasks.\nEven though the labels are different across the datasets, the pspBinary model, trained with a single output layer, outperforms the multi-headed baseline.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1902.05522"
    },
    "49": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Towards-a-Unified-Theory-of-State-Abstraction-for-MDPs",
        "transcript": "Towards a Unified Theory of State Abstraction for MDPs \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nTowards a Unified Theory of State Abstraction for MDPs\n2006\n\u2022\nMarkov Decision Process\n\u2022\nReinforcement Learning\n\u2022\nState Abstraction\n\u2022\nAI\n\u2022\nMDP\n\u2022\nRL\n26 Dec 2019\nIntroduction\nThe paper studies five different techniques for stat abstraction in MDPs (Markov Decision Processes) and evaluates their usefulness for planning and learning.\nThe general idea behind abstraction is to map the actual (or observed) state to an abstract state that should be more amenable for learning.\nIt can be thought of as a mapping from one representation to another representation while preserving some useful properties.\nLink to the paper\nGeneral Definition\nConsider a MDP \\(M = <S, A, P, R, \\gamma>\\) where \\(S\\) is the finite set of states, \\(A\\) is finite set of actions, \\(P\\) is the transition function, \\(R\\) is the bounded reward function and \\(\\gamma\\) is the discount factor.\nThe abstract version of the MDP is \\(\\widetilde{M} = <\\widetilde{S}, A, \\widetilde{P}, \\widetilde{R}, \\gamma>\\) where \\(\\widetilde{S}\\) is the finite set if abstract states, \\(\\widetilde{P}\\) is the transition function in the abstract state space and \\(\\widetilde{R}\\) is the bounded reward function in the abstract reward space.\nAbstraction function \\(\\phi\\) is a function that maps a given state \\(s\\) to its abstract counterpart \\(\\widetilde{s}\\).\nThe inverse image \\(\\phi^{-1}(\\widetilde{s})\\) is the set of ground states that map to the \\(\\widetilde{s}\\) under the abstraction function \\(\\phi\\).\nA wieghing functioon \\(w(s)\\) is used to measure how much does a state \\(s\\) contribute to the abstract state \\(\\phi(s)\\).\nTopology of Abstraction Space\nGiven two abstraction functions \\(\\phi_{1}\\) and \\(\\phi_{2}\\), \\(\\phi_{1}\\) is said to be\nfiner\nthan \\(\\phi_{2}\\) iff for any states \\(s_{1}, s_{2}\\) if \\(\\phi_{1}(s_{1}) = \\phi_{1}(s_{2})\\) then \\(\\phi_{2}(s_{1}) = \\phi_{2}(s_{2})\\).\nThis\nfiner\nrelation is reflex, antisymmetric, transitive and partially ordered.\nFive Types of Abstraction\nWhile many abstractions are possible, not all abstractions are equally important.\nModel-irrelevance abstraction \\(\\phi_{model}\\):\nIf two states $s_{1}$ and $s_{2}$ have the same abstracted state, then their one-step model is preserved.\nConsider any action \\(a\\) and any abstract state \\(\\widetilde{s}\\), if \\(\\phi_{model}(s_{1} = \\phi_{model}(s_{2})\\) then \\(R(s_1, a) = R(s_2, a)\\) and \\(\\sum_{s' \\in \\phi_{model}^{-1}\\widetilde(s)}P_{s_1, s'}^{a} = \\sum_{s' \\in \\phi_{model}^{-1}\\widetilde(s)}P_{s_2, s'}^{a}\\).\n\\(Q^{\\pi}\\)-irrelevance abstraction:\nIt preserves the state-action value finction for all the states.\n\\(\\phi_{Q^{\\pi}}(s_1) = \\phi_{Q^{\\pi}}(s_2)\\) implies \\(Q^{\\pi}(s_1, a) = Q^{\\pi}(s_1, a)\\).\n\\(Q^{*}\\)-irrelevance abstraction:\nIt preserves the optimal state-action value function.\n\\(a^{*}\\)-irrelevance abstraction:\nIt preserves the optimal action and its value function.\n\\(\\phi_{\\pi^{*}}\\)-irrelevance abstraction:\nIt preserves the optimal action.\nIn terms of\nfineness\n, \\(\\phi_0 \\geq \\phi_{model} \\geq \\phi_{Q^{\\pi}} \\geq \\phi_{Q^*} \\geq \\phi_{a^*} \\geq \\phi_{\\pi^*}\\). Here \\(\\phi_0\\) is the identity mapping ie \\(\\phi_0(s) = s\\)\nIf a property applies to any abstraction, it also applies to all the finer abstractions.\nKey Theorems\nAs we go from finer to coarser abstractions, the information loss increases (ie fewer components can be recovered) while the state-space reduces (ie the efficiency of solving the problem increases). This leads to a tradeoff when selecting abstractions.\nFor example, with abstractions \\(\\phi_{model}, \\phi_{Q^{\\pi}}, \\phi_{Q^*}, \\phi_{a^*}\\), the optimal abstract policy \\(\\widetilde(\\pi)^*\\) is optimal in the ground MDP.\nSimilarly, if each state-action pair is visited infinitely often and the step-size decays properly, Q-learning with \\(\\phi_{model}, \\phi_{Q^{\\pi}}, \\phi_{Q^*}\\) converges to the optimal state-action value functions in the MDP. More conditions are needed for convergence in the case of the remaining two abstractions.\nFor \\(\\phi_{model}, \\phi_{Q^{\\pi}}, \\phi_{Q^*}, \\phi_{a^*}\\), the model built with the experience converges to the true abstract model with infinite experience if the weighing function \\(w(s)\\) is fixed.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://pdfs.semanticscholar.org/ca9a/2d326b9de48c095a6cb5912e1990d2c5ab46.pdf"
    },
    "50": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations",
        "transcript": "ALBERT - A Lite BERT for Self-supervised Learning of Language Representations \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nALBERT - A Lite BERT for Self-supervised Learning of Language Representations\n2019\n\u2022\nICLR 2019\n\u2022\nNatural Language Processing\n\u2022\nRepresentation Learning\n\u2022\nAI\n\u2022\nAttention\n\u2022\nICLR\n\u2022\nNLP\n\u2022\nTransformer\n\u2022\nSOTA\n19 Dec 2019\nIntroduction\nThe paper proposes parameter-reduction techniques to lower the memory consumption (and improve training speed) of BERT.\nIt also proposes to use a self-supervised loss (based on inter-sentence coherence) and argues that this loss is better than the NSP loss used by BERT.\nLink to the paper\nArchitecture\nALBERT architecture is similar to that of BERT with three major differences.\nFactorized Embedding Parameterization\nIn BERT and followup works, the embedding size was tied to the size of the context vector.\nSince context vector is expected to encoder the entire context, it needs to have a large dimensionality.\nOne consequence of this choice is that even the embedding layer (which encodes the representation for each token) has a large size. This increases the overall memory footprint of the model.\nThe paper proposed to factorize the embedding parameters into two smaller matrics.\nThe embedding layer learns a low dimensional representation of the tokens and this representation is projected into a high dimensional space.\nCross-layer parameter sharing\nALBERT shares all the parameters across the layers.\nInter-sentence coherence loss\nBERT uses two losses - Masked Language Modeling loss (MLM) and Next Sentence Prediction (NSP).\nIn the NSP task, the model is provided a pair of sentences and it has to predict if the two sentences appear consecutively in the same document or not. Negative samples are created by sampling sentences from different documents.\nThe paper argues that NSP is not effective as a loss function as it merges topic prediction and coherence prediction into one task (as the two sentences come from different documents). The topic prediction is an easier task as compared to coherence prediction.\nHence the paper proposes to use the Sentence Order Prediction task where the model has to predict which of the two sentences comes first in a document. The negative samples are created by simply swapping the order in the positive samples. Hence both the sentences come from the same document and topic prediction alone can not be used to solve the task.\nSetup\nDifferent variants (in terms of size) of ALBERT and BERT models are compared (eg ALBERT, ALBERT-x, BERT-x, etc).\nIn general, ALBERT models have many-times fewer parameters as compared to the BERT models.\nDatasets - BookCorpus, English Wikipedia.\nObservations\nALBERT-xxlarge significantly outperforms the BERT-large model even though it has around 70% parameters as the BERT-large model.\nBERT-xlarge performs worse than BERT-base hinting that it is difficult to train such large models.\nALBERT models also have better data throughput as compared to BERT models.\nFor the ALBERT models, an embedding size of 128 performs the best.\nAs the hidden dimension is increased, the model obtains better performance, but with diminishing returns.\nVery wide ALBERT models (say with a context size of 1024) do not benefit much from depth.\nUsing additional training data boosts the performance for most of the downstream tasks.\nThe paper empirically shows that using dropout could hurt the performance of the ALBERT models. This observation may not hold for BERT as it does not share parameters across layers and hence may need regularization via dropout.\nALBERT also improves the state of the art performance on GLUE, SQuAD and RACE benchmarks, for both single-model and ensemble setup.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1909.11942"
    },
    "51": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Mastering-Atari,-Go,-Chess-and-Shogi-by-Planning-with-a-Learned-Model",
        "transcript": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nMastering Atari, Go, Chess and Shogi by Planning with a Learned Model\n2019\n\u2022\nDeep Reinforcement Learning\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nDRL\n\u2022\nModel-Based\n\u2022\nModel-Free\n\u2022\nPlanning\n\u2022\nRL\n05 Dec 2019\nIntroduction\nThe paper presents the MuZero algorithm that performs planning with a learned model.\nThe algorithm achieves state of the art results on Atari suite (where generally model-free approaches perform the best) and on planning-oriented games like Chess and Go (where generall planning approaches perform the best).\nLink to the paper\nRelation to standard Model-Based Approaches\nModel-based approaches generally focus on reconstructing the true environment state or the sequence of full observations.\nMuZero focuses on predicting only those aspects that are most relevant for planning - policy, value functions, and rewards.\nApproach\nThe model consists of three components: (representation) encoder, dynamics function, and the prediction network.\nThe learning agent has two kinds of interactions - real interactions (ie the actions that are actually executed in the real environment) and hypothetical or imaginary actions (ie the actions that are executed in the learned model or the dynamics function).\nAt any timestep\nt\n, the past observations\no\n1\n, \u2026\no\nt\nare encoded into the state\ns\nt\nusing the encoder.\nNow the model takes hypothetical actions for the next\nK\ntimesteps by unrolling the model for\nK\nsteps.\nFor each timestep\nk = 1, \u2026, K\n, the dynamics model predicts the immediate reward\nr\nk\nand a new hidden state\nh\nk\nusing the previous hidden state\nh\nk-1\nand action\na\nk\n.\nAt the same time, the policy\np\nk\nand the value function\nv\nk\nare computed using the prediction network.\nThe initial hidden state\nh\n0\nis initialized using the state\ns\nt\nAny MDP Planning algorithm can be used to search for optimal policy and value function given the state transitions and the rewards induced by the dynamics function.\nSpecifically, the MCTS (Monte Carlo Tree Search) algorithm is used and the action\na\nt+1\n(ie the action that is executed in the actual environment) is selected from the policy outputted by MCTS.\nCollecting Data for the Replay Buffer\nAt each timestep\nt\n, the MCTS algorithm is executed to choose the next action (which will be executed in the real environment).\nThe resulting next observation\no\nt+1\nand reward\nr\nt+1\nare stored and the trajectory is written to the replay buffer (at the end of the episode).\nObjective\nFor every hypothetical step\nk\n, match the predicted policy, value, and reward to the actual target values.\nThe target policy is generated by the MCTS algorithm.\nThe target value function and reward are generated by actually playing the game (or the MDP).\nRelation to AlphaZero\nMuZero leverages the search-based policy iteration from AlphaZero.\nIt extends AlphaZero to setups with a single agent (where self-play is not possible) and setups with a non-zero reward at the intermediate time steps.\nThe encoder and the predictions functions are similar to ones used by AlphZero.\nResults\nK\nis set to 5.\nEnvironments: 57 games in Atari along with Chess, Go and Shogi\nMuZero achieves the same level of performance as AlphaZero for Chess and Shogi. In Go, MuZero slightly outperforms AlphaZero despite doing fewer computations per node in the search tree.\nIn Atari, MuZero achieves a new state-of-the-art compared to both model-based and model-free approaches.\nThe paper considers a variant called MuZero Reanalyze that reanalyzes old trajectories by re-running the MCTS algorithm with the updated network parameter. The motivation is to have a better sample complexity.\nMuZero performs well even when using a single simulation of MCTS (during inference).\nDuring training, using more simulations of MCTS helps to achieve better performance through even just 6 simulations per move is sufficient to learn a good model for Ms. Pacman.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1911.08265"
    },
    "52": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Contrastive-Learning-of-Structured-World-Models",
        "transcript": "Contrastive Learning of Structured World Models \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nContrastive Learning of Structured World Models\n2019\n\u2022\nGraph Neural Network\n\u2022\nObject-Oriented Learning\n\u2022\nRelational Learning\n\u2022\nAI\n\u2022\nGraph\n\u2022\nGNN\n28 Nov 2019\nIntroduction\nThe paper introduces Contrastively-trained Structured World Models (C-SWMs).\nThese models use a contrastive approach for learning representations in environments with compositional structure.\nLink to the paper\nLink to the code\n.\nApproach\nThe training data is in the form of an experience buffer \\(B = \\{(s_t, a_t, s_{t+1})\\}_{t=1}^T\\) of state transition tuples.\nThe goal is to learn:\nan encoder \\(E\\) that maps the observed states $s_t$ (pixel state observations) to latent state $z_t$.\na transition model \\(T\\) that predicts the dynamics in the hidden state.\nThe model defines the enegry of a tuple \\((s_t, a_t, s_{t+1})\\) as \\(H = d(z_t + T(z_t, a_t), z_{t+1})\\).\nThe model has an inductive bias for modeling the effect of action as translation in the abstract state space.\nAn extra hinge-loss term is added: \\(max(0, \\gamma - d(z^{~}_{t}, z_{t+1}))\\) where \\(z^{~}_{t} = E(s^{~}_{t})\\) is a corrputed latent state corresponding to a randomly sampled state \\(s^{~}_{t}\\).\nObject-Oriented State Factorization\nThe goal is to learn object-oriented representations where each state embedding is structured as a set of objects.\nAssuming the number of object slots to be \\(K\\), the latent space, and the action space can be factored into \\(K\\) independent latent spaces (\\(Z_1 \\times ... \\times Z_K\\)) and action spaces (\\(A_1 \\times ... \\times A_k\\)) respectively.\nThere are\nK\nCNN-based object extractors and an MLP-based object encoder.\nThe actions are represented as one-hot vectors.\nA fully connected graph is induced over\nK\nobjects (representations) and the transition function is modeled as a Graph Neural Network (GNN) over this graph.\nThe transition function produces the change in the latent state representation of each object.\nThe factorization can be taken into account in the loss function by summing over the loss corresponding to each object.\nEnvironments\nGrid World Environments - 2D shapes, 3D blocks\nAtari games - Pong and Space Invaders\n3-body physics simulation\nSetup\nRandom policy is used to collect the training data.\nEvaluation is performed in the latent space (no reconstruction in the pixel space) using ranking metrics. The observations (to compare against) are randomly sampled from the buffer.\nBaselines - auto-encoder based World Models and\nPhysics as Inverse Graphics model\n.\nResults\nIn the grid-world environments, C-SWM models the latent dynamics almost perfectly.\nRemoving either the state factorization or the GNN transition model hurts the performance.\nC-SWM performs well on Atari as well but the results tend to have high variance.\nThe optimal values of $K$ should be obtained by hyperparameter tuning.\nFor the 3-body physics tasks, both the baselines and proposed models work quite well.\nInterestingly, the paper has a section on limitations:\nThe object extractor module can not disambiguate between multiple instances of the same object (in a scene).\nThe current formulation of C-SWM can only be used with deterministic environments.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1911.12247"
    },
    "53": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Gossip-based-Actor-Learner-Architectures-for-Deep-RL",
        "transcript": "Gossip based Actor-Learner Architectures for Deep RL \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nGossip based Actor-Learner Architectures for Deep RL\n2019\n\u2022\nDeep Reinforcement Learning\n\u2022\nDistributed Reinforcement Learning\n\u2022\nNeurips 2019\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nDRL\n\u2022\nNeurips\n\u2022\nRL\n12 Sep 2019\nLink to the paper\nThe paper considers the task of training an RL system by sampling data from multiple simulators (over parallel devices).\nThe setup is that of distributed RL setting with\nn\nagents or actor-learners (composed of a single learner and several actors). These agents are trying to maximize a common value function.\nOne (existing) approach is to perform on-policy updates with a shared policy. The policy could be updated in synchronous (does not scale well) or asynchronous manner (can be unstable due to stale gradients).\nOff policy approaches allow for better computational efficiency but can be unstable during training.\nThe paper proposed Gossip based Actor-Learner Architecture (GALA) which uses asynchronous communication (gossip) between the\nn\nagents to improve the training of Deep RL models.\nThese agents are expected to converge to the same policy.\nDuring training, the different agents are not required to share the same policy and it is sufficient that the agent\u2019s policies remain $\\epsilon$-close to each other. This relaxation allows the policies to be trained asynchronously.\nGALA approach is combined with A2C agents resulting in GALA-A2C agents. They have better computational efficiency and scalability (as compared to A2C) and similar in performance to A3C and Impala.\nTraining alternates between one local policy-gradient (and TD update) and asynchronous gossip between agents.\nDuring the gossip step, the agents send their parameters to some of the other agents (referred to as the peers) and update their parameters based on the parameters received from the other agents (for which the given agent is a peer).\nGALA agents are implemented using non-blocking communication so that they can operate asynchronously.\nThe paper includes the proof that the policies learned by the different agents are within $\\epsilon$ distance of each other (ie all the policies lie within an $\\epsilon$-distance ball) thus ensuring that the policies do not diverge much from each other.\nSix games from the Ataru 2600 games suite are used for the experiments.\nBaselines: A2C, A3C, Impala\nGALA agents are configured in a directed ring graph topology.\nWith A2C, as the number of simulators increases, the number of convergent runs (runs with a threshold reward) decreases.\nUsing gossip algorithms increases or maintains the number of convergent runs. It also improves the performance, sample efficiency and compute efficiency of A2C across all the six games.\nWhen compared to Impala and A3C, GALA-A2C generally outperforms (or performs as well as) those baselines.\nGiven that the learned policies remain within an $\\epsilon$ ball, the agent\u2019s gradients are less correlated as compared to the A2C agents.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.04585"
    },
    "54": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/How-to-train-your-MAML",
        "transcript": "How to train your MAML \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nHow to train your MAML\n2018\n\u2022\nEmpirical Advice\n\u2022\nICLR 2019\n\u2022\nMeta Learning\n\u2022\nAI\n\u2022\nICLR\n\u2022\nMAML\n05 Sep 2019\nIntroduction\nThe paper proposes MAML++ - a modification of MAML algorithm that stabilizes its training improves generalization performance and reduces the computational overhead.\nLink to the paper\nNotes\nUnstable Training\nTraining the outer loop requires unfolding the inner loop multiple times.\nIn absence of skip connections, the gradient is multiplied by the same parameter multiple times.\nLarge depth and absent skip connections could lead to exploding and vanishing gradients respectively.\nThe paper proposes to stabilize the gradient propagation by minimizing the target set loss computed by the base-network after every step towards a support set task.\nIt is important to anneal the contribution of earlier steps and increase the contribution of later steps over time.\nSecond Order derivatives are expensive to compute\nWhile the first-order MAML is faster, the resulting model may not have as good a generalization error as the second-order MAML.\nThe paper proposes to use derivative order annealing where the first order gradients are used for the first 50 epochs and the network uses second-order gradients from thereon.\nThis derivative order annealing appears to be more stable than models that use second-order derivatives only.\nBatch Normalization\nIn MAML, the statistics of the current batch are used for normalization instead of accumulating the running statistics.\nThe paper proposes to collect the statistics per step which can increase the convergence speed, stability, and generalization performance.\nIn MAML, the batch normalization biases are not updated in the inner-loop which can adversely impact the performance.\nThe paper proposes to learn a set of biases (per step) within the inner loop update.\nFixed Learning Rate\nMAML uses a single learning rate across all the steps and all the parameters. This means there is one single learning rate that needs to be hyperparameter to work well for all the layers and steps.\nAn alternate solution would be to learn a separate learning rate per parameter but this can be impractical as it doubles the number of parameters to be learned.\nThe paper proposes to learn a learning rate and direction for each layer in the network, for each step it takes in the inner loop.\nThe paper also proposed to anneal the learning rate of the outer loop (using cosine annealing) as it helps to achieve better generalization.\nResults\nUsing these modifications helps to outperform the MAML model on both Omniglot and MiniImagenet datasets.\nThe biggest benefit comes by learning the per-layer, per-step learning rates and by using the per-step batch normalization.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1810.09502"
    },
    "55": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/PHYRE-A-New-Benchmark-for-Physical-Reasoning",
        "transcript": "PHYRE - A New Benchmark for Physical Reasoning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nPHYRE - A New Benchmark for Physical Reasoning\n2019\n\u2022\nPhysical Reasoning\n\u2022\nAI\n\u2022\nBenchmark\n\u2022\nDataset\n\u2022\nPhysics\n\u2022\nReasoning\n29 Aug 2019\nIntroduction\nThe paper proposes the PHYRE (PHYsical REasoning) benchmark - consisting of classic mechanical puzzles in 2D physical environments - as a means to evaluate the physical reasoning ability of machine learning models.\nLink to the paper\nEnvironment\n2D world that obeys Newtonian mechanics.\nGravitational force + Friction.\nNon-deformable objects that can be static (ie fixed) or dynamic (ie can move and are affected by collisions etc).\nTask\nThe learning agent starts in some initial world state (ie configuration of objects).\nGoal is described in the form of (\nsubject\n,\nrelation\n,\nobject\n) where the agent\u2019s task is to satisfy the\nrelation\nbetween the\nsubject\nand the\nobject\n.\nCurrently, only the \u201ctouch\u201d\nrelation\nis supported.\nSetup\nThe learning agent has to take a single action - placing one or more new dynamic objects in the world.\nA simulator is run on the new configuration (for a fixed amount of time) to check if the goal condition is satisfied.\nAt the end of the simulation, a binary reward and intermediate observations (collected as the simulator executes) are provided to the learning agent.\nThese observations are 256*256 grids where each grid cell can take 1 of the 7 values (denoting different types of objects).\nSince only one relation supported currently, the color is sufficient to encode the goal.\nBenchmark Tiers\nTwo benchmark tiers are provided where each tier comprises of a combination of:\na predefined set of all the actions that the agent is allowed to perform.\nset of tasks that can be solved by at least one action from the allowed action set.\nPHYRE-B\n- The agent is allowed to place a single (ball of any radii) at any valid location.\nPHYRE-2B\n- The agent is allowed to place 2 balls at any valid pair of locations.\nEach of the two tiers has 25 task templates where each template comprises of variants of a single task (same goal but different initial conditions).\nEvaluation\nTwo evaluation setups are considered:\nwithin-template\nwhere the agent is trained on some tasks in a template and evaluated on a set of held-out tasks from the same template.\ncross-template\nwhere the agent is evaluated on tasks from a different template.\nIn the training phase, the model has access to the simulator (but not to the correct solution). So the model could learn an action-prediction model or forward dynamics model or both.\nIn the testing phase, the model can query the simulator only a few times. Each query provides it with the binary reward and the intermediate observations.\nPerformance Measure\nThe emphasis is on solving more tasks (in few queries) during the test phase.\nThis requirement is captured using a metric called AUCCESS.\nIn general, the tasks in PHYRE-2B are harder than tasks in PHYRE-B.\nBaseline Agents\nRandom Agent - Randomly samples actions\nNon-parametric agent (MEM) - generates R actions at random and uses the simulator to check how many tasks can be solved using these R random actions. During testing, try the R actions in the decreasing order of the number of tasks they solve.\nNon-parametric agent with online learning (MEM-O) - Variant of MEM where an online adaptation step is performed during test time (to update the rank of the actions).\nDeep Q Networks with an action encoder, observation encoder and fusion model (combine action and observation representation).\nDQN with online learning (DQN-0): Variant of DQN with online updates (during the test phase).\nContextual bandits.\nPolicy learning approaches like PPO and A2C.\nObservations\nBoth Contextual bandits and policy-based approaches show poor training stability.\nThe best agent, DQN-O, reaches AUCCESS of 56.2\\% on PHYRE-B and 39.26\\% on PHYRE-2B. In general, agents with online adaptation perform better.\nThe tasks are designed such that 100000 attempts are sufficient to solve 100\\% of tasks in PHYRE-B and 95\\% of tasks in PHYRE-2B.\nEven though only two tiers are provided right now, the benchmark is readily extensible and new tasks can be added in the future.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1908.05656"
    },
    "56": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Large-Memory-Layers-with-Product-Keys",
        "transcript": "Large Memory Layers with Product Keys \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nLarge Memory Layers with Product Keys\n2019\n\u2022\nKey Value\n\u2022\nNatural Language Processing\n\u2022\nAI\n\u2022\nAttention\n\u2022\nMemory\n\u2022\nNLP\n22 Aug 2019\nIntroduction\nThe paper proposes a structured key-value memory layer that:\nCan scale to a very large size (and capacity).\nHas very low computational overhead.\nSupports exact search in the keyspace.\nCan be easily integrated with neural networks.\nLink to the paper\nArchitecture\nThe memory layer is composed of 3 components:\nQuery Network\nMaps input to a latent space.\nCan be implemented as a feed-forward network.\nAdding batch-norm on top of the query network helps to spread out keys.\nKey selection module\nLets say there are a total of\nK\nkeys of dimensionality\nd\nq\nof which we want to select top\nk\nkeys.\nPartition the set of keys into two sets of\nsubkeys\n(say\nQ\n1\nand\nQ\n2\n) where each subset has\nK\nkeys of dimensionality\nd_q/2\n.\nThe query is split into two subqueries (say\nq\n1\nand\nq\n2\n).\nEach of these two queries are compared with every query in their corresponding set of\nsubkeys\n.\nFor example,\nq\n1\nis compared with every query is\nQ\n1\n.\nTop\nk\nranked keys are selected from each set to create two new sets\nC\n1\nand\nC2\n2\n.\nThe keys from these two sets are combined uder the concatenation operator to obtain\nk\n2\nvectors.\nthe final top\nk\n(concatenated) keys are searched from these *k\n2* keys.\nThe overall complexity is $O((\\sqrt K + k^2) \\times d_q)$ where\nK\nis the total number of keys (whiuc)\nValue lookup table\nThe values (corresponding to selected subkeys) are aggregated (using weighted sum operation) to obtain the output.\nAll the parameters are trainable, though, in practice, only the selected\nk\nmemory slots are updated.\nUsing Multihead attention mechanism helps to improve the performance further.\nExperiments\n1 or more feedforward layers in transformers are placed by the memory layers.\nThe model is evaluated on large scale language modeling tasks with 140 Gb of data from common crawl corpora (28n billion words).\nEvaluation metrics\nPerplexity on the test set.\nFraction of accessed values.\nKL divergence between the (normalized) weights of key access and uniform distribution.\nThe last two metrics are used together to determine how well the keys are utilized.\nResults\nGiven the large size of the training dataset, adding more layers to the transformer model helps.\nEffect of using memory layer is more powerful than the effect of adding new layers to the transformer. For example, a 12 layer transformer + memory layer outperforms a 24 layer transformer while being almost twice as fast.\nThe best position to place the memory is at an intermediate layer and placing the memory layer right after the input or just before the softmax layer does not work well in practice.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1907.05242"
    },
    "57": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Abductive-Commonsense-Reasoning",
        "transcript": "Abductive Commonsense Reasoning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nAbductive Commonsense Reasoning\n2019\n\u2022\nAbductive Reasoning\n\u2022\nNatural Language Inference\n\u2022\nNatural Language Processing\n\u2022\nAI\n\u2022\nDataset\n\u2022\nNLI\n\u2022\nNLP\n\u2022\nReasoning\n15 Aug 2019\nIntroduction\nThe paper presents the task of abductive NLP (pronounced as\nalpha NLP\n) where the model needs to perform abductive reasoning.\nAbductive reasoning is the inference to the most plausible explanation. Even though it is considered to be an important component for understanding narratives, the work in this domain is sparse.\nA new dataset called as Abstractive Reasoning in narrative Text (ART) consisting of 20K narrative contexts and 200k explanations is also provided. The dataset models the task as multiple-choice questions to make the evaluation process easy.\nLink to the paper\nTask Setup\nGiven a pair of observations\nO\n1\nand\nO\n2\nand two hypothesis\nh\n1\nand\nh\n2\n, the task is to select the most plausible hypothesis.\nIn general,\nP(h | O\n1\n, O\n2\n)\nis propotional to\nP(h |O\n1\n)P(O\n2\n|h, O\n1\n)\n.\nDifferent independence assumptions can be imposed on the structure of the problem eg one assumption could be that the hypothesis is independent of the observations or the \u201cfully connected\u201d assumption would jointly model both the observations and the hypothesis.\nDataset\nAlong with crowdsourcing several plausible hypotheses for each observation instance pair, an adversarial filtering algorithm (AF) is used to remove weak pairs of hypothesis.\nObservation pairs are created using the\nROCStories dataset\nwhich is a collection of short, manually crafted stories of 5 sentences.\nThe average word length for both the content and the hypothesis is between 8 to 9.\nTo collect plausible hypothesis, the crowd workers were asked to fill in a plausible \u201cin-between\u201d sentence in natural language.\nGiven the plausible hypothesis, the crowd workers were asked to create an implausible hypothesis by editing fewer than 6 words.\nAdversarial filtering approach from\nZellers et al.\nis used with BERT as the adversary. A temperature parameter is introduced to control the maximum number of instances that can be changed in each adversarial filtering iteration.\nKey Observations\nHuman performance: 91.4%\nBaselines like SVM classifier, the bag-of-words classifier (using Glove) and max-pooling overt BiLSTM representation: approx 50%\nEntailment NLI baseline: 59%. This highlights the additional complexity of abductive NLI as compared to entailment NLI.\nBERT: 68.9%\nGPT: 63.1%\nNumerical and spatial knowledge-based data points are particularly hard.\nThe model is more likely to fail when the narrative created by the incorrect hypothesis is plausible\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1908.05739"
    },
    "58": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Assessing-Generalization-in-Deep-Reinforcement-Learning",
        "transcript": "Assessing Generalization in Deep Reinforcement Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nAssessing Generalization in Deep Reinforcement Learning\n2018\n\u2022\nDeep Reinforcement Learning\n\u2022\nEvaluating Generalization\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nDRL\n\u2022\nEvaluation\n\u2022\nGeneralization\n\u2022\nRL\n01 Aug 2019\nThe paper presents a benchmark and experimental protocol (environments, metrics, baselines, training/testing setup) to evaluate RL algorithms for generalization.\nSeveral RL algorithms are evaluated and the key takeaway is that the \u201cvanilla\u201d RL algorithms can generalize better than the RL algorithms that are specifically designed to generalize, given enough diversity in the distribution of the training environments.\nLink to the paper\nThe focus is on evaluating generalization to environmental changes that affect the system dynamics (and not the goal or rewards).\nTwo generalization regimes are considered:\nInterpolation - parameters of the test environment are similar to the parameters of the training environment.\nExtrapolation - parameters of the test environment are different from the parameters of the training environment.\nFollowing algorithms are considered as part of the benchmark:\n\u201cVanilla\u201d RL algorithms - A2C, PPO\nRL algorithms that are designed to generalize:\nEPOpt - Learn a (robust) policy that maximizes the expected reward over the most difficult distribution of environments (ones with the worst expected reward).\nRL\n2\n- Learn an (adaptive) policy that can adapt to the current environment/task by considering the trajectory and not just the state transition sequence.\nThese specially designed RL algorithms can be optimized using either A2C or PPO leading to combinations like EPOpt-A2C or EPOpt-PPO etc.\nThe models are either composed of feedforward networks completely or feedforward + recurrent networks.\nEnvironments\nCartPole, MountainCar, Acrobot, and Pendulum from OpenAI Gym.\nHalfCheetah and Hopper from OpenAI Roboschool.\nThree versions of each environment are considered:\nDeterministic: Environment parameters are fixed. This case corresponds to the standard environment setup in classical RL.\nRandom: Environment parameters are sampled randomly. This case corresponds to sampling from a distribution of environments.\nExtreme: Environment parameters are sampled from their extreme values. This case corresponds to the edge-case environments which would not be encountered during training generally.\nPerformance Metrics\nAverage total reward per episode.\nSuccess percentage: Percentage of episodes where a certain goal (or reward) is obtained.\nEvaluation Metrics/Setups\nDefault: success percentage when training and evaluating the deterministic version of the environment.\nInterpolation: success percentage when training and evaluating on the random version of the environment.\nExtrapolation: the geometric mean of the success percentage of following three versions:\nTrain on deterministic and evaluate on the random version.\nTrain on deterministic and evaluate on extreme version.\nTrain on random and evaluate on the extreme version.\nObservations\nExtrapolation is harder than interpolation.\nIncreasing the diversity in the training environments improves the interpolation generalization of vanilla RL methods.\nEPOpt improves generalization only for continuous control environments and only with PPO.\nRL\n2\nis difficult to train on the environments considered and did not provide a clear advantage in terms of generalization.\nEPOpt-PPO outperforms PPO on only 3 environments and EPOpt-A2C does not\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1810.12282"
    },
    "59": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Quantifying-Generalization-in-Reinforcement-Learning",
        "transcript": "Quantifying Generalization in Reinforcement Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nQuantifying Generalization in Reinforcement Learning\n2018\n\u2022\nDeep Reinforcement Learning\n\u2022\nICML 2019\n\u2022\nEvaluating Generalization\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nDRL\n\u2022\nEnvironment\n\u2022\nEvaluation\n\u2022\nICML\n\u2022\nGeneralization\n\u2022\nRL\n25 Jul 2019\nIntroduction\nThe paper introduces a new, procedurally generated environment called as CoinRun that is designed to benchmark the generalization capabilities of RL algorithms.\nThe paper reports that deep convolutional architectures and techniques like L2 regularization, batch norm, etc (which were proposed in the context of generalization in supervised learning) are also useful for RL.\nLink to the paper\nCoinRun Environment\nCoinRun is made of multiple levels.\nIn each level, the agent spawns on the far left side and needs to collect a single coin that lies on the far right side.\nThere are many obstacles in between and colliding with an obstacle leads to agent\u2019s death.\nEach episode extends for a maximum for 1000 steps.\nCoinRun is designed such that given sufficient training time and levels, a near-optimal policy can be learned for all the levels.\nGeneralization\nGeneralization can be measure by training an agent on a given set of training tasks and evaluating on an unseen set of test tasks.\n9 agents are trained to play CoinRun, on different training sets (each with a different number of levels).\nThe first 8 agents are trained on sets of size 100 to 16000 levels while the last agent is trained on an unbounded set of levels.\nTraining a model on an unbounded set of levels provides a good proxy for the train-to-test generalization performance.\nEvaluating Architectures\nTwo convolutional architectures (of different sizes) are compared:\nNature-CNN: The CNN architecture used in the\nDeep Q Network\n. This is the smaller network among the two models.\nIMPALA-CNN: The CNN architecture used in the\nImapla architecture\n.\nIMPALA-CNN agent always outperforms the Nature-CNN agent indicating that larger architecture has more capacity for generalization. But increasing the network size beyond a limit gives diminishing returns.\nEvaluating Regularization\nWhile both L2 regularization and Dropout helps to improve generalization, L2 regularization is more impactful.\nA domain randomization/data augmentation approach is tested where rectangular regions of different sizes are masked and assigned a random color. This approach seems to improve performance.\nBatch Normalization helps to improve performance as well.\nEnvironment stochasticity is introduced by using sticky actions while policy stochasticity is introduced by controlling the entropy bonus. Both these forms of stochasticity boost performance.\nWhile combining different regularization methods help, the gains are only marginally better than using just 1 regularization approach. This suggests that these different approaches induce similar generalization properties.\nAdditional Environments\nTwo additional environments are also considered to verify the high degree of overfitting observed in the CoinRun environment:\nCoinRun-Platforms:\nUnlike CoinRun, each episode can have multiple coins and the time limit is 0increased to 1000 steps.\nLevels are larger as well so the agent might need to backtrack their steps.\nRandomMazes:\nPartially observed environment with square mazes of dimensions 3x3 to 25x25.\nTimelimit of 500 steps\nOverfitting is observed for both these environments as well.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1812.02341"
    },
    "60": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Set-Transformer-A-Framework-for-Attention-based-Permutation-Invariant-Neural-Networks",
        "transcript": "Set Transformer - A Framework for Attention-based Permutation-Invariant Neural Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nSet Transformer - A Framework for Attention-based Permutation-Invariant Neural Networks\n2018\n\u2022\nICML 2019\n\u2022\nRelation Learning\n\u2022\nRelational Learning\n\u2022\nAI\n\u2022\nICML\n\u2022\nSet\n18 Jul 2019\nIntroduction\nConsider problems where the input to the model is a set. In such problems (referred to as the set-input problems), the model should be invariant to the permutation of the data points.\nIn \u201cset pooling\u201d methods (\n1\n,\n2\n), each data point (in the input set) is encoded using a feed-forward network and the resulting set of encoded representations are pooled using the \u201csum\u201d operator.\nThis approach can be shown to be bot permutation-invariant and a universal function approximator.\nThe paper proposes an attention-based network module, called as the Set Transformer, which can model the interactions between the elements of an input set while being permutation invariant.\nLink to the paper\nTransformer\nAn attention function\nAttn(Q, K, V) = (QK\nT\n)V\nis used to map queries\nQ\nto output using key-value pairs\nK, V\n.\nIn case of multi-head attention, the key, query, and value are projected into\nh\ndifferent vectors and attention is applied on all these vectors. The output is a linear transformation of the concatenation of all the vectors.\nSet Transformer\n3 modules are introduced: MAB, SAB and ISAB.\nMultihead Attention Block (MAB) is a module very similar to to the encoder in the Transformer, without the positional encoding and dropout.\nSet Attention Block (SAB) is a module that takes as input a set and performs self-attention between the elements of the set to produce another set of the same size ie\nSAB(X) = MAB(X, X)\n.\nThe time complexity of the SAB operation is\nO(n\n2\n)\nwhere\nn\nis the number of elements in the set. It can be reduced to\nO(m*n)\nby using Induced Set Attention Blocks (ISAB) with\nm\ninduced point vectors (denoted as I).\nISAB\nm\n= MAB(X, MAB(I, X))\n.\nISAB can be seen as performing a low-rank projection of inputs.\nThese modules can be used to model the interactions between data points in any given set.\nPooling by Multihead Attention (PMA)\nAggregation is performed by applying multi-head attention on a set of\nk\nseed vectors.\nThe interaction between the\nk\noutputs (from PMA) can be modeled by applying another SAB.\nThus the entire network is a stack of SABs and ISABs. Both the modules are permutation invariant and so is any network obtained by stacking them.\nExperiments\nDatasets include:\nPredicting the maximum value from a set.\nCounting unique (Omniglot) characters from an image.\nClustering with a mixture of Gaussians (synthetic points and CIFAR 100).\nSet Anomaly detection (celebA).\nGenerally, increasing\nm\n(the number of inducing datapoints) improve performance, to some extent. This is somewhat expected.\nThe paper considers various ablations of the proposed approach (like disabling attention in the encoder or pooling layer) and shows that attention mechanism is needed during both the stages.\nThe work has two main benefits over prior work:\nReducing the\nO(n\n2\n)\ncomplexity to\nO(m*n)\ncomplexity.\nUsing self-attention mechanism for both encodings the inputs and for aggregating the encoded representations.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1810.00825"
    },
    "61": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Measuring-Abstract-Reasoning-in-Neural-Networks",
        "transcript": "Measuring abstract reasoning in neural networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nMeasuring abstract reasoning in neural networks\n2018\n\u2022\nICLR 2019\n\u2022\nRelation Learning\n\u2022\nRelational Learning\n\u2022\nAI\n\u2022\nICLR\n27 Jun 2019\nIntroduction\nThe paper proposes a dataset to diagnose the abstract reasoning capabilities of learning systems.\nThe paper shows that a variant of the relational networks, explicitly designed for abstract reasoning, outperforms models like ResNets.\nLink to the paper\nIdea\nVisual reasoning tasks, that are inspired by the human IQ test, are used to evaluate the models in terms of generalization.\nLet\u2019s say that we want to test if the model understands the abstract notion of \u201cincreasing\u201d. We could train the model on data that captures the notion of \u201cincreasing\u201d, in terms of say increasing size (or quantities) of objects and then test it on a dataset where the notion is expressed in terms of increasing intensity of color.\nThe dataset is then used to evaluate if the models can find any solution to such abstract reasoning tasks and how well they generalize when the abstract content is specifically controlled.\nDataset\nRaven\u2019s Progressive Matrics (RPMs):\nConsists of an incomplete 3x3 matrix of images where the missing image needs to be filled in, typically by choosing from a set of candidate images.\nAs such, it is possible to justify multiple answers to be correct though, in practice, the right answer is the one with the simplest explanation.\nProcedurally Generated Matrices (PGMs)\nGenerating RPM like matrices procedurally by building an abstract structure for matrices.\nThe abstract structure\nS\nconsists of 3 components: (i) Relation types (\nR\n), (ii) Object types (\nO\n) and (iii) Attribute types (\nA\n). ie *S = {(r, o, a)\nr in R, o in O and a in A}*.\nThis can be read as: \u201cStructure\nS\nis instantiated on attribute\na\nof object\no\nand exhibits the relation\nr\n\u201d. For example,\nS\nis instantiated on \u201ccolor\u201d of object \u201cshape\u201d and exhibits the relation \u201cincreasing\u201d.\nIn general, the structure could be made of more than one such tuple and more are the tuples, harder is the task.\nGiven the structure, sample values\nv\nfor each attribute\na\nwhile conforming with the relation\nr\n. For example, if the attribute is \u201ccolor\u201d and the relation is \u201cincreasing\u201d, the intensity of color must increase.\nThe resulting structure is rendered as pixels.\nTest for Generalization\nThe paper tests for the following generalization scenarios:\nNeutral: The structure of the training and test data can contain any tuple.\nInterpolation: The training data contains even-indexed members of the attribute values while the test data contains odd-indexed members of the attribute values.\nExtrapolation: The training data contains first-half of the attribute values while the test data contains the second-half of the attribute values.\nHeldout attribute: Training data contains no tuples with (o = shape, a = color) or (o = line, a = type).\nHeldout triples: Out of 29 possible triples, 7 are held out from training and only used during testing.\nHeldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing.\nHeldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing.\nHeldout attribute pair: Out of 20 (unordered) variable attribute pairs, 4 were held out and used only during testing.\nModels\nInput\n: 8 context panels (from the 3x3) matrix where the last panel needs to be filled.\nCNN-MLP - 4 layer CNN with batchnorm and ReLU.\nResNet - ResNet-50 (as it perfomed better than ResNet-101 and ResNet-152).\nLSTM\nWild Relation Network (WReN) - A CNN model encodes the 8 panels and the candidate answers and feeds them as input to a relational network.\nContext-blind ResNet - ResNet network without the context (or the 8 input panels).\nResults\nWReN model outperforms the other models on the Neutral setup.\nModels have a harder time differentiating between size than quantity.\nWRen is the best performing models in all the setups and rest of the discussion only applies to that model.\nGeneralisation is easy in the context of interpolation while worst in case of extrapolation, hinting at the limited generalization capability of the models.\nAuxiliary Training\nThe model is also trained to predict the relevant relation, object and attribute types using the meta-targets that encode this information.\nThe auxiliary training helps in all the cases. Further, the model\u2019s accuracy on the main task is where in the cases where it solves the auxiliary tasks well.\nKey Takeaway\nFor abstract visual reasoning tasks, the choice of models can make a large difference, the case in consideration of ResNets vs Relational Networks.\nUsing auxiliary loss that encourages the model to \u201cexplain\u201d its reasoning (in this case by predicting the attributes, relations, etc) helps to improve the performance on the main task as well.\nGiven that the challenge is motivated by tasks used to measure human IQ, it would have been interesting to get an estimate of human performance on at least a subset of this dataset.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v80/santoro18a/santoro18a.pdf"
    },
    "62": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Hamiltonian-Neural-Networks",
        "transcript": "Hamiltonian Neural Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nHamiltonian Neural Networks\n2019\n\u2022\nAI\n\u2022\nPhysics\n20 Jun 2019\nIntroduction\nThe paper proposes a very cool idea at the intersection of deep learning and physics.\nThe idea is to train a neural network architecture that builds on the concept of Hamiltonian Mechanics (from Physics) to learn physical conservation laws in an unsupervised manner.\nLink to the paper\nLink to the code\nLink to author\u2019s blog\nHamiltonian Mechanics\nIt is a branch of physics that can describe systems which follow some conservation laws and invariants.\nConsider a set of\nN\npair of coordinates [(q\n1\n, p\n1\n), \u2026, (q\nN\n, p\nN\n)] where\nq\n= [q\n1\n, \u2026, q\nN\n] dnotes the position of the set of objects while\np\n= [p\n1\n, \u2026, p\nN\n] denotes the momentum of the set of variables.\nTogether these\nN\npairs completely describe the system.\nA scalar function\nH(\nq\n,\np\n)\n, called as the Hamiltonian is defined such that the partial derivative of\nH\nwith respect to\np\nis equal to derivative of\nq\nwith respect to time\nt\nand the negative of partial derivative of\nH\nwith respect to\nq\nis equal to derivative of\np\nwith respect to time\nt\n.\nThis can be expressed in the form of the equation as follows:\nThe Hamiltonian can be tied to the total energy of the system and can be used in any system where the total energy is conserved.\nHamiltonian Neural Network (HNN)\nThe Hamiltonian\nH\ncan be parameterized using a neural network and can learn conserved quantities from the data in an unsupervised manner.\nThe loss function looks as follows:\nThe partial derivatives can be obtained by computing the\nin-graph\ngradient of the output variables with respect to the input variables.\nObservations\nFor setups where the energy must be conserved exactly, (eg ideal mass-spring and ideal pendulum), the HNN learn to preserve an energy-like scalar.\nFor setups where the energy need not be conserved exactly, the HNNs still learn to preserve the energy thus highlighting a limitation of HNNs.\nIn case of two body problems, the HNN model is shown to be much more robust when making predictions over longer time horizons as compared to the baselines.\nIn the final experiment, the model is trained on pixel observations and not state observations. In this case, two auxiliary losses are added: auto-encoder reconstruction loss and a loss on the latent space representations. Similar to the previous experiments, the HNN model makes robust predictions over much longer time horizons.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.01563"
    },
    "63": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Extrapolating-Beyond-Suboptimal-Demonstrations-via-Inverse-Reinforcement-Learning-from-Observations",
        "transcript": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nExtrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations\n2019\n\u2022\nICML 2019\n\u2022\nInverse Reinforcement Learning\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nICML\n\u2022\nIRL\n\u2022\nRL\n13 Jun 2019\nIntroduction\nThe paper proposes a new inverse RL (IRL) algorithm, called as Trajectory-ranked Reward EXtrapolation (T-REX) that learns a reward function from a collection of ranked trajectories.\nStandard IRL approaches aim to learn a reward function that \u201cjustifies\u201d the demonstration policy and hence those approaches cannot outperform the demonstration policy.\nIn contrast, T-REX aims to learn a reward function that \u201cexplains\u201d the ranking over demonstrations and can learn a policy that outperforms the demonstration policy.\nLink to the paper\nApproach\nThe input is a sequence of trajectories\nT\n1\n, \u2026 T\nm\nwhich are ranked in the order of preference. That is, given any pair of trajectories, we know which of the two trajectories is better.\nThe setup is to learn from observations where the learning agent does not have access to the true reward function or the action taken by the demonstration policy.\nReward Inference\nA parameterized reward function\nr\n\u03b8\nis trained with the ranking information using a binary classification loss function which aims to predict which of the two given trajectory would be ranked higher.\nGiven a trajectory, the reward function predicts the reward for each state. The sum of rewards (corresponding to the two trajectories) is used used to predict the preferred trajectory.\nT-REX uses partial trajectories instead of full trajectories as a data augmentation strategy.\nPolicy Optimization\nOnce a reward function has been learned, standard RL approaches can be used to train a new policy.\nResults\nEnvironments: Mujoco (Half Cheetah, Ant, Hooper), Atari\nDemonstrations generated using PPO (checkpointed at different stages of training).\nEnsemble of networks used to learn the reward functions.\nThe proposed approach outperforms the baselines\nBehaviour Cloning from Observations\nand\nGenerative Adversarial Imitation Learning\n.\nIn terms of reward extrapolation, T-REX can predict the reward for trajectories which are better than the demonstration trajectories.\nSome ablation studies considered the effect of adding noise (random swapping the preference between trajectories) and found that the model is somewhat robust to noise up to an extent.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1904.06387"
    },
    "64": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Relational-Reinforcement-Learning",
        "transcript": "Relational Reinforcement Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nRelational Reinforcement Learning\n2018\n\u2022\nDeep Reinforcement Learning\n\u2022\nICLR 2019\n\u2022\nReinforcement Learning\n\u2022\nRelational Learning\n\u2022\nAI\n\u2022\nICLR\n\u2022\nRL\n\u2022\nRRL\n01 Jun 2019\nIntroduction\nRelational Reinforcement Learning (RRL) paradigm uses relational state (and action) space and policy representation to leverage the generalization capability of relational learning for reinforcement learning.\nThe paper shows that effectiveness of RRL - in terms of generalization, sample efficiency and interplay - using box-world and StarCraft II minigames.\nLink to the paper\n.\nArchitecture\nThe main idea is to use neural network models that operate on structured representations and perform relational reasoning via iterated, message-passing style methods.\nUse of non-local computations using a shared function (in terms of pairwise interactions between entities) provides a better inductive bias.\nMulti-head dot product attention mechanism is used to model the pairwise interactions (with one or more attention blocks).\nIterative computations can be used to capture higher-order interactions between entities.\nEntity extraction is based on the assumption that entities are things located at a particular point in space.\nA CNN is used to parse the pixel space observation into\nk\nfeature maps of size\nnxn\n. The\n(x, y)\ncoordinates are concatenated to each\nk-\ndimensional pixel feature-vector to indicate the pixel\u2019s position in the map.\nThe resulting\nn\n2\nx k\nmatrix acts as the entity matrix.\nActor-critic architecture (using distributed agent IMPALA) is used.\nEnvironment\nBox-World\n12 x 12-pixel room with keys and boxes placed randomly.\nAgent can move in 4 directions.\nThe task is to collect gems by unlocking boxes (which may contain keys to unlock other boxes).\nEach level has a unique sequence in which boxes need to be opened as opening the wrong box could make the level unsolvable.\nDifficulty of a level can be controlled using: (i) Number of boxes in the path to the goal. (ii) The number of distractor branches, (iii)  Length of distractor branches.\nStarCraft II minigames\n9 mini games designed as specific scenarios in the Starcraft game are used.\nResults\nBox-World\nRRL agents solve over 98% of the levels while the RL agent solves less than 95% of the levels.\nVisualising the attention scores indicate that:\nkeys attend to locks they can unlock.\nall objects attend to agent\u2019s location.\nagent and gem attend to each other (and themselves).\nGeneralization capacity is tested in two ways:\nPerformance on levels that require opening a larger sequence of boxes than it is trained on.\nPerformance on levels that require key-lock combinations not seen during training.\nIn both the scenarios, the RRL agent significantly outperforms the RL agent.\nStarCraft\nRLL agent achieves better or equal results that the RL agent in all but one game.\nFor testing generalization, the agent, that was trained for controlling two marines, was transferred on the task which requires it to control 5 marines. These results are not conclusive given the high variability.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1806.01830"
    },
    "65": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Good-Enough-Compositional-Data-Augmentation",
        "transcript": "Good-Enough Compositional Data Augmentation \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nGood-Enough Compositional Data Augmentation\n2019\n\u2022\nData Augmentation\n\u2022\nSequential models\n\u2022\nAI\n\u2022\nCompositionality\n\u2022\nNLP\n21 May 2019\nIntroduction\nThe paper introduces a simple data augmentation protocol that provides a good compositional inductive bias for sequential models.\nSynthetic examples are created by taking real sequences and replacing the fragments in sequences which appear in similar environments. This operation is referred to as GECA (Good Enough Compositional Augmentation).\nThe underlying idea is that if two fragments of training examples occur in some environment, then any environment where the first fragment appears is also a valid environment for the second fragment.\nLink to the paper\nApproach\nDiscover substitutable fragments (ie pairs of fragments that co-occur with a common fragment) and use them to generate new sequences by swapping fragments.\nThe current work uses very simple criteria to decide if fragments are substitutable - fragments should occur in at least one lexical environment that is exactly the same. A lexical environment is the k-word window around each span of the fragment.\nThough the idea can be motivated by work in generative syntax and distributional semantics, it would not hold like a physical law when applied to the real data.\nThe authors view this tradeoff as a balance between the shortage of training data vs relative frequency of mistake in the proposed data augmentation approach.\nResults\nThe approach is evaluated on the SCAN dataset when the model is trained on the short sequence of English commands. Though the dataset augmentation helps the baseline models, it is not surprising given the nature of the SCAN dataset.\nMore challenging tasks (for evaluating the proposed approach) are semantic parsing (where the query is represented in the form of \u03bb calculus or SQL and low resource language modeling. While the improvement (in terms of metrics) is sometimes limited, the gains are consistent across different datasets.\nGiven that the proposed approach is relatively simple and straightforward, it appears to be quite promising.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1904.09545"
    },
    "66": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Multiple-Model-Based-Reinforcement-Learning",
        "transcript": "Multiple Model-Based Reinforcement Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nMultiple Model-Based Reinforcement Learning\n2002\n\u2022\nModel-Based\n\u2022\nNeural Computation\n\u2022\nNeural Computation 2002\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nRL\n14 May 2019\nThe paper presents some general ideas and mechanisms for multiple model-based RL. Even though the task and model architecture may not be very relevant now, I find the general idea and the mechanisms to be quite useful. As such, I am focusing only on high-level ideas and not the implementation details themselves.\nThe main idea behind Multiple Model-based RL (MMRL) is to decompose complex tasks into multiple domains in space and time so that the environment dynamics within each domain is predictable.\nLink to the paper\nMMRL proposes an RL architecture composes of multiple modules, each with its own state prediction model and RL controller.\nThe prediction error from each of the state prediction model defines the \u201cresponsibility signal\u201d for each module.\nThis responsibility signal is used to:\nWeigh the state prediction output ie the predicted state is the weighted sum of individual state predictions (weighted by the responsibility signal).\nWeigh the parameter update of the environment models as well as the RL controllers.\nWeighing the action output - ie predicted action is a weighted sum of individual actions.\nThe framework is amenable for incorporating prior knowledge about which module should be selected.\nIn the modular decomposition of a task, the modules should not change too frequently and some kind of spatial and temporal continuity is also desired.\nTemporal continuity can be accounted for by using the previous responsibility signal as input during the current timestep.\nSpatial continuity can b ensured by considering a spatial prior like the Gaussian spatial prior.\nThough model-free methods could be used for learning the RL controllers, model-based methods could be more relevant given that the modules are learning state-prediction models as well.\nExploration can be ensured by using a stochastic version of greedy action selection.\nOne failure mode for such modular architectures is when a single module tries to perform well across all the tasks. The modules themselves should be relatively simplistic (eg linear models) which can learn quickly and generalize well.\nNon-stationary hunting task in a grid world and non-linear, non-stationary control task of swinging up a pendulum provides the proof of concept for the proposed methods.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://www.mitpressjournals.org/doi/abs/10.1162/089976602753712972"
    },
    "67": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/To-Tune-or-Not-to-Tune-Adapting-Pretrained-Representations-to-Diverse-Tasks",
        "transcript": "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nTo Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks\n2019\n\u2022\nEmpirical Advice\n\u2022\nMulti Task\n\u2022\nNatural Language Processing\n\u2022\nTransfer Learning\n\u2022\nAI\n\u2022\nNLP\n\u2022\nQA\n16 Mar 2019\nLink to the paper\nThe paper provides useful empirical advice for adapting pretrained language models for a given target task.\nPre-trained models considered\nELMo\nBERT\nTasks considered\nNamed Entity Recognition (NER) - CoNLL 2003 dataset\nSentiment Analysis (SA) - Stanford Sentiment Treebank (SST-2) dataset\nNatural Language Inference (NLI) - MultiNLI and Sentences Involving Compositional Knowledge (SICK-E) dataset\nParaphrase Detection (PD) - Microsoft Research Paraphrase Corpus (MRPC)\nSemantic Textual Similarity (STS) - Semantic Textual Similarity Benchmark (STS-B) and SICK-R\nThe last 3 tasks (NLI, PD, STS) are defined for sentence pairs.\nAdaptation Strategies\nFeature Extraction\nThe pretrained model is only used for extracting features and its weights are kept fixed.\nFor both ELMo and BERT, the contextual representation of the words from all the layers are extracted.\nA weighted combination of these layers is used as an input to the task-specific model.\nTask-specific models\nNER - BiLSTM with CRF layer\nSA - bi-attentive classification network\nNLI, PD, STS -\nEnhanced Sequential Inference Model (ESIM)\nFine-tuning\nThe pretrained model is finetuned on the target task.\nTask-specific models for ELMO\nNER - CRF on top of LSTM states\nSA - Max-pool over the language model states followed by a softmax layer\nNLI, PD, STS - cross sentence bi-attention between the language model states followed by pooling and softmax layer.\nTask-specific models for BERT\nNER - Extract representation of the first-word piece of each token followed by the softmax layer\nSA, NLI, PD, STS - standard BERT training\nMain observations\nFeature extraction and fine-tuning have comparable performance in most cases unless the two tasks are highly similar(fine-tuning is better) or highly dissimilar (feature extraction is better).\nFor ELMo, feature extraction consistently outperforms fine-tuning for the sentence pair tasks (NLI, PD, STS). The reverse trend is observed for BERT with fine-tuning being better on sentence pair tasks.\nAdding extra parameters is helpful for feature extraction but not fine-tuning.\nELMo fine-tuning requires careful tuning and other tricks like triangular learning rates, gradual unfreezing and discriminative fine-tuning.\nFor the tasks considered, there is no correlation observed between the distance of the source and target domains and adaptation performance.\nTraining a diagnostic classifier (on the intermediate representations) suggests that fine-tuning improves the performance of the classifier at all the intermediate layers (which is sort of expected).\nIn terms of mutual information estimates, fine-tuned representations have a much higher mutual information as compared to the feature extraction based representations.\nKnowledge for single sentence tasks seems to be mostly concentrated in the last layers while for pair classification tasks, the knowledge seems gradually build un in the intermediate layers, all the way up to the last layer.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.05987"
    },
    "68": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Model-Primitive-Hierarchical-Lifelong-Reinforcement-Learning",
        "transcript": "Model Primitive Hierarchical Lifelong Reinforcement Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nModel Primitive Hierarchical Lifelong Reinforcement Learning\n2019\n\u2022\nAAMAS 2019\n\u2022\nCatastrophic Forgetting\n\u2022\nContinual Learning\n\u2022\nHierarchical Reinforcement Learning\n\u2022\nLifelong Learning\n\u2022\nReinforcement Learning\n\u2022\nAAMAS\n\u2022\nAI\n\u2022\nCL\n\u2022\nHRL\n\u2022\nRL\n12 Mar 2019\nIntroduction\nThe paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies.\nGiven a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner.\nThe framework is called as Model Primitive Hierarchical Reinforcement Learning (MPHRL).\nLink to the paper\nIdea\nInstead of learning a single transition model of the environment (aka\nworld model\n) that can model the transitions very well, it is sufficient to learn several (say\nk\n) suboptimal models (aka\nmodel primitives\n).\nEach\nmodel primitive\nwill be good in only a small part of the state space (aka\nregion of specialization\n).\nThese\nmodel primitives\ncan then be used to train a gating mechanism for selecting sub-policies to solve a given task.\nSince these\nmodel primitives\nare sub-optimal, they are not directly used with model-based RL but are used to obtain useful functional decompositions and sub-policies are trained with model-free approaches.\nSingle Task Learning\nA gating controller is trained to choose the sub-policy whose\nmodel primitive\nmakes the best prediction.\nThis requires modeling\np(M\nk\n| s\nt\n, a\nt\n, s\nt+1\n)\nwhere\np(M\nk\n)\ndenotes the probability of selecting the\nk\nth\nmodel primitive\n. This is hard to compute as the system does not have access to\ns\nt+1\nand\na\nt\nat time\nt\nbefore it has choosen the sub-policy.\nProperly marginalizing\ns\nt+1\nand\na\nt\nwould require expensive MC sampling. Hence an approximation is used and the gating controller is modeled as a categorical distribution - to produce\np(M\nk\n| s\nt\n)\n. This is trained via a conditional cross entropy loss where the ground truth distribution is obtained from transitions sampled in a rollout.\nThe paper notes that technique is biased but reports that it still works for the downstream tasks.\nThe gating controller composes the sub-policies as a mixture of Gaussians.\nFor learning, PPO algorithm is used with each\nmodel primitives\ngradient weighted by the probability from the gating controller.\nLifelong Learning\nDifferent tasks could share common subtasks but may require a different composition of subtasks. Hence, the learned sub-policies are transferred across tasks but not the gating controller or the baseline estimator (from PPO).\nExperiments\nDomains:\nMujoco ant navigating different mazes.\nStacker arm picking up and placing different boxes.\nImplementation Details:\nGaussian subpolicies\nPPO as the baseline\nModel primitives are hand-crafted using the true next state provided by the environment simulator.\nSingle Task\nOnly maze task is considered with the start position (of the ant) and the goal position is fixed.\nObservation includes distance from the goal.\nForcing the agent to decompose the problem, when a more direct solution may be available, causes the sample complexity to increase on one task.\nLifelong Learning\nMaze\n10 random Mujoco ant mazes used as the task distribution.\nMPHRL takes almost twice the number of steps (as compared to PPO baseline) to solve the first task but this cost gets amortized over the distribution and the model takes half the number of steps as compared to the baseline (summed over the 10 tasks).\nPick and Place\n8 Pick and Place tasks are created with max 3 goal locations.\nObservation includes the position of the goal.\nAblations\nOverlapping\nmodel primitives\ncan degrade the performance (to some extent). Similarly, the performance suffers when redundant primitives are introduced indicating that the gating mechanism is not very robust.\nSub-policies could quickly adapt to the previous tasks (on which they were trained initially) despite being finetuned on subsequent tasks.\nThe order of tasks (in the 10-Mazz task) does not degrage the performance.\nTransfering the gating controller leads to negative transfer.\nNotes\nI think the biggest strength of the work is that accurate dynamics model are not needed (which are hard to train anyways!) through the experimental results are not conclusive given the limited number of domains on which the approach is tested.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.01567"
    },
    "69": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Linguistic-Knowledge-as-Memory-for-Recurrent-Neural-Networks",
        "transcript": "Linguistic Knowledge as Memory for Recurrent Neural Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nLinguistic Knowledge as Memory for Recurrent Neural Networks\n2017\n\u2022\nNatural Language Processing\n\u2022\nAI\n\u2022\nMemory\n\u2022\nNLP\n\u2022\nQA\n\u2022\nRNN\n05 Feb 2019\nLink to the paper\nTraining RNNs to model long term dependencies is difficult but in some cases, the information about dependencies between elements (of the sequence) may be present in the form of symbolic knowledge.\nFor example, when encoding sentences, coreference, and hypernymy relations can be extracted between tokens.\nThese elements(tokens) can be connected with each other with different kind of edges resulting in the graph data structure.\nOne approach could be to model this knowledge(encoded in the graph) using a graph neural network (GNN).\nThe authors prefer to encode the information into 2 DAGs (via topological sorting) as training the GNN could add some extra overhead.\nThis results into the Memory as Acyclic Graph Encoding RNN (MAGE-RNN) architecture. Its GRU version is referred to as MAGE-GRU.\nGiven an input sequence of tokens [x\n1\n, x\n2\n, \u2026, x\nT\n] and information about which tokens relate to other tokens, a graph G is constructed with different (possibly typed) edges.\nGiven the graph\nG\n, two DFS orderings are computed - forward DFS and backward DFS.\nMAGE-RNN uses separate networks for accessing the forward and backward DFS orders.\nA separate hidden state is maintained for each entity type to separate memory content from addressing.\nFor any DFS order (forward or backward), the representation at time\nt\nis given as the concatenation of representation of different edge types at that time.\nThe hidden states (for different edge types at time t) are updated in the topological order using the current state of all incoming edges at x\nt\n.\nThe representation of the DFS order is given as the sequence of all the previous representations.\nIn some cases, elements across multiple sequences could be related to each other. In that case, the graph is decomposed into a collection of DAGs and use MAGE-GRU on the DAGs by taking one random permutation of the sequences and decomposing it into the forward and the backward graphs.\nThe model is evaluated on the task of text comprehension with coreference on bAbi dataset (story based QA), LAMBADA dataset (broad context language modeling) and CNN dataset (cloze-style QA).\nMAGE-GRU was used as a replacement for GRU units in bi-directional GRUs and GA-Reader architecture.\nDAG-RNN and shared version of MAGE-GRU (with shared edge types) are the other baselines.\nFor all the cases, the model with MAGE-GRU works the best.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.02620"
    },
    "70": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Diversity-is-All-You-Need-Learning-Skills-without-a-Reward-Function",
        "transcript": "Diversity is All You Need - Learning Skills without a Reward Function \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nDiversity is All You Need - Learning Skills without a Reward Function\n2019\n\u2022\nICLR 2019\n\u2022\nInformation Theory\n\u2022\nReinforcement Learning\n\u2022\nEntropy\n\u2022\nICLR\n\u2022\nRL\n\u2022\nUnsupervised\n29 Jan 2019\nIntroduction\nThe paper proposes an approach to learn useful skills without a reward function by maximizing an information theoretic objective by using a maximum entropy policy.\nSkills are defined as latent-conditioned policies that alter the state of the environment in a consistent way.\nLink to the paper\nLink to the code\nSetup\nUnsupervised \u201cexploration\u201d stage followed by supervised stage.\nDesirable Qualities of Skills\nSkills should dictate the states that the agent visits. Different skills should visit different states to be distinguishable.\nStates (not actions) should be used to distinguish between skills as not all actions change the state (for the outside observer).\nSkills are encouraged to be diverse and \u201cexploratory\u201d by learning skills that act randomly (have high entropy).\nLoss Formulation\n(S, A) - state and action\nz ~ p(z) - latent variable to condition the policy.\nSkill - policy conditioned on a fixed z.\nObjective is to maximize the mutual information between skill and state (MI(A; Z)) ie skill should control which state is visited or the skill should be inferrable from the state visited.\nSimultaneously minimize the mutual information between skills and actions given the state to ensure that the state (and not the action) is used to distinguish the skills.\nMaximize the entropy of the mixture of policies (p(z) and all the skills).\nImplementation\nPolicy \u03c0(a | s, z)\nTask reward replaced by the pseduoreward logq\n\u03c6\n(z | s) - log(p(z)).\nDuring unsupervised training, z is sampled at the start of the episode and then not changed during the episode.\nLearning agent gets rewards for visiting the states that are easy to discriminate while the discriminator updated to correctly predict z from the states visited.\nObservations\nAnalysis of Learned Skills\nThe agent learns a diverse set of primitive behaviors for all tasks ranging from 2 DoF to 111 DoF.\nfor inverted pendulum and mountain car, the skills become increasingly diverse throughout training.\nUse of uniform prior, in place of a learned prior, for p(z) allows for discovery of more diverse skills.\nThe proposed approach can be used as a pretraining technique where the best-performing primitives (from unsupervised training) can be finetuned with the task-specific rewards.\nThe discovered skills can be used for hierarchical RL by learning a meta-policy(which chooses the skill to execute for k steps).\nModifying the discriminator in the proposed formulation can be used to bias DIAYN towards discovering a particular type of policies. This provides a mechanism for incorporating \u201csupervision\u201d in the learning setup.\nThe \u201cdiscovered\u201d primitives can also be used for imitation learning.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.06070"
    },
    "71": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Modular-meta-learning",
        "transcript": "Modular meta-learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nModular meta-learning\n2018\n\u2022\nMeta Learning\n\u2022\nModular Meta Learning\n\u2022\nModular ML\n\u2022\nModular Network\n\u2022\nModule\n22 Jan 2019\nIntroduction\nThe paper proposes an approach for learning neural networks (modules) that can be combined in different ways to solve different tasks (combinatorial generalization).\nThe proposed model is called as BOUNCEGRAD.\nLink to the paper\nLink to the code\nSetup\nFocuses on supervised learning.\nTask distribution\np(T)\n.\nEach task is a joint distribution\np\nT\n(x, y)\nover\n(x, y)\ndata pairs.\nGiven data from\nm\nmeta-training tasks, and a meta-test task, find a hypothesis\nh\nwhich performs well on the unseen data drawn from the meta-test task.\nStructured Hypothesis\nGiven a compositional scheme\nC\n, a set of modules\nF\n1\n, \u2026, F\nk\n(represented as a whole by\nF\n) and the set of their respective parameters \u03b8\n1\n, \u2026, \u03b8\nk\n(represented as a whole by \u03b8),\n(C, F, \u03b8)\nrepresents the set of possible functional input-output mappings. These mappings form the hypothesis space.\nA structured hypothesis model is specified by what modules to use and their parametric forms (but not the values).\nExamples of compositional schemes\nChoosing a single module for the task at hand.\nFixed compositional structure but different modules selected every time.\nWeight ensemble (maybe using attention mechanism)\nGeneral function composition tree\nPhases\nOffline Meta Learning Phase:\nTake training and validation dataset for the first\nk\ntasks and generate a parameterization for each module\n\u03b8\n1\n, \u2026, \u03b8\nk\n.\nThe hypothesis (or composition) to use comes from the online meta-test learning phase.\nIn this stage, find the best \u03b8 given a structure.\nOnline Meta-test Learning Phase\nGiven a hypothesis space and \u03b8, the output is a compositional form (or hypothesis) that specifies how to compose the models.\nIn this stage, find the best structure, given a hypothesis space and \u03b8.\nLearning Algorithm\nDuring Meta-test learning phase, simulated annealing is used to find the optimal structure, with temperature\nT\ndecreased over time.\nDuring meta-learning phrase, the actual objective function is replaced by a surrogate, smooth objective function (during the search step) to avoid local minima.\nOnce a structure has been picked, any gradient descent based approach can be used to optimize the modules.\nBasically the state of optimization process comprises of the parameters and the temperature. Together, they are used to induce a distribution over the structures. Given a structure, \u03b8 is optimized and\nT\nis annealed over time.\nThe learning procedure can be improved upon by performing parameter tuning during the online (meta-test learning) phase as well. the resulting approach is referred to as MOMA - MOdular MAml.\nExperiments\nApproaches\nPooled - Single network using combined data of all the tasks.\nMAML - Single network using MAML\nBOUNCEGRAD - Modular Network without MAML adaptation in online learning.\nMOMA - BOUNCEGRAD with MAML adaptation in online learning.\nDomains\nSimple Functional Relationships\nSine-function prediction problem\nIn general, MOMA outperforms other models.\nWith a small amount of online training data, BOUNCEGRAD outperforms other models as it has a better structural prior.\nPredicting next frame of a kinematic skeleton (motion capture data)\n11 different objects (with different shapes) on 4 surfaces with different friction properties.\n2 meta-learning scenarios are considered. In the first case, the object-surface combination in the test case was present in some meta-training tasks and in the other case, it was not present.\nFor previously seen combinations, MOMA performs the best followed by BOUNCEGRAD and MAML.\nFor unseen combinations, all the 3 are equally good.\nCompositional scheme is the attention mechanism.\nAn interesting result is that the modules seem to specialize (and activate more often) based on the shape of the object.\nPredicting next frame of a kinematic selection (using motion capture data)\nComposition Structure - generating kinematics subtrees for each body part (2 legs, 2 arms, 2 torsi).\nAgain 2 setups are used - one where all activities in the training and the meta-test task are shared while the other setup where the activities are not shared.\nFor known activities MOMA and BOUNCEGRAD perform the best while for unknown activities, MOMS performs the best.\nNotes\nWhile the approach is interesting, maybe a more suitable set of tasks (from the point of composition) would be more convincing.\nIt would be useful to see the computational tradeoff between MAML, BOUNCEGRAD, and MOMA.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1806.10166"
    },
    "72": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Hierarchical-RL-Using-an-Ensemble-of-Proprioceptive-Periodic-Policies",
        "transcript": "Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nHierarchical RL Using an Ensemble of Proprioceptive Periodic Policies\n2019\n\u2022\nHierarchical RL\n\u2022\nICLR 2019\n\u2022\nAI\n\u2022\nICLR\n\u2022\nMujoco\n\u2022\nRL\n15 Jan 2019\nIntroduction\nThe paper proposes a simple and robust approach for hierarchically training an agent in the sparse reward setup.\nThe broad idea is to train low-level primitives that are sufficiently diverse (so that they can be composed for solving higher level tasks) and to train a high level primitive that learns to combine these primitives for any given downstream task.\nLink to the paper\nApproach\nThe state can be divided into two components: proprioceptive states s\np\n(measurement of agent\u2019s own body that can be directly controlled by the agent) and the external states s\ne\n/\nLow-Level Policy Training\nLow-level policies should be:\nDiverse: should cover all the skills that the agent might have to perform.\nEffective: can make significant changes to the environment.\nControllable: easy for high-level policies to use and control\nFor the low-level policy, the per-time step reward is directly proportional to change in the external state. The same reward is used for all the agents and environments(except regulated with environment specific controls and survival rewards).\nPhase conditioned policies\nGood movement policies are expected to be at least roughly periodic and phase input (or time index) is used to achieve periodicity.\nPhase conditioned policy (=f(s\np\n, \u03c6)) where \u03c6 = {0, 1, \u2026, k-1} is the phase index.\nAt each timestep\nt\n, the model receives observation s\np\nand phase index \u03c6 = t%k. The phase index is represented by a vector b\n\u03c6\n.\nFor phase conditioned policies, the agent state and actions are encouraged to be cyclic with the help of a cyclic loss.\nExperiments\nEnvironments: Ant and Humanoid from Mujoco.\nLow-level control:\nUsing phase-conditioning is helpful when training low-level primitives.\nHigh-level control:\nCross Maze Environment with fixed goals\n3 goals along 3 paths\nProposed method converges faster and to a smaller final distance to the goal showing that it is both efficient and consistent (with smaller variance across random seeds).\nRandom Goal Maze\nThe goal is randomly drawn from a set of goals.\n\u201cCross\u201d (shaped) maze and \u201cskull\u201d (shaped) mazes are considered.\nEven with velocity rewards and pretraining on low-level objectives (which can be thought of as exploration bonuses), the baseline fails to get close to the goal locations while the proposed model reach the goal most of the times.\nThe main results are reported using PPO though repeating the experiments with A2C and DQN show that the idea is fairly robust.\nThe paper reported that in their experiments, finetuning the lower level primitives did not help much though it might not be the case of other environments.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=SJz1x20cFQ"
    },
    "73": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Efficient-Lifelong-Learning-with-A-GEM",
        "transcript": "Efficient Lifelong Learning with A-GEM \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nEfficient Lifelong Learning with A-GEM\n2019\n\u2022\nCatastrophic Forgetting\n\u2022\nContinual Learning\n\u2022\nICLR 2019\n\u2022\nLifelong Learning\n\u2022\nAI\n\u2022\nCV\n\u2022\nCL\n\u2022\nICLR\n08 Jan 2019\nContributions\nA new (and more realistic) evaluation protocol for lifelong learning where each data point is observed just once and a disjoint set of tasks are used for training and validation.\nA new metric that focuses on the efficiency of the models - in terms of sample complexity and computational (and memory) costs.\nModification of\nGradient Episodic Memory ie GEM\nwhich reduces the computational overhead of GEM without compromising on the results.\nEmpirical validation that using task descriptors help lifelong learning models and improve their few-shot learning capabilities.\nLink to the paper\nLink to the code\nLearning Protocol\nTwo group of datasets - one for training and evaluation (D\nEV\n) and other for cross validation (D\nCV\n).\nData can be sampled multiple times for cross-validation dataset but only once from the training dataset.\nEach group of dataset (say D\nEV\nor D\nCV\n) is a list of task-specific datasets D\nk\n(k is the task index).\nEach sample in D\nk\nis of the form (x, t, y) where x is the data, t is the task descriptor and y is the output.\nD\nk\ncontains B\nk\nminibatches of data.\nMetrics\nAccuracy\na\nk,i,j\n= accuracy on test task j after training on ith minibatch of training task k.\nA\nk\n= mean over all j = 1 to k (a\nk, B\nk\n, j\n) ie train the model on data for task k and then test it on all the tasks.\nForgetting Measure\nf\nj\nk\n= forgetting on task j after training on all minibatches upto task k.\nf\nj\nk\n= max over all l = 1 to k-1 (a\nl, B\nl\nj\n- a\nk, B\nk\nj\n)\nForgetting = F\nk\n= mean over all j = 1 to k-1 (f\nj\nk\n)\nLCA - Learning Curve Area\nZ\nb\n= average b shot performance where b is the minibatch number.\nZ\nb\n= mean over all k = 0 to T (a\nk, b, k\n)\nLCA\n\u03b2\n= mean over all b = 0 to \u03b2 (Z\nb\n)\nOne special case is LCA\n0\nwhich is the forward transfer performance or performance on the unseen task.\nIn experiments, \u03b2 is kept small as we want the model to learn from few examples.\nModel\nGEM has been shown to be very effective in single epoch setting but introduces a very high computational overhead.\nAverage GEM (AGEM) reduces this overhead by sampling (and using) only some examples from the episodic memory instead of using all the examples.\nWhile GEM provides better guarantees in terms of worst-case forgetting, AGEM provides better guarantees in terms of average accuracy.\nJoint Embedding Model Using Compositional Task Descriptors\nCompositional Task Descriptors are used to speed training on the subsequent tasks.\nA matrix specifying the attribute value of objects (to be recognized in the task) are used.\nA joint-embedding space between image features and attribute embeddings is learned.\nExperiments\nDatasets\nPermuted MNIST\nSplit CIFAR\nSplit CUB\nSplit AWA\nSetup\nInteger task descriptors for MNIST and CIFAR and class attributes as descriptors for CUB and AWA\nBaselines include\nGEM\n,\niCaRL\n,\nElastic Weight Consolidation\n,\nProgressive Neural Networks\netc.\nResults\nAGEM outperforms other models on all the datasets expect MNIST where the Progressive Neural Networks lead. One reason could be that MNIST has a large number of training examples per task. But Progressive Neural Networks lead to bad utilization of capacity.\nWhile AGEM and GEM have similar performance, GEM has a much higher computational and memory overhead.\nUse of task descriptors improves the accuracy for all the models.\nIt seems that AGEM offers a good tradeoff between average accuracy performance and efficiency - in terms of sample efficiency, memory requirements and computational costs.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1812.00420"
    },
    "74": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Hindsight-Experience-Replay",
        "transcript": "Hindsight Experience Replay \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nHindsight Experience Replay\n2017\n\u2022\nNIPS 2017\n\u2022\nOff policy RL\n\u2022\nReinforcement Learning\n\u2022\nSample Efficient\n\u2022\nAI\n\u2022\nNIPS\n\u2022\nRL\n18 Dec 2018\nIntroduction\nHindsight Experience Replay(HER) is a sample efficient technique to learn from sparse rewards.\nLink to the paper\nIdea\nAssume a footballer misses the goal narrowly. Even though the player does not get any \u201creward\u201d(in terms of goal), the player realizes that had the goal post been shifted a bit, it would have resulted in a goal(reward).\nThe same intuition is applied for the RL agent - let us say that the true goal state was\ng\nwhile the agent ends up in the state\ns\n.\nWhile the action sequence is not useful for reaching the goal state\ng\n, it is indeed useful for reaching state\ns\n. Hence the trajectory could be replayed with the goal as\ns\n(and not\ng\n).\nTechnical Details\nMulti-goal policy trained using Universal Value Function Approximation (UVFA).\nEvery episode starts by sampling a start state and a goal state. Each goal has a different reward function.\nPolicy uses both the current state and the current goal state and leads to a state transition sequence\ns\n1\n, s\n2\n,\u2026, s\nn\n.\nEach of these transitions\ns\ni\n-> s\ni+1\nare stored in a buffer with both the original goal and a subset of the other goals.\nFor the goal selection, following strategies are tried:\nFuture\n- goal state is the state\nk\nsteps after observing the state transition.\nFinal\n- goal state is the final state of the current episode.\nEpisode\n-\nk\nrandom states are selected from the current episode.\nRandon\n-\nk\nstates are selected randomly.\nAny off-policy algorithm can be used. Specifically, DDPG is used.\nExperiments\nRobotic arm simulated using MuJoCo for\npush\n,\nslide\nand\npick and place\ntasks.\nDDPG with and without HER evaluated on the 3 tasks.\nDDPG with the HER variant significantly outperforms the baseline in all the cases.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1707.01495"
    },
    "75": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Representation-Tradeoffs-for-Hyperbolic-Embeddings",
        "transcript": "Representation Tradeoffs for Hyperbolic Embeddings \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nRepresentation Tradeoffs for Hyperbolic Embeddings\n2018\n\u2022\nICML 2018\n\u2022\nHyperbolic Embedding\n\u2022\nHyperboloid Model\n\u2022\nGraph Representation\n\u2022\nPoincare Ball Model\n\u2022\nAI\n\u2022\nEmbedding\n\u2022\nGraph\n11 Dec 2018\nIntroduction\nThe paper describes a combinatorial approach to embed trees into hyperbolic spaces without performing optimization.\nThe resulting mechanism is analyzed to obtain dimensionality-precision tradeoffs.\nTo embed any metric spaces in the hyperbolic spaces, a hyperbolic generalization of the multidimensional scaling (h-MDS) is proposed.\nLink to the paper\nPreliminaries\nHyperbolic Spaces\nHave the \u201ctree\u201d like property ie the shortest path between a pair of points is almost the same as the path through the origin.\nGenerally, Poincare ball model is used given its advantages like conformity to the Euclidean spaces.\nFidelity Measures\nMean Average Precision - MAP\nA local metric that ranks between distances of the immediate neighbors.\nDistortion\nA global metric that depends on the underlying distances and not just the local relationship between distances.\nCombinatorial Construction for embedding hierarchies into Hyperbolic spaces\nEmbed the given graph\nG = (V, E)\ninto a tree\nT\n.\nEmbed the tree\nT\ninto the poincare ball\nH\nd\nof dimensionality\nd\n.\nSarkar\u2019s construction to embed points in a 2-d Poincare ball\nConsider two points\na\nand\nb\n(from the tree) where\nb\nis the parent of\na\n.\nAssume that\na\nis embedded as\nf(a)\nand\nb\nis embedded as\nf(b)\nand the children of\na\nneeds to be embedded.\nReflect\nf(a)\nand\nf(b)\nacross a geodesic such that\nf(a)\nis mapped to 0 (origin) while\nf(b)\nis mapped to some new point\nz\n.\nChildren of\na\nare placed at points\ny\ni\nwhich are equally placed around a circle of radius\n(e\nr\n- 1) / (e\nr\n+ 1)\nand maximally seperated from\nz\n, where\nr\nis the scaling factor.\nThen all the points are reflected back across the geodesic so that all children are at a distance\nr\nfrom\nf(a)\n.\nTo embed the tree itself, place the root node at the origin, place its children around it in a circle, then place their children and so on.\nIn this construct, precision scales logarithmically with the degree of the tree but linearly with the maximum path length.\nd\n-dimensional hyperbolic spaces\nIn the\nd\n-dimensional space, the points are embedded into hyperspheres (instead of circles).\nThe number of children node that can be placed for a particular angle grows with the dimension.\nIncreasing dimension helps with bushy trees (with high node degree).\nHyperbolic multidimensional scaling (h-MDS)\nGiven the pairwise distance from a set of points in the hyperbolic space, how to recover the points?\nThe corresponding problem in the Euclidean space is solved using MDS.\nA variant of MDS called as h-MDS is proposed.\nMDS makes a centering assumption that points have 0 mean. In h-MDS, a new mean (called as the pseudo-Euclidean mean) is introduced to enable recovery via matrix factorization.\nInstead of the Poincare model, the hyperboloid model is used (though the points can be mapped back and forth).\npseudo-Euclidean Mean\nA set of points can always be centered without affecting their pairwise distance by simply finding their mean and sending it to 0 via isometry\nRecovery via matrix factorization\nGiven the pairwise distances, a new matrix\nY\nis constructed by applying\ncosh\non the pairwise distances.\nRunning PCA on\n-Y\nrecovers X up to rotation.\nDimensionality Reduction with PGA (Principal Geodesic Analysis)\nPGA is the counterpart of PCA in the hyperbolic spaces.\nFirst the\nKarcher\nmean of the given points is computed.\nAll points\nx\ni\nare reflected so that their mean is 0 in the Poincare disk model.\nCombining that with Euclidean reflection formula and hyperbolic metrics leads to a non-convex loss function which can be optimized using gradient descent algorithm.\nExperiments\nDatasets\nTrees: fully balanced and phylogenic trees expressing genetic heritage.\nTree-like hierarchy: WordNet hypernym and graph of Ph.D. advisor-advisee relationships.\nNo-tree like disease relationships, proteins interactions etc\nResults\nCombinatorial construction outperforms approaches based on optimization in terms of both MAP and distortion.\neg on WordNet, the combinatorial approach achieves a MAP of 0.989 with just 2 dimensions while the previous best was 0.87 with 200 dimensions.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1804.03329"
    },
    "76": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Learned-Optimizers-that-Scale-and-Generalize",
        "transcript": "Learned Optimizers that Scale and Generalize \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nLearned Optimizers that Scale and Generalize\n2017\n\u2022\nICML 2017\n\u2022\nHierarchial RNN\n\u2022\nLearning Optimizer'\n\u2022\nMeta Learning\n\u2022\nAI\n\u2022\nICML\n\u2022\nOptimizer\n\u2022\nRNN\n01 Nov 2018\nIntroduction\nThe paper introduces a learned gradient descent optimizer that has low memory and computational overhead and that generalizes well to new tasks.\nLink to the paper\nKey Advantage\nUses a hierarchial RNN architecture augmented by features like adapted input an output scaling, momentum etc.\nA meta-learning set of small diverse optimization tasks, with diverse loss landscapes is developed. The learnt optimizer generalizes to much more complex tasks and setups.\nArchitecture\nA hierarchical RNN is designed to act as a learned optimizer. This RNN is the meta-learner and its parameters are shared across different tasks.\nThe learned optimizer takes as input the gradient (and related metadata) for each parameter and outputs the update to the parameters.\nAt the lowest level of hierarchical, a small \u201cparameter RNN\u201d ingests the gradient (and related metadata).\nOne level up, an intermediate \u201cTensor RNN\u201d incorporates information from a subset of Parameter RNNS (eg one Tensor RNN per layer of feedforward network).\nAt the highest level is the glocal RNN which receives input from all the Tensor RNNs and can keep track of weight updates across the task.\nthe input of each RNN is averaged and fed as input to the subsequent RNN and the output of each RNN is fed as bias to the previous RNN.\nIn practice, the hidden states are fixed at 10, 30 and 20 respectively.\nFeatures inspired from existing optimizers\nAttention and Nesterov\u2019s momentum\nAttention mechanism is incorporated by attending to new regions of the loss surface (which are an offset from previous parameter location).\nTo incorporate momentum on multiple timescales, the exponential moving average of the gradient at several timescales is also provided as input.\nThe average gradients are rescaled (as in RMSProp and Adam)\nRelative log gradient magnitudes are also provided as input so that the optimizer can access how the gradient magnitude changes with time.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.04813"
    },
    "77": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/One-shot-Learning-with-Memory-Augmented-Neural-Networks",
        "transcript": "One-shot Learning with Memory-Augmented Neural Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nOne-shot Learning with Memory-Augmented Neural Networks\n2016\n\u2022\nMemory Augmented Neural Network\n\u2022\nMeta Learning\n\u2022\nOne shot learning\n\u2022\nAI\n\u2022\nMANN\n\u2022\nMemory\n25 Oct 2018\nIntroduction\nThe paper demonstrates that Memory Augmented Neural Networks (MANN) are suitable for one-shot learning by introducing a new method for accessing an external memory.\nThis method focuses on memory content while earlier methods additionally used memory location based focusing mechanisms.\nHere, MANN refers to neural networks that have an external memory. This includes Neural Turning Machines (NTMs) and excludes LSTMs.\nLink to the paper\nMeta-Learning\nIn meta-learning, a learner is learning at two levels.\nThe learner is shown a sequence of tasks D\n1\n, D\n2\n, \u2026, D\nT\n.\nWhen it is training on one of the datasets (say D\nT\n), it learns to solve the current dataset.\nAt the same time, the learner tries to incorporate knowledge about how task structure changes across different datasets (second level of learning).\nMANN + Meta Learning\nFollowing are the desirable characteristics for a scalable, combined architecture:\nMemory representation should be both stable and element-wise accessible.\nNumber of model parameters should not be tied to the size of the memory.\nTask Setup\nIn standard learning, the goal is to reduce error on some dataset D. In meta-learning, the goal is to reduce the error across a distribution of datasets p(D).\nEach dataset is presented to the model in the form (x\n1\n, null), (x\n1\n, y\n0\n), \u2026, (x\nt+1\n, y\nt\n) where y\nt\nis the correct label (or value) corresponding to the inpuit x\nt\n.\nFurther, the data labels are shuffled from dataset to dataset.\nThe model must learn to hold the data samples in memory till the appropriate candidate labels are presented in the next step.\nThe idea is that a model that meta learns would learn to map data representation to correct labels regardless of the actual context of data representation or the label.\nThe paper uses NTM as the MANN with one modification.\nIn the original formulation, the memories were addressed by both context and location. Location-based addressing is not optimal for the current setup where information encoding is not independent of the sequence.\nA new access module - LRUA - Least Recent Used Access - is used to write to memory.\nLRUA is purely content-based and writes to either least used memory location (to preserve recent information) or most recently used memory location (to overwrite recent information with more relevant information). This is decided on the basis of interpolation between previous read weights and weights scaled according to the usage weight.\nDatasets\nOmniglot (classification)\nSampled functions from Gaussian Processes\nResults\nFor the omniglot dataset, the model was trained with various combinations of randomly chosen classes with randomly chosen labels.\nAs baselines, following models were considered:\nRegular NTM\nLSTM\nFeedforward RNN\nNearest Neighbour Classifier\nSince each episode (dataset created by the combination of classes) contains unique classes (with their own unique labels) it is important to clear the memory across different episodes.\nFor the regression task, the data was generated from a GP prior with a fixed set of hyper-parameters which resulted in different functions.\nFor both the tasks, the MANN architecture outperforms the LSTM architecture baseline NTMs.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1605.06065"
    },
    "78": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/BabyAI-First-Steps-Towards-Grounded-Language-Learning-With-a-Human-In-the-Loop",
        "transcript": "BabyAI - First Steps Towards Grounded Language Learning With a Human In the Loop \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nBabyAI - First Steps Towards Grounded Language Learning With a Human In the Loop\n2018\n\u2022\nCurriculum Learning\n\u2022\nGrounded Language Learning\n\u2022\nICLR 2018\n\u2022\nInteractive Teaching\n\u2022\nNatural Language Processing\n\u2022\nReinforcement Learning\n\u2022\nAI\n\u2022\nEnvironment\n\u2022\nICLR\n\u2022\nNLP\n\u2022\nRL\n18 Oct 2018\nIntroduction\nBabyAI is a research platform to investigate and support the feasibility of including humans in the loop for grounded language learning.\nThe setup is a series of levels (of increasing difficulty) to train the agent to acquire a synthetic language (Baby Language) which is a proper subset of English language.\nLink to the paper\nMotivation\nBabyAI platform provides support for curriculum learning and interactive learning as part of its human-in-the-loop training setup.\nCurriculum learning is incorporated by having a curriculum of levels of increasing difficulty.\nInteractive learning is supported by including a heuristic expert which can provide new demonstrations on the fly to the learning agent.\nThe heuristic expert can be thought of as the human-in-the-loop which can guide the agent through the learning process.\nOne downside of human-in-the-loop is the poor sample complexity of the learning agent. The heuristic agent can be used to estimate the sample  efficiency.\nContribution\nBabyAI research platform for grounded language learning with a simulated human-in-the-loop.\nBaseline results for performance and sample efficiency for the different tasks.\nBabyAI Platform\nEnvironment\nMiniGrid - A partially observable 2D grid-world environment.\nEntities - Agent, ball, box, door, keys\nActions - pick, drop or move objects, unlock doors etc.\nBaby Language\nSynthetic Language (a proper subset of English) - Used to give instructions to the agent\nSupport for verifying if the task (and the subtasks) are completed or not\nLevels\nA level is an instruction-following task.\nFormally, a level is a distribution of missions - a combination of initial state of the environment and an instruction (in Baby Language)\nMotivated by curriculum learning, the authors create a series of tasks (with increasing difficulty).\nA subset of skills (competencies) is required for solving each task. The platform takes into account this constraint when creating a level.\nHeuristic Expert\nThe platform supports a Heuristic expert that simulates the role of a human teacher and knows how to solve each task.\nFor any level, it can suggest actions or generate demonstrations (given the state of the environment).\nExperiment\nAn imitation learning baseline is trained for each level.\nData requirement for each level and the benefits of curriculum learning and imitation learning are investigated (in terms of sample efficiency).\nModel Architecture\nGRU to encode the sentence, CNN to encode the input observation\nFiLM layer to combine the two representations\nLSTM to encode the per-timestep FiLM encoding (timesteps in the environment)\nTwo model variants are considered:\nLarge Model - Bidirectional GRU + attention + large hidden state\nSmall Model - Unidirectional GRU + No attention + small hidden state\nHeuristic expert used to generate trajectory and the models are trained by imitation learning (to be used as baselines)\nResults\nThe key takeaway is that the current deep learning approaches are extremely sample inefficient when learning a compositional language.\nData efficiency of RL methods is much worse than that of imitation learning methods showing that the current imitation learning and reinforcement learning methods scale and generalize poorly.\nCurriculum-based pretraining and interactive learning was found to be useful in only some cases.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1810.08272"
    },
    "79": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Poincare-Embeddings-for-Learning-Hierarchical-Representations",
        "transcript": "Poincar\u00e9 Embeddings for Learning Hierarchical Representations \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nPoincar\u00e9 Embeddings for Learning Hierarchical Representations\n2017\n\u2022\nHyperbolic Embedding\n\u2022\nGraph Representation\n\u2022\nNatural Language Processing\n\u2022\nPoincare Ball Model\n\u2022\nAI\n\u2022\nEmbedding\n\u2022\nGraph\n\u2022\nNLP\n\u2022\nTree\n11 Oct 2018\nIntroduction\nMuch of the work in representation leaning uses Euclidean vector spaces to embed datapoints (like words, nodes, entities etc).\nThis approach is not effective when data has a (latent) hierarchical structure.\nThe paper proposes to compute the embeddings in the hyperbolic space so as to preserve both the similarity and structure information.\nLink to the paper\nHyperbolic Geometry\nHyperbolic spaces are spaces with a constant negative curvature while Euclidean spaces have zero curvature.\nThe hyperbolic disc area and circle length increase exponentially with the radius r while in Euclidean space, it increases quadratically and linearly respectively.\nThis makes the hyperbolic space more suitable for embedding tree-like structures where the number of nodes increases as we move away from the root.\nHyperbolic spaces can be thought of as the continuous version of trees and trees can be thought of as the discrete version of hyperbolic spaces.\nPoincare Embeddings\nPoincare model is one of the several possible models of the hyperbolic space and is considered here as it is more amenable to gradient-based optimisation.\nDistance between 2 pints change smoothly and is symmetric. Thus the hierarchical organisation only depends on the distance from the origin which makes the model applicable in settings where the hierarchical structure needs to be inferred from the data.\nEventually the norm of a point represents its hierarchy and distance between the points represents similarity.\nOptimization\nRSGD (Riemannian SGD) method is used.\nRiemannian gradients can be computed from the Euclidean gradients by rescaling with the inverse of the Poincare ball metric tensor.\nThe embeddings are constrained to be within the Poincare ball by projection operation which normalizes the magnitude of embeddings to be 1.\nTraining Details\nInitializing the embeddings close to 0 (by sampling uniformly from (-0.001, 0.001)) helps.\nThe model is trained for an initial burn-out period of 10 epochs with 0.1 times the learning rate so as to find a better initial angular layout.\nEvaluation\nEmbedding taxonomy for wordnet task\nSetup\nReconstruction\nLink Prediction\nThe input data is a collection of a pair of words (u, v) which are related to each other.\nFor each word pair, 10 negative samples of the form (u, v\u2019) are sampled and the training procedure uses a soft ranking loss that aims to bring the related objects closer together.\nNetwork Embedding\nBaselines\nEuclidean Embeddings\nTranslational Embedding where a relation vector corresponding to the edge type is also learnt.\nDatasets\nASTROPH\nCONDMAT\nGRQC\nHEPPH\nLexical Entailment\n* Hyperlex - Gold standard to evaluate how well the semantics models capture lexical entailment on a scale of [0, 10].\n\n* The key takeaway is that for all the datasets/setups, hyperbolic embeddings give a performance benefit when the embedding dimension is small.\nChallenges\nHyperbolic embeddings are not suitable for all the datasets. Eg if the dataset is not tree-like or has cycles.\nHyperbolic embeddings are difficult to optimize as each operation needs to be modified to be usable in the hyperbolic space.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/pdf/1705.08039.pdf"
    },
    "80": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/When-Recurrent-Models-Don-t-Need-To-Be-Recurrent",
        "transcript": "When Recurrent Models Don\u2019t Need To Be Recurrent \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nWhen Recurrent Models Don\u2019t Need To Be Recurrent\n2018\n\u2022\nNatural Language Processing\n\u2022\nRecurrent Neural Network\n\u2022\nAI\n\u2022\nNLP\n\u2022\nRNN\n\u2022\nTheory\n04 Oct 2018\nIntroduction\nThe paper explores \u201cif a well behaved RNN can be replaced by a feed-forward network of comparable size without loss in performance.\u201d\n\u201cWell behaved\u201d is defined in terms of control-theoretic notion of stability. This roughly requires that the gradients do not explode over time.\nThe paper shows that under the stability assumption, feedforward networks can approximate RNNs for both training and inference. The results are empirically validated as well.\nLink to the paper\nProblem Setting\nConsider a general, non linear dynamical system given by a differential state transition map \u03a6\nw\n. The hidden h\nt\n= \u03a6\nw\n(h\nt-1\n, x\nt\n).\nAssumptions:\n\u03a6 is smooth in w and h.\nh\n0\n= 0\n\u03a6\nw\n(0, 0) = 0 (can be ensured by translation)\nStable models are the ones where \u03a6 is contractive ie \u03a6\nw\n(h, x) - \u03a6\nw\n(h\u2019, x) is less than \u039b * (h - h\u2019)\nFor example, in RNN, stability would require that norm(w) is less than (L\np\n)\n-1\nwhere L\np\nis the Lipschitz constant of the point-wise non linearity used.\nThe feedforward approximation uses a finite context (of length k) and is a truncated model.\nA non-parametric function f maps the output of the recurrent model to prediction. If f is desired to be a parametric model, its parameters can be pushed to the recurrent model.\nTheoretical Results\nFor a \u039b-contractive system, it can be proved that for a large k (and additional Lipschitz assumptions) the difference in prediction between the recurrent and truncated mode is negligible.\nIf the recurrent model and truncated feed-forward network are initialized at the same point and trained over the same input for N-step, then for an optimal k, the weights of the two models would be very close in the Euclidean space. It can be shown that this small difference does not lead to large gradient differences during subsequent update steps.\nThis can be roughly interpreted as - if the gradient descent can train a stable recurrent network, it can also train a feedforward model and vice-versa.\nThe stability condition is important as, without that, truncated models would be bad (even for large values of k). Further, it is difficult to show that gradient descent converges to a stationary point.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1805.10369"
    },
    "81": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/HoME-a-Household-Multimodal-Environment",
        "transcript": "HoME - a Household Multimodal Environment \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nHoME - a Household Multimodal Environment\n2017\n\u2022\nMulti Modal\n\u2022\nNIPS 2017\n\u2022\nNIPS Workskop\n\u2022\nVirtual Embodiment\n\u2022\nAI\n\u2022\nNIPS\n27 Sep 2018\nIntroduction\nEnvironment for learning using modalities like vision, audio, semantics, physics and interaction with objects and other agents.\nLink to the paper\nMotivation\nHumans learn by interacting with their surroundings (environment).\nSimilarly training an agent in an interactive multi-model environment (virtual embodiment) could be useful for a learning agent.\nCharacteristics\nOpen-source and Open-AI gym compatible\nBuilt on top of 45000 3D house layouts from SUNCG dataset.\nProvides both 3D visual and audio recording.\nSemantic image segmentation and langauge description of objects.\nComponents\nRendering Engine\nImplemented using Panda 3D game engine.\nRenders RGB+depth scenes based on textures, multi-source lightings and shadows.\nAcoustic Engine\nImplemented using EVERT\nSupports multiple microphones, sound sources, sound absorption based on material, atmospheric conditions etc.\nSemantics Engine\nProvides a short textual description for each object, along with information like color, category, material size, location etc.\nPhysics Engine\nImplemented using Bullet3 Engine\nSupports physical interaction, external forces like gravity and position and velocity information for multiple agents.\nPotential Applications\nVisual Question Answering\nConversational Agents\nTraining an agent to follow instructions\nMulti-agent communication\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.11017"
    },
    "82": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Emergence-of-Grounded-Compositional-Language-in-Multi-Agent-Populations",
        "transcript": "Emergence of Grounded Compositional Language in Multi-Agent Populations \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nEmergence of Grounded Compositional Language in Multi-Agent Populations\n2018\n\u2022\nAAAI 2018\n\u2022\nEmergent Language\n\u2022\nMulti-Agent\n\u2022\nNatural Language Processing\n\u2022\nAAAI\n\u2022\nAI\n\u2022\nNLP\n12 Sep 2018\nIntroduction\nThe paper provides a multi-agent learning environment and proposes a learning approach that facilitates the emergence of a basic compositional language.\nThe language is quite rudimentary and is essentially a sequence of abstract discrete symbols. But it does comprise of a defined vocabulary and syntax.\nLink to the paper\nSetup\nCooperative, partially observable Markov game (multi-agent extension of MDP).\nAll agents have identical action and observation spaces, use the same policy and receive a shared reward.\nGrounded Communication Environment\nPhysically simulated 2-D environment in continuous space and discrete time with N agents and M landmarks.\nThe agents and the landmarks would occupy some location and would have some attributes (colour, shape).\nWithin the environment, the agents can\ngo to\na location,\nlook\nat a location or\ndo nothing\n. Additionally, they can utter communication symbols c (from a shared vocabulary C). Agents themselves learn to assign a meaning to the symbols.\nEach agent has an internal goal (which could require interaction with other agents to complete) which the other agents cannot see.\nGoal for agent\ni\nconsists of an action to perform, a landmark location where to perform the action and another agent who should be performing the action.\nSince the agent is continuously emitting symbols, a memory module is provided and simple additive memory updates are done.\nFor interaction, the agents could use verbal utterances, non-verbal signals (gaze) or non-communicative strategies (pushing other agents).\nApproach\nA model of all agent and environment state dynamics is created over time and the return gradient is computed.\nGumbel-Softmax distribution is used to obtain categorical word emission c.\nA multi-layer perceptron is used to model the policy which returns action, communication symbol and the memory update for each agent.\nSince the number of agents (and hence the number of communication streams etc) can vary across instantiations, an identical model is instantiated per agent and per communication stream.\nThe output of individual processing modules are pooled into feature vectors corresponding to communication and physical observations. These pooled features and the goal vectors are fed to the final processing module from which actions and categorical symbols are sampled.\nIn practice, using an additional task (each agent predicts the goal for another agent) encouraged more meaningful communication utterances.\nCompositionality and Vocabulary Size\nAuthors recommend using a large vocabulary with a soft penalty that discourages use of too many words. This leads to use of a large vocabulary in the intermediate state which converges to a small vocabulary.\nAlong the lines of rich gets richer dynamics, the communication symbol c\u2019s are modelled as being generated by a Dirichlet process. The resulting reward across all agents is the log-likelihood of all communication utterances to have been generated by a Dirichlet process.\nSince the agents can only communicate in discrete symbols and do not have a global positioning reference, they need to unambiguously communicate landmark references to other agents.\nCase I - Agents can not see each other\nNon-verbal communication is not possible.\nWhen trained with just 2 agents, symbols are assigned for each landmark and action.\nAs the number of agents is increased, additional symbols are used to refer to agents.\nIf the agents of the same colour are asked to perform conflicting tasks, they perform the average of conflicting tasks. If distractor locations are added, the agents learn to ignore them.\nNon-verbal communication\nAgents are allowed to observe other agents\u2019 position, gaze etc.\nNow the location can be pointed to using gaze.\nIf gaze is disabled, the agent could indicate the goal landmark by moving to it.\nBasically even when the communication is disabled the agents can come up with strategies to complete the task.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.04908"
    },
    "83": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/A-Semantic-Loss-Function-for-Deep-Learning-with-Symbolic-Knowledge",
        "transcript": "A Semantic Loss Function for Deep Learning with Symbolic Knowledge \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nA Semantic Loss Function for Deep Learning with Symbolic Knowledge\n2018\n\u2022\nICML 2018\n\u2022\nLoss Function\n\u2022\nSemantic Loss\n\u2022\nSymbolic Knowledge\n\u2022\nAI\n\u2022\nICML\n\u2022\nLoss\n21 Aug 2018\nIntroduction\nThe paper proposes an approach for using symbolic knowledge in deep learning systems. These constraints are often expressed as boolean constraints on the output of the deep learning system and directly incorporating these constraints break the differentiability of the system.\nLink to the paper\nProblem Setting\nThe model is given some input data to perform predictions and symbolic knowledge is provided in form of boolean constraints like exactly-one constraint for one-hot output encoding.\nMost approaches tend to encode the symbolic knowledge in the vector space embedding to keep the model pipeline differentiable. In this process, the precise meaning of symbolic knowledge is often lost.\nA differentiable \u201csemantic loss\u201d is derived which captures the meaning of the constraint while being independent of its syntax.\nTerminology\nA state\nx\n(state refers to the instantiation of boolean variables) satisfies a sentence\na\nif\na\nevaluates to true when using the variables as specified by\nx\n.\nA sentence\na\nentails another sentence\nb\nif all states that satisfy\na\nalso satisfy\nb\n.\nThe row output vector of the neural network is denoted as\np\nwhere each value in\np\ndenotes the probability of an output.\nThree different output constraints are studied:\nExactly-one constraint\nExactly one value in\np\nshould be true.\nCan be expressed in boolean logic as follows: Let (x1, x2, \u2026, xn) be variables in\np\n. Then (not xi or not xj) for all pair of variables and (x1 or x2 or \u2026 xn).\nValid Simple Path Constraint\nSet of edges must form a valid path.\nOrdering Constraint\nDefining an ordering over the variables.\nSemantic Loss\nThe semantic loss\nL\ns\n(a, p)\nis a function of a propositional logic sentence\na\n(the symbolic knowldge constraint) and\np\n(output of the neural network).\na\nis defined over variables (x1, \u2026, xn) and\np\nis interpreted as a vector of probabilities corresponding to these variables\nxi\u2019s\n.\nThe semantic loss is directly proportional to the negative log likelihood of generating a state that satisfies the constraints when sampling values according to the distribution\np\n.\nMain Axioms and Insights\nMonotonicity\nIf a sentence\na\nentails another sentence\nb\nthen for any given\np\n,\nL\ns\n(a, p) > L\ns\n(b, p)\nie adding more constraints cannot decrease the semantic loss.\nSemantic Equivalence\nIf two sentences are logically equivalent, their semantic loss is the same.\nIdentity\nFor any given sentence\na\n, its representation as a sentence is equivalent to its representation as a deterministic vector ie writing the \u201cone-hot\u201d constraint as a boolean expression is equivalent to a one-hot vector.\nSatisfaction\nIf\np\nentails the sentence\na\nthen\nL\ns\n(a, p) = 0\n.\nLabel-literal correspondence\nWhen the constraint is defined in terms of a single variable, it can be interpreted as the supervised label.\nHence the semantic loss in case of a single variable should be equivalent to the cross-entropy loss.\nTruth\nThe semantic loss of a true sentence is 0\nNon-negativity\nSemantic loss should always be non-negative.\nProbabilities of variables that are not part of the constraint, do not affect the semantic loss.\nIt can be shown that the semantic loss function satisfies all these axioms (and the other axioms specified in the paper) and is the only function to do so, up to a multiplicative constant.\nExperimental Evaluation\nSemantic Loss is used in the semi-supervised setting for Permuted MNIST, Fashion MNIST and CIFAR-10.\nThe key takeaway is that using semantic loss improves the performance of the state-of-the-art models for Fashion MNIST and CIFAR-10.\nOne downside is that the effectiveness of the semantic loss in this type of constraint strongly depends on the performance of the underlying model. Further, the semantic loss does not improve the performance in case of fully supervised scenario.\nFurther experiments are performed to evaluate the performance of the semantic loss on complex constraints. Since these tasks aim to highlight the effect of using semantic loss, only simple models (MLPs) are evaluated.\nTractability of Semantic Loss\nThe semantic loss is similar to the automated reasoning task called as weight model counting (wmc).\nCircuit compiler techniques can be used to compute wmc while allowing backpropagation.\nNotes\nThe proposed idea is simple and intuitive and the results on semi-supervised classification task are quite good. It would be interesting to extend and scale this method for more complex constraints.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.11157"
    },
    "84": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Hierarchical-Graph-Representation-Learning-with-Differentiable-Pooling",
        "transcript": "Hierarchical Graph Representation Learning with Differentiable Pooling \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nHierarchical Graph Representation Learning with Differentiable Pooling\n2018\n\u2022\nGraph Neural Network\n\u2022\nGraph Representation\n\u2022\nAI\n\u2022\nGraph\n\u2022\nGNN\n\u2022\nPooling\n16 Aug 2018\nIntroduction\nMost existing GNN (Graph Neural Network) methods are inherently flat and are unable to process the information in a hierarchical manner.\nThe paper proposes a differentiable graph pooling operation, DIFFPOOL, that can generate hierarchical graph representations and can be easily plugged into many GNN architectures.\nLink to the paper\nKey Idea\nCNNs have spatial pooling operation that allows for deep CNN architectures to operate on coarse graph representations of input images.\nThis notion cannot be applied as-is to graphs as they do not have a natural notion of spatial locality like images do.\nDIFFPOOL attempts to resolve this problem by learning a differentiable soft-assignment at each layer which is equivalent to pooling the cluster of nodes to obtain a sparse representation.\nApproach\nGiven a graph\nG(A, F)\n, where\nA\nis the adjacency matrix and\nF\nis the feature matrix.\nGiven a permutation invariant GNN that follows the message passing architecture. The output of this GNN can be expressed as\nZ = GNN(A, X)\nwhere\nX\nis the current feature matrix.\nGoal is to stack\nL\nGNN layers on top of each other such that the\nl\nth\nlayer uses coarsened output from the\n(l-1)\nth\nlayer.\nThis coarsening operation uses a cluster assignment matrix\nS\n.\nThe learned cluster assignment matrix at layer\nl\nis denoted at\nS\nl\nGiven\nS\nl\n, the embedding matrix for the\n(l+1)\nth\nlayer is given as\ntranspose(S\nl\n)Z\nl\nand adjancecy matrix is given by\ntranspose(S\nl\n)A\nl\nS\nl\nA new GNN, called as GNN\npool\nis used to produce the assignment matrix\nS\nby taking a softmax over\nGNN\npool\n(A\nl\n, X\nl\n)\nAs long as the GNN model is permutation invariant, the resulting DIFFPOOL model is also permutation invariant.\nAuxiliary Losses\nThe paper uses 2 auxiliary losses to push the model away from spurious local minima early in the training.\nLink prediction objective - at each layer, link prediction loss ( = A - S(transpose(S))) is minimized with the intuition that the nearby nodes should be pooled together.\nIdeally, the cluster assignment for each node should be a one-hot vector so the entropy for cluster assignment per node is regularized.\nBaselines\nGNN based models\nGraphSage\nMean pooling\nSet2Set pooling\nSort pooling\nStructure2vec\nEdge conditioned filters in CNN\nPatchySan\nKernel based models\nGraphlet, shortest path etc\nModel Variants\nGraphSage\nMean pool + Diff pool (3 or 2 layers)\nStructure2Vec + Diffpool\nDiffpool-Det\nThe assignment matrix\nS\nare generated using graph clustering algorithms.\nDiffpool-NoLP\nThe link prediction objective function is turned off.\nAt each DiffPool layer, the number of classes is set to 25% of the number of nodes before the DiffPool layer.\nResults\nDiffPool obtains the highest average performance across all the pooling approaches and improves upon the base GraphSage architecture by an average of around 7%.\nIn terms of runtime complexity, the paper reports that DiffPool does not incur any significant additional running time. But given that now there are 2 GNN models per layer, the size of the model should increase.\nDiffPool can capture hierarchical community structure even when trained on just the graph classification loss.\nOne advantage of DiffPool is that the nodes are pooled in a non-uniform way so densely connected group of nodes would collapse into one cluster while sparsely connected nodes can retain their identity.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1806.08804"
    },
    "85": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Imagination-Augmented-Agents-for-Deep-Reinforcement-Learning",
        "transcript": "Imagination-Augmented Agents for Deep Reinforcement Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nImagination-Augmented Agents for Deep Reinforcement Learning\n2017\n\u2022\nNIPS 2017\n\u2022\nAI\n\u2022\nModel-Based\n\u2022\nModel-Free\n\u2022\nNIPS\n\u2022\nRL\n08 Aug 2018\nThe paper presents I2A (Imagination Augmented Agent) that combines the model-based and model-free approaches leading to data efficiency and robustness even with imperfect models.\nI2A agent uses the predictions from a learned environment model as an additional context in deep policy networks. This leads to improved data efficiency and robustness to imperfect models.\nLink to the paper\nI2A agent has two main modules - Imagination module and the Policy module.\nImagination Module\nEnvironment Model\nThis is a recurrent model, trained in an unsupervised manner using the agent trajectories. It can be used to predict the future state given the current state and action.\nThe environment model can be rolled out multiple times to obtain a simulated trajectory or an \u201cimagined\u201d trajectory.\nDuring each rollout, the actions are chosen using a rollout policy \u03c0\nr\n.\nRollout Encoder\nA rollout encoder\nE\n(LSTM) is used to process the entire imagined rollout.\nThe imagination module is used to generate\nn\ntrajectories. Each trajectory is a sequence of outputs of the environment model.\nThese\nn\ntrajectories are concatenated into a single \u201cimagination\u201d vector.\nThe training data for the environment model is generated from trajectories of a partially trained model-free agent.\nPretraining the environment model (instead of joint training with policy) leads to faster runtime.\nPolicy Module\nThis module uses the output of both model-based path and model-free path as its input. It generates the policy vector and value function.\nRollout Strategy\nOne rollout is performed for each possible action in the environment ie, the first action in the i\nth\nrollout is the i\nth\naction in the action set.\nSubsequent actions are generated using a shared rollout policy \u03c0\n\u2019\nAn effective strategy was to create a small model-free network \u03c0\n\u2019\n(o\nt\n) and then add a KL loss component that encourages \u03c0\n\u2019\n(o\nt\n)to be similar to the imagination augmented policy \u03c0(o\nt\n).\nBaselines\nModel-free agent\nCopy-model agent - same as I2A but the environment model is replaced by a \u201ccopy\u201d model that just returns the input observations.\nEnvironments\nSokoban\nTask is to push a number of boxes onto given target locations.\nI2A outperforms the baselines and gains in performance as the number of unrolling steps increases (though at a diminishing rate).\nIn case of poor environment models, the agent seems to be able to ignore the later part of the rollout when the error starts to accumulate.\nMonte Carlo search algorithm (without an explicit rollout encoder) performed poorly as compared to the model using rollout encoder.\nPredicting the reward along with value function and action seems to speed up training.\nIf a near-perfect model is available, I2A agent\u2019s performance can be improved by performing Monte Carlo search with the trained I2A agent for the rollout policy. The agent plays entire episodes in simulation and tries to find a successful action sequence within 10 retries.\nMiniPacman\nI2A agent is evaluated to see if a single model can be used to solve multiple tasks.\nA new environment is designed to define multiple tasks in an environment with shared state transitions.\nEach task is specified by a 5-dimensional reward vector that associates a reward with moving, eating food, eating a pill, eating a ghost and being eaten by a ghost.\nA single environment model is trained to predict both observations (frames) and events (eg \u201ceating a pill\u201d). This way, the environment model is shared across all tasks.\nBaseline agents and I2As are trained on each task separately. I2A architecture outperforms the standard agent in all tasks and the copy-model\nbaseline in all but one task.\nThe improvement in performance is higher for tasks where rewards are sparse and where the anticipation\nof ghost dynamics is especially important indicating that the I2A agent can use the environment model to explore the environment more effectively.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1707.06203"
    },
    "86": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Kronecker-Recurrent-Units",
        "transcript": "Kronecker Recurrent Units \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nKronecker Recurrent Units\n2018\n\u2022\nICML 2018\n\u2022\nRecurrent Neural Network\n\u2022\nAI\n\u2022\nICML\n\u2022\nKronecker\n\u2022\nKRU\n\u2022\nNLP\n\u2022\nRNN\n19 Jul 2018\nIntroduction\nRecurrent Neural Networks have two key issues:\nOver parameterization\nwhich increases the time for training and inference.\nIll conditioned\nrecurrent weight matrix which makes training difficult due to vanishing or exploding gradients.\nThe paper presents a flexible RNN model called as KRU (Kronecker Recurrent Units) which overcomes the above problems by using a Kronecker factored recurrent matrix and soft unitary constraints on the factors.\nLink to the paper\nRelated Work\nExisting solutions for overparameterization\nLow-rank decomposition.\nTraining a neural network on the soft targets predicted by a big pre-trained network.\nLow-bit precision training.\nHashing.\nExisting solutions for vanishing and exploding gradients\nGating mechanism like in LSTMs.\nGradient Clipping.\nOrthogonal Weight Initialization.\nParameterizing recurrent weight matrix.\nKRU\nUses a Kronecker factored recurrent matrix which enables controlling the number of parameters and number of factor matrices.\nVanishing and exploding gradients are taken care of by using a soft unitary constraint.\nWhy not use strict unitary constraint:\nRestricts the search space and makes learning process unstable.\nMakes forgetting (irrelevant) information difficult.\nRelaxing the strict constraint has shown to improve the convergence speed and generalization performance.\nKRU can be easily plugged into RNNs, LSTMs and other variants.\nThe recurrent matrix\nW\nis paramterized as a kronecker product of\nF\nmatrices\nW\n0\n, \u2026, W\nF-1\nwhere each\nW\nf\nis a complex matrix of shape\nP\nf\nx Q\nf\nand the product of all\nP\nf\nand producto of all\nQ\nf\nare both equal to\nN\n.\nWhy is\nW\na complex matrix?\nIn the real space, the set of all unitary matrices have the determinant as 1 or -1.\nGiven that determinant is a continuous function, the unitary set in the real space is disconnected.\nThe unitary set in the complex space is connected as its determinants are points on the unit circle.\nSoft Unitary Constraint\nA soft unitary constraint is introduced in the form of regularization term\nW\nf\nH\nW\nf\n- I\n2\n(per kronecker factored recurrent matrix).\nIf each of the Kronecker factors is unitary, the resulting matrix\nW\nwould also be unitary.\nIt is computationally inefficient to apply this constraint over the recurrent matrix\nW\nitself as the complexity of the regularizer is given as\nO(N\n3\n)\n.\nUse of Kronecker factorisation makes it computationally feasible to use this regulariser.\nExperiment\nThe Kronecker recurrent model is compared against the existing recurrent models for multiple tasks including copy memory, adding memory, pixel-by-pixel MNIST, char level language models, polyphonic music modelling, and framewise phoneme classification.\nFor most of the task, KRU model produces results comparable to the best performing models despite using fewer parameters.\nUsing soft unitary constraints in KRU provides a principled alternative to gradient clipping (a common heuristic to avoid exploding gradients).\nFurther, recent theoretical results suggest the gradient descent converges to a global optimizer of linear recurrent networks even if the learning problem is non-convex provided that the spectral norm of the recurrent matrix is bound by 1.\nThe key take away from the paper is that state should be high dimensional so that high capacity network can be used for encoding and decoding the input and output. The recurrent dynamics should be implemented via a low capacity model.s per task.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.10142"
    },
    "87": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Learning-Independent-Causal-Mechanisms",
        "transcript": "Learning Independent Causal Mechanisms \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nLearning Independent Causal Mechanisms\n2018\n\u2022\nICML 2018\n\u2022\nCausal Learning\n\u2022\nCausality\n\u2022\nAI\n\u2022\nICML\n11 Jul 2018\nIntroduction\nThe paper presents a very interesting approach for learning independent (inverse) data transformation from a set of transformed data points in an unsupervised manner.\nLink to the paper\nFormulation\nWe start with a given data distribution\nP\n(say the MNIST dataset) where each x \u03b5 R\nd\n.\nConsider N transformations M\n1\n, \u2026, M\nN\n(functions that map input x to transformed input x\u2019). Note that N need not be known before hand.\nThese transformations can be thought of as independent (from other transformations) causal mechanisms.\nApplying these transformation would give N new distributions Q\n1\n, \u2026, Q\nN\n.\nThese individual distributions are combined to form a single transformed distribution Q which contains the union of samples from the individual distributions.\nAt training time, two datasets are created. One dataset corresponds to untransformed objects (sampled from\nP\n), referred to as\nD\nP\n. The other dataset corresponds to samples from the transformed distribution\nQ\nand is referred to as\nD\nQ\n.\nNote that all the samples in\nD\nP\nand\nD\nQ\nare sampled independently and no supervising information is needed.\nA series of N\u2019 parametric models, called as experts, are initialized and would be trained to learn the different mechanisms.\nFor simplicity, assume that N = N\u2019. If N > N\u2019, some experts would learn more than one transformation or certain transformations would not be learnt. If N < N\u2019, some experts would not learn anything or some experts would learn the same distribution. All of these cases can be diagnosed and corrected by changing the number of experts.\nThe experts are trained with the goal of maximizing an objective parameter\nc\n: R\nd\nto R.\nc\ntakes high values on the support of\nP\nand low values outside.\nDuring training, an example x\nQ\n(from D\nQ\n) is fed to all the experts at the same time. Each expert produces a value\nc\nj\n= c(E\nj\n(x\nQ\n))\nThe winning expert is the one whose output is the max among all the outputs. Its parameters are updated to maximise its output while the other experts are not updated.\nThis forces the best performing model to become even better and hence specialize.\nThe objective\nc\ncomes from adversarial training where a discriminator network discriminates between the untransformed input and the output of the experts.\nEach expert can be thought of as a GAN that conditions on the input x\nQ\n(and not on a noise vector). The output of the different experts is fed to the discriminator which provides both a selection mechanism and the gradients for training the experts.\nExperiments\nExperiments are performed on the MNIST dataset using the transformations like translation along 4 directions and along 4 diagonals, contrast shift and inversion.\nThe discriminator is further trained against the output of all the losing experts thereby furthering strengthing the winning expert.\nApproximate Identity Initialization\nThe experts are initialized randomly and then pretrained to approximate the identity function by training with identical input-output pairs.\nThis ensures that the experts start from a similar level.\nIn practice, it seems necessary for the success of the proposed approach.\nObservations\nDuring the initial phase, there is a heavy competition between the experts and eventually different winners emerge for different transformations.\nThe approximate quality of reconstructed output was also evaluated using a downstream task.\n3 type of inputs were created:\nUntransformed images\nTransformed images\nTransformed images a being processed by experts.\nThese inputs are fed to a pretrained MNISTN classifier.\nThe classifier performs poorly on the transformed images while the performance for images processed by experts quickly catches up with the performance on untransformed images.\nThe experts E\ni\ngeneralize on the data points from a different dataset as well.\nTo test the generalisation capabilities of the expert, a sample of data from the omniglot dataset is transformed and fed to experts (which are trained only on MNIST).\nEach expert consistently applies the same transformation even though the inputs are outside the training domain.\nThis suggests that the experts have generalized to different transformations irrespective of the underlying dataset.\nComments\nThe experiments are quite limited in terms of complexity of dataset and complexity of transformation but it provides evidence for a promising connection between deep learning and causality.\nAppendix mentions that in case there are too many experts, for most of the tasks, only one model specialises and the extra experts do not specialize at all. This is interesting as there is no explicit regularisation penalty which prevents the emergence of multiple experts per task.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1712.00961"
    },
    "88": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Memory-Based-Parameter-Adaption",
        "transcript": "Memory-based Parameter Adaptation \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nMemory-based Parameter Adaptation\n2018\n\u2022\nICLR 2018\n\u2022\nContinual Learning\n\u2022\nIncremental Learning\n\u2022\nWeight Adaptation\n\u2022\nAI\n\u2022\nICLR\n04 Jul 2018\nIntroduction\nStandard Deep Learning networks are not suitable for continual learning setting as the change in the data distribution leads to catastrophic forgetting.\nThe paper proposes Memory-based Parameter Adaptation (MbPA), a technique that augments a standard neural network with an episodic memory (containing examples from the previous tasks).\nThis episodic memory allows for rapid acquisition of new knowledge (corresponding to the current task) while preserving performance on the previous tasks.\nLink to the paper\nArchitecture\nMbPA consists of 3 components:\nEmbedding Network\nf\nMemory\nM\nOutput network\ng\nf\nand\ng\nare parametric components while\nM\nis a non-parametric component.\nM\nis a dynamically sized dictionary where the key represents the output of the embedding network and the value represents the desired output for a given input (input to the model).\nWhen a new training tuple (x\nj\n, y\nj\n) is fed as input to the model, a key-value pair (h\nj\n, v\nj\n) is added to the memory. h\nj\n= f(x\nj\n)\nThe memory has a fixed size and acts as a circular buffer. When it gets filled up, earlier examples are dropped.\nWhen accessing the memory using a key\nh\nkey\n, the k-nearest neighbours (in terms of distance from the given key) are retrieved.\nTraining Phase\nDuring the training phase, the memory is only used to store the input examples and does not interfere with the training procedure.\nTesting Phase\nDuring testing, the memory is used to adapt the parameters of the output network\ng\nwhile the embedding network\nf\nremains the same.\nGiven the input x, obtain the embedding corresponding to x and using that as the key, retrieve the k-nearest neighbours from the memory.\nEach retrived neighbour is a tuple of the form (h\nk\n, v\nk\n, w\nk\n) where w\nk\nis propotional to the closeness between the input query and the key corresponding to the retrived example.\nThe collection of all the retrieved examples are referred to as the context\nC\n.\nThe parameters of the output network\ng\nare adapted from \u03b8 to \u03b8\nx\nwhere \u03b8\nx\n= \u03b8 + \u03b4\nM\n(x, \u03b8)\n\u03b4\nM\n(x, \u03b8) is referred to as the contextual update of parameters of the output network.\nInterpretation of MbPA\nMbPA can be interpreted as decreasing the weighted average of negative log likelihood over the retrieved neighbours in the context C.\nThe expression corresponding to  \u03b4\nM\n(x, \u03b8) can be obtained by performing gradient descent to minimise the max a posterior over the context C.\nThe a posterior expression can be written as a sum of two terms - one corresponding to a weighted likelihood of data in the context C and the other corresponding to a regularisation term to prevent overfitting the data.\nThis idea can be thought of as a generalisation of attention. Attention can be viewed as fitting a constant function over the neighbourhood of memories while MbPA fits a more general function which is parameterised by the output network of the given model. Refer appendix E in the paper for further details.\nExperiments\nMbPA aims to solve the fundamental problem of enabling the model to deal with changes in data distribution.\nIn that sense, it is evaluated on a wide range of settings: continual learning, incremental learning, unbalanced datasets and change in data distribution at test time.\nContinual Learning:\nIn this setting, the model encounters a sequence of tasks and cannot revisit a previous task.\nPermuted MNIST dataset was used.\nThe key takeaway is that once a task is catastrophically forgotten, only a few gradient updates on a carefully selected data, are sufficient to recover the performance.\nIncremental Learning:\nIn this setting, the model is trained on a subset of classes and then introduced to novel, unseen classes. The model is tested to see if it can incorporate the new knowledge while retaining the knowledge about the previous classes.\nImagenet dataset with Resnet V1 model is used. It is first pretrained on 500 classes and then fine-tuned to see how quickly could it adapt to new classes.\nUnbalanced Dataset:\nThis setting is similar to the incremental learning setting with the key difference that once the model has been trained on a part of the dataset and is to be finetuned to acquire new knowledge, the dataset used for finetuning is much smaller than the initial dataset thus creating the effect of unbalanced datasets.\nLanguage Modelling:\nMbPA is used to adapt to the shift in the word distribution that is common to language modelling tasks. PTB and WikiText datasets were used.\nMbPA exhibits strong performance on all these tasks showing that the memory-based parameter adaption technique is effective across a range of tasks in supervised learning.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.10542"
    },
    "89": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Born-Again-Neural-Networks",
        "transcript": "Born Again Neural Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nBorn Again Neural Networks\n2018\n\u2022\nICML 2018\n\u2022\nKnowledge Distillation\n\u2022\nKnowledge Transfer\n\u2022\nAI\n\u2022\nICML\n\u2022\nKD\n09 Jun 2018\nIntroduction\nThe paper explores knowledge distillation (KD) from the perspective of transferring knowledge between 2 networks of identical capacity.\nThis is in contrast to much of the previous work in KD which has focused on transferring knowledge from a larger network to a smaller network.\nThe paper reports that these Born Again Networks (BANs) outperform their teachers by significant margins in many cases.\nLink to the paper\nApproach\nThe standard KD setting is as follows:\nStart with an untrained network (or ensemble of networks) and train them for the given task. This network is referred to as the teacher network.\nNow start with another untrained network (generally of smaller size than the teacher network) and train it using the output of the teacher network. This network is referred to as the student network.\nThe paper augments this setting with an extra cross-entropy loss between the output of the teacher and the student networks. The student tried to predict the correct answer while matching the output distribution of the teacher.\nThe resulting student network is referred to as BAN - Born Again Network.\nThe same approach can be used multiple times (with diminishing returns) where the kth generation student is initialized by knowledge transfer from (k-1)th generation student.\nThe output of multiple generation BANs are combined via averaging to produce BANE (Born Again Network Ensemble).\nDark Knowledge\nHinton et al\nsuggested that even when the output of the teacher network is incorrect, it contains useful information about the similarity between the output classes. This information is referred to as the \u201cdark knowledge\u201d.\nThe current paper observed that the gradient of the correct output dimension during distillation and normal supervised training resembles the original gradient up to a  weight factor. This sample specific weight is defined by the value of the teacher\u2019s max output.\nThis suggests distillation may be performing some kind of importance weighing. To explore this further, the paper considers 2 cases:\nConfidence Weighted By Teacher Max (CWTM) - where each example in the student\u2019s loss function is weighted by the confidence that the teacher has on the prediction for that sample. The student incurs a higher loss if the teacher was more confident about the example.\nDark Knowledge with Permuted Predictions (DKPP) - The non-argmax output of teacher\u2019s predictive distribution are permuted thus destroying the information about which output classes are related.\nThe key effect of these variations is that the covariance between the output classes is lost and classical knowledge distillation would not be sufficient to explain improvements (if any).\nExperiments\nImage Data\nDatasets\nCIFAR10\nCIFAR100\nBaselines\nResNets\nDenseNets\nBAN Variants\nBAN-DenseNet and BAN-ResNet  - Train a sequence of 2 or 3 BANs using DenseNets and ResNets. Different variants constrain BANs to be similar to their teacher or penalize l2-distance between student and teacher activations etc.\nTwo settings with CWTM and DKPP as explained earlier.\nBAN-Resnet with DenseNet teacher and BAN-DenseNet with ResNet teacher\nText Data\nDatasets:\nPTB Dataset\nBaselines\nCNN-LSTM model\nBAN Variant\nLSTM\nResults\nBAN student models improved over their teachers in most of the configurations.\nTraining BANs across multiple generations leads to saturating improvements.\nThe student models exhibit improvements even in the control settings (CWTM and DKPP).\nOne reason could be that the permutation procedure did not remove the higher order moments of output distribution.\nImprovements in the CWTM model suggests that the pre-trained models can be used to rebalance the training set by giving lesser weight for samples where the teacher\u2019s output distribution is more spread.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1805.04770"
    },
    "90": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Net2Net-Accelerating-Learning-via-Knowledge-Transfer",
        "transcript": "Net2Net-Accelerating Learning via Knowledge Transfer \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nNet2Net-Accelerating Learning via Knowledge Transfer\n2016\n\u2022\nICLR 2016\n\u2022\nKnowledge Transfer\n\u2022\nLifelong Learning\n\u2022\nAI\n\u2022\nCV\n21 May 2018\nNotes\nThe paper presents a simple yet effective approach for transferring knowledge from a trained neural network (referred to as the teacher network) to a large, untrained neural network (referred to as the student network).\nThe key idea is to use a function-preserving transformation that guarantees that for any given input, the output from the teacher network and the newly created student network would be the same.\nLink to the paper\nLink to an implementation\nThe approach works as follows - Let us say that the teacher network was represented by the transformation\ny = f(x, \u03b8)\nwhere\n\u03b8\nrefer to the parameters of the network. The task is to choose a new set of parameters\n\u03b8\u2019\nfor the student network\ng(x, \u03b8\u2019)\nsuch that for all\nx, f(x, \u03b8) = g(x, \u03b8\u2019)\nTo start, we can assume that\nf\nand\ng\nare composed of standard linear layers. Layer\ni\nand\ni+1\nare represented by weights\nW\nmxn\ni\nand\nW\nnxp\ni+1\nWe want to grow layer\ni\nto have\nq\noutput units (where\nq\n>\nn\n) and layer\ni+1\nto have\nq\ninput units. The new weight matrix would be\nU\nmxq\ni\nand\nU\nqxp\ni+1\nThe first\nq\ncolumns (rows) of\nW\ni\n(\nW\ni+1\n) would be copied as it is into\nU\ni\n(\nU\ni+1\n).\nFor filling the remaining\nn-q\nslots, columns (rows) would be sampled randomly from\nW\ni\n(\nW\ni+1\n).\nFinally, each layer in\nU\ni\nis scaled by dividing by the corresponding replication factor to ensure that the output value of function remains unchanged by the operation.\nSince convolutions can be seen as multiplication by a double block circulant matrix, the approach can be readily extended for convolutional networks.\nThe benefits of using this approach are the following:\nThe newly created student network performs at least as good as the teacher network.\nAny changes to the network are guaranteed to be an improvement.\nIt is safe to optimize all the parameters in the network.\nThe variant discussed above is called the\nNet2WiderNet\nvariant. There is another variant called\nNet2DeeperNet\nthat enables the network to grow in depth.\nIn that case, a new matrix,\nU\n, initialized as the identity matrix, is added to the network. Note that unlike the\nNet2WiderNet\n, this approach would not work with arbitrary activation function between the layers.\nStrengths\nThe model can accelerate the training of neural networks, especially during development cycle when the designers try out different models.\nThe approach could potentially be used in life-long learning systems where the model is trained over a stream of data and needs to grow over time.\nLimitations\nThe function preserving transformations need to be worked out manually. Extra care needs to be taken when operations like concatenation or batch norm are present.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05641"
    },
    "91": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Learning-to-Count-Objects-in-Natural-Images-for-Visual-Question-Answering",
        "transcript": "Learning to Count Objects in Natural Images for Visual Question Answering \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nLearning to Count Objects in Natural Images for Visual Question Answering\n2018\n\u2022\nCount Based VQA\n\u2022\nICLR 2018\n\u2022\nAI\n\u2022\nCV\n\u2022\nICLR\n\u2022\nNLP\n\u2022\nVQA\n\u2022\nSOTA\n06 May 2018\nIntroduction\nMost of the visual question-answering (VQA) models perform poorly on the task of counting objects in an image. The main reasons are:\nMost VQA models use a soft attention mechanism to perform a weighted sum over the spatial features to obtain a single feature vector. These aggregated features helps in most category of questions but seems to hurt for counting based questions.\nFor the counting questions, we do not have a ground truth segmentation of where the objects to be counted are present on the image. This limits the scope of supervision.\nAdditionally, we need to ensure that any modification in the architecture, to enhance the performance on the counting questions, should not degrade the performance on other classes of questions.\nThe paper proposes to overcome these challenges by using the attention maps (and not the aggregated feature vectors) as input to a separate\ncount\nmodule.\nLink to the paper\nNotes\nThe basic idea is quite intuitive: when we perform weighted averaging based on different attention maps, we end up averaging the features corresponding to the difference instances of an object. This makes the feature vectors indistinguishable from the scenario where we had just one instance of the object in the image.\nEven multiple glimpses (multiple attention steps) can not resolve this problem as the weights given to one feature vector would not depend on the other feature vectors (that are attended to). Hard attention could be more useful than soft-attention but there is not much empirical evidence in support of this hypothesis.\nThe proposed\ncount\nmodule is a separate pipeline that can be integrated with most of the existing attention based VQA models without affecting the performance on non-count based questions.\nThe inputs to the\ncount\nmodule are the attention maps and the object proposals (coming from some pre-trained model like the RCNN model) and the output is an count-feature vector which is used to answer the count based question.\nThe top level idea is the following - given the object proposals and the attention maps, create a graph where nodes are objects (object proposals) and edges capture how similar two object proposals are (how much do they overlap). The graph is transformed (by removing and scaling edges) so that the count of the object can be obtained easily.\nTo explain their methodology, the paper simplifies the setting by making two assumptions:\nThe first assumption is that the attention weights are either 1 (when the object is present in the proposal) or 0 (when the object is absent from the proposal).\nThe second assumption is that any two object proposals either overlap completely (in which case, they are corresponding to the exact same object and hence receive the exact same weights) or the two proposals have zero overlap (in which case, they must be corresponding to completely different objects).\nThese simplifying assumptions are made only for the sake of exposition and do not limit the capabilities of the\ncount\nmodule.\nGiven the assumptions, the task of the count module is to handle the exact duplicates to prevent double-counting of objects.\nAs the first step, the attention weights (\na\n) are used to generate an attention matrix (\nA\n) by performing an outer product between\na\nand\na\nT\n. This corresponds to the step of creating a graph from the input.\nA\ncorresponds to the adjacency matrix of that graph. The attention weight for the\ni\nth\nproposal corresponds to the\ni\nth\nnode in the graph and the edge between the nodes\ni\nand\nj\nhas the weight\na\ni\n*a\nj\n.\nAlso note that the graph is a weighted directed graph and the subgraph of vertices satisfying the condition\na\ni\n= 1 is a complete directed graph with self-loops. Given such a graph, the number of vertices,\nV = sqrt(E)\nwhere\nE\ncould be computed by summing over the adjacency matrix.This implies that if the proposals are distinct, then the count can be obtained trivially by performing a sum over the adjacency matrix.\nThe objective is now to eliminate the edges such that the underlying objects are the vertices of a complete subgraph. This requires removing two type of duplicate edges - intra-object edges and inter-object edges.\nIntra-object edges can be removed by computing a distance matrix,\nD\n, defined as 1 - IoU, where IoU matrix corresponds to the Intersection-over-Union matrix. A modified adjacency matrix\nA\u2019\nis obtained by performing the element-wise product between f\n1\n(\nA\n) and f\n2\n(\nD\n) where f\n1\nand f\n2\nare piece-wise linear functions that are learnt via backpropogation.\nThe inter-object edges are removed in the following manner:\nCount the number of proposals that correspond of each instance of an object and then scale down the edges corresponding to the different instances by that number.\nThis creates the effect of reducing the weights of multiple proposals equivalent to a single proposal.\nThe number of proposals corresponding to an object is not available as an annotation in the training pipeline and is estimated based on the similarity between the different proposals (measured via the attention weights\na\n, adjacency matrix\nA\nand distance matrix\nD\n).\nThe matrix corresponding to the similarity between proposals  (\nsim\ni, j\n) is transformed into a vector corresponding to the scaling factor of each node (\ns\ni\n)\ns\ncan be converted into a matrix (by doing outer-product with itself) so as to scale both the incoming and the outgoing edges. The self edges (which were removed while computing\nA\u2019\nare added back (after scaling with\ns\n) to obtain a new transformed matrix\nC\n.\nThe transformed matrix\nC\nis a complete graph with self-loops where the nodes corresponds to all the relevant object instances and not to object proposals. The actual count can be obtained from\nC\nby performing a sum over all its values as described earlier. The original count problem was a regression problem but it is transformed into a classification problem to avoid scale issues. The network produces a\nk\n-hot\nn\n-dimensional vector called\no\nwhere\nn\nis the number of object proposals that were feed into the module (and hence the upper limit on upto how large a number could the module count). In the ideal setting,\nk\nshould be one, as the network would produce an integer value but in practice, the network produces a real number so\nk\ncan be upto 2. If\nc\nis an exact integer, the output is a 1-hot vector with the value in index corresponding to\nc\nset to 1. If\nc\nis a real number, the output is a linear interpolation between two one-hot vectors (the one-hot vectors correspond to the two integers between  which\nc\nlies).\ncount\nmodule supports computing the confidence of a prediction by defining two variables p\na\nand p\nD\nwhich compute the average distance of f\n6\n(\na\n) and $f\n7\n(\nD\n) from 0.5. The final output\no\u2019\nis defined as f\n8\n(p\na\n+ p\nD\n) .\no\nAll the different f functions are piece wise linear functions and are learnt via backpropagation.\nExperiments\nThe authors created a new category of count-based questions by filtering the number-type questions to remove questions like \u201cWhat is the time right now\u201d. These questions do have a neumerical answer but do not fall under the purview of count based questions and hence are not targeted by the\ncount\nmodel.\nThe authors augmented a state of the art\nVQA model\nwith their\ncount\nmodule and show substantial gains over the count-type questions for the\nVQA-v2 dataset\n. This augmentation does not drastically impact the performance on non-count questions.\nThe overall idea is quite crisp and intutive and the paper is easy to follow. It would be even better if there were some more abalation studies. For example, why are the piece-wise linear functions assumed to have 16 linear components? Would a smaller or larger number be better?\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.05766"
    },
    "92": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Neural-Message-Passing-for-Quantum-Chemistry",
        "transcript": "Neural Message Passing for Quantum Chemistry \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nNeural Message Passing for Quantum Chemistry\n2017\n\u2022\nMessage Passing\n\u2022\nNeural Message Passing\n\u2022\nGraph Representation\n\u2022\nAI\n\u2022\nChemistry\n\u2022\nGraph\n\u2022\nMPNN\n08 Apr 2018\nIntroduction\nThe paper presents a general message passing architecture called as Message Passing Neural Networks (MPNNs) that unify various existing models for performing supervised learning on molecules.\nVariants of the MPNN model achieve very good performance on the task of predicting the property of the molecules.\nLink to the paper\nMPNN\nSetting\nThe input to the model is an undirected graph\nG\nwhere node features are represented as\nx\nv\n(corresponding to node\nv\n) and edge features are\ne\nv, w\n(corresponding to edge between nodes\nv, w\n).\nThe idea is to learn a representation (or feature vector) for all the nodes (and possibly edges) in the graph and use that for the downstream supervised learning task.\nThe model can be easily extended to the setting of directed graphs.\nThe model works in 2 phases:\nMessage Passing Phase\nAll nodes send a\nmessage\nto their neighbouring nodes. The message is a function of the feature vectors corresponding to the sender node (or vertex), the receiver node and the edge connecting the two nodes. The feature vectors can be combined to form the message using the\nmessage function\nwhich can be implemented as a neural network.\nOnce a node has received messages from all its neighbours, it updated its feature vector by aggregating all the message. The function used to aggregate and update the feature vector is called as the\nupdate function\nand can be implemented as a neural network.\nAfter updating the feature vectors, the graph could initiate another round of message passing. After a sufficient number of message passing rounds, the Readout phase is invoked.\nReadout Phase\nThe feature vectors corresponding to different nodes in the graph are aggregated into a single feature vector (corresponding to the feature vector of the graph) using the\nreadout function\n.\nThe\nreadout function\ncan also be implemented using a neural network with the condition that it is invariant to the permutation of the nodes within the graph (to ensure that the MPNN is independent of the graph isomorphism).\nExisting Variants in literature\nThe paper provides various examples where the existing architectures could be explained in terms of the message passing framework. This includes examples like\nConvolutional Networks on Graphs for Learning Molecular Fingerprints\n,\nGated Graph Sequence Neural Networks\n,\nGraph Convolutional Networks\netc.\nExperiments\nSetup\nBroadly speaking, the task is to predict the properties of given molecules (regression problem).\nThe QM9 dataset consists of 130K molecules whose properties have been measured using Quantum Mechanical Simulations (DFT).\nProperties to be predicted include atomization energy, enthalpy, highest fundamental vibrational frequency etc.\nThere are two benchmarks for error:\nDFT Error - Estimated average error of DFT approximation\nChemical Accuracy - As established by the chemistry community\nModel\nFollowing variants of\nmessage function\nare explored:\nMatrix multiplication between\nA\nevw\nand\nh\nv\nwhere\nA\nis the adjacency matrix\nh\nv\nis the feature corresponding to node\nv\n.\nEdge Network which is same as matrix multiplication case with the difference that\nA\nis a learned matrix for each edge type.\nPair Network where the feature vector corresponding to the source node, target node and edge is fed to a neural network.\nVirtual Elements\nSince all messages are shared via edges, it could take a long time for the message to move between two ends of the graph. To fasten this process, virtual elements are provided.\nIn the first setting, \u201cvirtual edges\u201d are inserted between nodes.\nIn the second setting, a \u201cmaster\u201d node connects to all the other nodes.\nMessage Passing Complexity\nIn a graph with\nn\nnodes and\nd\ndimensional feature vectors, a single step of message passing would have the worst case time complexity of\nO(n\n2\nd\n2\n.\nThis complexity can be reduced by breaking the\nd\ndimensional embedding into\nk\ndifferent groups of\nd/k\nembeddings which can be updated in parallel. The complexity of the modified approach is\nO(n\n2\nd\n2\n/k\n.\nResults\nBest performing MPNN model uses edge network as the\nmessage function\nand\nset2set\nas the\nreadout function\n.\nUsing group of embeddings helps to improve generalization. This effect could also be because of ensemble-like nature of the modified architecture.\nThe model performs worse without the virtual elements.\nTakeaways\nLong range interaction between vertices is necessary.\nScaling to larger molecule sizes is challenging because the model creates a fully connected graph by incorporating virtual elements.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.01212"
    },
    "93": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Unsupervised-Learning-By-Predicting-Noise",
        "transcript": "Unsupervised Learning by Predicting Noise \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nUnsupervised Learning by Predicting Noise\n2017\n\u2022\nAI\n\u2022\nCV\n\u2022\nEmbedding\n\u2022\nUnsupervised\n02 Apr 2018\nIntroduction\nConvolutional Neural Networks are extremely good feature extractors in the sense that features extracted for one task (say image classification) can be easily transferred to another task (say image segmentation).\nExisting unsupervised approaches do not aim to learn discriminative features and supervised approaches for discriminative features do not scale well.\nThe paper presents an approach to learn features in an unsupervised setting by using a set of target representations called as Noise As Target (NAT) which acts as a kind of proxy supervising signal.\nLink to the paper\nApproach\nUnsupervised Setting\nGiven a collection of image X (x\n1\n, x\n2\n, \u2026, x\nn\n), we want to learn a parameterized mapping\nf\nsuch that\nf(x\ni\n)\ngives the features of image\nx\ni\n. We would jointly learn the target vectors\ny\ni\n(more on it later).\nLoss Function\nSquared L2 norm is used as the distance measure while making sure that final activations are unit normalized.\nFixed Target Representation\nIn the setting of the problem where we are learning both the features and the target representation, a trivial solution would be the one where all the input images map to the same target and are assigned the same representation. No discriminative features are learned in this case.\nTo avoid such situations, a set of k predefined target representations are chosen and each image is mapped to one of these k representations (based on the features).\nThere is an assumption that k > n so that each image is assigned a different target.\nOne simple choice of target representation is the standard one-hot vector which implies that all the class (and by extension, the associated images) are orthogonal and equidistant from each other. But this is not a reasonable approximation as not all the image pairs are equally similar or dissimilar.\nInstead, the target vectors are uniformly sampled from a d-dimensional unit sphere, where d is the dimensionality of the feature representation. That is, the idea is to map the features to the manifold of the d-dimensional L2 sphere by using the K predefined representations as for the discrete approximation of the manifold.\nSince each data point (image) is mapped to a new point on the manifold, the algorithm is suited for online training as well.\nOptimisation\nFor the training, the number of target K is reduced to the number of images n and an assignment matrix P is learned which ensures that the mapping between the image to target is 1-to-1.\nThe resulting optimisation equation can be solved using the Hungarian Algorithm but at a high-cost O(n^3). An optimisation is to take a batch of b images and update the square matrix P\nB\nfor dimension bXb (made of the images and their corresponding targets). This reduces the overall complexity of O(nb^2).\nOther optimisation techniques, that are common to supervised learning, like batch norm used in this setting as well.\nImplementation Detail\nUsed AlexNet with NATs to train the unsupervised model.\nAn MLP is trained on these features to learn the classifier.\nStandard preprocessing techniques like random cropping/flipping are used.\nExperimental Details\nDataset\nImageNet for training the AlexNet architecture with the proposed approach.\nPascal VOC 2007 for transfer learning experiments.\nBaselines\nUnsupervised approaches like autoencoder, GAN, BiGAN\nSelf-supervised\nSOTA models using hand-made features SIFT with Fisher Vector.\nObservation\nUsing squared loss instead of softmax does not deteriorate the performance too much.\nThe authors compare the effect of using discrete vs continuous target representations for transfer learning. For the discrete representation, elements of the canonical basis of a k-dimensional space (k=1000, 10000, 100000) are used. Experiments demonstrate that d-dimensional continuous vectors perform much better than the discrete vectors.\nWhile training the unsupervised network, its features were extracted after every 20 iterations to evaluate the performance on transfer learning task. The test accuracy increases up to around 100 iterations then saturate.\nComparing the visualization of the first convolutional layer filters (for AlexNet with and without supervision) shows that while unsupervised filters are less sharp, they maintain the edge and orientation information.\nThe proposed unsupervised method outperforms all the unsupervised baselines and is competitive with respect to the supervised baseline. But it is still far behind the model using handcrafted features.\nFor transfer learning, on Pascal VOC, the proposed approach beats the supervised baseline and works at par with the supervised approach.\nNotes\nThe paper proposed a simple unsupervised framework for learning discriminative features without having to rely on proxy tasks like image generation and without having to make an assumption about the input domain.\nThe key aspect of the proposed approach is that each image is assigned to a unique point in the d-dimensional manifold which means 2 images could be very close to each other on the manifold while being quite distinct in reality. It is interesting to see that such a simple strategy is able to give such good results.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.05310"
    },
    "94": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/The-Lottery-Ticket-Hypothesis-Training-Pruned-Neural-Networks",
        "transcript": "The Lottery Ticket Hypothesis - Training Pruned Neural Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nThe Lottery Ticket Hypothesis - Training Pruned Neural Networks\n2018\n\u2022\nPruning Network\n\u2022\nAI\n\u2022\nHypothesis\n\u2022\nInitialization\n25 Mar 2018\nIntroduction\nEmpirical evidence indicates that at training time, the neural networks need to be of significantly larger size than necessary.\nThe paper purposes a hypothesis called the\nlottery ticket hypothesis\nto explain this behaviour.\nThe idea is the following - Successful training of a neural network depends on a\nlucky\nrandom initialization of a subcomponent of the network. Such components are referred to as\nlottery tickets\n.\nLarger networks are more likely to have these\nlottery tickets\nand hence are easier to train.\nLink to the paper\nMethodology\nVarious aspects of the hypothesis are explored empirically.\nTwo tasks are considered - MNIST and XOR.\nFor each task, the paper considers networks of different sizes and empirically shows that larger networks are more likely to converge (or have better performance) for a fixed number of epochs as compared to the smaller networks.\nGiven a large, trained network, some weights (or units) of the network are pruned and the resulting network is reset to its initial random weights.\nThe resulting network is the\nlottery-ticket\nin the sense that when the pruned network is trained, it is more likely to converge than an otherwise randomly initialised network of the same size. Further, it is more likely to match the original, larger network in terms of performance.\nThe paper explores different aspects of this experiment:\nPruning Strategies:\nOne-shot strategy prunes the network in one-go while the iterative strategy prunes the network iteratively.\nThough the latter is computationally more intensive, it is more likely to find a lottery ticket.\nSize of the pruned network affects the speed of convergence when training the\nlottery ticket\n.\nIf only the architecture or only the initial weights of the\nlottery ticket\nare used, the resulting network tends to converge more slowly and achieves a lower level of performance.\nThis indicates that the lottery ticket depends on both the network architecture and the weight initialization.\nDiscussion\nThe paper includes some more interesting experiments. For instance, the distribution of the initialization in the weights that survived the pruning suggests that small weights from before training tend to remain small after training.\nOne interesting experiment would be to show the performance of the pruned network before resetting its weights and retraining again. This performance should be compared with the performance of the initial large network and the performance of the\nlottery ticket\nafter training.\nOverall, the experiments are not sufficient to conclude anything about the correctness of the hypothesis. The proposition itself is very interesting and could enhance our understanding of how the neural networks work.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1803.03635"
    },
    "95": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Cyclical-Learning-Rates-for-Training-Neural-Networks",
        "transcript": "Cyclical Learning Rates for Training Neural Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nCyclical Learning Rates for Training Neural Networks\n2017\n\u2022\nAccelerated Training\n\u2022\nLearning Rate\n\u2022\nWACV 2017\n\u2022\nAI\n\u2022\nLR\n\u2022\nWACV\n18 Mar 2018\nIntroduction\nConventional wisdom says that when training neural networks, learning rate should monotonically decrease. This insight forms the basis of the different type of adaptive learning rates.\nCounter to this expected behaviour, the paper demonstrates that using a cyclical learning rate (CLR), varying between a minimum and a maximum value, helps to train the neural network faster without requiring fine-tuning of learning rate.\nThe paper also provides a simple approach to estimate the lower and upper bound for CLR.\nLink to the paper\nLink to the implementation\nIntution\nDifficulty in minimizing the loss arises from saddle points and not from local minima.\n[Ref]\nIncreasing the learning rate allows for rapid traversal of saddle points.\nAlternatively, the optimal learning rate is expected to be between bounds of CLR and thus the learning rate would always be close to the optimal learning rate.\nParameter Estimation\nCycle Length = Number of iterations till learning rate returns to the initial value = 2 * step_size\nstep_size should be set to 2-10 times the number of iterations in an epoch.\nEstimating the CLR boundary values:\nRun the model for several epochs while increasing the learning rate between the allowed low and high values.\nPlot accuracy vs learning rate and note the learning rate values when the accuracy starts to fall.\nThis gives a good candidate value for upper and lower bound. Alternatively, the lower bound could be set to be 1/3 or 3/4 of the upper bound. But it is difficult to judge if the model has run for the sufficient number of epochs in the first place.\nNotes\nThe idea in itself is very simple and straight-forward to add to any existing model which makes it very appealing.\nThe author has experimented with various architectures and datasets (from vision domain) and has reported faster training results.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.01186"
    },
    "96": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Improving-Information-Extraction-by-Acquiring-External-Evidence-with-Reinforcement-Learning",
        "transcript": "Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nImproving Information Extraction by Acquiring External Evidence with Reinforcement Learning\n2016\n\u2022\nEMNLP 2016\n\u2022\nInformation Retrieval\n\u2022\nAI\n\u2022\nEMNLP\n\u2022\nNLP\n\u2022\nRL\n11 Mar 2018\nIntroduction\nInformation Extraction  - Given a query to be answered and an external search engine, information extraction entails the task of issuing search queries, extracting information from new sources and reconciling the extracted values till we are sufficiently confident about the extracted values.\nThe paper proposes the use of Reinforcement Learning (RL) to solve this task.\nLink to the paper\nImplementation\nKey Aspect\nUse of Reinforcement Learning to resolve the ambiguity inherent in the textual documents.\nGiven a query, the RL agent would use template statement to formulate the queries (to be performed on the black box search engine). It would further resolve and combine the result for the query from the set of retrieved documents.\nDatasets\nDatabase of Mass Shootings in the United States.\nFood Shield database of illegal food adulteration.\nFramework\nInformation extraction task is modelled as a Markov Decision Process (MDP) <S, A, T, R>\nS\n- Set of all possible states\nThe state consists of:\nExtractor\u2019s confidence in predicted entity values.\nContext from which values are extracted.\nSimilarity between the new document (extracted just now from the search engine) and the original document accompanying the given query.\nA\n- Set of all possible actions\nReconciliation decision - d\nAccept all entities values.\nReject all entities values.\nStop the current episode.\nQuery choice - q\nChoose the next query from a set of automatically generated alternatives.\nR\n- Rewards\nMaximise the final extraction accuracy while minimising the number of queries.\nQ\n- Queries\nGenerated using a template.\nThe query is searched on a search engine and the top k links are retrieved.\nTransition\nStart with a single source article x\ni\nand extract the initial set of entities.\nAt each timestep, the agent is given the state (s) on basis of which it chooses the action (d, q). The episode stops whenever the action is a stop action.\nDeep Q Network is used.\nParameters are learned using SGD and RMSProp.\nExperimental Setup\nExtraction Model\nMax Entropy Classifier is used as the base extraction system.\nFirst, all the words in the document are tagged as one of the entity types and the mode of these values is used to obtain the set of extracted entities.\nBaseline\nBasic Extractors\nAggregation System which either chooses the entity value with the highest confidence or takes a majority vote over all extracted values.\nMeta-Classifier which operates over the same input state space and produces the same set of reconciliation decisions as the DQN.\nOracle Extractor which is computed assuming perfect reconciliation and query decisions on the top of the Maxnet base extractor.\nRL Models\nRL Basic - Only reconciliation decision.\nRL Query - Only query decision with a fixed reconciliation strategy.\nRL Extract - the full system with both reconciliation and query decision.\nResult\nRL Extract obtains substantial gains eg up to 11% over Maxnet.\nSimple aggregation schemes do not handle the task well.\nIn terms of reward structure, providing rewards after each step works better than a single delayed reward.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.07954"
    },
    "97": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/An-Empirical-Investigation-of-Catastrophic-Forgetting-in-Gradient-Based-Neural-Networks",
        "transcript": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nAn Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks\n2014\n\u2022\nCatastrophic Forgetting\n\u2022\nICLR 2014\n\u2022\nAI\n\u2022\nActivation\n\u2022\nICLR\n05 Mar 2018\nIntroduction\nCatastrophic Forgetting\nrefers to the phenomenon where when a learning system is trained on two tasks in succession, it may forget how to perform the first task.\nThe paper investigates this behaviour for different learning activations in presence and absence of dropout.\nLink to the paper\nLink to the implementation\nExperiment Formulation\nFor each experiment, two tasks are defined - \u201cold\u201d task and \u201cnew\u201d task.\nThe network is first trained on the \u201cold\u201d task until the validation set error has not improved for the last 100 epochs.\nThe \u201cbest\u201d performing model is then trained for the \u201cnew\u201d task until the combined error on the \u201cold\u201d and the \u201cnew\u201d validation datasets has not improved in the last 100 epochs.\nAll the tasks used the same model architecture - 2 hidden layers followed by a softmax layer.\nFollowing activations were tested:\nSigmoid\nReLU\nHard Local Winner Takes It All\nMaxout\nModels were trained using SGD with or without dropout.\nFor each combination of the model, activation and the training mechanism, a random hyper param search was performed with set of 25 hyperparams.\nThe authors took care to keep the hyperparams and other settings consistent and comparable across different experiments. Deviations, wherever applicable, and their reasons were documented.\nObservations\nIn terms of the relationship between the \u201cold\u201d and the \u201cnew\u201d tasks, three kinds of settings are considered:\nThe tasks are very very similar but the input is processed in a different format. For this setting, MNIST dataset was used with a different permutation of pixels for the \u201cold\u201d and the \u201cnew\u201d task.\nThe tasks are similar but not exactly the same. For this setting, the task was to predict sentiments of reviews across 2 different product categories.\nIn the last setting, 2 dissimilar tasks were used. One task was to predict sentiment of reviews and another task was to perform classification over MNIST dataset (reduced to 2 classes).\nUsing Dropout improved the overall validation performance for all the models for all the tasks.\nUsing Dropout also increase the size of the optimal model across all the activations indicating that maybe the increased size of the model could explain the increased resistance to forgetting. It would have been interesting to check if dropout always selected the largest model possible given the set of the hyperparams.\nOn the dissimilar task, dropout improved the performance while reducing the model size so it might have other properties as well that helps to prevent forgetting.\nAs compared to the choice of training technique, the activation function has a less consistent effect on resistance to forgetting. The paper recommends performing cross-validation for the choice of the activation function. If that is not feasible, maxout activation function with dropout could be used.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1312.6211"
    },
    "98": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Learning-a-SAT-Solver-from-Single-Bit-Supervision",
        "transcript": "Learning an SAT Solver from Single-Bit Supervision \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nLearning an SAT Solver from Single-Bit Supervision\n2018\n\u2022\nGraph Neural Network\n\u2022\nAI\n\u2022\nGraph\n\u2022\nGNN\n\u2022\nSAT\n24 Feb 2018\nIntroduction\nThe paper presents NeuroSAT, a message passing neural network that is trained to predict if a given SAT can be solved. As a side effect of training, the model also learns how to solve the SAT problem itself without any extra supervision.\nLink to the paper\nBackground\nGiven an expression in the propositional logic, the task is to predict if there exists a substitution of variables that make the expression true.\nThe expression itself can be written as a conjunction of disjunctions (\u201cand\u201d over \u201cor\u201d) where each conjunct is called a clause and each variable within a clause is called a literal.\nInvariants\nThe variables or clauses or literals (within the clauses) can be permuted.\nEvery occurrence of a variable can be negated.\nModel\nGiven the SAT problem,  create an undirected graph of literals, their negations and the clauses they belong to.\nPut an edge between every literal and the clause to which it belongs and another kind of edge between every literal and its negation.\nPerform message passing between nodes to obtain vector representations corresponding to each node. Specifically, first, each clause received a message from its neighbours (literals) and updates its embeddings. Then every literal receives a message from its neighbours (both literals and clauses) and updates its embeddings.\nAfter T iterations, the nodes vote to decide the prediction of the model as a whole.\nThe model is trained end-to-end using the cross-entropy loss between logit and the true label.\nPermutation invariance is ensured by operating on the nodes and the edges in the topological order and negation invariance is ensured by treating all literals as the same.\nDecoding Satisfying Assignment\nThe most interesting aspect of this work is that even though the model was trained to predict if the SAT problem can be satisfied, it is actually possible to extract the correct assignment from the classifier.\nIn the early iterations, all the nodes vote \u201cunsolvable\u201d with low confidence. Then a few nodes start voting \u201csolvable\u201d and then a phase transition happens where most of the nodes start voting \u201csolvable\u201d with high confidence.\nThe model never becomes highly confident that problem is \u201cunsolvable\u201d and almost never guesses \u201csolvable\u201d on an \u201cunsolvable\u201d problem. So in some sense, the model is looking for the combination of literals that actually solves the problem.\nThe authors found that the 2 dimensional PCA projections of the literal embeddings are initially mixed up but become more and more linearly separable as the phase transition happens.\nBased on this insight, the authors propose to obtain cluster centres C1 and C2, partition the variables according to the cluster centres and then try assignments from both the partitions.\nThis alone provides a satisfying solution in over 70% of the cases when though there is no explicit supervising signal about how to solve the problem.\nThe other strengths of the paper includes\nGeneralizing to longer and more difficult SAT problems (than those seen during training).\nGeneralizing to another kind of search problems like graph colouring, clique detection etc (over small random graphs).\nThe paper also reports that by adding supervising signal about which clauses in the given expression are unsatisfiable, it is possible to decode the literals which prove the \u201cunsatisfiability\u201d of an expression at test time. Though not a lot of details have been provided about this part and would probably be covered in the next iteration of the paper.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.03685"
    },
    "99": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Neural-Relational-Inference-for-Interacting-Systems",
        "transcript": "Neural Relational Inference for Interacting Systems \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nNeural Relational Inference for Interacting Systems\n2018\n\u2022\nDynamical System\n\u2022\nGraph Representation\n\u2022\nLatent Variable\n\u2022\nRelational Inference\n\u2022\nAI\n\u2022\nGraph\n\u2022\nGNN\n\u2022\nVAE\n17 Feb 2018\nIntroduction\nThe paper presents Neural Relational Inference (NRI) model which can infer underlying interactions in a dynamical system in an unsupervised manner, using just the observational data in terms of the trajectories.\nFor instance, consider a simulated system where the particles are connected to each other by springs. The observational data does not explicitly specify which particles are connected to each other and only contains information like position and velocity of each particle at different timesteps.\nThe task is to explicitly infer the interaction structure (in this example, which pair of particles are connected to each other) while learning the dynamical model of the system itself.\nLink to the paper\nLink to the implementation\nModel\nThe model consists of an encoder that encodes the given trajectories into an interaction graph and a decoder that decodes the dynamical model given the interaction graph.\nThe model starts by assuming that a full connected interaction graph exists between the objects in the system.\nFor this latent graph\nz\n,\nz\ni, j\ndenotes the (discrete) edge type between object\nv\ni\nand\nv\nj\nwith the assumption that there are\nK\nedge types.\nThe object\nv\ni\nhas a feature vector\nx\ni\nt\nassociated with it at time\nt\n. This feature vector captures information like location and velocity.\nEncoder\nA Graph Neural Network (GNN) acts on the fully connected latent graph\nz\n, performs message passing from node to node via edges and predicts the discrete label for each edge.\nThe GNN architecture may itself use MLPs or ConvNets and returns a factorised distribution over the edge types\nq\n\u03c6\n(z|x)\n.\nDecoder\nThe decoder is another GNN (with separate params for each edge type) that predicts the future dynamics of the system and returns\np\n\u03b8\n(x|z)\n.\nThe overall model is a VAE that optimizes the ELBO given as:\nE\nq\n\u03c6\n(z|x)\n[log p\n\u03b8\n(x|z)] \u2212 KL[q\n\u03c6\n(z|x)||p\n\u03b8\n(z)]\np\n\u03b8\n(x)\nis the prior which is assumed to be uniform distribution over the edge types.\nInstead of predicting the dynamics of the system for just the next timestep, the paper chooses to use the prediction multiple steps (10) in the future. This ensures that the interactions can have a significant effect on the dynamics of the system.\nIn some cases, like real humans playing a physical sport, the dynamics of the system need not be Markovian and a recurrent decoder is used to model the time dependence.\nPipeline\nGiven the dynamical system, run the encoder to obtain\nq\n\u03c6\n(z|x)\n.\nSample\nz\ni, j\nfrom\nq\n\u03c6\n(z|x)\n.\nRun the decoder to predict the future dynamics for the next T timesteps.\nOptimise the ELBO loss.\nNote that since the latent variables (edge labels) are discrete in this case, the sampling is done from a continuous approximation of the discrete distribution and reparameterization trick is applied over this discrete approximation to get the (biased) gradients.\nObservations\nExperiments are performed using simulated systems like particles connected to springs, phase coupled oscillators and charged particles and using real-world data like CMU Motion Capture database and NBA tracking data.\nThe NRI system effectively predicts the dynamics of the systems and is able to reconstruct the ground truth interaction graph (for simulated systems).\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.04687"
    },
    "100": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Get-To-The-Point-Summarization-with-Pointer-Generator-Networks",
        "transcript": "Get To The Point - Summarization with Pointer-Generator Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nGet To The Point - Summarization with Pointer-Generator Networks\n2017\n\u2022\nACL 2017\n\u2022\nAbstract Summarization\n\u2022\nPointer Network\n\u2022\nACL\n\u2022\nAI\n\u2022\nNLP\n\u2022\nSOTA\n\u2022\nSummarization\n05 Feb 2018\nIntroduction\nSequence-to-Sequence models\nhave made abstract summarization viable but they still suffer from issues like\nout of vocabulary\nwords and repetitive sentences.\nThe paper proposes to overcome these limitations by using a hybrid Pointer-Generator network (to copy words from the source text) and a\ncoverage\nvector that keeps track of content that has already been summarized so as to discourage repetition.\nLink to the paper\nCode\nModel\nPointer Generator Network\nIt is a hybrid model between the Sequence-to-Sequence network and\nPointer Network\nsuch that when generating a word, the model decides whether the word would be generated using the softmax vocabulary (Sequence-to-Sequence) or using the source vocabulary (Pointer Network).\nSince the model can choose a word from the source vocabulary, the issue of\nout of vocabulary\nwords is handled.\nCoverage Mechanism\nThe model maintains a\ncoverage\nvector which is the sum of attention distributions over all previous decoder timesteps.\nThis\ncoverage\nvector is fed as an input to the attention mechanism.\nA\ncoverage loss\nis added to prevent the model from repeatedly attending to the same word.\nThe idea is to capture how much coverage different words have already received from the attention mechanism.\nObservation\nModel when evaluated on CNN/Daily Mail summarization task, outperforms the state-of-the-art by at least 2 ROUGE points though it still does not outperform the lead-3 baseline.\nLead-3 baseline uses first 3 sentences as the summary of the article which should be a strong baseline given that the dataset is actually about news articles.\nThe model is initially trained without coverage and then finetuned with the coverage loss.\nDuring training, the model first learns how to copy words and then how to generate words (p\ngen\nstarts from 0.3 and converges to 0.53).\nDuring testing, the model strongly prefers copying over generating (p\ngen\n= 0.17).\nFurther, whenever the model is at beginning of sentences or at the join between switched-together fragments, it prefers to generate a word instead of copying one from the source language.\nThe overall model is very simple, neat and interpretable and also performs well in practice.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.04368"
    },
    "101": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/StarSpace-Embed-All-The-Things",
        "transcript": "StarSpace - Embed All The Things! \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nStarSpace - Embed All The Things!\n2017\n\u2022\nGraph Representation\n\u2022\nMulti Task\n\u2022\nWord Vectors\n\u2022\nRepresentation Learning\n\u2022\nEmbedding\n\u2022\nGraph\n\u2022\nNLP\n29 Jan 2018\nIntroduction\nThe paper describes a general purpose neural embedding model where different type of entities (described in terms of discrete features) are embedded in a common vector space.\nA similarity function is learnt to compare these entities in a meaningful way and score their similarity. The definition of the similarity function could depend on the downstream task where the embeddings are used.\nLink to the paper\nLink to the implementation\nApproach\nEach entity is described as a set of discrete features. For example, for the recommendation use case, the users may be described as a bag-of-words of movies they have liked. For the search use case, the document may be described as a bag-of-words of words they are made up of.\nGiven a dataset and a task at hand, generate a set of positive samples\nE = (a, b)\nsuch that\na\nis the input to the task (from the dataset) and\nb\nis the expected label(answer/entity) for the given task.\nSimilarly, generate another set of negative samples\nE\n-\n= (a, b\ni\n-\n)\nsuch that\nb\ni\n-\nis one of the incorrect label(answer/entity) for the given task. The incorrect entity can be sampled randomly from the set of candidate entities. Multiple incorrect samples could be generated for each positive example. These incorrect samples are indexed using\ni\n.\nFor example, in case of supervised learning problem like document classification,\na\nwould be one of the documents (probably described in terms of words),\nb\nis the correct label and\nb\ni\n-\n)\nis one of the randomly sampled label from set of all the labels (excluding the correct label).\nIn case of collaborative filtering,\na\nwould be the user (either described as a discrete entity like a userid or in terms of items purchased so far),\nb\nis the next item the user purchases and\nb\ni\n-\n)\nis one of the randomly sampled item from the set of all the items.\nA similarity function is chosen to compare the representation of entities of type\na\nand\nb\n. The paper considered cosine similarity and inner product and observed that cosine similarity works better for the case with a large number of entities.\nA loss function compares the similarity between positive pairs\n(a, b)\nand\n(a, b\ni\n-\n)\n. The paper considered margin ranking loss and negative log loss of softmax and reported that margin ranking loss works better.\nThe norm of embeddings is capped at 1.\nObservations\nThe same model architecture is applied to a variety of tasks including multi-class classification, multi-label classification, collaborative filtering, content-based recommendation, link prediction, information retrieval, word embeddings and sentence embeddings.\nThe model provides a strong baseline on all the tasks and performs at par with much more complicated and task-specific networks.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1709.03856"
    },
    "102": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Emotional-Chatting-Machine-Emotional-Conversation-Generation-with-Internal-and-External-Memory",
        "transcript": "Emotional Chatting Machine - Emotional Conversation Generation with Internal and External Memory \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nEmotional Chatting Machine - Emotional Conversation Generation with Internal and External Memory\n2017\n\u2022\nAI\n\u2022\nConversational Agent\n\u2022\nNLP\n22 Jan 2018\nThe paper proposes ECM (Emotional Chatting Machine) which can generate both semantically and emotionally appropriate responses in a dialogue setting.\nMore specifically, given an input utterance or dialogue and the desired emotional category of the response, ECM is to generate an appropriate response that conforms to the given emotional category.\nLink to the paper\nMuch of the recent, deep learning based work on conversational agents has focused on the use of encoder-decoder framework where the input utterance (given sequence of words) is mapped to a response utterance (target sequence of words). This is the so-called seq2seq family of models.\nECM model can sit within this framework and introduces 3 new components:\nEmotion Category Embedding\nEmbed the emotion categories into a real-valued, low-dimensional vector space.\nThese embeddings are used as input to the decoder and are learnt along with rest of the model.\nInternal Memory\nPhysiological, emotional responses are relatively short-lived and involve changes.\nECM accounts for this effect by adding an Internal Memory which captures this dynamics of emotions during decoding.\nIt starts with \u201cfull\u201d emotions in the beginning and keeps decaying the emotion value over time.\nHow much of the emotion value is to be decayed is determined by a sigmoid gate.\nBy the time the sentence is decoded, the value becomes zero, signifying that the emotion has been completely expressed.\nExternal Memory\nEmotional responses are expected to carry emotionally strong words along with generic, neutral words.\nAn external memory is used to include the emotionally strong words explicitly by using 2 non-overlapping vocabularies -\ngeneric\nvocabulary and the\nemotion\nvocabulary (read from the external memory).\nBoth these vocabularies are assigned different generation probabilities and an output gate controls the weights of\ngeneric\nand\nemotion\nwords.\nThis way the\nemotion\nwords are included in an otherwise neutral response.\nLoss function\nThe first component is the cross-entropy loss between predicted and target token distribution.\nA regularization term on internal memory to make sure the emotional state decays to 0 at the end of the decoding process.\nAnother regularization term on external memory to supervise the probability of selection of a\ngeneric\nvs\nemotion\nword.\n*Dataset\nSTC Dataset (~220K posts and ~4300K responses) annotated by the emotional classifier. Any error on the part of the classifier degrades the quality of the training dataset.\nNLPCC Dataset - Emotion classification dataset with 23105 sentences.\nMetric\nPerplexity to evaluate the model at the content level.\nEmotion accuracy to evaluate the model at the emotional level.\nECM achieves a perplexity of 65.9 and emotional accuracy of 0.773.\nBased on human evaluations, ECM statistically outperforms the seq2seq baselines on both naturalness (likeliness of response being generated by a human) and emotion accuracy.\nNotes\nIt is an interesting idea to let the sigmoid gate decide how the emotion \u201cvalue\u201d be spent while decoding. It seems similar to the idea of how much do we want to \u201cattend\u201d to the emotion value the key difference being that your total attention is limited. It would be interesting to see the shape of the distribution of how much of the emotion value is spent at each decoding time step. If the curve is highly biased towards say using most of the emotion value towards the end of the decoding process, maybe another regularisation term is needed to ensure a more balanced distribution of how the emotion is spent.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.01074"
    },
    "103": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Exploring-Models-and-Data-for-Image-Question-Answering",
        "transcript": "Exploring Models and Data for Image Question Answering \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nExploring Models and Data for Image Question Answering\n2015\n\u2022\nNIPS 2015\n\u2022\nAI\n\u2022\nCV\n\u2022\nDataset\n\u2022\nNIPS\n\u2022\nNLP\n\u2022\nVQA\n14 Jan 2018\nIntroduction\nProblem Statement\n: Given an image, answer a given question about the image.\nLink to the paper\nAssumptions\n:\nThe answer is assumed to be a single word thereby bypassing the evaluation issues of multi-word generation tasks.\nVIS-LSTM Model\nTreat the input image as the first word in the question.\nObtain the vector representation (skip-gram) for words in the question.\nObtain the VGG Net embeddings of the image and use a linear transformation (dimensionality reduction weight matrix) to match the dimensions of word embeddings.\nKeep image embedding frozen during training and use an LSTM to combine the word vectors.\nLSTM outputs are fed into a softmax layer which generates the answer.\nDataset\nDAtaset for QUestion Ansering on Real-world images (DAQUAR)\n1300 images and 7000 questions with 37 object classes.\nDownside is that even guess work can yield good results.\nThe paper proposed an algorithm for generating questions using MS-COCO dataset.\nPerform preprocessing steps like breaking large sentences and changing indefinite determines to definite ones.\nobject\nquestions,\nnumber\nquestions,\ncolour\nquestions and\nlocation\nquestions can be generated by searching for nouns, numbers, colours and prepositions respectively.\nResulting dataset has ~120K questions across above 4 semantic types.\nModels\nVIS+LSTM - explained above\n2-VIS+BLSTM - Add the image features twice, in beginning and in the end (using different linear transformations) plus use bidirectional LSTM\nIMG+BOW - Multinomial logistic regression on image features without dimensionality reduction + bag of words (averaging word vectors).\nFULL - Simple average of above 2 models.\nBaseline\nIncludes models where the answer is guessed, or only image or question features are used or image features along with prior knowledge of object are used.\nAlso includes a KNN model where the system finds the nearest (image, question) pair.\nMetrics\nAccuracy\nWu-Palmer similarity measure\nObservations\nThe VIS-LSTM model outperforms the baselines while the FULL model benefits from averaging across all the models.\nSome useful information seems to be lost when downsizing the VGG vectors.\nFine tuning the word vectors helps with performance.\nNormalising CNN hidden image features into zero mean and unit variance leads to faster training.\nModel does not perform well on the task of considering spatial relations between multiple objects and counting objects when multiple objects are present\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1505.02074"
    },
    "104": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/How-transferable-are-features-in-deep-neural-networks",
        "transcript": "How transferable are features in deep neural networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nHow transferable are features in deep neural networks\n2014\n\u2022\nNIPS 2014\n\u2022\nTransfer Learning\n\u2022\nAI\n\u2022\nNIPS\n\u2022\nCV\n06 Jan 2018\nIntroduction\nWhen neural networks are trained on images, they tend to learn the same kind of features for the first layer (corresponding to Gabor filters or colour blobs). The first layer features are \u201cgeneral\u201d irrespective of the task/optimizer etc.\nThe final layer features tend to be \u201cspecific\u201d in the sense that they strongly depend on the task.\nThe paper studies the transition of generalization property across layers in the network. This could be useful in the domain of transfer learning where features are reused across tasks.\nLink to the paper\nSetup\nDegree of generality of a set of features, learned on task A, is defined as the extent to which these features can be used for another task B.\nRandomly split 1000 ImageNet classes into 2 groups (corresponding to tasks A and B). Each group has 500 classes and half the total number of examples.\nTwo 8-layer convolutional networks are trained on the two datasets and labelled as baseA and baseB respectively.\nNow choose a layer numbered n from {1, 2\u20267}.\nFor each layer n, train the following two networks:\nSelffer Network BnB\nCopy (and freeze) first n layers from baseB. The remaining layers are initialized randomly and trained on B.\nThis serves as the control group.\nTransfer Network AnB\nCopy (and freeze) first n layers from baseA. The remaining layers are initialized randomly and trained on B.\nThis corresponds to transferring features from A to B.\nIf AnB performs well, n\nth\nlayer features are \u201cgeneral\u201d.\nIn another setting, the transferred layers are also fine-tuned (BnB\n+\nand AnB\n+\n).\nImageNet dataset contains a hierarchy of classes which allow for creating the datasets A and B with high and low similarity.\nObservation\nDataset A and B are similar\nFor n = {1, 2}, the performance of the BnB model is same as baseB model. For n = {3, 4, 5, 6}, the performance of BnB model is worse.\nThis indicates the presence of \u201cfragile co-adaption\u201d features on successive layers where features interact with each other in a complex way and can not be easily separated across layers. This is more prominent across middle layers and less across the first and the last layers.\nFor model AnB, the performance of baseB for n = {1, 2}. Beyond that, the performance begins to drop.\nTransfer learning of features followed by fine-tuning gives better results than training the network from scratch.\nDataset A and B are dissimilar\nEffectiveness of feature transfer decreases as the two tasks become less similar.\nRandom Weights\nInstead of using transferred weights in BnB and BnA, the first n layers were initialized randomly.\nThe performance falls for layer 1 and 2. It further drops to near-random level for layers 3 and beyond.\nAnother interesting insight is that even for dissimilar tasks, transferring features is better than using random features.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf"
    },
    "105": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Distilling-the-Knowledge-in-a-Neural-Network",
        "transcript": "Distilling the Knowledge in a Neural Network \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nDistilling the Knowledge in a Neural Network\n2014\n\u2022\nNIPS 2014\n\u2022\nAI\n\u2022\nNIPS\n31 Dec 2017\nIntroduction\nIn machine learning, it is common to train a single large model (with a large number of parameters) or ensemble of multiple smaller models using the same dataset.\nWhile such large models help to improve the performance of the system, they also make it difficult and computationally expensive to deploy the system.\nThe paper proposes to transfer the knowledge from such \u201ccumbersome\u201d models into a single, \u201csimpler\u201d model which is more suitable for deployment. This transfer of knowledge is referred to as \u201cdistillation\u201d.\nLink to the paper\nIdea\nTrain the cumbersome model using the given training data in the usual way.\nTrain the simpler, distilled model using the class probabilities (from the cumbersome model) as the soft target. Thus, the simpler model is trained to generalise the same way as the cumbersome model.\nIf the soft targets have high entropy, they provide much more information than the hard targets and the gradient (between training examples) would vary lesser.\nOne approach is to minimise the L2 difference between logits produced by the cumbersome model and the simpler model. This approach was pursued by\nBucilu\u01ce et al.\nThe paper proposes a more general solution which they name \u201cdistillation\u201d. The temperature of the final softmax is increased till the cumbersome model produces a set of soft targets (from the final softmax layer). These soft targets are then used to train the simpler model.\nIt also shows that the proposed approach is, in fact, a more general case of the first approach.\nApproach\nIn the simplest setting, the cumbersome model is first trained with a high value of temperature and then the same temperature value is used to train the simpler model. The temperature is set to 1 when making predictions using the simpler model.\nIt helps to add an auxiliary objective function which corresponds to the cross-entropy loss with the correct labels. The second objective function should be given a much lower weight though. Further, the magnitude of the soft targets needs to be scaled by multiplying with the square of temperature.\nExperiment\nThe paper reports favourable results for distillation task for the following domains:\nImage Classification (on MNIST dataset)\nAn extra experiment is performed where the simpler model is not shown any images of \u201c3\u201d but the model fails for only 133 cases out of 1010 cases involving \u201c3\u201d.\nAutomatic Speech Recognition (ASR)\nAn extra experiment is performed where the baseline model is trained using both hard targets and soft targets alternatively. Further, only 3% of the total dataset is used.\nThe model using hard targets overfits and has poor test accuracy while the model using soft targets does not overfit and gets much better test accuracy. This shows the regularizing effect of soft targets.\nTraining ensemble specialists for very large datasets (JFT dataset - an internal dataset at Google)\nThe experiment shows that while training a single large model would take a lot of time, the performance of the model can be improved by learning a small number of specialised networks (which are faster to train).\nThough it is yet to be shown that the knowledge of such specialist models can be distilled back into a single model.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1503.02531"
    },
    "106": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Revisiting-Semi-Supervised-Learning-with-Graph-Embeddings",
        "transcript": "Revisiting Semi-Supervised Learning with Graph Embeddings \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nRevisiting Semi-Supervised Learning with Graph Embeddings\n2016\n\u2022\nICML 2016\n\u2022\nGraph Representation\n\u2022\nAI\n\u2022\nEmbedding\n\u2022\nGraph\n\u2022\nICML\n11 Dec 2017\nIntroduction\nThe paper presents a semi-supervised learning framework for graphs where the node embeddings are used to jointly predict both the class labels and neighbourhood context. Usually, graph embeddings are learnt in an unsupervised manner and can not leverage the supervising signal coming from the labelled data.\nThe framework is called\nPlanetoid (Predicting Labels And Neighbors with Embeddings Transductively Or Inductively from Data)\n.\nLink to the paper\nProblem Setting\nGiven a graph G = (V, E) and x\nL\nand x\nU\nas feature vectors for labelled and unlabelled nodes and y\nL\nas labels for the labelled nodes, the problem is to learn a mapping (classifier) f: x -> y\nThere are two settings possible:\nTransductive\n- Predictions are made only for those nodes which are already observed in the graph at training time.\nInductive\n- Predictions are made for nodes whether they have been observed in the graph at training time or not.\nApproach\nThe general semi-supervised learning loss would be\nL\nS\n+ \u03bbL\nU\nwhere\nL\nS\nis the supervised learning loss while\nL\nU\nis the unsupervised learning loss.\nThe unsupervised loss is a variant of the Skip-gram loss with negative edge sampling.\nMore specifically, first a random walk sequence S is sampled. Then either a positive edge is sampled from S (within a given context distance) or a negative edge is sampled.\nThe label information is injected by using the label as a context and minimising the distance between the positive edges (edges where the nodes have the same label) and maximising the distance between the negative edges (edges where the nodes have different labels).\nTransductive Formulation\nTwo separate fully connected networks are applied over the node features and node embeddings.\nThese 2 representations are then concatenated and fed to a softmax classifier to predict the class label.\nInductive Formulation\nIn the inductive setting, it is difficult to obtain the node embeddings at test time. One naive approach is to retrain the network to obtain the embeddings on the previously unobserved nodes but that is inefficient.\nThe embeddings of node x are parameterized as a function of its input feature vector and is learnt by applying a fully connected neural network on the node feature vector.\nThis provides a simple way to extend the original approach to the inductive setting.\nResults\nThe proposed approach is evaluated in 3 settings (text classification, distantly supervised entity extraction and entity classification) and it consistently outperforms approaches that use just node features or node embeddings.\nThe key takeaway is that the joint training in the semi-supervised setting has several benefits over the unsupervised setting and that using the graph context (in terms of node embeddings) is much more effective than using graph Laplacian-based regularization term.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.08861"
    },
    "107": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Two-Stage-Synthesis-Networks-for-Transfer-Learning-in-Machine-Comprehension",
        "transcript": "Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nTwo-Stage Synthesis Networks for Transfer Learning in Machine Comprehension\n2017\n\u2022\nEMNLP 2017\n\u2022\nAI\n\u2022\nEMNLP\n\u2022\nMachine Comprehension\n\u2022\nNLP\n\u2022\nQA\n\u2022\nTransfer Learning\n28 Nov 2017\nIntroduction\nThe paper proposes a two-stage synthesis network that can perform transfer learning for the task of machine comprehension.\nThe problem is the following:\nWe have a domain D\nS\nfor which we have labelled dataset of question-answer pairs and another domain D\nT\nfor which we do not have any labelled dataset.\nWe use the data for domain D\nS\nto train SynNet and use that to generate synthetic question-answer pairs for domain D\nT\n.\nNow we can train a machine comprehension model M on D\nS\nand finetune using the synthetic data for D\nT\n.\nLink to the paper\nSynNet\nWorks in two stages:\nAnswer Synthesis - Given a text paragraph, generate an answer.\nQuestion Synthesis - Given a text paragraph and an answer, generate a question.\nAnswer Synthesis Network\nGiven the labelled dataset for D\nS\n, generate a labelled dataset of <word, tag> pair such that each word in the given paragraph is assigned one of the 4 tags:\nIOB\nstart\n- if it is the starting word of an answer\nIOB\nmid\n- if it is the intermediate word of an answer\nIOB\nend\n- if it is the ending word of an answer\nIOB\nnone\n- if it is not part of any answer\nFor training, map the words to their GloVe embeddings and pass through a Bi-LSTM. Next, pass them through two-FC layers followed by a softmax layer.\nFor the target domain D\nT\n, all the consecutive word spans where no label is IOB\nnone\nare returned as candidate answers.\nQuestion Synthesis Network\nGiven an input paragraph and a candidate answer, Question Synthesis network generates question one word at a time.\nMap each word in the paragraph to their GloVe embedding. After the word vector, append a \u20181\u2019 if the word was part of the candidate answer else append a \u20180\u2019.\nFeed to a Bi-LSTM network (encoder-decoder) where the decoder conditions on the representation generated by the encoder as well as the question tokens generated so far. Decoding is stopped when \u201cEND\u201d token is produced.\nThe paragraph may contain some named entities or rare words which do not appear in the softmax vocabulary. To account for such words, a copying mechanism is also incorporated.\nAt each time step, a Pointer Network (C\nP\n) and a Vocabulary Predictor (V\nP\n) are used to generate probability distribution for the next word and a Latent Predictor Network is used to decide which of the two networks would be used for the prediction.\nAt inference time, a greedy decoding is used where the most likely predictor is chosen and then the most likely word from that predictor is chosen.\nMachine Comprehension Model\nGiven any MC model, first train it over domain D\nS\nand then fine-tune using the artificial questions generated using D\nT\n.\nImplementation Details\nData Regularization\n- There is a need to alternate between mini batches from source and target domain while fine-tuning the MC model.\nAt inference time, the fine-tuned MC model is used to get the distribution P(i=start) and P(i=end) (corresponding to the likelihood of choosing word I as the starting or ending word for the answer) for all the words and DP is used to find the optimal answer span.\nCheckpoint Averaging\n- Use the different checkpointed models to average the answer likelihood before running DP.\nUsing the synthetically generated dataset helps to gain a 2% improvement in terms of F-score (from SQuAD -> NewsQA). Using checkpointed models further improves the performance to overall 46.6% F score which closes the gap with respect to the performance of model trained on NewsQA itself (~52.3% F score)\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://www.microsoft.com/en-us/research/publication/two-stage-synthesis-networks-transfer-learning-machine-comprehension/"
    },
    "108": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Higher-order-organization-of-complex-networks",
        "transcript": "Higher-order organization of complex networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nHigher-order organization of complex networks\n2016\n\u2022\nScience 2016\n\u2022\nClustering\n\u2022\nGraph\n\u2022\nMotif\n\u2022\nNetwork\n\u2022\nScience\n19 Nov 2017\nIntroduction\nThe paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns.\nLink to the paper\nApproach\nGiven a\nmotif M\n, the framework aims to find a cluster of the set of nodes S such that nodes of S participate in many instances of M and avoid cutting instances of M (that is only a subset of nodes in instances of M appears in S).\nMathematically, the aim is to minimise the motif conductance metric given as\ncut\nM\n(S, S\u2019) / min[vol\nM\n(S), vol\nM\n(S\u2019)]\nwhere\nS\u2019\nis complement of\nS\n,\ncut\nM\n(S, S\u2019)\n= number of instances of M which have atleast one node from both\nS\nand\nS\u2019\nand\nvol\nM\n(S)\n= Number of nodes in instances of M that belong only to S.\nSolving the above equation is computationally infeasible and an approximate solution is proposed using eigenvalues and matrices.\nThe approximate solution is easy to implement, efficient and guaranteed to find clusters that are at most a quadratic factor away from the optimal.\nAlgorithm\nGiven the network and motif M, form a motif adjacency matrix W\nM\nwhere W\nM\n(i, j) is the number of instances of M that contains i and j.\nCompute spectral ordering of the nodes from normalized motif laplacian matrix.\nCompute prefix set of spectral ordering with small motif conductance.\nScalability\nWorst case\nO(m\n1.5\n)\n, based on experiments\nO(m\n1.2\n)\nwhere\nm\nis the number of edges.\nAdvantages\nApplicable to directed, undirected and weighted graphs (allows for negative edge weights as well).\nIn case the motif is not known beforehand, the framework can be used to compute significant motifs.\nThe proposed framework unifies the two fundamental tools of network science (motif analysis and network partitioning) along with some worst-case guarantees for the approximations employed and can be extended to identify higher order modular organization of networks.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "http://science.sciencemag.org/content/353/6295/163"
    },
    "109": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Network-Motifs-Simple-Building-Blocks-of-Complex-Networks",
        "transcript": "Network Motifs - Simple Building Blocks of Complex Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nNetwork Motifs - Simple Building Blocks of Complex Networks\n2002\n\u2022\nScience 2002\n\u2022\nGraph\n\u2022\nMotif\n\u2022\nNetwork\n\u2022\nScience\n12 Nov 2017\nIntroduction\nThe paper presents the concept of \u201cnetwork motifs\u201d to understand the structural design of a network or a graph.\nLink to the paper\nIdea\nA network motif is defined as \u201ca pattern of inter-connections occurring in complex networks in numbers that are significantly higher than those in randomized networks\u201d.\nIn the practical setting, given an input network, we first create randomized networks which have same single node characteristics (like a number of incoming and outgoing edges) as the input network.\nThe patterns that occur at a much higher frequency in the input graph (than the randomized graphs) are reported as motifs.\nMore specifically, the patterns for which the probability of appearing in a randomized network an equal or more number of times than in the real network is lower than a cutoff value (say 0.01).\nMotivation\nReal-life networks exhibit properties like \u201csmall world\u201d property ( the majority of nodes are within a distance of fewer than 7 hops from each other) and \u201cscale-free\u201d property (fraction of nodes having k edges decays as a power-law).\nMotifs are one such structural property that is exhibited by networks in biochemistry, neurobiology, ecology, and engineering. Further, motifs shared by graphs of different domains are different which hints at the usefulness of motifs as a fundamental structural property of the graph and relates to the process of evolution of the graph.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "http://science.sciencemag.org/content/298/5594/824"
    },
    "110": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Word-Representations-via-Gaussian-Embedding",
        "transcript": "Word Representations via Gaussian Embedding \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nWord Representations via Gaussian Embedding\n2015\n\u2022\nICLR 2015\n\u2022\nRepresentation Learning\n\u2022\nWord Vectors\n\u2022\nAI\n\u2022\nICLR\n\u2022\nNLP\n05 Nov 2017\nIntroduction\nExisting word embedding models like\nSkip-Gram\n,\nGloVe\netc map words to fixed sized vectors in a low dimensional vector space.\nThis fixed point setting cannot capture uncertainty about representation.\nFurther, these fixed point vectors are compared with measures like dot product and cosine similarity which are not suitable for capturing asymmetric properties like textual entailment and inclusion.\nThe paper proposes to learn Gaussian function embeddings (with diagonal covariance) for the word vectors.\nThis way, the words are mapped to soft regions in the embedding space which enables modeling uncertainty and asymmetric properties like inclusion and uncertainty.\nLink to the paper\nImplementation\nApproach\nKL divergence is used as the asymmetric distance function for comparing the distributions.\nUnlike the Word2Vec model, the proposed model uses ranking-based loss.\nSimilarity Measures used\nSymmetric Similarity\nFor two gaussian distributions,\nP\ni\nand\nP\nj\n, compute the inner product\nE(P\ni\n, P\nj\n)\nas\nN(0; mean\ni\n- mean\nj\n, sigma\ni\n+ sigma\nj\n)\n.\nCompute the gradient of\nmean\nand\nsigma\nwith respect to\nlog(E)\n.\nThe resulting loss function can be interpreted as pushing the means closer which encouraging the two gaussians to be more concentrated.\nAsymmetric Similarity\nUse KL divergence to encode the context distribution.\nThe benefit over the symmetric setting is that now entailment type relations can also be modeled.\nFor example, a low KL divergence from x to y indicates that y can be encoded as x or that y \u201centails\u201d x.\nLearning\nOne of the two notions of similarity is chosen and max-margin is used as the loss function.\nMean is regularized by adding a simple constraint on the L2-norm.\nFor covariance matrix, the eigenvalues are constrained to lie within a hypercube. This ensures that the positive-definite property of the covariance matrix is maintained while having a constraint on the size.\nObservations\nPolysemous words have higher variance in their word embeddings as compared to specific words.\nKL divergence (with diagonal covariance) outperforms other models.\nSimple tree hierarchies can also be modeled by embedding into the Gaussian space. A Gaussian is created for each node with randomly initialized mean and the same set of embeddings is used for nodes and context.\nFor word similarity benchmarks, embeddings with spherical covariance have a slight edge over embeddings with diagonal covariance and outperform the Skip-Gram model in all the cases.\nFuture Work\nUse combinations of low rank and diagonal matrices for covariances.\nImproved optimisation strategies.\nTrying other distributions like Student\u2019s-t distribution.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1412.6623"
    },
    "111": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/HARP-Hierarchical-Representation-Learning-for-Networks",
        "transcript": "HARP - Hierarchical Representation Learning for Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nHARP - Hierarchical Representation Learning for Networks\n2017\n\u2022\nGraph Representation\n\u2022\nAI\n\u2022\nEmbedding\n\u2022\nGraph\n\u2022\nSOTA\n28 Oct 2017\nIntroduction\nHARP is an architecture to learn low-dimensional node embeddings by compressing the input graph into smaller graphs.\nLink to the paper\n.\nGiven a graph\nG = (V, E)\n, compute a series of successively smaller (coarse) graphs\nG\n0\n, \u2026, G\nL\n. Learn the node representations in\nG\nL\nand successively refine the embeddings for larger graphs in the series.\nThe architecture is independent of the algorithms used to embed the nodes or to refine the node representations.\nGraph coarsening technique that preserves global structure\nCollapse edges and stars to preserve first and second order proximity.\nEdge collapsing\n- select the subset of\nE\nsuch that no two edges are incident on the same vertex and merge their nodes into a single node and merge their edges as well.\nStar collapsing\n- given star structure, collapse the pairs of neighboring nodes (of the central node).\nIn practice, first apply star collapsing, followed by edge collapsing.\nExtending node representation from coarse graph to finer graph\nLets say\nnode1\nand\nnode2\nwere merged into\nnode12\nduring coarsening. First copy the representation of\nnode12\ninto\nnode1\n,\nnode2\n.\nAdditionally, if hierarchical softmax was used, extend the B-tree such that\nnode12\nis replaced by 2 child nodes\nnode1\nand\nnode2\n.\nTime complexity for HARP + DeepWalk is\nO(number of walks * |V|)\nwhile for HARP + LINE is\nO(number of iterations * |E|)\n.\nThe asymptotic complexity remains the same as the HARP-less version for the two cases.\nMultilabel classification task shows that HAR improves all the node embedding technique with gains up to 14%.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.07845"
    },
    "112": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Swish-A-self-gated-activation-function",
        "transcript": "Swish - a Self-Gated Activation Function \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nSwish - a Self-Gated Activation Function\n2017\n\u2022\nActivation Function\n\u2022\nSelf Gated\n\u2022\nAI\n\u2022\nSOTA\n22 Oct 2017\nIntroduction\nThe paper presents a new activation function called Swish with formulation\nf(x) = x.sigmod(x)\nand its parameterised version called Swish-\u03b2 where\nf(x, \u03b2) = 2x.sigmoid(\u03b2.x)\nand \u03b2 is a training parameter.\nThe paper shows that Swish is consistently able to outperform RELU and other activations functions over a variety of datasets (CIFAR, ImageNet, WMT2014) though by small margins only in some cases.\nLink to the paper\nProperties of Swish\nSmooth, non-monotonic function.\nSwish-\u03b2 can be thought of as a smooth function that interpolates between a linear function and RELU.\nUses self-gating mechanism (that is, it uses its own value to gate itself). Gating generally uses multiple scalar inputs but since self-gating uses a single scalar input, it can be used to replace activation functions which are generally pointwise.\nBeing unbounded on the x>0 side, it avoids saturation when training is slow due to near 0 gradients.\nBeing bounded below induces a kind of regularization effect as large, negative inputs are forgotten.\nSince the Swish function is smooth, the output landscape and the loss landscape are also smooth. A smooth landscape should be more traversable and less sensitive to initialization and learning rates.\nCriticism\nSwish is much more complicated than ReLU (when weighted against the small improvements that are provided) so it might not end up with as strong an adoption as ReLU.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1710.05941"
    },
    "113": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Reading-Wikipedia-to-Answer-Open-Domain-Questions",
        "transcript": "Reading Wikipedia to Answer Open-Domain Questions \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nReading Wikipedia to Answer Open-Domain Questions\n2017\n\u2022\nACL 2017\n\u2022\nACL\n\u2022\nAI\n\u2022\nDataset\n\u2022\nMachine Comprehension\n\u2022\nNLP\n\u2022\nQA\n15 Oct 2017\nIntroduction\nThe paper presents a new machine comprehension dataset for question answering in real life setting (say when interacting with Cortana/Siri).\nLink to the paper\nUnique Aspects of the dataset\nExisting machine comprehension (MC) datasets are either too small or synthetic (with a distribution different from that or real-questions posted by humans). MARCO questions are sampled from real, anonymized user queries.\nMost datasets would provide a comparatively small and clean context to answer the question. In MARCO, the context documents (which may or may not contain the answer) are extracted using Bing from real-world documents. As such the questions and the context documents are noisy.\nIn general, the answer to the questions are restricted to an entity or text span within the document. In case of MARCO, the human judges are encouraged to generate complete sentences as answers.\nDataset Description\nFirst release consists of 100K questions with the aim of releasing 1M questions in the future releases.\nAll questions are tagged with segment information.\nA subset of questions has multiple answers and another subset has no answers at all.\nEach record in the dataset contains the following information:\nQuery\n- The actual question\nPassage\n- Top 10 contextual passages extracted from web search engine (which may or may not contain the answer to the question).\nDocument URLs\n- URLs for the top documents (which are the source of the contextual passages).\nAnswer\n- Answer synthesised by human evaluators.\nSegment\n- Query type, description, neumeric, entity, location, person.\nExperimental Results\nMetrics\nAccuracy and precision/recall for numeric questions\nROGUE-L/paraphrasing aware evaluation framework for long, textual answers.\nAmong generative models, Memory Networks performed better than seq-to-seq.\nIn the cloze-style test,\nReasoNet\nachieved an accuracy of approx. 59% while\nAttention Sum Reader\nachieved an accuracy of approx 55%.\nCurrent QA systems (including the ones using memory and attention) derive their power from supervised data and are very different from how humans do reasoning.\nImagenet dataset pushed the state-of-the-art performance on object classification to beyond human accuracy. Similar was the case with speech recognition dataset from DARPA which led to the advancement of speech recognition. Having a large, diverse and human-like questions dataset is a fundamental requirement to advance the field and the paper aims to provide just the right kind of dataset.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.00051"
    },
    "114": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Task-Oriented-Query-Reformulation-with-Reinforcement-Learning",
        "transcript": "Task-Oriented Query Reformulation with Reinforcement Learning \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nTask-Oriented Query Reformulation with Reinforcement Learning\n2017\n\u2022\nEMNLP 2017\n\u2022\nInformation Retrieval\n\u2022\nAI\n\u2022\nEMNLP\n\u2022\nNLP\n\u2022\nRL\n01 Oct 2017\nIntroduction\nThe paper introduces a query reformulation system that rewrites a query to maximise the number of \u201crelevant\u201d documents that are extracted from a given black box search engine.\nA Reinforcement Learning (RL) agent selects the terms that are to be added to the reformulated query and the rewards are decided on the basis of document recall.\nLink to the paper\nImplementation\nKey Aspect\nThe underlying problem is as follows: when the end user makes a query to a search engine, the engine often relies on word matching techniques to perform retrieval. This means relevant documents could be missed if there is no exactly matching words between the query and the document.\nThis problem can be handled at two levels: First, the search engine itself takes care of query semantics. Alternatively, we assume the search engine to be dumb and instead have a system in place that can improve the original queries (automatic query reformulation).\nThe paper takes the latter approach and expands the original query by adding terms from the set of retrieved documents (pseudo relevance feedback).\nDatasets\nTREC - Complex Answer Retrieval (TREC-CAR)\nJeopardy Q&A dataset\nMicrosoft Academic (MSA) dataset - created by the authors using papers crawled from Microsoft Academic API\nFramework\nQuery Reformulation task is modeled as an RL problem where:\nEnvironment is the search engine.\nActions are whether a word is to be added to the query or not and if yes, then what word is added.\nReward is the retrieval accuracy.\nThe input to the system is a query q\n0\nconsisting of a sequence of words w\n1\n, \u2026, w\nn\nand a candidate term t\ni\nwith some context words.\nCandidate terms are all the terms that appear in the original query and the documents retrieved using the query.\nThe words are mapped to vectors and then a fixed size representation is obtained for the sequence using CNN\u2019s or RNNs.\nSimilarly, a representation is obtained for the candidate words by feeding them and their context words to the CNN or RNNs.\nFinally, a sigmoidal score is computed for all the candidate words.\nAn RNN sequentially applies this model to emit query words till an end token is emitted.\nVocabulary is used only from the extracted documents and not the entire vocabulary set, to keep the inference fast.\nTraining\nThe model is trained using REINFORCE algorithm which minimizes the\nC\na\n= (R \u2212 R~) * sum(log(P(t|q))) where R~ is the baseline.\nValue network minimises\nC\nb\n= &\\alpha(||R-R~||\n2\n)\nC\na\nand\nC\nb\nare minimised using SGD.\nAn entropy regulation term is added to prevent the probability distribution from reaching the peak.\nExperiments\nBaseline Methods\nRaw\n- Original query is fed to the search engine without any modification.\nPseudo-Relevance Feedback (PRF-TFIDF)\n- The query is expanded using the top-N TF-IDF terms.\nPRF-Relevance Model (PRF-RM)\n- Probability of adding token\nt\nto the query\nq0\nis given by\nP(t|q0) = (1 \u2212 \u03bb)P\u2032(t|q0) + \u03bb sum (P(d)P(t|d)P(q0|d))\nProposed Methods\nSupervised Learning\nAssumes that the query words contribute indepently to the query retrival performace. (Too strong an assumption).\nA term is marked as relevant if\n(R(new_query) - R(old_query))/R(old_query) > 0.005\nReinforcement Learning\nRL-RNN/CNN - RL Framework + RNN/CNN to encode the input features.\nRL-RNN-SEQ - Add a sequential generator.\nMetrics\nRecall@K\nPrecision@K\nMean Average Precision@K\nReward\n- The paper uses Recall@K as a reward when training the RL-based models with the argument that the \u201cmetric has shown to be effective in improving the other metrics as well\u201d, without any justification though.\nSL-Oracle\n- classifier that perfectly selects terms that will increase performance based on the supervised learning approach.\nRL-Oracle\n- Produces a conservative upper-bound for the performance of the RL Agent. It splits the test data into N subsets and trains an RL agent for each subset. Then, the reward is averaged over all the N subsets.\nObservations\nReformulation based methods > original query\nRL methods > Supervised methods > unsupervised methods\nRL-RNN-SEQ performs slightly worse than RL-RNN but is much faster (as it produces shorter queries).\nRL-based model benefits from more candidate terms while the classical PRF method quickly saturates.\nComments\nInterestingly, for each raw query, they carried out the reformulation step just once and not multiple times. The number of times a query is reformulated could also have become a part of the RL framework.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.04572"
    },
    "115": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Refining-Source-Representations-with-Relation-Networks-for-Neural-Machine-Translation",
        "transcript": "Refining Source Representations with Relation Networks for Neural Machine Translation \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nRefining Source Representations with Relation Networks for Neural Machine Translation\n2017\n\u2022\nRelational Network\n\u2022\nRepresentation Learning\n\u2022\nAI\n\u2022\nNLP\n\u2022\nNMT\n22 Sep 2017\nIntroduction\nThe paper introduces Relation Network (RN) that refines the encoding representation of the given source document (or sentence).\nThis refined source representation can then be used in Neural Machine Translation (NMT) systems to counter the problem of RNNs forgetting old information.\nLink to the paper\nLimitations of existing NMT models\nThe RNN encoder-decoder architecture is the standard choice for NMT systems. But the RNNs are prone to forgetting old information.\nIn NMT models, the attention is modeled in the unit of words while the use of phrases (instead of words) would be a better choice.\nWhile NMT systems might be able to capture certain relationships between words, they are not explicitly designed to capture such information.\nContributions of the paper\nLearn the relationship between the source words using the context (neighboring words).\nRelation Networks (RNs) build pairwise relations between source words using the representations generated by the RNNs. The RN would sit between the encoder and the attention layer of the encoder-decoder framework thereby keeping the main architecture unaffected.\nRelation Network\nNeural network which is desgined for relational reasoning.\nGiven a set of inputs * O = o\n1\n, \u2026, o\nn\n*, RN is formed as a composition of inputs:\n   RN(O) = f(sum(g(o\ni\n, o\nj\n))), f and g are functions used to learn the relations (feed forward networks)\ng\nlearns how the objects are related hence the name \u201crelation\u201d.\nComponents\n:\nCNN Layer\nExtract information from the words surrounding the given word (context).\nThe final output of this layer is the sequence of vectors for different kernel width.\nGraph Propagation (GP) Layer\nConnect all the words with each other in the form of a graph.\nEach output vector from the CNN corresponds to a node in the graph and there is an edge between all possible pair of nodes.\nThe information flows between the nodes of the graph in a message passing sort of fashion (graph propagation) to obtain a new set of vectors for each node.\nMulti-Layer Perceptron (MLP) Layer\nThe representation from the GP Layer is fed to the MLP layer.\nThe layer uses residual connections from previous layers in form of concatenation.\nDatasets\nIWSLT Data - 44K sentences from tourism and travel domain.\nNIST Data - 1M Chinese-English parallel sentence pairs.\nModels\nMOSES - Open source translation system - http://www.statmt.org/moses/\nNMT - Attention based NMT\nNMT+ - NMT with improved decoder\nTRANSFORMER - Google\u2019s new NMT\nRNMT+ - Relation Network integrated with NMT+\nEvaluation Metric\ncase-insensitive 4-gram BLEU score\nObservations\nAs sentences become larger (more than 50 words), RNMT clearly outperforms other baselines.\nQualitative evaluation shows that RNMT+ model captures the word alignment better than the NMT+ models.\nSimilarly, NMT+ system tends to miss some information from the source sentence (more so for longer sentences). While both CNNs and RNNs are weak at capturing long-term dependency, using the relation layer mitigates this issue to some extent.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1709.03980"
    },
    "116": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Pointer-Networks",
        "transcript": "Pointer Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nPointer Networks\n2015\n\u2022\nNIPS 2015\n\u2022\nSeq2Seq\n\u2022\nAI\n\u2022\nNIPS\n\u2022\nNLP\n\u2022\nSoftmax\n27 Aug 2017\nIntroduction\nThe paper introduces a novel architecture that generates an output sequence such that the elements of the output sequence are discrete tokens corresponding to positions in the input sequence.\nSuch a problem can not be solved using\nSeq2Seq\nor Neural Turing Machines as the size of the output softmax is variable (as it depends on the size of the input sequence).\nLink to the paper\nArchitecture\nTraditional attention-base sequence-to-sequence models compute an attention vector for each step of the output decoder and use that to blend the individual context vectors of the input into a single, consolidated attention vector. This attention vector is used to compute a fixed size softmax.\nIn Pointer Nets, the normalized attention vector (over all the tokens in the input sequence) is normalized and treated as the softmax output over the input tokens.\nSo Pointer Net is a very simple modification of the attention model.\nApplication\nAny problem where the size of the output depends on the size of the input because of which fixed length softmax is ruled out.\neg combinatorial problems such as planar convex hull where the size of the output would depend on the size of the input.\nEvaluation\nThe paper considers the following 3 problems:\nConvex Hull\nDelaunay triangulations\nTravelling Salesman Problem (TSP)\nSince some of the problems are NP hard, the paper considers approximate solutions whereever the exact solutions are not feasible to compute.\nThe authors used the exact same architecture and model parameters of all the instances of the 3 problems to show the generality of the model.\nThe proosed Pointer Nets outperforms LSTMs and LSTMs with attention and can generalise quite well for much larger sequences.\nInterestingly, the order in which the inputs are fed to the system affects its performance. The authors discussed this apsect in their subsequent paper titled\nOrder Matters: Sequence To Sequence for Sets\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.03134"
    },
    "117": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/R-NET-Machine-Reading-Comprehension-with-Self-matching-Networks",
        "transcript": "R-NET - Machine Reading Comprehension with Self-matching Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nR-NET - Machine Reading Comprehension with Self-matching Networks\n2017\n\u2022\nAI\n\u2022\nMachine Comprehension\n\u2022\nNLP\n\u2022\nQA\n\u2022\nRL\n\u2022\nSOTA\n07 Aug 2017\nIntroduction\nR-NET is an end-to-end trained neural network model for machine comprehension.\nIt starts by matching the question and the given passage (using gated attention based RNN) to obtain question-aware passage representation.\nNext, it uses a self-matching attention mechanism to refine the passage representation by matching the passage against itself.\nLastly, it uses pointer networks to determine the position of the answer in the passage.\nLink to the paper\nDatasets\nSQuAD\nMS-MARCO\nArchitecture\nQuestion / Passage Encoder\nConcatenate the word level and character level embeddings for each word and feed into a bidirectional GRU to obtain question and passage representation.\nGated Attention based RNN\nGiven question and passage representation, sentence pair representation is generated via soft-alignment of the words in the question and in the passage.\nThe newly added gate captures the relation between the question and the current passage word as only some parts of the passage are relevant for answering the given question.\nSelf Matching Attention\nThe passage representation obtained so far would not capture most of the context.\nSo the current representation is matched against itself so as to collect evidence from the entire passage and encode the evidence relevant to the current passage word and question.\nOutput Layer\nUse pointer network (initialized using attention pooling over answer representation) to predict the position of the answer.\nLoss function is the sum of negative log probabilities of start and end positions.\nResults\nR-NET is ranked second on\nSQuAD Leaderboard\nas of 7th August, 2017 and achieves best-published results on MS-MARCO dataset.\nUsing ideas like sentence ranking, using syntax information performing multihop inference and augmenting question dataset (using seqToseq network) do not help in improving the performance.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://www.microsoft.com/en-us/research/publication/mrc/"
    },
    "118": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/ReasoNet-Learning-to-Stop-Reading-in-Machine-Comprehension",
        "transcript": "ReasoNet - Learning to Stop Reading in Machine Comprehension \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nReasoNet - Learning to Stop Reading in Machine Comprehension\n2017\n\u2022\nKDD 2017\n\u2022\nAI\n\u2022\nKDD\n\u2022\nMachine Comprehension\n\u2022\nNLP\n\u2022\nQA\n\u2022\nRL\n\u2022\nSOTA\n24 Jul 2017\nIntroduction\nIn the domain of machine comprehension, making multiple passes over the given document is an effective technique to extract the relation between the given passage, question and answer.\nUnlike previous approaches, which perform a fixed number of passes over the passage, Reasoning Network (ReasoNet) uses reinforcement learning (RL) to decide how many times a document should be read.\nEvery time the document is read, ReasoNet determines whether the document should be read again or has the termination state been reached. If termination state is reached, the answer module is triggered to generate the answer.\nSince the termination state is discrete and not connected to the final output, RL approach is used.\nLink to the paper\nDatasets\nCNN, DailyMail Dataset\nSQuAD\nGraph Reachability Dataset\n2 synthetic datasets to test if the network can answer questions like \u201cIs node_1 connected to node_12\u201d?\nArchitecture\nMemory (M)\n- Comprises of the vector representation of the document and the question (encoded using GRU or other RNNs).\nAttention\n- Attention vector (\nx\nt\n) is a function of current internal state\ns\nt\nand external memory\nM\n. The state and memory are passed through FCs and fed to a similarity function.\nInternal State (s\nt\n)\n- Vector representation of the question state computed by a RNN using the previous internal state and the attention vector\nx\nt\nTermination Gate (T\nt\n)\n- Uses a logistic regression model to generate a random binary variable using the current internal state\ns\nt\n.\nAnswer\n- Answer module is triggered when\nT\nt\n= 1\n.\nFor CNN and DailyMail, a linear projection of GRU outputs is used to predict the answer from candidate entities.\nFor SQuAD, the position of the first and the last word from the answer span are predicted.\nFor Graph Reachability, a logistic regression module is used to predict yes/no as the answer.\nReinforcement Learning\n- For the RL setting, reward at time\nt\n,\nr\nt\n= 1 if\nT\nt\n= 1 and answer is correct. Otherwise\nr\nt\n= 0\nWorkflow\n- Given a passage p, query q and answer a:\nExtract memory using p\nExtract initial hidden state using q\nReasoNet executes all possible episodes that can be enumerated by setting an upper limit on the number of passes.\nThese episodes generate actions and answers that are used to train the ReasoNet.\nResult\nCNN, DailyMail Corpus\nReasoNet outperforms all the baselines which use fixed number of reasoning steps and could benefit by capturing the word alignment signals between query and passage.\nSQuAD\nAt the time of submission, ReasoNet was ranked 2nd on the\nSQuAD leaderboard\nand as of 9th July 2017, it is ranked 4th.\nGraph Reachability Dataset\nReasoNet - Standard ReasoNet as described above.\nReasoNet-Last - Use the prediction from the\nT\nmax\nReasoNet > ReasoNet-Last > Deep LSTM Reader\nReasoNet converges faster than ReasoNet-Last indicating that the terminate gate is useful.\nNotes\nAs such there is nothing discouraging the ReasoNet to make unnecessary passes over the passage.\nIn fact, the modal value of the number of passes = upper bound on the number of passes.\nThis effect is more prominent for large graph indicating that the ReasoNet may try to play safe by performing extra passes.\nIt would be interesting to see if the network can be discouraged from making unnecessary passed by awarding a small negative reward for each pass.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1609.05284"
    },
    "119": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Principled-Detection-of-Out-of-Distribution-Examples-in-Neural-Networks",
        "transcript": "Principled Detection of Out-of-Distribution Examples in Neural Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nPrincipled Detection of Out-of-Distribution Examples in Neural Networks\n2017\n\u2022\nAI\n\u2022\nCV\n\u2022\nOut of Distribution\n\u2022\nSoftmax\n17 Jul 2017\nProblem Statement\nGiven a pre-trained neural network, which is trained using data from some distribution P (referred to as in-distribution data), the task is to detect the examples coming from a distribution Q which is different from P (referred to as out-of-distribution data).\nFor example, if a digit recognizer neural network is trained using MNIST images, an out-of-distribution example would be images of animals.\nNeural Networks can make high confidence predictions even in such cases where the input is unrecognisable or irrelevant.\nThe paper proposes\nODIN\nwhich can detect such out-of-distribution examples without changing the pre-trained model itself.\nLink to the paper\nODIN\nUses 2 major techniques\nTemperature Scaling\nSoftmax classifier for the classification network can be written as:\np\ni\n(x, T) = exp(f\ni\n(x)/T) / sum(exp(f\nj\n(x)/T))\nwhere\nx\nis the input,\np\nis the softmax probability and\nT\nis the temperature scaling parameter.\nIncreasing\nT\n(up to some extent) boosts the performance in distinguishing in-distribution and out-of-distribution examples.\nInput Preprocessing\nAdd small perturbations to the input (image) before feeding it into the network.\nx_perturbed = x - \u03b5 * sign(-\u03b4\nx\nlog(p\ny\n(x, T)))\nwhere \u03b5 is the perturbation magnitude\nThe perturbations are such that softmax scores between in-distribution and out-of-distribution samples become separable.\nGiven an input (image), first perturb the input.\nFeed the perturbed input to the network to get its softmax score.\nIf the softmax score is greater than some threshold, mark the input as in-distribution and feed in the unperturbed version of the input to the network for classification.\nOtherwise, mark the input as out-of-distribution.\nFor detailed mathematical treatment, refer section 6 and appendix in the\npaper\nExperiments\nCode available on\ngithub\nModels\nDenseNet with depth L = 100 and growth rate k = 12\nWide ResNet with depth = 28 and widen factor = 10\nIn-Distribution Datasets\nCIFAR-10\nCIFAR-100\nOut-of-Distribution Datasets\nTinyImageNet\nLSUN\niSUN\nGaussian Noise\nMetrics\nFalse Positive Rate at 95% True Positive Rate\nDetection Error - minimum misclassification probability over all thresholds\nArea Under the Receiver Operating Characteristic Curve\nArea Under the Precision-Recall Curve\nODIN outperforms the baseline across all datasets and all models by a good margin.\nNotes\nVery simple and straightforward approach with theoretical justification under some conditions.\nLimited to examples from Vision so can not judge its applicability for NLP tasks.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.02690"
    },
    "120": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Ask-Me-Anything-Dynamic-Memory-Networks-for-Natural-Language-Processing",
        "transcript": "Ask Me Anything -  Dynamic Memory Networks for Natural Language Processing \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nAsk Me Anything -  Dynamic Memory Networks for Natural Language Processing\n2016\n\u2022\nAI\n\u2022\nAttention\n\u2022\nNLP\n\u2022\nPOS\n\u2022\nQA\n\u2022\nSentiment Analysis\n\u2022\nSOTA\n09 Jul 2017\nIntroduction\nDynamic Memory Networks (DMN) is a neural network based general framework that can be used for tasks like sequence tagging, classification, sequence to sequence and question answering requiring transitive reasoning.\nThe basic idea is that all these tasks can be modelled as question answering task in general and a common architecture could be used for solving them.\nLink to the paper\nArchitecture\nDMN takes as input a document(sentence, story, article etc) and a question which is to be answered given the document.\nInput Module\nConcatenate all the sentences (or facts) in the document and encode them by feeding the word embeddings of the text to a GRU.\nEach time a sentence ends, extract the hidden representation of the GRU till that point and use as the encoded representation of the sentence.\nQuestion Module\nSimilarly, feed the question to a GRU to obtain its representation.\nEpisodic Memory Module\nEpisodic memory consists of an attention mechanism and a recurrent network with which it updates its memory.\nDuring each iteration, the network generates an episode\ne\nby attending over the representation of the sentences, question and the previous memory.\nThe episodic memory is updated using the current episode and the previous memory.\nDepending on the amount of supervision available, the network may perform multiple passes. eg, in the bAbI dataset, some tasks specify how many passes would be needed and which sentence should be attended to in each pass. For others, a fixed number of passes are made.\nMultiple passes allow the network to perform transitive inference.\nAttention Mechanism\nGiven the input representation\nc\n, memory\nm\nand question\nq\n, produce a scalar score using a 2-layer feedforward network, to use as attention mechanism.\nA separate GRU encodes the input representation and weights it by the attention.\nFinal state of the GRU is fed to the answer module.\nAnswer Module\nUse a GRU (initialized with the final state of the episodic module) and at each timestep, feed it the question vector, last hidden state of the same GRU and the previously predicted output.\nTraining\nThere are two possible losses:\nCross-entropy loss of the predicted answer (all datasets)\nCross-entropy loss of the attention supervision (for datasets like bAbI)\nExperiments\nQuestion Answering\nbAbI Dataset\nFor most tasks, DMN either outperforms or performs as good as Memory Networks.\nFor tasks like answering with 2 or 3 supporting facts, DMN lags because of limitation of RNN in modelling long sentences.\nText Classification\nStanford Sentiment Treebank Dataset\nDMN outperforms all the baselines for both binary and fine-grained sentiment analysis.\nSequence Tagging\nWall Street Journal Dataset\nDMN archives state of the art accuracy of 97.56%\nObservations\nMultiple passes help in reasoning tasks but not so much for sentiment/POS tags.\nAttention in the case of 2-iteration DMN is more focused than attention in 1-iteration DMN.\nFor 2-iteration DMN, attention in the second iteration focuses only on relevant words and less attention is paid to words that lose their relevance in the context of the entire document.\nNotes\nIt would be interesting to put some mechanism in place to determine the number of episodes that should be generated before an answer is predicted. A naive way would be to predict the answer after each episode and check if the softmax score of the predicted answer is more than a threshold.\nAlternatively, the softmax score and other information could be fed to a Reinforcement Learning (RL) agent which decided if the document should be read again. So every time an episode is generated, the state is passed to the RL agent which decides if another iteration should be performed. If it decides to predict the answer and correct answer is generated, the agent gets a large +ve reward else a large -ve reward.\nTo discourage unnecessary iterations, a small -ve reward could be given everytime the agent decides to perform another iteration.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.07285"
    },
    "121": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/One-Model-To-Learn-Them-All",
        "transcript": "One Model To Learn Them All \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nOne Model To Learn Them All\n2017\n\u2022\nAI\n\u2022\nAttention\n\u2022\nCV\n\u2022\nMulti Modal\n\u2022\nMulti Model\n\u2022\nNLP\n\u2022\nSpeech\n01 Jul 2017\nThe current trend in deep learning is to design, train and fine tune a separate model for each problem.\nThough multi-task models have been explored, they have been trained for problems from the same domain only and no competitive multi-task, multi-modal models have been proposed.\nThe paper explores the possibility of such a unified deep learning model that can solve different tasks across multiple domains by training concurrently on them.\nLink to the paper\nDesign Philosophy\nSmall, modality-specific subnetworks (called modality nets) should be used to map input data to a joint representation space and back.\nThe joint representation is to be of variable size.\nDifferent tasks from the same domain share the modality net.\nMultiModel networks should use computational blocks from different domains even if they are not specifically designed for the task at hand.\nEg the paper reports that attention and mixture-of-experts (MOE) layers slightly improve the performance on ImageNet even though they are not explicitly needed.\nArchitecture\nMulitModel Network consists of few, small modality nets, an encoder, I/O mixer and an autoregressive decoder.\nEncoder and decoder use the following computational blocks:\nConvolutional Block\nReLU activations on inputs followed by depthwise separable convolutions and layer normalization.\nAttention Block\nMultihead, dot product based attention mechanism.\nMixture-of-Experts (MoE) Block\nConsists of simple feed-forward networks (called experts) and a trainable gating network which selects a sparse combination of experts to process the inputs.\nFor further details, refer the\noriginal paper\n.\nEncoder\nconsists of 6 conv blocks with a MoE block in the middle.\nI/O mixer\nconsists of an attention block and 2 conv blocks.\nDecoder\nconsists of 4 blocks of convolution and attention with a MoE block in the middle.\nModality Nets\nLanguage Data\nInput is the sequence of tokens ending in a termination token.\nThis sequence is mapped to correct dimensionality using a learned embedding.\nFor output, the network takes the decoded output and performs a learned linear mapping followed by Softmax.\nImage\nand\nCategorical Data\nUses residual convolution blocks.\nSimilar to the exit flow for\nXception Network\nAudio Data\n1-d waveform over time or 2-d spectrogram operated upon by stack of 8 residual convolution blocks.\nTasks\nWSJ speech corpus\nImageNet dataset\nCOCO image captioning dataset\nWSJ parsing dataset\nWMT English-German translation corpus\nGerman-English translation\nWMT English-French translation corpus\nGerman-French translation\nExperiments\nThe experimental section is not very rigorous with many details skipped (would probably be added later).\nWhile MultiModel does not beat the state of the art models, it does outperform some recent models.\nJointly trained model performs similar to single trained models on tasks with a lot of data and sometimes outperformed single trained models on tasks with less data (like parsing).\nInterestingly, jointly training the model for parsing task and Imagenet tasks improves the performance of parsing task even though the two tasks are seemingly unrelated.\nAnother experiment was done to evaluate the effect of components (like MoE) on tasks (like Imagenet) which do not explicitly need them. It was observed that either the performance either went down or remained the same when MoE component was removed. This indicates that mixing different components does help to improve performance over multiple tasks.\nBut this observation is not conclusive as a different combination of say the encoder (that does not use MoE) could achieve better performance than one that does. The paper does not explore possibilities like these.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.05137"
    },
    "122": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Two-Too-Simple-Adaptations-of-Word2Vec-for-Syntax-Problems",
        "transcript": "Two/Too Simple Adaptations of Word2Vec for Syntax Problems \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nTwo/Too Simple Adaptations of Word2Vec for Syntax Problems\n2015\n\u2022\nACL 2015\n\u2022\nACL\n\u2022\nAI\n\u2022\nEmbedding\n\u2022\nNLP\n\u2022\nWord Vectors\n26 Jun 2017\nThe paper proposes two variants of Word2Vec model so that it may account for syntactic properties of words and perform better on syntactic tasks like POS tagging and dependency parsing.\nLink to the paper\nIn the original Skip-Gram setting, the model predicts the\n2c\nwords in the context window (\nc\nis the size of the context window). But it uses the same set of parameters whether predicting the word next to the centre word or the word farthest away, thus losing all information about the word order.\nSimilarly, the CBOW (Continuous Bas Of Words) model just adds the embedding of all the surrounding words thereby losing the word order information.\nThe paper proposes to use a set of\n2c\nmatrices each for a different word in the context window for both Skip-Gram and CBOW models.\nThis simple trick allows for accounting of syntactic properties in the word vectors and improves the performance of dependency parsing task and POS tagging.\nThe downside of using this is that now the model has far more parameters than before which increases the training time and needs a large enough corpus to avoid sparse representation.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf"
    },
    "123": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/A-Decomposable-Attention-Model-for-Natural-Language-Inference",
        "transcript": "A Decomposable Attention Model for Natural Language Inference \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nA Decomposable Attention Model for Natural Language Inference\n2016\n\u2022\nEMNLP 2016\n\u2022\nAI\n\u2022\nAttention\n\u2022\nEMNLP\n\u2022\nNatural Language Inference\n\u2022\nNLP\n\u2022\nSOTA\n17 Jun 2017\nIntroduction\nThe paper proposes an attention based mechanism to decompose the problem of Natural Language Inference (NLI) into parallelizable subproblems.\nFurther, it uses much fewer parameters as compared to any other model while obtaining state of the art results.\nLink to the paper\nThe motivation behind the paper is that the tasks like NLI do not require deep modelling of the sentence structure and comparison of local text substructures followed by aggregation can also work very well\nApproach\nGiven two sentences\na\nand\nb\n, the model has to predict whether they have an \u201centailment\u201d relationship, \u201cneutral\u201d relationship or \u201ccontradiction\u201d relationship.\nEmbed\nAll the words are mapped to their corresponding word vector representation. In subsequent steps, \u201cword\u201d refers to the word vector representation of the actual word.\nAttend\nFor each word\ni\nin\na\nand\nj\nin\nb\n, obtain unnormalized attention weights *e(i, j)=F(i)\nT\nF(j) where F is a feed-forward neural network.\nFor\ni\n, compute a \u03b2\ni\nby performing softmax-like normalization of\nj\nusing\ne(i, j)\nas the weight and normalizing for all words\nj\nin\nb\n.\n\u03b2\ni\ncaptures the subphrase in\nb\nthat is softly aligned to\na\n.\nSimilarly compute \u03b1\nj\nfor\nj\n.\nCompare\nCreate two set of comparison vectors, one for\na\nand another for\nb\nFor\na\n,\nv\n1, i\n= G(concatenate(i, \u03b2\ni\n)).\nSimilarly for\nb\n,\nv\n2, j\n= G(concatenate(j, \u03b1\nj\n))\nG is another feed-forward neural network.\nAggregate\nAggregate over the two set of comparison vectors to obtain\nv\n1\nand\nv\n2\n.\nFeed the aggregated results through the final classifier layer.\nMulti-class cross-entropy loss function.\nThe paper also explains how this representation can be augmented using intra-sentence attention to the model compositional relationship between words.\nComputational Complexity\nComputationally, the proposed model is asymptotically as good as LSTM with attention.\nAssuming that dimensionality of word vectors > length of the sentence (reasonable for the given SNLI dataset), the model is asymptotically as good as regular LSTM.\nFurther, the model has the advantage of being parallelizable.\nExperiment\nOn Stanford Natural Language Inference (SNLI) dataset, the proposed model achieves the state of the art results even when it uses an order of magnitude lesser parameters than the next best model.\nAdding intra-sentence attention further improve the test accuracy by 0.5 percent.\nNotes\nA similar approach could be tried on paraphrase detection problem as even that problem should not require very deep sentence representation.\nQuora Duplicate Question Detection Challenege\nwould have been an ideal dataset but it has a lot of out-of-vocabulary information related to named entities which need to be accounted for.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.01933"
    },
    "124": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/A-Fast-and-Accurate-Dependency-Parser-using-Neural-Networks",
        "transcript": "A Fast and Accurate Dependency Parser using Neural Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nA Fast and Accurate Dependency Parser using Neural Networks\n2014\n\u2022\nEMNLP 2014\n\u2022\nDependency Parsing\n\u2022\nEMNLP\n\u2022\nNLP\n03 Jun 2017\nIntroduction\nThe paper proposes a neural network classifier to perform transition-based dependency parsing using dense vector representation for the features.\nEarlier approaches used a large, manually designed sparse feature vector which took a lot of time and effort to compute and was often incomplete.\nLink to the paper\nDescription of the system\nThe system described in the paper uses\narc-standard\nsystem\n(a greedy, transition-based dependency parsing system).\nWords, POS tags and arc labels are represented as d dimensional vectors.\nS\nw\n, S\nt\n, S\nl\ndenote the set of words, POS and labels respectively.\nNeural network takes as input selected words from the 3 sets and uses a single hidden layer followed by Softmax which models the different actions that can be chosen by the arc-standard system.\nUses a cube activation function to allow interaction between features coming from the set of words, POS and labels in the first layer itself. These features come from different embeddings and are not related as such.\nUsing separate embedding for POS tags and labels allow for capturing aspects like NN (singular noun) should be closer to NNS (plural noun) than DT (determiner).\nInput to the network contains words on the stack and buffer and their left and right children (read upon transition-based parsing), their labels and corresponding arc labels.\nOutput generated by the system is the action to be taken (transition to be performed) when reading each word in the input.\nThis sequential and deterministic nature of the input-output mapping allows the problem to be modelled as a supervised learning problem and a cross entropy loss can be used.\nL2-regularization term is also added to the loss.\nDuring inference, a greedy decoding strategy is used and transition with the highest score is chosen.\nThe paper mentions a pre-computation trick where matrix computation of most frequent top 10000 words is performed beforehand and cached.\nExperiments\nDataset\nEnglish Penn Treebank (PTB)\nChinese Penn Treebank (CTB)\nTwo dependency representations used:\nCoNLL Syntactic Dependencies (CD)\nStanford Basic Dependencies (SD)\nMetrics:\nUnlabeled Attached Scores (UAS)\nLabeled Attached Scores (LAS)\nBenchmarked against:\nGreedy arc-eager parser\nGreedy arc-standard parser\nMalt-Parser\nMSTParser\nResults\nThe system proposed in the paper outperforms all other parsers in both speed and accuracy.\nAnalysis\nCube function gives a 0.8-1.2% improvement over tanh.\nPretained embeddings give 0.7-1.7% improvement over training embeddings from scratch.\nUsing POS and labels gives an improvement of 1.7% and 0.4% respectively.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf"
    },
    "125": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Neural-Module-Networks",
        "transcript": "Neural Module Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nNeural Module Networks\n2016\n\u2022\nCVPR 2016\n\u2022\nAI\n\u2022\nCV\n\u2022\nCVPR\n\u2022\nDataset\n\u2022\nNeural Module Network\n\u2022\nNLP\n\u2022\nVQA\n23 May 2017\nIntroduction\nFor the task of\nVisual Question Answering\n, decompose a question into its linguistic substructures and train a neural network module for each substructure.\nJointly train the modules and dynamically compose them into deep networks which can learn to answer the question.\nStart by analyzing the question and decide what logical units are needed to answer the question and what should be the relationship between them.\nThe paper also introduces a new dataset for Visual Question Answering which has challenging, highly compositional questions about abstract shapes.\nLink to the paper\nInspiration\nQuestions tend to be compositional.\nDifferent architectures are needed for different tasks - CNNs for object detection, RNNs for counting.\nRecurrent and Recursive Neural Networks also use the idea of a different network graph for each input.\nNeural Module Network for VQA\nTraining samples of form\n(w, x, y)\nw\n- Natural Language Question\nx\n- Images\ny\n- Answer\nModel specified by collection of modules\n{m}\nand a network layout predictor\nP\n.\nModel instantiates a network based on\nP(w)\nand uses that to encode a distribution\nP(y|w, x, model_params)\nModules\nFind: Finds objects of interest.\nTransform: Shift regions of attention.\nCombine: Merge two attention maps into a single one.\nDescribe: Map a pair of attention and input image to a distribution over the labels.\nMeasure: Map attention to a distribution over the labels.\nNatural Language Question to Networks\nMap question to the layout which specifies the set of modules and connections between them.\nAssemble the final network using the layout.\nParse the input question to obtain set of dependencies and obtain a representation similar to combinatory logic.\neg \u201cwhat is the colour of the truck?\u201d becomes \u201ccolour(truck)\u201d\nThe symbolic representation is mapped to a layout:\nAll leaves become\nfind\nmodule.\nAll internal nodes become\ntransform/combine\nmodule.\nAll root nodes become\ndescribe/measure\nmodule.\nAnswering Natural Language Question\nFinal model combines output from a simple LSTM question encoder with the output of the neural module network.\nThis helps in modelling the syntactic and semantic regularities of the question.\nExperiments\nSince some modules are updated more frequently than others, adaptive per weight learning rates are better.\nThe paper introduces a small SHAPES datasets (64 images and 244 unique questions per image).\nNeural Module Network achieves a score of 90% on SHAPES dataset while VIS + LSTM baseline achieves an accuracy of 65.3%.\nEven on natural images (VQA dataset), the neural module network outperforms the VIS + LSTM baseline.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.02799"
    },
    "126": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Making-the-V-in-VQA-Matter-Elevating-the-Role-of-Image-Understanding-in-Visual-Question-Answering",
        "transcript": "Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nMaking the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering\n2017\n\u2022\nAI\n\u2022\nCV\n\u2022\nDataset\n\u2022\nNLP\n\u2022\nVQA\n14 May 2017\nProblem Statement\nStandard VQA models benefit from the inherent bias in the structure of the world and the language of the question.\nFor example, if the question starts with \u201cDo you see a \u2026\u201d, it is more likely to be \u201cyes\u201d than \u201cno\u201d.\nTo truly assess the capability of any VQA system, we need to have evaluation tasks that require the use of both the visual and the language modality.\nThe authors present a balanced version of\nVQA dataset\nwhere each question in the dataset is associated with a pair of similar images such that the same question would give different answers on the two images.\nThe proposed data collection procedure enables the authors to develop a novel interpretable model which, given an image and a question, identifies an image that is similar to the original image but has a different answer to the same question thereby building trust for the system.\nLink to the paper\nDataset Collection\nGiven an (image, question, answer) triplet (I, Q, A) from the VQA dataset, a human worker (on AMT) is asked to identify an image I\u2019 which is similar to I but for which the answer to question Q is A\u2019 (different from A).\nTo facilitate the search for I\u2019, the worker is shown 24 nearest-neighbor images of I (based on VGGNet features) and is asked to choose the most similar image to I, for which Q makes sense and answer for Q is different than A. In case none of the 24 images qualifies, the worker may select \u201cnot possible\u201d.\nIn the second round, the workers were asked to answer Q for I\u2019.\nThis 2-stage protocol results in a significantly more balanced dataset than the previous dataset.\nObservation\nState-of-the-art models trained on unbalanced VQA dataset perform significantly worse on the new, balanced dataset indicating that those models benefitted from the language bias in the older dataset.\nTraining on balanced dataset improves performance on the unbalanced dataset.\nFurther, the VQA model, trained on the balanced dataset, learns to differentiate between otherwise similar images.\nCounter-example Explanations\nGiven an image and a question, the model not only answers the question, it also provides an image (from the k nearest neighbours of I, based on VGGNet features) which is similar to the input image but for which the model would have given different answer for the same image.\nSupervising signal is provided by the data collection procedure where humans pick the image I\u2019 from the same set of candidate images.\nFor each image in the candidate set, compute the inner product of question-image embedding and answer embedding.\nThe K inner product values are passed through a fully connected layer to generate K scores.\nTrained with pairwise hinge ranking loss so that the score of the human picked image is higher than the score of all other images by a margin of M (hyperparameter).\nThe proposed explanation model achieves a recall@5 of 43.49%\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.00837"
    },
    "127": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Conditional-Similarity-Networks",
        "transcript": "Conditional Similarity Networks \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nConditional Similarity Networks\nCV\n\u2022\nAI\n\u2022\nCVPR\n\u2022\nEmbedding\n\u2022\nCVPR 2017\n\u2022\n2017\n07 May 2017\nProblem Statement\nA common way of measuring image similarity is to embed them into feature spaces where distance acts as a proxy for similarity.\nBut this feature space can capture one (or a weighted combination) of the many possible notions of similarity.\nWhat if contracting notions of similarity could be captured at the same time - in terms of semantically distinct subspaces.\nThe paper proposes a new architecture called as Conditional Similarity Networks (CSNs) which learns a disentangled embedding such that the features, for different notions of similarity, are encoded into separate dimensions.\nIt jointly learns masks (or feature extractors) that select and reweights relevant dimensions to induce a subspace that encodes a specific notion of similarity.\nLink to the paper\nConditional Similarity Networks\nGiven an image,\nx\n, learn a non-linear feature embedding\nf(x)\nsuch that for any 2 images\nx\n1\nand\nx\n2\n, the euclidean distance between\nf(x\n1\n)\nand\nf(x\n2\n)\nreflects their similarity.\nConditional Similarity Triplets\nGiven a triplet of images\n(x\n1\n, x\n2\n, x\n3\n)\nand a condition\nc\n(the notion of similarity), an oracle (say crowd) is used to determmine if\nx\n1\nis more similar to\nx\n2\nor\nx\n3\nas per the given criteria\nc\n.\nIn general, for images\ni, j, l\n, the triplet\nt\nis ordered {i, j, l | c} if\ni\nis more similar to\nj\nthan\nl\n.\nLearning From Triplets\nDefine a loss function\nL\nT\n()\nto model the similarity structure over the triplets.\nL\nT\n(i, j, l) = max{0, D(i, j) - D(i, l) + h}\nwhere\nD\nis the euclidean distance function and\nh\nis the similarity scalar margin to prevent trivial solutions.\nTo model conditional similarities, masks\nm\nare defined as\nm = \u03c3(\u03b2)\nwhere \u03c3 is the RELU unit and \u03b2 is a set of parameters to be learnt.\nm\nc\ndenotes the selection of the c-th mask column from feature vector. It thus acts as an element-wise gating function which selects the relevant dimensions of the embedding to attend to a particular similarity concept.\nThe euclidean function\nD\nnow computes the masked distance (\nf(i, c)m\nc\n) between the two given images.\nTwo regularising terms are also added - L2 norm for\nD\nand L1 norm for\nm\n.\nExperiments\nDatasets\nFonts dataset by Bernhardsson\n3.1 million 64 by 64-pixel grey scale images.\nZappos50k shoe dataset\nContains 50,000 images of individual richly annotated shoes.\nCharacteristics of interest:\nType of the shoes (i.e., shoes, boots, sandals or slippers)\nSuggested gender of the shoes (i.e., for women, men, girls or boys)\nHeight of the shoes\u2019 heels (0 to 5 inches)\nClosing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up)\nModels\nInitial model for the experiments is a ConvNet pre-trained on ImageNet\nStandard Triplet Network\nLearn from all available triplets jointly as if they have the same notion of similarity.\nSet of Task Specific Triplet Networks\nTrain n separate triplet networks such that each is trained on a single notion of similarity.\nNeeds far more parameters and compute.\nConditional Similarity Networks - fixed disjoint masks\nIn this version, only the convolutional filters and the embedding is learnt and masks are predefined to be disjoint.\nAims to learn a fully disjoint embedding.\nConditional Similarity Networks - learned masks\nLearns all the components - conv filters, embedding and the masks.\nRefer paper for details on hyperparameters.\nResults\nVisual exploration of the learned subspaces (t-sne visualisation) show that network successfully disentangles different features in the embedded vector space.\nThe learned masks are very sparse and share dimensions. This shows that CSNs may learn to only use the required number of dimensions thereby doing away with the need of picking the right size of embedding.\nOrder of performance:\nCSNs with learned masks > CSNs with fixed masks > Task-specific networks > standard triplet network.\nThough CSNs with learned masks require more training data.\nCSNs also outperform Standard Triplet Network when used as off the shelf features for (brand) classification task and is very close to the performance of ResNet trained on ImageNet.\nThis shows that while CSN retained most of the information in the original network, the training mechanism of Standard Triplet Network hurts the underlying conv features and their generalising capability\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://vision.cornell.edu/se3/conditional-similarity-networks/"
    },
    "128": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/Simple-Baseline-for-Visual-Question-Answering",
        "transcript": "Simple Baseline for Visual Question Answering \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nSimple Baseline for Visual Question Answering\nCV\n\u2022\nVQA\n\u2022\nAI\n\u2022\nNLP\n\u2022\n2015\n28 Apr 2017\nProblem Statement\nVQA Task: Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.\nThe paper attempts to fine tune the simple baseline method of Bag-of-Words + Image features (iBOWIMG) to make it competitive against more sophisticated LSTM models.\nLink to the paper\nModel\nVQA modelled as a classification task where the system learns to choose among one of the top k most prominent answers.\nText Features\n- Convert input question to a one-hot vector and then transform to word vectors using a word embedding.\nImage Features\n- Last layer activations from GoogLeNet.\nText features are concatenated with image features and fed into a softmax.\nDifferent learning rates and weight clipping for word embedding layer and softmax layer with the learning rate for embedding layer much higher than that of softmax layer.\nResults\niBOWIMG model reports an accuracy of 55.89% for Open-ended questions and 61.97% for Multiple-Choice questions which is comparable to the performance of other, more sophisticated models.\nInterpretation of the model\nSince the model is very simple, it is possible to interpret the model to know what exactly is the model learning. This is the greatest strength of the paper even though the model is very simple and naive.\nThe model attempts to memorise the correlation between the answer class and the informative words (in the question) and image features.\nQuestion words generally can influence the answer given the bias in images occurring in COCO dataset.\nGiven the simple linear transformation being used, it is possible to quantify the importance of each single words (in the question) to the answer.\nThe paper uses the Class Activation Mapping (CAM) approach (which uses the linear relation between softmax and final image feature map) to highlight the informative image regions relevant to the predicted answer.\nWhile the results reported by the paper are not themselves so significant, the described approach provides a way to interpret the strengths and weakness of different VQA datasets.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "http://arxiv.org/pdf/1512.02167.pdf"
    },
    "129": {
        "sourceUrl": "https://shagunsodhani.com/papers-I-read/VQA-Visual-Question-Answering",
        "transcript": "VQA-Visual Question Answering \u00b7 Papers I Read\nI am trying a new initiative -\nA Paper A Week\n. This blog will hold all the notes and summaries.\nHome\nArchive\nTags\nGitHub project\nFeed\n\u00a9 2021. All rights reserved.\nPapers I Read\nNotes and Summaries\nVQA-Visual Question Answering\n2015\n\u2022\nICCV 2015\n\u2022\nAI\n\u2022\nCV\n\u2022\nDataset\n\u2022\nICCV\n\u2022\nNLP\n\u2022\nVQA\n27 Apr 2017\nProblem Statement\nGiven an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.\nLink to the paper\nVQA Challenge and Workshop\nThe authors organise an annual challenge and workshop to discuss the state-of-the-art methods and best practices in this domain.\nInterestingly, the second version is starting on 27th April 2017 (today).\nBenefits over tasks like image captioning:\nSimple,\nn-gram\nstatistics based methods are not sufficient.\nRequires the system to blend in different aspects of knowledge - object detection, activity recognition, commonsense reasoning etc.\nSince only short answers are expected, evaluation is easier.\nDataset\nCreated a new dataset of 50000 realistic, abstract images.\nUsed AMT to crowdsource the task of collecting questions and answers for MS COCO dataset (>200K images) and abstract images.\nThree questions per image and ten answers per question (along with their confidence) were collected.\nThe entire dataset contains over 760K questions and 10M answers.\nThe authors also performed an exhaustive analysis of the dataset to establish its diversity and to explore how the content of these question-answers differ from that of standard image captioning datasets.\nHighlights of data collection methodology\nEmphasis on questions that require an image, and not just common sense, to be answered correctly.\nWorkers were shown previous questions when writing new questions to increase diversity.\nAnswers collected from multiple users to account for discrepancies in answers by humans.\nTwo modalities supported:\nOpen-ended\n- produce the answer\nmultiple-choice\n- select from a set of options provided (18 options comprising of popular, plausible, random and ofc correct answer)\nHighlights from data analysis\nMost questions range from four to ten words while answers range from one to three words.\nAround 40% questions are \u201cyes/no\u201d questions.\nSignificant (>80%) inter-human agreement for answers.\nThe authors performed a study where human evaluators were asked to answer the questions without looking at the images.\nFurther, they performed a study where evaluators were asked to label if a question could be answered using common sense and what was the youngest age group, they felt, could answer the question.\nThe idea was to establish that a sufficient number of questions in the dataset required more than just common sense to answer.\nBaseline Models\nrandom\nselection\nprior (\u201cyes\u201d)\n- always answer as yes.\nper Q-type prior\n- pick the most popular answer per question type.\nnearest neighbor\n- find the k nearest neighbors for the given (image, question) pair.\nMethods\n2-channel model (using vision and language models) followed by softmax over (K = 1000) most frequent answers.\nImage Channel\nI\n- Used last hidden layer of VGGNet to obtain 4096-dim image embedding.\nnorm I\n- : l2 normalized version of\nI\n.\nQuestion Channel\nBoW Q\n- Bag-of-Words representation for the questions using the top 1000 words plus the top 1- first, second and third words of the questions.\nLSTM Q\n- Each word is encoded into 300-dim vectors using fully connected + tanh non-linearity. These embeddings are fed to an LSTM to obtain 1024d-dim embedding.\nDeeper LSTM Q\n- Same as LSTM Q but uses two hidden layers to obtain 2048-dim embedding.\nMulti-Layer Perceptron (MLP)\n- Combine image and question embeddings to obtain a single embedding.\nBoW Q + I\nmethod - concatenate BoW Q and I embeddings.\nLSTM Q + I, deeper LSTM Q + norm I\nmethods - image embedding transformed to 1024-dim using a FC layer and tanh non-linearity followed by element-wise multiplication of image and question vectors.\nPass combined embedding to an MLP - FC neural network with 2 hidden layers (1000 neurons and 0.5 dropout) with tanh, followed by softmax.\nCross-entropy loss with VGGNet parameters frozen.\nResults\nDeeper LSTM Q + norm I is the best model with 58.16% accuracy on open-ended dataset and 63.09% on multiple-choice but far behind the human evaluators (>80% and >90% respectively).\nThe best model performs well for answers involving common visual objects but performs poorly for answers involving counts.\nVision only model performs even worse than the model which always produces \u201cyes\u201d as the answer.\nRelated Posts\nDeep Neural Networks for YouTube Recommendations\n22 Mar 2021\nThe Tail at Scale\n15 Mar 2021\nPractical Lessons from Predicting Clicks on Ads at Facebook\n08 Mar 2021\nPlease enable JavaScript to view the\ncomments powered by Disqus.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1505.00468v6"
    }
}