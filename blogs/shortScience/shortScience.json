{
    "0": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/TamarLA16",
        "transcript": "Originally posted on my Github repo [paper-notes](https://github.com/karpathy/paper-notes/blob/master/vin.md).\n\n# Value Iteration Networks\n\nBy Berkeley group: Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel\n\nThis paper introduces a poliy network architecture for RL tasks that has an embedded differentiable *planning module*, trained end-to-end. It hence falls into a category of fun papers that take explicit algorithms, make them differentiable, embed them in a larger neural net, and train everything end-to-end.\n\n**Observation**: in most RL approaches the policy is a \"reactive\" controller that internalizes into its weights actions that historically led to high rewards.\n\n**Insight**: To improve the inductive bias of the model, embed a specifically-structured neural net planner into the policy. In particular, the planner runs the value Iteration algorithm, which can be implemented with a ConvNet. So this is kind of like a model-based approach trained with model-free RL, or something. Lol.\n\nNOTE: This is very different from the more standard/obvious approach of learning a separate neural network environment dynamics model (e.g. with regression), fixing it, and then using a planning algorithm over this intermediate representation. This would not be end-to-end because we're not backpropagating the end objective through the full model but rely on auxiliary objectives (e.g. log prob of a state given previous state and action when training a dynamics model), and in practice also does not work well.\n\nNOTE2: A recurrent agent (e.g. with an LSTM policy), or a feedforward agent with a sufficiently deep network trained in a model-free setting has some capacity to learn planning-like computation in its hidden states. However, this is nowhere near as explicit as in this paper, since here we're directly \"baking\" the planning compute into the architecture. It's exciting.\n\n\n## Value Iteration\n\nValue Iteration is an algorithm for computing the optimal value function/policy $V^*, \\pi^*$ and involves turning the Bellman equation into a recurrence:\n\n![Screen Shot 2016-08-13 at 3.26.04 PM](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/vin/Screen%20Shot%202016-08-13%20at%203.26.04%20PM.png)\nThis iteration converges to $V^*$ as $n \\rightarrow \\infty$, which we can use to behave optimally (i.e. the optimal policy takes actions that lead to the most rewarding states, according to $V^*$). \n\n\n\n## Grid-world domain\n\nThe paper ends up running the model on several domains, but for the sake of an effective example consider the grid-world task where the agent is at some particular position in a 2D grid and has to reach a specific goal state while also avoiding obstacles. Here is an example of the toy task:\n\n ![Screen Shot 2016-08-13 at 4.43.04 PM](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/vin/Screen%20Shot%202016-08-13%20at%204.43.04%20PM.png)\n\nThe agent gets a reward +1 in the goal state, -1 in obstacles (black), and -0.01 for each step (so that the shortest path to the goal is an optimal solution).\n\n\n\n## VIN model\n\nThe agent is implemented in a very straight-forward manner as a single neural network trained with TRPO (Policy Gradients with a KL constraint on predictive action distributions over a batch of trajectories). So the only loss function used is to maximize expected reward, as is standard in model-free RL. However, the policy network of the agent has a very specific structure since it (internally) runs value iteration.\n\nFirst, there's the core Value Iteration **(VI) Module** which runs the recurrence formula (reproducing again):\n\n![Screen Shot 2016-08-13 at 3.26.04 PM](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/vin/Screen%20Shot%202016-08-13%20at%203.26.04%20PM.png)\nThe input to this recurrence are the two arrays R (the reward array, reward for each state) and P (the dynamics array, the probabilities of transitioning to nearby states with each action), which are of course unknown to the agent, but can be predicted with neural networks as a function of the current state. This is a little funny because the networks take a _particular_ state **s** and are internally (during the forward pass) predicting the rewards and dynamics for all states and actions in the entire environment. Notice, extremely importantly and once again, that at no point are the reward and dynamics functions explicitly regressed to the observed transitions in the environment. They are just arrays of numbers that plug into value iteration recurrence module.\n\nBut anyway, once we have **R,P** arrays, in the Grid-world above due to the local connectivity, value iteration can be implemented with a repeated application of convolving **P** over **R**, as these filters effectively *diffuse* the estimated reward function (**R**) through the dynamics model (**P**), followed by max pooling across the actions. If **P** is a not a function of the state, it would simply be the filters in the Conv layer. Notice that posing this as convolution also assumes that the env dynamics are position-invariant. See the diagram below on the right:![Screen Shot 2016-08-13 at 4.58.42 PM](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/vin/Screen%20Shot%202016-08-13%20at%204.58.42%20PM.png)\n\nOnce the array of numbers that we interpret as holding the estimated $V^*$ is computed after running **K** steps of the recurrence (K is fixed beforehand. For example for a 16x16 map it is 20, since that's a bit more than the amount of steps needed to diffuse rewards across the entire map), we \"pluck out\" the state-action values $Q(s,.)$ at the state the agent happens to currently be in (by an \"attention\" operator $\\psi$), and (optionally) append these Q values to the feedforward representation of the current state $\\phi(s)$, and finally predicting the action distribution.\n\n\n\n## Experiments\n\n**Baseline 1**: A vanilla ConvNet policy trained with TRPO. [(50 3x3 filters)\\*2, 2x2 max pool, (100 3x3 filters)\\*3, 2x2 max pool, FC(100), FC(4), Softmax].\n\n**Baseline 2**: A fully convolutional network (FCN), 3 layers (with a filter that spans the whole image), of 150, 100, 10 filters. i.e. slightly different and perhaps a bit more domain-appropriate ConvNet architecture.\n\n**Curriculum** is used during training where easier environments are trained on first. This is claimed to work better but not quantified in tables. Models are trained with TRPO, RMSProp, implemented in Theano.\n\nResults when training on **5000** random grid-world instances (hey isn't that quite a bit low?):![Screen Shot 2016-08-13 at 5.47.23 PM](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/vin/Screen%20Shot%202016-08-13%20at%205.47.23%20PM.png)\n\nTLDR VIN generalizes better.\n\nThe authors also run the model on the **Mars Rover Navigation** dataset (wait what?), a **Continuous Control** 2D path planning dataset, and the **WebNav Challenge**, a language-based search task on a graph (of a subset of Wikipedia). Skipping these because they don't add _too_ much to the core cool idea of the paper.\n\n## Misc\n\n**The good**: I really like this paper because the core idea is cute (the planner is *embedded* in the policy and trained end-to-end), novel (I don't think I saw this idea executed on so far elsewhere), the paper is well-written and clear, and the supplementary materials are thorough.\n\n**On the approach**: Significant challenges remain to make this approach more practicaly viable, but it also seems that much more exciting followup work can be done in this framework. I wish the authors discussed this more in the conclusion. In particular, it seems that one has to explicitly encode the environment connectivity structure in the internal model $\\bar{M}$. How much of a problem is this and what could be done about it? Or how could we do the planning in more higher-level abstract spaces instead of the actual low-level state space of the problem? Also, it seems that a potentially nice feature of this approach is that the agent could dynamically \"decide\" on a reward function at runtime, and the VI module can diffuse it through the dynamics and hence do the planning.  A potentially interesting outcome is that the agent could utilize this kind of computation so that  an LSTM controller could learn to \"emit\" reward function subgoals and the VI planner computes how to meet them. A nice/clean division of labor one could hope for in principle.\n\n**The experiments**. Unfortunately, I'm not sure why the authors preferred breadth of experiments and sacrificed depth of experiments. I would have much preferred a more in-depth analysis of the gridworld environment. For instance:\n\n- Only 5,000 training examples are used for training, which seems very little. Presumable, the baselines get stronger as you increase the number of training examples?\n- Lack of visualizations: Does the model actually learn the \"correct\" rewards **R** and dynamics **P**? The authors could inspect these manually and correlate them to the actual model. This would have been reaaaallllyy cool. I also wouldn't expect the model to exactly learn these, but who knows.\n- How does the model compare to the baselines in the number of parameters? or FLOPS? It seems that doing VI for 30 steps at each single iteration of the algorithm should be quite expensive.\n- The authors should study the performance as a function of the number of recurrences **K**. A particularly funny experiment would be K = 1, where the model would be effectively predicting **V*** directly, without planning. What happens?\n- If the output of VI $\\psi(s)$ is concatenated to the state parameters, are these Q values actually used? What if all the weights to these numbers are zero in the trained models?\n- Why do the authors only evaluate success rate when the training criterion is expected reward?\n\n\n\nOverall a very cute idea, well executed as a first step and well explained, with a bit of unsatisfying lack of depth in the experiments in favor of breadth that doesn't add all that much.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.02867"
    },
    "1": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/VinyalsBLKW16",
        "transcript": "Originally posted on my Github [paper-notes](https://github.com/karpathy/paper-notes/blob/master/matching_networks.md) repo.\n\n# Matching Networks for One Shot Learning\n\nBy DeepMind crew: **Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra**\n\nThis is a paper on **one-shot** learning, where we'd like to learn a class based on very few (or indeed, 1) training examples. E.g. it suffices to show a child a single giraffe, not a few hundred thousands before it can recognize more giraffes.\n\nThis paper falls into a category of *\"duh of course\"* kind of paper, something very interesting, powerful, but somehow obvious only in retrospect. I like it.\n\nSuppose you're given a single example of some class and would like to label it in test images.\n\n- **Observation 1**: a standard approach might be to train an Exemplar SVM for this one (or few) examples vs. all the other training examples - i.e. a linear classifier. But this requires optimization.\n- **Observation 2:** known non-parameteric alternatives (e.g. k-Nearest Neighbor) don't suffer from this problem. E.g. I could immediately use a Nearest Neighbor to classify the new class without having to do any optimization whatsoever. However, NN is gross because it depends on an (arbitrarily-chosen) metric, e.g. L2 distance. Ew.\n- **Core idea**: lets train a fully end-to-end nearest neighbor classifer!![Screen Shot 2016-08-07 at 10.08.44 PM](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2010.08.44%20PM.png)\n\n## The training protocol\n\nAs the authors amusingly point out in the conclusion (and this is the *duh of course* part), *\"one-shot learning is much easier if you train the network to do one-shot learning\"*. Therefore, we want the test-time protocol (given N novel classes with only k examples each (e.g. k = 1 or 5), predict new instances to one of N classes) to exactly match the training time protocol.\n\nTo create each \"episode\" of training from a dataset of examples then:\n\n1. Sample a task T from the training data, e.g. select 5 labels, and up to 5 examples per label (i.e. 5-25 examples).\n2. To form one episode sample a label set L (e.g. {cats, dogs}) and then use L to sample the support set S and a batch B of examples to evaluate loss on.\n\nThe idea on high level is clear but the writing here is a bit unclear on details, of exactly how the sampling is done.\n\n## The model\n\nI find the paper's model description slightly wordy and unclear, but basically we're building a **differentiable nearest neighbor++**. The output \\hat{y} for a test example \\hat{x} is computed very similar to what you might see in Nearest Neighbors:![Screen Shot 2016-08-07 at 11.14.26 PM](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.14.26%20PM.png)\nwhere **a** acts as a kernel, computing the extent to which \\hat{x} is similar to a training example x_i, and then the labels from the training examples (y_i) are weight-blended together accordingly. The paper doesn't mention this but I assume for classification y_i would presumbly be one-hot vectors.\n\nNow, we're going to embed both the training examples x_i and the test example \\hat{x}, and we'll interpret their inner products (or here a cosine similarity) as the \"match\", and pass that through a softmax to get normalized mixing weights so they add up to 1. No surprises here, this is quite natural:\n\n![Screen Shot 2016-08-07 at 11.20.29 PM](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.20.29%20PM.png)\nHere **c()** is cosine distance, which I presume is implemented by normalizing the two input vectors to have unit L2 norm and taking a dot product. I assume the authors tried skipping the normalization too and it did worse? Anyway, now all that's left to define is the function **f** (i.e. how do we embed the test example into a vector) and the function **g** (i.e. how do we embed each training example into a vector?).\n\n**Embedding the training examples.** This (the function **g**) is a bidirectional LSTM over the examples:\n\n ![Screen Shot 2016-08-07 at 11.57.10 PM](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.57.10%20PM.png)\n\ni.e. encoding of i'th example x_i is a function of its \"raw\" embedding g'(x_i) and the embedding of its friends, communicated through the bidirectional network's hidden states. i.e. each training example is a function of not just itself but all of its friends in the set. This is part of the ++ above, because in a normal nearest neighbor you wouldn't change the representation of an example as a function of the other data points in the training set.\n\nIt's odd that the **order** is not mentioned, I assume it's random? This is a bit gross because order matters to a bidirectional LSTM; you'd get different embeddings if you permute the examples. \n\n**Embedding the test example.** This (the function **f**) is a an LSTM that processes for a fixed amount (K time steps) and at each point also *attends* over the examples in the training set. The encoding is the last hidden state of the LSTM. Again, this way we're allowing the network to change its encoding of the test example as a function of the training examples. Nifty: ![Screen Shot 2016-08-08 at 12.11.15 AM](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2012.11.15%20AM.png)\n\nThat looks scary at first but it's really just a vanilla LSTM with attention where the input at each time step is constant (f'(\\hat{x}), an encoding of the test example all by itself) and the hidden state is a function of previous hidden state but also a concatenated readout vector **r**, which we obtain by attending over the encoded training examples (encoded with **g** from above).\n\nOh and I assume there is a typo in equation (5), it should say r_k = \u2026 without the -1 on LHS. \n\n\n\n## Experiments\n\n**Task**: N-way k-shot learning task. i.e. we're given k (e.g. 1 or 5) labelled examples for N classes that we have not previously trained on and asked to classify new instances into he N classes.\n\n**Baselines:** an \"obvious\" strategy of using a pretrained ConvNet and doing nearest neighbor based on the codes. An option of finetuning the network on the new examples as well (requires training and careful and strong regularization!).\n\n**MANN** of Santoro et al. [21]: Also a DeepMind paper, a fun NTM-like Meta-Learning approach that is fed a sequence of examples and asked to predict their labels.\n\n**Siamese network** of Koch et al. [11]: A siamese network that takes two examples and predicts whether they are from the same class or not with logistic regression. A test example is labeled with a nearest neighbor: with the class it matches best according to the siamese net (requires iteration over all training examples one by one). Also, this approach is less end-to-end than the one here because it requires the ad-hoc nearest neighbor matching, while here the *exact* end task is optimized for. It's beautiful.\n\n\n\n### Omniglot experiments \n\n### ![Screen Shot 2016-08-08 at 10.21.45 AM](https://github.com/karpathy/paper-notes/raw/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2010.21.45%20AM.png)\n\nOmniglot of [Lake et al. [14]](http://www.cs.toronto.edu/~rsalakhu/papers/LakeEtAl2015Science.pdf) is a MNIST-like scribbles dataset with 1623 characters with 20 examples each.\n\nImage encoder is a CNN with 4 modules of [3x3 CONV 64 filters, batchnorm, ReLU, 2x2 max pool]. The original image is claimed to be so resized from original 28x28 to 1x1x64, which doesn't make sense because factor of 2 downsampling 4 times is reduction of 16, and 28/16 is a non-integer >1. I'm assuming they use VALID convs?\n\nResults: ![Screen Shot 2016-08-08 at 10.27.46 AM](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2010.27.46%20AM.png)\n\nMatching nets do best. Fully Conditional Embeddings (FCE) by which I mean they the \"Full Context Embeddings\" of Section 2.1.2 instead are not used here, mentioned to not work much better. Finetuning helps a bit on baselines but not with Matching nets (weird).\n\nThe comparisons in this table are somewhat confusing:\n\n- I can't find the MANN numbers of 82.8% and 94.9% in their paper [21]; not clear where they come from. E.g. for 5 classes and 5-shot they seem to report 88.4% not 94.9% as seen here. I must be missing something.\n- I also can't find the numbers reported here in the Siamese Net [11] paper. As far as I can tell in their Table 2 they report one-shot accuracy, 20-way classification to be 92.0, while here it is listed as 88.1%?\n- The results of Lake et al. [14] who proposed Omniglot are also missing from the table. If I'm understanding this correctly they report 95.2% on 1-shot 20-way, while matching nets here show 93.8%, and humans are estimated at 95.5%. That is, the results here appear weaker than those of Lake et al., but one should keep in mind that the method here is significantly more generic and does not make any assumptions about the existence of strokes, etc., and it's a simple, single fully-differentiable blob of neural stuff.\n\n(skipping ImageNet/LM experiments as there are few surprises)\n\n## Conclusions\n\nGood paper, effectively develops a differentiable nearest neighbor trained end-to-end. It's something new, I like it!\n\nA few concerns: \n\n- A bidirectional LSTMs (not order-invariant compute) is applied over sets of training examples to encode them. The authors don't talk about the order actually used, which presumably is random, or mention this potentially unsatisfying feature. This can be solved by using a recurrent attentional mechanism instead, as the authors are certainly aware of and as has been discussed at length in [ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS](https://arxiv.org/abs/1511.06391), where Oriol is also the first author. I wish there was a comment on this point in the paper somewhere.\n\n- The approach also gets quite a bit slower as the number of training examples grow, but once this number is large one would presumable switch over to a parameteric approach.\n\n- It's also potentially concerning that during training the method uses a specific number of examples, e.g. 5-25, so this is the number of that must also be used at test time. What happens if we want the size of our training set to grow online? It appears that we need to retrain the network because the encoder LSTM for the training data is not \"used to\" seeing inputs of more examples? That is unless you fall back to iteratively subsampling the training data, doing multiple inference passes and averaging, or something like that. If we don't use FCE it can still be that the attention mechanism LSTM can still not be \"used to\" attending over many more examples, but it's not clear how much this matters. An interesting experiment would be to not use FCE and try to use 100 or 1000 training examples, while only training on up to 25 (with and fithout FCE). Discussion surrounding this point would be interesting.\n\n- Not clear what happened with the Omniglot experiments, with incorrect numbers for [11], [21], and the exclusion of Lake et al. [14] comparison.\n\n- A baseline that is missing would in my opinion also include training of an [Exemplar SVM](https://www.cs.cmu.edu/~tmalisie/projects/iccv11/), which is a much more powerful approach than encode-with-a-cnn-and-nearest-neighbor.\n\n  \u200b",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.04080"
    },
    "2": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BengioTPPB17",
        "transcript": "**Summary**\n\nRepresentation (or feature) learning with unsupervised learning has yet really to yield the type of results that many believe to be achievable. For example, we\u2019d like to unleash an unsupervised learning algorithm on all web images and then obtain a representation that captures the various factors of variation we know to be present (e.g. objects and people). One popular approach for this is to train a model that assumes a high-level vector representation with independent components. However, despite a large body of literature on such models by now, such so-called disentangling of these factors of variation still seems beyond our reach.\n\nIn this short paper, the authors propose an alternative to this approach. They propose that disentangling might be achievable by learning a representation whose dimensions are each separately **controllable**, i.e. that each have an associated policy which changes the value of that dimension **while letting other dimensions fixed**. \n\nSpecifically, the authors propose to minimize the following objective:\n\n$\\mathop{\\mathbb{E}}_s\\left[\\frac{1}{2}||s-g(f(s))||^2_2 \\right] - \\lambda \\sum_k \\mathbb{E}_{a,s}\\left[\\sum_a \\pi_k(a|s) \\log sel(s,a,k)\\right]$\n\nwhere \n- $s$ is an agent\u2019s state (e.g. frame image) which encoder $f$ and decoder $g$ learn to autoencode\n- $k$ iterates over all dimensions of the representation space (output of encoder)\n- $a$ iterates over actions that the agent can take\n- $\\pi_k(a|s)$ is the policy that is meant to control the $k^{\\rm th}$ dimension of the representation space $f(s)_k$\n- $sel(s,a,k)$ is the selectivity of $f(s)_k$ relative to other dimensions in the representation, at state $s$:\n\n$sel(s,a,k) = \\mathop{\\mathbb{E}}_{s\u2019\\sim {\\cal P}_{ss\u2019}^a}\\left[\\frac{|f_k(s\u2019)-f_k(s)|}{\\sum_{k\u2019} |f_{k\u2019}(s\u2019)-f_{k\u2019}(s)| }\\right]$\n\n${\\cal P}_{ss\u2019}^a$ is the conditional distribution over the next step state $s\u2019$ given that you are at state $s$ and are taking action $a$ (i.e. the environment transition distribution). One can see that selectivity is higher when the change $|f_k(s\u2019)-f_k(s)|$ in dimension $k$ is much larger than the change \n$|f_{k\u2019}(s\u2019)-f_{k\u2019}(s)|$ in the other dimensions $k\u2019$. A directed version of selectivity is also proposed (and I believe was used in the experiments), where the absolute value function is removed and $\\log sel$ is replaced with $\\log(1+sel)$ in the objective.\n\nThe learning objective will thus encourage the discovery of a representation that is informative of the input (in that you can reconstruct it) and for which there exists policies that separately control these dimensions.\n\nAlgorithm 1 in the paper describes a learning procedure for optimizing this objective. In brief, for every update, a state $s$ is sampled from which an update for the autoencoder part of the loss can be made. Then, iterating over each dimension $k$, REINFORCE is used to get a gradient estimate of the selectivity part of the loss, to update both the policy $\\pi_k$ and the encoder $f$ by using the policy to reach a next state $s\u2019$.\n\n**My two cents**\n\nI find this concept very appealing and thought provoking. Intuitively, I find the idea that valuable features are features which reflect an aspect of our environment that we can control more sensible and possibly less constraining than an assumption of independent features. It also has an interesting analogy of an infant learning about the world by interacting with it.\n\nThe caveat is that unfortunately, this concept is currently fairly impractical, since it requires an interactive environment where an agent can perform actions, something we can\u2019t easily have short of deploying a robot with sensors. Moreover, the proposed algorithm seems to assume that each state $s$ is sampled independently for each update, whereas a robot would observe a dependent stream of states. \n\nAccordingly, the experiments in this short paper are mostly \u201cproof of concept\u201d, on simplistic synthetic environments. Yet they do a good job at illustrating the idea.\n\nTo me this means that there\u2019s more interesting work worth doing in what seems to be a promising direction!\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.07718"
    },
    "3": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1610.06258",
        "transcript": "This paper presents a recurrent neural network architecture in which some of the recurrent weights dynamically change during the forward pass, using a hebbian-like rule. They correspond to the matrices $A(t)$ in the figure below:\n\n![Fast weights RNN figure](http://i.imgur.com/DCznSf4.png)\n\nThese weights $A(t)$ are referred to as *fast weights*. Comparatively, the recurrent weights $W$ are referred to as slow weights, since they are only changing due to normal training and are otherwise kept constant at test time.\n\nMore specifically, the proposed fast weights RNN compute a series of hidden states $h(t)$ over time steps $t$, but, unlike regular RNNs, the transition from $h(t)$ to $h(t+1)$ consists of multiple ($S$) recurrent layers $h_1(t+1), \\dots, h_{S-1}(t+1), h_S(t+1)$, defined as follows:\n\n$$h_{s+1}(t+1) = f(W h(t) + C x(t) + A(t) h_s(t+1))$$\n\nwhere $f$ is an element-wise non-linearity such as the ReLU activation. The next hidden state $h(t+1)$ is simply defined as the last \"inner loop\" hidden state $h_S(t+1)$, before moving to the next time step. \n\nAs for the fast weights $A(t)$, they too change between time steps, using the hebbian-like rule:\n\n$$A(t+1) = \\lambda A(t) + \\eta h(t) h(t)^T$$\n\nwhere $\\lambda$ acts as a decay rate (to partially forget some of what's in the past)  and $\\eta$ as the fast weight's \"learning rate\" (not to be confused with the learning rate used during backprop). Thus, the role played by the fast weights is to rapidly adjust to the recent hidden states and remember the recent past.\n\nIn fact, the authors show an explicit relation between these fast weights and memory-augmented architectures that have recently been popular. Indeed, by recursively applying and expending the equation for the fast weights, one obtains\n\n$$A(t) = \\eta \\sum_{\\tau = 1}^{\\tau = t-1}\\lambda^{t-\\tau-1} h(\\tau) h(\\tau)^T$$\n\n*(note the difference with Equation 3 of the paper... I think there was a typo)* which implies that when computing the $A(t) h_s(t+1)$ term in the expression to go from $h_s(t+1)$ to $h_{s+1}(t+1)$, this term actually corresponds to\n\n$$A(t) h_s(t+1) = \\eta \\sum_{\\tau =1}^{\\tau = t-1} \\lambda^{t-\\tau-1} h(\\tau) (h(\\tau)^T h_s(t+1))$$\n\ni.e. $A(t) h_s(t+1)$ is a weighted sum of all previous hidden states $h(\\tau)$, with each hidden states weighted by an \"attention weight\" $h(\\tau)^T h_s(t+1)$. The difference with many recent memory-augmented architectures is thus that the attention weights aren't computed using a softmax non-linearity.\n\nExperimentally, they find it beneficial to use [layer normalization](https://arxiv.org/abs/1607.06450). Good values for $\\eta$ and $\\lambda$ seem to be 0.5 and 0.9 respectively. I'm not 100% sure, but I also understand that using $S=1$, i.e. using the fast weights only once per time steps, was usually found to be optimal. Also see Figure 3 for the architecture used on the image classification datasets, which is slightly more involved.\n\nThe authors present a series 4 experiments, comparing with regular RNNs (IRNNs, which are RNNs with ReLU units and whose recurrent weights are initialized to a scaled identity matrix) and LSTMs (as well as an associative LSTM for a synthetic associative retrieval task and ConvNets for the two image datasets). Generally, experiments illustrate that the fast weights RNN tends to train faster (in number of updates) and better than the other recurrent architectures. Surprisingly, the fast weights RNN can even be competitive with a ConvNet on the two image classification benchmarks, where the RNN traverses glimpses from the image using a fixed policy.\n\n**My two cents**\n\nThis is a very thought provoking paper which, based on the comparison with LSTMs, suggests that fast weights RNNs might be a very good alternative. I'd be quite curious to see what would happen if one was to replace LSTMs with them in the myriad of papers using LSTMs (e.g. all the Seq2Seq work). Intuitively, LSTMs seem to be able to do more than just attending to the recent past. But, for a given task, if one was to observe that fast weights RNNs are competitive to LSTMs, it would suggests that the LSTM isn't doing something that much more complex. So it would be interesting to determine what are the tasks where the extra capacity of an LSTM is actually valuable and exploitable. Hopefully the authors will release some code, to facilitate this exploration. \n\nThe discussion at the end of Section 3 on how exploiting the \"memory augmented\" view of fast weights is useful to allow the use of minibatches is interesting. However, it also suggests that computations in the fast weights RNN scales quadratically with the sequence size (since in this view, the RNN technically must attend to all previous hidden states, since the beginning of the sequence). This is something to keep in mind, if one was to consider applying this to very long sequences (i.e. much longer than the hidden state dimensionality).\n\nAlso, I don't quite get the argument that the \"memory augmented\" view of fast weights is more amenable to mini-batch training. I understand that having an explicit weight matrix $A(t)$ for each minibatch sequence complicates things. However, in the memory augmented view, we also have a \"memory matrix\" that is different for each sequence, and yet we can handle that fine. The problem I can imagine is that storing a *sequence of arbitrary weight matrices* for each sequence might be storage demanding (and thus perhaps make it impossible to store a forward/backward pass for more than one sequence at a time), while the implicit memory matrix only requires appending a new row at each time step. Perhaps the argument to be made here is more that there's already mini-batch compatible code out there for dealing with the use of a memory matrix of stored previous memory states.\n\nThis work strikes some (partial) resemblance to other recent work, which may serve as food for thought here. The use of possibly multiple computation layers between time steps reminds me of [Adaptive Computation Time (ACT) RNN]( http://www.shortscience.org/paper?bibtexKey=journals/corr/Graves16). Also, expressing a backpropable architecture that involves updates to weights (here, hebbian-like updates) reminds me of recent work that does backprop through the updates of a gradient descent procedure (for instance as in [this work]( http://www.shortscience.org/paper?bibtexKey=conf/icml/MaclaurinDA15)). \n\nFinally, while I was familiar with the notion of fast weights from the work on [Using Fast Weights to Improve Persistent Contrastive Divergence](http://people.ee.duke.edu/~lcarin/FastGibbsMixing.pdf), I didn't realize that this concept dated as far back as the late 80s. So, for young researchers out there looking for inspiration for research ideas, this paper confirms that looking at the older neural network literature for inspiration is probably a very good strategy :-)\n\nTo sum up, this is really nice work, and I'm looking forward to the NIPS 2016 oral presentation of it!",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1610.06258"
    },
    "4": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.05690",
        "transcript": "This paper derives an algorithm for passing gradients through a sample from a mixture of Gaussians. While the reparameterization trick allows to get the gradients with respect to the Gaussian means and covariances, the same trick cannot be invoked for the mixing proportions parameters (essentially because they are the parameters of a multinomial discrete distribution over the Gaussian components, and the reparameterization trick doesn't extend to discrete distributions).\n\nOne can think of the derivation as proceeding in 3 steps:\n\n1. Deriving an estimator for gradients a sample from a 1-dimensional density $f(x)$ that is such that $f(x)$ is differentiable and its cumulative distribution function (CDF) $F(x)$ is tractable:\n\n  $\\frac{\\partial \\hat{x}}{\\partial \\theta} = - \\frac{1}{f(\\hat{x})}\\int_{t=-\\infty}^{\\hat{x}} \\frac{\\partial f(t)}{\\partial \\theta} dt$\n\n  where $\\hat{x}$ is a sample from density $f(x)$ and $\\theta$ is any parameter of $f(x)$ (the above is a simplified version of Equation 6). This is probably the most important result of the paper, and is based on a really clever use of the general form of the Leibniz integral rule.\n\n2. Noticing that one can sample from a $D$-dimensional Gaussian mixture by decomposing it with the product rule $f({\\bf x}) = \\prod_{d=1}^D f(x_d|{\\bf x}_{<d})$ and using ancestral sampling, where each $f(x_d|{\\bf x}_{<d})$ are themselves 1-dimensional mixtures (i.e. with differentiable densities and tractable CDFs)\n\n3. Using the 1-dimensional gradient estimator (of Equation 6) and the chain rule to backpropagate through the ancestral sampling procedure. This requires computing the integral in the expression for $\\frac{\\partial \\hat{x}}{\\partial \\theta}$ above, where $f(x)$ is one of the 1D conditional Gaussian mixtures and $\\theta$ is a mixing proportion parameter $\\pi_j$. As it turns out, this integral has an analytical form (see Equation 22).\n\n**My two cents**\n\nThis is a really surprising and neat result. The author mentions it could be applicable to variational autoencoders (to support posteriors that are mixtures of Gaussians), and I'm really looking forward to read about whether that can be successfully done in practice. \n\nThe paper provides the derivation only for mixtures of Gaussians with diagonal covariance matrices. It is mentioned that extending to non-diagonal covariances is doable. That said, ancestral sampling with non-diagonal covariances would become more computationally expensive, since the conditionals under each Gaussian involves a matrix inverse.\n\nBeyond the case of Gaussian mixtures, Equation 6 is super interesting in itself as its application could go beyond that case. This is probably why the paper also derived a sampling-based estimator for Equation 6, in Equation 9. However, that estimator might be inefficient, since it involves sampling from Equation 10 with rejection, and it might take a lot of time to get an accepted sample if $\\hat{x}$ is very small. Also, a good estimate of Equation 6 might require *multiple* samples from Equation 10. \n\nFinally, while I couldn't find any obvious problem with the mathematical derivation, I'd be curious to see whether using the same approach to derive a gradient on one of the Gaussian mean or standard deviation parameters gave a gradient that is consistent with what the reparameterization trick provides.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.05690"
    },
    "5": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/FernandoG16",
        "transcript": "This paper describes how rank pooling, a very recent approach for pooling representations organized in a sequence $\\\\{{\\bf v}_t\\\\}_{t=1}^T$, can be used in an end-to-end trained neural network architecture.\n\nRank pooling is an alternative to average and max pooling for sequences, but with the distinctive advantage of maintaining some order information from the sequence. Rank pooling first solves a regularized (linear) support vector regression (SVR) problem where the inputs are the vector representations ${\\bf v}_t$ in the sequence and the target is the corresponding index $t$ of that representation in the sequence (see Equation 5). The output of rank pooling is then simply the linear regression parameters $\\bf{u}$ learned for that sequence. Because of the way ${\\bf u}$ is trained, we can see that ${\\bf u}$ will capture order information, as successful training would imply that ${\\bf u}^\\top {\\bf v}_t <\u00a0{\\bf u}^\\top {\\bf v}_{t'} $ if $t < t'$. See [this paper](https://www.robots.ox.ac.uk/~vgg/rg/papers/videoDarwin.pdf) for more on rank pooling.\n\nWhile previous work has focused on using rank pooling on hand-designed and fixed representations, this paper proposes to use ConvNet features (pre-trained on ImageNet) for the representation and backpropagate through rank pooling to fine-tune the ConvNet features. Since the output of rank pooling corresponds to an argmin operation, passing gradients through this operation is not as straightforward as for average or max pooling. However, it turns out that if the objective being minimized (in our case regularized SVR) is twice differentiable, gradients with respect to its argmin can be computed (see Lemmas 1 and 2). The authors derive the gradient for rank pooling (Equation 21). Finally, since its gradient requires inverting a matrix (corresponding to a hessian), the authors propose to either use an efficient procedure for computing it by exploiting properties of sums of rank-one matrices (see Lemma 3) or to simply use an approximation based on using a diagonal hessian.\n\nIn experiments on two small scale video activity recognition datasets (UCF-Sports and Hollywood2), the authors show that fine-tuning the ConvNet features significantly improves the performance of rank pooling and makes it superior to max and average pooling.\n\n**My two cents**\n\nThis paper was eye opening for me, first because I did not realize that one could backpropagate through an operation corresponding to an argmin that doesn't have a closed form solution (though apparently this paper isn't the first to make that observation). Moreover, I did not know about rank pooling, which itself is a really thought provoking approach to pooling representations in a way that preserves some organizational information about the original representations.\n\nI wonder how sensitive the results are to the value of the regularization constant of the SVR problem. The authors mention some theoretical guaranties on the stability of the solution found by SVR in general, but intuitively I would expect that the regularization constant would play a large role in the stability.\n\nI'll be looking forward to any future attempts to increase the speed of rank pooling (or any similar method). Indeed, as the authors mention, it is currently too slow to be used on the larger video datasets that are currently available. \n\nCode for computing rank pooling (though not for computing its gradients) seems to be available [here](https://bitbucket.org/bfernando/videodarwin).",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v48/fernando16.html"
    },
    "6": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.08803",
        "transcript": "This paper presents a novel neural network approach (though see [here](https://www.facebook.com/hugo.larochelle.35/posts/172841743130126?pnref=story) for a discussion on prior work) to density estimation, with a focus on image modeling. At its core, it exploits the following property on the densities of random variables. Let $x$ and $z$ be two random variables of equal dimensionality such that $x = g(z)$, where $g$ is some bijective and deterministic function (we'll note its inverse as $f = g^{-1}$). Then the change of variable formula gives us this relationship between the densities of $x$ and $z$:\n\n$p_X(x) = p_Z(z) \\left|{\\rm det}\\left(\\frac{\\partial g(z)}{\\partial z}\\right)\\right|^{-1}$\n\nMoreover, since the determinant of the Jacobian matrix of the inverse $f$ of a function $g$ is simply the inverse of the Jacobian of the function $g$, we can also write:\n\n$p_X(x) = p_Z(f(x)) \\left|{\\rm det}\\left(\\frac{\\partial f(x)}{\\partial x}\\right)\\right|$\n\nwhere we've replaced $z$ by its deterministically inferred value $f(x)$ from $x$.\n\nSo, the core of the proposed model is in proposing a design for bijective functions $g$ (actually, they design its inverse $f$, from which $g$ can be derived by inversion), that have the properties of being easily invertible and having an easy-to-compute determinant of Jacobian. Specifically, the authors propose to construct $f$ from various modules that all preserve these properties and allows to construct highly non-linear $f$ functions. Then, assuming a simple choice for the density $p_Z$ (they use a multidimensional Gaussian), it becomes possible to both compute $p_X(x)$ tractably and to sample from that density, by first samples $z\\sim p_Z$ and then computing $x=g(z)$.\n\nThe building blocks for constructing $f$ are the following:\n\n**Coupling layers**: This is perhaps the most important piece. It simply computes as its output $b\\odot x + (1-b) \\odot (x \\odot \\exp(l(b\\odot x)) + m(b\\odot x))$, where $b$ is a binary mask (with half of its values set to 0 and the others to 1) over the input of the layer $x$, while $l$ and $m$ are arbitrarily complex neural networks with input and output layers of equal dimensionality. \n\nIn brief, for dimensions for which $b_i = 1$ it simply copies the input value into the output. As for the other dimensions (for which $b_i = 0$) it linearly transforms them as $x_i * \\exp(l(b\\odot x)_i) + m(b\\odot x)_i$. Crucially, the bias ($m(b\\odot x)_i$) and coefficient ($\\exp(l(b\\odot x)_i)$) of the linear transformation are non-linear transformations (i.e. the output of neural networks) that only have access to the masked input (i.e. the non-transformed dimensions). While this layer might seem odd, it has the important property that it is invertible and the determinant of its Jacobian is simply $\\exp(\\sum_i (1-b_i) l(b\\odot x)_i)$. See Section 3.3 for more details on that.\n\n**Alternating masks**: One important property of coupling layers is that they can be stacked (i.e. composed), and the resulting composition is still a bijection and is invertible (since each layer is individually a bijection) and has a tractable determinant for its Jacobian (since the Jacobian of the composition of functions is simply the multiplication of each function's Jacobian matrix, and the determinant of the product of square matrices is the product of the determinant of each matrix). This is also true, even if the mask $b$ of each layer is different. Thus, the authors propose using masks that alternate across layer, by masking a different subset of (half of) the dimensions. For images, they propose using masks with a checkerboard pattern (see Figure 3). Intuitively, alternating masks are better because then after at least 2 layers, all dimensions have been transformed at least once.\n\n**Squeezing operations**: Squeezing operations corresponds to a reorganization of a 2D spatial layout of dimensions into 4 sets of features maps with spatial resolutions reduced by half (see Figure 3). This allows to expose multiple scales of resolutions to the model. Moreover, after a squeezing operation, instead of using a checkerboard pattern for masking, the authors propose to use a per channel masking pattern, so that \"the resulting partitioning is not redundant with the previous checkerboard masking\". See Figure 3 for an illustration.\n\nOverall, the models used in the experiments usually stack a few of the following \"chunks\" of layers: 1) a few coupling layers with alternating checkboard masks, 2) followed by squeezing, 3) followed by a few coupling layers with alternating channel-wise masks. Since the output of each layers-chunk must technically be of the same size as the input image, this could become expensive in terms of computations and space when using a lot of layers. Thus, the authors propose to explicitly pass on (copy) to the very last layer ($z$) half of the dimensions after each layers-chunk, adding another chunk of layers only on the other half. This is illustrated in Figure 4b.\n\nExperiments on CIFAR-10, and 32x32 and 64x64 versions of ImageNet show that the proposed model (coined the real-valued non-volume preserving or Real NVP) has competitive performance (in bits per dimension), though slightly worse than the Pixel RNN.\n\n**My Two Cents**\n\nThe proposed approach is quite unique and thought provoking. Most interestingly, it is the only powerful generative model I know that combines A) a tractable likelihood, B) an efficient / one-pass sampling procedure and C) the explicit learning of a latent representation. While achieving this required a model definition that is somewhat unintuitive, it is nonetheless mathematically really beautiful!\n\nI wonder to what extent Real NVP is penalized in its results by the fact that it models pixels as real-valued observations. First, it implies that its estimate of bits/dimensions is an upper bound on what it could be if the uniform sub-pixel noise was integrated out (see Equations 3-4-5 of [this paper](http://arxiv.org/pdf/1511.01844v3.pdf)). Moreover, the authors had to apply a non-linear transformation (${\\rm logit}(\\alpha + (1-\\alpha)\\odot x)$) to the pixels, to spread the $[0,255]$ interval further over the reals. Since the Pixel RNN models pixels as discrete observations directly, the Real NVP might be at a disadvantage.\n\nI'm also curious to know how easy it would be to do conditional inference with the Real NVP. One could imagine doing approximate MAP conditional inference, by clamping the observed dimensions and doing gradient descent on the log-likelihood with respect to the value of remaining dimensions. This could be interesting for image completion, or for structured output prediction with real-valued outputs in general. I also wonder how expensive that would be.\n\nIn all cases, I'm looking forward to saying interesting applications and variations of this model in the future!",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.08803"
    },
    "7": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/JohanssonSS16",
        "transcript": "This paper presents a method to train a neural network to make predictions for *counterfactual* questions. In short, such questions are questions about what the result of an intervention would have been, had a different choice for the intervention been made (e.g. *Would this patient have lower blood sugar had she received a different medication?*).\n\nOne approach to tackle this problem is to collect data of the form $(x_i, t_i, y_i^F)$ where $x_i$ describes a situation (e.g. a patient), $t_i$ describes the intervention made (in this paper $t_i$ is binary, e.g. $t_i = 1$ if a new treatment is used while $t_i = 0$ would correspond to using the current treatment) and $y_i^F$ is the factual outcome of the intervention $t_i$ for $x_i$. From this training data, a predictor $h(x,t)$ taking the pair $(x_i, t_i)$ as input and outputting a prediction for $y_i^F$ could be trained. \n\nFrom this predictor, one could imagine answering counterfactual questions by feeding $(x_i, 1-t_i)$ (i.e. a description of the same situation $x_i$ but with the opposite intervention $1-t_i$) to our predictor and comparing the prediction $h(x_i, 1-t_i)$ with $y_i^F$. This would give us an estimate of the change in the outcome, had a different intervention been made, thus providing an answer to our counterfactual question.\n\nThe authors point out that this scenario is related to that of domain adaptation (more specifically to the special case of covariate shift) in which the input training distribution (here represented by inputs $(x_i,t_i)$) is different from the distribution of inputs that will be fed at test time to our predictor (corresponding to the inputs $(x_i, 1-t_i)$). If the choice of intervention $t_i$ is evenly spread and chosen independently from $x_i$, the distributions become the same. However, in observational studies, the choice of $t_i$ for some given $x_i$ is often not independent of $x_i$ and made according to some unknown policy. This is the situation of interest in this paper.\n\nThus, the authors propose an approach inspired by the domain adaptation literature. Specifically, they propose to have the predictor $h(x,t)$ learn a representation of $x$ that is indiscriminate of the intervention $t$ (see Figure 2 for the proposed neural network architecture). Indeed, this is a notion that is [well established][1] in the domain adaptation literature and has been exploited previously using regularization terms based on [adversarial learning][2] and [maximum mean discrepancy][3]. In this paper, the authors used instead a regularization (noted in the paper as $disc(\\Phi_{t=0},\\Phi_ {t=1})$) based on the so-called discrepancy distance of [Mansour et al.][4], adapting its use to the case of a neural network. \n\nAs an example, imagine that in our dataset, a new treatment ($t=1$) was much more frequently used than not ($t=0$) for men. Thus, for men, relatively insufficient evidence for counterfactual inference is expected to be found in our training dataset. Intuitively, we would thus want our predictor to not rely as much on that \"feature\" of patients when inferring the impact of the treatment. \n\nIn addition to this term, the authors also propose incorporating an additional regularizer where the prediction $h(x_i,1-t_i)$ on counterfactual inputs is pushed to be as close as possible to the target $y_{j}^F$ of the observation $x_j$ that is closest to $x_i$ **and** actually had the counterfactual intervention $t_j = 1-t_i$. \n\nThe paper first shows a bound relating the counterfactual generalization error to the discrepancy distance. Moreover, experiments simulating counterfactual inference tasks are presented, in which performance is measured by comparing the predicted treatment effects (as estimated by the difference between the observed effect $y_i^F$ for the observed treatment and the predicted effect $h(x_i, 1-t_i)$ for the opposite treatment) with the real effect (known here because the data is simulated). The paper shows that the proposed approach using neural networks outperforms several baselines on this task.\n\n**My two cents**\n\nThe connection with domain adaptation presented here is really clever and enlightening. This sounds like a very compelling approach to counterfactual inference, which can exploit a lot of previous work on domain adaptation.\n\nThe paper mentions that selecting the hyper-parameters (such as the regularization terms weights) in this scenario is not a trivial task. Indeed, measuring performance here requires knowing the true difference in intervention outcomes, which in practice usually cannot be known (e.g. two treatments usually cannot be given to the same patient once). In the paper, they somewhat \"cheat\" by using the ground truth difference in outcomes to measure out-of-sample performance, which the authors admit is unrealistic. Thus, an interesting avenue for future work would be to design practical hyper-parameter selection procedures for this scenario. I wonder whether the *reverse cross-validation* approach we used in our work on our adversarial approach to domain adaptation (see [Section 5.1.2][5]) could successfully be used here.\n\nFinally, I command the authors for presenting such a nicely written description of counterfactual inference problem setup in general, I really enjoyed it!\n\n[1]: https://papers.nips.cc/paper/2983-analysis-of-representations-for-domain-adaptation.pdf\n[2]: http://arxiv.org/abs/1505.07818\n[3]: http://ijcai.org/Proceedings/09/Papers/200.pdf\n[4]: http://www.cs.nyu.edu/~mohri/pub/nadap.pdf\n[5]: http://arxiv.org/pdf/1505.07818v4.pdf#page=16",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1605.03661"
    },
    "8": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.02185",
        "transcript": "This paper can be thought as proposing a variational autoencoder applied to a form of meta-learning, i.e. where the input is not a single input but a dataset of inputs. For this, in addition to having to learn an approximate inference network over the latent variable $z_i$ for each input $x_i$ in an input dataset $D$, approximate inference is also learned over a latent variable $c$ that is global to the dataset $D$. By using Gaussian distributions for $z_i$ and $c$, the reparametrization trick can be used to train the variational autoencoder.\n\nThe generative model factorizes as\n\n$p(D=(x_1,\\dots,x_N), (z_1,\\dots,z_N), c) = p(c) \\prod_i p(z_i|c) p(x_i|z_i,c)$\n\nand learning is based on the following variational posterior decomposition:\n\n$q((z_1,\\dots,z_N), c|D=(x_1,\\dots,x_N)) = q(c|D) \\prod_i q(z_i|x_i,c)$.\n\nMoreover, latent variable $z_i$ is decomposed into multiple ($L$) layers $z_i = (z_{i,1}, \\dots, z_{i,L})$. Each layer in the generative model is directly connected to the input. The layers are generated from $z_{i,L}$ to $z_{i,1}$, each layer being conditioned on the previous (see Figure 1 *Right* for the graphical model), with the approximate posterior following a similar decomposition.\n\nThe architecture for the approximate inference network $q(c|D)$ first maps all inputs $x_i\\in D$ into a vector representation, then performs mean pooling of these representations to obtain a single vector, followed by a few more layers to produce the parameters of the Gaussian distribution over $c$. \n\nTraining is performed by stochastic gradient descent, over minibatches of datasets (i.e. multiple sets $D$).\n\nThe model has multiple applications, explored in the experiments. One is of summarizing a dataset $D$ into a smaller subset $S\\in D$. This is done by initializing $S\\leftarrow D$ and greedily removing elements of $S$, each time minimizing the KL divergence between $q(c|D)$ and $q(c|S)$ (see the experiments on a synthetic Spatial MNIST problem of section 5.3).\n\nAnother application is few-shot classification, where very few examples of a number of classes are given, and a new test example $x'$ must be assigned to one of these classes. Classification is performed by treating the small set of examples of each class $k$ as its own dataset $D_k$. Then, test example $x$ is classified into class $k$ for which the KL divergence between $q(c|x')$ and $q(c|D_k)$ is smallest. Positive results are reported when training on OMNIGLOT classes and testing on either the MNIST classes or unseen OMNIGLOT datasets, when compared to a 1-nearest neighbor classifier based on the raw input or on a representation learned by a regular autoencoder.\n\nFinally, another application is that of generating new samples from an input dataset of examples. The approximate posterior is used to compute $q(c|D)$. Then, $c$ is assigned to its posterior mean, from which a value for the hidden layers $z$ and finally a sample $x$ can be generated. It is shown that this procedure produces convincing samples that are visually similar from those in the input set $D$.\n\n**My two cents**\n\nAnother really nice example of deep learning applied to a form of meta-learning, i.e. learning a model that is trained to take *new* datasets as input and generalize even if confronted to datasets coming from an unseen data distribution. I'm particularly impressed by the many tasks explored successfully with the same approach: few-shot classification and generative sampling, as well as a form of summarization (though this last probably isn't really meta-learning). Overall, the approach is quite elegant and appealing.\n\nThe very simple, synthetic experiments of section 5.1 and 5.2 are also interesting. Section 5.2 presents the notion of a *prior-interpolation layer*, which is well motivated but seems to be used only in that section. I wonder how important it is, outside of the specific case of section 5.2.\n\nOverall, very excited by this work, which further explores the theme of meta-learning in an interesting way.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.02185"
    },
    "9": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.06465",
        "transcript": "This paper presents Swapout, a simple dropout method applied to Residual Networks (ResNets). In a ResNet, a layer $Y$ is computed from the previous layer $X$ as\n\n$Y = X + F(X)$\n\nwhere $F(X)$ is essentially the composition of a few convolutional layers. Swapout simply applies dropout separately on both terms of a layer's equation:\n\n$Y = \\Theta_1 \\odot X + \\Theta_2 \\odot F(X)$\n\nwhere $\\Theta_1$ and $\\Theta_2$ are independent dropout masks for each term. \n\nThe paper shows that this form of dropout is at least as good or superior as other forms of dropout, including the recently proposed [stochastic depth dropout][1]. Much like in the stochastic depth paper, better performance is achieved by linearly increasing the dropout rate (from 0 to 0.5) from the first hidden layer to the last.\n\nIn addition to this observation, I also note the following empirical observations:\n\n1. At test time, averaging the output layers of multiple dropout mask samples (referenced to as stochastic inference) is better than replacing the masks by their expectation (deterministic inference), the latter being the usual standard.\n2. Comparable performance is achieved by making the ResNet wider (e.g. 4 times) and with fewer layers (e.g. 32) than the orignal ResNet work with thin but very deep (more than 1000 layers) ResNets. This would confirm a similar observation from [this paper][2].\n\nOverall, these are useful observations to be aware of for anyone wanting to use ResNets in practice.\n\n[1]: http://arxiv.org/abs/1603.09382v1\n[2]: https://arxiv.org/abs/1605.07146\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.06465"
    },
    "10": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Lopez-PazNCSB16",
        "transcript": "This paper tests the following hypothesis, about features learned by a deep network trained on the ImageNet dataset: \n\n*Object features and anticausal features are closely related. Context features and causal features are not necessarily related.*\n\nFirst, some definitions. Let $X$ be a visual feature (i.e. value of a hidden unit) and $Y$ be information about a label (e.g. the log-odds of probability of different object appearing in the image). A causal feature would be one for which the causal direction is $X \\rightarrow Y$. An anticausal feature would be the opposite case, $X \\leftarrow Y$. \n\nAs for object features, in this paper they are features whose value tends to change a lot when computed on a complete original image versus when computed on an image whose regions *falling inside* object bounding boxes have been blacked out (see Figure 4). Contextual features are the opposite, i.e. values change a lot when blacking out the regions *outside* object bounding boxes. See section 4.2.1 for how \"object scores\" and \"context scores\" are computed following this description, to quantitatively measure to what extent a feature is an \"object feature\" or a \"context feature\".\n\nThus, the paper investigates whether 1) for object features, their relationship with object appearance information is anticausal (i.e. whether the object feature's value seems to be caused by the presence of the object) and whether 2) context features are not clearly causal or anticausal.\n\nTo perform this investigation, the paper first proposes a generic neural network model (dubbed the Neural Causation Coefficient architecture or NCC) to predict a score of whether the relationship between an input variable $X$ and target variable $Y$ is causal. This model is trained by taking as input datasets of $X$ and $Y$ pairs synthetically generated in such a way that we know whether $X$ caused $Y$ or the opposite. The NCC architecture first embeds each individual $X$,$Y$ instance pair into some hidden representation, performs mean pooling of these representations and then feeds the result to fully connected layers (see Figure 3). The paper shows that the proposed NCC model actually achieves SOTA performance on the T\u00fcbingen dataset, a collection of real-world cause-effect observational samples. \n\nThen, the proposed NCC model is used to measure the average object score of features of a deep residual CNN identified as being most causal and most anticausal by NCC. The same is done with the context score. What is found is that indeed, the object score is always higher for the top anticausal features than for the top causal features. However, for the context score, no such clear trend is observed (see Figure 5).\n\n**My two cents**\n\nI haven't been following the growing literature on machine learning for causal inference, so it was a real pleasure to read this paper and catch up a little bit on that. Just for that I would recommend the reading of this paper. The paper does a really good job at explaining the notion of *observational causal inference*, which in short builds on the observation that if we assume IID noise on top of a causal (or anticausal) phenomenon, then causation can possibly be inferred by verifying in which direction of causation the IID assumption on the noise seems to hold best (see Figure 2 for a nice illustration, where in (a) the noise is clearly IID, but isn't in (b)).\n\nAlso, irrespective of the study of causal phenomenon in images, the NCC architecture, which achieves SOTA causal prediction performance, is in itself a nice contribution.\n\nRegarding the application to image features, one thing that is hard to wrap your head around is that, for the $Y$ variable, instead of using the true image label, the log-odds at the output layer are used instead in the study. The paper justifies this choice by highlighting that the NCC network was trained on examples where $Y$ is continuous, not discrete. On one hand, that justification makes sense. On the other, this is odd since the log-odds were in fact computed directly from the visual features, meaning that technically the value of the log-odds are directly caused by all the features (which goes against the hypothesis being tested). My best guess is that this isn't an issue only because NCC makes a causal prediction between *a single feature* and $Y$, not *from all features* to $Y$. I'd be curious to read the authors' perspective on this.\n\nStill, this paper at this point is certainly just scratching the surface on this topic. For instance, the paper mentions that NCC could be used to encourage the learning of causal or anticausal features, providing a new and intriguing type of regularization. This sounds like a very interesting future direction for research, which I'm looking forward to.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1605.08179"
    },
    "11": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.06065",
        "transcript": "This paper proposes a variant of Neural Turing Machine (NTM) for meta-learning or \"learning to learn\", in the specific context of few-shot learning (i.e. learning from few examples). Specifically, the proposed model is trained to ingest as input a training set of examples and improve its output predictions as examples are processed, in a purely feed-forward way. This is a form of meta-learning because the model is trained so that its forward pass effectively executes a form of \"learning\" from the examples it is fed as input.\n\nDuring training, the model is fed multiples sequences (referred to as episodes) of labeled examples $({\\bf x}_1, {\\rm null}), ({\\bf x}_2, y_1), \\dots, ({\\bf x}_T, y_{T-1})$, where $T$ is the size of the episode. For instance, if the model is trained to learn how to do 5-class classification from 10 examples per class, $T$ would be $5 \\times 10 = 50$. Mainly, the paper presents experiments on the Omniglot dataset, which has 1623 classes. In these experiments, classes are separated into 1200 \"training classes\" and 423 \"test classes\", and each episode is generated by randomly selecting 5 classes (each assigned some arbitrary vector representation, e.g. a  one-hot vector that is consistent within the episode, but not across episodes) and constructing a randomly ordered sequence of 50 examples from within the chosen 5 classes. Moreover, the correct label $y_t$ of a given input ${\\bf x}_t$ is always provided only at the next time step, but the model is trained to be good at its prediction of the label of ${\\bf x}_t$ at the current time step. This is akin to the scenario of online learning on a stream of examples, where the label of an example is revealed only once the model has made a prediction.\n\nThe proposed NTM is different from the original NTM of Alex Graves, mostly in how it writes into its memory. The authors propose to focus writing to either the least recently used memory location or the most recently used memory location. Moreover, the least recently used memory location is reset to zero before every write (an operation that seems to be ignored when backpropagating gradients).\n\nIntuitively, the proposed NTM should learn a strategy by which, given a new input, it looks into its memory for information from other examples earlier in the episode (perhaps similarly to what a nearest neighbor classifier would do) to predict the class of the new input.\n\nThe paper presents experiments in learning to do multiclass classification on the Omniglot dataset and regression based on functions synthetically generated by a GP. The highlights are that:\n\n1. The proposed model performs much better than an LSTM and better than an NTM with the original write mechanism of Alex Graves (for classification).\n2. The proposed model even performs better than a 1st nearest neighbor classifier.\n3. The proposed model is even shown to outperform human performance, for the 5-class scenario.\n4. The proposed model has decent performance on the regression task, compared to GP predictions using the groundtruth kernel.\n\n**My two cents**\n\nThis is probably one of my favorite ICML 2016 papers. I really think meta-learning is a problem that deserves more attention, and this paper presents both an interesting proposal for how to do it and an interesting empirical investigation of it. \n\nMuch like previous work\u00a0[\\[1\\]][1] [\\[2\\]][2], learning is based on automatically generating a meta-learning training set. This is clever I think, since a very large number of such \"meta-learning\" examples (the episodes) can be constructed, thus transforming what is normally a \"small data problem\" (few shot learning) into a \"big data problem\", for which deep learning is more effective.\n\nI'm particularly impressed by how the proposed model outperforms a 1-nearest neighbor classifier. That said, the proposed NTM actually performs 4 reads at each time step, which suggests that a fairer comparison might be with a 4-nearest neighbor classifier. I do wonder how this baseline would compare.\n\nI'm also impressed with the observation that the proposed model surpassed humans.\n\nThe paper also proposes to use 5-letter words to describe classes, instead of one-hot vectors. The motivation is that this should make it easier for the model to scale to much more than 5 classes. However, I don't entirely follow the logic as to why one-hot vectors are problematic. In fact, I would think that arbitrarily assigning 5-letter words to classes would instead imply some similarity between classes that share letters that is arbitrary and doesn't reflect true class similarity. \n\nAlso, while I find it encouraging that the performance for regression of the proposed model is decent, I'm curious about how it would compare with a GP approach that incrementally learns the kernel's hyper-parameter (instead of using the groundtruth values, which makes this baseline unrealistically strong).\n\nFinally, I'm still not 100% sure how exactly the NTM is able to implement the type of feed-forward inference I'd expect to be required. I would expect it to learn a memory representation of examples that combines information from the input vector ${\\bf x}_t$ *and* its label $y_t$. However, since the label of an input is presented at the following time step in an episode, it is not intuitive to me then how the read/write mechanisms are able to deal with this misalignment. My only guess is that since the controller is an LSTM, then it can somehow remember ${\\bf x}_t$ until it gets $y_t$ and appropriately include the combined information into the memory. This could be supported by the fact that using a non-recurrent feed-forward controller is much worse than using an LSTM controller. But I'm not 100% sure of this either.\n\nAll the above being said, this is still a really great paper, which I hope will help stimulate more research on meta-learning. Hopefully code for this paper can eventually be released, which would help in popularizing the topic.\n\n[1]: http://snowedin.net/tmp/Hochreiter2001.pdf\n[2]: http://www.thespermwhale.com/jaseweston/ram/papers/paper_16.pdf\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.06065"
    },
    "12": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/EslamiHWTKH16",
        "transcript": "This paper presents an unsupervised generative model, based on the variational autoencoder framework, but where the encoder is a recurrent neural network that sequentially infers the identity, pose and number of objects in some input scene (2D image or 3D scene). \n\nIn short, this is done by extending the DRAW model to incorporate discrete latent variables that determine whether an additional object is present or not. Since the reparametrization trick cannot be used for discrete variables, the authors estimate the gradient through the sampling operation using a likelihood ratio estimator. \n\nAnother innovation over DRAW is the application to 3D scenes, in which the decoder is a graphics renderer. Since it is not possible to backpropagate through the renderer, gradients are estimated using finite-difference estimates (which require going through the renderer several times).\n\nExperiments are presented where the evaluation is focused on the ability of the model to detect and count the number of objects in the image or scene.\n\n**My two cents**\n\nThis is a nice, natural extension of DRAW. I'm particularly impressed by the results for the 3D scene setting. Despite the fact that setup is obviously synthetic and simplistic, I really surprised that estimating the decoder gradients using finite-differences worked at all. It's also interesting to see that the proposed model does surprisingly well compared to a CNN supervised approach that directly predicts the objects identity and pose. Quite cool!\n\nTo see the model in action, see [this cute video][1].\n\n[1]: https://www.youtube.com/watch?v=4tc84kKdpY4 ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.08575"
    },
    "13": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/CooijmansBLC16",
        "transcript": "This paper describes how to apply the idea of batch normalization (BN) successfully to recurrent neural networks, specifically to LSTM networks. The technique involves the 3 following ideas:\n\n**1) Careful initialization of the BN scaling parameter.** While standard practice is to initialize it to 1 (to have unit variance), they show that this situation creates problems with the gradient flow through time, which vanishes quickly. A value around 0.1 (used in the experiments) preserves gradient flow much better.\n\n**2) Separate BN for the \"hiddens to hiddens pre-activation and for the \"inputs to hiddens\" pre-activation.** In other words, 2 separate BN operators are applied on each contributions to the pre-activation, before summing and passing through the tanh and sigmoid non-linearities.\n\n**3) Use of largest time-step BN statistics for longer test-time sequences.** Indeed, one issue with applying BN to RNNs is that if the input sequences have varying length, and if one uses per-time-step mean/variance statistics in the BN transformation (which is the natural thing to do), it hasn't been clear how do deal with the last time steps of longer sequences seen at test time, for which BN has no statistics from the training set. The paper shows evidence that the pre-activation statistics tend to gradually converge to stationary values over time steps, which supports the idea of simply using the training set's last time step statistics.\n\nAmong these ideas, I believe the most impactful idea is 1). The papers mentions towards the end that improper initialization of the BN scaling parameter probably explains previous failed attempts to apply BN to recurrent networks.\n\nExperiments on 4 datasets confirms the method's success.\n\n**My two cents**\n\nThis is an excellent development for LSTMs. BN has had an important impact on our success in training deep neural networks, and this approach might very well have a similar impact on the success of LSTMs in practice.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.09025"
    },
    "14": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Weston16",
        "transcript": "This paper investigates different paradigms for learning how to answer natural language queries through various forms of feedback. Most interestingly, it investigates whether a model can learn to answer correctly questions when the feedback is presented purely in the form of a sentence (e.g. \"Yes, that's right\", \"Yes, that's correct\", \"No, that's incorrect\", etc.). This later form of feedback is particularly hard to leverage, since the model has to somehow learn that the word \"Yes\" is a sign of a positive feedback, but not the word \"No\". \n\nNormally, we'd trained a model to directly predict the correct answer to questions based on feedback provided by an expert that always answers correctly. \"Imitating\" this expert just corresponds to regular supervised learning.\n\nThe paper however explores other variations on this learning scenario. Specifically, they consider 3 dimensions of variations.\n\nThe first dimension of variation is who is providing the answers. Instead of an expert (who is always right), the paper considers the case where the model is instead observing a different, \"imperfect\" expert whose answers come from a fixed policy that answers correctly only a fraction of the time (the paper looked at 0.5, 0.1 and 0.01). Note that the paper refers to these answers as coming from \"the learner\" (which should be the model), but since the policy is fixed and actually doesn't depend on the model, I think one can also think of it as coming from another agent, which I'll refer to as the imperfect expert (I think this is also known as \"off policy learning\" in the RL world).\n\nThe second dimension of variation on the learning scenario that is explored is in the nature of the \"supervision type\" (i.e. nature of the labels). There are 10 of them (see Figure 1 for a nice illustration). In addition to the real expert's answers only (Type 1), the paper considers other types that instead involve the imperfect expert and fall in one of the two categories below:\n\n1. Explicit positive / negative rewards based on whether the imperfect expert's answer is correct.\n2. Various forms of natural language responses to the imperfect expert's answers, which vary from worded positive/negative feedback, to hints, to mentions of the supporting fact for the correct answer.\n\nAlso, mixtures of the above are considered.\n\nFinally, the third dimension of variation is how the model learns from the observed data. In addition to the regular supervised learning approach of imitating the observed answers (whether it's from the real expert or the imperfect expert), two other distinct approaches are considered, each inspired by the two categories of feedback mentioned above:\n\n1. Reward-based imitation: this simply corresponds to ignoring answers from the imperfect expert for which the reward is not positive (as for when the answers come from the regular expert, they are always used I believe). \n2. Forward prediction: this consists in predicting the natural language feedback to the answer of the imperfect expert. This is essentially treated as a classification problem over possible feedback (with negative sampling, since there are many possible feedback responses), that leverages a soft-attention architecture over the answers the expert could have given, which is also informed by the actual answer that was given (see Equation 2).\n\nAlso, a mixture of both of these learning approaches is considered.\n\nThe paper thoroughly explores experimentally all these dimensions, on two question-answering datasets (single supporting fact bAbI dataset and MovieQA). The neural net model architectures used are all based on memory networks. Without much surprise, imitating the true expert performs best. But quite surprisingly, forward prediction leveraging only natural language feedback to an imperfect expert often performs competitively compared to reward-based imitation.\n\n#### My two cents\nThis is a very thought provoking paper! I very much like the idea of exploring how a model could learn a task based on instructions in natural language. This makes me think of this work \\cite{conf/iccv/BaSFS15} on using zero-shot learning to learn a model that can produce a visual classifier based on a description of what must be recognized.\n\nAnother component that is interesting here is studying how a model can learn without knowing a priori whether a feedback is positive or negative. This sort of makes me think of [this work](http://www.thespermwhale.com/jaseweston/ram/papers/paper_16.pdf) (which is also close to this work \\cite{conf/icann/HochreiterYC01}) where a recurrent network is trained to process a training set (inputs and targets) to later produce another model that's applied on a test set, without the RNN explicitly knowing what the training gradients are on this other model's parameters. In other words, it has to effectively learn to execute (presumably a form of) gradient descent on the other model's parameters.\n\nI find all such forms of \"learning to learn\" incredibly interesting.\n\nComing back to this paper, unfortunately I've yet to really understand why forward prediction actually works. An explanation is given, that is that \"this is because there is a natural coherence to predicting true answers that leads to greater accuracy in forward prediction\" (see paragraph before conclusion). I can sort of understand what is meant by that, but it would be nice to somehow dig deeper into this hypothesis. Or I might be misunderstanding something here, since the paper mentions that changing how wrong answers are sampled yields a \"worse\" accuracy of 80% on Task 2 for the bAbI dataset and a policy accuracy of 0.1, but Table 1 reports an accuracy 54% for this case (which is not better, but worse).\n\nSimilarly, I'd like to better understand Equation 2, specifically the \u03b2* term, and why exactly this is an appropriate form of incorporating which answer was given and why it works. I really was unable to form an intuition around Equation 2.\n\nIn any case, I really like that there's work investigating this theme and hope there can be more in the future!",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.06045"
    },
    "15": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/OllivierC15",
        "transcript": "This paper suggests a method (NoBackTrack) for training recurrent neural networks in an online way, i.e. without having to do backprop through time. One way of understanding the method is that it applies the [forward method for automatic differentiation](//en.wikipedia.org/wiki/Automatic_differentiation#Forward_accumulation), but since it requires maintaining a large Jacobian matrix (nb. of hidden units times nb. of parameters), they propose a way of obtaining a stochastic (but unbiased!) estimate of that matrix. Moreover, the method is improved by using Kalman filtering on that estimate, effectively smoothing the estimate over time.\n\n#### My two cents\n\nOnline training of RNNs is a big, unsolved problem. The current approach people use is to truncate backprop to only a few steps in the past, which is more of a heuristic.\n\nThis paper makes progress towards a more principled approach. I really like the \"rank-one trick\" of Equation 7, really cute! And it is quite central to this method too, so good job on connecting those dots!\n\nThe authors present this work as being preliminary, and indeed they do not compare with truncated backprop. I really hope they do in a future version of this work. \n\nAlso, I don't think I buy their argument that the \"theory of stochastic gradient descent applies\". Here's the reason. So the method tracks the Jacobian of the hidden state wrt the parameter, which they note $G(t)$. It is update into $G(t+1)$, using a recursion which is based on the chain rule. However, between computing $G(t)$ and $G(t+1)$, a gradient step is performed during training. This means that $G(t)$ is now slightly stale, and corresponds to the gradient with respect to old value of the parameters, not the current value. As far as I understand, this implies that $G(t+1)$ (more specifically, its stochastic estimate as proposed in this paper) isn't unbiased anymore. So, unless I'm missing something (which I might!), I don't think we can invoke the theory of SGD as they suggest. \n\nBut frankly, that last issue seems pretty unavoidable in the online setting. I suspect this will never be solved, and future research will have to somehow have to design learning algorithms that are robust to this issue (or develop new theory that shows it isn't one).\n\nSo overall, kudos to the authors, and I'm really looking forward to read more about where this research goes!",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1507.07680"
    },
    "16": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BouchardTPG15",
        "transcript": "SGD is a widely used optimization method for training the parameters of some model f on some given task. Since the convergence of SGD is related to the variance of the stochastic gradient estimate, there's been a lot of work on trying to come up with such stochastic estimates with smaller variance. This paper does it using an importance sampling (IS) Monte Carlo estimate of the gradient, and learning the proposal distribution $q$ of the IS estimate. \n\nThe proposal distribution $q$ is parametrized in some way, and is trained to minimize the variance of the gradient estimate. It is trained simultaneously while the model $f$ that SGD (i.e. the SGD that uses IS to get its gradient) is training. To make this whole story more recursive, the proposal distribution $q$ is also trained with SGD :-) This makes sense, since one expects the best proposal to depend on the value of the parameters of model $f$, so the best proposal $q$ should vary as $f$ is trained.\n\nOne application of this idea is in optimizing a classification model over a distribution that is imbalanced class-wise (e.g. there are classes with much fewer examples). In this case, the proposal distribution determines how frequently we sample examples from each class (conditioned on the class, training examples are chosen uniformly).\n\n\n#### My two cents\n\nThis is a really cool idea. I particularly like the application to training on an imbalanced classification problem. People have mostly been using heuristics to tackle this problem, such as initially sampling each class equally as often, and then fine-tuning/calibrating the model using the real class proportions. This approach instead proposes a really elegant, coherent, solution to this problem.\n\nI would have liked to see a comparison with that aforementioned heuristic (for mainly selfish reasons :-) ). They instead compare with an importance sampling approach with proposal that assigns the same probability to each class, which is a reasonable alternative (though I don't know if it's used as often as the more heuristic approach).\n\nThere are other applications, to matrix factorization and reinforcement learning, that are presented in the paper and seem neat, though I haven't gone through those as much.\n\nOverall, one of my favorite paper this year: it's original, tackles a problem for which I've always hated the heuristic solution I'm using now, proposes an elegant solution to it, and is applicable even more widely than that setting.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.09016"
    },
    "17": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/RasmusBHVR15",
        "transcript": "This paper describes a learning algorithm for deep neural networks that can be understood as an extension of stacked denoising autoencoders. In short, instead of reconstructing one layer at a time and greedily stacking, a unique unsupervised objective involving the reconstruction of all layers is optimized jointly by all parameters (with the relative importance of each layer cost controlled by hyper-parameters). \n\nIn more details:\n\n* The encoding (forward propagation) adds noise (Gaussian) at all layers, while decoding is noise-free.\n* The target at each layer is the result of noise-less forward propagation.\n* Direct connections (also known as skip-connections) between a layer and its decoded reconstruction are used. The resulting encoder/decoder architecture thus ressembles a ladder (hence the name Ladder Networks).\n* Miniature neural networks with a single hidden unit and skip-connections are used to decode the left and top layers into a reconstruction. Each network is applied element-wise (without parameter sharing across reconstructed units).\n* The unsupervised objective is combined with a supervised objective, corresponding to the regular negative class log-likelihood objective (using an output softmax layer). Two losses are used for each input/target pair: one based on the noise-free forward propagation (which also provides the target of the denoising objective) and one with the noise added (which also corresponds to the encoding stage of the unsupervised autoencoder objective).\nBatch normalization is used to train the network.\nSince the model combines unsupervised and supervised learning, it can be used for semi-supervised learning, where unlabeled examples can be used to update the network using the unsupervised objective only. State of the art results in the semi-supervised setting are presented, for both the MNIST and CIFAR-10 datasets.\n\n#### My two cents\n\nWhat I find most exciting about this paper is its performance. On MNIST, with only 100 labeled examples, it achieves 1.13% error! That is essentially the performance of stacked denoising autoencoders, trained on the entire training set (though that was before ReLUs and batch normalization, which this paper uses)! This confirms a current line of thought in Deep Learning (DL) that, while recent progress in DL applied on large labeled datasets does not rely on any unsupervised learning (unlike at the \"beginning\" of DL in the mid 2000s), unsupervised learning might instead be crucial for success in low-labeled data regime, in the semi-supervised setting.\n\nUnfortunately, there is one little issue in the experiments, disclosed by the authors: while they used few labeled examples for training, model selection did use all 10k labels in the validation set. This is of course unrealistic. But model selection in the low data regime is arguably, in itself, an open problem. So I like to think that this doesn't invalidate the progress made in this paper, and only suggests that some research needs to be done on doing effective hyper-parameter search with a small validation set.\n\nGenerally, I really hope this paper will stimulate more research on DL methods to the specific case of small labeled dataset / large unlabeled dataset. While this isn't a problem that is as \"flashy\" as tasks such as the ImageNet Challenge which comes with lots of labeled data, I think this is a crucial research direction for AI in general. Indeed, it seems naive to me to expect that we will be able to collect large labeled dataset for each and every task, on our way to real AI.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks"
    },
    "18": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/PengLLW15",
        "transcript": "This paper presents a neural network architecture that can take as input a question and a sequence of facts expressed in natural language (i.e. a sequence of words) and produce its output the answer to that question. The main components of the architecture are as follows:\n\n* The question (q) and the facts (f_1, ... , f_K) are each individually transformed into a fixed size vector using the same GRU RNN (with the last hidden layer serving as the vector representation).\n* These vectors are each passed through \"reasoning layers\", where each layer transforms the question q and the facts f_k into a new vector representation. This is done by feeding each question fact pair (q,f_k) to a neural network that outputs a new representation for the fact f_k (which replaces its old representation in the layer), as well as a new representation for the question. All K new question representations are then pooled to obtain a single question representation that replace the old one in the layer.\n* The last reasoning layer is either fed to a softmax layer for binary questions, or to a scoring layer for questions with multiple and varying candidate answers.\n\nThis so-called Neural Reasoner can be trained by backpropagation, in an end-to-end, supervised way. The authors also suggest the use of auxiliary tasks, to improve results. The first (\"original\") adds an autoencoder reconstuction cost, that reproduces the question and facts from its first layer encoding. The second (\"abstract\") instead reconstructs a more abstract version of the sentences (e.g. \"The triangle is above the pink rectangle.\" becomes \"x is above y\").\n\nImportantly, while the Neural Reasoner framework is presented in this paper as covering many different variants, the version that is experimentally tested is one where the fact representations f_k are actually left unchanged throughout the reasoning layers, with only the question representation being changed.\n\nThe paper presents experiments on two synthetic reasoning tasks and report performances that compare favorably with previously published alternatives (based on the general Memory Network architecture). The experiments also show that the auxiliary tasks can substantially improve the performance of the model\n\n\n#### My two cents\n\nThe proposed Neural Reasoner framework is actually very close to work published on arXiv at about the same time on End-to-End Memory Networks \\cite{conf/nips/SukhbaatarSWF15}. In fact, the version tested in the paper, with unchanged fact representations throughout layers, is extremely close to End-to-End Memory Networks. \n\nThat said, there are also lots of differences. For instance, this paper proposes the use of multilayer networks within each Reasoning Layer, to produce updated question representations. In fact, experiments suggest that using several layers can be very beneficial for the path finding task. The sentence representation at the first layer is also different, being based on a non-linear RNN instead of being based on linear operations on embeddings as in Memory Networks.\n\nThe most interesting aspect of this paper to me is probably the demonstration that the use of an auxiliary task such as \"original\", which is unsupervised, can substantially improve the performance, again for the path finding task. That is, to me, probably the most exciting direction of future research that this paper highlights as promising.\n\nI also liked how the model is presented. It didn't take me much time to understand the model, and I actually found it easier to absorb than the Memory Network model, despite both being very similar. I think this model is indeed a bit simpler than Memory Networks, which is a good thing. It also suggests a different approach to the problem, one where the facts representations are also updated during forward propagation, not just the question's representation (which is the version initially described in the paper... I hope experiments on that variant are eventually presented).\n\nIt's unfortunate that the authors only performed experiments on 2 of the 20 synthetic question-answering tasks. I hope a future version of this work can report results on the full benchmark and directly compare with End-to-End Memory Networks. \n\nI was also unable to find out which of the question representation pooling mechanism (section 3.2.2) was used in the experiments. Perhaps the authors forgot to state it?\n\nOverall, a pretty interesting paper that open different doors towards reasoning with neural networks.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1508.05508"
    },
    "19": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BurdaGS15",
        "transcript": "This paper proposes to train a neural network generative model by optimizing an importance sampling (IS) weighted estimate of the log probability under the model. The authors show that the case of an estimate based on a single sample actually corresponds to the learning objective of variational autoencoders (VAE). Importantly, they exploit this connection by showing that, similarly to VAE, a gradient can be passed through the approximate posterior (the IS proposal) samples, thus yielding an importance weighted autoencoder (IWAE). The authors also show that, by using more samples, this objective, which is a lower bound of the actual log-likelihood, becomes an increasingly tighter approximation to the log-likelihood. In other words, the IWAE is expected to better optimize the real log-likelihood of the neural network, compared to VAE.\n\nThe experiments presented show that the model achieves competitive performance on a version of the binarized MNIST benchmark and on the Omniglot dataset.\n\n#### My two cents\n\nThis is a really neat contribution! While simple (both conceptually and algorithmically), it really seems to be an important step forward for the VAE framework. I really like the theoretical result showing that IWAE provides a better approximation to the real log-likelihood, it's quite neat and provides an excellent motivation for the method.\n\nThe results on binarized MNIST are certainly impressive. Unfortunately, it appears that the training setup isn't actually comparable to the majority of published results on this dataset. Indeed, it seems that they didn't use the stochastic but *fixed* binarization of the inputs that other publications on this benchmark have used (since my paper on NADE with Iain Murray, we've made available that fixed training set for everyone to use, along with fixed validation and test sets as well). I believe instead they've re-sampled the binarization for each minibatch, effectively creating a setup with a somewhat larger training set than usual. It's unfortunate that this is the case, since it makes this result effectively impossible to compare directly with previous work.\n\nI'm being picky on this issue only because I'm super interested in this problem (that is of generative modeling with neural networks) and this little issue is pretty much the only thing that stops this paper from being a slam dunk. Hopefully the authors (or perhaps someone interested in reimplementing IWAE) can clarify this question eventually.\n\nOtherwise, it seems quite clear to me that IWAE is an improvement over VAE. The experiments of section 5.2, showing that fine-tuning a VAE model with IWAE training improves performance, while fine-tuning a IWAE model using VAE actually makes things worse, is further demonstration that IWAE is indeed a good idea.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1509.00519"
    },
    "20": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/BengioVJS15",
        "transcript": "This paper considers the problem of structured output prediction, in the specific case where the output is a sequence and we represent the sequence as a (conditional) directed graphical model that generates from the first token to the last. The paper starts from the observation that training such models by maximum likelihood (ML) does not reflect well how the model is actually used at test time. Indeed, ML training implies that the model is effectively trained to predict each token conditioned on the previous tokens *from the ground truth* sequence (this is known as \"teacher forcing\"). Yet, when making a prediction for a new input, the model will actually generate a sequence by generating tokens one after another and conditioning on *its own predicted tokens* instead. \n\nSo the authors propose a different training procedure, where at training time each *conditioning* ground truth token is sometimes replaced by the model's previous prediction. The choice of replacing the ground truth by the model's prediction is made by \"flipping a coin\" with some probability, independently for each token. Importantly, the authors propose to start with a high probability of using the ground truth (i.e. start close to ML) and anneal that probability closer to 0, according to some schedule (thus the name Schedule Sampling). \n\nExperiments on 3 tasks (image caption generation, constituency parsing and speech recognition) based on neural networks with LSTM units, demonstrate that this approach indeed improves over ML training in terms of the various performance metrics appropriate for each problem, and yields better sequence prediction models. \n\n\n\n#### My two cents\n\nBig fan of this paper. It both identifies an important flaw in how sequential prediction models are currently trained and, most importantly, suggests a solution that is simple yet effective. I also believe that this approach played a non-negligible role in Google's winner system for image caption generation, in the Microsoft COCO competition. \n\nMy alternative interpretation of why Scheduled Sampling helps is that ML training does not inform the model about the relative quality of the errors it can make. In terms of ML, it is as bad to put high probability on an output sequence that has just 1 token that's wrong, than it is to put the same amount of probability on a sequence that has all tokens wrong. Yet, say for image caption generation, outputting a sentence that is one word away from the ground truth is clearly preferable from making a mistake on a words (something that is also reflected in the performance metrics, such as BLEU). \n\nBy training the model to be robust to its own mistakes, Scheduled Sampling ensures that errors won't accumulate and makes predictions that are entirely off much less likely.\n\nAn alternative to Scheduled Sampling is DAgger (Dataset Aggregation: \\cite{journals/jmlr/RossGB11}), which briefly put alternates between training the model and adding to the training set examples that mix model predictions and the ground truth. However, Scheduled Sampling has the advantage that there is no need to explicitly create and store that increasingly large dataset of sampled examples, something that isn't appealing for online learning or learning on large datasets.\n\nI'm also very curious and interested by one of the direction of future work mentioned in the conclusion: figuring out a way to backprop through the stochastic predictions made by the model. Indeed, as the authors point out, the current algorithm ignores the fact that, by sometimes taking as input its previous prediction, this induces an additional relationship between the model's parameters and its ultimate prediction, a relationship that isn't taken into account during training. To take it into account, you'd need to somehow backpropagate through the stochastic process that generated the previous token prediction. While the work on variational autoencoders has shown that we can backprop through gaussian samples, backpropagating through the sampling of a discrete multinomial distribution is essentially an open problem. I do believe that there is work that tried to tackle propagating through stochastic binary units however, so perhaps that's a start. Anyways, if the authors could make progress on that specific issue, it could be quite useful not just in the context of Schedule Sampling, but possibly in the context of training networks with discrete stochastic units in general!",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks"
    },
    "21": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GalG15",
        "transcript": "This paper presents an interpretation of dropout training as performing approximate Bayesian learning in a deep Gaussian process (DGP) model. This connection suggests a very simple way of obtaining, for networks trained with dropout, estimates of the model's output uncertainty. This estimate is based and computed from an ensemble of networks each obtained by sampling a new dropout mask.\n\n#### My two cents\n\nThis is a really nice and thought provoking contribution to our understanding of dropout. Unfortunately, the paper in fact doesn't provide a lot of comparisons with either other ways of estimating the predictive uncertainty of deep networks, or to other approximate inference schemes in deep GPs (actually, see update below). The qualitative examples provided however do suggest that the uncertainty estimate isn't terrible.\n\nIrrespective of the quality of the uncertainty estimate suggested here, I find the observation itself really valuable. Perhaps future research will then shed light on how useful that method is compared to other approaches, including Bayesian dark knowledge \\cite{conf/nips/BalanRMW15}.\n\n`Update: On September 27th`, the authors uploaded to arXiv a new version that now includes comparisons with 2 alternative Bayesian learning methods for deep networks, specifically the stochastic variational inference approach of Graves and probabilistic back-propagation of Hernandez-Lobato and Adams. Dropout actually does very well against these baselines and, across datasets, is almost always amongst the best performing method!",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.02142"
    },
    "22": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/BlumHP15",
        "transcript": "This paper starts by introducing a trick to reduce the variance of stochastic gradient variational Bayes (SGVB) estimators. In neural networks, SGVB consists in learning a variational (e.g. diagonal Gaussian) posterior over the weights and biases of neural networks, through a procedure that (for the most part) alternates between adding (Gaussian) noise to the model's parameters and then performing a model update with backprop. \n\nThe authors present a local reparameterization trick, which exploits the fact that the Gaussian noise added into the weights could instead be added directly into the pre-activation (i.e. before the activation fonction) vectors during forward propagation. This is due to the fact that computing the pre-activation is a linear operation, thus noise at that level is also Gaussian. The advantage of doing so is that, in the context of minibatch training, one can efficiently then add independent noise to the pre-activation vectors for each example of the minibatch. The nature of the local reparameterization trick implies that this is equivalent to using one corrupted version of the weights for each example in the minibatch, something that wouldn't be practical computationally otherwise. This is in fact why, in normal SGVB, previous work would normally use a single corrupted version of the weights for all the minibatch. \n\nThe authors demonstrate that using the local reparameterization trick yields stochastic gradients with lower variance, which should improve the speed of convergence.\n\nThen, the authors demonstrate that the Gaussian version of dropout (one that uses multiplicative Gaussian noise, instead of 0-1 masking noise) can be seen as the local reparameterization trick version of a SGVB objective, with some specific prior and variational posterior. In this SGVB view of Gaussian dropout, the dropout rate is an hyper-parameter of this prior, which can now be tuned by optimizing the variational lower bound of SGVB. In other words, we now have a method to also train the dropout rate! Moreover, it becomes possible to tune an individual dropout rate parameter for each layer, or even each parameter of the model.\n\nExperiments on MNIST confirm that tuning that parameter works and allows to reach good performance of various network sizes, compared to using a default dropout rate. \n\n\n##### My two cents\n\nThis is another thought provoking connection between Bayesian learning and dropout. Indeed, while Deep GPs have allowed to make a Bayesian connection with regular (binary) dropout learning \\cite{journals/corr/GalG15}, this paper sheds light on a neat Bayesian connection for the Gaussian version of dropout. This is great, because it suggests that Gaussian dropout training is another legit way of modeling uncertainty in the parameters of neural networks. It's also nice that that connection also yielded a method for tuning the dropout rate automatically.\n\nI hope future work (by the authors or by others) can evaluate the quality of the corresponding variational posterior in terms of estimating uncertainty in the network and, in particular, in obtaining calibrated output probabilities.\n\nLittle detail: I couldn't figure out whether the authors tuned a single dropout rate for the whole network, or used many rates, for instance one per parameter, as they suggest can be done.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick"
    },
    "23": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/KondaBMV15",
        "transcript": "This paper suggests a novel explanation for why dropout training is helpful: because it corresponds to an adaptive data augmentation method. Indeed, the authors point out that, when sampling a mask of the hidden units in a network (effectively setting the corresponding units to 0), the same effect would have been obtained by feeding as input an example tailored to yield activations of 0 for these units and otherwise the same activation for all other units. Since this \"ghost\" example will have to be different from the original example, and since each different mask would correspond to a different \"ghost\" example, then effectively mask sampling is similar to data augmentation.\n\nWhile in practice finding a ghost example that replicates exactly the same dropout hidden activations might not be possible, the authors show that finding an \"approximate\" ghost example that minimizes a distance between the target dropout activation and the deterministic activation of the ghost example works well. Indeed, they show that training a deep neural net on additional data generated by this procedure yields results that are at least as good as regular dropout on MNIST and CIFAR-10 (actually, the deterministic neural net still uses regular dropout at the input layer, however they do show that the additional ghost examples are necessary to match the neural net trained with dropout at all layers).\n\nThen the authors use that interpretation to justify a variation of dropout where the dropout rate isn't fixed, but itself is randomly sampled in some range for each example. Indeed, if we think of dropout at a fixed rate as a specific class of ghost data being added, varying the dropout rate corresponds to enriching even more the ghost data pool. The experiments show that this can help, though not by much.\n\nFinally, the authors propose an explanation of a property of dropout: that it tends to generate hidden representations that are sparser. Again, the authors rely on their interpretation of dropout as data augmentation. The explanation goes as follows. Training on the ghost data distribution might imply that the classification problem has become significantly harder. Specifically, it is quite possible that the addition of new ghost examples generates new isolated class clusters in input space that the model most now learn to  discriminate. And they hypothesize that the generation of such additional clusters would encourage sparsity. To test this hypothesis, the authors synthetically simulate this scenario, by sampling data on a circle, which is clustered in small arcs each assigned to one of 10 possible classes in cycling order. Decreasing the arc length thus increases the number of arcs, i.e. class clusters. They show that training deep networks on datasets with increasing number of class clusters does yield representations that are increasingly sparser. This thus suggests that dropout might indeed be equivalent to modifying the input distribution by adding such isolated class-specific clusters in input space. \n\nOne assumption behind this analysis is that the sparsity patterns (i.e. the set of non-zero dimensions) play an important role in classification and incorporate most of the discriminative class information. This assumption is also confirmed in experiments, where converting the ReLU activation function by a binary activation (that is 1 if the pre-activation is positive and 0 otherwise) after training still yields a network with good performance (though slightly worse).\n\n\n#### My two cents\n\nThis is a really original and thought provoking paper. One interpretation I make of these results is that the inductive bias corresponding to using a deep neural network with ReLU activations is more valuable than one might have thought, and that the usefulness of deep neural networks goes beyond just being black boxes that can learn data-dependent representations. Otherwise, it's not clear to me why the ghost data implicitly generated by the architecture would be useful at all. This also suggests an experiment where such ghost samples would be fed to  another type of classifier, such as an SVM, to test whether the data augmentation is useful in itself and reflects meaningful structure in the data, as opposed to being somehow useful only for neural nets.\n\nI note that the results are mostly specific to architectures based on ReLU activations (not that this is a problem, but one should keep this in mind).\n\nI'd really like to see what the ghost samples look like. Do they correspond to interpretable images? The authors also mention that exploring how the samples change with training would be interesting to investigate, and I agree.\n\nFinally, I think there might be a typo in Figure 1. While the labels of a) and b) states that the arc length is smaller for a) than b), the plot clearly show otherwise.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.08700"
    },
    "24": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GreffSKSS15",
        "transcript": "This paper presents an extensive evaluation of variants of LSTM networks. Specifically, they start from what they consider to be the vanilla architecture and, from it, also consider 8 variants which correspond to small modifications on the vanilla case. The vanilla architecture is the one described in Graves & Schmidhuber (2005) \\cite{journals/nn/GravesS05}, and the variants consider removing single parts of it (input,forget,output gates or activation functions), coupling the input and forget gate (which is inspired from GRU) or having full recurrence between all gates (which comes from the original LSTM formulation).\n\nIn their experimental setup, they consider 3 datasets: TIMIT (speech recognition), IAM Online Handwriting Database (character recognition) and JSB Chorales (polyphonic music modeling). For each, they tune the hyper-parameters of each of the 9 architectures, using random search based on 200 samples. Then, they keep the 20 best hyper-parameters and use the statistics of those as a basis for comparing the architectures.\n\n#### My two cents\n\nThis was a very useful ready. I'd make it a required read for anyone that wants to start using LSTMs. First, I found the initial historical description of the developments surrounding LSTMs very interesting and clarifying. But more importantly, it presents a really useful picture of LSTMs that can both serve as a good basis for starting to use LSTMs and also an insightful (backed with data) exposition of the importance of each part in the LSTM.\n\nThe analysis based on an fANOVA (which I didn't know about until now) is quite neat. Perhaps the most surprising observation is that momentum actually doesn't seem to help that much. Investigating second order interaction between hyper-parameters was a smart thing to do (showing that tuning the learning rate and hidden layer jointly might not be that important, which is a useful insight).The illustrations in Figure 4, layout out the estimated relationship (with uncertainty) between learning rate / hidden layer size / input noise variance and performance / training time is also full of useful information. \n\nI wont repeat here the main observations of the paper, which are laid out clearly in the conclusion (section 6). \n\nAdditionally, my personal take-away point is that, in an LSTM implementation, it might still be useful to support the removal peepholes or having coupled input and forget gates, since they both yielded the ultimate best test set performance on at least one of the datasets (I'm assuming it was also best on the validation set, though this might not be the case...)\n\nThe fANOVE analysis makes it clear that the learning rate is the most critical hyper-parameter to tune (can be \"make or break\"). That said, this is already well known. And the fact that it explains so much of the variance might reflect a bias of the analysis towards a situation where the learning rate isn't tuned as well as it could be in practice (this is afterall THE hyper-parameter that neural net researcher spend the most time tuning in practice). So, as future work, this suggests perhaps doing another round of the same analysis (which is otherwise really neatly setup), where more effort is always put on tuning the learning rate, individually for each of the other hyper-parameters. In other words, we'd try to ignore the regions of hyper-parameter space that correspond to bad learning rates, in order to \"marginalize out\" its effect. This would thus explore the perhaps more realistic setup that assumes one always tunes the learning rate as best as possible. \n\nAlso, considering a less aggressive gradient clipping into the hyper-parameter search would be interesting since, as the authors admit, clipping within [-1,1] might have been too much and could explain why it didn't help \n\nOtherwise, a really great and useful read!",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1503.04069"
    },
    "25": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HermannKGEKSB15",
        "transcript": "This paper explores the problem of question answering based on natural text. While this has been explored recently in the context of Memory Networks, the problems tackled so far have been synthetically generated. In this paper, the authors propose to extract from news sites more realistic question answering examples, by treating the main body of a news article as the content (the \"facts\") and extracting questions from the article's bullet point summaries. Specifically, by detecting the entities in these bullet points and replacing them with a question place older (e.g. \"Producer X will not press charges\"), they are able to generate queries which, while grammatically not being questions, do require to perform a form of question answering. Thanks to this procedure, two large *supervised* datasets are created, with several thousands of questions, based on the CNN and Daily Mail news sites.\n\nThen, the authors investigate neural network based systems for solving this task. They consider a fairly simple Deep LSTM network, which is first fed the article's content and then the query. They also consider two architectures that incorporate an attentional mechanism, based on softmax weighting. The first (\"Attentive Reader\") attends once in the document (i.e. uses a single softmax weight vector) while the second (\"Impatient Reader\") attends after every word in the query (akin to the soft attention architecture in the \"Show Attend and Tell\" paper). \n\nThese neural network architectures are also compared with simpler baselines, which are closer to what a more \"classical\" statistical NLP solution might look like.\n\nResults on both datasets demonstrate that the neural network approaches have superior performance, with the attentional models being significantly better than the simpler Deep LSTM model.\n\n\n#### My two cents\n\nThis is welcome development in the research on reasoning models based on neural networks. I've always thought it was unfortunate that the best benchmark available is based on synthetically generated cases. This work fixes this problem in a really clever way, while still being able to generate a large amount of training data. Particularly clever is the random permutation of entity markers when processing each case. Thanks to that, a system cannot simply use general statistics on words to answer questions (e.g. just from the query \"The hi-tech bra that helps you beat breast X\" it's obvious that \"cancer\" is an excellent answer). In this setup, the system is forced to exploit the content of the article, thus ensuring that the benchmark is indeed measuring the system's question-answering abilities. \n\nSince the dataset itself is an important contribution of this paper, I hope the authors release it publicly in the near future.\n\nThe evaluation of the different neural architectures is also really thoroughly done. The non-neural baselines are reasonable and the comparison between the neural nets is itself interesting, bringing more evidence that the softmax weighted attentional mechanism (which has been gaining in popularity) indeed brings something over a regular LSTM approach.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend"
    },
    "26": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/StadieLA15",
        "transcript": "The main idea in this paper is to use the agent's ability to predict observations at the next step as a measure of how much exploration of that action should be encouraged. This prediction is based on a deep architecture, specifically a deep autoencoder representation of observations, and accuracy of prediction is measured at the level of that learned, deep representation. Exploration is encourage by increasing the reward whenever the models prediction of the representation at the next time step is bad.\n\n#### My two cents\n\nI'm not sure how novel this idea is in RL, but at the very least it's interesting that it was explored the way it was here, with deep learning. As a non-expert in RL, I certainly enjoyed reading the paper. Also, this implements nicely an idea that just seems like common sense, as an exploration strategy for an agent: actions that merit exploration are those that yield results that are unexpected to you. \n\nIt will be interesting to see if this general approach will be able to exploit upcoming progress in the development of better generative deep learning models, an area that is currently very active.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1507.00814"
    },
    "27": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/VincentBB15",
        "transcript": "This paper presents a linear algebraic trick for computing both the value and the gradient update for a loss function that compares a very high-dimensional target with a (dense) output prediction. Most of the paper exposes the specific case of the squared error loss, though it can also be applied to some other losses such as the so-called spherical softmax. One use case could be for training autoencoders with the squared error on very high-dimensional but sparse inputs. While a naive (i.e. what most people currently do) implementation would scale in $O(Dd)$ where $D$ is the input dimensionality and d the hidden layer dimensionality, they show that their trick allows to scale in $O(d^2)$. \n\nTheir experiments show that they can achieve speedup factors of over 500 on the CPU, and over 1500 on the GPU. \n\n#### My two cents\n\nThis is a really neat, and frankly really surprising, mathematical contribution. I did not suspect getting rid of the dependence on D in the complexity would actually be achievable, even for the \"simpler\" case of the squared error. \n\nThe jury is still out as to whether we can leverage the full power of this trick in practice. Indeed, the squared error over sparse targets isn't the most natural choice in most situations. The authors did try to use this trick in the context of a version of the neural network language model that uses the squared error instead of the negative log-softmax (or at least I think that's what was done... I couldn't confirm this with 100% confidence). They showed that good measures of word similarity (Simlex-999) could be achieved in this way, though using the hierarchical softmax actually achieves better performance in about the same time. \n\nBut as far as I'm concerned, that doesn't make the trick less impressive. It's still a neat piece of new knowledge to have about reconstruction errors. Also, the authors mention that it would be possible to adapt the trick to the so-called (negative log) spherical softmax, which is like the softmax but where the numerator is the square of the pre-activation, instead of the exponential. I hope someone tries this out in the future, as perhaps it could be key to making this trick a real game changer!",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5865-efficient-exact-gradient-update-for-training-deep-networks-with-very-large-sparse-targets"
    },
    "28": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MohamedR15",
        "transcript": "This paper presents a variational approach to the maximisation of mutual information in the context of a reinforcement learning agent. Mutual information in this context can provide a learning signal to the agent that is \"intrinsically motivated\", because it relies solely on the agent's state/beliefs and does not require from the (\"outside\") user an explicit definition of rewards.\n\nSpecifically, the learning objective, for a current state s, is the mutual information between the sequence of K actions a proposed by an exploration distribution $w(a|s)$ and the final state s' of the agent after performing these actions. To understand what the properties of this objective, it is useful to consider the form of this mutual information as a difference of conditional entropies:\n\n$$I(a,s'|s) = H(a|s) - H(a|s',s)$$\n\nWhere $I(.|.)$ is the (conditional) mutual information and $H(.|.)$ is the (conditional) entropy. This objective thus asks that the agent find an exploration distribution that explores as much as possible (i.e. has high $H(a|s)$ entropy) but is such that these actions have predictable consequences (i.e. lead to predictable state s' so that $H(a|s',s)$ is low). So one could think of the agent as trying to learn to have control of as much of the environment as possible, thus this objective has also been coined as \"empowerment\".\n\nThe main contribution of this work is to show how to train, on a large scale (i.e. larger state space and action space) with this objective, using neural networks. They build on a variational lower bound on the mutual information and then derive from it a stochastic variational training algorithm for it. The procedure has 3 components: the exploration distribution $w(a|s)$, the environment $p(s'|s,a)$ (can be thought as an encoder, but which isn't modeled and is only interacted with/sampled from) and the planning model $p(a|s',s)$ (which is modeled and can be thought of as a decoder). The main technical contribution is in how to update the exploration distribution (see section 4.2.2 for the technical details).\n\nThis approach exploits neural networks of various forms. Neural autoregressive generative models are also used as models for the exploration distribution as well as the decoder or planning distribution. Interestingly, the framework allows to also learn the state representation s as a function of some \"raw\" representation x of states. For raw states corresponding to images (e.g. the pixels of the screen image in a game), CNNs are used.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5668-variational-information-maximisation-for-intrinsically-motivated-reinforcement-learning"
    },
    "29": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/BalanRMW15",
        "transcript": "This paper combines two ideas. The first is stochastic gradient Langevin dynamics (SGLD), which is an efficient Bayesian learning method for larger datasets, allowing to efficiently sample from the posterior over the parameters of a model (e.g. a deep neural network). In short, SGLD is stochastic (minibatch) gradient descent, but where Gaussian noise is added to the gradients before each update. Each update thus results in a sample from the SGLD sampler. To make a prediction for a new data point, a number of previous parameter values are combined into an ensemble, which effectively corresponds to Monte Carlo estimate of the posterior predictive distribution of the model.\n\nThe second idea is distillation or dark knowledge, which in short is the idea of training a smaller model (student) in replicating the behavior and performance of a much larger model (teacher), by essentially training the student to match the outputs of the teacher. \n\nThe observation made in this paper is that the step of creating an ensemble of several models (e.g. deep networks) can be expensive, especially if many samples are used and/or if each model is large. Thus, they propose to approximate the output of that ensemble by training a single network to predict to output of ensemble. Ultimately, this is done by having the student predict the output of a teacher corresponding to the model with the last parameter value sampled by SGLD.\n\nInterestingly, this process can be operated in an online fashion, where one alternates between sampling from SGLD (i.e. performing a noisy SGD step on the teacher model) and performing a distillation update (i.e. updating the student model, given the current teacher model). The end result is a student model, whose outputs should be calibrated to the bayesian predictive distribution.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5965-bayesian-dark-knowledge"
    },
    "30": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/AgrawalCM15",
        "transcript": "This paper proposes to learn features for images using neural networks that predict the relative motion of the camera that captured two successive images. The main motivation for this approach is that such data would be very cheap to collect, as it would not require any labelling by a human and only relies on \"egomotion\" (and thus readily available) information. More concretely, what must be predicted is the X/Y/Z rotation or translation movements. This is converted into a classification problem by binning each movement into a fixed number of ranges of movement magnitude. The neural network architecture then consists in a siamese-style CNN (SCNN). First two Base-CNN (BCNN) with tied weights process the input image pair (one image per BCNN) to produce features for each image. These features are then concatenated and fed to a Top-CNN (TCNN) which produces a prediction for the relative transformation that relates the two images. The output layer thus contains groups of softmax units, one for each dimension of variation of the transformation (e.g. 3 for X/Y/Z rotation). \n\nThe experiments show that pretraining on this task is competitive with pretraining a CNN on the same amount of ImageNet classification data.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ICCV.2015.13"
    },
    "31": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/AuvolatV15",
        "transcript": "`Update 2015/11/23: Since I first wrote this note, I became involved in the next iterations of this work, which became v2 of the arXiv manuscript. The notes below were made based on v1.`\n\nThis paper considers the problem of Maximum Inner Product Search (MIPS). In MIPS, given a query $q$ and a set of inputs $x_i$, we want to find the input (or the top n inputs) with highest inner product, i.e. $argmax_i q' x_i$.\n\nRecently, it was shown that a simple transformation to the query and input vectors made it possible to approximately solve MIPS using hashing methods for Maximum Cosine Similarity Search (MCSS), a problem for which solutions are readily available (see section 2.4 for a brief but very clear description of the transformation). \n\nIn this paper, the authors combine this approach with clustering, in order to improve the quality of retrieved inputs. Specifically, they consider the spherical k-means algorithm, which is a variant of k-means in which data points are clustered based on cosine similarity instead of the euclidean similarity (in short, data points are first scaled to be of unit norm, then in the training inner loop points are assigned to the cluster centroid with highest dot product and cluster centroids are updated as usual, except that they are always rescaled to unit norm). Moreover, they consider a bottom-up application of the algorithm to yield a hierarchical clustering tree.\n\nThey propose to use such a hierarchical clustering tree to find the top-n candidates for MIPS. The key insight here is that, since spherical k-means relies on cosine similarity for finding the best cluster, and since we have a transformation that allows the maximisation of inner product to be approximated by the maximisation of cosine similarity, then a tree to find MIPS candidates could be constructed by running spherical k-means on the inputs transformed by the same transformation used for hashing-based MIPS. \n\nIn order to make the search more robust to border issues when a query is close to the frontier between clusters, at each level of the tree they consider more than one candidate cluster during top-down search, so as to merge the candidates in several leaves of the tree at the very end of a full top down query.\n\nTheir experiments using search with word embeddings show that the quality of the top 1, 10 and 100 MIPS candidates using their spherical k-means approach is better than using two hashing-based search methods.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1507.05910"
    },
    "32": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MasseO15",
        "transcript": "This paper presents a method for \"learning the learning rate\" of a stochastic gradient descent method, in the context of online learning. Indeed, variations on the chosen learning rate or learning rate schedule can have a large impact in observed performance of stochastic gradient descent. Moreover, in the context of online learning, where we are interested in achieving high performance not only at convergence but every step of the way, the \"choosing the learning rate\" problem is even more crucial.\n\nThe authors present a method which attempts to train the learning rate itself by gradient descent. This is achieved by \"unrolling\" the parameter updates of our model across the time steps of online learning, which exposes the interaction between the learning rate and the sum of losses of the model across these time steps. The authors then propose a way to approximate the gradient of the sum of losses with respect to the learning rate, so that it can be used to perform gradient updates on the learning rate itself.\n\nThe gradient on the learning rate has to be approximated, for essentially the same reason that gradients to train a recurrent neural network online must be approximated (see also my notes on another good paper by Yann Ollivier here: \\cite{journals/corr/OllivierC15}). Another approximation is introduced to avoid having to compute an Hessian matrix. Nevertheless, results suggest that the proposed approximation works well and can improve over a fixed learning with a reasonable rate decay schedule\n\n#### My two cents\n\nI think the authors are right on the money as to the challenges posed by online learning. I think these challenges are likely to be greater in the context of training neural networks online, for which little satisfactory solutions exist right now. So this is a direction of research I'm particularly excited about.\n\nAt this points, the experiments consider fairly simple learning scenarios, but I don't see any obstacle in applying the same method to neural networks. One interesting observation from the results is that results are fairly robust to variations of \"the learning rate of the learning rate\", compared to varying and fixing the learning rate itself.\n\nFinally, I haven't had time to entirely digest one of their theoretical result, suggesting that their approximation actually corresponds to an exact gradient taken \"alongside the effective trajectory\" of gradient descent. However, that result seems quite interesting and would deserve more attention.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.02540"
    },
    "33": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/MaclaurinDA15",
        "transcript": "This is another \"learning the learning rate\" paper, which predates (and might have inspired) the \"Speed learning on the fly\" paper I recently wrote notes about (see \\cite{journals/corr/MasseO15}). In this paper, they consider the off-line training scenario, and propose to do gradient descent on the learning rate by unrolling the *complete* training procedure and treating it all as a function to optimize, with respect to the learning rate. This way, they can optimize directly the validation set loss.\n\nThe paper in fact goes much further and can tune many other hyper-parameters of the gradient descent procedure: momentum, weight initialization distribution parameters, regularization and input preprocessing.\n\n#### My two cents\n\nThis is one of my favorite papers of this year. While the method of unrolling several steps of gradient descent (100 iterations in the paper) makes it somewhat impractical for large networks (which is probably why they considered 3-layer networks with only 50 hidden units per layer), it provides an incredibly interesting window on what are good hyper-parameter choices for neural networks. Note that, to substantially reduce the memory requirements of the method, the authors had to be quite creative and smart about how to encode changes in the network's weight changes.\n\nThere are tons of interesting experiments, which I encourage the reader to go check out (see section 3).\n\nOne experiment on training the learning rates, separately for each iteration (i.e. learning a learning rate schedule), for each layer and for either weights or biases (800 hyper-parameters total) shows that a good schedule is one where the top layer first learns quickly (large learning), then the bottom layer starts training faster, and finally the learning rates of all layers is decayed towards zero. Note that some of the experiments presented actually optimized the training error, instead of the validation set error.\n\nAnother looked at finding optimal scales for the weight initialization. Interestingly, the values found weren't that far from an often prescribed scale of $1 / \\sqrt{N}$, where $N$ is the number of units in the previous layer.\n\nThe experiment on \"training the training set\", i.e. generating the 10 examples (one per class) that would minimize the validation set loss of a network trained on these examples is a pretty cool idea (it essentially learns prototypical images of the digits from 0 to 9 on MNIST).\n\nAnother experiment tried to optimize a multitask regularization matrix, in order to encourage forms of soft-weight-tying across tasks.\n\nNote that approaches like the one in this paper make tools for automatic differentiation incredibly valuable. Python autograd, the author's automatic differentiation Python library https://github.com/HIPS/autograd (which inspired our own Torch autograd https://github.com/twitter/torch-autograd) was in fact developed in the context of this paper.\n\nFinally, I'll end with a quote from the paper, that I found particularly funny: \"The last remaining parameter to SGD is the initial parameter vector. Treating this vector as a hyperparameter blurs the distinction between learning and meta-learning. In the extreme case where all elementary learning rates are set to zero, the training set ceases to matter and the meta-learning procedure exactly reduces to elementary learning on the validation set. Due to philosophical vertigo, we chose not to optimize the initial parameter vector.\"",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/maclaurin15.html"
    },
    "34": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/NalisnickR15",
        "transcript": "This paper introduces a version of the skipgram word embeddings learning algorithm that can also learn the size (nb. of dimensions) of these embeddings. The method, coined infinite skipgram (iSG), is inspired from my work with Marc-Alexandre C\u00f4t\u00e9 on the infinite RBM, in which we describe a mathematical trick for learning the size of a latent representation. This is done by introducing an additional latent variable $z$ representing the number of dimensions effectively involved in the energy function. Moreover, a term penalizing increasing values for $z$ is also incorporated, such that the infinite sum over $z$ is converging.\n\nIn this paper, the authors extend the probabilistic model behind skipgram with such a variable $z$, now corresponding to the number of dimensions involved in the dot product between word embeddings. They also propose a few approximations required to allow for an efficient training algorithm. Mainly they optimize an upper bound on the regular skipgram objective (see Section 3.2) and they approximate the computation of the conditional over $z$ for a given word $w$, which requires summing over all possible context words $c$, by summing only over the words observed in the immediate current context of $w$ (thus this sum will very across training example of the same word $w$).\n\nExperiments show that the iSG better learns to exploit different dimensions to model different senses of words, better than the original skipgram model. Quantitatively, the iSG seems to provide better probabilities to context words.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05392"
    },
    "35": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LiTBZ15",
        "transcript": "This paper presents a feed-forward neural network architecture for processing graphs as inputs, inspired from previous work on Graph Neural Networks.\n\nIn brief, the architecture of the GG-NN corresponds to $T$ steps of GRU-like (gated recurrent units) updates, where T is a hyper-parameter. At each step, a vector representation is computed for all nodes in the graph, where a node's representation at step t is computed from the representation of nodes at step $t-1$. Specifically, the representation of a node will be updated based on the representation of its neighbors in the graph. Incoming and outgoing edges in the graph are treated differently by the neural network, by using different parameter matrices for each. Moreover, if edges have labels, separate parameters can be learned for the different types of edges (meaning that edge labels determine the configuration of parameter sharing in the model). Finally, GG-NNs can incorporate node-level attributes, by using them in the initialization (time step 0) of the nodes' representations.\n\nGG-NNs can be used to perform a variety of tasks on graphs. The per-node representations can be used to make per-node predictions by feeding them to a neural network (shared across nodes). A graph-level predictor can also be obtained using a soft attention architecture, where per-node outputs are used as scores into a softmax in order to pool the representations across the graph, and feed this graph-level representation to a neural network. The attention mechanism can be conditioned on a \"question\" (e.g. on a task to predict the shortest path in a graph, the question would be the identity of the beginning and end nodes of the path to find), which is fed to the node scorer of the soft attention mechanism. Moreover, the authors describe how to chain GG-NNs to go beyond predicting individual labels and predict sequences.\n\nExperiments on several datasets are presented. These include tasks where a single output is required (on a few bAbI tasks) as well as tasks where a sequential output is required, such as outputting the shortest path or the Eulerian circuit of a graph. Moreover, experiments on a much more complex and interesting program verification task are presented.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05493"
    },
    "36": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ChenGS15",
        "transcript": "This paper presents an approach to initialize a neural network from the parameters of a smaller and previously trained neural network. This is effectively done by increasing the size (in width and/or depth) of the previously trained neural network, in such of a way that the function represented by the network doesn't change (i.e. the output of the larger neural network is still the same). The motivation here is that initializing larger neural networks in this way allows to accelerate their training, since at initialization the neural network will already be quite good.\n\nIn a nutshell, neural networks are made wider by adding several copies (selected randomly) of the same hidden units to the hidden layer, for each hidden layer. To ensure that the neural network output remains the same, each incoming connection weight must also be divided by the number of replicas that unit is connected to in the previous layer. If not training using dropout, it is also recommended to add some noise to this initialization, in order to break its initial symmetry (though this will actually break the property that the network's output is the same). As for making a deeper network, layers are added by initializing them to be the identity function. For ReLU units, this is achieved using an identity matrix as the connection weight matrix. For units based on sigmoid or tanh activations, unfortunately it isn't possible to add such identity layers.\n\nIn their experiments on ImageNet, the authors show that this initialization allows them to train larger networks faster than if trained from random initialization. More importantly, they were able to outperform their previous validation set ImageNet accuracy by initializing a very large network from their best Inception network.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05641"
    },
    "37": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/VendrovKFU15",
        "transcript": "This paper proposes to learn embeddings of text and/or images according to a dissimilarity metric that is asymmetric and implements the notion of partial order. For example, we'd like the metric to capture that the sentence \"a dog in the yard\" is more specific than just \"a dog\". Similarly, given the image of a scene and a caption describing it, we'd also like to capture that the image is more specific than the caption, since captions only describe the main elements of the scene. We'd also like to capture the hypernym relation between single words, e.g. where \"woman\" is more specific than \"person\".\n\nTo achieve this, they propose to use the following dissimilarity metric:\n\n$$E(x,y) = ||max(0,y-x)||^2$$\n\nwhere x and y are embedding vectors and the max operation is applied element-wise. The way to use this metric is to learn embeddings such that, for a pair x,y where the object (e.g. \"a dog in the yard\") represented by $x$ is more specific than the object (e.g. \"a dog\") represented by $y$, then $E(x,y)$ is as small as possible.\n\nFor example, let's assume that $x$ and y are the output of a neural network, where each output dimension detects a certain concept, i.e. is non-zero only if the concept associated with that dimension is present in the input. For x representing \"a dog in the yard\", we could expect having only two dimensions that are non-zero: one detecting the concept \"dog\" (let's note it $x_j$) and another detecting the concept \"yard\" ($x_k$). For y representing \"a dog\", only the dimension associated with \"dog\" ($y_j$) would be non-zero and have the same value as $x_j$. In this situation, it is easy to see that $E(x,y)$ would be 0, but $E(y,x)$ would be greater than zero, thus capturing appropriately the asymmetric relationship between the two.\n\nThe authors show in the paper how to leverage this new asymmetric metric in training losses that are appropriate for 3 problems: hypernym detection, caption-image retrieval and textual entailment. They show that the proposed metric yields superior performance on these problems compared to symmetric metrics that have been used by prior work.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.06361"
    },
    "38": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/RanzatoCAZ15",
        "transcript": "This paper is concerned with the problem of predicting a sequence at the output, e.g. using an RNN. It aims at addressing the issue it refers to as exposure bias, which here refers to the fact that while at training time the RNN producing the output sequence is being fed the ground truth previous tokens (words) when producing the next token (something sometimes referred to as teacher forcing, which really is just maximum likelihood), at test time this RNN makes predictions using recursive generation, i.e. it is instead recursively fed by its own predictions (which might be erroneous). \n\nMoreover, it also proposes a training procedure that can take into account a rich performance measure that can't easily be optimized directly, such as the BLEU score for text outputs. \n\nThe key observation is that the REINFORCE algorithm could be used to optimize the expectation of such arbitrarily complicated performance measures, for outputs produced by (stochastic) recursive generation. However, REINFORCE is a notoriously unstable training algorithm, which can often work terribly (in fact, the authors mention that they have tried using REINFORCE only, without success). Thus, they instead propose to gradually go from training according to maximum likelihood / teacher forcing to training using the REINFORCE algorithm on the expected performance measure.\n\nThe proposed procedure, dubbed MIXER (Mixed Incremental Cross-Entropy Reinforce), goes as follows:\n1. Train model to optimize the likelihood of the target sequence, i.e. minimize the per time-step cross-entropy loss.\n2. Then, for a target sequence of size T, optimize the cross-entropy for the T-\u0394 first time steps of the sequence and use Reinforce to get a gradient on the expected loss (e.g. negative BLEU) for the recursive generation of the rest of the \u0394 time steps.\n3. Increase \u0394 and go back to 2., until \u0394 is equal to T.\n\nExperiments on 3 text benchmarks (summarization, machine translation and image captioning) show that this approach yields models that produces much better outputs when not using beam search (i.e. using greedy recursive generation) to generate an output sequence, compared to other alternatives such as regular maximum likelihood and Data as Demonstrator (DaD). DaD is similar to the scheduled sampling method of Bengio et al. (see my note: \\cite{conf/nips/BengioVJS15}), in that at training time, some of the previous tokens fed to the model are predicted tokens instead of ground truths. When using beam search, MIXER is only outperformed by DaD on the machine translation task.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.06732"
    },
    "39": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GuLSM15",
        "transcript": "This paper presents a method for training feed-forward neural networks with stochastic hidden units (e.g. sigmoid belief networks), to optimize the expectation (over the stochastic units) of some arbitrary loss function. While the proposed method is applicable to any type of stochastic units, it is most interesting for the case of discrete stochastic units, since the reparametrization trick of variational autoencoders cannot be applied to backprop through the sampling step. \n\nIn short, the method builds on the likelihood ratio method (of which REINFORCE is a special case) and proposes a baseline (also known as control variate) which, according to the authors, is such that an unbiased gradient is obtained. Specifically, the baseline corresponds to the first-order Taylor expansion of the loss function around some deterministic value of the hidden units (x\u0304) that doesn't depend on the stochastic hidden units (noted x in the paper).\n\nFor a likelihood ratio method to be unbiased, it is required that the expectation of the baseline (times the gradient of the model's log distribution) with respect to the model's distribution be tractable. For the proposed baseline, it can be shown that computing this expectation requires the gradient of the mean (\u03bc) of each stochastic unit in the network with respect to each parameter. The key idea behind the proposed method is that 1) an estimate of this expectation can be obtained simply using mean-field and 2)  since mean-field is estimated by a feedforward deterministic pass over the network, it is thus possible to compute the gradients of \u03bc by backpropagation through the mean-field pass (hence the name of the method, MuProp).\n\nExperiments show that this method converges much faster than previously proposed unbiased methods and often performs better. Experiments also show that the method obtains competitive performance compared to biased methods (such as the \"straight through\" method).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05176"
    },
    "40": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/TheisOB15",
        "transcript": "This paper presents a variety of issues related to the evaluation of image generative models. Specifically, they provide evidence that evaluations of generative models based on the popular Parzen windows estimator or based on a visual fidelity (qualitative) measure both present serious flaws.\n\nThe Parzen windows approach to generative modeling evaluation works by taking a finite set of samples generated from a given model and then using those as the centroids of a Parzen windows Gaussian mixture. The constructed Parzen windows mixture is then used to compute a log-likelihood score on a set of test examples.\n\nSome of the key observations made in this paper are:\n1. A simple, k-means based approach can obtain better Parzen windows performance than using the original training samples for a given dataset, even though these are samples from the true distribution!\n2. Even for the fairly low dimensional space of 6x6 image patches, a Parzen windows estimator would require an extremely large number of samples to come close to the true log-likelihood performance of a model.\n3. Visual fidelity is a bad predictor of true log-likelihood performance, as it is possible to\nObtain great visual fidelity and arbitrarily low log-likelihood, with a Parzen windows model made of Gaussians with very small variance.\nObtain bad visual fidelity and high log-likelihood by taking a model with high log-likelihood and mixing it with a white noise model and putting as much as 99% of the mixing probability on the white noise model (i.e. which would produce bad samples 99% of the time).\n4. Measuring overfitting of a model by taking samples from the model and making sure their training set nearest neighbors are different is ineffective, since it is actually trivial to generate samples that are each visually almost identical to a training example, but that yet each have large euclidean distance with their corresponding (visually similar) training example. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.01844"
    },
    "41": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/OordKK16",
        "transcript": "This paper explores the use of convolutional (PixelCNN) and recurrent units (PixelRNN) for modeling the distribution of images, in the framework of autoregression distribution estimation. In this framework, the input distribution $p(x)$ is factorized into a product of conditionals $\\Pi p(x_i | x_i-1)$. Previous work has shown that very good models can be obtained by using a neural network parametrization of the conditionals (e.g. see our work on NADE \\cite{journals/jmlr/LarochelleM11}). Moreover, unlike other approaches based on latent stochastic units that are directed or undirected, the autoregressive approach is able to compute log-probabilities tractably. \n\nSo in this paper, by considering the specific case of x being an image, they exploit the topology of pixels and investigate appropriate architectures for this.\n\nAmong the paper's contributions are:\n\n1. They propose Diagonal BiLSTM units for the PixelRNN, which are efficient (thanks to the use of convolutions) while making it possible to, in effect, condition a pixel's distribution on all the pixels above it (see Figure 2 for an illustration).\n\n2. They demonstrate that the use of residual connections (a form of skip connections, from hidden layer i-1 to layer $i+1$) are very effective at learning very deep distribution estimators (they go as deep as 12 layers).\n\n3. They show that it is possible to successfully model the distribution over the pixel intensities (effectively an integer between 0 and 255) using a softmax of 256 units.\n\n4. They propose a multi-scale extension of their model, that they apply to larger 64x64 images.\n\nThe experiments show that the PixelRNN model based on Diagonal BiLSTM units achieves state-of-the-art performance on the binarized MNIST benchmark, in terms of log-likelihood. They also report excellent log-likelihood on the CIFAR-10 dataset, comparing to previous work based on real-valued density models. Finally, they show that their model is able to generate high quality image samples.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1601.06759"
    },
    "42": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MnihR16",
        "transcript": "This paper explores the use of so-called Monte Carlo objectives for training directed generative models with latent variables. Monte Carlo objectives take the form of the logarithm of a Monte Carlo estimate (i.e. an average over samples) of the marginal probability $P(x)$. One important motivation for using Monte Carlo objectives is that they can be shown (see the Importance Weighted Variational Autoencoder paper \\cite{journals/corr/BurdaGS15} and my notes on it) to correspond to bounds on the true likelihood of the model, and one can tighten the bound simply by drawing more samples in the Monte Carlo objective. Currently, the most successful application of Monte Carlo objectives is based on an importance sampling estimate, which involves training a proposal distribution $Q(h|x)$ in addition to the model $P(x,h)$.\n\nThis paper considers the problem of training with gradient descent on such objectives, in the context of a model to which the reparametrization trick cannot be used (e.g. for discrete latent variables). They analyze the sources of variance in the estimation of the gradients (see Equation 5) and propose a very simple approach to reducing the variance of a sampling-based estimator of these gradients. \n\nFirst, they argue that gradients with respect to the $P(x,h)$ parameters are less susceptible to problems due to high variance gradients.\n\nSecond, and most importantly, they derive a multi-sample estimate of the gradient that is meant to reduce the variance of gradients on the proposal distribution parameters $Q(h|x)$. The end result is the gradient estimate of Equations 10-11. It is based on the observation that the first term of the gradient of Equation 5 doesn't distinguish between the contribution of each sampled latent hi. The key contribution is this: they notice that one can incorporate a variance reducing baseline for each sample hi, corresponding to the Monte Carlo estimate of the log-likelihood when removing hi from the estimate (see Equation 10). The authors show that this is a proper baseline, in that using it doesn't introduce a bias in the estimation for the gradients.\n\nExperiments show that this approach yields better performance than training based on Reweighted Wake Sleep \\cite{journals/corr/BornscheinB14} or the use of NVIL baselines \\cite{conf/icml/MnihG14}, when training sigmoid belief networks as generative models or as structured output prediction (image completion) models on binarized MNIST.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.06725"
    },
    "43": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/FengZKXM16",
        "transcript": "This paper presents the theoretical notion of ensemble robustness and how it might provide an explanation for the success of deep learning algorithms. This work is an extension of some of the author's previous work (see Definition 2), demonstrating a theoretical relationship between a notion of robustness to adversarial examples and generalization performance. One initial observation made in this work is that this previous notion of robustness cannot explain the good performance of deep neural networks, since they have been shown to in fact not be robust to adversarial examples.\n\nSo in this paper, the authors propose to study a notion of ensemble robustness (see Definition 3), and show that it can also be linked to generalization performance (see Theorem 1 and Corollary 1). The \"ensemble\" part comes from taking into account the stochasticity of the learning algorithm, i.e. the fact that the models they produce can vary from one run to another, even if applied on the same training set. The stochasticity here can come from the use of dropout, of SGD with random ordering of the training examples or from the random parameter initialization. Other theoretical results are also presented, such as one relating the variance of the robustness to generalization performance and another specific to the use of dropout.\n\nFinally, the paper also proposes a semi-supervised learning algorithm inspired from their definition of ensemble robustness, in which a model is trained to classify the perturbed (adversarial) version of an example in the same class as the original (non perturbed) example. On MNIST, they achieve excellent results, matching the performance of the state-of-the-art Ladder Networks.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.02389"
    },
    "44": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LingGHKSWB16",
        "transcript": "This paper presents a conditional generative model of text, where text can be generated either one character at a time or by copying some full chunks of character taken directly from the input into the output. At each step of the generation, the model can decide which of these two modes of generation to use, mixing them as needed to generate a correct output. They refer to this structure for generation as Latent Predictor Networks \\cite{conf/nips/VinyalsFJ15}. The character-level generation part of the model is based on a simple output softmax over characters, while the generation-by-copy component is based on a Pointer Network architecture. Critically, the authors highlight that it is possible to marginalize over the use of either types of components by dynamic programming as used in semi-Markov models \\cite{conf/nips/SarawagiC04}.\n\nOne motivating application is machine translation, where the input might contain some named entities that should just be directly copied at the output. However, the authors experiment on a different problem, that of generating code that would implement the action of a card in the trading card games Magic the Gathering and Hearthstone. In this application, copying is useful to do things such as copy the name of the card or its numerically-valued effects. \n\nIn addition to the Latent Predictor Network structure, the proposed model for this application includes a slightly adapted form of soft-attention as well as character-aware word embeddings as in \\cite{conf/emnlp/LingDBTFAML15} Also, the authors experiment with a compression procedure on the target programs, that can help in reducing the size of the output space.\n\nExperiments show that the proposed neural network approach outperforms a variety of strong baselines (including systems based on machine translation or information retrieval). ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.06744"
    },
    "45": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Graves16",
        "transcript": "This paper proposes a neural architecture that allows to backpropagate gradients though a procedure that can go through a variable and adaptive number of iterations. These \"iterations\" for instance could be the number of times computations are passed through the same recurrent layer (connected to the same input) before producing an output, which is the case considered in this paper. \n\nThis is essentially achieved by pooling the recurrent states and respective outputs computed by each iteration. The pooling mechanism is essentially the same as that used in the really cool Neural Stack architecture of Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman and Phil Blunsom \\cite{conf/nips/GrefenstetteHSB15}. It relies on the introduction of halting units, which are sigmoidal units computed at each iteration and which gives a soft weight on whether the computation should stop at the current iteration.\n\nCrucially, the paper introduces a new ponder cost $P(x)$, which is a regularization cost that penalizes what is meant to be a smooth upper bound on the number of iterations $N(t)$ (more on that below).\n\nThe paper presents experiment on RNNs applied on sequences where, at each time step t (not to be confused with what I'm calling computation iterations, which are indexed by n) in the sequence the RNN can produce a variable number $N(t)$ of intermediate states and outputs. These are the states and outputs that are pooled, to produce a single recurrent state and output for the time step t. During each of the $N(t)$ iterations at time step t, the intermediate states are connected to the same time-step-t input. After the $N(t)$ iterations, the RNN pools the $N(t)$ intermediate states and outputs, and then moves to the next time step $t+1$. To mark the transitions between time steps, an extra binary input is appended, which is 1 only for the first intermediate computation iteration. \n\nResults are presented on a variety of synthetic problems and a character prediction problem.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.08983"
    },
    "46": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/KendallBC15",
        "transcript": "**Contributions**: \n* Use dropout to get segmentation with a measure of model uncertainty.\n\n**Explanation**:\n\nWe can consider dropout as a way of getting samples from a posterior distribution of models [see these papers: [1]( https://arxiv.org/abs/1506.02142), [2](https://arxiv.org/abs/1506.02158)) and thus can be be used to do Bayesian inference. \n\nThis amounts to using dropout both during train and test time and getting multiple outputs (i.e sampling from model distribution) in test time. Mean of these outputs is taken as final segmentation and variation as model uncertainty.\n\nSample averaging performs better than weight averaging (i.e usual test time method for dropout) if averaged over more than 6 samples. Paper used 50 samples.\n\nGeneral technique which can be applied to any segmentation model. Sample averaging alone improves scores by 2-3%.\n\n**Benchmarks**:\n\n<score without dropout -> score with dropout>\n\n*VOC2012*\n\nDilation Network: 71.3 -> 73.1 [Source](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&challengeid=11&compid=6&submid=6103#KEY_Bayesian%20Dilation%20Network)\n\nFCN8:    62.2 -> 65.4 [Source](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&challengeid=11&compid=6&submid=6103#KEY_Bayesian%20FCN)\n\nSegNet: 59.1 -> 60.5 (Source: reported in the paper)\n\n*CamVid* \n\nSegNet: 71.20 -> 76.3 (Source: reported in the paper)\n\n**My comments**\n\nNothing very new. Gotcha is that sample averaging performs better.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.02680"
    },
    "47": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SamekBMBM15",
        "transcript": "Layer-wise Relevance Propagation (LRP)  is a novel technique has been used by authors in multiple use-cases (apart from this publication) to demonstrate the robustness and advantage of a *decomposition* method over other heatmap generation methods. Such heatmap generation methods are very crucial for increasing interpretability of Deep Learning models as such.  Apart from LRP relevance, authors also discuss quantitative ways to measure the accuracy of the heatmap generated.\n\n### LRP & Alternatives\n\nWhat is LRP ?\n\nLRP is a principled approach to decompose a classification decision into pixel-wise relevances indicating the contributions of a pixel to the overall classification score. The approach is derived\nfrom a layer-wise conservation principle , which forces the propagated quantity (e.g. evidence for a predicted class) to be preserved between neurons of two adjacent layers. \n\nDenoting by R(l) [i] the relevance associated to the ith neuron of layer and by R (l+1) [j] the relevance associated to the jth neuron in the next layer, the conservation principle requires that\n\n![](https://i.imgur.com/GQxrnCT.png)\n\nwhere R(l) [i] is given as \n![](https://i.imgur.com/FD7AAfF.png)\n\n\nwhere z[i,j]  is the activation of jth neuron because of input from ith neuron \n\nAs per authors this is not necssarily the only relevance funtion which is conserved. The intuition behind using such a function is  that lower-layer neurons that mostly contribute to the activation of the higher-layer neuron receive a larger share of the relevance Rj of the neuron j.\n\nA downside of this propagation rule (at least if *epsilon*\u000f = 0) is that the denominator may tend to zero if lower-level contributions to neuron j cancel each other out. The numerical instability can be overcome by setting *epsilon*\u000f > 0. However in that case, the conservation idea is relaxated in order to gain better numerical properties.  To conserve relevance, it can be formulated as sum of positive and negative activations\n![](https://i.imgur.com/lo7f8AI.png)\n such that *alpha* - *beta* = 1\n\n#### Alternatives to LRP for heatmap\n\n**Senstiivity measurement**\n\nIn such methods of generating heamaps, gradient of the output with respect to input is used for generating heatmap. This quantity measures how much small changes in the pixel value locally affect the network output. \n##### Disadvantages\nGiven most models use ReLU as activation function, the gradient flows only through activation with positive output - thereby making  makes the backward mapping discontinuous, and consequently strongly local. Also same applies for maxpool activations - wherein gradients only flow through neurons with maximum intensity in local neighbourhood.\n\nAlso, given most of these methods use absolute impact on prediction cause by changes in pixel intensities, the granularity of whether the pixel intensity was in favour or against evidence is lost.\n\n**Deconvolutional Networks**\n\n##### Disadvantages\n\nHere the backward discontinuity problem of sensitivity based methods are absent, hence global features can be captured. However, since the method only takes in activation from final layer (which learns the presence or absence of features mostly) , using this for generating heatmaps is likely to yield avergae maps, lacking image specific localisation effects\n\nLRP is able to counter the effects nicely because of the way it uses relevance\n\n#### Performance of heatmaps\n\nFew concerns that the authors raise are \n- A heatmap is not a segmentation mask  on the contrary missing evidence or the context may be very important for classification\n- Salient features represent average explanations of what distinguishes one image category from another. For individual images these explanations may be meaningless or even wrong. For instance, salient features for the class \u2018bicycle\u2019 may be the wheels and the handlebar. However, in some images a bicycle\nmay be partly occluded so that these parts of a bike are not visible. In these images salient features fail to explain the classifier\u2019s decision (which still may be correct).\n\nAuthors propose a novel method (MoRF - *Most Relevant First* ) of objectively quantifying quality of a heatmap. A good detailed idea of the measure can best be obtained from the paper. To give an idea, the most reliable method should ideally rank the most relevant regions in the same order even if small perturbations in pixel intensities are observed (in non-relevant areas.\n\nThe quantity of interest in this case is the area over the MoRF perturbation curve (AOPC).\n\n#### Observation\n\nMost of the sensitivity based methods answer to the question - *what change would make the image more or less belong to the category car* which isn't really the classifier's question.  LRP plans to answer the real classifier question *what speaks for the presence of a car in the image*\n\nAn image below would be a good example of how LRPs can denoise heatmaps generated on the basis of sensitivity.\n\n![](https://i.imgur.com/Sq0b5yg.png)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1509.06321"
    },
    "48": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeGDG17",
        "transcript": "Mask RCNN takes off from where Faster RCNN left, with some augmentations aimed at bettering instance segmentation (which was out of scope for FRCNN). Instance segmentation was achieved remarkably well in *DeepMask* , *SharpMask* and later *Feature Pyramid Networks* (FPN).\n\nFaster RCNN was not designed for pixel-to-pixel alignment between network inputs and outputs. This is most evident in how RoIPool , the de facto core operation for attending to instances, performs coarse spatial quantization for feature extraction. Mask RCNN fixes that by introducing RoIAlign in place of RoIPool.\n\n#### Methodology\n\nMask RCNN retains most of the architecture of Faster RCNN. It adds the a third branch for segmentation. The third branch takes the output from RoIAlign layer and predicts binary class masks for each class.\n\n##### Major Changes and intutions\n\n**Mask prediction**\n\nMask prediction segmentation predicts a binary mask for each RoI using fully convolution - and the stark difference being usage of *sigmoid* activation for predicting final mask instead of *softmax*, implies masks don't compete with each other. This *decouples* segmentation from classification. The class prediction branch is used for class prediction and for calculating loss, the mask of predicted loss is used calculating Lmask.\n\nAlso, they show that a single class agnostic mask prediction works almost as effective as separate mask for each class, thereby supporting their method of decoupling classification from segmentation\n\n**RoIAlign**\n\nRoIPool first quantizes a floating-number RoI to the discrete granularity of the feature map, this quantized RoI is then subdivided into spatial bins which are themselves quantized, and finally feature values covered by each bin are aggregated (usually by max pooling). Instead of  quantization of the RoI boundaries\nor bin bilinear interpolation is used to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and aggregate the result (using max or average).\n\n**Backbone architecture**\n\nFaster RCNN uses a VGG like structure for extracting features from image, weights of which were shared among RPN and region detection layers. Herein, authors experiment with 2 backbone architectures - ResNet based VGG like in FRCNN and ResNet based [FPN](http://www.shortscience.org/paper?bibtexKey=journals/corr/LinDGHHB16) based. FPN uses convolution feature maps from previous layers and recombining them to produce pyramid of feature maps to be used for prediction instead of single-scale feature layer (final output of conv layer before connecting to fc layers was used in Faster RCNN) \n\n**Training Objective**\n\nThe training objective looks like this \n![](https://i.imgur.com/snUq73Q.png)\n\nLmask is the addition from Faster RCNN. The method to calculate was mentioned above\n\n#### Observation\n\nMask RCNN performs significantly better than COCO instance segmentation winners *without any bells and whiskers*. Detailed results are available in the paper\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.06870"
    },
    "49": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LinDGHHB16",
        "transcript": "Feature Pyramid Networks (FPNs) build on top of the state-of-the-art implementation for object detection net - Faster RCNN. Faster RCNN faces a major problem in training for scale-invariance as the computations  can be memory-intensive and extremely slow. So FRCNN only applies multi-scale approach while testing. \n\nOn the other hand, feature pyramids were mainstream when hand-generated features were used -primarily to counter scale-invariance. Feature pyramids are collections of features computed at multi-scale versions of the same image. Improving on a similar idea presented in *DeepMask*, FPN brings back feature pyramids using different feature maps of conv layers with differing spatial resolutions with predictiosn happening on all levels of pyramid. Using feature maps directly as it is, would be tough as initial layers tend to contain lower level representations and poor semantics but good localisation whereas deeper layers tend to constitute higher level representations with rich semantics but suffer poor localisation due to multiple subsampling.\n\n##### Methodology\n\nFPN can be used with any normal conv architecture, that's used for classification. In such an architecture all layers have progressively decreasing spatial resolutions (say C1, C2,..C5). FPN would now take C5 and convolve with 1x1 kernel to reduce filters to give P5. Next, P5 is upsampled and merged it to C4 (C4 is convolved with 1x1 kernel to decrease filter size in order to match that of upsampled P5) by adding element wise to produce P4. Similarly P4 is upsampled and merged with C3(in a similar way) to give P3 and so on. The final set of feature maps, in this case {P2 .. P5} are used as feature pyramids. \n\nThis is how pyramids would look like \n![](https://i.imgur.com/oHFmpww.png)\n\n*Usage of combination of {P2,..P5} as compared to only P2* : P2 produces highest resolution, most semantic features and could as well be the default choice but because of shared weights across rest of feature layers and the learned scale invariance makes the pyramidal variant more robust to generating false ROIs\n\nFor next steps, it could be RPN or RCNN, the regression and classifier would share weights across for all *anchors* (of varying aspect ratios) at each level of the feature pyramids. This step is similar to [Single Shot Detector (SSD) Networks ](http://www.shortscience.org/paper?bibtexKey=conf/eccv/LiuAESRFB16)\n\n##### Observation\nThe FPN was used in FRCNN in both parts of RPN and RCNN separately and then combined FPN in both parts and produced state-of-the-art result in MS COCO challenges bettering results of COCO '15 & '16 winner models ( Faster RCNN +++ & GMRI) for mAP. FPN also can be used for instance segmentation by using fully convolutional layers on top of the image pyramids. FPN outperforms results from *DeepMask*, *SharpMask*, *InstanceFCN*\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.03144"
    },
    "50": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eccv/LiuAESRFB16",
        "transcript": "SSD aims to solve the major problem with most of the current state of the art object detectors namely Faster RCNN and like. All the object detection algortihms have same  methodology \n\n- Train 2 different nets - Region Proposal Net (RPN) and advanced classifier to detect class of an object and bounding box separately.\n- During inference, run the test image at different scales to detect object at multiple scales to account for invariance\n\nThis makes the nets extremely slow. Faster RCNN could operate at **7 FPS with 73.2% mAP** while SSD could achieve **59 FPS with 74.3% mAP ** on VOC 2007 dataset.\n\n#### Methodology\n\nSSD uses a single net for predict object class and bounding box. However it doesn't do that directly. It uses a mechanism for choosing ROIs, training end-to-end for predicting class and boundary shift for that ROI.\n\n##### ROI selection\n\nBorrowing from FasterRCNNs SSD uses the  concept of anchor boxes for generating ROIs from the feature maps of last layer of shared conv layer. For each pixel in layer of feature maps, k default boxes with different aspect ratios are chosen around every pixel in the map. So if there are feature maps each of m x n resolutions - that's *mnk* ROIs for a single feature layer. Now SSD uses multiple feature layers (with differing resolutions) for generating such ROIs primarily to capture size invariance of objects. But because earlier layers in deep conv net tends to capture low level features, it uses features after certain levels and layers henceforth.\n\n##### ROI labelling\nAny ROI that matches to Ground Truth for a class after applying appropriate transforms and having Jaccard overlap greater than 0.5 is positive. Now, given all feature maps are at different resolutions and each boxes are at different aspect ratios, doing that's not simple. SDD uses simple scaling and aspect ratios to get to the appropriate  ground truth dimensions for calculating Jaccard overlap for default boxes for each pixel at the given resolution\n\n##### ROI classification\n\nSSD uses single convolution kernel of 3*3 receptive fields to predict for each ROI the 4 offsets (centre-x offset, centre-y offset, height offset , width offset) from the Ground Truth box for each RoI, along with class confidence scores for each class. So that is if there are c classes (including background), there are (c+4) filters for each convolution kernels that looks at a ROI. \n\nSo summarily we have convolution kernels  that look at ROIs (which are default boxes around each pixel in feature map layer) to generate (c+4) scores for each RoI. Multiple feature map layers with different resolutions are used for generating such ROIs. Some ROIs are positive and some negative depending on jaccard overlap after ground box has scaled appropriately taking resolution differences in input image and feature map into consideration.\n\nHere's how it looks :\n![](https://i.imgur.com/HOhsPZh.png)\n\n##### Training\n\nFor each ROI a combined loss is calculated as a combination of localisation error and classification error. The details are best explained in the figure. \n![](https://i.imgur.com/zEDuSgi.png)\n\n##### Inference\nFor each ROI predictions a small threshold is used to first filter out irrelevant predictions, Non Maximum Suppression (nms) with jaccard overlap of 0.45 per class is applied then on the remaining candidate ROIs and the top 200 detections per image are kept.\n\nFor further understanding of the intuitions regarding the paper and the results obtained please consider giving the full paper a read. \n\nThe open sourced code is available at this [Github repo](https://github.com/weiliu89/caffe/tree/ssd)\n ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-46448-0_2"
    },
    "51": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ValindriaLBKARR17",
        "transcript": "#### Idea\nReverse Classification Accuracy (RCA) models are aims to answer the question on how to estimate performance of models (semantic segmentation models were explained in the paper) in cases where ground truth is not available. \n\n#### Why is it important\n\nBefore deployment, performance is quantified using different metrics, for which the predicted segmentation is compared to a reference segmentation, often obtained manually by an expert. But little is known about the real performance after deployment when a reference is unavailable. RCA aims to quantify the performance in those deployment scenarios\n\n#### Methodology\nThe RCA model pipeline follows a simple enough pipeline for the same:\n\n1. Train a model M on training dataset T containing input images  and ground truth  {**I**,**G**}\n2. Use M to predict segmentation map for an input image II to get segmentation map SS\n3. Train a RCA model that uses input image II to predict SS. As it's a single datapoint for the model it would overfit. There's no validation set for the RCA model\n4. Test the performance of RCA model on Images which have ground truth G and the best performance of the model is an indicator of the performance (DSC - Dice Similarity Coefficient)  of how the original image would perform on a new image whose ground truth is not available to compute segmentation accuracy (DSC)\n\n#### Observation\nFor validation of the RCA method, the predicted DSC and the real DSC were compared and the correlation between the 2 was calculated. For all calculations 3  types of methods of segmentation were used and 3  slightly different types methods for RCA were used for comparison. The predicted DSC and real DSC were highly correlated for most of the cases.\n\nHere's a snap of the results that they obtained\n\n![](http://i.imgur.com/2ra0wQm.png)\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1702.03407"
    },
    "52": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.03275",
        "transcript": "[Batch Normalization Ioffe et. al 2015](Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift) is one of the remarkable ideas in the era of deep learning that sits with the likes of Dropout and Residual Connections. Nonetheless, last few years have shown a few shortcomings of the idea, which two years later Ioffe has tried to solve through the concept that he calls Batch Renormalization.\n\nIssues with Batch Normalization\n\n- Different parameters used to compute normalized output during training and inference\n- Using Batch Norm with small minibatches\n- Non-i.i.d minibatches can have a detrimental effect on models with batchnorm. For e.g. in a metric learning scenario, for a minibatch of size 32, we may randomly select 16 labels then choose 2 examples for each of these labels, the examples interact at every layer and may cause model to overfit to the specific distribution of minibatches and suffer when used on individual examples.\n\nThe problem with using moving averages in training, is that it causes gradient optimization and normalization in opposite direction and leads to model blowing up.\n\nIdea of Batch Renormalization\n\nWe know that,\n\n${\\frac{x_i - \\mu}{\\sigma} = \\frac{x_i - \\mu_B}{\\sigma_B}.r + d}$\n\nwhere, \n\n${r = \\frac{\\sigma_B}{\\sigma}, d = \\frac{\\mu_B - \\mu}{\\sigma}}$\n\nSo the batch renormalization algorithm is defined as follows\n\n![Batch Renorm Algo](https://fractalanalytic-my.sharepoint.com/personal/shubham_jain_fractalanalytics_com/_layouts/15/guestaccess.aspx?docid=0c2c627424786442f8de65367755e1fd1&authkey=ARSCi3QfpM_uBVuWCYARKNg)\n\nIoffe writes further that for practical purposes, \n\n> In practice, it is beneficial to train the model for a certain number of iterations with batchnorm alone, without the correction, then ramp up the amount of allowed correction. We do this by imposing bounds on r and d, which initially constrain them to 1 and 0, respectively, and then are gradually relaxed.\n\nIn experiments,\n\nFor Batch Renorm, author used $r_{max}$ = 1, $d_{max}$ = 0 (i.e. simply batchnorm) for the first 5000 training steps, after which these were gradually relaxed to reach $r_{max}$ = 3 at 40k steps, and $d_{max}$ = 5 at 25k steps. A training step means, an update to the model.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1702.03275"
    },
    "53": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.09326",
        "transcript": "In the paper, authors  Bengio et al. , use the DenseNet for semantic segmentation. DenseNets iteratively concatenates input feature maps to output feature maps. The biggest contribution was the use of a novel upsampling path - given conventional upsampling would've caused severe memory cruch.\n\n#### Background\n\nAll fully convolutional semantic segmentation nets generally follow a conventional path - a downsampling path which acts as feature extractor, an upsampling path that restores the locational information of every feature extracted in the downsampling path. \n\nAs opposed to Residual Nets (where input feature maps are added to the output) , in DenseNets,the output is concatenated to input which has some interesting implications:  \n- DenseNets  are efficient in the parameter usage, since all the feature maps are reused\n- DenseNets perform deep supervision thanks to short path to all feature maps in the architecture \n\nUsing DenseNets for segmentation though had an issue with upsampling in the conventional way of concatenating feature maps through skip connections as feature maps could easily go beyond 1-1.5 K. So Bengio et al. suggests a novel way - wherein only feature maps produced in the last Dense layer are only upsampled and not the entire feature maps. Post upsampling, the output is concatenated with feature maps of same resolution from downsampling path through skip connection. That way, the information lost during pooling in the downsampling path can be recovered.\n\n#### Methodology & Architecture\n\nIn the downsampling path, the input is concatenated with the output of a dense block, whereas for upsampling the output of dense block is upsampled (without concatenating it with the input) and then concatenated with the same resolution output of downsampling path. \n\nHere's the overall architecture \n![](https://i.imgur.com/tqsPj72.png)\n\nHere's how a Dense Block looks like \n![](https://i.imgur.com/MMqosoj.png)\n\n#### Results\nThe 103 Conv layer based DenseNet (FC-DenseNet103) performed better than shallower networks when compared on CamVid dataset. Though the FC-DenseNets were not pre-trained or used any post-processing like CRF or temporal smoothening etc.  When comparing to other nets FC-DenseNet architectures achieve state-of-the-art, improving upon models with 10 times more parameters. It is also worth mentioning that  small model FC-DenseNet56 already outperforms  popular  architectures  with  at  least 100 times more parameters. ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.09326"
    },
    "54": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ShiCHTABRW16",
        "transcript": "Sub-pixel CNN proposes a novel architecture for solving the ill-posed problem of super-resolution (SR). Super-resolution involves the recovery of a high resolution (HR) image or video from its low resolution (LR) counter part  and is topic of great interest in digital image processing. \n\n#### Background & problems with current approaches \nLR images can be understood as low-pass filtered versions of HR images. A key assumption that underlies\nmany SR techniques is that much of the high-frequency spatial data is redundant and thus can be accurately reconstructed from low frequency components. \n\nThere are 2 kinds of SR techniques - one which assumes multiple LR images as different instances of HR images and uses CV techniques like registration and fusion to construct HR images. Apart from constraints, of requiring multiple images, they are inaccurate as well. The other one  single image super-resolution (SISR) techniques learn implicit redundancy that is present in natural data to recover missing HR information from a single LR instance. \n\nAmong SISR techniques, the ones which deployed deep learning, most methods would tend to upsample LR images (using bicubic interpolation, with learnable filters) and learn the filters from the upsampled image. The problem with the methods are, normally creasing the resolution of the LR images before the image enhancement step increases the computational complexity. This is especially problematic for convolutional networks, where the processing speed directly depends on the input image resolution. Secondly, interpolation methods typically used to accomplish the task, such as bicubic interpolation\ndo not bring additional information to solve the ill-posed reconstruction problem.\n\nAlso, among other techniques like deconvolution, the pixels are filled with zero values between pixels, hence hese zero values have no gradient information that can be backpropagated through.\n\n#### Innovation\nIn sub-pixel CNN authors propose an architecture (ESPCNN) that learns all the filters in 2 convolutional layers with the resolutions in LR space. Only in the last layer, a convolutional layer is implemented which transforms into HR space, using sub-pixel convolutional layer. The layer, in order to tide over problems from deconvolution, uses something known as Periodic Shuffling (PS) operator for upsampling. The idea behind periodic shuffling is derived from the fact that activations between pixel shouldn't be zero for upsamling. The kernel in sub-pixel CNN does a rearrangement following the equation\n![PSoperator](https://i.imgur.com/OJtHL3w.png)\nThe image explains the architecture of Sub-pixel CNN. The colouring in the last layer explains the methodology for PS\n![ESPCNN_structure](https://i.imgur.com/py0vceQ.png)\n\n#### Results\nThe ESPCNN was trained and tested on ImageNet along with other databases . As is evident from the image, ESPCNN performs significantly better than Super-Resolution Convolutional Neural Network (SRCNN) &  TRNN, which are currently the best performing approach published, as of date of publication of ESPCNN.\n![ESPCNN_results](https://i.imgur.com/hOEjSeE.png)\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1609.05158"
    },
    "55": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/PaszkeCKC16",
        "transcript": "ENet, a fully convolutional net is aimed at making real-time inferences for segmentation.  ENet is up to 18\u00d7\nfaster, requires 75\u00d7 less FLOPs, has 79\u00d7 less parameters, and provides similar or better accuracy to existing models. SegNet has been used for benchamarking purposes.\n##### Innovation\nENet has been able to achieve faster speeds because  instead of following the regular encoder-decoder structure followed by most state-of-art semantic segmentation nets, it uses a ResNet kind of approach. So while most nets learn features from a highly down-sampled convolutional map, and then upsample the learnt features, ENet tries to learn the features using underlying principles of ResNet from a convolution map that is only downsampled thrice in the encoder part. The encoder part is designed to have same functionality as convolutional architectures used for classification tasks. Also, the author mentions that multiple downsampling tends to hurt accuracy apart from huritng memory constraints by way of upsampling layera. Hence , by way of including only 2 upsampling operations & a single deconvolution, ENet aims to achieve faster semantic segmentation.\n\n##### Architecture\nThe architecture of the net looks like this. \n![enet_architecture](https://i.imgur.com/rw1lVKQ.png)\n\nwhere the initial block and each of the bottlenecks look like this respectively\n![enet_bottlenecks](https://i.imgur.com/sveifk5.png)\n\nIn oder to reduce FLOPs further, no bias terms were included in any of the projection steps as cuDNN uses separate kernels for convolution and bias addition. \n\n##### Training\nThe encoder and decoder parts are trained sparately. The encoder part was trained to categorize downsampled regions of the input image, then the decoder was appended and trained the network to perform upsampling and pixel-wise classification\n\n##### Results\nThe results are reported on  on widely used NVIDIA Titan X GPU as well as on NVIDIA TX1 embedded system module. For segmentation results on CamVid, CityScape & SUN RGB-D datasets, the inference times and model sizes were significantly lower, while the IoU or accuracy (as applicable) was nearly equivalent to SegNet for most cases",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.02147"
    },
    "56": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1610.02357",
        "transcript": "Xception Net or Extreme Inception Net brings a new perception of looking at the Inception Nets. Inception Nets, as was first published (as GoogLeNet) consisted of Network-in-Network modules like this\n![Inception Modules](http://i.imgur.com/jwYhi8t.png)\n\nThe idea behind Inception modules was to look at cross-channel correlations ( via 1x1 convolutions) and spatial correlations (via 3x3 Convolutions). The main concept being  that cross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly. This idea is the genesis of Xception Net, using depth-wise separable convolution ( convolution which looks into spatial correlations across all channels independently and then uses pointwise convolutions to project to the requisite channel space leveraging inter-channel correlations). Chollet, does a wonderful job of explaining how regular convolution (looking at both channel & spatial correlations simultaneously) and depthwise separable convolution (looking at channel & spatial correlations independently in successive steps) are end points of spectrum with the original Inception Nets lying in between.\n\n![Extreme version of Inception Net](http://i.imgur.com/kylzfIQ.png)\n*Though for Xception Net, Chollet uses, depthwise separable layers which perform 3x3 convolutions for each channel and then 1x1 convolutions on the output from 3x3 convolutions (opposite order of operations depicted in image above)*\n\n##### Input\nInput for would be images that are used for classification  along with corresponding labels.\n\n##### Architecture\nArchitecture of Xception Net uses one for VGG-16 with convolution-maxpool blocks replaced by residual blocks of  depthwise separable convolution layers. The architecture looks like this \n\n![architecture of Xception Net](http://i.imgur.com/9hfdyNA.png)\n\n##### Results\nXception Net was trained using hyperparameters tuned for best performance of Inception V3 Net. And for both internal dataset and ImageNet dataset, Xception outperformed Inception V3. Points to be noted \n- Both Xception & Inception V3 have roughly similar no of parameters (~24 M), hence any improvement in performance can't be attributed to network size\n- Xception normally takes slightly lower training time compared to Inception V3, which can be configured to be lower in future",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1610.02357"
    },
    "57": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Hosseini-AslKE16",
        "transcript": "3D Image Classification is one of the utmost important requirements in healthcare. But due to huge size of such images and less amount of data, it has become difficult to train traditional end-to-end models of classification such as AlexNet, Resnet, VGG. Major approaches considered for 3D images use slices of the image and not the image as whole. This approach at times leads to loss of information across third axis. \n\nThe paper uses the complete 3D image for training, and since the number of images are less we use unsupervised techniques for weights initialization and fine tune the fully connected layers using supervised learning for classification. The major contribution of the paper is the extension of 2D Convolutional Auto-encoders to 3D Domain and taking it further for MRI Classification which can be useful in numerous other applications.\n\n**Algorithm**\n\n1. Train 3D Convolution Auto-encoders using [CADDementia dataset](https://grand-challenge.org/site/caddementia/home/) \n2. Extract the encoder layer from the trained auto-encoders and add fully connected layers to it, followed by softmax\n3. Train and test the classification model using [ADNI data](http://adni.loni.usc.edu/)\n\n**3D CAN Model**\n\nThis model is 3D extension of [2D Convolutional Auto-encoders](http://people.idsia.ch/~ciresan/data/icann2011.pdf) (2D CAN)\n\n![3D Convolutional Auto-encoders](http://i.imgur.com/66y52uQ.png)\n\nEach convolution in encoder is a 3D Convolution followed by ReLU and maxpool, while in decoder, each convolution is a maxunpool followed by 3D Full Convolution and ReLU. In the last decoder block, instead of ReLU, Sigmoid will be used.\n\nHere the weights were shared in the decoder and encoder as specified in [2D CAN](http://people.idsia.ch/~ciresan/data/icann2011.pdf)\n\n**3D Classification model**\n\n![Classification of MRIs using 3D Convolutional Auto-encoders](http://i.imgur.com/3hOeR8o.png)\n\nThe weights of the classification model are initialized using the trained 3D-CAN model as mentioned in algorithm.\n\n**Loss Function**\n\nWeighted negative log likelihood for Classification and Mean Squared Error for Unsupervised learning\n\n**Training algorithm**\n\n[Adadelta](https://arxiv.org/abs/1212.5701)\n\n**Results**\n\nObtained 89.1% on 10-fold cross validation on dataset of 270 patients for classification of MRI into Mild Cognitive Impairment (MCI), Normal Control (NC) and Alzheimer's disease (AD)\n\n**Image Source**\n\nAll images are taken from the [paper](https://arxiv.org/abs/1607.00455) itself.\n\n\n\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1607.00455"
    },
    "58": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/PinheiroCD15",
        "transcript": "Facebook has [released a series of papers](https://research.facebook.com/blog/learning-to-segment/) for object segmentation and detection. This paper is the first in that series.\n\nThis is how modern object detection works (think [RCNN](https://arxiv.org/abs/1311.2524), [Fast RCNN](http://arxiv.org/abs/1504.08083)):\n\n1.  A rich set of object proposals (i.e., a set of image regions which are likely to contain an object) is generated using a fast (but possibly imprecise) algorithm.\n2.  A CNN classifier is applied on each of the proposals.\n\nThe current paper improves the step 1, i.e., region/object proposals.\n\nMost object proposals approaches fall into three categories: \n* Objectness scoring\n* Seed Segmentation\n* Superpixel Merging\n\nCurrent method is different from these three. \nIt share similarities with [Faster R-CNN](https://arxiv.org/abs/1506.01497) in that proposals are generated using a CNN.\nThe method predicts a segmentation mask given an input *patch* and assigns a score corresponding to how likely the patch is to contain an object.\n\n\n## Model and Training\n\nBoth mask and score predictions are achieved with a single convolutional network but with multiple outputs. All the convolutional layers except the last few are from VGG-A pretrained model.\n\nEach training sample is a triplet of RGB input patch, the binary mask corresponding to the input patch, a label which specifies whether the patch contains an object. A patch is given label 1 only if it satisfies the following constraints:\n* the patch contains an object roughly centered in the input patch\n* the object is fully contained in the patch and in a given scale range\n\nNote that the network must output a mask for a single object at the center even when multiple objects are present.\n\nFigure 1 shows the architecture and sampling for training.\n![figure1](https://i.imgur.com/zSyP0ij.png)\n\nModel is then jointly trained for segmentation and objectness. Negative samples are not used for segmentation.\n\n## Inference\n\nDuring full image inference, model is applied densely at multiple locations and scales.\nThis can be done efficiently since all computations are convolutional like in a fully convolutional network (FCN). \n\n![figure2](https://i.imgur.com/dQWfy8R.png)\n\nThis approach surpasses the previous state of the art by a large margin in both box and segmentation proposal generation. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.06204"
    },
    "59": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/Girshick15",
        "transcript": "Fast RCNN is  a proposal detection net for object detection tasks.\n\n##### Input & Output\nThe input to a Fast RCNN would be the input image and the region proposals (generated using Selective Search). There are 2 outputs of the net, probability map of all possible objects & background ( e.g. 21 classes for Pascal VOC'12)  and corresponding bounding box parameters for each object classes.\n\n##### Architecture\nThe Fast RCNN version of any deep net would need 3 major modifications. For e.g. for VGG'16 \n1. A ROI pooling layer needs to be added after the final maxpool output before fully connected layers\n2. The final FC layer is replaced by 2 sibling branched layers - one for giving a softmax output for probability classes, other one is for predicting an encoding of 4 bounding box parameters  (x,y, width,height) w.r.t. region proposals\n3. Modifying the input 2 take 2 input. images and corresponding prposals\n\n**ROI Pooling layer** - The most notable contribution from the paper is designed to maxpool the features inside a proposed region into a fixed size (for VGG'16 version of FCNN it was 7 x 7) . The intuition behind the layer is make it faster as compared to SPPNets, (which used spatial pyramidal pooling) and RCNN.\n\n\n##### Results\nThe net is trained with dual loss (log loss on probability output +  squared error loss on bounding box parameters) . \nThe results were very impressive, on the VOC '07, '10 & '12 datasets with Fast RCNN outperforming the rest of the nets, in terms of mAp accuracy",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ICCV.2015.169"
    },
    "60": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1701.07875",
        "transcript": "This very new paper, is currently receiving quite a bit of attention by the [community](https://www.reddit.com/r/MachineLearning/comments/5qxoaz/r_170107875_wasserstein_gan/).\n\nThe paper describes a new training approach, which solves the two major practical problems with current GAN training:\n\n1) The training process comes with a meaningful loss. This can be used as a (soft) performance metric and will help debugging, tune parameters and so on.\n\n2) The training process does not suffer from all the instability problems. In particular the paper reduces mode collapse significantly.\n\nOn top of that, the paper comes with quite a bit mathematical theory, explaining why there approach works and other approachs have failed. This paper is a must read for anyone interested in GANs.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1701.07875"
    },
    "61": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1038/nature20101",
        "transcript": "The paper introduces an approach to model external memory in a differentiable way. Memory is modeled as $N \\times W$ matrix. The memory has $N$ independent storage location each being able to store a datum of length $W$. \n\n### DNC Architecture\n\nA controller network is trained to utilize the memory. Memory access is modeled in cycles. At each time-step $t$ the network emits read and write command as part of its output. The commands are than processed, read data is given to the network at time-step $t+1$ as part of its input. Any deep learning network architecture can be used as Controller network (e.g. standard feed-forward CNN). The paper utilizes a deep LSTM architecture is used as controller network. \n\nhttps://storage.googleapis.com/deepmind-live-cms/images/dnc_figure1.width-1500.png\n\n### Memory Interaction\n\nThe control commands allow the network to interact with the data in three different ways:\n1. Content Lookup: access is controlled by how closely a given location matches a predefined key. \n2. Sequential read access: For each read vector $v$ the network receives as input at time $t$ it can access the data which was written directly after $v$.\n3. Usage based write access: A \"usage\" vector $u \\in [0,1]^N$ models the importance of each location. The network can choose to write data based on the lowest usage level. $u$ can be decreased (\"erased memory\") at each time-step.\n\n\n#### Differentiable Operations \n\nAll memory operations are modeled in a differentiable way, so that the entire model can be trained end-to-end using gradient descend. To do this all read commands are mapped to a soft read vector $w^r \\in [0,1]^N$, such that $\\sum^N_{i=1} w^r_i = 1$. The read vector $r$ is then defined as weighted sum over the rows of the memory: $r = \\sum^N_{i=1} M[i, \\cdot] w^r_i = 1$. \n\n### Experiments\n\nThe network was trained to perform a variety of different, memory intensive tasks. The results show, that the network is able to learn to take advantage of the external memory.\n\nhttps://www.youtube.com/watch?v=B9U8sI7TcMY\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1038/nature20101"
    },
    "62": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.02960",
        "transcript": "**Problem Setting:**\n\nSequence to Sequence learning (seq2seq) is one of the most successful techniques in machine learning nowadays. The basic idea is to encode a sequence into a vector (or a sequence of vectors if using attention based encoder) and then use a recurrent decoder to decode the target sequence conditioned on the encoder output. While researchers have explored various architectural changes to this basic encoder-decoder model, the standard way of training such seq2seq models is to maximize the likelihood of each successive target word conditioned on the input sequence and the *gold* history of target words. This is also known as *teacher-forcing* in RNN literature. Such an approach has three major issues:\n\n1. **Exposure Bias:** Since we teacher-force the model with *gold* history during training, the model is never exposed to its errors during training. At test time, we will not have access to *gold* history and we feed the history generated by the model. If it is erroneous, the model does not have any clue about how to rectify it.\n\n2. **Loss-Evaluation Mismatch:** While we evaluate the model using sequence level metrics (such as BLEU for Machine Translation), we are training the model with word level cross entropy loss.\n\n3. **Label bias:** Since the word probabilities are normalized at each time step (by using softmax over the final layer of the decoder), this can result in label bias if we vary the number of possible candidates in each step. More about this later. \n\n**Solution:**\n\nThis paper proposes an alternative training procedure for seq2seq models which attempt to solve all the 3 major issues listed above. The idea is to pose seq2seq learning as beam-search optimization problem. Authors begin by removing the final softmax activation function from the decoder. Now instead of probability distributions, we will get score for next possible word. Then the training procedure is changed as follows: At every time step $t$, they maintain a set $S_t$ of $K$ candidate sequences of length $t$. Now the loss function is defined with the following characteristics:\n\n1. If the *gold* sub-sequence of length $t$ is in set $S_t$ and the score for *gold* sub-sequence exceeds the score of the $K$-th ranked candidate by a margin, the model incurs no loss. Now the candidates for next time-step are chosen in a way similar to regular beam-search with beam-size $K$.\n\n2. If the *gold* sub-sequence of length $t$ is in set $S_t$ and it is the $K$-th ranked candidate, then the loss will push the *gold* sequence up by increasing its score. The candidates for next time-step are chosen in a way similar as first case.\n\n3. If the *gold* sub-sequence of length $t$ is NOT in set $S_t$, then the score of the *gold* sequence is increased to be higher than $K$-th ranked candidate by a margin. In this case, candidates for next step or chosen by only considering *gold* word at time $t$ and getting its top-$K$ successors.\n\n4. Further, since we want the full *gold* sequence to be at top of the beam at the end of the search, when $t=T$, the loss is modified to require the score of *gold* sequence to exceed the score of the *highest* ranked incorrect prediction by a margin. \n\nThis non-probabilistic training method has several advantages:\n\n* The model is trained in a similar way it would be tested, since we use beam-search during training as well as testing. Hence this helps to eliminate exposure bias.\n\n* The score based loss can be easily scaled by a mistake-specific cost function. For example, in MT, one could use a cost function which is inversely proportional to BLEU score. So there is no loss-evaluation mismatch.\n\n* Each time step can have different set of successor words based on any hard constraints in the problem. Note that the model is non-probabilistic and hence this varying successor function will not introduce any label bias. Refer [this set of slides][1] for an excellent illustration of label bias.\n\nCost of forward-prop grows linearly with respect to beam size $K$. However, GPU implementation should help to reduce this cost. Authors propose a clever way of doing BPTT which makes the back-prop almost same cost as ordinary seq2seq training.\n\n**Additional Tricks**\n\n1. Authors pre-train the seq2seq model with regular word level cross-entropy loss and this is crucial since random initialization did not work.\n\n2. Authors use \"curriculum beam\" strategy in training where they start with beam size of 2 and increase the beam size by 1 for every 2 epochs until it reaches the required beam size. You have to train your model with training beam size of at least test beam size + 1. (i.e $K_{tr} >= K_{te} + 1$).\n\n3. When you use drop-out, you need to be careful to use the same dropout value during back-prop. Authors do this by sharing a single dropout across all sequences in a time step.\n\n**Experiments**\n\nAuthors compare the proposed model against basic seq2seq in word ordering, dependency parsing and MT tasks. The proposed model achieves significant improvement over the strong baseline.\n\n**Related Work:**\n\nThe whole idea of the paper is based on [learning as search optimization (LaSO) framework][2] of Daume III and Marcu (2005). Other notable related work are training seq2seq models using mix of cross-entropy and REINFORCE called [MIXER][3] and [an actor-critic based seq2seq training][4]. Authors compare with MIXER and they do significantly better than MIXER. \n\n**My two cents:**\n\nThis is one of the important research directions in my opinion. While other recent methods attempt to use reinforcement learning to avoid the issues in word-level cross-entropy training, this paper proposes a really simple score based solution which works very well. While most of the language generation research is stuck with probabilistic framework (I am saying this w.r.t Deep NLP research), this paper highlights the benefits on non-probabilistic generation models. I see this as one potential way of avoiding the nasty scalability issues that come with softmax based generative models.\n\n[1]: http://www.cs.stanford.edu/~nmramesh/crf\n[2]: https://www.isi.edu/~marcu/papers/daume05laso.pdf\n[3]: http://arxiv.org/pdf/1511.06732v7.pdf\n[4]: https://arxiv.org/pdf/1607.07086v2.pdf\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.02960"
    },
    "63": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.05473",
        "transcript": "Everyone has been thinking about how to apply GANs to discrete sequence data for the past year or so. This paper presents the model that I would guess most people thought of as the first-thing-to-try:\n\n1. Build a recurrent generator model which samples from its softmax outputs at each timestep.\n2. Pass sampled sequences to a recurrent discriminator model which distinguishes between sampled sequences and real-data sequences.\n3. Train the discriminator under the standard GAN loss.\n4. Train the generator with a REINFORCE (policy gradient) objective, where each trajectory is assigned a single episodic reward: the score assigned to the generated sequence by the discriminator.\n\nSounds hacky, right? We're learning a generator with a high-variance model-free reinforcement learning algorithm, in a very seriously non-stationary environment. (Here the \"environment\" is a discriminator being jointly learned with the generator.)\n\nThere's just one trick in this paper on top of that setup: for non-terminal states, the reward is defined as the *expectation* of the discriminator score after stochastically generating from that state forward. To restate using standard (somewhat sloppy) RL syntax, in different terms than the paper: (under stochastic sequential policy $\\pi$, with current state $s_t$, trajectory $\\tau_{1:T}$ and discriminator $D(\\tau)$)\n\n$$r_t = \\mathbb E_{\\tau_{t+1:T} \\sim \\pi(s_t)} \\left[ D(\\tau_{1:T}) \\right]$$\n\nThe rewards are estimated via Monte Carlo \u2014 i.e., just take the mean of $N$ rollouts from each intermediate state. They claim this helps to reduce variance. That makes intuitive sense, but I don't see any results in the paper demonstrating the effect of varying $N$.\n\n---\n\nYep, so it turns out that this sort of works.. with a big caveat:\n\n## The big caveat\n\nGraph from appendix:\n\n![](https://www.dropbox.com/s/5fqh6my63sgv5y4/Bildschirmfoto%202016-09-27%20um%2021.34.44.png?raw=1)\n\nSeqGANs don't work without supervised pretraining. Makes sense \u2014 with a cold start, the generator just samples a bunch of nonsense and the discriminator overfits. Both the generator and discriminator are pretrained on supervised data in this paper (see Algorithm 1).\n\nI think it must be possible to overcome this with the proper training tricks and enough sweat. But it's probably more worth our time to address the fundamental problem here of developing better RL for structured prediction tasks.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1609.05473"
    },
    "64": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.07133",
        "transcript": "This article makes the argument for *interactive* language learning, motivated by some nice recent small-domain success. I can certainly agree with the motivation: if language is used in conversation, shouldn't we be building models which know how to behave in conversation?\n\nThe authors develop a standard multi-agent communication paradigm, where two agents learn communicate in a single-round reference game. (No references to e.g. Kirby or any [ILM][1] work, which is in the same space.) Agent `A1` examines a referent `R` and \"transmits\" a one-hot utterance representation to `A2`, who must successfully identify `R` given the utterance. The one-round conversations are a success when `A2` picks the correct referent `R`. The two agents are jointly trained to maximize this success metric via REINFORCE (policy gradient).\n\n**This is mathematically equivalent to [the NVIL model (Mnih and Gregor, 2014)][2]**, an autoencoder with \"hard\" latent codes which is likewise trained by policy gradient methods.\n\nThey perform a nice thorough evaluation on both successful and \"cheating\" models. This will serve as a useful reference point / starting point for people interested in interactive language acquisition.\n\nThe way clear is forward, I think: let's develop agents in more complex environments, interacting in multi-round conversations, with more complex / longer utterances.\n\n[1]: http://cocosci.berkeley.edu/tom/papers/IteratedLearningEvolutionLanguage.pdf\n[2]: https://arxiv.org/abs/1402.0030",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.07133"
    },
    "65": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.00150",
        "transcript": "(See also a more thorough summary in [a LaTeX PDF][1].)\n\nThis paper has some nice clear theory which bridges maximum likelihood (supervised) learning and standard reinforcement learning. It focuses on *structured prediction* tasks, where we want to learn to predict $p_\\theta(y \\mid x)$ where $y$ is some object with complex internal structure.\n\nWe can agree on some deficiencies of maximum likelihood learning:\n\n- ML training fails to assign **partial credit**. Models are trained to maximize the likelihood of the ground-truth outputs in the dataset, and all other outputs are equally wrong. This is an increasingly important problem as the space of possible solutions grows.\n- ML training is potentially disconnected from **downstream task reward**. In machine translation, we usually want to optimize relatively complex metrics like BLEU or TER. Since these metrics are non-differentiable, we have to settle for optimizing proxy losses that we hope are related to the metric of interest.\n\nReinforcement learning offers an attractive alternative in theory. RL algorithms are designed to optimize non-differentiable (even stochastic) reward functions, which sounds like just what we want. But RL algorithms have their own problems with this sort of structured output space:\n\n- Standard RL algorithms rely on samples from the model we are learning, $p_\\theta(y \\mid x)$. This becomes intractable when our output space is very complex (e.g. 80-token sequences where each word is drawn from a vocabulary of 80,000 words).\n- The reward spaces for problems of interest are extremely sparse. Our metrics will assign 0 reward to most of the 80^80K possible outputs in the translation problem in the paper.\n- Vanilla RL doesn't take into account the ground-truth outputs available to us in structured prediction.\n\nThis paper designs a solution which combines supervised learning with a reinforcement learning-inspired smoothing method. Concretely, the authors design an **exponentiated payoff distribution** $q(y \\mid y^*; \\tau)$ which assigns high mass to high-reward outputs $y$ and low mass elsewhere. This distribution is used to effectively smooth the loss function established by the ground-truth outputs in the supervised data. We end up optimizing the following objective:\n\n$$\\mathcal L_\\text{RML} = - \\mathbb E_{x, y^* \\sim \\mathcal D}\\left[ \\sum_y q(y \\mid y^*; \\tau) \\log p_\\theta(y \\mid x) \\right]$$\n\nThis optimization depends on samples from our dataset $\\mathcal D$ and, more importantly, the stationary payoff distribution $q$. This contrasts strongly with standard RL training, where the objective depends on samples from the non-stationary model distribution $p_\\theta$. To make that clear, we can rewrite the above with another expectation:\n\n$$\\mathcal L_\\text{RML} = - \\mathbb E_{x, y^* \\sim \\mathcal D, y \\sim q(y \\mid y^*; \\tau)}\\left[ \\log p_\\theta(y \\mid x) \\right]$$\n\n### Model details\n\nIf you're interested in the low-level details, I wrote up the gist of the math in [this PDF][1].\n\n### Analysis\n\n#### Relationship to label smoothing\n\nThis training approach is mathematically equivalent to label smoothing, applied here to structured output problems. In next-word prediction language modeling, a popular trick involves smoothing the target distributions by combining the ground-truth output with some simple base model, e.g. a unigram word frequency distribution. (This just means we take a weighted sum of the one-hot vector from our supervised data and a normalized frequency vector calculated on some corpus.) Mathematically, the cross entropy with label smoothing is\n\n$$\\mathcal L_\\text{ML-smooth} = - \\mathbb E_{x, y^* \\sim \\mathcal D} \\left[ \\sum_y p_\\text{smooth}(y; y^*) \\log p_\\theta(y \\mid x) \\right]$$\n\n(The equation above leaves out a constant entropy term.)\n\nThe gradient of this objective looks exactly the same as the reward-augmented ML gradient from the paper:\n\n$$\\nabla_\\theta \\mathcal L_\\text{ML-smooth} = \\mathbb E_{x, y^* \\sim \\mathcal D, y \\sim p_\\text{smooth}} \\left[ \\log p_\\theta(y \\mid x) \\right]$$\n\nSo reward-augmented likelihood is equivalent to label smoothing, where our smoothing distribution is log-proportional to our downstream reward function.\n\n#### Relationship to distillation\n\nOptimizing the reward-augmented maximum likelihood is equivalent to minimizing the KL divergence $$D_\\text{KL}(q(y \\mid y^*; \\tau) \\mid\\mid p_\\theta(y \\mid x))$$\n\nThis divergence reaches zero iff $q = p$. We can say, then, that the effect of optimizing on $\\mathcal L_\\text{RML}$ is to **distill** the reward function (which parameterizes $q$) into the model parameters $\\theta$ (which parameterize $p_\\theta$).\n\nIt's exciting to think about other sorts of more complex models that we might be able to distill in this framework. The unfortunate (?) restriction is that the \"source\" model of the distillation ($q$ in this paper) must admit to efficient sampling.\n\n#### Relationship to adversarial training\n\nWe can also view reward-augmented maximum likelihood training as a data augmentation technique: it synthesizes new \"partially correct\" examples using the reward function as a guide. We then train on all of the original and synthesized data, again weighting the gradients based on the reward function.\n\nAdversarial training is a similar data augmentation technique which generates examples that force the model to be robust to changes in its input space (robust to changes of $x$). Both adversarial training and the RML objective encourage the model to be robust \"near\" the ground-truth supervised data. A high-level comparison:\n\n- Adversarial training can be seen as data augmentation in the input space; RML training performs data augmentation in the output space.\n- Adversarial training is a **model-based data augmentation**: the samples are generated from a process that depends on the current parameters during training. RML training performs **data-based augmentation**, which could in theory be done independent of the actual training process.\n\n---\n\nThanks to Andrej Karpathy, Alec Radford, and Tim Salimans for interesting discussion which contributed to this summary.\n\n[1]: https://drive.google.com/file/d/0B3Rdm_P3VbRDVUQ4SVBRYW82dU0/view",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1609.00150"
    },
    "66": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1608.02996",
        "transcript": "This is a simple unsupervised method for learning word-level translation\nbetween embeddings of two different languages.\n\nThat's right -- unsupervised.\n\nThe basic motivating hypothesis is that there should be an isomorphism between\nthe \"semantic spaces\" of different languages:\n\n> we hypothesize that, if languages are used to convey thematically similar information in similar contexts, these random processes should be approximately isomorphic between languages, and that this isomorphism can be learned from the statistics of the realizations of these processes, the monolingual corpora, in principle without any form of explicit alignment.\n\nIf you squint a bit, you can make the more aggressive claim from this premise\nthat there should be a nonlinear / MLP mapping between *word embedding spaces*\nthat gets us the same result.\n\nThe author uses the adversarial autoencoder (AAE, from Makhzani last year)\nframework in order to enforce a cross-lingual semantic mapping in word\nembedding spaces. The basic setup for adversarial training between a source and\na target language:\n\n1. Sample a batch of words from the source language according to the language's\n   word frequency distribution.\n2. Sample a batch of words from the target language according to its word\n   frequency distribution. (No sort of relationship is enforced between the two\n   samples here.)\n3. Feed the word embeddings corresponding to the source words through an\n   *encoder* MLP. This corresponds to the standard \"generator\" in a GAN setup.\n4. Pass the generator output to a *discriminator* MLP along with the\n   target-language word embeddings.\n5. Also pass the generator output to a *decoder* which maps back to the source\n   embedding distribution.\n6. Update weights based on a combination of GAN loss + reconstruction loss.\n\n### Does it work?\n\nWe don't really know. The paper is unfortunately short on evaluation --- we\njust see a few examples of success and failure on a trained model. One easy\nevaluation would be to compare accuracy in lexical mapping vs. corpus frequency\nof the source word. I would bet that this would reveal the model hasn't done\nmuch more than learn to align a small set of high-frequency words.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1608.02996"
    },
    "67": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/miccai/RonnebergerFB15",
        "transcript": "1. U-NET learns segmentation in an end to end images.\n2. They solved Challenges are\n        * Very few annotated images (approx. 30 per application).\n        * Touching objects of the same class.\n# How:\n* Input image is fed in to the network, then the data is propagated through the network along all possible path at the end segmentation maps comes out.\n* In U-net architecture, each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.\nhttps://i.imgur.com/Usxmv6r.png\n* In two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2for down sampling. At each down sampling step they double the number of feature channels.\n* Contracting path (left side from up to down) is increases the feature channel and reduces the steps and an expansive path (right side from down to up) consists of sequence of up convolution and concatenation with the corresponds high resolution features from contracting path.\n* The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image.\n\n## Challenges:\n1. Overlap-tile strategy for seamless segmentation of arbitrary large images:\n     * To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image.\n     * In fig, segmentation of the yellow area uses input data of the blue area and the raw data extrapolation by mirroring.\nhttps://i.imgur.com/NUbBRUG.png\n2. Augment training data using deformation:\n    * They use excessive data augmentation by applying elastic deformations to the available training images. \n    * Then the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus.\n    * Deformation used to be the most common variation in tissue and realistic deformations can be simulated efficiently. \nhttps://i.imgur.com/CyC8Hmd.png\n3. Segmentation of touching object of the same class:\n    * They propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function.\n    * Ensure separation of touching objects, in that segmentation mask for training (inserted background between touching objects) get the loss weights for each pixel.\nhttps://i.imgur.com/ds7psDB.png\n4. Segmentation of neural structure in electro-microscopy(EM):\n    * Ongoing challenge since ISBI 2012 in this dataset structures with low contrast, fuzzy membranes and other cell components.\n    * The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila first instar larva ventral nerve cord (VNC). Each image comes with corresponding fully annotated ground truth segmentation map for cells(white) and membranes (black).\n    * An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding  the map at 10 different levels and computation of the warping error, the Rand error and the pixel error.\n\n### Results:\n* The u-net (averaged over 7 rotated versions of the input data) achieves with-out any further pre or post-processing a warping error of 0.0003529, a rand-error of 0.0382 and a pixel error of 0.0611.\nhttps://i.imgur.com/6BDrByI.png\n* ISBI cell tracking challenge 2015, one of the dataset contains cell phase contrast microscopy has strong shape variations,weak outer borders, strong irrelevant inner borders and cytoplasm has same structure like background.\nhttps://i.imgur.com/vDflYEH.png\n* The first data set PHC-U373 contains Glioblastoma-astrocytoma U373 cells on a polyacrylimide substrate recorded by phase contrast microscopy- It contains 35 partially annotated training images. Here we achieve an average IOU (\"intersection over union\") of 92%,which is significantly better than the second best algorithm with 83%.\nhttps://i.imgur.com/of4rAYP.png\n* The second data set DIC-HeLa are HeLa cells on a flat glass recorded by differential interference contrast (DIC) microscopy - It contains 20 partially annotated training images. Here we achieve an average IOU of 77.5% which is significantly better than the second best algorithm with 46%.\nhttps://i.imgur.com/Y9wY6Lc.png\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-24574-4_28"
    },
    "68": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1706.01427",
        "transcript": "The paper proposes a reusable neural network module to `reason about the relations between entities and their properties`:\n\n$$ RN(O) = f_\\phi \\left( \\sum_{i,j} g_\\theta(o_i, o_j) \\right), $$\n\n- $O$ is a set of input objects $\\{o_1, o_2, ..., o_n\\}, o_i \\in R^m$\n- $g_\\theta$ is a neural network (MLP) which approximates object-to-object relation function\n- $f_\\phi $ is a neural network (MLP) which transforms summed pairwise object-to-object relations to some desired output\n\nRN's operate on sets (due to summation in the formula) and thus are invariant to the order of objects in the input.\n\nIn terms of architecture, RN module is used at the tail of a neural network taking input objects in form of CNN or LSTM embeddings.\n\nThis work is evaluated on several tasks where it achieves reasonably good (even superhuman) performance:\n- CLEVR and Sort-of-CLEVR - question answering about an image\n- bAbI - text based question answering\n- Dynamic physical system - MuJoCo simulations with physical relation between entities",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1706.01427"
    },
    "69": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=mikolov2013distributed",
        "transcript": "The main contributions of [Distributed representations of words and phrases and their compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) by Mikolov et al. are several extension to their previously introduced skip-gram (and CBOW) model for word vector representations learned on large text corpora.\n\nFor a given word in a training text corpus, the skip-gram model tries to predict surrounding words. Thus, training the skip-gram model on a text corpus adapts vector representations of words in a way that  maximizes the probability for correct surrounding words. This leads to distributed vector representations that capture a large number of precise syntactic and semantic word relationships.\n\nThey propose a method to identify idiomatic phrases that are not compositions of individual words (like \u201cBoston Globe\u201d) in order to improve the vocabulary base. Also, an alternative training method to the hierarchical softmax, negative sampling, is introduced and analysed in detail. Further, they show that meaningful linear arithmetic operations can be performed on the trained vector representations, which makes precise analogical reasoning possible.\n",
        "sourceType": "blog",
        "linkToPaper": "?name=fabianboth"
    },
    "70": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MnihBMGLHSK16",
        "transcript": "The main contribution of [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/pdf/1602.01783v1.pdf) by Mnih et al. is a ligthweight framework for reinforcement learning agents. \n\nThey propose a training procedure which utilizes asynchronous gradient decent updates from multiple agents at once. Instead of training one single agent who interacts with its environment, multiple agents are interacting with their own version of the environment simultaneously.\n\nAfter a certain amount of timesteps, accumulated gradient updates from an agent are applied to a global model, e.g. a Deep Q-Network. These updates are asynchronous and lock free. \n\nEffects of training speed and quality are analyzed for various reinforcement learning methods. No replay memory is need to decorrelate successive game states, since all agents are already exploring different game states in real time. Also, on-policy algorithms like actor-critic can be applied.\n\nThey show that asynchronous updates have a stabilizing effect on policy and value updates. Also, their best method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.01783"
    },
    "71": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2003.05856",
        "transcript": "disclaimer: I'm the first author of the paper\n\n## TL;DR \n\nWe have made a lot of progress on catastrophic forgetting within the standard evaluation protocol,\ni.e. sequentially learning a stream of tasks and testing our models' capacity to remember them all.\n\nWe think it's time a new approach to Continual Learning (CL), coined OSAKA, which is more aligned with real-life applications of CL. It brings CL closer to Online Learning and Open-World Learning.\n\nmain modifications we propose:\n- bring CL closer to Online learning i.e. at test time, the model is continually learning and evaluated on its online predictions \n- it's fine to forget, as long as you can quickly remember (just like we humans do)\n- we allow pretraining, (because you wouldn't deploy an untrained CL system, right?) but at test time, the model will have to quickly learn new out-of-distribution (OoD) tasks (because the world is full of surprises)\n- the tasks distribution is actually a hidden Markov chain. This implies:\n    - new and old tasks can re-occur (just like in real life). Better remember them quickly if you want to get a good total performance!\n    - tasks have different lengths\n    - and the tasks boundaries are unknown (task agnostic setting)\n\n### Bonus:\nWe provide a unifying framework explaining the space of machine learning setting {supervised learning, meta learning, continual learning, meta-continual learning, continual-meta learning} in case it was starting to get confusing :p\n\n## Motivation\n\nWe imagine an agent, embedded or not, first pre-trained in a controlled environment and later deployed in the real world, where it faces new or unexpected situations. This scenario is relevant for many applications. For instance, in robotics, the agent is pre-trained in a factory and deployed in homes or\nin manufactures where it will need to adapt to new domains and maybe solve new tasks. Likewise, a virtual assistant can be pre-trained on static datasets and deployed in a user\u2019s life to fit its personal needs. \n\nFurther motivations can be found in time series forecasting, e.g., market prediction,\ngame playing, autonomous customer service, recommendation systems, autonomous driving, to name a few. In this scenario, we are interested in the cumulative performance of the agent throughout its lifetime. Differently, standard CL reports the agent\u2019s final performance on all tasks at the end of its life. In order\nto succeed in this scenario, agents need the ability to learn new tasks as well as quickly remembering old ones.\n\n## Unifying Framework\n\nWe propose a unifying framework explaining the space of machine learning setting {supervised learning, meta learning, continual learning, meta-continual learning, continual-meta learning} with meta learning terminology. \n\nhttps://i.imgur.com/U16kHXk.png\n\n(easier to digest with accompanying text)\n\n## OSAKA\n\nThe main features of the evaluation framework are\n- task agnosticism\n- pre-training is allowed, but OoD tasks at test time\n- task revisiting\n- controllable non-stationarity\n- online evaluation\n\n(see paper for the motivations of the features)\n\n## Continual-MAML: an initial baseline\n\nA simple extension of MAML that is better suited than previous methods in the proposed setting.\n\nhttps://i.imgur.com/C86WUc8.png\n\nFeatures are:\n- Fast Adapatation\n- Dynamic Representation\n- Task boundary detection\n- Computational efficiency\n\n\n## Experiments\n\nWe provide a suite of 3 benchmarks to test algorithms in the new setting.\n\nThe first includes the Omniglot, MNIST and FashionMNIST dataset. \n\nThe second and third use the Synbols (Lacoste et al. 2018) and TieredImageNet datasets, respectively.\n\nThe first set of experiments shows that the baseline outperforms previous approaches, i.e., supervised learning, meta learning, continual learning, meta-continual learning, continual-meta learning, in the new setting.\n\nhttps://i.imgur.com/IQ1WYTp.png\n\nThe second and third experiments lead us to similar conclusions\n\ncode: https://github.com/ElementAI/osaka\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2003.05856v1"
    },
    "72": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-02425",
        "transcript": "## Introduction\n\nBayesian Neural Networks (BNN): intrinsic importance model based on weight uncertainty; variational inference can approximate posterior distributions using Monte Carlo sampling for gradient estimation; acts like an ensemble method in that they reduce the prediction variance but only uses 2x the number of parameters. \n\nThe idea is to use BNN's uncertainty to guide gradient descent to not update the important weight when learning new tasks.\n\n## Bayes by Backprop (BBB):\n\nhttps://i.imgur.com/7o4gQMI.png\n\nWhere $q(w|\\theta)$ is our approximation of the posterior $p(w|x)$. $q$ is most probably gaussian with diagonal covariance. We can optimize this via the ELBO:\n\nhttps://i.imgur.com/OwGm20b.png\n\n## Uncertainty-guided CL with BNN (UCB):\n\nUCB the regularizing is performed with the learning rate such that the learning rate of each parameter and hence its gradient update becomes a function of its importance.  They set the importance to be inversely proportional to the standard deviation $\\sigma$ of $q(w|\\theta)$\n\nSimply put, the more confident the posterior is about a certain weight, the less is this weight going to be updated. \n\nYou can also use the importance for weight pruning (sort of a hard version of the first idea)\n\n## Cartoon\n\nhttps://i.imgur.com/6Ld79BS.png\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.02425"
    },
    "73": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/FinnRKL19",
        "transcript": "## Introduction\n\nTwo distinct research paradigms have studied how prior tasks or experiences can be used by an agent to inform future learning.\n\n* Meta Learning:  past experience is used to acquire a prior over model parameters or a learning procedure, and typically studies a setting where a set of meta-training tasks are made available together upfront\n* Online learning : a sequential setting where tasks are revealed one after another, but aims to attain zero-shot generalization without any task-specific adaptation.\n\nWe argue that neither setting is ideal for studying continual lifelong learning. Meta-learning deals with learning to learn, but neglects the sequential and non-stationary aspects of the problem. Online learning offers an appealing theoretical framework, but does not generally consider how past experience can accelerate adaptation to a new task.\n\n## Online Learning\n\nOnline learning focuses on regret minimization. Most standard notion of regret is to compare to the cumulative loss of the best fixed model in hindsight:\nhttps://i.imgur.com/pbZG4kK.png\nOne way minimize regret is with Follow the Leader (FTL):\nhttps://i.imgur.com/NCs73vG.png\n\n## Online Meta-learning Setting:\n\nlet $U_t$ be the update procedure for task $t$\ne.g. in MAML:\nhttps://i.imgur.com/Q4I4HkD.png\n\n\nThe overall protocol for the setting is as follows:\n1. At round t, the agent chooses a model defined by $w_t$\n2. The world simultaneously chooses task defined by $f_t$ \n3. The agent obtains access to the update procedure $U_t$, and uses it to update parameters as $\\tilde w_t = U_t(w_t)$\n4. The agent incurs loss $f_t(\\tilde w_t )$. Advance to round t + 1.\n\nthe goal for the agent is to minimize regrets over rounds.\nAchieving sublinear regrets means you're improving and converging to upper bound (joint training on all tasks)\n\n\n## Algorithm and Analysis:\n\nFollow the meta-leader (FTML): \nhttps://i.imgur.com/qWb9g8Q.png\n\nFTML\u2019s regret is sublinear (under some assumption)\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v97/finn19a.html"
    },
    "74": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1908.04742",
        "transcript": "Disclaimer: I am an author\n\n# Intro\n\nExperience replay (ER) and generative replay (GEN) are two effective continual learning strategies. In the former, samples from a stored memory are replayed to the continual learner to reduce forgetting. In the latter, old data is compressed with a generative model and generated data is replayed to the continual learner. Both of these strategies assume a random sampling of the memories. But learning a new task doesn't cause **equal** interference (forgetting) on the previous tasks!  \n\nIn this work, we propose a controlled sampling of the replays. Specifically, we retrieve the samples which are most interfered, i.e. whose prediction will be most negatively impacted by the foreseen parameters update. The method is called Maximally Interfered Retrieval (MIR).\n\n## Cartoon for explanation\n\nhttps://i.imgur.com/5F3jT36.png\n\nLearning about dogs and horses might cause more interference on lions and zebras than on cars and oranges. Thus, replaying lions and zebras would be a more efficient strategy.\n\n# Method\n\n1) incoming data: $(X_t,Y_t)$\n\n2) foreseen parameter update: $\\theta^v= \\theta-\\alpha\\nabla\\mathcal{L}(f_\\theta(X_t),Y_t)$\n\n### applied to ER (ER-MIR)\n3) Search for the top-$k$ values $x$ in the stored memories using the criterion $$s_{MI}(x) = \\mathcal{L}(f_{\\theta^v}(x),y) -\\mathcal{L}(f_{\\theta}(x),y)$$\n\n### or applied to GEN (GEN-MIR)\n3)   \n$$\n     \\underset{Z}{\\max} \\, \\mathcal{L}\\big(f_{\\theta^v}(g_\\gamma(Z)),Y^*\\big) -\\mathcal{L}\\big(f_{\\theta}(g_\\gamma(Z)),Y^*\\big)\n$$\n$$\n         \\text{s.t.}   \\quad ||z_i-z_j||_2^2 > \\epsilon \\forall  z_i,z_j \\in Z \\,\\text{with} \\, z_i\\neq z_j\n$$\ni.e. search in the latent space of a generative model $g_\\gamma$ for samples that are the most forgotten given the foreseen update.\n\n4) Then add theses memories to incoming data $X_t$ and train $f_\\theta$\n\n# Results\n\n### qualitative\nhttps://i.imgur.com/ZRNTWXe.png\n\nWhilst learning 8s and 9s (first row), GEN-MIR mainly retrieves 3s and 4s (bottom two rows) which are similar to 8s and 9s respectively.\n\n### quantitative \n\nGEN-MIR was tested on MNIST SPLIT and Permuted MNIST, outperforming the baselines in both cases.\n\nER-MIR was tested on MNIST SPLIT, Permuted MNIST and Split CIFAR-10, outperforming the baselines in all cases.\n\n\n# Other stuff\n### (for avid readers)\n\nWe propose a hybrid method (AE-MIR) in which the generative model is replaced with an autoencoder to facilitate the compression of harder dataset like e.g. CIFAR-10.\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1908.04742"
    },
    "75": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.01392",
        "transcript": "### Summary\nKnowing when a model is qualified to make a prediction is critical to safe deployment of ML technology. Model-independent / Unsupervised Out-of-Distribution (OoD) detection is appealing mostly because it doesn't require task-specific labels to train. It is tempting to suggest a simple one-tailed test in which lower likelihoods are OoD (assigned by a Likelihood Model), but the intuition that In-Distribution (ID) inputs should have highest likelihoods _does not hold in higher dimension_. The authors propose to use the Watanabe-Akaike Information Criterion (WAIC) to circumvent this problem and empirically show the robustness of the approach.\n\n### Counterintuitive Properties of Likelihood Models:\nhttps://i.imgur.com/4vo0Ff5.png\nSo a GLOW model with Gaussian prior maps SVHN closer to the origin than Cifar (but never actually generates SVHN because Gaussian samples are on the shell). This is bad news for OoD detection.\n\n### Proposed Methodology:\nUse the WAIC criterion for OoD detection which gives an asymptotically correct estimate of the gap between the training set and test set expectations:\nhttps://i.imgur.com/vasSxuk.png\nBasically,  the correction term subtracts the variance in likelihoods across independent samples from the posterior. This acts to robustify the estimate, ensuring that points that are sensitive to the particular choice of posterior are penalized. They use an ensemble of generative models as a proxy for posterior samples i.e. the ensembles acts as approximate posterior samples.\nNow, OoD can be detected with a Likelihood Model:\nhttps://i.imgur.com/M3CDKOA.png\n### Discussion\nInterestingly, GLOW maps Cifar and other datasets INSIDE the gaussian shell (which is an annulus of radius $\\sqrt{dim} = \\sqrt{3072} \\approx 55.4$\nhttps://i.imgur.com/ERdgOaz.png\nThis is in itself quite disturbing, as it suggests that better flow-based generative models (for sampling) can be obtained by encouraging the training distribution to overlap better with the typical set in latent\nspace.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.01392"
    },
    "76": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1801.04381",
        "transcript": "- **Linear Bottlenecks**. Authors show, that even though theoretically activations can be working in linear regime, removing activation from bottlenecks of residual network gives a boost to performance.\n-**Inverted residuals**. The shortcut connecting bottleneck perform better than shortcuts connecting the expanded layers\n- **SSDLite**.  Authors propose to replace convolutions in SSD by depthwise convolutions, significantly reducing both number of parameters and number of calculations, with minor impact on precision. \n- **MobileNetV2**. A new architecture, which is basically ResNet with changes mentioned above, outperforms or shows comaparable performance with MobileNetV1, ShuffleNet and NASNet for same number of MACs. Object detection with SSDLite can be ran on ARM core in 200ms. Also a potential of semantic segmentation on mobile devices is chown: a network achieving 75.32% mIOU  on PASCAL and  only requiring 2.75B MACs.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1801.04381"
    },
    "77": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1712.02029",
        "transcript": "**TL;DR**: You can increase batch size in advanced phases of training without hurting accuracy and gaining some speedup. You should multiply the learning rate by the same value you multiplied batch size.\n\n**Long version**: Authors propose to increase batch size gradually, starting with a  small  batch  size $r$, and then progressively increase the batch size while adapting the learning rate $\\alpha$ so that the ratio $\\alpha/r$ remains constant (without taking in account scheduled LR reduce). In this paper, they double the batch size by schedule. At the same time, learning rate is decayed and then multiplied by 2 to compensate batch size increase: if in baseline lr is multiplied by $0.375$, it is multiplied by $0.75$ now.\n\nThe experiments on CIFAR-100 dataset show that the gradual increase of batch size allows to converge to the same values as constant small batch size. However, bigger batches allow faster training, providing $\\times 1.5$ speedup on AlexNet, and around $\\times 1.2$ speedup on ResNet and VGG, for both forward and backward passes on single GPU.\n\nOn multiple GPUs the approach allows to further increase batchsize. On fortunate setups authors manage to get up to $\\times 1.6$ speedup compared to constant batch size equal to initial value, while the error is almost unchanged. For bigger batch sizes lr warmup is used.\n\nFor ImageNet, same behavior is shown for accuracy: gradual increase of batch size converges to same values as setup with initial batch size. Since authors haven't access to a system capable of processing large batches on ImageNet, no performance results are reported.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1712.02029"
    },
    "78": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.01578",
        "transcript": "### Main Idea:\nIt basically tunes the hyper-parameters of the neural network architecture using reinforcement learning. The reward signal is taken as evaluation on the validation set. The method is policy gradient as the cost function is non-differentiable. \n\n### Method:\n #### i.   Actions:\n1. There is controller RNN which predicts some hyper-parameters of the layer conditioned on the previous predictions. This prediction is just a one-hot vector based on the previous hyper-parameter chosen. At the start for the first prediction - this vector is just all zeros.\n2. Once the network is generated completely, it is trained for a fixed number of epochs, the reward signal is calculated based on the evaluation on the validation set.\n\n#### ii.  Training:\n1. Nothing fancy in the reinforcement learning approach simple policy gradients.\n2. Baseline is added to reduce the variance.\n3.  It takes 2-3 weeks to train it over 800 GPUs!!\n\n#### iii. Results:\n1. Use it to generate CNNs and LSTM cells. Close to state-of-art results with generated architectures.\n\n### Possible new directions:\n1. Use better techniques RL techniques like TRPO, PPO etc.\n2. Right now, they generate fixed length architecture. Their reason is for variable length architectures, it is difficult to determine how much time each architecture is trained. Smaller networks are easier to train. Thus, somehow determine training time as function of the learning capacity of the network.\n\n\nCode:\nThey haven't released the code yet. I tried to simulate it in torch.(https://github.com/abhigenie92/nn_search)\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.01578"
    },
    "79": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/NgYD15",
        "transcript": "In this paper, the authors raise a very important point for instance based image retrieval. For a task like an image recognition features extracted from higher layer of deep networks works really well in general, but for task like instance based image retrieval features extracted from higher layers don't prove to be that useful, so the authors suggest that we take features from lower layer and on those features, apply [VLAD encoding](https://www.robots.ox.ac.uk/~vgg/publications/2013/arandjelovic13/arandjelovic13.pdf). On top of the VLAD encoding as part of post processing, we perform steps like intra-normalisation and then apply PCA and reduce the encoding to a size of 128 Dimension. The authors have performed their experiments using [Googlenet](https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf) and [VGG-16](https://arxiv.org/pdf/1409.1556v6.pdf), and they tried Inception 3a, Inception 4a and Inception 4e on GoogleNet and conv4_2, conv5_1 and conv5_2 on VGG-16. The above mentioned layers has almost similar performance on the dataset they have used. The performance metric used by the authors is Mean Average Precision(MAP). ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPRW.2015.7301272"
    },
    "80": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.06693",
        "transcript": "This paper introduces a new regularization technique that aims at reducing over-fitting without reducing the capacity of a model. It draws on the claim that models start to over-fit data when co-label similarities start to disappear, e.g. when the model output does not show that dogs of similar breeds like German shepherd and Belgian shepherd are similar anymore.\n\nThe idea is that models in an early training phase *do* show these similarities. In order to keep this information in the model, target labels $Y^t_c$ for training step $t$ are changed by adding the exponential mean of output labels of previous training steps $\\hat{Y}^t$:\n\n$$\n\\hat{Y}^t = \\beta \\hat{Y}^{t-1} + (1-\\beta)F(X, W),\\\\\nY_c^t = \\gamma\\hat{Y}^t  + (1-\\gamma)Y,\n$$\n\nwhere $F(X,W)$ is the current network's output, and $Y$ are the ground truth labels. This way, the network should remember which classes are similar to each other. The paper shows that training using the proposed regularization scheme preserves co-label similarities (compared to an over-fitted model) similarly to dropout. This confirms the intuition the proposed method is based on.\n\nThe method introduces several new hyper-parameters:\n - $\\beta$, defining the exponential decay parameter for averaging old predictions\n - $\\gamma$, defining the weight of soft targets to ground truth targets\n - $n_b$, the number of 'burn-in' epochs, in which the network is trained with hard targets only\n - $n_t$, the number of epochs between soft-target updates\n\nResults on MNIST, CIFAR-10 and SVHN are encouraging, as networks with soft-target regularization achieve lower losses on almost all configurations. However, as of today, the paper does not show how this translates to classification accuracy. Also, it seems that the results are from one training run only, so it is difficult to assess if this improvement is systematic.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1609.06693"
    },
    "81": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.07571",
        "transcript": "This paper is based on an intriguing idea of combining state-space models (SSMs) and recurrent neural networks (RNNs). Ideally, it is very much needed: For the sequences which have distinct structure and high variability, probabilistic modelling is a big problem. The handcrafted and parameterised feature representations are widely used to ease the problem and it is customary to develop the probabilistic model on top of these extracted features from the signal (such as using a short-time Fourier transform and _then_ probabilistic modelling over these features).\n\nBut machines should be able to handle learning the representation part as well. So the story of the paper. Here, it is termed neural network with stochastic layers but one can safely say that the model is a state-space model with a deterministic neural network layer which is tied to both hidden variables and observations. Still, due to the complicated nonlinearities, it is not easy to understand what's going on.\n\nOne thing I found missing from the paper is the exact form of nonlinearity used for the NN layer as I am looking at it from the perspective of probabilistic modelling. But this is probably because of space reasons.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.07571"
    },
    "82": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/spm/BronsteinBLSV17",
        "transcript": "This paper surveys progress on adapting deep learning techniques to non-Euclidean data and suggests future directions. One of the strengths (and weaknesses) of deep learning--specifically exploited by convolutional neural networks--is that the data is assumed to exhibit translation invariance/equivariance and invariance to local deformations. Hence, long-range dependencies can be learned with multi-scale, hierarchical techniques where spatial resolution is reduced. However, this means that any information about the data that can't be learned when spatial resolution is reduced can get lost (I believe that residual networks aim to address this by the skip connections that are able to learn an identity operation; also, in computer vision, multi-scale versions of the data are often fed to CNNs). Key areas where this assumption about the data appears to be true is computer vision and speech recognition.\n\n#### Some quick background \n\nThe *Laplacian*, a self-adjoint (symmetric) positive semi-definite operator, which is defined for smooth manifolds and graphs in this paper, can be thought of as the difference between the local average of a function around a point and the value of the function at the point itself. It's generally defined as $\\triangle = -\\text{div} \\nabla$. When discretizing a continuous, smooth manifold with a *mesh*, note that the graph Laplacian might not converge to the continuous Laplacian operator with increasing sampling density. To be consistent, need to create a triangular mesh, i.e., represent the manifold as a polyhedral surface.\n\n### Spectral methods\n\nFourier analysis on non-Euclidean domains is possible by considering the eigendecomposition of the Laplacian operator. A possible transformation of the Convolution Theorem to functions on manifolds and graphs is discussed, but is noted as not being shift-invariant. \n\nThe Spectral CNN can be defined by introducing a spectral convolutional layer acting on the vertices of the graph and using filters in the frequency domain and the eigenvectors of the Laplacian. However, the spectral filter coefficients will be dependent on the particular eigenvectors (basis) - domain dependency == bad for generalization!\n\nThe non-Euclidean analogy of pooling is *graph coarsening*- only a fraction of the vertices of the graph are retained. Strided convolutions can be generalized to the spectral construction by only keeping the low-frequency components - must recompute the graph Laplacian after applying the nonlinearity in the spatial domain, however. \n\nPerforming matrix multiplications on the eigendecomposition of the Laplacian is expensive!\n\n### Spectrum-free Methods\n\n**A polynomial of the Laplacian acts as a polynomial on the eigenvalues**. ChebNet (Defferrard et al.) and Graph Convolutional Networks (Kipf et al.) boil down to applying simple filters acting on the r- or 1-hop neighborhood of the graph in the spatial domain.\n\nSome examples of generalizations of CNNs that define weighting functions for a locally Euclidean coordinate system around a point on a manifold are the \n  * Geodesic CNN\n  * Anisotropic CNN\n  * Mixture Model network (MoNet)\n\n\n#### What problems are being solved with these methods?\n\n* Ranking and community detection on social networks\n* Recommender systems\n* 3D geometric data in Computer Vision/Graphics\n    * Shape classification\n    * Feature correspondence for 3D shapes\n* Behavior of N-particle systems (particle physics, LHC)\n* Molecule design\n* Medical imaging\n\n### Open Problems\n\n* *Generalization* spectral analogues of convolution learned on one graph cannot be readily applied to other ones (domain dependency). Spatial methods generalize across different domains, but come with their own subtleties\n* *Time-varying domains*\n* *Directed graphs* non-symmetric Laplacian that do not have orthogonal eigendecompositions for interpretable spectral-domain constructions\n* *Synthesis problems* generative models\n* *Computation* extending deep learning frameworks for non-Euclidean data",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/MSP.2017.2693418"
    },
    "83": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1523/jneurosci.0763-16.2016",
        "transcript": "## Summary \n\nThe research question the authors answered was whether by shifting from an episodic to a \"schematic\", or gist-like, memory system, a reinforcement learning agent could learn to achieve its goals in a dynamic environment. The authors focused on 2D navigation tasks where the reward locations constantly changed, such that new reward locations were correlated in the short-term but where independent and sampled from a stable distribution in the long-term. I found it interesting that the authors claimed the real world is like this, and consequentially they staked a lot of the significance of their work on this fact. \n\n**The main conclusion they came to was that given the existence of a stable long-term distribution for reward location (or whatever random variable the agent is concerned with estimating a distribution for), the optimal strategy for an agent is to shift from utilizing episodic to schematic memories slowly.** \n\nThe authors implemented their agent using a novel neural network architecture that consisted of, in general, an episodic memory system, a schematic memory system, a critic to generate a TD-error. \n\nThe episodic memory system was:\n* a spatial encoder which took in the (x,y)-pair of the current location of the agent,\n* an autoencoder implemented as a 3-layer recurrent network\n* a network of place field units \n\nThe output of the spatial encoder fed into the autoencoder, and the output of this fed into the place cells. \"Retrieving\" memory from the place cells was implemented as a fully-connected sigmoid layer.\n\nThe use of place field units was quite interesting; the idea behind this was to learn to associate activation patterns of place cells with specific locations within the environment where rewards were recently found. \n\nThe schematic memory was implemented as a Restricted Boltzman Machine. The first layer was a direct projection of the place cells from the episodic network. The ultimate goal of the RBM was to learn a general statistical model of the reward locations. It was trained in an offline manner (i.e., while the agent was \"at rest\" between trials) by using random activity in the spatial encoder, and propagating that through to the RBM. This was curious, but apparently since they also had added a TD-prediction error to the episodic system via a critic, this was more biologically plausible than iid sampling from the episodic memory. \n\nThe agent has a parameter that controls how much it can mix its episodic and schematic memories; the resultant \"mixed\" memory influences action-selection.\n\n## Take-aways\nWe're seeing a shift towards more complex RL environments that this could be applied to; for example, 3D navigation tasks where there are multiple goals that could potentially move over time. \n\nPerhaps this could also be applied to modeling of the dynamic behavior of other agents in a multi-agent setting? \n\nCool use of unsupervised learning to enhance RL! ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1523/jneurosci.0763-16.2016"
    },
    "84": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Hadfield-Menell16",
        "transcript": "In the future, AI and people will work together; hence, we must concern ourselves with ensuring that AI will have interests aligned with our own. \nThe authors suggest that it is in our best interests to find a solution to the \"value-alignment problem\". As recently pointed out by Ian Goodfellow, however,\n[this may not always be a good idea](https://www.quora.com/When-do-you-expect-AI-safety-to-become-a-serious-issue).\n\nCooperative Inverse Reinforcement Learning (CIRL) is a formulation of a cooperative, partial information game between a human and a robot. Both share a reward \nfunction, but the robot does not initially know what it is. One of the key departures from classical Inverse Reinforcement Learning\nis that the teacher, which in this case is the human, is not assumed to act optimally. Rather, it is shown that sub-optimal actions\non the part of the human can result in the robot learning a better reward function. The structure of the CIRL formulation is such that it should encourage the \nhuman to not attempt to teach by demonstration in a way that greedily maximizes immediate reward. Rather, the human learns how to \"best respond\" to the robot.\n\nCIRL can be formulated as a dec-POMDP, and reduced to a single-agent POMDP. The authors solved a 2D navigation task with CIRL to demonstrate the inferiority of having the human follow a \"demonstration-by-expert\" policy as opposed to a \"best-response\" policy.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.03137"
    },
    "85": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/IandolaMAHDK16",
        "transcript": "$\\bf Summary:$\nThe paper is about squeezing the number of parameters in a convolutional neural network. The number of parameters in a convolutional layer is given by (number of input channels)$\\times$(number of filters)$\\times$(size of filter$\\times$size of filter).\n\nThe paper proposes 2 strategies: (i) replace 3x3 filters with 1x1 filters and (ii) decrease the number of input channels. They assume the budget of the filter is given, i,e., they do not tinker with the number of filters. Decrease in number of parameters will lead to less accuracy. To compensate, the authors propose to downsample late in the network. \n\nThe results are quite impressive. Compared to AlexNet, they achieve a 50x reduction is model size while preserving the accuracy. Their model can be further compressed with existing methods like Deep Compression which are orthogonal to this paper's approach and this can give in total of around 510x reduction while still preserving accuracy of AlexNet.\n\n$\\bf Question$: The impact on running times (specially on feed forward phase which may be more typical on embedded devices) is not clear to me. Is it certain to be reduced as well or at least be *no worse* than the baseline models? ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.07360"
    },
    "86": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/www/ShenHGDM14",
        "transcript": "Here the authors present a model which projects queries and documents into a low dimensional space, where you can fetch relevant documents by computing distance, *here cosine is used*, between the query vector and the document vectors.\n\n### Model Description\n\n#### Word Hashing Layer\nThey have used bag of tri-grams for representing words(office -> #office# -> {#of, off, ffi, fic, ice, ce#}). This is able to generalize unseen words and maps morphological variation of same words to points which are close in n-gram space.\n\n#### Context Window Vector\nThen for representing a sentence they are taking a `Window Size` around a word and appending them to form a context window vector. If we take `Window Size` = 3: \n\n(He is going to Office -> { [vec of 'he', vec of 'is', vec of 'going'], [vec of 'is', vec of 'going', vec of 'to'], [vec of 'going', vec of 'to', vec of 'Office'] }\n\n#### Convolutional Layer and Max-Pool layer\n\nRun a convolutional layer over each of the context window vector (for an intuition these are local features). Max pool over the resulting features to get global features. The output dimension is taken here to be 300.\n\n#### Semantic Layer\n\nUse a fully connected layer and project the 300-D vector to a 128-D vector.\n\n\nThey have used two different networks, one for queries and other for documents. Now for each query and document (we are given labeled documents, one of them is positive and rest are negative) they compute the cosine similarity of the 128-D output vector. And then they learn the weights of convolutional filters and the fully connected layer by maximizing conditional likelihood of positive documents. \n\nMy thinking is that they have used two different networks as their is significant difference between Query length and Document Length.\n ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2567948.2577348"
    },
    "87": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1603.01417",
        "transcript": "Dynamic Memory Network has:\n1. **Input module**: This module processes the input data about which a question is being asked into a set of vectors termed facts. This module consists of GRU over input words.\n2. **Question Module**: Representation of question as a vector. (final hidden state of the GRU over the words in the question)\n3. **Episodic Memory Module**: Retrieves the information required to answer the question from the input facts (input module). Consists of two parts\n    1. attention mechanism \n    2. memory update mechanism  \n\n  To get it more intuitive: When we see a question, we only have the question in our memory(i.e. the initial memory vector == question vector), then based on our question and previous memory we pass over the input facts and generate a contextual vector (this is the work of attention mechanism), then memory is updated again based upon the contextual vector and the previous memory, this is repeated again and again.\n4. **Answer Module**: The answer module uses the question vector and the most updated memory from 3rd module to generate answer. (a linear layer with softmax activation for single word answers, RNNs for complicated answers)\n\n**Improved DMN+**\n\nThe input module used single GRU to process the data. Two shortcomings: \n1. The GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Therfore bi-directional GRUs were used in DMN+.\n2. The supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU. In DMN+ they used sentence embeddings rather than word embeddings. And then used the GRUs to interact between the sentence embeddings(input fusion layer).\n\n**For Visual Question Answering**\n\n\nSplit the image into parts, consider them parallel to sentences in input module for text. Linear layer with tanh activation to project the regional vectors(from images) to textual feature space (for text based question answering they used positional encoding for embedding sentences). Again use bi-directional GRUs to form the facts. Now use the same process as mentioned for text based question answering.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1603.01417"
    },
    "88": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/esann/KrizhevskyH11",
        "transcript": "Here the author presents a way to retrieve similar images in O(distance) time (where distance can be termed as 100 - correlation in percentage between two images), but it uses O(2^n) memory (n is the number of bits in which we are encoding the image). Therefore this approach is independent of the size of database (Though the value of 'n' is very important, it is kind of precision measure for the correlation, if we choose very small 'n' the difference between similar images to given image can't be scored efficiently, while if we use very large 'n' the semantic similarity won't be captured efficiently enough).\n(But this is only hashing, what is peculiar about this paper? It uses semantic hashing.)\n\nFor all the images, they are fed to autoencoder as input, a code is generated as output which is used for hashing. And then for the query image, again a code is generated and k-nearest images are retrieved from the output space.\n\nBut what is autoencoder? It is an artificial neural network with layers which are not intra-connected but inter-connected. They are comprised of two parts: encoder and decoder.\n\nIn the paper n is 28 that means the autoencoder will have 28 units in the middle layer.\n\n*Training:*\n1. The encoder training is done greedily one by one (layer by layer) using the way Restricted Boltzmann Machines are trained.\n2. Then the decoder is just the inverse of the encoder layer (*Unsymmetrical auto-encoders generally give poor results.)\n3. And then they are fine tuned using back-propagation, using the input image itself for calculating the loss.\n(*Due to huge number of weights, the back-propagation training converges very slowly).\n\nIn short the encoder transforms the high dimensional input to low dimensional output. \nAnd then the decoder represents it back to high dimensional space.\n\nIn the paper this low dimensional output has been used as the code representation for the input image. (Note that these are rounded so that they are binary.) And these capture the semantic content of the image. The author has also written that this approach wont be useful if applied to words with respect to documents as words w.r.t. documents are much more semantically meaningful than pixels w.r.t. images.\n\nThen they have used one more thing to tackle translation related variance. They have taken 81 patches of each of the image (regularly spaced patches), and have applied the above mentioned algorithm to compute the hashes. (*It is a kind of convolution except for the fact that we are not summing anything, just iterating over the bags to find their representation. It won't be much effective for tackling rotational variance.)\n\nIn the array of size 2^28, for the images whose code comes to be `a1,a2..a28` the value of `a1,a2..a28` is computed in decimal representation as 'i' and the image is stored at index 'i'.\n\nNow for a query image, it is broken into 81 overlapping patches which are fed to the network and its code is computed. Then all the images at indexes whose difference is less than 3 bits are returned and given a score based on difference in bits. And then scores are summed for each of the image and then images are returned as per descending order of score.\n\nThe author has used two layer searching, where in first layer for the given input image, the output images are returned using the 28-bit codes, then the 256 bit code of input image and the previously returned output images are compared and based on the 256-bit codes a refined order is returned.\n\nThough I recommend people to study A Practical Guide to Training Restricted Boltzmann Machines for a better understanding of the learning used in the training part above.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2011-10.pdf"
    },
    "89": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MnihHGK14",
        "transcript": "The main contribution of this paper is introducing a (recurrent) visual attention model (RAM). Convolutional networks (CNs) seem to do a great job in computer vision tasks. Unfortunately, the amount of computation they require grows (at least) linearly in the size of the image. The RAM surges as an alternative that performs as well as CNs, but where the amount of computation can be controlled independently of the image size.\n\n#### What is RAM?\nA model that describes a sequential decision process of a goal-directed agent interacting with a visual environment. It involves deciding where to look in a constrained visual environment and taking decisions to maximize a reward. It uses a recurrent neural network to combine information from the past to decide its future actions.\n\n#### What do we gain?\nThe attention mechanism takes care of deciding the parts of the image that are worth looking to solve the task. Therefore, it will ignore clutter. In addition, the amount of computation can be decided independently of the image sizes. Furthermore, this could also be directly applied to variable size images as well as detecting multiple objects in one image.\n\n#### What follows?\nAn extension that may be worth exploring is whether the attention mechanism can be made differentiable. This might be already done in other papers.\n\n#### Like:\n* Can be used for analyzing videos and playing games.\nUseful in cluttered environments.\n\n#### Dislike:\n* The model is non-differentiable.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention"
    },
    "90": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/GregorDGRW15",
        "transcript": "The paper introduces a sequential variational auto-encoder that generates complex images iteratively. The authors also introduce a new spatial attention mechanism that allows the model to focus on small subsets of the image. This new approach for image generation produces images that can\u2019t be distinguished from the training data.\n\n#### What is DRAW:\nThe deep recurrent attention writer (DRAW) model has two differences with respect to other variational auto-encoders. First, the encoder and the decoder are recurrent networks. Second, it includes an attention mechanism that restricts the input region observed by the encoder and the output region observed by the decoder.\n\n#### What do we gain?\nThe resulting images are greatly improved by allowing a conditional and sequential generation. In addition, the spatial attention mechanism can be used in other contexts to solve the \u201cWhere to look?\u201d problem.\n\n#### What follows?\nA possible extension to this model would be to use a convolutional architecture in the encoder or the decoder. Although this might be less useful since we are already restricting the input of the network.\n\n#### Like:\n* As observed in the samples generated by the model, the attention mechanism works effectively by reconstructing images in a local way.\n* The attention model is fully differentiable.\n\n#### Dislike:\n* I think a better exposition of the attention mechanism would improve this paper.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/gregor15.html"
    },
    "91": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/IoffeS15",
        "transcript": "The main contribution of this paper is introducing a new transformation that the authors call Batch Normalization (BN). The need for BN comes from the fact that during the training of deep neural networks (DNNs) the distribution of each layer\u2019s input change. This phenomenon is called internal covariate shift (ICS).\n\n#### What is BN?\nNormalize each (scalar) feature independently with respect to the mean and variance of the mini batch. Scale and shift the normalized values with two new parameters (per activation) that will be learned. The BN consists of making normalization part of the model architecture.\n\n#### What do we gain?\nAccording to the author, the use of BN provides a great speed up in the training of DNNs. In particular, the gains are greater when it is combined with higher learning rates. In addition, BN works as a regularizer for the model which allows to use less dropout or less L2 normalization. Furthermore, since the distribution of the inputs is normalized, it also allows to use sigmoids as activation functions without the saturation problem.\n\n#### What follows?\nThis seems to be specially promising for training recurrent neural networks (RNNs). The vanishing and exploding gradient problems \\cite{journals/tnn/BengioSF94} have their origin in the iteration of transformation that scale up or down the activations in certain directions (eigenvectors). It seems that this regularization would be specially useful in this context since this would allow the gradient to flow more easily. When we unroll the RNNs, we usually have ultra deep networks.\n\n#### Like\n* Simple idea that seems to improve training.\n* Makes training faster.\n* Simple to implement. Probably.\n* You can be less careful with initialization.\n\n#### Dislike\n* Does not work with stochastic gradient descent (minibatch size = 1).\n* This could reduce the parallelism of the algorithm since now all the examples in a mini batch are tied.\n* Results on ensemble of networks for ImageNet makes it harder to evaluate the relevance of BN by itself. (Although they do mention the performance of a single model).",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/ioffe15.html"
    },
    "92": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/YanDMW03",
        "transcript": "In binary classification task on an imbalanced dataset, we often report *area under the curve* (AUC) of *receiver operating characteristic* (ROC) as the classifier's ability to distinguish two classes.\nIf there are $k$ errors, accuracy will be the same irrespective of how those $k$ errors are made i.e. misclassification of positive samples or misclassification of negative samples. \nAUC-ROC is a metric that treats these misclassifications asymmetrically, making it an appropriate statistic for classification tasks on imbalanced datasets. \n\nHowever, until this paper, AUC-ROC was hard to quantify and differentiate to gradient-descent over. \nThis paper approximated AUC-ROC by a Wilcoxon-Mann-Whitney statistic which counts the \"number of wins\" in all the pairwise comparisons -\n$\nU = \\frac{\\sum_{i=1}^{m}\\sum_{j=1}^{n}I(x_i, x_j)}{mn},\n$\nwhere $m$ is the total number of positive samples, $n$ is the number of negative samples, and $I(x_i, x_j)$ is $1$ if $x_i$ is ranked higher than $x_j$. \nFigure 1 in the paper shows the variance of this statistic with an increasing imbalance in the dataset, justifying the close correspondence with AUC-ROC.\n\nFurther, to make this metric smooth and differentiable, the step function of pairwise comparison is replaced by sigmoid or hinge functions. \nFurther extensions are made to apply this to multi-class classification tasks and focus on top-K predictions i.e. optimize lower-left part of AUC. \n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aaai.org/Library/ICML/2003/icml03-110.php"
    },
    "93": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1909.04630",
        "transcript": "This paper builds upon the previous work in gradient-based meta-learning methods. \nThe objective of meta-learning is to find meta-parameters ($\\theta$) which can be \"adapted\" to yield \"task-specific\" ($\\phi$) parameters.\nThus, $\\theta$ and $\\phi$ lie in the same hyperspace. \nA meta-learning problem deals with several tasks, where each task is specified by its respective training and test datasets. \nAt the inference time of gradient-based meta-learning methods, before the start of each task, one needs to perform some gradient-descent (GD) steps initialized from the meta-parameters to obtain these task-specific parameters. \nThe objective of meta-learning is to find $\\theta$, such that GD on each task's training data yields parameters that generalize well on its test data. \n\nThus, the objective function of meta-learning is the average loss on the training dataset of each task ($\\mathcal{L}_{i}(\\phi)$), where the parameters of that task ($\\phi$) are obtained by performing GD initialized from the meta-parameters ($\\theta$). \n\n\\begin{equation}\nF(\\theta) = \\frac{1}{M}\\sum_{i=1}^{M} \\mathcal{L}_i(\\phi)\n\\end{equation}\n\nIn order to backpropagate the gradients for this task-specific loss function back to the meta-parameters, one needs to backpropagate through task-specific loss function ($\\mathcal{L}_{i}$) and the GD steps (or any other optimization algorithm that was used), which were performed to yield $\\phi$.\nAs GD is a series of steps, a whole sequence of changes done on $\\theta$ need to be considered for backpropagation. \nThus, the past approaches have focused on RNN + BPTT or Truncated BPTT. \n\nHowever, the author shows that with the use of the proximal term in the task-specific optimization (also called inner optimization), one can obtain the gradients without having to consider the entire trajectory of the parameters. \nThe authors call these implicit gradients.\nThe idea is to constrain the $\\phi$ to lie closer to $\\theta$ with the help of proximal term which is similar to L2-regularization penalty term.\nDue to this constraint, one obtains an implicit equation of $\\phi$  in terms of $\\theta$ as \n\\begin{equation}\n\\phi = \\theta - \\frac{1}{\\lambda}\\nabla\\mathcal{L}_i(\\phi)\n\\end{equation}\nThis is then differentiated to obtain the implicit gradients as \n\n\\begin{equation}\n\\frac{d\\phi}{d\\theta} = \\big( \\mathbf{I} + \\frac{1}{\\lambda}\\nabla^{2} \\mathcal{L}_i(\\phi) \\big)^{-1}\n\\end{equation}\n\nand the contribution of gradients from $\\mathcal{L}_i$ is thus, \n\\begin{equation}\n \\big( \\mathbf{I} + \\frac{1}{\\lambda}\\nabla^{2} \\mathcal{L}_i(\\phi) \\big)^{-1} \\nabla \\mathcal{L}_i(\\phi)\n\\end{equation}\n\nThe hessian in the above gradients are memory expensive computations, which become infeasible in deep neural networks.\nThus, the authors approximate the above term by minimizing the quadratic formulation using conjugate gradient method which only requires Hessian-vector products (cheaply available via reverse backpropagation).\n\\begin{equation}\n\\min_{\\mathbf{w}} \\mathbf{w}^\\intercal \\big( I + \\frac{1}{\\lambda}\\nabla^{2} \\mathcal{L}_i(\\phi) \\big) \\mathbf{w} - \\mathbf{w}^\\intercal \\nabla \\mathcal{L}_i(\\phi)\n\\end{equation} \n\nThus, the paper introduces computationally cheap and constant memory gradient computation for meta-learning.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1909.04630"
    },
    "94": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/FinnAL17",
        "transcript": "## TL;DR\n\nThe paper presents a model-agnostic strategy to perform few-shot learning taking advantage of prior knowledge acquired during in multitask learning. Such prior knowledge derives from priors acquired about generalized model parameters (e.g. weights or hyperparameters) during the Model Agnostic Meta-Learning (MAML) algorithm. The strategy can be applied to any algorithm trained with gradient descent (not only neural networks) being more general and perhaps effective than transfer learning. It can loosely be referred to as \"learning to learn\".\n\n## Why this is interesting\n\n* Suitable in combination with any technique that uses gradient descent (supervised learning, reinforcement learning)\n* Interesting idea: instead of further optimize existing models for performances, search a representation that can be subsequently tuned\n* when only a few and diverse data is available, multiple tasks can be defined to harness the ability of the meta-model to learn while preserving generalization (see Experiments)\n\n## Details\n\nThe key idea is performing the meta-learner update on a different data batch with respect to the parameters update(s) done for a single task. This leads   (formally) the same update procedure both for the learning and meta-learning phases of the algorithm (see Figure below) and provides a general framework for MAML. \n\nhttps://i.imgur.com/xq1wCai.png\nimage from [lilianweng post](https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html)\n\nAs [clearly worked out here](https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html), the method requires to compute the second derivatives for the outer-loop update. Surprisingly enough, omitting them and performing a first-order MAML does not sensibly affect the results according to the reported experiment. It is hypothesized that this is because ReLU networks are almost linear (and hence depends upon the actual network structure). \n\n## Experiments\n### Supervised learning\n1. regression from input to output of a sine wave. Amplitude and phase are varied among tasks. It is shown that MAML leads to good results and can generalize better than fine-tuning in the experiment conditions (\"due to the often contradictory outputs on pre-training tasks\". See Figure 2 in the paper)\n2. few-shot image classification on Omniglot and MiniImageNet datasets (N, unseen, multiclass trained with K different instances). SOTA performance on the first dataset, and better-than SOTA on the second one where first-order MAML is also tested. \n\n### Reinforcement learning\n1. 2D navigation: a single agent point must move to different positions (tasks). The model trained with MAML shows better performances, for the same number of gradient steps (Figure 4)\n2. Locomotion: two simulated robots are provided with a set of tasks. MAML can learn a model that adapts much faster to new tasks (this is a case where pretraining is detrimental)\n \n## Related work and resources\n* official [gthub repo](https://github.com/cbfinn/maml)\n* [videos](https://sites.google.com/view/maml) of the learned policies in MAML paper\n* paper appendix: the part on the multi-task baseline is interesting\n* [how to train your MAML](https://arxiv.org/abs/1810.09502): discusses various modifications to MAML to stabilize and improve performances\n* [Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML](https://arxiv.org/abs/1909.09157)\n",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v70/finn17a.html"
    },
    "95": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1312.6211",
        "transcript": "The paper discusses and empirically investigates by empirical testing the effect of \"catastrophic forgetting\" (**CF**), i.e. the inability of a model to perform a task it was previously trained to perform if retrained to perform a second task. \n\nAn illuminating example is what happens in ML systems with convex objectives: regardless of the initialization (i.e. of what was learnt by doing the first task), the training of the second task will always end in the global minimum, thus totally \"forgetting\" the first one. \n\nNeuroscientific evidence (and common sense) suggest that the outcome of the experiment is deeply influenced by the similarity of the tasks involved. Namely, if (i) the two tasks are *functionally identical but input is presented in a different format* or if (ii)  *tasks are similar* and the third case for (iii) *dissimilar tasks*. \n\nRelevant examples may be provided respectively by (i) performing the same image classification task starting from two different image representations as RGB or HSL, (ii) performing image classification tasks with semantically similar as classifying two similar animals and (iii) performing a text classification followed by image classification. \n\nThe problem is investigated by an empirical study covering two methods of training (\"SGD\" and \"dropout\") combined with 4 activations functions (logistic sigmoid, RELU, LWTA, Maxout). A random search is carried out on these parameters. \n\nFrom a practitioner's point of view, it is interesting to note that dropout has been set to 0.5 in hidden units and 0.2 in the visible one since this is a reasonably well-known parameter. \n\n## Why the paper is important\nIt is apparently the first to provide a systematic empirical analysis of CF. Establishes a framework and baselines to face the problem.\n\n## Key conclusions, takeaways and modelling remarks\n* dropout helps in preventing CF\n* dropout seems to increase the optimal model size with respect to the model without dropout \n* choice of activation function has a less consistent effect than dropout\\no dropout choice\n* dissimilar task experiment provides a  notable exception of then dissimilar task experiment\n* the previous hypothesis that LWTA activation is particularly resistant to CF is rejected  (even if it performs best in the new task in the dissimilar task pair the behaviour is inconsistent)\n* choice of activation function should always be cross-validated\n* If computational resources are insufficient for cross-validation the combination dropout + maxout activation function is recommended.\n\n\n\n\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1312.6211"
    },
    "96": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1809.01442",
        "transcript": "_Disclaimer: I'm the first author of this paper._\n\nThe code for this paper can be found at https://github.com/fabioperez/skin-data-augmentation.\n\nIn this work, we wanted to compare different data augmentation scenarios for skin lesion analysis. We tried 13 scenarios, including commonly used augmentation techniques (color and geometry transformations), unusual ones (random erasing, elastic transformation, and a novel lesion mix to simulate collision lesions), and a combination of those. \n\nExamples of the augmentation scenarios:\n\nhttps://i.imgur.com/TpgxzLZ.png\n\na) no augmentation\n\nb) color (saturation, contrast, and brightness)\n\nc) color (saturation, contrast, brightness, and hue)\n\nd) affine (rotation, shear, scaling)\n\ne) random flips\n\nf) random crops\n\ng) random erasing\n\nh) elastic\n\ni) lesion mix\n\nj) basic set (f, d, e, c)\n\nk) basic set + erasing (f, g, d, e, c)\n\nl) basic set + elastic (f, d, h, e, c)\n\nm) basic set + mix (i, f, d, e, c)\n\n---\n\nWe used the ISIC 2017 Challenge dataset (2000 training images, 150 validation images, and 600 test images).\n\nWe tried three network architectures: Inception-v4, ResNet-152, and DenseNet-161.\n\nWe also compared different test-time data augmentation methods: a) no augmentation; b) 144-crops; c) same data augmentation as training (64 augmented copies of the original image). Final prediction was the average of all augmented predictions.\n\n\n## Results\n\nhttps://i.imgur.com/WK5VKUf.png\n\n* Basic set (combination of commonly used augmentations) is the best scenario.\n* Data augmentation at test-time is very beneficial.\n* Elastic is better than no augmentation, but when compared incorporated to the basic set, decreases the performance.\n* The best result was better than the winner of the challenge in 2017, without using ensembling.\n* Test data augmentation is very similar with 144-crop, but takes less images during prediction (64 vs 144), so it's faster.\n\n# Impact of data augmentation on dataset sizes\n\nWe also used the basic set scenarios on different dataset sizes by sampling random subsets of the original dataset, with sizes 1500, 1000, 500, 250 and 125.\n\nhttps://i.imgur.com/m3Ut6ht.png\n\n## Results\n\n* Using data augmentation can be better than using more data (but you should always use more data since the model can benefit from both). For instance, using 500 images with data augmentation on training and test for Inception is better than training with no data augmentation with 2000 images.\n* ResNet and DenseNet works better than Inception for less data.\n* Test-time data augmentation is always better than not augmenting on test-time.\n* Using data augmentation on train only was worse than not augmenting at all in some cases.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1809.01442"
    },
    "97": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.10217",
        "transcript": "Authors investigated why humans play some video games better than machines. That is the case for games that do not have continuous rewards (e.g., scores). They experimented with a game -- inspired by _Montezuma's Revenge_ -- in which the player has to climb stairs, collect keys and jump over enemies. RL algorithms can only know if they succeed if they finish the game, as there is no rewards during the gameplay, so they tend to do much worse than humans in these games.\n\nTo compare between humans and machines, they set up RL algorithms and recruite players from Amazon Mechanical Turk. Humans did much better than machines for the original game setup. However, authors wanted to check the impact of semantics and prior knowledge on humans performance. They set up scenarios with different levels of reduced semantic information, as shown in Figure 2.\n\nhttps://i.imgur.com/e0Dq1WO.png\n\nThis is what the game originally looked like:\n\nhttps://rach0012.github.io/humanRL_website/main.gif\n\nAnd this is the version with lesser semantic clues:\n\nhttps://rach0012.github.io/humanRL_website/random2.gif\n\nYou can try yourself in the [paper's website](https://rach0012.github.io/humanRL_website/).\n\nNot surprisingly, humans took much more time to complete the game in scenarios with less semantic information, indicating that humans strongly rely on prior knowledge to play video games.\n\nThe authors argue that this prior knowledge should also be somehow included into RL algorithms in order to move their efficiency towards the human level.\n\n## Additional reading\n\n[Why humans learn faster than AI\u2014for now](https://www.technologyreview.com/s/610434/why-humans-learn-faster-than-ai-for-now/).\n\n[OpenReview submission](https://openreview.net/forum?id=Hk91SGWR-)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.10217v3"
    },
    "98": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeGDG17",
        "transcript": "####  Mask R-CNN framework for instance segmentation\n### Goal:\n* classify individual objects\n*  localize each using a bounding box, \n* semantic segmentation\nhttps://i.imgur.com/XfBRa5O.png\n*  classify each pixel into a fixed set of categories without differentiating object instances.\n* extends Faster R-CNN  by adding a branch for predicting segmentation masks on each Region of Interest (RoI), in parallel with the existing branch for classification and bounding box regression.\n*  FCN applied to each RoI, predicting a segmentation mask in a pixel-to-pixel manner\n1. RoIAlign:\n * Used to fix the misalignment that faithfully preserves exact spatial locations\n * improves mask accuracy by relative 10% to 50%, fast speed\n2. Decouple mask and class prediction:\n * predict a binary mask for each class independently, without competition among classes\n\n History:\n* RCNN: The Region-based CNN (R-CNN) approach to bounding-box object detection\n* Fast RCNN:  Speeding up and Simplifying R-CNN\n *  RoI (Region of Interest) Pooling\n *  jointly train the CNN, classifier, and bounding box regressor in a single model\n*  Faster R-CNN - Speeding Up Region Proposal\n * reuse the same CNN results for region proposals instead of running a separate selective search algorithm it can be done by Region Proposal Network\n * only one CNN needs to be trained \n\n Related Work\n*  Instance Segmentation: \u201cfully convolutional instance segmentation\u201d (FCIS)\n* Faster R-CNN: * Region Proposal Network (RPN), proposes candidate object bounding boxes\n* Fast R-CNN [12], extracts features using RoIPool from each candidate box and performs classification and bounding-box regression\n*  Mask R-CNN: Mask R-CNN adopts the same two-stage of Faster RCNN And has third stage i.e binary mask for each RoI\n* Mask Representation: pixel to pixel representation of image done by RoIAlign layer (7X7)\n#### Network Architecture\n*  convolutional backbone architecture used for feature extraction over an entire image (ResNet-50-C4, FPN)\n*  network head for bounding-box recognition (classification and regression) and mask prediction\nhttps://i.imgur.com/pUvKdmx.png\n#### Training:\n* Images resized:800 pixel                                     \n* mini-batch : 2 images per GPU\n* N : 64\n* train: on 8 GPUs for 160k iterations\n* learning : 0.02\n*  train images: 80K\n* val images: 35K\n* minival:5K\nhttps://i.imgur.com/6ZLpewi.png\nhttps://i.imgur.com/5o3um0Y.png",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.06870"
    },
    "99": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/GirshickDDM14",
        "transcript": "# Object detection system overview.\n\nhttps://i.imgur.com/vd2YUy3.png\n \n1.\ttakes an input image,\n2.\textracts around 2000 bottom-up region proposals, \n3.\tcomputes features for each proposal using a large convolutional neural network (CNN), and then\n4.\tclassifies each region using class-specific linear SVMs.\n* R-CNN achieves a mean average precision (mAP) of 53.7% on PASCAL VOC 2010.\n* On the 200-class ILSVRC2013 detection dataset, R-CNN\u2019s mAP is 31.4%, a large improvement over OverFeat , which had the previous best result at 24.3%.\n\n## There is a 2 challenges faced in object detection \n1.\tlocalization problem\n2.\tlabeling the data\n\n1 localization problem :\n*  One approach frames localization as a regression problem.  they report a mAP of 30.5% on VOC 2007 compared to the 58.5% achieved by our method.\n* An alternative is to build a sliding-window detector.  considered adopting a sliding-window approach increases the number of convolutional layers to 5, have very large receptive fields (195 x 195 pixels) and strides (32x32 pixels) in the input image, which makes precise localization within the sliding-window paradigm.\n\n2 labeling the data:\n* The conventional solution to this problem is to use unsupervised pre-training, followed by supervise fine-tuning \n* supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL),\n* fine-tuning for detection improves mAP performance by 8 percentage points. \n* Stochastic gradient descent via back propagation was used to  effective for training convolutional neural networks (CNNs)\n \n##  Object detection with R-CNN\nThis  system consists of three modules\n* The first generates category-independent region proposals. These proposals define the set of candidate detections available to our detector.\n*  The second module is a large convolutional neural network that extracts a fixed-length feature vector from each region.\n* The third module is a set of class specific linear SVMs.\n\nModule design\n\n1 Region proposals \n* which detect mitotic cells by applying a CNN to regularly-spaced square crops.\n* use selective search method in fast mode (Capture All Scales, Diversification, Fast to Compute).\n* the time spent computing region proposals and features (13s/image on a GPU or 53s/image on a CPU)\n\n2 Feature extraction.\n* extract a 4096-dimensional feature vector from each region proposal using the Caffe  implementation of the CNN \n* Features are computed by forward propagating a mean-subtracted 227x227 RGB image through five convolutional layers and two fully connected layers.\n* warp all pixels in a tight bounding box around it to the required size\n* The feature matrix is typically 2000x4096 \n\n3 Test time detection\n* At test time, run selective search on the test image to extract around 2000 region proposals (we use selective search\u2019s \u201cfast mode\u201d in all experiments).\n*  warp each proposal and forward propagate it through the CNN in order to compute features. Then, for each class, we score each extracted feature vector using the SVM trained for that class.\n* Given all scored regions in an image, we apply a greedy non-maximum suppression (for each class independently) that rejects a region if it has an intersection-over union (IoU) overlap with a higher scoring selected region larger than a learned threshold.\n## Training\n\n1 Supervised pre-training:\n * pre-trained the CNN on a large auxiliary dataset (ILSVRC2012 classification)  using image-level annotations only (bounding box labels are not available for this data)\n\n2  Domain-specific fine-tuning.\n* use the stochastic gradient descent (SGD) training of the CNN parameters using only warped region proposals with  learning rate of 0.001.\n\n3  Object category classifiers.\n*   use intersection-over union (IoU) overlap threshold method  to label a region with The overlap threshold of 0.3.\n* Once features are extracted and training labels are applied, we optimize one linear SVM per class.\n* adopt the standard hard negative mining method to fit large training data in memory.\n\n### Results on PASCAL VOC 201012\n\n1  VOC 2010\n* compared against four strong baselines including SegDPM, DPM, UVA, Regionlets. \n* Achieve a large improvement in mAP, from 35.1% to 53.7% mAP,  while also being much faster\n https://i.imgur.com/0dGX9b7.png\n2 ILSVRC2013 detection.\n* ran R-CNN on the 200-class ILSVRC2013 detection dataset\n* R-CNN achieves a mAP of 31.4%\nhttps://i.imgur.com/GFbULx3.png\n#### Performance layer-by-layer, without fine-tuning\n1 pool5 layer \n* which is the max pooled output of the network\u2019s fifth and final convolutional layer.\n*The pool5 feature map is 6 x6 x 256 = 9216 dimensional\n* each pool5 unit has a receptive field of 195x195 pixels in the original 227x227 pixel input\n\n2 Layer fc6\n* fully connected to pool5\n*  it multiplies a 4096x9216 weight matrix by the pool5 feature map (reshaped as a 9216-dimensional vector) and then adds a vector of biases\n\n3 Layer fc7\n* It is implemented by multiplying the features computed by fc6 by a 4096 x 4096 weight matrix, and similarly adding a vector of biases and applying half-wave rectification\n#### Performance layer-by-layer, with fine-tuning\n* CNN\u2019s parameters  fine-tuned on PASCAL.\n* fine-tuning increases mAP by 8.0 % points to 54.2%\n\n### Network architectures\n* 16-layer deep network, consisting of 13 layers of 3 _ 3 convolution kernels, with five max pooling layers interspersed, and topped with three fully-connected layers. We refer to this network as \u201cO-Net\u201d for OxfordNet and the baseline as \u201cT-Net\u201d for TorontoNet.\n* RCNN with O-Net substantially outperforms R-CNN with TNet, increasing mAP from 58.5% to 66.0%\n* drawback in terms of compute time, with in terms of compute time, with than T-Net.\n\n1 The ILSVRC2013 detection dataset\n* dataset is split into three sets: train (395,918), val (20,121), and test (40,152) \n\n#### CNN features for segmentation.\n* full R-CNN: The first strategy (full) ignores the re region\u2019s shape and computes CNN features directly on the warped window. Two regions might have very similar bounding boxes while having very little overlap. \n* fg R-CNN: the second strategy (fg) computes CNN features only on a region\u2019s foreground mask. We replace the background with the mean input so that background regions are zero after mean subtraction.\n* full+fg R-CNN: The third strategy (full+fg) simply concatenates the full and fg features\n https://i.imgur.com/n1bhmKo.png\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/CVPR.2014.81"
    },
    "100": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1904-07846",
        "transcript": "# Overview\n\nThis paper presents a novel way to align frames in videos of similar actions temporally in a self-supervised setting.  To do so, they leverage the concept of cycle-consistency. They introduce two formulations of cycle-consistency which are differentiable and solvable using standard gradient descent approaches. They name their method Temporal Cycle Consistency (TCC). They introduce a dataset that they use to evaluate their approach and show that their learned embeddings allow for few shot classification of actions from videos. \n\nFigure 1 shows the basic idea of what the paper aims to achieve. Given two video sequences, they wish to map the frames that are closest to each other in both sequences. The beauty here is that this \"closeness\" measure is defined by nearest neighbors in an embedding space, so the network has to figure out for itself what being close to another frame means. The cycle-consistency is what makes the network converge towards meaningful \"closeness\".\n\n![image](https://user-images.githubusercontent.com/18450628/63888190-68b8a500-c9ac-11e9-9da7-925b72c731c3.png)\n\n# Cycle Consistency\n\nIntuitively, the concept of cycle-consistency can be thought of like this: suppose you have an application that allows you to take the photo of a user X and  increase their apparent age via some transformation F and decrease their age via some transformation G. The process is cycle-consistent if you can age your image, then using the aged image, \"de-age\" it and obtain something close to the original image you took; i.e. F(G(X))  ~= X.\n\nIn this paper, cycle-consistency is defined in the context of nearest-neighbors in the embedding space. Suppose you have two video sequences, U and V. Take a frame embedding from U, u_i, and find its nearest neighbor in V's embedding space, v_j. Now take the frame embedding v_j and find its closest frame embedding in U, u_k, using a nearest-neighbors approach. If k=i, the frames are cycle consistent. Otherwise, they are not. The authors seek to maximize cycle consistency across frames.\n \n# Differentiable Cycle Consistency\n\nThe authors present two differentiable methods for cycle-consistency; cycle-back classification and cycle-back regression.\n\nIn order to make their cycle-consistency formulation differentiable, the authors use the concept of soft nearest neighbor:\n\n![image](https://user-images.githubusercontent.com/18450628/63891061-5477a680-c9b2-11e9-9e4f-55e11d81787d.png)\n\n## cycle-back classification\n\nOnce the soft nearest neighbor v~ is computed, the euclidean distance between v~ and all u_k  is computed for a total of N frames (assume N frames in U) in a logit vector x_k and softmaxed to a prediction \u0177 = softmax(x): \n\n![image](https://user-images.githubusercontent.com/18450628/63891450-38c0d000-c9b3-11e9-89e9-d257be3fd175.png). \n\n![image](https://user-images.githubusercontent.com/18450628/63891746-e92ed400-c9b3-11e9-982c-078ebd1d747e.png)\n\n\nNote the clever use of the negative, which will ensure the softmax selects for the highest distance. The ground truth label is a one-hot vector of size 1xN where position i is set to 1 and all others are set to 0. Cross-entropy is then used to compute the loss.\n\n## cycle-back regression\n\nThe concept is very similar to cycle-back classification up to the soft nearest neighbor calculation. However the similarity metric of v~ back to u_k is not computed using euclidean distance but instead soft nearest neighbor again:\n\n![image](https://user-images.githubusercontent.com/18450628/63893231-9bb46600-c9b7-11e9-9145-0c13e8ede5e6.png)\n\nThe idea is that they want to penalize the network less for \"close enough\" guesses. This is done by imposing a gaussian prior on beta centered around the actual closest neighbor i.\n\n![image](https://user-images.githubusercontent.com/18450628/63893439-29905100-c9b8-11e9-81fe-fab238021c6d.png)\n\nThe following figure summarizes the pipeline:\n\n![image](https://user-images.githubusercontent.com/18450628/63896278-9f4beb00-c9bf-11e9-8be1-5f1ad67199c7.png)\n\n# Datasets\n\nAll training is done in a self-supervised fashion. To evaluate their methods, they annotate the Pouring dataset, which they introduce and the Penn Action dataset. To annotate the datasets, they limit labels to specific events and phases between phases \n\n![image](https://user-images.githubusercontent.com/18450628/63894846-affa6200-c9bb-11e9-8919-2f2cdf720a88.png)\n\n# Model\n\nThe network consists of two parts: an encoder network and an embedder network.\n\n## Encoder\n\nThey experiment with two encoders: \n\n* ResNet-50 pretrained on imagenet. They use conv_4 layer's output which is 14x14x1024 as the encoder scheme.\n\n* A Vgg-like model from scratch whose encoder output is 14x14x512.\n\n## Embedder\n\nThey then fuse the k following frame encodings along the time dimension, and feed it to an embedder network which uses 3D Convolutions and 3D pooling to reduce it to a 1x128 feature vector. They find that k=2 works pretty well.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1904.07846"
    },
    "101": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/XuBKCCSZB15",
        "transcript": "# Summary\n\nThe authors present a way to generate captions describing the content of images using attention-based mechanisms. They present two ways of training the network, one via standard backpropagation techniques and another using stochastic processes. They also show how their model can selectively \"focus\" on the relevant parts of an image to generate appropriate captions, as shown in the classic example of the famous woman throwing a frisbee. Finally, they validate their model on Flicker8k, Flicker30k and MSCOCO.\n\n![image](https://user-images.githubusercontent.com/18450628/61397054-10639300-a897-11e9-8b4a-f4cd804c3229.png)\n\n# Model\n\nAt a very high level, the model takes as input an image I and returns a caption generated from a pre-defined vocabulary:\n\n![image](https://user-images.githubusercontent.com/18450628/61398513-20c93d00-a89a-11e9-8e93-72ccf7a61be1.png)\n\nA high-level overview of the model is presented in Figure 1:\n\n![image](https://user-images.githubusercontent.com/18450628/61398365-de076500-a899-11e9-8413-55ec755f0f83.png)\n\n## Visual extractor\n\nA CNN is used to extract features from the image. The authors experimented with VGG-19 pretrained on ImageNet and not finetuned. They use the features from the last convolutional layer as their representations. Starting with images of 224x224, the last CNN feature map has shape 14x14x512, which they flatten along width and height to obtain a vector representation of 196x512. These 512 vectors are used as inputs to the language model.\n\n## Sentence generation\n\nAn LSTM network is used to generate a sequence of words from a fixed vocabulary of size L. As input, a weighted sum based on attention values of the vectors of the flattened image features is used. The previous word is also fed as input to the LSTM. The hidden layer from the previous timestep as well as the layers from the CNN a_i are fed through an MLP + softmax layer and used to generate attention values for each flattened image feature vector that sum to one. \n\n![image](https://user-images.githubusercontent.com/18450628/61462206-41e46900-a940-11e9-991d-e3a9e4b98837.png)\n\n![image](https://user-images.githubusercontent.com/18450628/61462544-c9ca7300-a940-11e9-8c31-dbf85bf8301f.png)\n\n\n\n\nThe authors propose two ways to compute phi, i.e. the attention, which they refer to as \"soft attention\" and \"hard attention\". These will be covered in a later section.\n\nThe output of the LSTM, z, is then fed to a deep network to generate the next word. This is detailed in the following figure.\n\n![image](https://user-images.githubusercontent.com/18450628/61408594-6132b600-a8ae-11e9-894c-392396e299b0.png)\n\n\n## Attention\n\nThe paper proposes two methods of attention, a \"soft\" attention and a \"hard\" attention.\n\n### Soft attention\n\nSoft attention is the most intuitive one and is relatively straight forward. In order to compute the vector representing the image as input to the LSTM, **z**, the expectation of the context vector is computed by using a weighted average scheme:\n\n![image](https://user-images.githubusercontent.com/18450628/61408939-34cb6980-a8af-11e9-989e-24308be3ed3c.png)\n\nwhere alpha are the attention weights and a_i are the vectors of the feature representation.\n\nTo ensure that all image features are used somewhat equally, a regularization term is added to the loss function:\n\n![image](https://user-images.githubusercontent.com/18450628/61412057-d2c23280-a8b5-11e9-9d9c-7f35edc650ef.png)\n\nThis ensures that the image feature vectors over time sum to 1 as closely as possible and that no part of the image is ignored.\n\n### Hard attention\n\nThe authors propose an alternative method to calculate attention. Each attention parameter is treated as an intermediate latent variables that can be represented in one-hot encoding, i.e. on or off. To do so, they use a multinoulli distribution parametrized by alpha, the softmax output of f_att. They show how they can approximate the gradient using monte-carlo methods:\n\n![image](https://user-images.githubusercontent.com/18450628/61413158-d1463980-a8b8-11e9-8aef-b2a9d9bb6bad.png)\n\nRefer to the paper for more mathemagical details. Finally, they use soft attention with probability 0.5 when using hard attention.\n\n## Visualizing features\n\nOne of the contributions of this work is showing what the network is \"attending\" to. To do so, the authors use the final layer of VGG-19, which consists of 14x14x512 features, upsample the resulting filters to the original image size of 224x224 and use a Gaussian blur to recreate the receptive field.\n\n\n\n## Results\n\nThe authors evaluate their methods on 3 caption datasets, i.e. Flicker8k, Flicker30k and MSCOCO.  For all our experiments, they used a fixed vocabulary size of 10,000. They report both BLEU and METEOR scores on the task.\n\nAs can be seen in the following figure, both the soft and hard attention mechanisms beat all state of the art methods at the time of publishing. Hard attention outperformed soft attention most of the time.\n\n![image](https://user-images.githubusercontent.com/18450628/61412281-77dd0b00-a8b6-11e9-882f-73e86638bc9d.png)\n\n# Comments\n\nCool paper which lead the way in terms of combining text and images and using attention mechanisms. They show an interesting way to visualize what the network is attending to, although it was not clear to me why they should expect the final layer of the CNN to be showing that in the first place since they did not finetune on the datasets they were training on. I would expect that to mean that their method would work best on datasets most \"similar\" to ImageNet. \n\nTheir hard attention mechanism seems a lot more complicated than the soft attention mechanism and it isn't always clear that it is much better other than it offers a stronger regularization and a type of dropout.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/xuc15.html"
    },
    "102": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/cvpr.2018.00636",
        "transcript": "# Summary\n\nThis paper presents state-of-the-art methods for both caption generation of images and visual question answering (VQA). The authors build on previous methods by adding what they call a \"bottom-up\" approach to previous \"top-down\" attention mechanisms. They show that using their approach they obtain SOTA on both Image captioning (MSCOCO) and the Visual Question and Answering (2017 VQA challenge). They propose a specific network configurations for each. Their biggest contribution is using Faster-R-CNN to retrieve the \"important\" parts of an image to focus on in both models.\n\n## Top Down\n\nUp until this paper, the traditional approach was to use a \"top-down\" approach, in which the last feature map layer of a CNN is used to obtain a latent representation of the given input image. These features, along with the context of the caption being generated, were used to generate attention weights that were used to predict the next sequence in the context of caption generation. The network would learn to focus its attention on regions of the feature map that matters most. This is the approach used in previous SOTA methods like [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044).\n\n## Bottom-up\n\nThe authors argue that the feature map of a CNN is too generic and can be thought of operating on a uniform, grid-like feature map. In other words, there is no particular reason to think that the feature map of generated by a CNN would give optimal regions to attend to. Also, carefully choosing the dimensions of the feature map can be very arbitrary.\n\nIn order to fix this, the authors propose combining object detection methods in a *bottom-up* approach. To do so, the authors propose using Faster-R-CNN to identify regions of interest in an image. Given an input image, Faster-R-CNN will identify bounding boxes of the image that likely correspond to objects of a given category and simultaneously compute a feature vector of that bounding box. Figure 1 shows the difference between the Bottom-up and Top-Down approach.\n\n![image](https://user-images.githubusercontent.com/18450628/61817263-2683cd00-ae1c-11e9-971a-d3b531dbbd98.png)\n\n## Combining the two\n\nIn this paper, the authors suggest using the bottom-up approach to compute the salient regions of the image the network should focus on using Faster-R-CNN. FRCNN is carefully pretrained on both imagenet and the Visual Genome dataset. It is then frozen and only used to generate bounding boxes of regions with high confidence of being of interest. The top-down approach is then used on the features obtained from the bottom-up approach. In order to \"enhance\" the FRCNN performance, they initialize their FRCNN with a ResNet-101 pre-trained on imagenet. They train their FRCNN on the Visual Genome dataset, adding attributes to the loss function that are available from the Visual Genome dataset, attributes such as color (black, white, gold etc.),  state (open, close, dark, bright, etc.). A sample of FRCNN outputs are shown in figure 2. It is important to stress that only the feature representations and not the actual outputs (i.e. not the labels) are used in their model.\n\n![image](https://user-images.githubusercontent.com/18450628/61817487-aca01380-ae1c-11e9-90fa-134033b95bb0.png)\n\n## Caption Generation\n\nFigure 3 provides a high-level overview of the model being used for caption generation for images. The image is first passed through FRCNN which produces a set of image features *V*. In their specific implementation, *V* consists of *k* vectors of size 1x2048. Their model consists of two LSTM blocks, one for attention and the other for language generation.\n\n![image](https://user-images.githubusercontent.com/18450628/61818488-effb8180-ae1e-11e9-8ae4-14355115429a.png)\n\nThe first block of their model is a Top-Down Attention LSTM layer. It takes as input the mean-pooled features *V* , i.e. 1/k*sum(v_i), concatenated with the previous timestep's hidden representation of the language LSTM as well as the word embedding of the previously generated word. The word embedding is learned and not pretrained. \n\nThe output of the first LSTM is used to compute the attention for each vector using an MLP and softmax:\n\n![image](https://user-images.githubusercontent.com/18450628/61819982-21298100-ae22-11e9-80a9-99640896413d.png)\n\nThe attention weighted image feature is then used as an input to the language LSTM model, concatenated with the output from the top-down Attention LSTM and a softmax is used to predict the next word in the sequence. The loss function seeks to minimize the cross-entropy of the generated sentence. \n\n## VQA Model\n\nThe VQA task differs to the image generation in that a text-based question accompanies an input image and the network must produce an answer. The VQA model proposed is different to that of the caption generation model previously described, however they both use the same bottom-up approach to generate the feature vectors of the image based on the FRCNN architecture. A high-level overview of the architecture for the VQA model is presented in Figure 4.\n\n![image](https://user-images.githubusercontent.com/18450628/61821988-8da67f00-ae26-11e9-8456-3c9e5ec60787.png)\n\nEach word from the question is converted to a learned word embedding which is used as input to a GRU. The number of words for each question is limited to 14 for computational efficiency. The output from the GRU is concatenated with each of the *k* image features, and attention weights are computed for each *k*th feature using an MLP and softmax, similar to what is done in the attention for caption generation. The weighted sum of the feature vectors is then passed through an linear layer such that its shape is compatible with the gru output, and the Hadamard product (element-wise product) is computed over the GRU output and attention-weighted image feature representation. Finally, a tanh non-linear activation is used. This results in a \"gated tanh\", which have been shown empirically to outperform both ReLU and tanh. Finally, a softmax probability distribution is generated at the output which selects a candidate answer among all possible candidate answers.\n\n## Results and experiments\n\n### Resnet Baseline\n\nTo demonstrate that their contribution of bottom-up mechanism actually improves on results, the authors use a ResNet trained on imagenet as a baseline for generating the image feature vectors (they resize the final CNN layers using bilinear interpolation when needed). They consistently obtain better results when using the bottom-up approach over the ResNet approach in both caption generation and VQA.\n\n## MSCOCO\n\nThe authors demonstrate that they outperform all results on all metrics on the MSCOCO test server.\n\n![image](https://user-images.githubusercontent.com/18450628/61824157-4f5f8e80-ae2b-11e9-8d90-657db453e26e.png)\n\nThey also show how using the bottom-up approach over ResNet consistently scores them higher on detecting instances of objects, attributes, relations, etc:\n\n![image](https://user-images.githubusercontent.com/18450628/61824238-7fa72d00-ae2b-11e9-81b3-b5a7f80153f3.png)\n\nThe authors, like their predecessors, insist on demonstrating their network's frisbee ability:\n\n![image](https://user-images.githubusercontent.com/18450628/61824344-bed57e00-ae2b-11e9-87cd-597568587e1d.png)\n\n## VQA Results\n\nThey also demonstrate that the addition of bottom-up attention improves results over a ResNet baseline. \n\n![image](https://user-images.githubusercontent.com/18450628/61824500-28ee2300-ae2c-11e9-9016-2120a91917e4.png)\n\nThey also show that their model outperformed all other submissions on the VQA submission. They mention using an ensemble of 30 models for their submission. \n\n![image](https://user-images.githubusercontent.com/18450628/61824634-83877f00-ae2c-11e9-8d84-9589e0ea2be2.png)\n\n\nA sample image of what is attended in an image given a proper answer is shown in figure 6.\n\n![image](https://user-images.githubusercontent.com/18450628/61824608-736f9f80-ae2c-11e9-9d4e-8cb6bd0a1a92.png)\n\n\n# Comments\n\nThe authors introduce a new way to select portions of the image on which to focus attention. The idea is very original and came at a time when object detection was making significant progress (i.e. FRCNN).\n\nA few comments:\n\n* This method might not generalize well to other types of data. It requires pre-training on larger datasets (visual genome, imagenet, etc.) which consist of categories that overlap with both the MSCOCO and VQA datasets (i.e. cars, people, etc.). It would be interesting to see an end-to-end model that does not rely on pre-training on other similar datasets.\n\n* No insight is given to the computational complexity nor to the inference time or training time. I imagine that FRCNN is resource intensive, and having to do a forward pass of FRCNN for every pass of the network must be a computational bottleneck. Not to mention that they ensembled 30 of them!",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/cvpr.2018.00636"
    },
    "103": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/prl/BailoRJPBK18",
        "transcript": "Keypoint detection is an important step in various tasks such as SLAM, panorama stitching, camera calibration, and more. Efficient keypoint detectors, FAST (Features from Accelerated and Segments Test) for example, would detect keypoints where a relatively high brightness change is observed in relation to surrounding pixels. Most probably, the keypoints would be located on edges, as shown below:\nhttps://i.imgur.com/ylC4BM3.jpg\nLet's consider another image shown below. Here, while the detector is capable of detecting many keypoints, they are mostly located on trees (see subfigure (a) below). This causes redundancy and this paper focuses on solving it by selecting locally strong keypoints that are well distributed all over the image (subfigure (c)).\n\nhttps://i.imgur.com/1MqZhmT.png\n\n\nThe algorithm requires input keypoints to be sorted in decreasing order of strength. The keypoints are then processed in that order and points that fall within the suppression range of a current keypoint are removed from the consideration. The process is repeated for the next unsuppressed keypoint in the sorted order. The process continues until no points remain. If the number of filtered points is not close enough to what we require, the suppression range is modified and the process is repeated. The suppression range is modified using binary search.\n\nhttps://i.imgur.com/qryscZP.png\n\nBinary search requires lower and upper bounds to operate. Naive initialization would be setting lower bound to 1 pixel, while upper bound - image width or height. On the other hand, this paper proposes better initialization of the suppression range that is dependent on image height $H_I$, width $W_I$, number of input keypoints $n$ as well as the number of output keypoints $m$.\n* Upper bound: $\\frac{H_I + W_I + 2m - \\sqrt{\\Delta}}{2m - 1}$ (see paper for more details)\n* Lower bound: $\\frac{1}{2}\\sqrt{\\frac{n}{m}}$\n\nThis initializing helps to decrease the number of iterations to convergence by a factor of three:\n\nhttps://i.imgur.com/7NCpgbi.png\n\nHomogenous location of keypoints is beneficial for SLAM algorithm and reduce translational and rotational errors compared to naive filtering when evaluated on KITTI:\n\nhttps://i.imgur.com/4wq0kLK.png\n\nOverall, this paper proposes a fast, efficient, and effective method to post-process noisy and redundant keypoint detections with a clear benefit to SLAM. The codes are publically available in multiple languages: C++, Python, Java, and Matlab. See https://github.com/BAILOOL/ANMS-Codes",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1016/j.patrec.2018.02.020"
    },
    "104": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/tvcg.2019.2893247",
        "transcript": "This paper proposes an approach to measure motion similarity between human-human and human-object interaction. The authors claim that human activities are usually defined by the interaction between individual characters, such as a high-five interaction.\n\nAs the interaction datasets are not available authors provide multiple small-scale interaction datasets:\nhttps://i.imgur.com/P815TYu.png\nwhere:\n- 2C = a Character-Character (2C) database using kick-boxing motions\n- CRC = Character-Retargeted Character where the size of characters is adjusted while maintaining the nature of the interaction\nhttps://i.imgur.com/XX1WNpO.png\n- HOI = Human-Object Interaction where a Chair is used as an object\nhttps://i.imgur.com/Z6cxd7R.png\n- 2PB = 2 People Boxing \nhttps://i.imgur.com/yUxmpY5.png\n- 2PD = 2 People Daily Interaction where people represented as a surface point cloud\nhttps://i.imgur.com/EzyELg3.png\n\n\n**Methodology**\n- *Customized Interaction Mesh Structure*. An interaction mesh is created by generating a volumetric mesh using Delaunay Tetrahedralization. Interaction is therefore represented by a series of interaction meshes. To reduce the bias of unequal number of points per human body part, synthetic points (shown in blue) are derived from available skeleton structure (shown in red):\n https://i.imgur.com/jtlrH49.png\n\n The edges after Delaunay Tetrahedralization are filtered in a way that all edges connecting to the same character are removed, as they do not contribute to the interaction.\n https://i.imgur.com/nWeUNl1.png\n The temporal sequence of interaction is a series of interaction meshes.\n\n- *Distance between interaction meshes*.\n   - Distance between two interaction meshes of two-character interactions:\n\n   $d(e_i, e_j) = (|e_{i1} - e_{j1}| + |e_{i2} - e_{j2}|) \\times \\frac{1}{2} (1 - cos\\theta),$\n   https://i.imgur.com/jMHGx3o.png\n   where $e_{i1}$ and $e_{i2}$ are two endpoints of an edge.\n\n   - *Earth Mover's Distance*. Earth Mover\u2019s Distance (EMD) is used to find the best correspondence between the input interaction meshes to achieve the comparison of two interactions with different semantic meaning:\n\n    $EMD(E_I^{t_I}, E_J^{tJ}) = \\frac{D(E_I^{t_I}, E_J^{tJ})}{\\sum_{i=1}^{m} \\sum_{j=1}^{n} f_{i,j}^*},$\n\n    where $D(E_I^{t_I}, E_J^{tJ}) = \\sum_{i=1}^{m} \\sum_{j=1}^{n} d(e_i, e_j) f_{i,j}^*$ represents the minimum distance between two interaction meshes and $f_{i,j}^*$ is the optimal set of flow values returned by the mass transport solver that finds the optimal edge-level correspondence between two interaction meshes. The concept of the mass transport solver is visualized below:\n    https://i.imgur.com/wmRhcxP.png\n\n\n - *Distance between interactions sequences*.\n    - spatial normalization - removing its pelvis translation and its horizontal facing angle in each frame\n\n    - temporal sampling - non-linear sampling strategy based on the frame distance measured by EMD. The sampling algorithm samples fewer in temporal regions with high similarity, which contribute less to the context of the interaction.\n\n    - temporal alignment - keyframes are aligned using Dynamic Time Warping (DTW)\n\nPossible functionality with the proposed method:\n- Interaction motion similarity analysis\n- Interaction motion retrieval\n\nNotice:\n>The pre-process took 1.5 hours, 0.5 hour and 4 hours for the 2C, CRC and HOI databases respectively. Given the meshes, computing the distance between two interactions took 0.2 seconds on average.\n\nDiscussion:\n- the algorithm focuses on boxing/kickboxing as they have clear rules. Extending the proposed algorithm for measuring motion similarity for daily activities would require careful annotation.\n- While the method, in theory, can be applied to single human activities, it quite clear that it would perform worse than other baselines for the task. This implies that it is better not applied for use cases where a comparison between two independent movements having no interaction with each other (dancing, yoga) is required.\n- The method works best for close interaction activities as the edges in the interaction mesh would tend to have a similar structure (e.g. edge length) in case of distant interacting objects. \n- Application-wise, the algorithm is not suitable for online real-time use.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/tvcg.2019.2893247"
    },
    "105": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tog/AbermanWLCC19",
        "transcript": "This paper presents a method to extract motion (dynamic) and skeleton / camera-view (static) representations from the video of a person represented as a 2D joints skeleton. This decomposition allows transferring the motion to different skeletons (retargeting) and many more.  It does so by utilizing deep neural networks. \nhttps://i.imgur.com/J5jBzcs.png\n\nThe architecture consists of motion and skeleton / camera-view encoders that decompose an input sequence of 2D joint positions into latent spaces and a decoder that reconstructs a sequence from such components. The motion vector varies in length, while skeleton and camera view representations are fixed.\nhttps://i.imgur.com/QaDksg1.png\n\nThis is achieved by the nature of the network design. Specifically, motion encoder uses 1D convolutions with strides, thus output dimensions are proportionally related to the input. On the other hand, the static encoder uses global average pooling in the final layer to produce a fixed-size latent representation:\nhttps://i.imgur.com/Cf7TVKA.png\n\nMore detailed design of the encoders and decoder is shown below:\nhttps://i.imgur.com/cpaveFm.png\n\n**Dataset**. Adobe Mixamo is used to obtain sequences of poses of different 3D characters. It allows creating multiple samples where different characters (with different skeleton structure) perform the same motions. These 3D video clips are then projected into 2D by selecting arbitrary view angles and distance to the object. Thus, we can easily create multiple pairs of 2D image sequences of characters (same or different) performing various actions (same or different) from various views.\n\n**Loss functions** used to for training (refer the paper for the detailed formulas):\n- *Cross Reconstruction Loss*\n\n It is a sum of two other losses. The first one is the reconstruction loss where the network tries to reconstruct original input. The second one is cross reconstruction loss where the network tries to reconstruct the sequence where a different character performs the exact same action as the input. It is best shown in the Figure below:\nhttps://i.imgur.com/ewZOAox.png\n\n- *Triplet Loss*\n This loss aims to bring latent spaces of similar motions closer together, while separate apart the ones that are different. It takes two triplets, where each contains two samples that share the same (or very similar) motion and one with different. The same concept is applied to the static latent space.\n\n- Foot velocity loss\n This loss helps to remove the foot skating phenomenon - hands and feet exhibit larger errors that the other keypoints. \nhttps://i.imgur.com/DclJEde.png\nwhere $V_{global}$ and $V_{joint_n}$ extract the global and local ($n$th joint) velocities from the reconstructed output $\\hat{p}_{ij}$, respectively, and map them back to the image units, and $V_{orig_n}$ returns the original global velocity of the $n$th joint from the ground truth, $p_{ij}$\n\n**Normalization**\n - subtract the root position from all joint locations in every frame\n - subtract the mean joint position and divide by the standard deviation (averaged over the entire dataset) \n - per-frame global velocity is not touched\n\n**Data Augmentation** applied during training:\n - temporal clipping during the batch creation process\n - scaling - same as to use different camera distance to the object\n - flipping symmetrical joints\n - dropping joints to simulate behavior of a real keypoint detector as they often miss some joints\n - adding real video data to the training and use reprojection loss in case no labels are given\n\n**Results and Evaluation** (to be continued) ...\n\nWhile the summary becomes too long to be a called a summary it is worth mentioning that there are several applications possible with this approach:\n - performance cloning - make any 2D skeleton repeat particular motions\n - motion retrieval - search videos that contain the particular target motion",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3306346.3322999"
    },
    "106": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=Pavllo_2019_CVPR",
        "transcript": "This paper proposes a 3D human pose estimation in video method based on the dilated temporal convolutions applied on 2D keypoints (input to the network). 2D keypoints can be obtained using any person keypoint detector, but Mask R-CNN with ResNet-101 backbone, pre-trained on COCO and fine-tuned on 2D projections from Human3.6M, is used in the paper.\nhttps://i.imgur.com/CdQONiN.png\n\nThe poses are presented as 2D keypoint coordinates in contrast to using heatmaps (i.e. Gaussian operation applied at the keypoint 2D location). Thus, 1D convolutions over the time series are applied, instead of 2D convolutions over heatmaps. The model is a fully convolutional architecture with residual connections that takes a sequence of 2D poses ( concatenated $(x,y)$ coordinates of the joints in each frame) as input and transforms them through temporal convolutions.\nhttps://i.imgur.com/tCZvt6M.png\nThe `Slice` layer in the residual connection performs padding (or slicing) the sequence with replicas of boundary frames (to both left and right) to match the dimensions with the main block as zero-padding is not used in the convolution operations.\n\n3D pose estimation is a difficult task particularly due to the limited data available online. Therefore, the authors propose semi-supervised approach of training the 2D->3D pose estimation by exploiting unlabeled video. Specifically, 2D keypoints are detected in the unlabeled video with any keypoint detector, then 3D keypoints are predicted from them and these 3D points are reprojected back to 2D (camera intrinsic parameters are required). This is idea similar to cycle consistency in the [CycleGAN](https://junyanz.github.io/CycleGAN/), for instance.\nhttps://i.imgur.com/CBHxFOd.png\nIn the semi-supervised part (bottom part of the image above) training penalizes when the reprojected 2D keypoints are far from the original input. Weighted mean per-joint position error (WMPJPE) loss, weighted by the inverse of the depth to the object (since far objects should contribute less to the training than close ones) is used as the optimization goal.\n\nThe two networks (`supervised` above, `semi-supervised` below) have the same architecture but do not share any weights. They are jointly optimized where `semi-supervised` part serves as a regularizer. They communicate through the path aiming to make sure that the mean bone length of the above and below branches match.\n\nThe interesting tendency is observed from the MPJPE analysis with different amounts of supervised and unsupervised data available. Basically, the `semi-supervised` approach becomes more effective when less labeled data is available.\nhttps://i.imgur.com/bHpVcSi.png\n\nAdditionally, the error is reduced when the ground truth keypoints are used. This means that a robust and accurate 2D keypoint detector is essential for the accurate 3D pose estimation in this setting.\nhttps://i.imgur.com/rhhTDfo.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://openaccess.thecvf.com/content_CVPR_2019/papers/Pavllo_3D_Human_Pose_Estimation_in_Video_With_Temporal_Convolutions_and_CVPR_2019_paper.pdf"
    },
    "107": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/0009XLW19",
        "transcript": "This paper is a top-down (i.e. requires person detection separately) pose estimation method with a focus on improving high-resolution representations (features) to make keypoint detection easier. \n\nDuring the training stage, this method utilizes annotated bounding boxes of person class to extract ground truth images and keypoints. The data augmentations include random rotation, random scale, flipping, and [half body augmentations](http://presentations.cocodataset.org/ECCV18/COCO18-Keypoints-Megvii.pdf) (feeding upper or lower part of the body separately). Heatmap learning is performed in a typical for this task approach of applying L2 loss between predicted keypoint locations and ground truth locations (generated by applying 2D Gaussian with std = 1).\n\nDuring the inference stage, pre-trained object detector is used to provide bounding boxes. The final heatmap is obtained by averaging heatmaps obtained from the original and flipped images. The pixel location of the keypoint is determined by $argmax$ heatmap value with a quarter offset in the direction to the second-highest heatmap value.\n\nWhile the pipeline described in this paper is a common practice for pose estimation methods, this method can achieve better results by proposing a network design to extract better representations. This is done through having several parallel sub-networks of different resolutions (next one is half the size of the previous one) while repeatedly fusing branches between each other:\nhttps://raw.githubusercontent.com/leoxiaobin/deep-high-resolution-net.pytorch/master/figures/hrnet.png\n\nThe fusion process varies depending on the scale of the sub-network and its location in relation to others:\nhttps://i.imgur.com/mGDn7pT.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Deep_High-Resolution_Representation_Learning_for_Human_Pose_Estimation_CVPR_2019_paper.html"
    },
    "108": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1007/978-3-030-01252-6_26",
        "transcript": "The method is a multi-task learning model performing person detection, keypoint detection, person segmentation, and pose estimation. It is a bottom-up approach as it first localizes identity-free semantics and then group them into instances.\nhttps://i.imgur.com/kRs9687.png\n\nModel structure:\n - **Backbone**. A feature extractor is presented by ResNet-(50 or 101) with one [Feature Pyramid Network](https://arxiv.org/pdf/1612.03144.pdf) (FPN) for keypoint branch and one for person detection branch. FPN enhances extracted features through multi-level representation.\n -  **Keypoint detection** detects keypoints as well as produces a pixel-level segmentation mask.\nhttps://i.imgur.com/XFAi3ga.png\nFPN features $K_i$ are processed with multiple $3\\times3$ convolutions followed by concatenation and final $1\\times1$  convolution to obtain predictions for each keypoint, as well as segmentation mask (see Figure for details). This results in #keypoints_in_dataset_per_person + 1 output layers. Additionally, intermediate supervision (i.e. loss) is applied at the FPN outputs. $L_2$ loss between predictions and Gaussian peaks at the keypoint locations is used. Similarly, $L_2$ loss is applied for segmentation predictions and corresponding ground truth masks.\n - **Person detection** is essentially a [RetinaNet](https://arxiv.org/pdf/1708.02002.pdf), a one-stage object detector, modified to only handle *person* class.\n - **Pose estimation**. Given initial keypoint predictions, Pose Estimation Network (PRN) selects a single keypoint for each class. \nhttps://i.imgur.com/k8wNP5p.png\nDuring inference, PRN takes cropped outputs from keypoint detection branch defined by the predicted bounding boxes from the person detection branch, resizes it to a fixed size, and forwards it through a multilayer perceptron with residual connection. During the training, the same process is performed, except the cropped keypoints come from the ground truth annotation defined by a labeled bounding box.\n\nThis model is not an end-to-end trainable model. While keypoint and person detection branches can, in theory, be trained simultaneously, PRN network requires separate training.\n\n**Personal note**. Interestingly, PRN training with ground truth inputs (i.e. \"perfect\" inputs) only reaches 89.4 mAP validation score which is surprisingly quite far from the max possible score. This presumably means that even if preceding networks or branches perform god-like, the PRN might become a bottleneck in the performance. Therefore, more efforts should be directed to PRN itself. Moreover, modifying the network to support end-to-end training might help in boosting the performance.\n\nOpen-source implementations used to make sure the paper apprehension is correct: [link1](https://github.com/LiMeng95/MultiPoseNet.pytorch), [link2](https://github.com/IcewineChen/pytorch-MultiPoseNet).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-030-01252-6_26"
    },
    "109": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1007/978-3-030-01228-1_25",
        "transcript": "This paper tackles the challenge of action recognition by representing a video as space-time graphs: **similarity graph** captures the relationship between correlated objects in the video while the **spatial-temporal graph** captures the interaction between objects.\n\nThe algorithm is composed of several modules:\n\nhttps://i.imgur.com/DGacPVo.png\n\n1. **Inflated 3D (I3D) network**. In essence, it is usual 2D CNN (e.g. ResNet-50) converted to 3D CNN by copying 2D weights along an additional dimension and subsequent renormalization. The network takes *batch x 3 x 32 x 224 x 224* tensor input and outputs *batch x 16 x 14 x 14*.\n\n2. **Region Proposal Network (RPN)**. This is the same RPN used to predict initial bounding boxes in two-stage detectors like Faster R-CNN. Specifically, it predicts a predefined number of bounding boxes on every other frame of the input (initially input is 32 frames, thus 16 frames are used) to match the temporal dimension of I3D network's output. Then, I3D network output features and projected on them bounding boxes are passed to ROIAlign to obtain temporal features for each object proposal. Fortunately, PyTorch comes with a [pretrained Faster R-CNN on MSCOCO](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) which can be easily cut to have only RPN functionality.\n\n3. **Similarity Graph**. This graph represents a feature similarity between different objects in a video. Having features $x_i$ extracted by RPN+ROIAlign for every bounding box predictions in a video, the similarity between any pair of objects is computed as $F(x_i, x_j) = (wx_i)^T * (w'x_j)$, where $w$ and $w'$ are learnable transformation weights. Softmax normalization is performed on each edge on the graph connected to a current node $i$. Graph convolutional network is represented as several graph convolutional layers with ReLU activation in between. Graph construction and convolutions can be conveniently implemented using [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric).\n\n4. **Spatial-Temporal Graph**. This graph captures a spatial and temporal relationship between objects in neighboring frames. To construct a graph $G_{i,j}^{front}$, we need to iterate through every bounding box in frame $t$ and compute Intersection over Union (IoU) with every object in frame $t+1$. The IoU value serves as the weight of the edge connecting nodes (ROI aligned features from RPN) $i$ and $j$. The edge values are normalized so that the sum of edge values connected to proposal $i$ will be 1. In a similar manner, the backward graph $G_{i,j}^{back}$ is defined by analyzing frames $t$ and $t-1$. \n\n5. **Classification Head**. The classification head takes two inputs. One is coming from average pooled features from I3D model resulting in *1 x 512* tensor. The other one is from pooled sum of features (i.e. *1 x 512* tensor) from the graph convolutional networks defined above. Both inputs are concatenated and fed to Fully-Connected (FC) layer to perform final multi-label (or multi-class) classification.\n\n**Dataset**. The authors have tested the proposed algorithm on [Something-Something](https://20bn.com/datasets/something-something) and [Charades](https://allenai.org/plato/charades/) datasets. For the first dataset, a softmax loss function is used, while the second one utilizes binary sigmoid loss to handle a multi-label property. The input data is sampled at 6fps, covering about 5 seconds of a video input.\n\n**My take**. I think this paper is a great engineering effort. While the paper is easy to understand at the high-level, implementing it is much harder partially due to unclear/misleading writing/description. I have challenged myself with [reproducing this paper](https://github.com/BAILOOL/Videos-as-Space-Time-Region-Graphs). It is work in progress, so be careful not to damage your PC and eyes :-)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-030-01228-1_25"
    },
    "110": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1808.07371",
        "transcript": "This paper presents a per-frame image-to-image translation system enabling copying of a motion of a person from a source video to a target person. For example, a source video might be a professional dancer performing complicated moves, while the target person is you. By utilizing this approach, it is possible to generate a video of you dancing as a professional. Check the authors' [video](https://www.youtube.com/watch?v=PCBTZh41Ris) for the visual explanation.\n\n**Data preparation**\n\nThe authors have manually recorded high-resolution video ( at 120fps ) of a person performing various random moves. The video is further decomposed to frames, and person's pose keypoints (body joints, hands, face) are extracted for each frame. These keypoints are further connected to form a person stick figure. In practice, pose estimation is performed by open source project [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose).\n\n**Training**\nhttps://i.imgur.com/VZCXZMa.png\n\nOnce the data is prepared the training is performed in two stages:\n1. **Training pix2pixHD model with temporal smoothing**.\n    \n   The core model is an original [pix2pixHD](https://tcwang0509.github.io/pix2pixHD/)[1] model with temporal smoothing. Specifically, if we were to use vanilla pix2pixHD, the input to the model would be a stick person image, and the target is the person's image corresponding to the pose. The network's objective would be $min_{G} (Loss1 + Loss2 + Loss3)$, where: \n\n - $Loss1 = max_{D_1, D_2, D_3} \\sum_{k=1,2,3} \\alpha_{GAN}(G, D_k)$ is adverserial loss;\n \n - $Loss2 = \\lambda_{FM} \\sum_{k=1,2,3} \\alpha_{FM}(G,D_k)$ is feature matching loss;\n\n  - $Loss3 = \\lambda_{VGG}\\alpha_{VGG}(G(x),y)]$ is VGG perceptual loss.\n\n However, this objective does not account for the fact that we want to generate video composed of frames that are temporally coherent. The authors propose to ensure *temporal smoothing* between adjacent frames by including pose, corresponding image, and generated image from the previous step (zero image for the first frame) as shown in the figure below:\n\n  https://i.imgur.com/0NSeBVt.png\n\n   Since the generated output $G(x_t; G(x_{t-1}))$ at time step $t$ is now conditioned on the previously generated frame $G(x_{t-1})$ as well as current stick image $x_t$, better temporal consistency is ensured. Consequently, the discriminator is now trying to determine both correct generation, as well as temporal consitency for a fake sequence $[x_{t-1}; x_t; G(x_{t-1}), G(x_t)]$.\n\n2. **Training FaceGAN model**.\n\n   https://i.imgur.com/mV1xuMi.png\n  \n   In order to improve face generation, the authors propose to use specialized FaceGAN. In practice, this is another smaller pix2pixHD model (with a global generator only, instead of local+global) which is fed with a cropped face area of a stick image and cropped face area of a corresponding generated image (from previous step 1) and aims to generate a residual which is added to the previously generated full image. \n\n**Testing**\n\n During testing, we extract frames from the input video, obtain pose stick image for each frame, normalize the stick pose image and feed it to pix2pixHD (with temporal consistency) and, further, to FaceGAN to produce final generated image with improved face features. Normalization is needed to capture possible pose variation between a source and a target input video.\n\n**Remarks**\n\nWhile this method produces a visually appealing result, it is not perfect. The are several reasons for being so:\n\n 1. *Quality of a pose stick image*: if the pose detector \"misses\" the keypoint, the generator might have difficulties to generate a properly rendered image;\n 2. *Motion blur*: motion blur causes pose detector to miss keypoints;\n 3. *Severe scale change*: if source person is very far, keypoint detector might fail to detect proper keypoints.\n\nAmong video rendering challenges, the authors mention self-occlusion, cloth texture generation, video jittering (training-test motion mismatch).\n\nReferences:\n\n[1] \"High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs\"",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1808.07371v1"
    },
    "111": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.09081",
        "transcript": "This paper introduces a CNN based segmentation of an object that is defined by a user using four extreme points (i.e. bounding box). Interestingly, in a related work, it has been shown that clicking extreme points is about 5 times more efficient than drawing a bounding box in terms of speed. \nhttps://i.imgur.com/9GJvf17.png\n\nThe extreme points have several goals in this work. First, they are used as a bounding box to crop the object of interest. Secondly, they are utilized to create a heatmap with activations in the regions of extreme points. \n\nThe heatmap is created as a 2D Gaussian centered around each of the extreme points. This heatmap is matched to the size of the resized crop (i.e. 512x512) and is concatenated with the original RGB channels of the crop. \n\nThe concatenated input of channel depth=4 is fed to the network which is a ResNet-101 with FC and last two maxpool layers removed. In order to maintain the same receptive field, an astrous convolution is used. Pyramid scene parsing module from PSPNet is used to aggregate global context. The network is trained with a standard cross-entropy loss weighted by a normalization factor (i.e. a frequency of a class in a dataset).\n\nHow does it compare to \"Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++\n\" paper in terms of accuracy? Specifically, if the polygon is wrong it is easy to correct points on the polygon that are wrong. However, it is unclear how to obtain preferred segmentation when no matter how many (greater than four) extreme points are selected, the object of interest is not segmented properly.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.09081"
    },
    "112": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1803.09693",
        "transcript": "In this paper, the authors develop a system for automatic as well as an interactive annotation (i.e. segmentation) of a dataset. In the automatic mode, bounding boxes are generated by another network (e.g. FasterRCNN), while in the interactive mode, the input bounding box around an object of interest comes from the human in the loop.\n\nThe system is composed of the following parts:\nhttps://github.com/davidjesusacu/polyrnn-pp/raw/master/readme/model.png\n1. **Residual encoder with skip connections**. This step acts as a feature extractor. The ResNet-50 with few modifications (i.e. reducing stride, usage of dilation, removal of average pooling and FC layers) serve as a base CNN encoder. Instead of utilizing the last features of the network, the authors concatenate outputs from different layers - resized to highest feature resolution -  to capture multi-level representations. This is shown in the figure below:\nhttps://www.groundai.com/media/arxiv_projects/226090/x4.png.750x0_q75_crop.png\n\n2. **Recurrent decoder** is a two-layer ConvLSTM which takes image features, previous (or first) vertex position and outputs one-hot encoding of 28x28 representing possible vertex position, +1 indicates that the polygon is closed (i.e. the end of the sequence). Attention weight per location is utilized using CNN features, 1st and 2nd layers of ConvLSTM. Training is formulated as reinforcement learning since recurrent decoder is considered as sequential decision-making agent. The reward function is IoU between mask generated by the enclosed polygon and ground-truth mask.\n\n3. **Evaluator network** chooses the best polygon among multiple candidates. CNN features, last state tensor of ConvLSTM, and the predicted polygon are used as input, and the output is the predicted IoU. The best polygon is selected from the polygons which are generated using 5 top scoring first vertex predictions.\nhttps://i.imgur.com/84amd98.png\n\n4. **Upscaling with Graph Neural Network** takes the list of vertices generated by ConvLSTM decode, adds a node in between two consecutive nodes (to produce finer details at higher resolution), and aims to predict relative offset of each node at a higher resolution. Specifically, it extracts features around every predicted vertex and forwards it through GGNN (Gated Graph Neural Network) to obtain the final location (i.e. offset) of the vertex (treated as classification task).  \nhttps://www.groundai.com/media/arxiv_projects/226090/x5.png.750x0_q75_crop.png\n\nThe whole system is not trained end-to-end. While the network was trained on CityScapes dataset, it has shown reasonable generalization to different modalities (e.g. medical data). It would be very nice to observe the opposite generalization of the model. Meaning you train on medical data and see how it performs on CityScapes data.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1803.09693"
    },
    "113": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1712.01238",
        "transcript": "This paper is about interactive Visual Question Answering (VQA) setting in which agents must ask questions about images to learn. This closely mimics how people learn from each other using natural language and has a strong potential to learn much faster with fewer data. It is referred as learning by asking (LBA) through the paper. The approach is composed of three models:\nhttp://imisra.github.io/projects/lba/approach_HQ.jpeg\n\n1. **Question proposal module** is responsible for generating _important_ questions about the image. It is a combination of 2 models:\n    - **Question generator** model produces a question. It is LSTM that takes image features and question type (random choice from available options) as input and outputs a question. \n    - **Question relevance** model that selects questions relevant to the image. It is a stacked attention architecture network (shown below) that takes in generated question and image features and filters out irrelevant to the image questions.  https://i.imgur.com/awPcvYz.png\n\n2. **VQA module** learns to predict answer given the image features and question. It is implemented as stacked attention architecture shown above.\n\n3. **Question selection module** selects the most informative question to ask. It takes current state of VQA module and its output to calculate expected accuracy improvement (details are in the paper) to measure how fast the VQA module has a potential to improve for each answer. The single question selection (i.e. best question for VQA to improve the fastest) strategy is based on epsilon-greedy policy.\n\nThis method (i.e. LBA) is shown to be about 50% more data efficient than naive VQA method. As an interesting future direction of this work, the authors propose to use real-world images and include a human in the training as an answer provider.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1712.01238"
    },
    "114": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1803.07485",
        "transcript": "This paper performs pixel-wise segmentation of the object of interest which is specified by a sentence.  The model is composed of three main components: a **textual encoder**, a **video encoder**, and a **decoder**.https://i.imgur.com/gjbHNqs.png\n\n- **Textual encoder** is word2vec pre-trained model followed by 1D CNN.\n- **Video encoder** is a 3D CNN to obtain a visual representation of the video (can be combined with optical flow to obtain motion information).\n- **Decoder**. Given a sentence representation $T$ a separate filter $f^r = tanh(W^r_fT + b^r_f)$  is created\nto match each feature map in the video frame decoder and combined with visual features as $S^r_t = f^r * V^r_t$, for each $r$esolution at $t$imestep. The decoder is composed of sequence of transpose convolution layers to get the response map of the same size as the input video frame.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1803.07485"
    },
    "115": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.11543",
        "transcript": "This paper introduces a new AI task - Embodied Question Answering. The goal of this task for an agent is to be able to answer the question by observing the environment through a single egocentric RGB camera while being able to navigate inside the environment. The agent has 4 natural modules:\nhttps://i.imgur.com/6Mjidsk.png\n1. **Vision**. 224x224 RGB images are processed by CNN to produce a fixed-size representation. This CNN is pretrained on pixel-to-pixel tasks such as RGB reconstruction, semantic segmentation, and depth estimation.\n\n2. **Language**. Questions are encoded with 2-layer LSTMs with 128-d hidden states. Separate question encoders are used for the navigation and answering modules to capture important words for each module.\n\n3. **Navigation** is composed of a planner (forward, left, right, and stop actions) and a controller that executes planner selected action for a variable number of times. The planner is LSTM taking hidden state, image representation, question, and previous action. Contrary, a controller is an MLP with 1 hidden layer which takes planner's hidden state, action from the planner, and image representation to execute an action or pass the lead back to the planner.\n\n4. **Answering** module computes an image-question similarity of the last 5 frames via a dot product between image features (passed through an fc-layer to align with question features) and question encoding. This similarity is converted to attention weights via a softmax, and the attention-weighted image features are combined with the question features and passed through an answer classifier. Visually this process is shown in the figure below. https://i.imgur.com/LeZlSZx.png\n\n[Successful results](https://www.youtube.com/watch?v=gVj-TeIJfrk) as well as [failure cases](https://www.youtube.com/watch?v=4zH8cz2VlEg) are provided.\n\nGenerally, this is very promising work which literally just scratches the surface of what is possible. There are several constraints which can be mitigated to push this field to more general outcomes. For example, use more general environments with more realistic graphics and broader set of questions and answers.\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.11543"
    },
    "116": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1804.08328",
        "transcript": "The goal of this work is to perform transfer learning among numerous tasks and to discover visual relationships among them. Specifically, while we intiutively might guess the depth of an image and surface normals are related, this work takes a step forward and discovers a beneficial relationship among 26 tasks in terms of task transferability - many of them are not obvious. This is important for scenarios when an insufficient budget is available for target task for annotation, thus, learned representation from the 'cheaper' task could be used along with small dataset for the target task to reach sufficient performance on par with fully supervised training on a large dataset.\n\nThe basis of the approach is to compute an affinity matrix among tasks based on whether the solution for one task can be sufficiently easily used for another task. This approach does not impose human intuition about the task relationships and chooses task transferability based on the quality of a transfer operation in a fully computational manner.\n\nThe task taxonomy (i.e. **taskonomy**) is a computationally found directed hypergraph that captures the notion of task transferability over any given task dictionary. It performed using a four-step process depicted in the figure below: ![Process overview. The steps involved in creating the taxonomy.](http://taskonomy.stanford.edu/img/svg/Process.svg)\n\n-  In stage I (**Task-specific Modelling**), a task-specific network is trained in a fully supervised manner. The network is composed of the encoder (modified ResNet-50), and fully convolutional decoder for pixel-to-pixel tasks, or 2-3 FC layers for low-dimensional tasks. Dataset consists of 4 million images of indoor scenes from about 600 buildings; every image has an annotation for every task.\n\n- In stage II (**Transfer modeling**), all feasible transfers between sources and targets are trained (multiple inputs task to single target transfer is also considered). Specifically, after the task-specific networks are trained in stage I, the weights of an encoder are fixed (frozen network is used to extract representations only) and the representation from the encoder is used to train a small readout network (similar to a decoder from stage I) with a new task as a target (i.e. ground truth is available). In total, about 3000 transfer possibilities are trained.\n\n- In stage III (**Taxonomy solver**), the task affinities acquired from the transfer functions performance are normalized. This is needed because different tasks lie in different spaces and transfer function scale. This is performed using ordinal normalization - Analytical Hierarchy Process (details are in the paper - Section 3.3). This results in an affinity matrix where a complete graph of relationships is completely normalized and this graph quantifies a pair-wise set of tasks evaluated in terms of a transfer function (i.e. task dependency).\n\n- In stage IV (**Computed Taxonomy**), a hypergraph which can predict the performance of any transfer policy and optimize for the optimal one is synthesized. This is solved using Binary Integer Program as a subgraph selection problem where tasks are nodes and transfers are edges. After the optimization process, the solution devices a connectivity that solves all target tasks, maximizes their collective performance while using only available source tasks under user-specified constraints (e.g. budget).\n\nSo, if you want to train your network on an unseen task, you can obtain pretrained weights for existing tasks from the [project page](https://github.com/StanfordVL/taskonomy/tree/master/taskbank), train readout functions against each task (as well as combination of multiple inputs), build an affinity matrix to know where your task is positioned against the other ones, and through subgraph selection procedure observe what tasks have favourable influence on your task. Consequently, you can train your task with much less data by utilizing representations from the existing tasks which share visual significance with your task. Magnificent!\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1804.08328"
    },
    "117": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/ZimmermannB17",
        "transcript": "This paper estimate 3D hand shape from **single** RGB images based on deep learning. The overall pipeline is the following:\nhttps://i.imgur.com/H72P5ns.png\n1. **Hand Segmentation** network is derived from this [paper](https://arxiv.org/pdf/1602.00134.pdf) but, in essence, any segmentation network would do the job. Hand image is cropped from the original image by utilizing segmentation mask and resized to a fixed size (256x256) with bilinear interpolation.\n2. **Detecting hand keypoints**. 2D Keypoint detection is formulated as predicting score map for each hand joints (fixed size = 21). Encoder-decoder architecture is used. \n3. **3D hand pose estimation**. \nhttps://i.imgur.com/uBheX3o.png\n    - In this paper, the hand pose is represented as $w_i = (x_i, y_i, z_i)$, where $i$ is index for a particular hand joint. This representation is further normalized $w_i^{norm} = \\frac{1}{s} \\cdot w_i$, where $s = ||w_{k+1} - w_{k} ||$, and relative position to a reference joint $r$ (palm) is obtained as $w_i^{rel} = w_i^{norm} - w_r^{norm}$.\n    - The network predicts coordinates within a canonical frame and additionally estimate the transformation into the canonical frame (as opposite to predicting absolute 3D coordinates). Therefore, the network predicts $w^{c^*} = R(w^{rel}) \\cdot w^{rel}$ and $R(w^{rel}) = R_y \\cdot R_{xz}$.\nInformation whether left/right hand is the input is concatenated to flattened feature representation. The training loss is composed of a separate term for canonical coordinates and canonical transformation matrix L2 losses.\n\nContribution: \n- Apparently, the first method to perform 3D hand shape estimation from a single RGB image rather than using both RGB and depth sensors;\n- Possible extension to sign language recognition problem by attaching classifier on predicted 3D poses.\n\nWhile this approach quite accurately predicts hand 3D poses among frames, they often fluctuate among frames. Probably several techniques (i.e. optical flow, RNN, post-processing smoothing) can be used for ensuring temporal consistency and make predictions more stable across frames.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/ICCV.2017.525"
    },
    "118": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1802-10548",
        "transcript": "Given microscopy cell data, this work tries to determine the number of cells in the image. The whole pipeline is composed of two steps:\n1. Cells segmentation:\n    - Feature Pyramid Network is used for generating a foreground mask;\n    - The last output of FPN is used for predicting mean foreground masks and aleatoric uncertainty masks. Each mask in both outputs is trained with aleatoric loss $ \\frac{||y_{pred} - y_{gt} ||^2}{2\\sigma} + \\log{2\\sigma}$  and [total-variational](https://en.wikipedia.org/wiki/Total_variation_denoising) loss. https://i.imgur.com/ssTuGVe.png\n2. Cell counting:\n    - VGG-11 network is used as a feature extractor from the predicted foreground segmentation masks. There are two output branches following VGG: cell count branch and estimated variance branch. Training is done using L2 loss function with aleatoric uncertainty for cell counts.\nhttps://i.imgur.com/aijZn7e.png\n\nWhile the idea to utilize neural networks to count cells in the image seems fascinating, the real benefit of such system in production is quite questionable. Specifically, why would you need to add a VGG-like feature extractor on top of already predicted cell segmentation masks, if you could simply do more work in segmentation network (i.e. separate cells better, predict objectness/contour) and get the number of cells directly from the predicted masks?\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.10548"
    },
    "119": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tog/SuwajanakornSK17",
        "transcript": "This paper synthesizes a high-quality video of Barack Obama given the audio. Practically, it only synthesizes the region around the mouth, while the rest of the elements (i.e. pixels) come from a video in a database. \nThe overall pipeline is the following:\n- Given a video, an audio and a mouth shape are extracted. Audio is represented as MFCC coefficients; mouth shape - 18 lip markers;\n- Train audio to mouth shape mapping with time-delayed unidirectional LSTM.\n- Synthesize mouth texture: retrieve a number of video frames in a database where a mouth shape is similar to the output of LSTM; synthesize median texture by applying weighted median on mouth shapes from retrieved video frames; manually select teeth target frame (selection criteria are purely subjected) and enhance teeth median texture with selected teeth target frame.\n- Re-timing to avoid situations where Obama is not speaking but his head is moving which looks very unnatural. \n- Final composition into the target video involves jaw correction to make it more natural.\n![Algorithm flow](http://www.kurzweilai.net/images/Obama-lip-Sync-Graphic.jpg)\n\nThe results look ridiculously natural. Authors suggest that one of the applications of this paper is speech summarization, where you summarize a speech not only with selected parts as text and audio but also synthesize a video for it. Personally, this work inspires me to work on a method that is able to generate natural sign language interpreter that takes sound/text as input and produces sign language moves. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/3072959.3073640"
    },
    "120": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/CamgozHKB17",
        "transcript": "This paper tackles a challenging task of hand shape and continuous Sign Language Recognition (SLR) directly from images obtained from a common RGB camera (rather than utilizing motion sensors like Kinect). The basic idea is to create a network that is end-to-end trainable with input (i.e. images) and output (i.e. hand shape labels, word labels) sequences. The network is composed of three parts:\n - CNN as a feature extractor\n - Bidirectional LSTMs for temporal modeling\n - Connectionist Temporal Classification as a loss layer \n![Network structure](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/3269d3541f0eec006aee6ce086db2665b7ded92d/1-Figure1-1.png)\n\nResults:\n - Observed state-of-art results (at the time of publishing) on \"One-Million Hands\" and \"RWTH-PHOENIX-Weather-2014\" datasets.\n - Utilizing full images rather than hand patches provides better performance for continuous SLR. \n - A network that recognizes hand shape and a network that recognizes word sequence can be combined and trained together to recognize word sequences. Finetuning combined system from for all layers works better than fixing \"feature extraction\" layers.\n - Combination of two networks where each network trained on separate task performs slightly better than training each network on word sequences.\n - Marginal difference in performance observed for different decoding and post-processing techniques during sequence-to-sequence predictions.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/ICCV.2017.332"
    },
    "121": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ODonoghueMKM16",
        "transcript": "This paper proposes an approach to incorporate off-line samples in Policy Gradient. The authors were able to do this by drawing a parallel between Policy Gradient and Q-Learning. This is the second paper in ICLR 2017 that tries to use off-line samples to accelerate the learning in Policy Gradient. The summary of the first paper can be found [here](http://www.shortscience.org/paper?bibtexKey=journals/corr/WangBHMMKF16]). \n\nThis is the first paper that describes the relationship between policy gradient and Q-learning. Lets consider a simple case of a grid world problem. In this problem, at every state, you can take one of the four actions: left, right, up, or down. Lets assume that, you are following a policy $\\pi_\\theta(\\cdot|s_t)$. Let $\\beta^{\\pi_\\theta}(s_t)$ denotes the stationary probability distribution of states under policy $\\pi_\\theta$ and $r(s, a)$ be the reward that we get after taking action $a$ at state $s$. Then our goal is to maximize\n$$\n\\begin{array}{ccc}\n&\\arg\\max_{\\theta} \\mathbb{E}_{s \\sim \\beta^{\\pi_\\theta}} \\sum_{a}\\pi_\\theta(a|s) r(s, a) + \\alpha H(\\pi_\\theta(\\cdot| s))&\\\\\n& \\sum_{a} \\pi_\\theta(a|s) = 1 \\;\\;\\forall\\;\\;  s\n\\end{array}\n$$ \n\nNote that the above equation is a regularized policy optimization approach where we added a entropy bonus term, $H(\\pi_{\\theta}(\\cdot | s))$ to enhance exploration. Mathematically,\n$$\nH(\\pi_{\\theta}(\\cdot | s)) = - \\sum_{a} \\pi_\\theta(a|s)\\log\\pi_\\theta(a|s)\n$$\nand \n$$\n\\begin{array}{ccl}\n\\nabla_\\theta H(\\pi_{\\theta}(\\cdot | s))& = &-  \\sum_{a} \\left(1 + \\log\\pi_\\theta(a|s)\\right) \\nabla_\\theta \\pi_\\theta(a|s) \\\\\n& = &- \\mathbb{E}_{a \\sim \\pi_\\theta(\\cdot|s)}\\left(1 + \\log\\pi_\\theta(a|s)\\right) \\nabla_\\theta \\log\\pi_\\theta(a|s) \\;\\; \\text{(likelihood trick)}\n\\end{array}\n$$\nLagrange multiplier tells us that at the critical point $\\theta^*$ of the above optimization problem, the gradient of optimization function should be parallel to gradient of constraint. Using the Policy Gradient Theorem, the gradient of the objective at optimal policy $\\theta^*$ is \n$$\n\\mathbb{E}_{s \\sim \\beta^{\\pi_{\\theta^*}}, a \\sim \\pi_{\\theta^*}(\\cdot|s)} \\nabla_{\\theta} \\log\\pi_{\\theta^*}(a|s) \\left(Q^{\\pi_{\\theta^*}}(s, a) - \\alpha  - \\alpha \\log\\pi_{\\theta^*}(a|s)\\right)\n$$ \n\nThe gradient of the constraint at the optimal point $\\theta^*$ is \n$$\n\\mathbb{E}_{s \\sim \\beta^{\\pi_{\\theta^*}}, a \\sim \\pi_{\\theta^*}(\\cdot|s)} \\lambda_s  \\nabla_{\\theta^*}\\log\\pi_{\\theta^*}(a|s)\n$$\n\nUsing the theory of Lagrange multiplication\n$$\n\\begin{array}{lll}\n&&\\mathbb{E}_{s \\sim \\beta^{\\pi_{\\theta^*}}, a \\sim \\pi_{\\theta^*}(\\cdot|s)} \\nabla_{\\theta}\\log\\pi_{\\theta^*}(a|s) \\left(Q^{\\pi_{\\theta^*}}(s, a) - \\alpha  - \\alpha \\log\\pi_{\\theta^*}(a|s)\\right) = \\\\\n&&\\mathbb{E}_{s \\sim \\beta^{\\pi_{\\theta^*}}, a \\sim \\pi_{\\theta^*}(\\cdot|s)} \\lambda_s  \\nabla_{\\theta}\\log\\pi_{\\theta^*}(a|s)\n\\end{array}\n$$\n\n\nIf $\\beta^{\\pi_{\\theta^*}}(s) > 0\\;\\; \\forall\\;\\; s $ and $0 < \\pi_{\\theta^*}(a | s) < 1\\;\\; \\forall\\;\\; s, a$, then for the tabular case of grid world, we get \n$$\nQ^{\\pi_{\\theta^*}}(s, a) -  \\alpha \\log\\pi_{\\theta^*}(a|s) = \\lambda_{s}\\;\\; \\forall \\;\\; a \n$$\nBy multiplying both sides in above equation with $\\pi_{\\theta^*}(a|s)$ and summing over $a$, we get \n$$\n\\lambda_s = V^{\\pi_{\\theta^*}}(s) + \\alpha H(\\pi_\\theta(\\cdot | s))\n$$\nUsing the value of Lagrange Multiplier, the action-value function of the optimal policy $\\pi_{{\\theta^*}}$ is\n$$\n{Q}^{\\pi_{\\theta^*}}(s, a) = V^{\\pi_{\\theta^*}}(s) + \\alpha \\left(H(\\pi_{\\theta^*}(\\cdot | s)) + \\log\\pi_{\\theta^*}(a|s)\\right)\n$$\nand the optimal policy $\\pi_{\\theta^*}$ is a softmax policy \n$$\n\\pi_{\\theta^*}(a|s) = \\exp\\left( \\frac{Q^{\\pi_{\\theta^*}}(s, a) - V^{\\pi_{\\theta^*}}(s)}{\\alpha} - H(\\pi_{\\theta^*}(\\cdot|s))\\right)\n$$\n\nThe above relationship suggests that the optimal policy for the tabular case is a softmax policy of action-value function. Mainly;\n$$\n\\pi_{\\theta^*}(a|s) = \\frac{\\exp\\left(\\frac{Q^{\\pi_{\\theta^*}}(s,a)}{\\alpha}\\right)}{\\sum_b \\exp\\left(\\frac{Q^{\\pi_{\\theta^*}}(s,b)}{\\alpha}\\right)}\n$$\nNote that as the $\\alpha \\rightarrow 0$, the above policy becomes a greedy policy. Since we know that $Q$ learning reaches to an optimal policy, we can say that the above softmax policy will also converge to the optimal policy as the $\\alpha \\rightarrow 0$.\n  \nThe authors suggest that even when the policy $\\pi_{\\theta}$ is not an optimal policy, we can still use the \n$\\tilde{Q}^{\\pi_\\theta}(s, a)$ as an estimate for action-value of policy $\\pi_\\theta$ where  \n$$\n\\tilde{Q}^{\\pi_{\\theta}}(s, a) = V^{\\pi_{\\theta}}(s) + \\alpha \\left(H(\\pi_{\\theta}(\\cdot | s)) + \\log\\pi_{\\theta}(a|s)\\right)\n$$\nIn the light of above definition of $\\tilde{Q}^{\\pi_\\theta}(s, a)$, the update rule for the regularized policy gradient can be written as\n$$\n\\mathbb{E}_{s \\sim \\beta^{\\pi_\\theta}, a \\sim \\pi_\\theta(\\cdot|s)} \\nabla_{\\theta} \\log\\pi_\\theta(a|s) \\left(Q^{\\pi_\\theta}(s, a) - {\\tilde{Q}}^{\\pi_\\theta}(s, a) \\right)\n$$\nNote that all the term in the above equation that depend only on the state $s$ can be incorporated using the baseline trick.   \n\n**Author shows a strong similarity between Duelling DQN and Actor-critice method** \n\nIn a duelling architecture, action-values are represented as summation of Advantage and Value function. Mathematically,\n$$\nQ(s, a) =  A^w(s, a) - \\sum_a \\mu(a|s) A^w(s, a) + V^\\phi(s)\n$$\n\nThe goal of the middle term in the above equation to obtain advantage, $A^w(s, a)$, and value function, $V(s)$, uniquely given $Q(s, a) \\forall a$. In the Duelling architecture, we will minimize the following error \n$$\n(r+ \\max_b Q(s', b) - Q(s, a))^2\n$$\nConsequently, we will update the $w$ and $\\phi$ parameters as following:\n$$\n\\begin{array}{ccc}\n\\Delta W &\\sim& (r+ \\max_b Q(s', b) - Q(s, a)) \\nabla \\left( A^w(s, a) - \\sum_a \\mu(a|s) A^w(s, a) \\right) \\\\\n\\Delta \\phi &\\sim& (r+ \\max_b Q(s', b) - Q(s, a)) \\nabla \\left( V^\\phi(s) \\right)\n\\end{array}\n$$\n\nNow assume an actor-critic approach, where policy is parametrized by $A^w(s, a)$ and there is a critic $V^\\phi(s)$ of value-function. The policy $\\pi_w(s, a)$ is \n$$\n\\pi_w(s, a) = \\frac{e^{A^w(s, a)/\\alpha}}{\\sum_a e^{A^w(s, a)/\\alpha}}\n$$\n\nNote that \n$$\n\\nabla_w \\log\\pi_w(s, a) = \\nabla_w \\frac{1}{\\alpha}\\left(A^w(s, a) - \\sum_a \\pi_w(s, a) A^w(s, a)\\right)\n$$\n\nTo estimate the $Q$-values of policy $\\pi_w$, we use the estimator obtained from optimal policy:\n$$\n\\begin{array}{ccc}\n\\hat{Q}(s, a) &=& \\alpha \\left(-\\sum_a \\pi_w(a | s)\\log\\pi_w(a | s) + \\log\\pi_w(a|s)\\right) + V^\\phi(s) \\\\\n&=& A^w(s, a) - \\sum_a \\pi_w(a | s) A^w(s, a) + V^\\phi(s)\n\\end{array}\n$$\n\nNote that the $Q-$estimate in the actor critic and duelling architecture are different in using $\\pi$ instead of $\\mu$. The actor update rule in the regularized policy gradient will be \n$$\n\\Delta W \\sim (r+ \\max_b Q(s', b) - Q(s, a)) \\nabla \\left( A^w(s, a) - \\sum_a \\pi_w(a|s) A^w(s, a) \\right) \n$$\nand the critic update rule is \n$$\n\\Delta \\phi \\sim (r+ \\max_b Q(s', b) - Q(s, a)) \\nabla V^\\phi(s)\n$$\n\nNote that the rules to update $V^{\\phi}$ is same in both DQN and actor-critic. The rule to update the critic varies in the probability distribution that is used to normalize the advantage function to make it unique.\n\n** PGQ Algorithm**\n\nGiven this information PGQ Algorithm is simple and consist of the following steps:\n1. Lets $\\pi_\\theta(a|s)$ represent our policy network and $V^w(s)$ represent our value network.\n2. do $N$ times\n   1. We collect samples $\\{s_1, a_1, r_1, s_2, a_2, r_2, \\cdots, s_T\\}$ using policy $\\pi_\\theta$.\n   2. We compute $Q^{\\pi_\\theta}(s_t, a_t) = \\alpha(\\log \\pi(a_t| s_t) + H(\\pi(\\cdot|s_t))) + V(s_t)\\;\\; \\forall t$. \n   3. We update $\\theta$ using the regularized policy gradient approach:\n   $$\n   \\begin{array}{ccc}\n   \\Delta \\theta & = & \\nabla_\\theta \\log \\pi_\\theta(a|s)(r_t + \\max_b \\tilde{Q}^{\\pi_\\theta}(s_{t+1}, b) - \\tilde{Q}^{\\pi_\\theta}(s_{t}, a_{t} )\\\\\n   \\Delta \\theta & = &  \\nabla_\\theta (W_\\theta(s, a) - \\sum_{a} \\pi_\\theta(a|s) W_\\theta(s, a))(r_t + \\max_b \\tilde{Q}^{\\pi_\\theta}(s_{t+1}, b) - \\tilde{Q}^{\\pi_\\theta}(s_{t}, a_{t} )\n   \\end{array}\n   $$\n   where $\\pi_{\\theta}(s, a) = \\frac{e^{W(s, a)}}{\\sum_b e^{W(s, b)}}$\n  We update the critic by minimizing the mean square error:\n   $$\n   ((r_t + \\max_b \\tilde{Q}^{\\pi_\\theta}(s_{t+1}, b) - \\tilde{Q}^{\\pi_\\theta}(s_{t}, a_{t} ))^2      \n   $$\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1611.01626"
    },
    "122": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/WangBHMMKF16",
        "transcript": "In many policy gradient algorithms, we update the parameters in online fashion. We collect trajectories  from a policy, use the trajectories to compute the gradient of policy parameters with respect to the long-term cumulative reward, and update the policy parameters using this gradient. It is to be noted here that we do not use these samples again after updating the policies. The main reason that we do not use these samples again because we need to use **importance sampling** and **importance sampling** suffers from high variance and can make the learning potentially unstable. \n\nThis paper proposes an update on **Asynchronous Advantage Actor Critic (A3C)** to incorporate off-line data (the trajectories collected using previous policies). \n\n** Incorporating offline data in Policy Gradient **\nThe offline data is incorporated using importance sampling. Mainly; lets $J(\\theta)$ denote the total reward using policy $\\pi(\\theta)$, then using Policy Gradient Theorem\n$$\n\\Delta J(\\theta) \\propto \\mathbb{E}_{x_t \\sim \\beta_\\mu, a_t \\sim \\mu}[\\rho_t \\nabla_{\\theta} \\log \\pi(a_t | x_t) Q^{\\pi}(x_t, a_t)] \n$$\nwhere $\\rho_t = \\frac{\\pi(a_t | x_t)}{\\mu({a_t|x_t})}$. $\\rho_t$ is called the importance sampling term. $\\beta_\\mu$ is the stationary probability distribution of states under the policy $\\mu$.\n\n**Estimating $Q^{\\pi}(x_t, a_t)$ in above equation:** The authors used a *retrace-$\\lambda$* approach to estimate $Q^{\\pi}$. Mainly; the action-values were computed using the following recursive equation:\n$$\nQ^{\\text{ret}}(x_t, a_t) = r_t + \\gamma \\bar{\\rho}_{t+1}\\left(Q^{\\text{ret}}(x_{t+1}, a_{t+1}) - Q(x_{t+1}, a_{t+1})\\right) + \\gamma V(x_{t+1})\n$$\nwhere $\\bar{\\rho}_t = \\min\\{c, \\rho_t\\}$ and $\\rho_t$ is the importance sampling term. $Q$ and $V$ in the above equation are the estimate of action-value and state-value respectively. \n\nTo estimate $Q$, the authors used a similar architecture as A3C except that the final layer outputs $Q-$values instead of state-values $V$.\n\nTo train $Q$, the authors used the $Q^{\\text{ret}}$.\n\n** Reducing the variance because of importance sampling in the above equation:** The authors used a technique called *importance weight truncation with bias correction* to keep the variance bounded in the policy gradient equation. Mainly; they use the following identity:\n$$\n\\begin{array}{ccc}\n&&\\mathbb{E}_{x_t \\sim \\beta_\\mu, a_t \\sim \\mu}[\\rho_t \\nabla_{\\theta} \\log \\pi(a_t | x_t) Q^{\\pi}(x_t, a_t)] \\\\\n&=& \\mathbb{E}_{x_t \\sim \\beta_\\mu}\\left[ \\mathbb{E}_{a_t \\sim \\mu}[\\bar{\\rho}_t \\nabla_{\\theta} \\log \\pi(a_t | x_t) Q^{\\pi}(x_t, a_t)] \\right] \\\\\n&+& \\mathbb{E}_{a\\sim \\pi}\\left[\\left[\\frac{\\rho_t(a) - c}{\\rho_t(a)}\\right] \\nabla_{\\theta} \\log\\pi_{\\theta}(a | x_t) Q^{\\pi}(x_t, a)\\right]\n\\end{array}\n$$\nNote that in the above identity, the variance in the both the terms on the right hand side is bounded. \n\n** Results: ** The authors showed that by using the off-line data, they were able to match the performance of best DQN agent with the less data and the same amount of computation. \n\n**Continuous task: ** The authors used a stochastic duelling architecture for tasks having continuous action spaces while utilizing the innovation of discrete cases.  \n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1611.01224"
    },
    "123": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MnihBMGLHSK16",
        "transcript": "Most of the Q-learning methods such as DQN or Duelling network relies on Experience Replay. However, experience replay is memory intensive. Experience replay also forces us to use only offline learning algorithm such as Q-learning. Authors suggest to use multiple agents in parallel.  These multiple agents update a shared global parameters. The **benefits** of using multiple agents are as following:\n\n1. The use of multiple agents provides a stabilizing effect.\n2. Learning can be much faster without using GPUs. It is possible to run the agents as a CPU thread. Learning is faster because more updates are being made and more data is being consumed in the same time because of multiple agents. \n3. Learning is more robust and stable because there exist a wide range of learning rates and initial weights for which a good score can be achieved. \n\n**Key Points**\n\n1. The best performing algorithm is Asynchronous Advantage Actor Critic (A3C).\n2. A3C uses $n-$step updates to tradeoff between bias and variance in the policy gradient. Essentially, the policy-gradient update is proportional to\n$$\n \\nabla \\log \\pi(a_t | x_t;  \\theta) (r_t + \\gamma r_{t+1}+\\ldots + \\gamma^{n-1}r_{t+n-1} + \\gamma^n V(x_{t+n}) - V(x_t)) \n$$\nwhere $V(\\cdot)$ is the value function of the underlying MDP.\n3.  All the parameters in the value network and policy network are shared except the last layer that exclusively predict the action-probabilities and values.\n4. The authors found that the use of an entropy bonus helped the network to not converge into sub-optimal policies. \n5. The hyper-parameters (learning rate and gradient-norm clipping) were chosen by a random search on 6 games and keep fixed for the rest of the games.\n6. A3C-LSTM also incorporates a LSTM layer with 128 cells. Each cell outputs  the action probabilities and value function. \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.01783"
    },
    "124": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=series/lncs/MartensS12",
        "transcript": "## Very Short Summary\nThe authors introduce a number of modifications to traditional hessian-free optimisation that makes the method work better for neural networks. The modifications are:\n* Use the Generalised Gauss Newton Matrix (GGN) rather than the Hessian.\n* Damp the GGN so that $G' = G + \\lambda I$ and adjust $\\lambda$ using levenberg-marquardt heuristic.\n* Use an efficient recursion to calculate the GGN.\n* Initialise each round of conjugated gradients with the final vector of the previous iteration.\n* A new simpler termination criterion for CG. Terminate CG when the relative decrease in the objective falls below some threshold.\n* Back-tracking of the CG solution. ie you store intermediate solutions to CG and only update if the new CG solution actually decreases the over all problem objective.\n\n## Less Short Summary\n\n### Hessian Free Optimisation in General\nHessian free optimisation is used when one wishes to optimise some objective $f(\\theta)$ using second order methods but inversion or even computation of the Hessian is intractable or infeasible. The method is an iterative method and at each iteration, we take a second order approximation to the objective. i.e at iterantion n, we take a second order taylor expansion of $f$ to get:\n\n$M^n(\\theta) = f(\\theta^n) + \\nabla_{\\theta}^Tf(\\theta^n)(\\theta - \\theta^n) + (\\theta - \\theta^n)^TH(\\theta - \\theta^n) $\n\nWhere $H$ is the hessian matrix. If we minimise this second order approximation with respect to $\\theta$ we would find that that $\\theta^{n+1} = H^{-1}(-\\nabla_{\\theta}^Tf(\\theta^n))$. However, inverting $H$ is usually not possible for even moderately sized neural networks.\n\nThere does however exist an efficient algorithm for calculating hessian vector products $Hv$ for any $v$. The insight of hessian-free optimisation is that one can solve linear problems of the form $Hx = v$ using only hessian vector products via the linear conjugated gradients algorithm. You therefore avoid the need to ever actually compute either the Hessian or its inverse.\n\nTo run vanilla hessian free all you need to do at each iteration is:\n\n1) Calculate the gradient vector using standard backprop.\n\n2) Calculate $H\\theta$ product using an efficient recursion.\n\n3) calculate the next update $\\theta^{n+1} = ConjugatedGradients(H, -\\nabla_{\\theta}^Tf(\\theta^n))$\n\nThe main contribution of this paper is to take the above algorithm and make the changes outlined in the very short summary.\n\n## Take aways\nHessian-Free optimisation was perhaps the best method at the time of publication. Recently it seems that first order methods using per-parameter learning rates like ADAM or even learning-to-learn can outperform Hessian-Free. This is primarily because of the increased cost per iteration of Hessian Free. However it still seems that using curvature information if its available is beneficial though expensive.\n\nMore resent second order curvature appoximations like Kroeniker Factored Approximate Curvature (KFAC) and Kroeniker Factored Recursive Approximation (KFRA) are cheaper ways to achieve the same benefit.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-642-35289-8_27"
    },
    "125": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.00071",
        "transcript": "## Headline Summary\nThe problem of vanishing or exploding gradients when training recurrent neural networks can be partially overcome by enforcing an orthogonality constraint on the weight matrices between consecutive hidden states. This paper explores the trade-off between enforced orthogonality and the rate of learning by slowly increasing the extent to which the weight matrices are allowed to deviate from orthogonality.  They find that a hard orthogonality constraint can be too restrictive and perseveres gradients at the cost of slowing learning.\n\n### Why orthogonal matrices are likely to help with exploding gradients\n\nFor a  neural network  with N hidden layers we can write the gradient of the loss wrt to the weight matrix between hidden states $W_i$ as: \n\n$ \\frac{\\partial L}{\\partial W_i} =  \\frac{\\partial a_i}{\\partial w_i} \\prod_j^N (\\frac{\\partial h_i}{\\partial a_i} \\frac{\\partial a_i}{\\partial h_i}) \\frac{\\partial L}{\\partial a_{N+1}} = \\frac{\\partial a_i}{\\partial w_i} \\prod_j^N (D_i W_{j+1}) \\frac{\\partial L}{\\partial a_{N+1}}$\n\nwhere L is the loss function and D is the jacobian of the activation function. It's in this long chain of products that the gradient tends to either vanish or explode. This problem is particularly bad in recurrent nets where the chain is the length of the unrolled computation graph.\n\nThe norm of the derivative $\\|\\frac{\\partial h_i}{\\partial a_i} \\frac{\\partial a_i}{\\partial h_i} \\|= \\|D_i W_{j+1}\\| \\leq  \\|D_i \\| \\| W_{j+1}\\| \\leq \\lambda_D \\lambda_W$ \n\nwhere $\\lambda_{D/W}$ is the largest singular value of the given matrix. Its clear that if $\\lambda$ is less than or greater than 1, that the norm of the gradient will grow or decay exponentially with length of the chain of products given above.\n\nConstraining the weight matrices to be orthogonal ensures that the singular values are 1 and so perseveres the gradient norm.\n\n### How to enforce the orthogonality constraints\n\nThe set of all orthogonal matrices from a continuos manifold known as the stiefel manifold. One way to ensure that the weight matrices remain orthogonal is to initialise them to be orthogonal and then to perform gradient optimisation on this manifold. The orthogonality of the matrix can be preserved by a Caley transformation outlined in the paper. \n\nIn order to allow the weight  matrix to deviate slightly from the stiefel manifold, the authors took the SVD of $W = USV^T $ and parameterised the singular values. They used geodesic gradient descent to maintain the orthogonality of $U$ and $V$. The singular values were restricted to between $ 1 ^+_- m$ where  $m$ is a margin by parameterisation with a sigmoid.\n\n### Experiment Summary\n\nThey experiment on a number of synthetic and real data-sets with elman and factorial RNNs. The synthetic tasks are the hochrieter memory and addition tasks, classifying sequential mnist and the real task is a language modelling task using the Penn Tree Bank dataset.\n\nIn the synthetic memory tasks they found that pure othogonalisation hindered performance but that a small margin away from orthogonality was better than no orthogonal constraint.\n\nOn the mnist tasks, the LSTM outperformed all other versions of simple RNNs but the simple RNNs with orthogonal constraints did quite well. Orthonal initialisation outperformed identity and glorot initialisation.\n\nOn the language modelling task with short sentences, orthogonal constraints were detrimental but on the longer sentences beneficial. They did not compare to LSTM. They also found that orthogonal initialisation was almost as good as orthogonal constraints during learning suggesting that preserving gradient flow is most important early in learning.\n\n### My Two Cents\nThis paper explores an interesting approach to learning long term dependencies even with very simple RNNs. However, as theoretically appealing as the results may be, they still fail to match the memory capacities of an LSTM so for the time being it doesn't seem that there are any major practical take aways.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1702.00071"
    },
    "126": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ImTB16",
        "transcript": "### Brief Summary:\n\nDoes what it says on the tin. The authors examine the loss surface of the Network-In-Network and VGG neural networks near the critical points found by 5 different stochastic optimisation algorithms. These algorithms are Adam, Adadelta, SGD, SGD with momentum and a new Runge-Kutta based optimiser. They examine the loss function by interpolating along lines between the initial and final parameter values.\n\nThey find that the different algorithms find qualitatively genuinely different types of minima. They find that the basin around the minima of ADAM is bigger than those for other optimisation algorithms. They find also that all of the minima are similarly good.\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.04010"
    },
    "127": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.04649",
        "transcript": "#### Very Brief Summary:\n\nThis paper combines stochastic variational inference with memory-augmented recurrent neural networks. The authors test 4 variants of their models against the Variational Recurrent Neural Network on 7 artificial tasks requiring long term memory. The reported log-likelihood lower bound is not obviously improved by the new models on all tasks but is slightly better on tasks requiring high capacity memory.\n\n#### Slightly Less Brief Summary:\n\nThe authors propose a general class of generative models for time-series data with both deterministic and stochastic latents. The deterministic latents, $h_t$, evolve as a recurrent net with augmented memory and the stochastic latents, $z_t$ are gaussians whose mean and variance are a deterministic function of $h_t$. The observations at each time-step $x_t$ are also gaussians whose mean and variance are parametrised by a function of $h_{<t}, x_{<t}$.\n\n#### Generative Temporal Models without Augmented Memory:\n\nThe family of generative temporal models is fairly broad and includes kalman filters, non-linear dynamical systems, hidden-markov models and switching state-space models. More recent non-linear models such as the variational RNN are most similar to the new models in this paper. In general all of the mentioned temporal models can be written as: \n\n\n$P_\\theta(x_{\\leq T}, z_{\\leq T} ) = \\prod_t P_\\theta(x_t | f_x(z_{\\leq t}, x_{\\leq t}))P_\\theta(z_t | f_z(z_{\\leq t}, x_{\\leq t}))$\n\nThe differences between models then come from the the exact forms of  $f_x$ and $f_z$ with most models making strong conditional independence assumptions and/or having linear dependence. For example in a Gaussian State Space model both $f_x$ and $f_z$ are linear, the latents form a first order Markov chain and the observations $x_t$ are conditionally independent of everything given $z_t$.\n\nIn the Variational Recurrent Neural Net (VRNN) an additional deterministic latent variable $h_t$ is introduced and at each time-step $x_t$ is the output of a VAE whose prior $z_t$ is conditioned on $h_t$. $h_t$ evolves as an RNN.\n\n#### Types of Model with Augmented Memory:\n\nThis paper follows the same strategy as the VRNN but adds more structure to the underlying recurrent neural net. The authors motivate this by saying that the VRNN \"scales poorly when higher capacity storage is required\".\n\n* \"Introspective\" Model: In the first augmented memory model, the deterministic latent M_t is simply a concatenation of the last $L$ latent stochastic variables $z_t$. A soft method of attention over the latent memory is used to generate a \"memory context\" vector at each time step. The observed output $x_t$ is a gaussian with mean and variance parameterised by the \"memory context' and the stochastic latent $z_t$. Because this model does not learn to write to memory it is faster to train.\n\n* In the later models the memory read and write operations are the same as those in the neural turing machine or differentiable neural computer.\n\n#### My Two Cents:\n\nIn some senses this paper feels fairly inevitable since VAE's have already been married with RNNs and so it's a small leap to add augmented memory.\n\nThe actual read write operations introduced in the \"introspective\" model feel a little hacky and unprincipled.\n\nThe actual images generated are quite impressive.\n\nI'd like to see how these kind of models do on language generation tasks and wether they can be adapted for question answering.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1702.04649"
    },
    "128": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/isbi45749.2020.9098686",
        "transcript": "The reconstruction of high-fidelity resolution brain MR images is especially challenging because of the highly complex brain structure. Most promising approaches for this task are autoencoders and generative models such as Variational Autoencoders (VAE) or Generative Adversarial Networks (GAN). In Unsupervised Anomaly Detection (UAD), these architectures are only trained with images of healthy brain anatomy and not with images containing anomalies such as lesions. Therefore, processing an anomalous input image $\\pmb{x}$ with an architecture trained to reconstruct healthy MR images should produce a high reconstruction error $r$ indicating that the input MR image is likely to exhibit anomalies.\n\nSo far developed models for this task are either limited to low resolution reconstructions so that quite a lot of information is lost or to only small image regions. Baur et al. hypothesize that these restrictions are caused by the low dimensionality of the latent space of the models even though a high capacity would be necessary to reconstruct highly complex brain MR images. Thus, Baur et al. introduce skip connections in the autoencoder to enable detailed high resolution reconstructions due to the enhanced gradient flow. In addition, the application of dropout on the skip connections shall prevent the model from learning the identity of the input image since only the corresponding healthy anatomy of the possibly anomalous input image shall be reconstructed. Sampling the networks parameter $\\pmb{\\theta}$ from a Bernoulli distribution $\\pmb{\\theta} \\sim \\mathcal{B}(p)$ with $p$ being the dropout rate  also turns the Skip-Autoencoder into a Bayesian neural network by using a Monte Carlo dropout at test time.\nThe proposed Skip Autoencoder and the Bayesian Skip-Autoencoder are tested on Mutiple Sclerosis and Glioblastoma datasets and evaluated using the Precision-Recall-Curve (PRC), the AUPRC, and the Dice Score. An investigation of the Bernoulli dropout yields that no input identity is learnt independent on the dropout. A random weight initialization seems to be sufficient. Considering the skip-connections, the best performance is achieved by a single skip-connection close to the bottleneck layer. Applying more skip connections improves the performance on the Glioblastoma test set but reduces the performance on the Multiple Sclerosis test set.\n\nIn general, the non-Bayesian Skip-Autoencoder exceeds the performance of the Bayesian Skip-Autoencoder because in the calculation of the reconstruction residual $r = \\rvert \\pmb{x} - \\pmb{\\hat{x}} \\rvert$ for the Bayesian Skip-Autoencoder the predictive mean of $n$ Monte Carlo samples is applied as reconstructed image $\\pmb{\\hat{x}}$ which causes a blurriness in the reconstructions  $\\pmb{\\hat{x}}$. However, the Bayesian Skip-Autoencoder enables to quantify the epistemic uncertainty because the pixels of hyperintense anomalies have a low variance compared to normal tissue.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/isbi45749.2020.9098686"
    },
    "129": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/ac/Rasmussen03",
        "transcript": "In this tutorial paper, Carl E. Rasmussen gives an introduction to Gaussian Process Regression focusing on the definition, the hyperparameter learning and future research directions.\n\nA Gaussian Process is completely defined by its mean function $m(\\pmb{x})$ and its covariance function (kernel) $k(\\pmb{x},\\pmb{x}')$. The mean function $m(\\pmb{x})$ corresponds to the mean vector $\\pmb{\\mu}$ of a Gaussian distribution whereas the covariance function $k(\\pmb{x}, \\pmb{x}')$ corresponds to the covariance matrix $\\pmb{\\Sigma}$. Thus, a Gaussian Process $f \\sim \\mathcal{GP}\\left(m(\\pmb{x}), k(\\pmb{x}, \\pmb{x}')\\right)$ is a generalization of a Gaussian distribution over vectors to a distribution over functions. A random function vector $\\pmb{\\mathrm{f}}$ can be generated by a Gaussian Process through the following procedure:\n\n1. Compute the components $\\mu_i$ of the mean vector $\\pmb{\\mu}$ for each input $\\pmb{x}_i$ using the mean function $m(\\pmb{x})$\n2. Compute the components $\\Sigma_{ij}$ of the covariance matrix $\\pmb{\\Sigma}$ using the covariance function $k(\\pmb{x}, \\pmb{x}')$\n3. A function vector $\\pmb{\\mathrm{f}} = [f(\\pmb{x}_1), \\dots, f(\\pmb{x}_n)]^T$ can be drawn from the Gaussian distribution $\\pmb{\\mathrm{f}} \\sim \\mathcal{N}\\left(\\pmb{\\mu}, \\pmb{\\Sigma} \\right)$\n\nApplying this procedure to regression, means that the resulting function vector $\\pmb{\\mathrm{f}}$ shall be drawn in a way that a function vector $\\pmb{\\mathrm{f}}$ is rejected if it does not comply with the training data $\\mathcal{D}$. This is achieved by conditioning the distribution on the training data $\\mathcal{D}$ yielding the posterior Gaussian Process $f \\rvert \\mathcal{D} \\sim \\mathcal{GP}(m_D(\\pmb{x}), k_D(\\pmb{x},\\pmb{x}'))$ for noise-free observations with the posterior mean function $m_D(\\pmb{x}) = m(\\pmb{x}) + \\pmb{\\Sigma}(\\pmb{X},\\pmb{x})^T \\pmb{\\Sigma}^{-1}(\\pmb{\\mathrm{f}} - \\pmb{\\mathrm{m}})$  and the posterior covariance function $k_D(\\pmb{x},\\pmb{x}')=k(\\pmb{x},\\pmb{x}') - \\pmb{\\Sigma}(\\pmb{X}, \\pmb{x}')$ with $\\pmb{\\Sigma}(\\pmb{X},\\pmb{x})$ being a vector of covariances between every training case of $\\pmb{X}$ and $\\pmb{x}$.\n\nNoisy observations $y(\\pmb{x}) = f(\\pmb{x}) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,\\sigma_n^2)$ can be taken into account with a second Gaussian Process with mean $m$ and covariance function $k$ resulting in $f \\sim \\mathcal{GP}(m,k)$ and $y \\sim \\mathcal{GP}(m, k + \\sigma_n^2\\delta_{ii'})$. The figure illustrates the cases of noisy observations (variance at training points) and of noise-free observationshttps://i.imgur.com/BWvsB7T.png (no variance at training points).\n\nIn the Machine Learning perspective, the mean and the covariance function are parametrised by hyperparameters and provide thus a way to include prior knowledge e.g. knowing that the mean function is a second order polynomial. To find the optimal hyperparameters $\\pmb{\\theta}$, \n\n1. determine the log marginal likelihood $L= \\mathrm{log}(p(\\pmb{y} \\rvert \\pmb{x}, \\pmb{\\theta}))$,\n2. take the first partial derivatives of $L$ w.r.t. the hyperparameters, and\n3. apply an optimization algorithm.\n\nIt should be noted that a regularization term is not necessary for the log marginal likelihood $L$ because it already contains a complexity penalty term. Also, the tradeoff between data-fit and penalty is performed automatically.\n\nGaussian Processes provide a very flexible way for finding a suitable regression model. However, they require the high computational complexity $\\mathcal{O}(n^3)$ due to the inversion of the covariance matrix. In addition, the generalization of Gaussian Processes to non-Gaussian likelihoods remains complicated. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q38665406"
    },
    "130": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1903-11981",
        "transcript": "The typical model based reinforcement learning (RL) loop consists of collecting data, training a model of the environment, using the model to do model predictive control (MPC). If however the model is wrong, for example for state-action pairs that have been barely visited, the dynamics model might be very wrong and the MPC fails as the imagined model and the reality align to longer. Boney et a. propose to tackle this with a denoising autoencoder for trajectory regularization according to the familiarity of a trajectory. \n\nMPC uses at each time t the learned model $s_{t+1} = f_{\\theta}(s_t, a_t)$  to select a plan of actions, that is maximizing the sum of expected future reward:\n$\nG(a_t, \\dots, a_{t+h}) = \\mathbb{E}[\\sum_{k=t}^{t+H}r(o_t, a_t)] ,$\n\n where $r(o_t, a_t)$ is the observation and action dependent reward. The plan obtained by trajectory optimization is subsequently unrolled for H steps. \n\nBoney et al. propose to regularize trajectories by the familiarity of the visited states leading to the regularized objective:\n$G_{reg} = G + \\alpha \\log p(o_k, a_k, \\dots, o_{t+H}, a_{t+H})\n$\nInstead of regularizing over the whole trajectory they propose to regularize over marginal probabilities of windows of length w:\n$G_{reg} = G + \\alpha \\sum_{k = t}^{t+H-w} \\log p(x_k), \\text{ where } x_k = (o_k, a_k, \\dots, o_{t+w}, a_{t+w}).$\nInstead of explicitly learning a generative model of the familiarity $p(x_k)$ a denoising auto-encoder is used that approximates instead the derivative of the log probability density $\\frac{\\delta}{\\delta x} \\log p(x)$. This allows the following back-propagation rule:\n$ \\frac{\\delta G_{reg}}{\\delta_i} = \\frac{\\delta G}{\\delta a_i} + \\alpha \\sum_{k = i}^{i+w} \\frac{\\delta x_k}{\\delta a_i} \\frac{\\delta}{\\delta x} \\log p(x).$\n\nThe experiments show that the proposed method has competitive sample-efficiency. For example on Halfcheetah the asymptotic performance of PETS is not matched.\n\nThis is due to the biggest limitation of this approach, the hindering of exploration. Penalizing the unfamiliarity of states is in contrast to approaches like optimism in the face of uncertainty, which is a core principle of exploration. Aiming to avoid states of high unfamiliarity, the proposed method is the precise opposite of curiosity driven exploration. The appendix proposes preliminary experiments to account for exploration. I would expect, that the pure penalization of unfamiliarity works best in a batch RL setting, which would be an interesting extension of this work.  \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.11981"
    },
    "131": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/KumarFSTL19",
        "transcript": "Kumar et al. propose an algorithm to learn in batch reinforcement learning (RL), a setting where an agent learns purely form a fixed batch of data, $B$, without any interactions with the environments. The data in the batch is collected according to a batch policy $\\pi_b$. Whereas most previous methods (like BCQ) constrain the learned policy to stay close to the behavior policy, Kumar et al. propose bootstrapping error accumulation reduction (BEAR), which constrains the newly learned policy to place some probability mass on every non negligible action. \nThe difference is illustrated in the picture from the BEAR blog post:\nhttps://i.imgur.com/zUw7XNt.png\nThe behavior policy is in both images the dotted red line, the left image shows the policy matching where the algorithm is constrained to the purple choices, while the right image shows the support matching. \n\n**Theoretical Contribution:**\nThe paper analysis formally how the use of out-of-distribution actions to compute the target in the Bellman equation influences the back-propagated error.\n\nFirstly a distribution constrained backup operator is defined as\n$T^{\\Pi}Q(s,a) = \\mathbb{E}[R(s,a) + \\gamma \\max_{\\pi \\in \\Pi} \\mathbb{E}_{P(s' \\vert s,a)} V(s')]$ and $V(s) = \\max_{\\pi \\in \\Pi} \\mathbb{E}_{\\pi}[Q(s,a)]$\nwhich considers only policies $\\pi \\in \\Pi$. \n\nIt is possible that the optimal policy $\\pi^*$ is not contained in the policy set $\\Pi$, thus there is a suboptimallity constant $\\alpha (\\Pi) = \\max_{s,a} \\vert \\mathcal{T}^{\\Pi}Q^{*}(s,a) - \\mathcal{T}Q^{*}(s,a) ]\\vert $ which captures how far $\\pi^{*}$ is from $\\Pi$. \n\nLetting $P^{\\pi_i}$ be the transition-matrix when following policy $\\pi_i$, $\\rho_0$ the state marginal distribution of the training data in the batch and $\\pi_1, \\dots, \\pi_k \\in \\Pi $. The error analysis relies upon a concentrability assumption $\\rho_0 P^{\\pi_1} \\dots P^{\\pi_k} \\leq c(k)\\mu(s)$, with $\\mu(s)$ the state marginal. Note that $c(k)$ might be infinite if the support of $\\Pi$ is not contained in the state marginal of the batch. Using the coefficients $c(k)$ a concentrability coefficient is defined as:\n$C(\\Pi) = (1-\\gamma)^2\\sum_{k=1}^{\\infty}k \\gamma^{k-1}c(k).$\nThe concentrability takes values between 1 und $\\infty$, where 1 corresponds to the case that the batch data were collected by $\\pi$ and $\\Pi = \\{\\pi\\}$ and $\\infty$ to cases where $\\Pi$ has support outside of $\\pi$. \n\nCombining this Kumar et a. get a bound of the Bellman error for distribution constrained value iteration with the constrained Bellman operator $T^{\\Pi}$:\n\n$\\lim_{k \\rightarrow \\infty} \\mathbb{E}_{\\rho_0}[\\vert V^{\\pi_k}(s)- V^{*}(s)]  \\leq  \\frac{\\gamma}{(1-\\gamma^2)} [C(\\Pi) \\mathbb{E}_{\\mu}[\\max_{\\pi \\in \\Pi}\\mathbb{E}_{\\pi}[\\delta(s,a)] + \\frac{1-\\gamma}{\\gamma}\\alpha(\\Pi)  ] ]$, where $\\delta(s,a)$ is the Bellman error. \n\nThis presents the inherent batch RL trade-off between keeping policies close to the behavior policy of the batch (captured by $C(\\Pi)$ and keeping $\\Pi$ sufficiently large (captured by $\\alpha(\\Pi)$). \n\nIt is finally proposed to use support sets to construct $\\Pi$, that is $\\Pi_{\\epsilon} = \\{\\pi \\vert \\pi(a \\vert s)=0 \\text{ whenever } \\beta(a \\vert s) < \\epsilon \\}$. This amounts to the set of all policies that place probability on all non-negligible actions of the behavior policy. For this particular choice of $\\Pi = \\Pi_{\\epsilon}$ the concentrability coefficient can be bounded. \n\n**Algorithm**:\nThe algorithm has an actor critic style, where the Q-value to update the policy is taken to be the minimum over the ensemble. The support constraint to place at least some probability mass on every non negligible action from the batch is enforced via sampled MMD. The proposed algorithm is a member of the policy regularized algorithms as the policy is updated to optimize:\n$\\pi_{\\Phi} = \\max_{\\pi} \\mathbb{E}_{s \\sim B} \\mathbb{E}_{a \\sim \\pi(\\cdot \\vert s)} [min_{j = 1 \\dots, k} Q_j(s,a)] s.t. \\mathbb{E}_{s \\sim B}[MMD(D(s), \\pi(\\cdot \\vert s))] \\leq \\epsilon$\n\nThe Bellman target to update the Q-functions is computed as the convex combination of minimum and maximum of the ensemble. \n\n**Experiments**\nThe experiments use the Mujoco environments Halfcheetah, Walker, Hopper and Ant. Three scenarios of batch collection, always consisting of 1Mio. samples,  are considered:\n- completely random behavior policy\n- partially trained behavior policy\n- optimal policy as behavior policy\n\nThe experiments confirm that BEAR outperforms other off-policy methods like BCQ or KL-control. The ablations show further that the choice of MMD is crucial as it is sometimes on par and sometimes substantially better than choosing KL-divergence. \n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/9349-stabilizing-off-policy-q-learning-via-bootstrapping-error-reduction"
    },
    "132": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1911-11361",
        "transcript": "Wu et al. provide a framework (behavior regularized actor critic (BRAC)) which they use to empirically study the impact of different design choices in batch reinforcement learning (RL). Specific instantiations of the framework include BCQ, KL-Control and BEAR. \n\nPure off-policy rl describes the problem of learning a policy purely from a batch $B$ of one step transitions collected with a behavior policy $\\pi_b$. The setting allows for no further interactions with the environment. This learning regime is for example in high stake scenarios, like education or heath care, desirable. \n\nThe core principle of batch RL-algorithms in to stay in some sense close to the behavior policy. The paper proposes to incorporate this firstly via a regularization term in the value function, which is denoted as **value penalty**. In this case the value function of BRAC takes the following form:\n\n$\nV_D^{\\pi}(s) = \\sum_{t=0}^{\\infty} \\gamma ^t \\mathbb{E}_{s_t \\sim P_t^{\\pi}(s)}[R^{pi}(s_t)- \\alpha D(\\pi(\\cdot\\vert s_t) \\Vert \\pi_b(\\cdot \\vert s_t)))], \n$\n\nwhere $\\pi_b$ is the maximum likelihood estimate of the behavior policy based upon $B$. \nThis results in a Q-function objective:\n$\\min_{Q} = \\mathbb{E}_{\\substack{(s,a,r,s') \\sim D \\\\ a' \\sim \\pi_{\\theta}(\\cdot \\vert s)}}\\left[(r + \\gamma \\left(\\bar{Q}(s',a')-\\alpha D(\\pi(\\cdot\\vert s) \\Vert \\pi_b(\\cdot \\vert s) \\right) - Q(s,a) \\right]  \n$\n\nand the corresponding policy update:\n$\n\\max_{\\pi_{\\theta}} \\mathbb{E}_{(s,a,r,s') \\sim D} \\left[ \\mathbb{E}_{a^{''} \\sim \\pi_{\\theta}(\\cdot \\vert s)}[Q(s,a^{''})] - \\alpha  D(\\pi(\\cdot\\vert s) \\Vert \\pi_b(\\cdot \\vert s)  \\right] \n$\n \nThe second approach is **policy regularization** . Here the regularization weight $\\alpha$ is set for value-objectives (V- and Q) to zero and is non-zero for the policy objective.\n\nIt is possible to instantiate for example the following batch RL algorithms in this setting:\n- BEAR: policy regularization with sample-based kernel MMD as D and min-max mixture of the two ensemble elements for $\\bar{Q}$\n- BCQ: no regularization but policy optimization over restricted space\n\nExtensive Experiments over the four Mujoco tasks Ant, HalfCheetah,Hopper Walker show:\n1. for a BEAR like instantiation there is a  modest advantage of keeping $\\alpha$ fixed\n2. using a mixture of a two or four Q-networks ensemble as target value yields better returns that using one Q-network\n3. taking the minimum of ensemble Q-functions is slightly better than taking a mixture (for Ant, HalfCeetah & Walker, but not for Hooper\n4. the use of value-penalty yields higher return than the policy-penalty\n5. no choice for D (MMD, KL (primal), KL(dual) or Wasserstein (dual)) significantly outperforms the other (note that his contradicts the BEAR paper where MMD was better than KL)\n6. the value penalty version consistently outperforms BEAR which in turn outperforms BCQ with improves upon a partially trained baseline. \n\nThis large scale study of different design choices helps in developing new methods. It is however surprising to see, that most design choices in current methods are shown empirically to be non crucial. This points to the importance of agreeing upon common test scenarios within a community to prevent over-fitting new algorithms to a particular setting. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1911.11361"
    },
    "133": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1903-08254",
        "transcript": "Rakelly et al. propose a method to do off-policy meta reinforcement learning (rl). The method achieves a 20-100x improvement on sample efficiency compared to on-policy meta rl like MAML+TRPO. \n\nThe key difficulty for offline meta rl arises from the meta-learning assumption, that meta-training and meta-test time match. However during test time the policy has to explore and sees as such on-policy data which is in contrast to the off-policy data that should be used at meta-training. The key contribution of PEARL is an algorithm that allows for online task inference in a latent variable at train and test time, which is used to train a Soft Actor Critic, a very sample efficient off-policy algorithm, with additional dependence of the latent variable.\n\nThe implementation of Rakelly et al. proposes to capture knowledge about the current task in a latent stochastic variable Z. A inference network $q_{\\Phi}(z \\vert c)$ is used to predict the posterior over latents given context c of the current task in from of transition tuples $(s,a,r,s')$ and trained with an information bottleneck. Note that the task inference is done on samples according to a sampling strategy sampling more recent transitions. The latent z is used as an additional input to policy $\\pi(a \\vert s, z)$ and Q-function $Q(a,s,z)$ of a soft actor critic algorithm which is trained with offline data of the full replay buffer. \n\nhttps://i.imgur.com/wzlmlxU.png \n\n\nSo the challenge of differing conditions at test and train times is resolved by sampling the content for the latent context variable at train time only from very recent transitions (which is almost on-policy) and at test time by construction on-policy. Sampling $z \\sim q(z \\vert c)$ at test time allows for posterior sampling of the latent variable, yielding efficient exploration. \n\nThe experiments are performed across 6 Mujoco tasks with ProMP, MAML+TRPO and $RL^2$ with PPO as baselines. They show:\n- PEARL is 20-100x more sample-efficient \n- the posterior sampling of the latent context variable enables deep exploration that is crucial for sparse reward settings\n- the inference network could be also a RNN, however it is crucial to train it with uncorrelated transitions instead of trajectories that have high correlated transitions\n- using a deterministic latent variable, i.e. reducing $q_{\\Phi}(z \\vert c)$ to a  point estimate, leaves the algorithm unable to solve sparse reward navigation tasks which is attributed to the lack of temporally extended exploration. \n\nThe paper introduces an algorithm that allows to combine meta learning with an off-policy algorithm that dramatically increases the sample-efficiency compared to on-policy meta learning approaches. This increases the chance of seeing meta rl in any sort of real world applications. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.08254"
    },
    "134": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1906.05374",
        "transcript": "Bechtle et al. propose meta learning via learned loss ($ML^3$) and derive and empirically evaluate the framework on classification, regression, model-based and model-free reinforcement learning tasks. \n\nThe problem is formalized as learning parameters $\\Phi$ of a meta loss function $M_\\phi$ that computes loss values $L_{learned} = M_{\\Phi}(y, f_{\\theta}(x))$. Following the outer-inner loop meta algorithm design the learned loss $L_{learned}$ is used to update the parameters of the learner in the inner loop via gradient descent:\n$\\theta_{new} = \\theta - \\alpha \\nabla_{\\theta}L_{learned} $. The key contribution of the paper is the way to construct a differentiable learning signal for the loss parameters $\\Phi$.\n\nThe framework requires to specify a task loss $L_T$ during meta train time, which can be for example the mean squared error for regression tasks. After updating the model parameters to $\\theta_{new}$ the task loss is used to measure how much learning progress has been made with loss parameters $\\Phi$. The key insight is the decomposition via chain-rule of  $\\nabla_{\\Phi} L_T(y, f_{\\theta_{new}})$:\n\n$\\nabla_{\\Phi} L_T(y, f_{\\theta_{new}}) = \\nabla_f L_t \\nabla_{\\theta_{new}}f_{\\theta_{new}} \\nabla_{\\Phi} \\theta_{new} = \\nabla_f L_t \\nabla_{\\theta_{new}}f_{\\theta_{new}} [\\theta - \\alpha \\nabla_{\\theta} \\mathbb{E}[M_{\\Phi}(y, f_{\\theta}(x))]]$.\n\nThis allows to update the loss parameters with gradient descent as: $\\Phi_{new} = \\Phi - \\eta \\nabla_{\\Phi} L_T(y, f_{\\theta_{new}})$. \n\nThis update rules yield the following $ML^3$ algorithm for supervised learning tasks:\n\nhttps://i.imgur.com/tSaTbg8.png\n\nFor reinforcement learning the task loss is the expected future reward of policies induced by the policy $\\pi_{\\theta}$, for model-based rl with respect to the approximate dynamics model and for the model free case a system independent surrogate: $L_T(\\pi_{\\theta_{new}}) = -\\mathbb{E}_{\\pi_{\\theta_{new}}} \\left[ R(\\tau_{\\theta_{new}})  \\log \\pi_{\\theta_{new}}(\\tau_{new})\\right] $. \n\nThe allows further to incorporate extra information via an additional loss term $L_{extra}$ and to consider the augmented task loss $\\beta L_T + \\gamma L_{extra} $, with weights $\\beta, \\gamma$ at train time. Possible extra loss terms are used to add physics priors, encouragement of exploratory behavior or to incorporate expert demonstrations. The experiments show that this, at test time unavailable information, is retained in the shape of the loss landscape. \n\nThe paper is packed with insightful experiments and shows that the learned loss function:\n- yields in regression and classification better accuracies at train and test tasks\n- generalizes well and speeds up learning in model based rl tasks\n- yields better generalization and faster learning in model free rl\n- is agnostic across a bunch of evaluated architectures (2,3,4,5 layers)\n- with incorporated extra knowledge yields better performance than without and is superior to alternative approaches like iLQR in a MountainCar task. \n\nThe paper introduces a promising alternative, by learning the loss parameters, to MAML like approaches that learn the model parameters. It would be interesting to see if the learned loss function generalizes better than learned model parameters to a broader distribution of tasks like the meta-world tasks. \n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1906.05374"
    },
    "135": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1910.01708",
        "transcript": "The authors propose a unified setting to evaluate the performance of batch reinforcement learning algorithms. The proposed benchmark is discrete and based on the popular Atari Domain. The authors review and benchmark several current batch RL algorithms against a newly introduced version of BCQ (Batch Constrained Deep Q Learning) for discrete environments. \n\nhttps://i.imgur.com/zrCZ173.png\n\nNote in line 5 that the policy chooses actions with a restricted argmax operation, eliminating actions that have not enough support in the batch. \n\nOne of the key difficulties in batch-RL is the divergence of value estimates. In this paper the authors use Double DQN, which means actions are selected with a value net $Q_{\\theta}$ and the policy evaluation is done with a target network $Q_{\\theta'}$ (line 6). \n\n**How is the batch created?**\nA partially trained DQN-agent (trained online for 10mio steps, aka 40mio frames) is used as behavioral policy to collect a batch $B$ containing 10mio transitions. The DQN agent uses either with probability 0.8 an $\\epsilon=0.2$ and with probability 0.2 an $\\epsilon = 0.001$. The batch RL agents are trained on this batch for 10mio steps and evaluated every 50k time steps for 10 episodes. This process of batch creation differs from the settings used in other papers in i) having only a single behavioral policy, ii) the batch size and iii) the proficiency level of the batch policy.\n\nThe experiments, performed on the arcade learning environment include DQN, REM, QR-DQN, KL-Control, BCQ, OnlineDQN and Behavioral Cloning and show that:\n- for conventional RL algorithms distributional algorithms (QR-DQN) outperform the plain algorithms (DQN)\n- batch RL algorithms perform better than conventional algorithms with BCQ outperforming every other algorithm in every tested game\n\nIn addition to the return the authors plot the value estimates for the Q-networks. A drop in performance corresponds in all cases to a divergence (up or down) in value estimates. \n\nThe paper is an important contribution to the debate about what is the right setting to evaluate batch RL algorithms. It remains however to be seen if the proposed choice of i) a single behavior policy, ii) the batch size and iii) quality level of the behavior policy will be accepted as standard. Further work is in any case required to decide upon a benchmark for continuous domains.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1910.01708"
    },
    "136": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/FujimotoMP19",
        "transcript": "Interacting with the environment comes sometimes at a high cost, for example in high stake scenarios like health care or teaching. Thus instead of learning online, we might want to learn from a fixed buffer $B$ of transitions, which is filled in advance from a behavior policy.\nThe authors show that several so called off-policy algorithms, like DQN and DDPG fail dramatically in this pure off-policy setting. \n\nThey attribute this to the extrapolation error, which occurs in the update of a value estimate $Q(s,a)$, where the target policy selects an unfamiliar action $\\pi(s')$ such that $(s', \\pi(s'))$ is unlikely or not present in $B$. Extrapolation error is caused by the mismatch between the true state-action visitation distribution of the current policy and the state-action distribution in $B$ due to:\n- state-action pairs (s,a) missing  in $B$, resulting in arbitrarily bad estimates of $Q_{\\theta}(s, a)$ without sufficient data close to (s,a). \n- the finiteness of the batch of transition tuples $B$, leading to a biased estimate of the transition dynamics in the Bellman operator $T^{\\pi}Q(s,a) \\approx \\mathbb{E}_{\\boldsymbol{s' \\sim B}}\\left[r + \\gamma Q(s', \\pi(s')) \\right]$\n- transitions are sampled uniformly from $B$, resulting in a loss weighted w.r.t the frequency of data in the batch: $\\frac{1}{\\vert B \\vert} \\sum_{\\boldsymbol{(s, a, r, s') \\sim B}} \\Vert r + \\gamma Q(s', \\pi(s')) - Q(s, a)\\Vert^2$\n\nThe proposed algorithm Batch-Constrained deep Q-learning (BCQ) aims to choose actions that:\n 1. minimize distance of taken actions to actions in the batch\n 2. lead to states contained in the buffer\n 3.  maximizes the value function,\n\nwhere 1. is prioritized over the other two goals to mitigate the extrapolation error. \n\nTheir proposed algorithm (for continuous environments) consists informally of the following steps that are repeated at each time $t$:\n 1. update generator model of the state conditional marginal likelihood $P_B^G(a \\vert s)$\n 2. sample n actions form the generator model\n 3. perturb each of the sampled actions to lie in a range $\\left[-\\Phi, \\Phi \\right]$\n 4. act according to the argmax of respective Q-values of perturbed actions\n 5. update value function\n\nThe experiments considers Mujoco tasks with four scenarios of batch data creation:\n - 1 million time steps from training a DDPG agent with exploration noise $\\mathcal{N}(0,0.5)$ added to the action.This aims for a diverse set of states and actions. \n - 1 million time steps from training a DDPG agent with an exploration noise $\\mathcal{N}(0,0.1)$ added to the actions as behavior policy. The batch-RL agent and the behavior DDPG are trained concurrently from the same buffer. \n - 1 million transitions from rolling out a already trained DDPG agent\n - 100k transitions from a behavior policy that acts with probability 0.3 randomly and follows otherwise an expert demonstration with added exploration noise  $\\mathcal{N}(0,0.3)$\n\nI like the fourth choice of behavior policy the most as this captures high stake scenarios like education or medicine the closest, in which training data would be acquired by human experts that are by the nature of humans not optimal but significantly better than learning from scratch. \n\nThe proposed BCQ algorithm is the only algorithm that is successful across all experiments. It matches or outperforms the behavior policy. Evaluation of the value estimates showcases unstable and diverging value estimates for all algorithms but BCQ that exhibits a stable value function. \n\nThe paper outlines a very important issue that needs to be tackled in order to use reinforcement learning in real world applications. ",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v97/fujimoto19a.html"
    },
    "137": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1905.12558",
        "transcript": "The authors analyse in the very well written paper the relation between Fisher $F(\\theta) = \\sum_n \\mathbb{E}_{p_{\\theta}(y \\vert x)}[\\nabla_{\\theta} \\log(p_{\\theta}(y \\vert x_n))\\nabla_{\\theta} \\log(p_{\\theta}(y \\vert x_n))^T] $ and empirical Fisher $\\bar{F}(\\theta) = \\sum_n [\\nabla_{\\theta} \\log(p_{\\theta}(y_n \\vert x_n))\\nabla_{\\theta} \\log(p_{\\theta}(y_n \\vert x_n))^T] $, which has recently seen a surge in interest. . The definitions differ in that $y_n$ is a training label instead of a sample of the model $p_{\\theta}(y \\vert x_n)$, thus even so the name suggests otherwise $\\bar{F}$ is not a empirical, for example Monte Carlo, estimate of the Fisher. The authors rebuff common arguments used to justify the use of the empirical fisher by an amendment to the generalized Gauss-Newton, give conditions when the empirical Fisher does indeed approach the Fisher and give an argument why the empirical fisher might work in practice nonetheless.   \n\nThe Fisher, capturing the curvature of the parameter space, provides information about the geometry of the parameters pace, the empirical Fisher might however fail so capture the curvature as the striking plot from the paper shows:\nhttps://i.imgur.com/c5iCqXW.png\n\nThe authors rebuff the two major justifications for the use of empirical Fisher:\n1. \"the empirical Fisher matches the construction of a generalized Gauss-Newton\"\n * for the log-likelihood $log(p(y \\vert f) = \\log \\exp(-\\frac{1}{2}(y-f)^2))$ the generalized Gauss-Newton intuition that small residuals $f(x_n, \\theta) - y_n$ lead to a good approximation of the Hessian is not satisfied. Whereas the Fisher approaches the Hessian, the empirical Fisher approaches 0\n2. \"the empirical Fisher converges to the true Fisher when the model is a good fit for the data\"\n * the authors sharpen the argument to \"the empirical Fisher converges at the minimum to the Fisher as the number of samples grows\", which is unlikely to be satisfied in practice. \n\nThe authors provide an alternative perspective on why the empirical Fisher might be successful, namely to adapt the gradient to the gradient noise in stochastic optimization. The empirical Fisher coincides with the second moment of the stochastic gradient estimate and encodes as such covariance information about the gradient noise. This allows to reduce the effects of gradient noise by scaling back the updates in high variance aka noise directions. ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1905.12558"
    },
    "138": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=grathwohl2020cutting",
        "transcript": "The authors introduce a new, sampling-free method for training and evaluating energy-based models (aka EBMs, aka unnormalized density models). There are two broad approches for training EBMs. Sampling-based approaches like contrastive divergence try to estimate the likelihood with MCMC, but can be biased if the chain is not sufficiently long. The speed of training also greatly depends on the sampling parameters. Other approches, like score matching, avoid sampling by solving a surrogate objective that approximates the likelihood. However, using a surrogate objective also introduces bias in the solution. In any case, comparing goodness of fit of different models is challenging, regardless of how the models were trained. \n\nThe authors introduce a measure of probability distance between distributions $p$ and $q$ called the Learned Stein Discrepancy ($LSD$): $$ LSD(f_{\\phi}, p, q)\n = \\mathbb{E}_{p(x)} [\\nabla_x \\log q(x)^T f_{\\phi}(x) + Tr(\\nabla_x f_{\\phi} (x)) $$ This measure is derived from the Stein Discrepancy $SD(p,q)$. Note that like the $SD$, the $LSD$ is 0 iff $p = q$. Typically, $p$ is the data distribution and $q$ is the learned approximate distribution (an EBM), although this doesn't have to be the case. Note also that this objective only requires a differentiable unnormalized distribution $\\tilde{q}$, and does not require MCMC sampling or computation of the normalizing constant $Z$, since $\\nabla_x \\log q(x) = \\nabla_x \\log \\tilde{q}(x) - \\nabla_x \\log Z = \\nabla_x \\log \\tilde{q}(x)$.\n\n$f_\\phi$ is known as the critic function, and minimizing the $LSD$ with respect to $\\phi$ (i.e. with gradient descent) over a bounded space of functions $\\mathcal{F}$ can approximate the $SD$ over that space. The authors choose to define the function space $\\mathcal{F} = \\{ f: \\mathbb{E}_{p(x)} [f(x)^Tf(x)] < \\infty \\}$, which is convenient because it can be optimized by introducing a simple L2 regularizer on the critic's output: $\\mathcal{R}_\\lambda (f_\\phi) = \\lambda \\mathbb{E}_{p(x)} [f_\\phi(x)^T f_\\phi(x)]$. \n\nSince the trace of a matrix is expensive to backpropagate through, the authors use a single-sample Monte Carlo estimate $Tr(\\nabla_x f_\\phi(x)) \\approx \\mathbb{E}_{\\mathbb{N}(\\epsilon|0,1)} [\\epsilon^T \\nabla_x f_\\phi(x) \\epsilon] $, which is more efficient since $\\epsilon^T \\nabla_x f_\\phi(x)$ is a vector-Jacobian product. \n\nThe overall objective is thus the following: $$ \\text{arg} \\max_\\phi  \\mathbb{E}_{p(x)} [\\nabla_x \\log q(x)^T f_{\\phi}(x) +  \\mathbb{E}_{\\epsilon} [\\epsilon^T \\nabla_x f_{\\phi} (x) \\epsilon)] -  \\lambda f_\\phi(x)^T f_\\phi(x)] $$\n\nIt is possible to compare two different EBMs $q_1$ and $q_2$ by optimizing the above objective for two different critic parameters $\\phi_1$ and $\\phi_2$, using the training and validation data for critic optimization (then evaluating on the held-out test set). Note that when computing the $LSD$ on the test set, the exact trace can be computed instead of the Monte Carlo approximation to reduce variance, since gradients are no longer required. The model that is closer to 0 has achieved a better fit. \n\nSimilarly, a hypothesis test using the $LSD$ can be used to test if $p = q$ for the data distribution $p$ and  model distribution $q$.\n\nThe authors then show how EBM parameters $\\theta$ can actually be optimized by gradient descent on the $LSD$ objective, in a minimax problem that is similar to the problem of optimizing a generative adversarial network (GAN).  For given $\\theta$, you first optimize the critic $f_\\phi$ w.r.t. $\\phi$ to try to get the $LSD(f_\\phi, p, q_\\theta)$ close to its theoretical optimum with the current $q_\\theta$, then you take a single gradient step $\\nabla_\\theta LSD$ to minimize the $LSD$. They show some experiments that indicates that this works pretty well.\n\nOne thing that was not clear to me when reading this paper is whether the $LSD(f_\\phi,p,q)$ should be minimized or maximized with respect to $\\phi$ to get it close to the true $SD(p,q)$. Although it it possible for $LSD$ to be above or below 0 for a given choice of $q$ and $f_\\phi$, the problem can always be formulated as minimization by simply changing the sign of $f_\\phi$ at the beginning such that the $LSD$ is positive (or as maximization by making it negative).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2002.05616"
    },
    "139": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iclr/LuoSumo2020",
        "transcript": "In this note, I'll implement the [Stochastically Unbiased Marginalization Objective (SUMO)](https://openreview.net/forum?id=SylkYeHtwr) to estimate the log-partition function of an energy funtion. \n\nEstimation of log-partition function has many important applications in machine learning. Take latent variable models or Bayeisian inference. The log-partition function of the posterior distribution $$p(z|x)=\\frac{1}{Z}p(x|z)p(z)$$ is the log-marginal likelihood of the data $$\\log Z = \\log \\int p(x|z)p(z)dz = \\log p(x)$$. \n\nMore generally, let $U(x)$ be some energy function which induces some density function $p(x)=\\frac{e^{-U(x)}}{\\int e^{-U(x)} dx}$. The common practice is to look at a variational form of the log-partition function, \n$$\n\\log Z = \\log \\int e^{-U(x)}dx = \\max_{q(x)}\\mathbb{E}[-U(x)-\\log q(x)] \\nonumber\n$$\nPlugging in an arbitrary $q$ would normally yield a strict lower bound, which means \n$$\n\\frac{1}{n}\\sum_{i=1}^n \\left(-U(x_i) - \\log q(x_i)\\right) \\nonumber\n$$\nfor $x_i$ sampled *i.i.d.* from $q$, would be a biased estimate for $\\log Z$. In particular, it would be an underestimation. \n\n\n\nTo see this, lets define the energy function $U$ as follows:\n$$\nU(x_1,x_2)= - \\log \\left(\\frac{1}{2}\\cdot e^{-\\frac{(x_1+2)^2 + x_2^2}{2}} + \\frac{1}{2}\\cdot\\frac{1}{4}e^{-\\frac{(x_1-2)^2 + x_2^2}{8}}\\right) \\nonumber\n$$\nIt is not hard to see that $U$ is the energy function of a mixture of Gaussian distribution $\\frac{1}{2}\\mathcal{N}([-2,0], I) + \\frac{1}{2}\\mathcal{N}([2,0], 4I)$ with a normalizing constant $Z=2\\pi\\approx6.28$ and $\\log Z\\approx1.8379$.\n\n```python\ndef U(x):\n  x1 = x[:,0]\n  x2 = x[:,1]\n  d2 = x2 ** 2\n  return - np.log(np.exp(-((x1+2) ** 2 + d2)/2)/2 + np.exp(-((x1-2) ** 2 + d2)/8)/4/2)\n```\n\n\n\nTo visualize the density corresponding to the energy $p(x)\\propto e^{-U(x)}$\n\n```python\nxx = np.linspace(-5,5,200)\nyy = np.linspace(-5,5,200)\nX = np.meshgrid(xx,yy)\nX = np.concatenate([X[0][:,:,None], X[1][:,:,None]], 2).reshape(-1,2)\nunnormalized_density = np.exp(-U(X)).reshape(200,200)\nplt.imshow(unnormalized_density)\nplt.axis('off')\n```\n\nhttps://i.imgur.com/CZSyIQp.png\n\n\n\nAs a sanity check, lets also visualize the density of the mixture of Gaussians. \n\n```python\nN1, N2 = mvn([-2,0], 1), mvn([2,0], 4)\ndensity = (np.exp(N1.logpdf(X))/2 + np.exp(N2.logpdf(X))/2).reshape(200,200)\nplt.imshow(density)\nplt.axis('off')\nprint(np.allclose(unnormalized_density / density - 2*np.pi, 0))\n```\n\n`True`\n\nhttps://i.imgur.com/g4inQxB.png\n\n\n\nNow if we estimate the log-partition function by estimating the variational lower bound, we get\n\n```python\nq = mvn([0,0],5)\n\nxs = q.rvs(10000*5)\nelbo = - U(xs) - q.logpdf(xs)\nplt.hist(elbo, range(-5,10))\nprint(\"Estimate:  %.4f  / Ground true:  %.4f\" % (elbo.mean(), np.log(2*np.pi)))\nprint(\"Empirical variance: %.4f\" % elbo.var())\n```\n\n`Estimate:  1.4595  / Ground true:  1.8379`\n\n`Empirical variance: 0.9921`\n\nhttps://i.imgur.com/vFzutuY.png\n\n\n\nThe lower bound can be tightened via [importance sampling):\n$$\n\\log \\int e^{-U(x)} dx \\geq \\mathbb{E}_{q^K}\\left[\\log\\left(\\frac{1}{K}\\sum_{j=1}^K \\frac{e^{-U(x_j)}}{q(x_j)}\\right)\\right] \\nonumber\n$$\n\n> This bound is tighter for larger $K$ partly due to the [concentration of the average](https://arxiv.org/pdf/1906.03708.pdf) inside of the $\\log$ function: when the random variable is more deterministic, using a local linear approximation near its mean is more accurate as there's less \"mass\" outside of some neighborhood of the mean.  \n\n\n\nNow if we use this new estimator with $K=5$\n\n```python\nk = 5\nxs = q.rvs(10000*k)\nelbo = - U(xs) - q.logpdf(xs)\niwlb = elbo.reshape(10000,k)\niwlb = np.log(np.exp(iwlb).mean(1))\nplt.hist(iwlb, range(-5,10))\nprint(\"Estimate:  %.4f  / Ground true:  %.4f\" % (iwlb.mean(), np.log(2*np.pi)))\nprint(\"Empirical variance: %.4f\" % iwlb.var())\n```\n\n`Estimate:  1.7616  / Ground true:  1.8379`\n\n`Empirical variance: 0.1544`\n\nhttps://i.imgur.com/sCcsQd4.png\n\nWe see that both the bias and variance decrease. \n\n\n\nFinally, we use the [Stochastically Unbiased Marginalization Objective](https://openreview.net/pdf?id=SylkYeHtwr) (SUMO), which uses the *Russian Roulette* estimator to randomly truncate a telescoping series that converges in expectation to the log partition function. Let $\\text{IWAE}_K = \\log\\left(\\frac{1}{K}\\sum_{j=1}^K \\frac{e^{-U(x_j)}}{q(x_j)}\\right)$ be the importance-weighted estimator, and $\\Delta_K = \\text{IWAE}_{K+1} - \\text{IWAE}_K$ be the difference (which can be thought of as some form of correction). The SUMO estimator is defined as \n$$\n\\text{SUMO} = \\text{IWAE}_1 + \\sum_{k=1}^K \\frac{\\Delta_K}{\\mathbb{P}(\\mathcal{K}\\geq k)} \\nonumber\n$$\nwhere $K\\sim p(K)=\\mathbb{P}(\\mathcal{K}=K)$. To see why this is an unbiased estimator,\n$$\n\\begin{align*}\n\\mathbb{E}[\\text{SUMO}] &= \\mathbb{E}\\left[\\text{IWAE}_1 + \\sum_{k=1}^K \\frac{\\Delta_K}{\\mathbb{P}(\\mathcal{K}\\geq k)} \\right] \\nonumber\\\\\n&= \\mathbb{E}_{x's}\\left[\\text{IWAE}_1 + \\mathbb{E}_{K}\\left[\\sum_{k=1}^K \\frac{\\Delta_K}{\\mathbb{P}(\\mathcal{K}\\geq k)} \\right]\\right] \\nonumber\n\\end{align*}\n$$\nThe inner expectation can be further expanded\n$$\n\\begin{align*}\n\\mathbb{E}_{K}\\left[\\sum_{k=1}^K \\frac{\\Delta_K}{\\mathbb{P}(\\mathcal{K}\\geq k)} \\right]\n&= \\sum_{K=1}^\\infty P(K)\\sum_{k=1}^K \\frac{\\Delta_K}{\\mathbb{P}(\\mathcal{K}\\geq k)} \\\\\n&= \\sum_{k=1}^\\infty \\frac{\\Delta_K}{\\mathbb{P}(\\mathcal{K}\\geq k)} \\sum_{K=k}^\\infty P(K) \\\\\n&= \\sum_{k=1}^\\infty \\frac{\\Delta_K}{\\mathbb{P}(\\mathcal{K}\\geq k)} \\mathbb{P}(\\mathcal{K}\\geq k) \\\\\n&= \\sum_{k=1}^\\infty\\Delta_K \\\\\n&= \\text{IWAE}_{2} - \\text{IWAE}_1 + \\text{IWAE}_{3} - \\text{IWAE}_2 + ... = \\lim_{k\\rightarrow\\infty}\\text{IWAE}_{k}-\\text{IWAE}_1\n\\end{align*}\n$$\nwhich shows $\\mathbb{E}[\\text{SUMO}] = \\mathbb{E}[\\text{IWAE}_\\infty] = \\log Z$. \n\n>  (N.B.) Some care needs to be taken care of for taking the limit. See the paper for more formal derivation.\n\n\n\nA choice of $P(K)$ proposed in the paper satisfy $\\mathbb{P}(\\mathcal{K}\\geq K)=\\frac{1}{K}$. We can sample such a $K$ easily using the [inverse CDF](https://en.wikipedia.org/wiki/Inverse_transform_sampling),  $K=\\lfloor\\frac{u}{1-u}\\rfloor$ where $u$ is sampled uniformly from the interval $[0,1]$. \n\n\n\nNow putting things all together, we can estimate the log-partition using SUMO. \n\n```python\ncount = 0\nbs = 10\niwlb = list()\nwhile count <= 1000000:\n  u = np.random.rand(1)\n  k = np.ceil(u/(1-u)).astype(int)[0]\n  xs = q.rvs(bs*(k+1))\n  elbo = - U(xs) - q.logpdf(xs)\n  iwlb_ = elbo.reshape(bs, k+1)\n  iwlb_ = np.log(np.cumsum(np.exp(iwlb_), 1) / np.arange(1,k+2))\n  iwlb_ = iwlb_[:,0] + ((iwlb_[:,1:k+1] - iwlb_[:,0:k]) * np.arange(1,k+1)).sum(1)\n  count += bs * (k+1)\n  iwlb.append(iwlb_)\n\niwlb = np.concatenate(iwlb)\nplt.hist(iwlb, range(-5,10))\nprint(\"Estimate:  %.4f  / Ground true:  %.4f\" % (iwlb.mean(), np.log(2*np.pi)))\nprint(\"Empirical variance: %.4f\" % iwlb.var())\n```\n\n`Estimate:  1.8359  / Ground true:  1.8379`\n\n`Empirical variance: 4.1794`\n\nhttps://i.imgur.com/04kPKo5.png\n\nIndeed the empirical average is quite close to the true log-partition of the energy function. However we can also see that the distribution of the estimator is much more spread-out. In fact, it is very heavy-tailed. Note that I did not tune the proposal distribution $q$ based on the ELBO, or IWAE or SUMO. In the paper, the authors propose to tune $q$ to minimize the variance of the $\\text{SUMO}$ estimator, which might be an interesting trick to look at next. \n\n(Reposted, see more details and code from https://www.chinweihuang.com/pages/sumo)\n",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=SylkYeHtwr"
    },
    "140": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/asunam/JamshidiRL18",
        "transcript": "During the past few years, sellers have increasingly offered discounted or free products to selected reviewers of ecommerce platforms in exchange for their reviews. Such incentivized (and often very positive) reviews can improve the rating of a product which in turn sways other users\u2019 opinions about the product. \n\nHere, we examine the problem of detecting and characterizing incentivized reviews in two primary categories of Amazon products.  We show that the key features of EIRs and normal reviews exhibit different characteristics. \n\nhttps://i.imgur.com/1BdwsK4.png\n\nFurthermore, we illustrate how the prevalence of EIRs has evolved and been affected by Amazon\u2019s ban. \n\nhttps://i.imgur.com/1XsSaxX.png\n\nOur examination of the temporal pattern of submitted reviews for sample products reveals promotional campaigns by the corresponding sellers and their effectiveness in attracting other users. \nhttps://i.imgur.com/TvQiDlc.png\n\nFinally, we demonstrate that a classifier that is trained by EIRs (without explicit keywords) and normal reviews can accurately detect other EIRs as well as implicitly incentivized reviews. Overall, this analysis sheds an insightful light on the impact of EIRs on Amazon products and users.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/ASONAM.2018.8508267"
    },
    "141": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=ramanujan2019whats",
        "transcript": "The paper: \"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask\" by Zhou et al., 2019 found that by just learning binary masks one can find random subnetworks that do much better than chance on a task. This new paper builds on this method by proposing a strong algorithm than Zhou et al. for finding these high-performing subnetworks.https://i.imgur.com/vxDqCKP.png\n\nThe intuition follows: \"If a neural network with random weights (center) is sufficiently overparameterized, it will contain a subnetwork (right) that performs as well as a trained neural network (left) with the same number of parameters.\"\n\nWhile Zhou et al. learned a probability for each weight this paper learns a score for each weight and takes the top k percent at evaluation. The scores are learned through their primary contribution that they call the edge-popup algorithm: \n\nhttps://i.imgur.com/9KcIbxd.png\n\n\"In the edge-popup Algorithm, we associate a score with each edge. On the forward pass we choose the top edges by score. On the backward pass we update the scores of all the edges with the straight-through estimator, allowing helpful edges that are \u201cdead\u201d to re-enter the subnetwork. *We never update the value of any weight in the network, only the score associated with each weight.*\"\n\nThey're able to find higher-performing random subnetworks than Zhou et al.\n\nhttps://i.imgur.com/T3D7OsZ.png",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1911.13299"
    },
    "142": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1805.06370",
        "transcript": "Proposes a two-stage approach for continual learning. An active learning phase and a consolidation phase. The active learning stage optimizes for a specific task that is then consolidated into the knowledge base network via Elastic Weight Consolidation (Kirkpatrick et al., 2016). The active learning phases uses a separate network than the knowledge base, but is not always trained from scratch - authors suggest a heuristic based on task-similarity. Improves EWC by deriving a new online method so parameters don\u2019t increase linearly with the number of tasks.\n\nDesiderata for a continual learning solution:\n\n- A continual learning method should not suffer from catastrophic forgetting. That is, it should be able to perform reasonably well on previously learned tasks. \n\n- It should be able to learn new tasks while taking advantage of knowledge extracted from previous tasks, thus exhibiting positive forward transfer to achieve faster learning and/or better final performance. \n\n- It should be scalable, that is, the method should be trainable on a large number of tasks. \n\n- It should enable positive backward transfer as well, which means gaining improved performance on previous tasks after learning a new task which is similar or relevant. \n\n- Finally, it should be able to learn without requiring task labels, and ideally, it should even be applicable in the absence of clear task boundaries.\n\nExperiments:\n\n- Sequential learning of handwritten characters of 50 alphabets taken from the Omniglot dataset.\n- Sequential learning of 6 games in the Atari suite (Bellemare et al., 2012) (\u201cSpace Invaders\u201d, \u201cKrull\u201d, \u201cBeamrider\u201d, \u201cHero\u201d, \u201cStargunner\u201d and \u201cMs. Pac-man\u201d).\n- 8 navigation tasks in 3D environments inspired by experiments with Distral (Teh et al., 2017).\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1805.06370"
    },
    "143": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.10571",
        "transcript": "A novel method for adversarially-robust learning with theoretical guarantees under small perturbations.\n\n1) Given the default distribution P_0, defines a proximity of it as a set of distributions which are \\rho-close to P_0 in terms of Wasserstein metric with a predefined cost function c (e.g. L2);\n\n2) Formulates the robust learning problem as minimization of the worst-case example in the proximity and proposes a Lagrangian relaxation of it;\n\n3) Given it, provides a data-dependent upper bound on the worst-case loss, demonstrates that the problem of finding the worst-case adversarial perturbation, which is generally NP hard, renders to optimization of a concave function if the maximum amount of perturbation \\rho is low.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1710.10571"
    },
    "144": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1907-02057",
        "transcript": "This is not a detailed summary, just general notes:\n\nAuthors make a excellent and extensive comparison of Model Free, Model based methods in 18 environments. In general, the authors compare 3 classes of Model Based Reinforcement Learning (MBRL) algorithms using as metric for comparison the total return in the environment after 200K steps (reporting the mean and std by taking windows of 5000 steps throughout the whole training - and averaging across 4 seeds for each algorithm). They compare MBRL classes:\n\n- **Dyna style:** using a policy to gather data, training a transition function model on this data(i.e. dynamics function / \"world model\"), and using data predicted by the model (i.e. \"imaginary\" data) to train the policy)\n\n- **Policy Search with Backpropagation through Time (BPTT):** starting at some state $s_0$ the policy rolls out an episode using the model. Then given the trajectory and its sum of rewards (or any other objective function to maximize) one can differentiate this expression with respect to the policies parameters $\\theta$\n to obtain the gradient. The training process iterates between collecting data using the current policy and improving the policy via computing the BPTT gradient ... Some version include dynamic programming approaches where the ground -truth dynamics need to be known\n\n- **Model Predictive Control (MPC) / Shooting methods:** There is in general no explicit policy to choose actions, rather the actions sequence is chosen by: starting with a set of candidates of actions sequences $a_{t:t+\\tau}$ , propagating this actions sequences in the dynamics model, and then choosing the action sequence which achieved the highest return through out the propagated episode. Then, the agent only applies the first action from the optimal sequence and re-plans at every time-step.\n\nThey also compare this to Model Free (MF) methods such as SAC and TD3.\n\n**Brief conclusions which I noticed from MB and MF comparisons:** (note the $>$\n indicates better than )\n\n- **MF:** SAC & TD3 $>$ PPO & TRPO\n\n- **Performance:** MPC (shooting, robust performance except for complex env.) $>$ Dyna (bad for long H) $>$ BPTT (SVG very good for complex env.)\n\n- **State and Action Noise:** MPC (shooting, re-planning compensates for noise) $>$ Dyna (lots of Model predictive errors \u2026 although meta learning actually benefits from noisy do to lack of exploration)\n\n- **MB dynamics error accumulation:** MB performance plateaus, more data $\\neq$ better performance $\\rightarrow$ 1. prediction error accumulates through time 2. As we the policy and model improvement are closely link, we can (early) fall into local minima\n\n- **Early Termination (ET):** including ET always negatively affects MB methods. Different ways of incorporating ET into planning horizon (see appendix G for details) work better for some environment but worst for more complex envs. - so, tbh theres no conclusion to be made about early termination schemes (same as for there entire paper :D, but it's not the authors fault, is just the (sad) direction in which most RL / DL research is moving in)\n\n**Some stuff which seems counterintuitive:**\n\n- Why can\u2019t we see a significant sample efficiency of MB w.r.t to MF ?\n- Why does PILCO suck at almost everything ? original authors/ implementation seems to excel at several tasks\n- When using ensembles (e.g. PETS), why is predicting the next state as the Expectation over the ensemble (PE-E: $\\boldsymbol{s}_{t+1}=\\mathbb{E}\\left[\\widetilde{f}_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{s}_{t}, \\boldsymbol{a}_{t}\\right)\\right]$) better or at least highly comparable to propagating a particle by leveraging the ensemble of models (PE-TS: given $P$ initial states $\\boldsymbol{s}_{t=0}^{p}=\\boldsymbol{s}_{0} \\forall \\boldsymbol{p}$, propagate each state / particle $p$ using *one* model $b(p)$ from the entire ensemble ($B$), such that $\\boldsymbol{s}_{t+1}^{p} \\sim \\tilde{f}_{\\boldsymbol{\\theta}_{b(p)}}\\left(\\boldsymbol{s}_{t}^{\\boldsymbol{p}}, \\boldsymbol{a}_{t}\\right)$ ), which should in theory better capture uncertainty / multimodality of the State space ??",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1907.02057"
    },
    "145": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1907.03626",
        "transcript": "Benchmarking Deep Learning Hardware and Frameworks: Qualitative Metrics \n\nPrevious papers on benchmarking deep neural networks offer knowledge of deep learning hardware devices and software frameworks. This paper introduces benchmarking principles, surveys machine learning devices including GPUs, FPGAs, and ASICs, and reviews deep learning software frameworks. It also qualitatively compares these technologies with respect to benchmarking from the angles of our 7-metric approach to deep learning frameworks and 12-metric approach to machine learning hardware platforms. \n\nAfter reading the paper, the audience will understand seven benchmarking principles, generally know that differential characteristics of mainstream artificial intelligence devices,  qualitatively compare deep learning hardware through the 12-metric approach for benchmarking neural network hardware, and read benchmarking results of 16 deep learning frameworks via our 7-metric set for benchmarking  frameworks.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1907.03626v2"
    },
    "146": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.09883",
        "transcript": "The paper proposes a standardized benchmark for a number of safety-related problems, and provides an implementation that can be used by other researchers. The problems fall in two categories: specification and robustness. Specification refers to cases where it is difficult to specify a reward function that encodes our intentions. Robustness means that agent's actions should be robust when facing various complexities of a real-world environment. Here is a list of problems:\n\n1. Specification:\n  1. Safe interruptibility: agents should neither seek nor avoid interruption.\n  2. Avoiding side effects: agents should minimize effects unrelated to their main objective.\n  3. Absent supervisor: agents should not behave differently depending on presence of supervisor.\n  4. Reward gaming: agents should not try to exploit errors in reward function.\n\n2. Robustness:\n  1. Self-modification: agents should behave well when environment allows self-modification.\n  2. Robustness to distributional shift: agents should behave robustly when test differs from train.\n  3. Robustness to adversaries: agents should detect and adapt to adversarial intentions in environment.\n  4. Safe exploration: agent should behave safely during learning as well.\n\nIt is worth noting that problems 1.2, 1.4, 2.2, and 2.4 have been described back in \"Concrete Problems in AI Safety\".\n\nIt is suggested that each of these problems be tackled in a \"gridworld\" environment \u2014 a 2D environment where the agent lives on a grid, and the only actions it has available are up/down/left/right movements. The benchmark consists of 10 environments, each corresponding to one of 8 problems mentioned above. Each of the environments is an extremely simple instance of the problem, but nevertheless they are of interest as current SotA algorithms usually don't solve the posed task.\n\nSpecifically, the authors trained A2C and Rainbow with DQN update on each of the environments and showed that both algorithms fail on all of specification problems, except for Rainbow on 1.1. This is expected, as neither of those algorithms are designed for cases where reward function is misspecified. Both algorithms failed on 2.2--2.4, except for A2C on 2.3. On 2.1, the authors swapped A2C for Rainbow with Sarsa update and showed that Rainbow DQN failed while Rainbow Sarsa performed well.\n\nOverall, this is a good groundwork paper with only a few questionable design decisions, such as the design of actual reward in 1.2. It is unlikely to have impact similar to MNIST or ImageNet, but it should stimulate safety-related research.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.09883"
    },
    "147": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1903.00374",
        "transcript": "This paper shows exciting results on using Model-based RL for Atari. \n\nModel-based RL has shown impressive improvements in sample efficiency on Mujoco tasks ([Chua et. al, 2018](https://arxiv.org/abs/1805.12114)), so its nice to see that the sample efficiency improvements carry over to Pixel-based envs like Atari too. \n\nSpecifically, the authors show that their model-based method can do well on several Atari games after training on only 100K env steps (400K frames with FrameSkip 4) which roughly corresponds to 2 hours of game play. They compare to SOTA model-free variants (Rainbow, PPO) after similar number of frames and show that the model-based version achieves much better scores. \n\nThe overall training procedure has a very Dyna like flavor. The algorithm, termed SimPLe follows an iterative scheme of:\n* Collect experience from the real environment using a policy (initialized to random). \n* Use this experience to train the world model (a next-step frame prediction model, and a reward prediction model). This amounts to supervised learning on `{(s, a) -> s\u2019}` and `{(s, a) -> r}` pairs. \n* Generate rollouts using the world model, and learn a policy with these rollouts using PPO.\n\nhttps://i.imgur.com/SZLmdME.png\n\n**Countering distributional shift:**\n\nA key issue when training models is compounding errors when doing multi-step rollouts. This is similar to the problem of making predictions with RNNs trained via teacher-forcing, and hence it's natural to leverage existing techniques from that literature. \n\nThis paper uses one such technique: scheduled sampling, that is during training randomly replace some frames of the input by the prediction from the previous step. This seems like a natural way to make the model robust to slight distributional changes. \n\n\n**Commentary / possible future work:** \n\n* The paper evaluated only on 26 out of 60 Atari games in ALE. I would have really liked if the authors showed performance numbers on all the games even if they weren\u2019t good.  \n* Related: I suspect the method would not work well when the initial diversity of frames given by the random policy is not sufficient (ex. Sparse reward games like Montezuma\u2019s revenge/Pitfall). Using sample efficient exploration algorithms to augment model learning would be really interesting. \n* The trained world-model is able to rollout only for 50 time-steps (compounding errors don't allow for longer rollouts), it might be worthwhile to explore models that can do long-horizon predictions [(TD-VAE?)](https://openreview.net/forum?id=S1x4ghC9tQ). \n* Apart from sample-efficiency gains, one reason I am excited about models is their potential ability to generalize to different tasks in the same environment. Benchmarking their generalization capability should thus be an exciting next step. \n\nFinally, props to authors for open-sourcing the code: [tensor2tensor/tensor2tensor/rl at master \u00b7 tensorflow/tensor2tensor \u00b7 GitHub](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/rl) and providing detailed instructions to run. ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1903.00374"
    },
    "148": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1812.09916",
        "transcript": "**TL;DR**: Rearranging the terms in Maximum Mean Discrepancy yields a much better loss function for the discriminator of Generative Adversarial Nets.\n\n**Keywords**: Generative adversarial nets, Maximum Mean Discrepancy, spectral normalization, convolutional neural networks, Gaussian kernel, local stability.\n\n**Summary**\n\nGenerative adversarial nets (GANs) are widely used to learn the data sampling process and are notoriously difficult to train. The training of GANs may be improved from three aspects: loss function, network architecture, and training process.\n\nThis study focuses on a loss function called the Maximum Mean Discrepancy (MMD), defined as:\n$$\nMMD^2(P_X,P_G)=\\mathbb{E}_{P_X}k_{D}(x,x')+\\mathbb{E}_{P_G}k_{D}(y,y')-2\\mathbb{E}_{P_X,P_G}k_{D}(x,y)\n$$\nwhere $G,D$ are the generator and discriminator networks, $x,x'$ are real samples, $y,y'$ are generated samples, $k_D=k\\circ D$ is a learned kernel that calculates the similariy between two samples. Overall, MMD calculates the distance between the real and the generated sample distributions. Thus, traditionally, the generator is trained to minimize $L_G=MMD^2(P_X,P_G)$, while the discriminator minimizes $L_D=-MMD^2(P_X,P_G)$.\n\nThis study makes three contributions:\n-  It argues that $L_D$ encourages the discriminator to ignores the fine details in real data. By minimizing $L_D$, $D$ attempts to maximize $\\mathbb{E}_{P_X}k_{D}(x,x')$, the similarity between real samples scores. Thus, $D$ has to focus on common features shared by real samples rather than fine details that separate them. This may slow down training. Instead, a repulsive loss is proposed, with no additional computational cost to MMD:\n$$\nL_D^{rep}=\\mathbb{E}_{P_X}k_{D}(x,x')-\\mathbb{E}_{P_G}k_{D}(y,y')\n$$\n- Inspired by the hinge loss, this study proposes a bounded Gaussian kernel for the discriminator to facilitate stable training of MMD-GAN.\n- The spectral normalization method divides the weight matrix at each layer by its spectral norm to enforce that each layer is Lipschitz continuous. This study proposes a simple method to calculate the spectral norm of a convolutional kernel.\n\nThe results show the efficiency of proposed methods on CIFAR-10, STL-10, CelebA and LSUN-bedroom datasets. In Appendix, we prove that MMD-GAN training using gradient method is locally exponentially stable (a property that the Wasserstein loss does not have), and show that the repulsive loss works well with gradient penalty. \n\nThe paper has been accepted at ICLR 2019 ([OpenReview link](https://openreview.net/forum?id=HygjqjR9Km)). The code is available at [GitHub link](https://github.com/richardwth/MMD-GAN). ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1812.09916"
    },
    "149": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1007/978-3-319-91704-7_11",
        "transcript": "# Contributions\nThe contribution of this paper is three-fold:\n1. We present a method to use *process models* as interpretable sequence models that have a stronger notion of interpretability than what is generally used in the machine learning field (see Section *process models* below),\n2. We show that this approach enables the comparison of traditional sequence models (RNNs, LSTMs, Markov Models) with techniques from the research field of *automated process discovery*,\n3. We show on a collection of three real-life datasets that a better fit of sequence data can be obtained with LSTMs than with techniques from the *automated process discovery* field\n\n# Process Models\nProcess models are visually interpretable models that model sequence data in such a way that the generated model is represented in a notation that has *formal semantics*, i.e., it is well-defined which sequences are and which aren't allowed by the model. Below you see an example of a Petri net (a type of model with formal semantics) which allows for the sequences <A,B,C>, <A,C,B>, <D,B,C>, and <D,C,B>.\nhttps://i.imgur.com/SbVYMvX.png\nFor an overview of automated process discovery algorithms to mine a process model from sequnce data, we refer to [this recent survey and benchmark paper](https://ieeexplore.ieee.org/abstract/document/8368306/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-91704-7_11"
    },
    "150": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1709.04326",
        "transcript": "Normal RL agents in multi-agent scenarios treat their opponents as a static part of the environment, not taking into account the fact that other agents are learning as well. This paper proposes LOLA, a learning rule that should take the agency and learning of opponents into account by optimizing \"return under one step look-ahead of opponent learning\"\n\nSo instead of optimizing under the current parameters of agent 1 and 2 \n$$V^1(\\theta_i^1, \\theta_i^2)$$\n\nLOLA proposes to optimize taking into account one step of opponent (agent 2) learning\n$$V^1(\\theta_i^1, \\theta_i^2 + \\Delta \\theta^2_i)$$\n\nwhere we assume the opponent's naive learning update $\\Delta \\theta^2_i = \\nabla_{\\theta^2} V^2(\\theta^1, \\theta^2) \\cdot \\eta$ and we add a second-order correction term\n\non top of this, the authors propose\n- a learning rule with policy gradients in the case that the agent does not have access to exact gradients\n- a way to estimate the parameters of the opponent, $\\theta^2$, from its trajectories using maximum likelihood in the case you can't access them directly\n$$\\hat \\theta^2 = \\text{argmax}_{\\theta^2} \\sum_t \\log \\pi_{\\theta^2}(u_t^2|s_t)$$\n\nLOLA is tested on iterated prisoner's dilemma and converges to a tit-for-tat strategy more frequently than the naive RL learning algorithm, and outperforms it. LOLA is tested on iterated matching pennies (similar to prisoner's dilemma) and stably converges to the Nash equilibrium whereas the naive learners do not. In testing on coin game (a higher dimensional version of prisoner's dilemma) they find that naive learners generally choose the defect option whereas LOLA agents have a mostly-cooperative strategy.\n\nAs well, the authors show that LOLA is a dominant learning rule in IPD, where both agents always do better if either is using LOLA (and even better if both are using LOLA).\n\nFinally, the authors also propose second order LOLA, which instead of assuming the opponent is a naive learner, assumes the opponent uses a LOLA learning rule. They show that second order LOLA does not lead to improved performance so there is no need to have a $n$th order LOLA arms race.\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1709.04326v3"
    },
    "151": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.05365",
        "transcript": "This paper introduces a deep universal word embedding based on using a bidirectional LM (in this case, biLSTM). First words are embedded with a CNN-based, character-level, context-free, token embedding into $x_k^{LM}$ and then each sentence is parsed using a biLSTM, maximizing the log-likelihood of a word given it's forward and backward context (much like a normal language model). \n\nThe innovation is in taking the output of each layer of the LSTM ($h_{k,j}^{LM}$ being the output at layer $j$) \n$$\n\\begin{align}\nR_k &= \\{x_k^{LM}, \\overrightarrow{h}_{k,j}^{LM}, \\overleftarrow{h}_{k,j}^{LM} | j = 1 \\ldots L \\} \\\\\n&= \\{h_{k,j}^{LM} | j = 0 \\ldots L \\}\n\\end{align}\n$$\nand allowing the user to learn a their own task-specific weighted sum of these hidden states as the embedding:\n$$\nELMo_k^{task} = \\gamma^{task} \\sum_{j=0}^L s_j^{task} h_{k,j}^{LM}\n$$\n\nThe authors show that this weighted sum is better than taking only the top LSTM output (as in their previous work or in CoVe) because it allows capturing syntactic information in the lower layer of the LSTM and semantic information in the higher level. Table below shows that the second layer is more useful for the semantic task of word sense disambiguation, and the first layer is more useful for the syntactic task of POS tagging.\n\nhttps://i.imgur.com/dKnyvAa.png\n\nOn other benchmarks, they show it is also better than taking the average of the layers (which could be done by setting $\\gamma = 1$)\nhttps://i.imgur.com/f78gmKu.png\n\nTo add the embeddings to your supervised model,  ELMo is concatenated with your context-free embeddings, $\\[ x_k; ELMo_k^{task} \\]$. It can also be concatenated with the output of your RNN model $\\[ h_k; ELMo_k^{task} \\]$ which can show improvements on the same benchmarks\n\nhttps://i.imgur.com/eBqLe8G.png\n\nFinally, they show that adding ELMo to a competitive but simple baseline gets SOTA (at the time) on very many NLP benchmarks\n\nhttps://i.imgur.com/PFUlgh3.png\n\nIt's all open-source and there's a tutorial [here](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md)\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.05365"
    },
    "152": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1801.00631",
        "transcript": "Deep Learning has a number of shortcomings.\n\n(1)Requires lot of data: Humans can learn abstract concepts with far less training data compared to current deep learning. E.g. If we are told who an \u201cAdult\u201d is, we can answer questions like how many adults are there in home?, Is he an adult? etc. without much data. Convolution networks can solve translational invariance but requires lot more data to identify other translations or more filters or different architectures. \n\n(2)Lack of transfer: Most of claims of Deep RL helping in transfer is ambiguous. Consider Deepmind claim of concept learning in Breakout such as digging a tunnel through a wall which was soon proved false by Vicarious experiments that added wall in middle and increased Y coordinate of paddle. Current attempt of transfer is based on correlations between trained sequences and test scenario, which is bound to fail when current scenario is tweaked. \n\n(3)Hierarchical structure not learnt: Deep learning learns correlations which are non-hierarchical in nature. So sentences like \u201cSalman Khan, who was excellent driver, died in a car accident\u201d can never be represented as major clause(Salman Khan) and minor clause(who was excellent driver) format. Subtleties like these cannot be captured by RNN even though hierarchical RNN tries to capture obvious hierarchies like (letters -> words -> sentences). If hierarchies were captured in Deep RL, transfer would have been easy in Breakout which is not the case. \n\n(4)Poor inference in language: Sentences that have subtle differences like \u201cJohn promised Mary to leave\u201d and \u201cJohn promised to leave Mary\u201d are treated as same by deep learning. This causes major problems during inferencing because questions related to combining various sentences fail. \n\n(5)Not transparent: Why the neural network made the decision in a certain way can help in debuggability and prove to be beneficial in medical diagnosis systems where it is critical to reason out methodology. \n\n(6)No priors and commonsense reasoning: Humans function with commonsense reasoning(If A is dad of B, A is elder to B) and priors(physics laws). Deep Learning does not tailor to incorporate this. With heavy interest in end to end learning from raw data, such attempts have been discouraged. \n\n(7)Deep Learning is correlation not causation: Causality or analogical reasoning or any abstract concepts of left brain is not dealt by deep learning. (8)Lacks generalization outside training distribution: Fails to incorporate scenario in which nature of data is varying. E.g. Stock prediction. (9)Easily fooled: E.g. Parking signs mistaken for refrigerators, turtle mistaken as rifle. \n\nThis can be addressed by:\n(1)Unsupervised learning: Build systems that can set their own goals, use abstract knowledge(priors, affordances as objects can be used in any way etc) and solve problem at high level(like symbolic AI). \n\n(2)Symbolic AI - Deep Learning does what primary sensory cortex does of taking raw inputs and converting it into low level representation. Symbolic AI builds abstract concepts like causal, analogical reasoning which is what prefrontal Cortex does. Humans make decisions based on these abstract concepts.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1801.00631v1"
    },
    "153": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1801.04016",
        "transcript": "Paper overviews importance of Causality in AI and highlights important aspects of it. Current state of AI deals with only association/curve fitting of data without need of a model. But this is far from human-like intelligence who have a mental representation that is manipulated from time-to-time using data and queried with What If? questions. To incorporate this, one needs to add two more layers on top of curve fitting module which are interventions(What if I do this?) and counterfactuals(What if I had done this?). Interventions are represented by P(y|do(x)) where do(x) is action 'x' performed leading to change in behavior of certain variables, thereby making previous data useless for its estimation. Counterfactuals are represented by P(y(x)|x',y') where x',y' are observed and goal is to determine probability of y given x. Pearl suggests use of Structural Causal Models(SCM) for interventions and counterfactuals. SCM takes a query(association, intervention or counterfactual) and graphical model(based on assumptions) to build a estimand(mathematical recipe). Estimand takes data and produces an estimate(answer) with confidence. Assumptions are fine tuned based on data. There are lot of advantages provided by Causal Models - (1)Graphical models make it easier to read the assumptions, thereby providing transparency. It also makes it easier to verify all dependencies encoded in data with the help of d-separation, thereby providing testability (2)Causal models help in mediation analysis that identify mechanisms that change cause to effect for explainability (3)Current transfer learning approaches are tried at association level but it cannot identify mechanisms that are affected by changes (4)Causality provides tools to recover causal relationships when data has missing attributes unlike statistical analysis that provide tools only when values are missing at random i.e. independent of other variables.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1801.04016"
    },
    "154": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.05518",
        "transcript": "DRL has lot of disadvantages like large data requirement, slow learning, difficult interpretation, difficult transfer, no causality, analogical reasoning done at a statistical level not at a abstract level etc. This can be overcome by adding a symbolic front end on top of DL layer before feeding it to RL agent. Symbolic front end gives advantage of smaller state space generalization, flexible predicate length and easier combination of predicate expressions. DL avoids manual creation of features unlike symbolic reasoning. Hence DL along with symbolic reasoning might be the way to progress for AGI. State space reduction in symbolic reasoning is carried out by using object interactions(object positions and object types) for state representation. Although certain assumptions are made in the process such as objects of same type behave similarly etc, one can better understand causal relations in terms of actions, object interactions and reward by using symbolic reasoning. \n\nBroadly, pipeline consists of (1)CNN layer - Raw pixels to representation (2)Salient pixel identification - Pixels that have activations in CNN above a certain threshold (3)Identify objects of similar kind by using activation spectra of salient pixels (4)Identify similar objects in consecutive time steps to track object motion using spatial closeness(as objects can move only by a small distance in consecutive frames) and similar neighbors(different type of objects can be placed close to each other and spatial closeness alone cannot identify similar objects) (4)Building symbolic interactions by using relative object positions for all pairs of objects located within a certain maximal distance. Relative object position is necessary to capture object dynamics. Maximal distance threshold is required to make the learning quicker eventhough it may reach a locally optimal policy (4)RL agent uses object interactions as states in Q-Learning update. Instead of using all object interactions in a frame as one state, number of states are further reduced by considering interactions between two types to be independent of other types and doing a Q-Learning update separately for each type pair. Intuitive explanation for doing so is to look at a frame as a set of independent object type interactions. Action choice at a state is then the one that maximizes sum of Q values across all type pairs. \n\nResults claim that using DRL with symbolic reasoning, transfer in policies can be observed by first training on evenly spaced grid world and using it for randomly spaced grid world with a performance close to 70% contrary to DQN that achieves 50% even after training for 1000 epochs with epoch length of 100. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1609.05518v2"
    },
    "155": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/ZhangS13",
        "transcript": "Main Purpose:\n* The main goal of the proposed method is to exploit a global perception\nmechanism, known as figure-ground segregation and Boolean Map Theory of\nvisual attention to compute saliency map.\n\nDrawbacks of previous works:\n* Most of the previous works do not exploit the topological structures of an image to\nsaliency calculation. Thus this paper aims to exploit the topological structure of a\nscene in saliency calculation.\n\nMain Idea:\n* Relying on Boolean Map Theory of visual attention, an observer\u2019s momentary\nconscious awareness of a scene can be represented by a Boolean map. Given an\ninput image, a set of Boolean maps can be generated by randomly thresholding its\nfeature channels, e.g. color channel. When the set of Boolean maps is generated,\nsome concepts from figure-ground segregation, introduced by Gestalt\npsychological studies, can be utilized to create a saliency map. As Gestalt\npsychological studies suggest, figures are more likely to be attended than to\nbackground elements. But how the figures are characterized? There is some factors\nthat are likely to influence figure-ground segregation such as size, surroundedness,\nsymmetry and etc. The proposed method uses the surroundedness factor to form a\nsaliency map. This factor implies that figures tends to be surrounded, that is,\nthey\u2019re likely to have a closed outer contour. Therefore, this factor can be\nevaluated over Boolean maps and each region which has the closed outer contour,\nnot connected to the image borders, gets higher attention value. So each Boolean\nmap result in an attention map. Then, attentions maps are summed up lineally to\nform a saliency map.\nIn brief, the contribution of the proposed method is twofold: 1) It uses Boolean\nmaps to characterizes an image. 2) It exploits the figure-ground segregation\nconcepts to form a saliency map.\n\nImplementation Details:\n* Given an input image, a set of Boolean maps are generated by thresholding the\ncolor channels (Lab) with uniformly selected thresholds from 0 to 255. Then, each\nBoolean map is evaluated by surroundedness. Those regions which are surrounded\n(closed outer contour) get value 1 and others 0. These maps are called attention\nmaps. After some normalization steps, the attention maps (per Boolean map) are\nsummed up linearly to form a mean attention map. Again, some normalization and\npost-processing steps are applied and the final saliency map is obtained.\n\n* Conclusion\nIn this work, a novel Boolean Map based Saliency model is proposed to leverage\nthe surroundedness cue that helps in figure-ground segregation. The model\nborrows the concept of Boolean map from the Boolean Map Theory of visual\nattention and characterizes an image by a set of Boolean maps. This representation\nleads to an efficient algorithm for saliency detection. BMS is the only model that\nconsistently achieves state-of-the-art performance on five benchmark eye tracking\ndatasets, and it is also shown to be useful in salient object detection.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/ICCV.2013.26"
    },
    "156": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1803-08840",
        "transcript": "Main purpose:\n* This work proposes a software-based resolution augmentation method which is more agile and simpler to implement than hardware engineering solutions.\n* The paper examines three deep learning single image super resolution techniques on pCLE images \n* A video-registration based method is proposed to estimate ground truth HR pCLE images (this can be assumed as the main objective of the paper)\n\nHighlights:\n* The papers emphasise that this is the first work to address the image resolution problem in pCLE image acquisitions\n* The paper introduces useful information on how pCLE devices work\n* Strong related work \n* Clear story\n* Comprehensive evaluation\n\nMain Idea:\n* Use video-registration based techniques to estimate the HR images (real ground truth HR image is not available)\n* Simulate LR images from estimate HR images with help of Voronoi diagram and Delaunay-based linear interpolation.\n* Train an Exemplar-based SR model (EBSR -- DL-based approach) to learn the mapping between simulated LR and estimate HR images. \n\nMethodology Details\n* To estimate the HR images, a video-registration based mosaicking techniques (by the same authors in MIA 2006) is used which fuses a collection of input images by averaging the temporal information. \n* Since mosaicking generates  single large filed-of-view mosaic image from LR images, the mosaic-to-image diffeomorphic spatial transformation is used which results from the mosaicking process to propagate and crop the fused information from the mosaic back into each input LR image space. \n* At this point, the authors observe that the misalignment between input LR images (used in the video-registration based mosaicking technique) and estimate HR cause training problem for the EBSR model. So, they treat the HR images as realistic and chose to simulate LR images from them!!!!\n* Simulated LR images by obtained using the Voronoi diagram (averaging the Voronoi cell on HR image) + additive noise on estimate HR images. \n* Finally, they build to experimental datasets 1) LR_org and HR and 2) LR_synth and HR and train three CNN SR models on these twor datasets. \n* They train FSRCNN, EDSR, SRGAN\n* The networks are trained using L1+SSIM loss functions\n\nExperiment Notes:\n* SSIM and GCF are used to quantitatively assess the performance of the models.\n* A composite score is also used to take SSIM and GCF into account jointly\n* In the ideal case, when the models are trained and etsted on simulated LR and HR images, the quantitative results are convincing.  \n* \"From this experiment, it is possible to conclude that the proposed solution is capable of performing SR reconstruction when the models are trained on synthetic data with no domain gap at test time\"\n* When models are trained and tested on original LR and estimate HR images, the performance is not reasonable\n* When the models are trained on simulated LR images and tested on original LR images, the results become better compared to the previous case, \n* For a solid conclusion, and MOS study was carried out. The models are trained on simulated LR images. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1803.08840"
    },
    "157": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1038/nbt.3519",
        "transcript": "This paper from 2016 introduced a new k-mer based method to estimate isoform abundance from RNA-Seq data called kallisto.  The method provided a significant improvement in speed and memory usage compared to the previously used methods while yielding similar accuracy.   In fact, kallisto is able to quantify expression in a matter of minutes instead of hours.\n\nThe standard (previous) methods for quantifying expression rely on mapping, i.e. on the alignment of a transcriptome sequenced reads to a genome of reference.  Reads are assigned to a position in the genome and the gene or isoform expression values are derived by counting the number of reads overlapping the features of interest. \n\nThe idea behind kallisto is to rely on a pseudoalignment which does not attempt to identify the positions of the reads in the transcripts, only the potential transcripts of origin. Thus,  it avoids doing an alignment of each read to a reference genome. In fact, kallisto only uses the transcriptome sequences (not the whole genome) in its first step which is the generation of  the kallisto index.  Kallisto builds a colored de Bruijn graph (T-DBG) from all the k-mers found in the transcriptome.  Each node of the graph corresponds to a k-mer (a short sequence of k nucleotides) and retains the information about the transcripts in which they can be found in the form of a color.  Linear stretches having the same coloring in the graph correspond to transcripts. Once the T-DBG is built, kallisto stores a hash table mapping each k-mer to its transcript(s) of origin along with the position within the transcript(s).  This step is done only once and is dependent on a provided annotation file (containing the sequences of all the transcripts in the transcriptome).  \n  \nThen for a given sequenced sample, kallisto decomposes each read into its k-mers and uses those k-mers to find a path covering in the T-DBG.  This path covering of the transcriptome graph, where a path corresponds to a transcript, generates k-compatibility classes for each k-mer, i.e. sets of potential transcripts of origin on the nodes.   The potential transcripts of origin for a read can be obtained using the intersection of its k-mers k-compatibility classes. To make the pseudoalignment faster, kallisto removes redundant k-mers since neighboring k-mers often belong to the same transcripts. Figure1, from the paper, summarizes these different steps.\n\nhttps://i.imgur.com/eNH2kuO.png\n\n**Figure1**. Overview of kallisto. The input consists of a reference transcriptome and reads from an RNA-seq experiment. (a) An example of a read (in black) and three overlapping transcripts with exonic regions as shown. (b) An index is constructed by creating the transcriptome de Bruijn Graph (T-DBG) where nodes (v1, v2, v3, ... ) are k-mers, each transcript corresponds to a colored path as shown and the path cover of the transcriptome induces a k-compatibility class for each k-mer. (c) Conceptually, the k-mers of a read are hashed (black nodes) to find the k-compatibility class of a read. (d) Skipping (black dashed lines) uses the information stored in the T-DBG to skip k-mers that are redundant because they have the same k-compatibility class. (e) The k-compatibility class of the read is determined by taking the intersection of the k-compatibility classes of its constituent k-mers.[From Bray et al. Near-optimal probabilistic RNA-seq quantification, Nature Biotechnology, 2016.]\n\nThen, kallisto optimizes the following RNA-Seq likelihood function using the expectation-maximization (EM) algorithm.  \n\n$$L(\\alpha) \\propto \\prod_{f \\in F} \\sum_{t \\in T} y_{f,t} \\frac{\\alpha_t}{l_t} = \\prod_{e \\in E}\\left(  \\sum_{t \\in e} \\frac{\\alpha_t}{l_t} \\right )^{c_e}$$\n\nIn this function,  $F$ is the set of fragments (or reads), $T$ is the set of transcripts, $l_t$ is the (effective) length of transcript $t$ and **y**$_{f,t}$ is a compatibility matrix defined as 1 if  fragment $f$ is compatible with $t$ and 0 otherwise.  The parameters $\u03b1_t$ are the probabilities of selecting reads from a transcript $t$.  These $\u03b1_t$ are the parameters of interest since they represent the isoforms abundances or relative expressions.\n\nTo make things faster, the compatibility matrix is collapsed (factorized) into equivalence classes. An equivalent class consists of all the reads compatible with the same subsets of transcripts. The EM algorithm is applied to equivalence classes (not to reads).  Each $\u03b1_t$ will be optimized to maximise the likelihood of transcript abundances given observations of the equivalence classes. The speed of the method makes it possible to evaluate the uncertainty of the  abundance estimates for each RNA-Seq sample using a bootstrap technique.  For a given sample containing $N$ reads, a bootstrap sample is generated from the sampling of $N$ counts from a multinomial distribution over the equivalence classes derived from the original sample.  The EM algorithm is applied on those sampled equivalence class counts to estimate transcript abundances. The bootstrap information is then used in downstream analyses such as determining which genes are differentially expressed.\n\nPractically, we can illustrate the different steps involved in kallisto using a small example.  Starting from a tiny genome with 3 transcripts, assume that the RNA-Seq experiment produced 4 reads as depicted in the image below.\n\nhttps://i.imgur.com/5JDpQO8.png\n\nThe first step is to build the T-DBG graph and the kallisto index.  All transcript sequences are decomposed into k-mers (here k=5) to construct the colored de Bruijn graph. Not all nodes are represented in the following drawing.  The idea is that each different transcript will lead to a different path in the graph.  The strand is not taken into account, kallisto is strand-agnostic.\n\nhttps://i.imgur.com/4oW72z0.png\n\nOnce the index is built, the four reads of the sequenced sample can be analysed.  They are decomposed into k-mers (k=5 here too) and the pre-built index is used to determine the k-compatibility class of each k-mer. Then, the k-compatibility class of each read is computed. For example, for read 1, the intersection of all the k-compatibility classes of its k-mers suggests that it might come from transcript 1 or transcript 2.\n\nhttps://i.imgur.com/woektCH.png\n\nThis is done for the four reads enabling the construction of the compatibility matrix  **y**$_{f,t}$ which is part of the RNA-Seq likelihood function.  In this equation, the $\u03b1_t$ are the parameters that we want to estimate.\n\nhttps://i.imgur.com/Hp5QJvH.png\n\nThe EM algorithm being too slow to be applied on millions of reads, the compatibility matrix **y**$_{f,t}$ is factorized into equivalence classes and a count is computed for each class (how many reads are represented by this equivalence class). The EM algorithm uses this collapsed information to maximize the new equivalent RNA-Seq likelihood function and optimize the $\u03b1_t$.\n\nhttps://i.imgur.com/qzsEq8A.png\n\nThe EM algorithm stops when for every transcript $t$, $\u03b1_tN$ > 0.01 changes less than 1%, where $N$ is the total number of reads. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1038/nbt.3519"
    },
    "158": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.00124",
        "transcript": "The central argument of the paper is that pruning deep neural networks by removing the smallest weights is not always wise. They provide two examples to show that regularisation in this form is unsatisfactory.\n\n## **Pruning via batchnorm**\nAs an alternative to the traditional approach of removing small weights, the authors propose pruning filters using regularisation on the gamma term used to scale the result of batch normalization. \n\nConsider a convolutional layer with batchnorm applied:\n```\nout = max{ gamma * BN( convolve(W,x)  + beta, 0 }\n```\n\nBy imposing regularisation on the gamma term the resulting image becomes constant almost everywhere (except for padding) because of the additive beta. The authors train the network using regularisation on the gamma term and after convergence remove any constant filters before fine-tuning the model with further training.   \n\nThe general algorithm is as follows:   \n- **Compute the sparse penalty for each layer.** This essentially corresponds to determining the memory footprint of each channel of the layer. We refer to the penalty as lambda.\n- **Rescale the gammas.** Choose some alpha in {0.001, 0.01, 0.1, 1} and use them to scale the gamma term of each layer - apply `1/alpha` to the successive convolutional layers.\n- **Train the network using ISTA regularisation on gamma.** Train the network using SGD but applying the ISTA penalty to each layer using `rho * lambda` , where rho is another hyperparameter and lambda is the sparse penalty calculated in step 1. \n- **Remove constant filters.**\n- **Scale back.** Multiply gamma by `1 / gamma` and gamma respectively to scale the parameters back up.\n- **Finetune.** Retrain the new network format for a small number of epochs.\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.00124"
    },
    "159": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1111/cdep.12282",
        "transcript": "Joint summary from https://mitpress.mit.edu/books/developmental-robotics\n\nDevelopmental robotics is the interdisciplinary approach to the autonomous design of behavioural and cognitive capabilities in artificial agents (robots) that takes direct inspiration from the developmental principles and mechanisms observed in the natural cognitive systems.  It relies on a highly interdisciplinary effort of empirical developmental sciences such as developmental psychology, neuroscience, and comparative psychology, and computational and engineering disciplines such as robotics and artificial intelligence.  The implementation of these principles and mechanisms into a robot-s control architecture and the testing through experiments where the robot interacts with its physical and social environment simultaneously permits the validation of such principles and the actual design of complex behavioural and mental capabilities in robots. Developmental psychology and developmental robotics mutually benefit from such a combined effort.\n\nSynonym terms of Developmental Robotics include cognitive developmental robotics, autonomous mental development as well as epigenetic robotics \\cite{Cangelosi18}. The  later term borrows the term \u2018epigenetic\u2019 from Piaget\u2019s Epigenetic Theory of human development, where the child\u2019s cognitive system develops as a result of the interaction between genetic predispositions and the organism\u2019s interaction with the environment.  Therefore, \u2018Epigenetic robotics\u2019 was justified by Piaget\u2019s stress on the importance of the role of interaction with the environment (later complemented with the emphasis on social interaction of Zlatev and Balkenius in 2001), and in particular, on the sensorimotor bases of higher-order cognitive capabilities. \nDue to the challenges of robotics domain not only on acquisition of skills in an incremental manner but also in open-ended environments, it is perhaps worth noticing that the state of the art is not as advanced as in other more constrained well defined unique task settings. Because of the importance of embodiment in robotics for open ended learning \\cite{Cangelosi18} and because of the scarcity of continual learning strategies on robotics and merely assessed  on perception tasks, we believe both fields methodologies should borrow from each other. The blend could result not only in more effective or robust lifelong learning but also in less forgetful agents (i.e., the main rationale behind CL), which has not yet empirically shown is impossible to reach. We hope this survey helps reach this goal.\n\nDynamical systems development: In math it is characterized by complex changes over time in the phase state. The complex interaction of nonlinear phenomena results in the production of unpredictable states of the system, often referred to as emergent states. This concept was borrowed by developmental psychologists \\cite{Thelen94} to explain child development as the emergent product of the intricate and dynamic interaction of many decentralized and local interactions related to the child growing body and brain and her environment.\n\n\nSome definitions: \n\nNovelty, curiosity and surprise: focuses on the autonomy of learning, i.e. on the agents freedom to choose what, when and how it will learn. Intrinsic motivation (IM) is a mechanism to drive autonomous learning, not only in developmental robotics but also more broadly within the field of machine learning \\cite{Mirolli13}\\cite{Oudeyer07}. IM is task-independent, the agent could be placed in a completely new environment with no prior knowledge  or experience, and through self-directed exploration the robot will potentially learn not only important features of the environment but also the behavioural skills necessary for dealing with the environment. Second, IM promotes the hierarchical learning rather than solving a specific, predefined task. \n\nThe Theory of MInd (ToM): A robot can use its own theory of mind to improve the interaction with human users, for example. It aims at reading the intention of the others and reacting appropriately to the emotional, attentional and cognitive states of the other agents, to anticipate their reactions, and to modify its own behaviour to satisfy these expectation and needs \\cite{Scasellati02}.\n\nCascades: developmental theories refer to the far reach of early developments on later ones in terms of the \u201cdevelopmental cascade\u201d.\n\nSaliency in robotics: \nIn robotics vision, a set of feature extraction methods typically can be applied to derive a saliency map \\cite{Itti01}, i.e. the identification of the parts of the image that are important for the robot behaviour. It can be created, e.g., by combining a set of feature extraction method for color, motion, orientation and brightness. These features are combined to generate the whole saliency  map used by the model to fixate the objects.\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1111/cdep.12282"
    },
    "160": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1805.09733",
        "transcript": "Through a likelihood-focused derivation of a variational inference (VI) loss, Variational Generative Experience Replay (VGER) presents the closest appropriate likelihood- focused alternative to Variational Continual Learning (VCL), the state-of the art prior-focused approach to continual learning.\n\n\nIn non continual learning, the aim is to learn parameters $\\omega$ using labelled training data $\\mathcal{D}$ to infer $p(y|\\omega, x)$. In the continual learning context, instead, the data is not independently and identically distributed (i.i.d.), but may be split into separate tasks $\\mathcal{D}_t = (X_t, Y_t)$ whose examples $x_t^{n_t}$ and $y_t^{n_t}$ are assumed to be i.i.d. \n\n\nIn \\cite{Farquhar18}, as the loss at time $t$ cannot be estimated for previously discarded datasets, to approximate the distribution of past datasets $p_t(x,y)$, VGER (Variational Generative Experience Replay) trains a GAN $q_t(x, y)$ to produce ($\\hat{x}, \\hat{y}$) pairs for each class in each dataset as it arrives (generator is kept while data is discarded after each dataset is used). The variational free energy $\\mathcal{F}_T$ is used to train on dataset $\\mathcal{D}_T$ augmented with samples generated by the GAN. In this way the prior is set as the posterior approximation from the previous task.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1805.09733"
    },
    "161": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1801.07440",
        "transcript": "Exploring an environment with non-linearities in a continuous action space can be optimized by regulating the agent curiosity with an homeostatic drive. This means that a heterostatic drive to move away from habitual states is blended with a homeostatic motivation to encourage actions that lead to states where the agent is familiar with a state-action pair. \n\nThis approach  improves upon forward models and ICM Pathak et al 17 with an enhanced information gain that basically consists of the following: while the reward in \\cite{Pathak17} is formulated as the forward model prediction error, the extended forward model loss in this paper is extended by substracting from the forward model prediction error the error knowing not only $s_t$ and $a_t$, but also $a_{t+1}$.\n\nCuriosity-driven reinforcement learning shows that an additional homeostatic drive enhances the information gain of a classical curious/heterostatic agent. \n\nImplementation: They take advantage of a new Bellman-like equation of information gain and simplify the computation of the local rewards. It could help by prioritizing the exploration of the state-action space according to how hard is to learn each region. \n\nBackground: The concept of homeostatic regulation in social robots was first proposed in Breazeal et al. 04. They extend existing approaches by compensating the heterostacity drive encouraged by the curiosity reward with an additional homeostatic drive.  1) The first component implements the heterostatic drive (same as referred to in Pathak et al 17). In other words, this one refers to the tendency to push away our agent from its habitual state; 2) Homeostatic motivation: the second component is our novel contribution. It encourages taking actions $a_t$ that lead to future states $s_{t+1}$ where the corresponding future action $a_{t+1}$ gives us additional information about $s_{t+1}$. This situation happens when the agent is \"familiar\" with the state-action pair: $\\{s_{t+1}, a_{t+1}\\}$.\n\nThe article misses exact comparison with Pathak et al regarding a joint task. In this paper the tasks consists of a 3 room navigation map is used to measure exploration.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1801.07440v1"
    },
    "162": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.08408",
        "transcript": "# Semantic Segmentation using Adversarial networks\n## Luc, Couprie, Chintala, Verbeek, 2016\n\n  * The paper aims to improve segmentation performance (IoU) by extending the network\n  * The authors derive intuition from GAN's, where a game is played between generator and discriminator.\n  * In this work, the game works as follows: a segmentation network maps an image WxHx3 to a label map WxHxC. a discriminator CNN is equipped with the task to discriminate the generated label maps from the ground truth. It is an adversarial game, because the segmentor aims for _more real_ label maps and the discriminator aims to distuinguish them from ground truth.\n  * The discriminator is a CNN that maps from HxWxC to a binary label.\n  * Section 3.2 outlines how to feed the label maps in three ways\n    * __Basic__ where the label maps are concatenated to the image and fed to the discriminator. Actually, the authors observe that leaving the image out does not change performance. So they end up feeding only the label maps for _basic_\n    * __Product__ where the label maps and input are multiplied, leading to an input of 3C channels\n    * __Scaling__ which resembles basic, but the one-hot distribution is perturbed a bit. This avoids the discriminator from trivially detecting the entropy rather than anything useful\n  * The discriminator is constructed with two axes of variation, leading to 4 architectures\n    * __FOV__: either a field of view of 18x18 or 34x34 over the label map\n    * __light__: an architecture with more or less capacity, e.g. number of channels\n  * The paper shows some fair result on the Stanford dataset, but keep in mind that it only contains 700 images\n  * The results in the Pascal dataset are minor, with the IoU improving from 71.8 to 72.0.\n  * Authors tried to pretrain the adversary, but they found this led to instable training. They end up training in an alternating scheme between segmentor and discriminator. They found that slow alternations work best.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.08408"
    },
    "163": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/TzengHDS15",
        "transcript": "# Simultaneous Deep transfer across domains and tasks\n## Tzeng, Hoffman, Saenko, 2015\n\n  * The paper aims to exploit unlabeled and sparsely labeled data from the target domain.\n  * As a baseline, they mention that one could match feature distributions between source and target domain. This work will also explore correlation between categories, such as _bottle_ and _mug._\n  * The authors derive inspiration from the _Name the dataset_ game by Torralbe and Efros. In this game, you train a classifier to predict which dataset an image originates from. This idea transpires into the domain confusion loss. The domain classifier measures the confusion between learned features from source and target domain. The image classifier learns a feature representation that makes the domain inditinguishable, as measured by the domain confusion.\n  * The second idea also learns the similarity structure between objects in the target domain. This works as follows. _We first compute the average output probability distribution, or \u201csoftlabel,\u201d over the source training examples in each category. Then, for each target labeled example, we directly optimize our model to match the distribution over classes to the soft label. In this way we are able to perform task adaptation by transferring information to categories with no explicit labels in the target domain._\n  * The experiments take place in two situations. The _supervised_ case, where only few labels are present in the target domain. The _semi supervised_ case, where only few labels of a subset of the classes are present.  \n  * In the final section, the authors perform analysis on theis own result. They show how the image classifier correctly labeled monitor, while no labels for monitor were present in the target domain.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ICCV.2015.463"
    },
    "164": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/BalanRMW15",
        "transcript": "It's not clear to me how predicting the variance with a neural network is a robust estimator of uncertainty. We all know the adversarial examples where we can simply fool a neural network with an example that is a little off. By a same argument, we could make adversarial examples to _fool_ the uncertainty estimator. I would like to see more work on this",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5965-bayesian-dark-knowledge"
    },
    "165": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.04049",
        "transcript": "### Contribution\n\nThe author conducts five experiments on EC2 to assess the impact of software-defined virtual networking with HTTP on composite container applications. Compared to previous container performance studies, it contributes new insight into the overlay networking aspect specifically for VM-hosted containers. Evidently, the SDVN causes a major performance loss whereas the container itself as well as the encryption cause minor (but still not negligible) losses. The results indicate that further practical work on container networking tools and stacks is needed for performance-critical distributed applications.\n\n### Strong points\n\nThe methodology of measuring the performance and using a baseline performance result is appropriate. The author provides the benchmark tooling (ppbench) and reference results (in dockerised form) to enable recomputable research.\n\n### Weak points\n\nThe title mentions microservices and the abstract promises design recommendations for microservice architectures. Yet, the paper only discusses containers which are a potential implementation technology but neither necessary for nor guaranteed to be microservices. Reducing the paper scope to just containers would be fair. The introduction contains an unnecessary redundant mention of Kubernetes, CoreOS, Mesos and reference [9] around the column wrap. The notation of SDN vs. SDVN is inconsistent between text and images; due to SDN being a wide area of research, the consistent use of SDVN is recommended. Fig. 3b is not clearly labelled. Resulting transfer losses - 100% means no loss, this is confusing. The y axis should presumably be inverted so that losses show highest for SDN with about 70%. The performance breakdown around 300kB messages in Fig. 2 is not sufficiently explained. Is it a repeating phenomenon which might be related to packet scheduling? The \"just Docker\" networking configuration is not explained, does it run in host or bridge mode? Which version of Docker was used? The size and time distribution of the 6 million HTTP requests should also be explained in greater detail to see how much randomness was involved.\n\n### Further comments\n\nThe work assumes that containers are always hosted in virtual machines while bare metal container hosting in the form of CaaS becomes increasingly available (Triton, CoreOS OnMetal, etc.). The results by Felter et al. are mentioned but not put into perspective. A comparison of how the networking is affected by VM/BM hosting would be a welcome addition, although AWS would probably not be a likely environment due to ECS running atop EC2.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1710.04049v1"
    },
    "166": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.00179",
        "transcript": "In this article, the authors provide a framework for training two translation models with large accessible monolingual corpus.\n\nIn traditional methods, machine translation models always require large parallel corpus to train a good quality model, which is expensive to acquire. However, the massive monolingual data is not fully utilized. The monolingual corpus are typically used in pretraining the NMT decoder rnn and augmenting initial parallel corpus through self-generated translations.\n\nThe authors embed machine translation task into a reinforcement learning framework, in which two agents act as two different native speakers respectively and know little about each other and then they learn to translate by trying to communicate with each other. \n\n**The two speakers**, `A` and `B`, obviously know well about their corresponding language respectively, this situation is easily simulated by two well-trained language models for `A` and `B`. Then, speaker `A` tries to tell a sentence $x$ to `B` by translating it into $y$ in `B`'s language. Since they don't know each other, `B` is uncertain about what `A` truly means by saying $y$. However, `B` is capable of evaluate the degree of sensibility of $y$  from his own understanding. Next, `B` informs `A` his sensibility evaluation score and tries to recover what `A` truly means in `A`'s language, i.e. $x'$. And similarly, `A` can also evaluate the degree of sensibility of $x'$  from his own understanding.\n\nIn general, the very original idea that `A` tried to convey, is passed through a noisy channel to `B`, and then back to `A` through another noisy channel. The former noisy channel is a `A-B` translation model and the latter a `B-A` translation model in the framework.\n\nThink about how the first American learnt Chinese in history and I think it is intuitively similar to the principle in this work.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.00179"
    },
    "167": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.04492",
        "transcript": "### General Approach\n\nThe Neural Tree Indexer (NTI) approach succeeded to reach 87.3\\% test accuracy on SNLI. Here I'll attempt to clearly describe the steps involved based on the publication [1] and open sourced codebase [2].\n\nNTI is a method to apply attention over a tree, specifically applied to sentence pairs. There are three main steps, each giving an incrementally more expressive representation of the input. It's worth noting that the tree is a full binary tree, so sentence lengths are padded to a factor of 2. In this case, the padded length used is $2^5 = 32$.\n\n- **Sequence Encoding.** Run an RNN over your sentence to get new hidden states for each element.\n\n$$h_t = f_1^{rnn}(i_t, h_{t-1})$$\n    \n- **Tree Encoding.** Using the hidden states from the previous step, use a variant of TreeLSTM to combine leaves until you have a single hidden state representing the entire sentence. Keep all of the intermediary hidden states for the next step.\n    \n$$ h_t^{tree} = f^{tree}(h_l^{tree},h_r^{tree})$$\n    \n- **Attention on Opposite Tree.** Until now we've only been describing how to encode a single sentence. When incorporating attention, we attend on the opposite tree by using the hidden states from the previous step. For instance, here is how we'd encode the premise (where the $p,h$ superscripts denote the premise or hypothesis, and $\\vec{h}^{h,tree}$ denotes all of the hidden states of the non-attended hypothesis tree.:\n    \n$$h_t^p = f_1^{rnn}(i_t^p, h_{t-1}^p) \\\\\n    h_t^{p,tree} = f^{tree}(h_l^{p,tree},h_r^{p,tree}) \\\\\n    i_t^{p,attn} = f^{attn}(h_t^{p,tree}, \\vec{h}^{h,tree}) \\\\\n    h_t^{p,attn} = f_2^{rnn}(i_t^{p,attn}, h_{t-1}^{p,attn})\n$$\n\n### Datasets\n\nNTI was evaluated on three datasets. Some variant of the model achieved state-of-the-art in some category for each dataset:\n\n- SNLI [3]: Sentence Pair Classification.\n- WikiQA [4]: Answer Sentence Selection.\n- Stanford Sentiment TreeBank (SST) [5]: Sentence Classification.\n\n### Implementation Details\n\n- Batch size is $32$ pairs (so $32$ of each premise and hypothesis).\n- Tree is full binary tree with $2^5 = 32$ leaves.\n- All sentences are padded left to length $32$, matching the full binary tree.\n- Steps 1 (sentence encoding) runs on all sentence simultaneously. So is Step 2 (tree encoding). Step 3 (attention) is done first on the premise, then on the hypothesis.\n- The variant of TreeLSTM used is S-LSTM. It's available as a standard function in Chainer.\n- Dropout is applied liberally in each step. The keep rate is fixed at $80\\%$.\n- MLP has $1$ hidden layer with dimension $1024$. Dimensions of the entire MLP are: $(2 \\times H) \\times 1024 \\times 3$. $H$ is the size of the hidden states and is $300$.\n- Uses Chainer's Adam optimizer with $\\alpha=0.0003,\\beta_1=0.9,\\beta_2=0.999,\\epsilon=10^{-8}$. Gradient clipping using L2 norm of $40$. Parameters periodically scaled by $0.00003$ (weight decay).\n- Weights are initialized uniformly random between $-0.1$ and $0.1$.\n\n[1]: https://arxiv.org/abs/1607.04492\n[2]: https://bitbucket.org/tsendeemts/nti/overview\n[3]: nlp.stanford.edu/projects/snli/\n[4]: https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/\n[5]: http://www.socher.org/index.php/Main/SemanticCompositionalityThroughRecursiveMatrix-VectorSpaces",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.04492"
    },
    "168": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1016/j.tree.2011.01.002",
        "transcript": "There is no correlation between body size and cancer incidence across animal species. From a logical point of view such a correlation would be expected. If one assumes that each proliferating cell in an individual multicellular organism has an equal probability of acquiring a cancerous mutation, then organisms with significantly more cells should have a higher probability of developing a cancerous tumour. This is known as Peto's paradox. \n\nThis paper puts Peto's paradox in the context of an evolutionary strategy to allow large multicellular organisms to live beyond reproductive age. Evolutionary theory describes the genetic instabilities (and variations) leading to the development of tumour suppression mechanisms. The evolutionary rules are clearly stated in box 1 and are reasonable considering the known heterogeneity of tumours.\n\nI particularly like this paper because it puts some captivating numbers on the effect of tumour suppression in some animals. It takes one extreme of animal size, and a well known character in terms of tumour suppression, the blue whale, and makes back of the envelope calculations to what its cancer incidence should theoretically be. Given that the blue whale is 1000 times the size of a human the authors predict that all blue whales should have colorectal cancer by the age of 80. For information, blue whales can live to more than 100 years.\n\nWithin a species size is related to cancer risk (Hooray I knew there would be some advantage to being only 165cm) whereby a 3-4 mm increase above the average leg length results in an 80% higher risk of non smoking related cancers. All this suggests that large organisms (that also happen to live longer) have acquired mechanism to suppress cancer.\n\nThe authors describe such cancer suppression mechanisms including lower somatic mutation rates, different tissue architecture, redundancy in tumour suppressor genes and a somehow lower selective advantage of mutant cells and increased sensitivity to contact exhibition to name but a few.\n\nA particularly tantalising theory suggested by Nagy et al is that of \"hypertumours\" whereby a parasitic growth from the cancerous tumour results in a lowering of the overall fitness tumour fitness. This hypothesis has yet to be tested.\n\nMy understanding is that comparing cancer suppression mechanisms across the species will lead to a better understanding of the evolutionary process involved in cancer progression and perhaps will reveal knowledge to help better develop strategies for cancer therapies in humans.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/j.tree.2011.01.002"
    },
    "169": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.09549",
        "transcript": "Summary:\nThis paper suggests an approach to find correlation score between different sub-window of a search image with a query image. Using a fully convolutional siamese network architecture that they describe helps in getting this correlation for different sub windows for search images in one forward pass of the network. For every video, they compute the features for the object being tracked once and use it for entire duration of video for computing correlation.\n\nMy take:\nThis is in the same spirit as GOTURN tracker. Although having fully convolutional helps in having translation invariance, it is not directly an advantage over predicting bounding boxes directly as adopted in GOTURN paper. Also, results are not directly comparable as this has been trained on a different data-set.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.09549"
    },
    "170": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1604.01802",
        "transcript": "This paper introduces a bunch of tricks which make sense for visual tracking. These tricks are as followed:\n1. At test time, a crop with center at the previous frame's bounding box's center with size larger than the bounding box is given along with the search area in the current frame.\n2. Training offline on a large set of videos (where object bounding boxes are given for a subset of frames) and images with object bounding boxes.\n3. Network takes two images: i) a crop of the image/frame around the bounding box and  ii) the image centered at the center of the bounding box. Given the later, network regresses the bounding box in i).\n4. Above crops are sampled such that the ground truth bounding box center in i) is not very far from the center in ii), hence network prefers smooth motion.\n\nMy take: This is very nice way to use still images to train image correlation task and hence can be used for tracking. Speed on gpu is very impressive but still not comparable on CPUs.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1604.01802"
    },
    "171": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.08803",
        "transcript": "Just a brief comment about your summary: \nThis is indeed excellent work, but contrary to what you seem to say, the basic ideas behind this framework were already there in previous work, notably Laurent Dinh et al's previous paper and very related model, dubbed NICE (arXiv 2014, ICLR 2015). NVP extends the building blocks and uses more recent tricks (BatchNorm and ResNets) in a way that ends up being highly successful in bringing impressive performance, but your review made it sound like if the basic framework was completely new. NICE itself builds on a long series of attempts to exploit the change of variable formula for density estimation using neural networks, including in my thesis and in ICA...",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.08803"
    },
    "172": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/JozefowiczVSSW16",
        "transcript": "#### This nice paper looks amazing at the first sight since it brings a mixture of:\n- Fancy models \n- State-of-art training procedure(considering the 32-GPU distributed training effort which takes 21 days to get the best result) \n- Significant theory metric improvement(single model: 51.3 -> 30 perplexity reduction, ensemble model:41.0 -> 23.7)\n- Benchmark on a somewhat industry scale(vocabulary of 793471 words,  0,8B words training data) data-set rather than a pure research one.\n    \n#### However, I also want to add some criticism:\n- As [1] mentioned perplexity is somewhat confusing metric, big perplexity may not reflect the real improvement, it would rather bring some kind of \"exaggerating\" effect.\n- This paper only provide the language model improvement, however, LMs are usually embedded into a complex usage scenario, such as speech recognition or machine translation. It would be more insightful if the LMs provided in this paper could share its result with integrating into some end-to-end products. Since the authors are working for Google Brain team, this is not too much a stringent requirement. \n- So far as I know, the data set used by this paper is from news stories[2], this kind of data set is more formal than oral one. And for real application, what we face are usually less formal data(such as search engine and speech recognition). It is still a question what the best model mentioned in this paper will perform in a more realistic scenario. Again, for Google Brain team, this should not be a big obstacles for integrating it with existing system just by replacing or complementing the existing LMs.\n     \nAlthough I posted some personal criticism, I do still appreciate this nice paper and recommend this as a \"must-read\" for NLP and related guys since I do think this paper provide a unifying and comprehensive survey-style perspective for us to help grasp the latest state-of-art language model technology in an efficient way. \n \nReferences:\n- [1].http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf\n- [2].http://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41880.pdf\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.02410"
    },
    "173": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/VenugopalanRDMD15",
        "transcript": "It is a nice paper on video captioning. They exploit LSTM ability to learn long term dependencies to modeling the problem of translating video sequence to language sequence. The new thing in this paper is that they have two LSTM layers for modeling frames in videos and also words in sentences.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ICCV.2015.515"
    },
    "174": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZhangL17d",
        "transcript": "* Output can contain several sentences, that are considered as a single long sequence. \n* Seq2Seq+attention:\n  * Oddly they use the formula used by Bahdanau attention weights to combine the weighted attention $c_t$ with the decoder output $h_t^T =  W_0 \\tanh \\left( U_h h_t^T + W_h c_t \\right) $ while the attention weights are computed with softmax over dot product between encoder and decoder outputs $h_t^T \\cdot h_i^S$\n  * Glove 300\n  * 2 layer LSTM 256\n* RL model\n  * Reward=Simplicity+Relevance+Fluency = $\\lambda^s r^S + \\lambda^R r^R + \\lambda^F r^F$\n    * $r^S = \\beta \\text{SARI}(X,\\hat{Y},Y) + (1-\\beta) \\text{SARI}(X,Y,\\hat{Y})$\n    * $r^R$ cosine of output of RNN auto encoder run on input and a separate auto encoder run on output\n    * $r^F$ perplexity of LM trained on output\n  * Learning exactly as in [MIXER](https://arxiv.org/abs/1511.06732)\n* Lexical Simplification model: they train a second model $P_{LS}$ which uses pre-trained attention weights and then use the weighted output of an encoder LSTM as the input to a softmax\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.10931"
    },
    "175": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1705.04304",
        "transcript": "Generates abstractive summaries from news articles. Also see [blog](https://metamind.io/research/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization)\n* Input:\n * vocab size 150K\n * start with $W_\\text{emb}$ Glove 100\n* Seq2Seq:\n  * bidirectional LSTM, `size=200` in each direction. Final hidden states are concatenated and feed as initial hidden state of the decoder an LSTM of `size=400`. surprising it's only one layer.\n* Attention:\n  * Add standard attention mechanism between each new hidden state of the decoder and all the hidden states of the encoder\n  * A new kind of attention mechanism is done between the new hidden state of the decoder and all previous hidden states of the decoder\n  * the new hidden state is concatenated with the two attention outputs and feed to dense+softmax to model next word in summary (output vocab size 50K). The weight matrix $W_h$ is reduced to $W_h = \\tanh \\left( W_\\text{emb} W_\\text{proj} \\right) $ resulting in faster converges, see [1](arXiv:1611.01462) and [2](https://arxiv.org/abs/1608.05859)\n* Pointer mechanism:\n  * The concatenated values are also feed to logistic classifier to decide if the softmax output should be used or one of the words in the article should be copied to the output. The article word to be copied is selected using same weights computed in the attention mechanism\n* Loss\n  * $L_\\text{ml}$: NLL of the example summary $y^*$. If only $L_\\text{ml}$ is used then 25% of the times use generated instead of given sample as input to next step. \n  * $L_\\text{rl}$: sample an entire summary from the model $y^s$ (temperature=1) and the loss is the NLL of the sample multiplied by a reward. The reward is $r(y^s)-r(\\hat{y})$ where $r$ is ROUGE-L and $\\hat{y}$ is a generated greedy sequences\n * $L=\\gamma L_\\text{rl} + (1-\\gamma)L_\\text{ml}$ where $\\gamma=0.9984$\n* Training\n  * `batch=50`, Adam,  `LR=1e-4` for RL/ML+RL training\n  * The training labels are summary examples and an indication if copy was used in the pointer mechanism and which word was copied. This is indicated when the summary word is OOV or if it appears in the article and its NER is one of PERSON, LOCATION, ORGANIZATION or MISC\n* Generation\n  * 5 beams\n  * force trigrams not to appear twice in the same beam\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1705.04304"
    },
    "176": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.03382",
        "transcript": "### Read-Again\nTwo options:\n* GRU: run a pass of regular GRU on the input text $x_1,\\ldots,x_n$. Use its hidden states $h_1,\\ldots,h_n$ to compute weights vector for every step $i$ :\n$\\alpha_i = \\tanh \\left( W_e h_i  + U_e h_n + V_e x_i\\right)$ and then runs a second GRU pass on the same input text. In the second pass the weights $\\alpha_i$, from the first pass, are multiplied with the internal $z_i$ GRU gatting (controlling if hidden state is directly copied) of the second pass.\n* LSTM: concatenate the hidden states from the first pass with the input text\n$\\left[ x_i, h_i, h_n \\right]$ and run a second pass on this new input.\n\nIn case of multiple sentences the above passes are done per sentence. In addition the $h^s_n$ of each sentence $s$ is concatenated with the $h^{s'}_n$ of the other sentences or with $\\tanh \\left( \\sum_s V_s h_s  + v\\right)$\n\n### Decoder with copy mechanism\nLSTM with hidden state $s_t$. Input is previously generated word $y_{t-1}$ and context computed with attention mechanism: $c_t = \\sum_i^n \\beta_{it} h_i$. Here $h_i$ are the hidden states of the 2nd pass of the encoder. The weights are $\\beta_{it} = \\text{softmax}  \\left(  v_a^T \\tanh \\left( W_a s_{t-1}  + U_a h_i\\right) \\right)$\n\nThe decoder vocabulary $Y$ used is small. If $y_{t-1}$ does not appear in $Y$ but does appear in the input at $x_i$ then its embedding is replaced with $p_t = \\tanh \\left( W_c h_i + b_c\\right)$ and <UNK> otherwise.\n\n$p_t$ is also used to copy the input to the output (details not given)\n\n### Experiments\nabstractive summarization [DUC2003 and DUC2004 competitions](http://www-nlpir.nist.gov/projects/duc/data.html).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.03382"
    },
    "177": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1610.02424",
        "transcript": "[Code](https://github.com/ashwinkalyan/dbs), [Live Demo](http://dbs.cloudcv.org/) ([code for demo site]( https://github.com/Cloud-CV/diverse-beam-search))\n\nDiverse Beam Search (DBS) is an alternative to Beam Search (BS). Decodes diverse lists by dividing the beam budget $B$ (e.g. 6) into groups $G$ (e.g. 3) and enforcing diversity between groups of beams.\n\nFor every time step $t$ iterate over all groups. In 1st group find $B'=B/G$ (e.g. 2) partial beams $Y^1_{[t]} = \\{y^1_{b,t} : b \\in [B']\\}$ using BS with NLL. In 2nd group find partial beams $y^2_{b,t}$ using BS with partial beam score taken to be the sum of NLL and the distance between the partial beam and the partial beams in 1st group. The distance is multiplied by a factor $\\lambda_t$. For group $g$ the distance is measured between the partial beam $y^g_{b,t}$ and all the partial beams in all groups that were already optimized for current time step. $\\Delta(Y^1_{[t]},\\ldots,Y^{g-1}_{[t]})[y^g_{b,t}]$\n\nEvaluation Metrics:\n* Oracle Accuracy: maximum value of the metric (BLEU) over a list of final beams\n* Diversity Statistics: number of distinct n-grams in all final beams\n* Human preference\n\nParameters:\n* $G=B$ allows for the maximum exploration and  found to improve oracle accuracy.\n* $\\lambda \\in [0.2-0.8]$\n\nDistance between partial beam and all other groups is broken to a sum of the distances with each group:\n$$\\Delta(Y^1_{[t]},\\ldots,Y^{g-1}_{[t]}) = \\sum^{g-1}_{h=1}\\Delta(Y^h_{[t]})$$\nindividual $\\Delta(Y^h_{[t]})[y^g_{b,t}]$ is taken to be one of:\n* Hamming (gives best oracle performance): proportional to the number of times latest token in $y^g_{b,t}$ was selected as latest token in beams in $Y^h_{[t]}$.\n* Cumulative: cancels out Hamming: $\\exp\\{-(\\sum_{\\tau \\in t} \\sum_{b \\in B'} \\mathbb{1}_{[y^h_{b,\\tau} \\neq y^g_{b,\\tau}]})/\\Gamma\\}$\n* n-gram: number of times each n-gram in a candidate occurred in previous groups\n* Neural-embedding: in all previous methods replace hamming similarity with cosine of word2vec of token (or sum of word2vec of n-gram tokens)\n\n\nMy 2 cents:\n* Once a beam reaches EOS you need to stop comparing it with other groups\n* Using DBS cause results to be longer. Perhaps too much. You can reduce length by adding a penalty to length",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1610.02424v1"
    },
    "178": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.08144",
        "transcript": "This is a very techincal paper and I only covered items that interested me\n* Model\n  * Encoder\n    * 8 layers LSTM \n    * bi-directional only first encoder layer\n    * top 4 layers add input to output (residual network)\n  * Decoder\n    * same as encoder except all layers are just forward direction\n  * encoder state is not passed as a start point to Decoder state\n  * Attention\n    * energy computed using NN with one hidden layer as appose to dot product or the usual practice of no hidden layer and $\\tanh$ activation at the output layer\n    * computed from output of 1st decoder layer\n    * pre-feed to all layers\n* Training has two steps: ML and RL\n  * ML (cross-entropy) training:\n    * common wisdom, initialize all trainable parameters uniformly between [-0.04, 0.04]\n    * clipping=5, batch=128\n    * Adam (lr=2e-4) 60K steps followed by SGD (lr=.5 which is probably a typo!) 1.2M steps + 4x(lr/=2 200K steps)\n    * 12 async machines, each machine with 8 GPUs (K80) on which the model is spread X 6days\n    * [dropout](http://www.shortscience.org/paper?bibtexKey=journals/corr/ZarembaSV14) 0.2-0.3 (higher for smaller datasets)\n  * RL - [Reinforcement Learning](http://www.shortscience.org/paper?bibtexKey=journals/corr/RanzatoCAZ15) \n    * sequence score, $\\text{GLEU} = r = \\min(\\text{precision}, \\text{recall})$ computed on n-grams of size 1-4\n    * mixed loss $\\alpha \\text{ML} + \\text{RL}, \\alpha =0.25$\n    * mean $r$ computed from $m=15$ samples\n    * SGD, 400K steps, 3 days, no drouput\n* Prediction (i.e. Decoder)\n  * beam search (3 beams)\n  * A normalized score is computed to every beam that ended (died)\n    * did not normalize beam score by $\\text{beam_length}^\\alpha , \\alpha \\in [0.6-0.7]$\n    * normalized with similar formula in which 5 is add to length and a coverage factor is added, which is the sum-log of attention weight of every input word (i.e. after summing over all output words)\n    * Do a second pruning using normalized scores\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1609.08144"
    },
    "179": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1608.05343",
        "transcript": "http://deliprao.com/archives/187",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1608.05343"
    },
    "180": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.02960",
        "transcript": "This paper is covered by author in this [talk](https://github.com/udibr/notes/blob/master/Talk%20by%20Sasha%20Rush%20-%20Interpreting%2C%20Training%2C%20and%20Distilling%20Seq2Seq%E2%80%A6.pdf)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.02960"
    },
    "181": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1603.06042",
        "transcript": "[Parsey McParseface](http://github.com/tensorflow/models/tree/master/syntaxnet) is  a parser of English sentences capable of finding parts of speech and dependency parsing. By Michael Collins and google NY.\n\nThis paper is more than just about google's data collection and computing powers. The parser uses a feed forward NN, which is much faster than the RNN usually used for parsing. Also the paper is using a global method to solve the label bias problem. This method can be used for many tasks and indeed in the paper it is used also to shorten sentences by throwing unnecessary words.\n\nThe label bias problem arises when predicting each label in a sequence using a softmax over all possible label values in each step. This is a local approach but what we are really interested in is a global approach in which the sequence of all labels that appeared in a training example are normalized by all possible sequences. This is intractable so instead a beam search is performed to generate alternative sequences to the training sequence. The search is stopped when the training sequence drops from the beam or ends. The different beams with the training sequence are then used to compute the global loss. \n\nSimilar method is used in [seq2seq by Sasha Rush](http://arxiv.org/pdf/1606.02960.pdf) and  [talk](https://github.com/udibr/notes/blob/master/Talk%20by%20Sasha%20Rush%20-%20Interpreting%2C%20Training%2C%20and%20Distilling%20Seq2Seq%E2%80%A6.pdf)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1603.06042"
    },
    "182": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/AllamanisPS16",
        "transcript": "[web site](http://groups.inf.ed.ac.uk/cup/codeattention/), [code (Theano)](https://github.com/mast-group/convolutional-attention), [working version of code](https://github.com/udibr/convolutional-attention), [ICML](http://icml.cc/2016/?page_id=1839#971), [external notes](https://github.com/jxieeducation/DIY-Data-Science/blob/master/papernotes/2016/02/conv-attention-network-source-code-summarization.md)\n\nGiven an arbitrary snippet of Java code (~72 tokens) generate the methods name (~3 tokens):\ngeneration starts with a $m_0 = \\text{start-symbol}$ and state $h_0$, to generate next output token $m_t$ do:\n* convert code tokens $c_i$ and embed to $E_{c_i}$\n* convert all $E_{c_i}$ to $\\alpha$ and $\\kappa$ all same length as code using a network of `Conv1D` and padding (`Conv1D` because the code is highly structured, unambiguous.) The convertion is done using following network:\n![](http://i.imgur.com/cHbiSIi.png?1)\n* $\\alpha$ and $\\kappa$ are probabilities over length of code (using softmax).\n* In addition compute $\\lambda$ by running another `Conv1D` over $L_\\text{feat}$ with $\\sigma$ activation and take the maximal value. \n* use $\\alpha$ to weight average $E_{c_i}$ and pass the average through FC layer to end with a softmax over output vocabulary $V$. Probability for output word $m_t$ is $n_{m_t}$.\n* As an alternative use $\\kappa$ to give probability to use as output each of the tokens $c_i$ which can be inside $V$ or outside it. This is also called \"translation-invariant features\" ([ref](https://papers.nips.cc/paper/5866-pointer-networks.pdf))\n* $\\lambda$ is used as a meta-attention deciding which to use:\n$P(m_t \\mid h_{t-1},c) = \\lambda \\sum_i \\kappa_i I_{c_i = m_t} + (1-\\lambda) \\mu n_{m_t}$\nwhere $\\mu$ is $1$ unless you are in training and $m_t$ is UNK and the correct value for $m_t$ appears in $c$ in which case it is $e^{-10}$\n* Advance $h_{t-1}$ to $h_t$ with GRU and using as input the embedding of output token $m_{t-1}$ (while training this is taken from the training labels or with small probability the argmax of the generated output.)\n* Generating using hybrid breadth-first search and beam search: keep a heap of all suggestions and always try to extend the best suggestion so far. Remove suggestions that are worse than all the completed suggestions (dead) so far.\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.03001"
    },
    "183": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.03474",
        "transcript": "multi layer RNN in which first layer is LSTM, following layers $l$ have $t$,$c$ gates that control whether the state of the layer is carried from previous state or transferred previous layer:\n$s_l^{[t]} = h_l^{[t]} \\cdot t_l^{[t]} + s_{l-1}^{[t]} \\cdot c\n_l^{[t]}$",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.03474"
    },
    "184": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SalimansGZCRC16",
        "transcript": "[code](https://github.com/openai/improved-gan),\n[demo](http://infinite-chamber-35121.herokuapp.com/cifar-minibatch/1/?),\n[related](http://www.inference.vc/understanding-minibatch-discrimination-in-gans/)\n\n### Feature matching\nproblem: overtraining on the current discriminator\n\nsolution:\n\ufffc$||E_{x \\sim p_{\\text{data}}}f(x) - E_{z \\sim p_{z}(z)}f(G(z))||_{2}^{2}$\n\nwere f(x) activations intermediate layer in discriminator\n### Minibatch discrimination\nproblem: generator to collapse to a single point\n\nsolution: for each sample i, concatenate to $f(x_i)$ features $b$ measuring its distance to other samples j (i and j are both real or generated samples in same batch):\n$\\sum_j \\exp(-||M_{i, b} - M_{j, b}||_{L_1})$\n\ufffc\nthis generates visually appealing samples very quickly\n### Historical averaging\nproblem: SGD fails by going into extended orbits\n\nsolution: parameters revert to the mean\n$|| \\theta - \\frac{1}{t} \\sum_{i=1}^t \\theta[i] ||^2$\n\ufffc\n### One-sided label smoothing\nproblem: discriminator vulnerability to adversarial examples\n\nsolution: discriminator target for positive samples is 0.9 instead of 1\n\n### Virtual batch normalization\nproblem: using BN cause output of examples in batch to be dependent\n\nsolution: use reference batch chosen once at start of training and each sample is normalized using itself and the reference. It's\nexpensive so used only on generation\n\n### Assessment of image quality\nproblem: MTurk not reliable\n\nsolution: use inception model p(y|x) to compute \n\ufffc$\\exp(\\mathbb{E}_x \\text{KL}(p(y | x) || p(y)))$\non 50K generated images x\n\n### Semi-supervised learning\nuse the discriminator to also classify on K labels when known and\nuse all real samples (labels and unlabeled) in the discrimination task\n\ufffc$D(x) = \\frac{Z(x)}{Z(x) + 1}, \\text{ where } Z(x) = \\sum_{k=1}^{K} \\exp[l_k(x)]$.\nIn this case use feature matching but not minibatch discrimination.\nIt also improves the quality of generated images.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.03498"
    },
    "185": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1807-11626",
        "transcript": "When machine learning models need to run on personal devices, that implies a very particular set of constraints: models need to be fairly small and low-latency when run on a limited-compute device, without much loss in accuracy. A number of human-designed architectures have been engineered to try to solve for these constraints (depthwise convolutions, inverted residual bottlenecks), but this paper's goal is to use Neural Architecture Search (NAS) to explicitly optimize the architecture against latency and accuracy, to hopefully find a good trade-off curve between the two. \n\nThis paper isn't the first time NAS has been applied on the problem of mobile-optimized networks, but a few choices are specific to this paper. \n\n1. Instead of just optimizing against accuracy, or optimizing against accuracy with a sharp latency requirement, the authors here construct a weighted loss that includes both accuracy and latency, so that NAS can explore the space of different trade-off points, rather than only those below a sharp threshold.  \n2. They design a search space where individual sections or \"blocks\" of the network can be configured separately, with the hope being that this flexibility helps NAS trade off complexity more strongly in the early parts of the network, where, at a higher spatial resolution, it implies greater computation cost and latency, without necessary dropping that complexity later in the network, where it might be lower-cost. Blocks here are specified by the type of convolution op, kernel size, squeeze-and-excitation ratio, use of a skip op, output filter size, and the number of times an identical layer of this construction will be repeated to constitute a block. \n\nMechanically, models are specified as discrete strings of tokens (a block is made up of tokens indicating its choices along these design axes, and a model is made up of multiple blocks). These are represented in a RL framework, where a RNN model sequentially selects tokens as \"actions\" until it gets to a full model specification . This is repeated multiple times to get a batch of models, which here functions analogously to a RL episode. These models are then each trained for only five epochs (it's desirable to use a full-scale model for accurate latency measures, but impractical to run its full course of training). After that point, accuracy is calculated, and latency determined by running the model on an actual Pixel phone CPU. These two measures are weighted together to get a reward, which is used to train the RNN model-selection model using PPO. \n\nhttps://i.imgur.com/dccjaqx.png\n\nAcross a few benchmarks, the authors show that models found with MNasNet optimization are able to reach parts of the accuracy/latency trade-off curve that prior techniques had not.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1807.11626"
    },
    "186": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2010-13321",
        "transcript": "The goal of this paper is to learn a model that embeds 2D keypoints(the locations of specific key body parts in 2D space) representing a particular pose into a vector embedding where nearby points in embedding space are also nearby in 3D space. This sort of model is useful because the same 3D pose can generate a wide variety of 2D pose projections, and it can be useful to learn which apparently-distinct representations actually map to the same 3D pose. \n\nTo do this, the basic approach used by the authors (with has a few variants), is\n\n- Take a dataset of 3D poses, and corresponding 2D projections\n- Define a notion of \"matching\" 3D poses, based on a parameter kappa, which designates the maximum average per-joint distance at which two 3D poses can be considered the same\n- Construct triplets composed of an anchor pose, a \"positive\" pose (a different 2D pose with a matching 3D pose), and a \"negative\" pose (some other 2D pose sampled from the dataset using a strategy that explicitly seeks out hard negative examples)\n- Calculate a triplet loss, that pushes positive examples closer together, and pulls negative examples farther apart. This is specifically done by defining a probabilistic representation of p(match | z1, z2), or, the probability of a match in 3D space given the embeddings of the two 2D poses. This is parametrized using a sigmoid with trainable parameters, as shown below\n\nhttps://i.imgur.com/yFCCVuA.png\n\n- They they calculate a distance kernel as the negative log of that probability, and calculate the basic triplet loss, which tries to maximize the diff between the the distance between negative examples, and the distance between positive examples.\n- They also add an additional loss further incentivizing the match probability to be higher on the positive pair (in addition to just pushing the positive and negative pair further apart)\n- The final loss is a Gaussian prior loss, incentivizing the learned embeddings z to be in the shape of a Gaussian\n\nhttps://i.imgur.com/SxvcvJG.png\nThis represents the central shape of the method. Some additional ablations include: \n\n- Camera Augmentation: Creational additional triplets by taking existing 3D poses and generating artificial pairs of 2D poses at different camera views\n- Temporal Pose Embedding -  Embedding multiple temporally connected pose, rather than just a single one\n- Keypoint Dropout - To simulate situations where some keypoints are occluded, the authors tried training with some keypoints dropped out, either keypoints selected at random, or selected jointly and non-independently based on a model of which keypoints are likely to be occluded together\n\nThe authors found that their method was generally quite a bit stronger that prior approaches for the task of querying similar 3D poses given a 2D pose input, including some alternate methods that do direct 3D estimation.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2010.13321"
    },
    "187": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=MahMoo16Communication",
        "transcript": "Federated learning is the problem of training a model that incorporates updates from the data of many individuals, without having direct access to that data, or having to store it. This is potentially desirable both for reasons of privacy (not wanting to have access to private data in a centralized way), and for potential benefits to transport cost when data needed to train models exists on a user's device, and would require a lot of bandwidth to transfer to a centralized server.  \n\nHistorically, the default way to do Federated Learning was with an algorithm called FedSGD, which worked by: \n\n- Sending a copy of the current model to each device/client\n- Calculating a gradient update to be applied on top of that current model given a batch of data sampled from the client's device\n- Sending that gradient back to the central server\n- Averaging those gradients and applying them all at once to a central model\n\nThe authors note that this approach is equivalent to one where a single device performs a step of gradient descent locally, sends the resulting *model* back to the the central server, and performs model averaging by averaging the parameter vectors there. Given that, and given their observation that, in federated learning, communication of gradients and models is generally much more costly than the computation itself (since the computation happens across so many machines), they ask whether the communication required to get to a certain accuracy could be better optimized by performing multiple steps of gradient calculation and update on a given device, before sending the resulting model back to a central server to be average with other clients models. \n\nSpecifically, their algorithm, FedAvg, works by: \n\n- Dividing the data on a given device into batches of size B\n- Calculating an update on each batch and applying them sequentially to the starting model sent over the wire from the server\n- Repeating this for E epochs\n\nConceptually, this should work perfectly well in the world where data from each batch is IID - independently drawn from the same distribution. But that is especially unlikely to be true in the case of federated learning, when a given user and device might have very specialized parts of the data space, and prior work has shown that there exist pathological cases where averaged models can perform worse than either model independently, even *when* the IID condition is met. \n\nThe authors experiment empirically ask the question whether these sorts of pathological cases arise when simulating a federated learning procedure over MNIST and a language model trained on Shakespeare, trying over a range of hyperparameters (specifically B and E), and testing the case where data is heavily non-IID (in their case: where different \"devices\" had non-overlapping sets of digits). \n\nhttps://i.imgur.com/xq9vi8S.png\n\nThey show that, in both the IID and non-IID settings, they are able to reach their target accuracy, and are able to do so with many fewer rounds of communciation than are required by FedSGD (where an update is sent over the wire, and a model sent back, for each round of calculation done on the device.) The authors argue that this shows the practical usefulness of a Federated Learning approach that does more computation on individual devices before updating, even in the face of theoretical pathological cases.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.05629"
    },
    "188": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2110.15349",
        "transcript": "In certain classes of multi-agent cooperation games, it's useful for agents to be able to coordinate on future actions, which is an obvious use case for having a communication channel between the two players. However, prior work in multi-agent RL has shown that it's surprisingly hard to train agents that (1) consistently learn to use a communication channel in a way that is informative rather than random, and (2) if they do use communication, can come to a common grounding on the meaning of symbols, to use them in an effective way. \n\nThis paper suggests the straightforward and clever approach of, instead of just having agents communicate using arbitrary vectors produced as part of a policy, having those communication vectors be directly linked to the content of an agent's observations. Specifically, this is done by taking the encoding of the image that is used for making policy decisions, and passing that encoding through an autoencoder, using the bottleneck at the middle of the autoencoder as the communication vector sent to other agents. This structure incentivizes the agent to generate communication vectors that are intrinsically grounded in the observation, enforcing a certain level of consistency that the authors hope makes it easier for the other agent to follow and interpret the communication.  \n\nhttps://i.imgur.com/u9OAZm8.png\n\nEmpirically, there seem to be fairly compelling evidence that this autoencoder-based form of grounding is more stable and thus more mutually learnable than learning from RL alone. The authors even found that adding RL training to the autoencoder-based training deteriorated performance.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2110.15349"
    },
    "189": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2104.11178",
        "transcript": "This strikes me as a really straightforward, clever, and exciting paper that uses the supervision intrinsic in the visual, audio, and text streams of a video to train a shared multimodal model. \n\nThe basic premise is: \n\n- Tokenize all three modalities into a sequence of embedding tokens. For video, split into patches, and linearly project the voxels of these patches to get a per-token representation. For audio, a similar strategy but with waveform patches. For text, the normal per-token embedding is done. Combine this tokenization with a modality-specific positional encoding.\n- Run all of these embeddings through a Transformer with shared weights for all three modalities\n- Take the final projected CLS representation for each the video patches, and perform contrastive learning against both an aligned audio patch, and an aligned text region. This contrastive loss is calculated by, for each pair, projecting into a shared space (video and audio each project into a shared audio-video space, video and text each project into a shared video-text space, with specific projection weights), and then doing a normal contrastive setup where positive pairs come either from a direct alignment of audio and video, or from a soft \"nearest neighbors\" alignment of text with video, to account for not all video snippets containing text\n\nOne technique that was fun in its simplicity was the author's DropToken strategy, which basically just said \"hey, we have a high-resolution input, what if we just randomly dropped tokens within our sequence to reduce the S^2 sequence length cost. This obviously leads to some performance cost, but they found it not very dramatic. \n\nExperimental results were all-around impressive, achieving SOTA on a number of modality-specific tasks (action prediction in video, audio prediction) with their cross-modality model.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2104.11178"
    },
    "190": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1801.04381",
        "transcript": "This work expands on prior techniques for designing models that can both be stored using fewer parameters, and also execute using fewer operations and less memory, both of which are key desiderata for having trained machine learning models be usable on phones and other personal devices. \n\nThe main contribution of the original MobileNets paper was to introduce the idea of using \"factored\" decompositions of Depthwise and Pointwise convolutions, which separate the procedures of \"pull information from a spatial range\" and \"mix information across channels\" into two distinct steps. In this paper, they continue to use this basic Depthwise infrastructure, but also add a new design element: the inverted-residual linear bottleneck. \n\nThe reasoning behind this new layer type comes from the observation that, often, the set of relevant points in a high-dimensional space (such as the 'per-pixel' activations inside a conv net) actually lives on a lower-dimensional manifold. So, theoretically, and naively, one could just try to use lower dimensional internal representations to map the dimensionality of that assumed manifold. However, the authors argue that ReLU non-linearities kill information (because of the region where all inputs are mapped to zero), and so having layers contain only the number of dimensions needed for the manifold would mean that you end up with too-few dimensions after the ReLU information loss. However, you need to have non-linearities somewhere in the network in order to be able to learn complex, non-linear functions. \n\nSo, the authors suggest a method to mostly use smaller-dimensional representations internally, but still maintain ReLus and the network's needed complexity. \n\nhttps://i.imgur.com/pN4d9Wi.png\n\n- A lower-dimensional output is \"projected up\" into a higher dimensional output\n- A ReLu is applied on this higher-dimensional layer\n- That layer is then projected down into a smaller-dimensional layer, which uses a linear activation to avoid information loss\n- A residual connection between the lower-dimensional output at the beginning and end of the expansion\n\nThis way, we still maintain the network's non-linearity, but also replace some of the network's higher-dimensional layers with lower-dimensional linear ones",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1801.04381"
    },
    "191": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=DBLP:journals/corr/abs-2003-10555",
        "transcript": "I'm a little embarrassed that I'm only just now reading what seems like a fairly important paper from a year and a half ago, but, in my defense, March 2020 was not the best time for keeping up with the literature in a disciplined way. \n\nAnyhow, musings aside: this paper proposes an alternative training procedure for large language models, which the authors claim result in models that reach strong performance more efficiently than previous BERT, XLNet, or RoBERTa baselines. As some background context, the previously-canonical Masked Learning Model (MLM) task works by: \n\n- Replacing some percentage of tokens with a [MASK] indicator\n- Using the final-layer representation at the locations of those [MASK]s to predict the true input token\n- Using as a training signal the Maximum Likelihood of that prediction, or, how high the model's predicted probability on the true input.\n\nELECTRA authors argue that there are a few notable disadvantages to this structure, if your goal is to train useful representations for downstream tasks. Firstly, your loss only consists of information (i.e. the true token) from the tokens you randomly masked, so a good amount of the data is going in some sense unused (except as context). Secondly, learning a full generative model of language requires a lot of data and training time, and it may not be all that beneficial for performance on your downstream tasks of interest. \n\nAs an alternative, they propose: \n\n- Co-learning a (small) generator, trained in typical MLM fashion, alongside a discriminator. Randomly select tokens from the input to replace with fake tokens drawn from the distribution of the discriminator\n- The goal of the discriminator is to distinguish the true tokens from the fake ones. (minor note: if the generator happens to get lucky and generate the real token, that's counted as a \"real\" rather than \"fake\" token, even though it was generated by a generator). This uses more of the training data in the loss, since you can ask \"real or fake\" for every token in the input data, not (obviously) just the ones that are actually fake\n- An important note for those familiar with GANs is that the generator isn't trained to confuse the discriminator (as is GAN-standard), but is simply trained with it's own maximum likelihood loss, independent of the discriminator's performance.\n\nThey argue, and show fairly convincingly, that ELECTRA is able to reach a higher efficiency-to-performance trade-off curve compared to BERT - matching the performance of previous models with notably less training, and outperforming them with comparable amounts of training. \n\nThey go on to perform a few ablations, some of which felt more convincing than others. The most confusing ablation, which I'm not sure if I just misunderstood, was meant to ask how much of the value of ELECTRA came from calculating its loss over all the tokens in the training data, rather than just the masked ones. So, they tried just calculating the loss for the masked/replaced tokens. The resulting discriminator performs very poorly downstream. But, I find this a little odd as a design choice, since couldn't the discriminator learn to almost always predict that a replaced token was fake, since the only way it could be otherwise would be if the generator got lucky and produced the true word? They also did the (more sensible, to me) experiment of calculating the loss on a similarly-sized percentage of tokens, but not fully overlapping with the replacement mask, and that did more similarly to base ELECTRA. \n\nThey also tested training a combined MLM/ELECTRA loss, where generated tokens were used in lieu of masking, and the full-sized MLM generator predicts the true token at every point in the sequence (which could be the token it gets as input, or could not be, in the case of a replacement). That model performed more closely to ELECTRA than BERT, which suggests that the efficiency gain of calculating a loss on every element in the training set was more important in practice than the gain from focusing a discriminator more directly on what was valuable for downstream tasks, rather than generating.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://dblp.org"
    },
    "192": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2103.03206",
        "transcript": "This new architecture out of Deepmind applies combines information extraction and bottlenecks to a traditional Transformer base to get a model that can theoretically apply self-attention to meaningfully larger input sizes than earlier architectures allowed. \n\nCurrently, self-attention models are quite powerful and capable, but because attention is quadratic-in-sequence-length in both time, and, often more saliently, memory, it's infeasible to use on long sequences without some modification. This paper propose what they call \"cross-attention,\" where some smaller-dimensional latent vector attends to the input (the latent generates the queries, the input the keys and values). This lets the network pull information out of the larger-dimensional input into a smaller and fixed-by-hyperparameter, size of latent. From there, multiple self-attention layers are applied to generate a new latent, which can be fed back into the beginning of the process to query new information from the input, accounting for the \"iterative\" in the title of this work. \n\nThe authors argue this approach lets them take larger inputs, and create deeper models, because the cost of each self-attention layer (going from latent-dim to latent-dim) is small and controlled. Like many other Transformer-based architectures, they use positional encodings, theirs based on Fourier features at different frequencies. \n\nhttps://i.imgur.com/Wc8rzII.png\n\nMy overall take from the results presented is that it is competitive on many of the audio and vision tasks tested, with none of the convolutional priors that even something like Vision Transformer (which does course convolution-style preprocessing before going into Transformer layers) require, though it didn't dramatically outperform the state-of-the-art on any of the tested tasks. One thing that was strange to me was that they didn't (at least in the main paper, haven't read the appendix) seem to evaluate on text, which would seem like an obvious benchmark if you're proposing a Transformer-alternate architecture.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2103.03206"
    },
    "193": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2006-03236",
        "transcript": "This was an amusingly-timed paper for me to read, because just yesterday I was listening to a different paper summary where the presenter offhandedly mentioned the idea of compressing the sequence length in Transformers through subsequent layers (the way a ConvNet does pooling to a smaller spatial dimension in the course of learning), and it made me wonder why I hadn't heard much about that as an approach. And, lo, I came on this paper in my list the next day, which does exactly that. \n\nAs a refresher, Transformers work by starting out with one embedding per token in the first layer, and, on each subsequent layer, they create new representations for each token by calculating an attention mechanism over all tokens in the prior layer. This means you have one representation per token for the full sequence length, and for the full depth of the network. In addition, you typically have a CLS token that isn't connected to any particular word, but is the designated place where sequence-level representations aggregate and are used for downstream tasks. \n\nThis paper notices that many applications of trained transformers care primarily about that aggregated representation, rather than precise per-word representations. For cases where that's true, you're spending a lot of computation power on continually calculating the SeqLength^2 attention maps in later layers, when they might not be bringing you that much value in your downstream transfer tasks. \n\nA central reason why you do generally need per-token representations in training Transformers, though, even if your downstream tasks need them less, is that the canonical Masked Language Model and newer ELECTRA loss functions require token-level predictions for the specific tokens being masked. To accommodate this need, the authors of this paper structure their \"Funnel\" Transformer as more of an hourglass. It turns it into basically a VAE-esque Encoder/Decoder structure, where attention downsampling layers reduce the length of the internal representation down, and then a \"decoder\" amplifies it back to the full sequence size, so you have one representation per token for training purposes (more on the exact way this works in a bit). The nifty thing here is that, for downstream tasks, you can chop off the decoder, and be left with a network with comparatively less computation cost per layer of depth. \n\nhttps://i.imgur.com/WC0VQXi.png\n\nThe exact mechanisms of downsampling and upsampling in this paper are quite clever. \n\nTo perform downsampling at a given attention layer, you take a sequence of representations h, and downsampling it to h' of half the size by mean-pooling adjacent tokens. However, in the attention calculation, you only use h' for the queries, and use the full sequence h for the keys and values. Essentially, this means that you have an attention layer where the downsampled representations attend to and pull information from the full scope of the (non-downsampled) representations of the layer below. This means you have a much more flexible downsampling operation, since the attention mechanism can choose to pull information into the downsampled representation, rather than it being calculated automatically by a pooling operation \n\nThe paper inflates the bottleneck-ed representations back up to the full sequence length by first tiling the downsampled representation (for example, if you had downsampled from 20 to 5, you would tile the first representation 4 times, then the second representation 4 times, and so on until you hit 20). That tiled representation, which can roughly be though to represent a large region of the sequence, is then added, ResNet-style, to the full-length sequence of representations that came out of the first attention layer, essentially combining shallow token-level representations with deep region-level representations. This aggregated representation is then used for token-level loss prediction \n\nThe authors benchmark again common baseline models, using deeper models with fewer tokens per layer, and find that they can reach similar or higher levels of performance with fewer FLOPs on text aggregation tasks. They fall short of full-sequence models for tasks that require strong per-token representations, which fits with my expectation.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2006.03236"
    },
    "194": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2011.12948",
        "transcript": "This summary builds substantially on my summary of NERFs, so if you haven't yet read that, I recommend doing so first! \n\nThe idea of a NERF is learn a neural network that represents a 3D scene, and from which you can, once the model is trained, sample an image of that scene from any desired angle. This involves structuring your neural network as a function that predicts the RGB color and density/opacity for a given point in 3D space (x, y, z), from a given viewing angle (theta, phi). With such a function, you can generate predictions of what images taken from certain angles would look like by sampling along a viewing ray, and integrating the combined hue and opacity into an aggregated view. This prediction can then be compared to a true image taken from that direction, and gradients passed backwards into the prediction model. \n\nAn important assumption of this model is that the scene being photographed is static; specifically, that every point in space is always inhabited by the same part of the 3D object, regardless of what angle it's viewed from. This is a reasonable assumption for photos of inanimate objects, or of humans in highly controlled lab settings, but it is often not true for humans when you, say, ask them to take a selfie video of themselves. Even if they're trying to keep roughly still, there will be slight shifts in the location and position of their head between frames, and the authors of this paper show that this can lead to strange artifacts if you naively try to train a NERF from the images (including a particularly odd one where it hallucinates tiny copies of the image in the air surrounding the face). \n\nhttps://i.imgur.com/IUVh6uM.png\n\nThe fix proposed by this paper is to apply a learnable deformation field to each image, where the notion is to deform each view into being in one canonical position (fixed per network, since, again, one network corresponds to a single scene). This means that, along with learning the parameters of the NERF itself, you're also learning what deformation to apply to each training image to get it into this canonical position. This is done by parametrizing the deformation in a particular way, and then having that deformation be conditioned by a latent vector that's trained similar to how you'd train an embedding (one learned vector per image example). The parametrization of the deformation is honestly a little bit over my head, given my lack of grounding in 3D modeling, but my general sense is that it applies some constraints and regularization to ensure that the learned deformations are realistic, insofar as humans are mostly rigid (one patch of skin on my forehead generally doesn't move except in concordance with the rest of my forehead), but with some possibility for elasticity (skin can stretch if I, say, smile). The authors also include an annealing scheme whereby, early in training, the model focuses on learning course (large-scale) deformations, and later in training, it's allowed to also learn weights for more precise deformations. This is to hopefully match macro-scale shifts before adding the noise of precise changes. \n\nThis addition of a learned deformation is most of the contribution of this method: with it applied, they show that they're able to learn realistic NERFs from selfies, which they term \"NERFIES\". They mention a few pieces of concurrent work that try to solve the same problem of non-static human subjects in different ways, but I haven't had a chance to read those, so I can't really comment on how NERFIES stacks up to alternate approaches, but it appears to be as least one empirically convincing solution to the problem it's aiming at.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2011.12948"
    },
    "195": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=mildenhall2020representing",
        "transcript": "This summary builds extensively on my prior summary of SIRENs, so if you haven't read that summary or the underlying paper yet, I'd recommend doing that first! \n\nAt a high level, the idea of SIRENs is to use a neural network to learn a compressed, continuous representation of an image, where the neural network encodes a mapping from (x, y) to the pixel value at that location, and the image can be reconstructed (or, potentially, expanded in size) by sampling from that function across the full range of the image. To do this effectively, they use sinusoidal activation functions, which let them match not just the output of the neural network f(x, y) to the true image, but also the first and second derivatives of the neural network to the first and second derivatives of the true image, which provides a more robust training signal. \n\nNERFs builds on this idea, but instead of trying to learn a continuous representation of an image (mapping from 2D position to 3D RGB), they try to learn a continuous representation of a scene, mapping from position (specified with with three coordinates) and viewing direction (specified with two angles) to the RGB color at a given point in a 3D grid (or \"voxel\", analogous to \"pixel\"), as well as the *density* or opacity of that point. \n\nWhy is this interesting? Because if you have a NERF that has learned a good underlying function of a particular 3D scene, you can theoretically take samples of that scene from arbitrary angles, even angles not seen during training. It essentially functions as a usable 3D model of a scene, but one that, because it's stored in the weights of a neural network, and specified in a continuous function, is far smaller than actually storing all the values of all the voxels in a 3D scene (the authors give an example of 5MB vs 15GB for a NERF vs a full 3D model). To get some intuition for this, consider that if you wanted to store the curve represented by a particular third-degree polynomial function between 0 and 10,000 it would be much more space-efficient to simply store the 3 coefficients of that polynomial, and be able to sample from it at your desired granularity at will, rather than storing many empirically sampled points from along the curve. \n\nhttps://i.imgur.com/0c33YqV.png\n\nHow is a NERF model learned?\n\n- The (x, y, z) position of each point is encoded as a combination of sine-wave, Fourier-style curves of increasingly higher frequency. This is similar to the positional encoding used by transformers. In practical turns, this means a location in space will be represented as a vector calculated as [some point on a low-frequency curve, some point on a slightly higher frequency curve..., some point on the highest-frequency curve]. This doesn't contain any more *information* than the (x, y, z) representation, but it does empirically seem to help training when you separate the frequencies like this\n- You take a dataset of images for which viewing direction is known, and simulate sending a ray through the scene in that direction, hitting some line (or possibly tube?) of voxels on the way. You calculate the perceived color at that point, which is an integral of the color information and density/opacity returned by your model, for each point. Intuitively, if you have a high opacity weight early on, that part of the object blocks any voxels further in the ray, whereas if the opacity weight is lower, more of the voxels behind will contribute to the overall effective color perceived. You then compare these predicted perceived colors to the actual colors captured by the 2D image, and train on the prediction error.\n- (One note on sampling: the paper proposes a hierarchical sampling scheme to help with sampling efficiently along the ray, first taking a course sample, and then adding additional samples in regions of high predicted density)\n- At the end of training, you have a network that hopefully captures the information from *that particular scene*. A notable downside of this approach is that it's quite slow for any use cases that require training on many scenes, since each individual scene network takes about 1-2 days of GPU time to train",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2003.08934"
    },
    "196": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=sitzmann2020implicit",
        "transcript": "[First off, full credit that this summary is essentially a distilled-for-my-own-understanding compression of Yannic Kilcher's excellent video on the topic] \n\nI'm interested in learning more about Neural Radiance Fields (or NERFs), a recent technique for learning a representation of a scene that lets you generate multiple views from it, and a paper referenced as a useful prerequisite for that technique was SIRENs, or Sinuisodial Representation Networks. In my view, the most complex part of understanding this technique isn't the technique itself, but the particularities of the problem being solved, and the ways it differs from a more traditional ML setup. \n\nTypically, the goal of machine learning is to learn a model that extracts and represents properties of a data distribution, and that can generalize to new examples drawn from that distribution. Instead, in this framing, a single network is being used to capture information about a single image, essentially creating a compressed representation of that image that brings with it some nice additional properties. \n\nConcretely, the neural network is representing a function that maps inputs of the form (x, y), representing coordinates within the image, to (r, g, b) values, representing the pixel values of the image at that coordinate. If you're able to train an optimal version of such a network, it would mean you have a continuous representation of the image. A good way to think about \"continuous,\" here, is that, you could theoretically ask the model for the color value at pixel (3.5, 2.5), and, given that it's simply a numerical mapping, it could give you a prediction, even though in your discrete \"sampling\" of pixels, that pixel never appears.\n\nGiven this problem setting, the central technique proposed by SIRENs is to use sinusoidal non-linearities between the layers. On the face of it, this may seem like a pretty weird choice: non-linearities are generally monotonic, and a sine wave is absolutely not that. The appealing property of sinusoidal activations in this context is: if you take a derivative of a sine curve, what you get is a cosine curve (which is essentially a shifted sine curve), and the same is true in reverse. This means that you can take multiple derivatives of the learned function (where, again, \"learned function\" is your neural network optimized for this particular image), and have them still be networks of the same underlying format, with shifting constants. \n\nThis allows SIRENs to use an enhanced version of what would be a typical training procedure for this setting. Simplistically, the way you'd go about training this kind of representation would be to simply give the inputs, and optimize against a loss function that reduced your prediction error in predicting the output values, or, in other words, the error on the f(x, y) function itself. When you have a model structure that makes it easy to take first and second derivatives of the function calculated by the model, you can, as this paper does, decide to train against a loss function of matching, not just the true f(x, y) function (again, the pixel values at coordinates), but also the first and second-derivatives (gradients and Laplacian) of the image at those coordinates. This supervision lets you learn a better underlying representation, since it enforces not just what comes \"above the surface\" at your sampled pixels, but the dynamics of the true function between those points. \n\nOne interesting benefit of this procedure of using loss in a first or second derivative space (as pointed out in the paper), is that if you want to merge the interesting parts of multiple images, you can approximate that by training a SIREN on the sum of their gradients, since places where gradients are zero likely don't contain much contrast or interesting content (as an example: a constant color background). \n\nThe Experiments section goes into a lot of specific applications in boundary-finding problems, which I understand at less depth, and thus won't try to explain. It also briefly mentions trying to learn a prior over the space of image functions (that is, a prior over the set of network weights that define the underlying function of an image); having such a prior is interesting in that it would theoretically let you sample both the implicit image function itself (from the prior), and then also points within that function.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2006.09661"
    },
    "197": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2007-12223",
        "transcript": "This is an interesting paper, investigating (with a team that includes the original authors of the Lottery Ticket paper) whether the initializations that result from BERT pretraining have Lottery Ticket-esque properties with respect to their role as initializations for downstream transfer tasks. \n\nAs background context, the Lottery Ticket Hypothesis came out of an observation that trained networks could be pruned to remove low-magnitude weights (according to a particular iterative pruning strategy that is a bit more complex than just \"prune everything at the end of training\"), down to high levels of sparsity (5-40% of original weights, and that those pruned networks not only perform well at the end of training, but also can be \"rewound\" back to their initialization values (or, in some cases, values from early in training) and retrained in isolation, with the weights you pruned out of the trained network still set to 0, to a comparable level of accuracy. This is thought of as a \"winning ticket\" because the hypothesis Frankle and Carbin generated is that the reason we benefit from massively overparametrized neural networks is that we are essentially sampling a large number of small subnetworks within the larger ones, and that the more samples we get, the likelier it is we find a \"winning ticket\" that starts our optimization in a place conducive to further training. \n\nIn this particular work, the authors investigate a slightly odd variant of the LTH. Instead of looking at training runs that start from random initializations, they look at transfer tasks that start their learning from a massively-pretrained BERT language model. They try to find out: \n\n1) Whether you can find \"winning tickets\" as subsets of the BERT initialization for a given downstream task\n\n2) Whether those winning tickets generalize, i.e. whether a ticket/pruning mask for one downstream task can also have high performance on another. If that were the case, it would indicate that much of the value of a BERT initialization for transfer tasks could be captured by transferring only a small percentage of BERT's (many) weights, which would be beneficial for compression and mobile applications \n\nAn interesting wrinkle in the LTH literature is the question of whether true \"winning tickets\" can be found (in the sense of the network being able to retrain purely from the masked random initializations), or whether it can only retrain to a comparable accuracy by rewinding to an early stage in training, but not the absolute beginning of training. Historically, the former has been difficult and sometimes not possible to find in more complex tasks and networks. \n\nhttps://i.imgur.com/pAF08H3.png\n\nOne finding of this paper is that, when your starting point is BERT initialization, you can indeed find \"winning tickets\" in the first sense of being able to rewind the full way back to the beginning of (downstream task) training, and retrain from there. (You can see this above with the results for IMP, Iterative Magnitude Pruning, rolling back to theta-0). This is a bit of an odd finding to parse, since it's not like BERT really is a random initialization itself, but it does suggest that part of the value of BERT is that it contains subnetworks that, from the start of training, are in notional optimization basins that facilitate future training. \n\nA negative result in this paper is that, by and large, winning tickets on downstream tasks don't transfer from one to another, and, to the extent that they do transfer, it mostly seems to be according to which tasks had more training samples used in the downstream mask-finding process, rather than any qualitative properties of the task. The one exception to this was if you did further training of the original BERT objective, Masked Language Modeling, as a \"downstream task\", and took the winning ticket mask from that training, which then transferred to other tasks. This is some validation of the premise that MLM is an unusually good training task in terms of its transfer properties. \n\nAn important thing to note here is that, even though this hypothesis is intriguing, it's currently quite computationally expensive to find \"winning tickets\", requiring an iterative pruning and retraining process that takes far longer than an original training run would have. The real goal here, which this is another small step in the hopeful direction of, is being able to analytically specify subnetworks with valuable optimization properties, without having to learn them from data each time (which somewhat defeats the point, if they're only applicable for the task they're trained on, though is potentially useful is they do transfer to some other tasks, as has been shown within a set of image-prediction tasks).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2007.12223"
    },
    "198": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2006-07589",
        "transcript": "This a nice, compact paper testing a straightforward idea: can we use the contrastive loss structure so widespread in unsupervised learning as a framework for generating and training against adversarial examples? In the context of the adversarial examples literature, adversarial training - or, training against examples that were adversarially generated so as to minimize the loss of the model you're training - is the primary strategy used to train robust models (robust here in the sense of not being susceptible to said adversarial attacks). Typically, these attacks are generated with the use of class labels, since they are meant to attack supervised classifiers that assign a class label to an image. Therefore, the goal of the adversarial attack is to push down the probability of the correct class label (either in favor of a specific alternate class, or just in favor of any class that isn't the true one). \n\nHowever, labels are hard and expensive, so, one wonders: in the same way that you can learn representations from unlabeled data, can you also make those representations (otherwise referred to as \"embeddings\") robust in a similarly label-free way. This paper tests an approach that does so in a quite simple way, by just generating adversarial examples against your contrastive loss target. This works by: \n\n1) Taking an image, and generating two augmentations (or transformations) of it. This is part of the standard contrastive pipeline\n\n2) Applying an adversarial perturbation to one of those transformations, where the perturbation is optimized to maximize the contrastive loss (ability to differentiate an augmented version of the same image from augmented versions of other images) \n\n3) Training on that adversarial sample to generate more robustness \n\nhttps://i.imgur.com/ttF6k1A.png\n\nAnd this simple approach appears to work quite well! They find that, in defending against supervised adversarial attacks, it performs comparably to supervised adversarial training, and that it has the added benefits of (1) slightly higher accuracy on clean examples (in general, robustness is known to decrease your clean-sample accuracy), and (2) better robustness against attack types other than the attack type used for the adversarial training. It also achieves better transfer performance (that is, adversarially training on one dataset, and then evaluating robustness on another) than a supervised method, when evaluated on both CIFAR10 \u2192 CIFAR100 and CIFAR100 \u2192 CIFAR10. This does make pretty good sense to me, since instance-level stability does seem like it's getting at a more fundamental set of invariances that to would transfer better to different distributions of classes.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2006.07589"
    },
    "199": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2007.00224",
        "transcript": "The premise of contrastive loss is that we want to push together the representations of objects that are similar, and push dissimilar representations farther apart. However, in an unlabeled setting, we don't generally have class labels to tell which images (or objects in general) are supposed to be similar or dissimilar along the axes that matter to us, so we use the shortcut of defining some transformation on a given anchor frame that gets us a frame we're confident is related enough to that anchor that it can be considered a \"positive\" or target similarity-wise. Some of these transformations are data augmentations performed on a frame, or choosing temporally adjacent frames in a video sequence (which, since the real world evolves smoothly, are assumed to be similar). \n\nAnyhow, all of this is well and good, except for the fact that, especially in an image classification setting like CIFAR or ImageNet, sampling randomly from the other images in a given batch doesn't give you a set of things that are entirely \"negatives\" in terms of being dissimilar to the anchor image. It is true that most of the objects you get by sampling randomly are negatives (especially in a many-class setting), but some of them will be other samples from the same class. By treating all of those as negatives, we penalize the model for having representations of them that are chose to our anchor representation, even though, for many downstream tasks, we'd probably prefer elements of the same class to have more similar representations. However, the whole premise of the unsupervised setting is that we don't have class labels, so we don't know, for a given sample from the batch (of things that aren't specifically transformations of the anchor) whether it's an actual negative or secretly a positive (i.e. of the same class). And, that's true, but this paper argues that, even if you can't identify which specific elements in a batch are secret positives, you can try to account for them in aggregate, if you have some reasonably good estimate of the overall class probabilities, which will tell you how many positives you expect to find in a given batch in expectation. \n\nGiven that, they reformulate the loss to be \"debiased\". They do this by taking the expectation over negatives in the denominator, which is actually a sample over the full p(x), not just the distribution over negatives, and trying to make it a better estimate of the actual distribution over negatives. \n\nhttps://i.imgur.com/URN4RBF.png\n\nThis they accomplish by writing out the full p(x) as a weighted combination of the distributions over positive and negative (which here is \"every class that doesn't match the anchor\"), as shown above, and noticing that you can represent the negative part of the distribution by taking the full distribution, and subtracting out the positive distribution (which we have an estimator for by construction, with our transformations), weighted by the prior over how frequent the positives are in our overall distribution. \n\nhttps://i.imgur.com/5IgGIhu.png\n\nThis leads to a change of estimating the similarity between the anchor and positives (which we already have in the numerator, but which we can also calculate with more augmentations/positive samples to get a better estimate) and doing a (weighted) subtraction of that from the similarity over negative examples. Intuitively, we keep in the part where we penalize similarity with negatives (by adding magnitude to the denominator), but reduce that penalty in accordance with how much we think that \"similarity with negatives\" is actually similarity with other positives in the batch, which we actually would like to keep around. \n\nhttps://i.imgur.com/kUGoemA.png\n\nhttps://i.imgur.com/5Gitdi7.png\n\nIn terms of experimental results, my read is that this is most useful on problems - like CIFAR10 and STL10 - that don't have many classes (they each, per their names, have 10). The results there are meaningfully stronger than for the 200-class ImageNet. And, that makes pretty good intuitive sense, since you would expect the scale of the \"secret positives in our random sample of images\" bias problem to be a lot more acute in a setting where we've got a 1 in 10 chance of sampling a same-class image, compared to a 1-in-200 chance.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2007.00224"
    },
    "200": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2007-02835",
        "transcript": "Large-scale transformers on unsupervised text data have been wildly successful in recent years; arguably, the most successful single idea in the last ~3 years of machine learning. Given that, it's understandable that different domains within ML want to take their shot at seeing whether the same formula will work for them as well. This paper applies the principles of (1) transformers and (2) large-scale unlabeled data to the problem of learning informative embeddings of molecular graphs. \n\nLabeling is a problem in much of machine learning - it's costly, and narrowly defined in terms of a certain task - but that problem is even more exacerbated when it comes to labeling properties of molecules, since they typically require wetlab chemistry to empirically measure. Given that, and also given the fact that we often want to predict new properties - like effectiveness against a new targetable drug receptor - that we don't yet have data for, finding a way to learn and transfer from unsupervised data has the potential to be quite valuable in the molecular learning sphere. \n\nThere are two main conceptual parts to this paper and its method - named GROVER, in true-to-ML-form tortured acronym style. The first is the actual architecture of their model itself, which combines both a message-passing Graph Neural Network to aggregate local information, and a Transformer to aggregate global information. The paper was a bit vague here, but the way I understand it is: \n\nhttps://i.imgur.com/JY4vRdd.png\n- There are parallel GNN + Transformer stacks for both edges and nodes, each of which outputs both a node and edge embedding, for four embeddings total. I'll describe the one for nodes, and the parallel for edges operates the same way, except that hidden states live on edges rather than nodes, and attention is conducted over edges rather than nodes\n- In the NodeTransformer version, a message passing NN (of I'm not sure how many layers) performs neighborhood aggregation (aggregating the hidden states of neighboring nodes and edges, then weight-transforming them, then aggregating again) until each node has a representation that has \"absorbed\" in information from a few hops out of its surrounding neighborhood. My understanding is that there is a separate MPNN for queries, keys, and values, and so each nodes end up with three different vectors for these three things.\n- Multi-headed attention is then performed over these node representations, in the normal way, where all keys and queries are dot-product-ed together, and put into a softmax to calculate a weighted average over the values\n- We now have node-level representations that combine both local and global information. These node representations are then aggregated into both node and edge representations, and each is put into a MLP layer and Layer Norm before finally outputting a node-based node and edge representation. This is then joined by an edge-based node and edge representation from the parallel stack. These are aggregated on a full-graph level to predict graph-level properties\n\nhttps://i.imgur.com/NNl6v4Y.png\n\nThe other component of the GROVER model is the way this architecture is actually trained - without explicit supervised labels. The authors use two tasks - one local, and one global. The local task constructs labels based on local contextual properties of a given atom - for example, the atom here has one double-bonded Nitrogen and one single-bonded Oxygen in its local environment - and tries to predict those labels given the representations of that atom (or node). The global task uses RDKit (an analytically constructed molecular analysis kit) to identify 85 different modifs or functional groups in the molecule, and encodes those into an 85-long one-hot vector that is being predicted on a graph level. \n\nhttps://i.imgur.com/jzbYchA.png\n\nWith these two components, GROVER is pretrained on 10 million unlabeled molecules, and then evaluated in transfer settings where its representations are fine-tuned on small amounts of labeled data. The results are pretty impressive - it achieves new SOTA performance by relatively large amounts on all tasks, even relative to exist semi-supervised pretraining methods that similarly have access to more data. The authors perform ablations to show that it's important to do the graph-aggregation step before a transformer (the alternative being just doing a transformer on raw node and edge features), and also show that their architecture without pretraining (just used directly in downstream tasks) also performs worse. One thing I wish they'd directly ablated was the value-add of the local (also referred to as \"contextual\") and global semi-supervised tasks. Naively, I'd guess that most of the performance gain came from the global task, but it's hard to know without them having done the test directly.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2007.02835"
    },
    "201": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2004-02860",
        "transcript": "I tried my best, but I'm really confused by the central methodology of this paper. Here are the things I do understand: \n\n1. The goal of the method is to learn disentangled representations, and, specifically, to learn representations that correspond to factors of variation in the environment that are selected by humans. That means, we ask humans whether a given image is higher or lower on a particular relevant axis, and aggregate those rankings into a vector, where a particular index of the vector corresponds to a particular factor. Given a small amount of supervision, the hope is to learn an encoder that takes in an image, and produces a Z code that encodes where the image is on that particular axis \n2. With those disentangled representations, the authors hope they can learn goal-conditioned policies, where the distance between the current image's representation and the goal image's representation can serve as a reward. In particular, they're trying to show that their weakly supervised disentangled representation performs better as a metric space to do goal-conditioning distance calculations in, relative to other learned spaces \n3. The approach uses a GAN-based design, where a generator generates the images that correspond with a given z1 and z2, and the discriminator tries to tell the difference between the two real images, paired with their supervision vector, and two generated images, with their fake supervision vector \n\n[Here is the relevant equation, along with some notation-explaining text] \n\nhttps://i.imgur.com/XNbxK6i.png\nThe thing I'm confused by is the actual mechanism for why (3) gets you disentangled representations. To my understanding, the thing the generator should be trying to do is generate images whose relationship to one another is governed by the relationship between z1 and z2; if z is really capturing your factors of variation, the two images should differ in places and in ways governed by where those z values are different. Based on this, I'd expect the fake supervision vector here to be some kind of binarized element-wise difference between the two (randomly sampled) vectors, z1 and z2.  But the authors claim that the fake supervision vector that the generator is trying to replicate is just the zero vector. That seems like it would just result in the generator trying to generate images that don't differ on any axes, with two different z vectors as input.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2004.02860"
    },
    "202": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=yang2020rethinking",
        "transcript": "This is a really cool paper that posits a relatively simple explanation for the strange phenomena known as double descent - both the fact of seeing it in the first place, and the difficulty in robustly causing it to appear. In the classical wisdom of statistics, increasing model complexity too far will lead to increase in variance, and thus an increase in test error (or \"test risk\" or \"empirical risk\"), leading to a U-shaped test error curve as a function of model complexity. Double descent is the name given to the observation that, in modern neural networks, we tend to not see this phenomenon, and, in fact, sometimes see test error first increasing but then descend again below its initial minimum. Test error going up, and then back down again: double descent. However, this phenomenon proved to be a bit elusive: often in order to see it, you had to  add artificial noise to your labels. \n\nThis paper provides a cohesive theory for both the existence of double descent, and the fact that it sometimes can only be elicited with increased label noise. They empirically estimate the bias and variance components of test error for a range of neural nets on a range of datasets, and show that when they directly estimate bias and variance this way, they see bias decreasing (or, at least, non-increasing) monotonically with model complexity, as expected. But, they also see variance, rather than strictly increasing with model complexity, exhibiting unimodal behavior, where it first increases, and then decreases, as a function of model complexity. \n\nTaking a step back, bias is here understood as the component of your test error that comes from the difference between your expected learned estimator and the true underlying function. Variance is the squared difference between the expected learned estimator (that is, the one you get if you average over different splits in the data), and the estimator learned on each split of the data. The actual estimator you get is a function of both your average estimator, and the particular estimator you draw in the distribution around that average, which is defined by the variance. The authors empirically measure bias and variance by conducting k different N-way splits of their datasets, and averaging these k*N estimates to get an average or expected estimator. Given that, they can (as shown below), take the variance to be the squared difference between the k*N individual estimators and the average. Since test error is composed of bias + variance, we can then simply calculate bias as whatever remains of test error when variance has been accounted for. \n\nhttps://i.imgur.com/VPzujaZ.png\n\n\nThis provides an elegant explanation for the different relationships we see between complexity and test error. In regimes where the decrease in bias from additional complexity is much larger than the increase in variance - which they argue is the case in modern deep networks - we don't see double descent, because the \"bump\" due to the variance peak is overshadowed by the continuing decrease in variance. However, in regimes where the overall scale of variance (at all levels of complexity) is higher, we see the increasing variance overwhelming the decreasing bias, and test error increases (before, ultimately, going down again, after the variance peaks). This explains why double descent has previously appeared preferentially in cases of injected label noise: more label noise means higher irreducible variability in the model learned from different sets of data, which makes the scale of the variance peak more pronounced compared to the bias drop. In addition to their empirical work, the authors also analytically analyze a two-layer linear neural network, and show that you would theoretically expect a peaked variance shape in that setting.\n\nIn a certain sense, this just pushes the problem down the road, since the paper doesn't explain why, in any kind of conceptual or statistical sense, we would expect variance to be unimodal in this way. (They do offer a conjecture, but it was not the main thrust of the paper, and I didn't fully follow it). However, it does offer conceptual clarity into a previously somewhat more murky empirical phenomenon, and hopefully will let us focus on understanding why variance behaves in this way.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2002.11328"
    },
    "203": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2006.15134",
        "transcript": "Offline reinforcement learning is potentially high-value thing for the machine learning community learn to do well, because there are many applications where it'd be useful to generate a learnt policy for responding to a dynamic environment, but where it'd be too unsafe or expensive to learn in an on-policy or online way, where we continually evaluate our actions in the environment to test their value. In such settings, we'd like to be able to take a batch of existing data - collected from a human demonstrator, or from some other algorithm - and be able to learn a policy from those pre-collected transitions, without being able to query the environment further by taking arbitrary actions. \n\nThere are two broad strategies for learning a policy from precollected transitions. One is to simply learn to mimic the action policy used by the demonstrator, predicting the action the demonstrator would take in a given state, without making use of reward data at all. This is Behavioral Cloning, and has the advantage of being somewhat more conservative (in terms of not experimenting with possibly-unsafe-or-low-reward actions the demonstrator never took), but this is also a disadvantage, because it's not possible to get higher reward than the demonstrator themselves got if you're simply copying their behavior. Another approach is to learn a Q function - estimating the value of a given action in a given state - using the reward data from the precollected transitions. This can also have some downsides, mostly in the direction of overconfidence. Q value Temporal Difference learning works by using the current reward added to the max Q value over possible next actions as the target for the current-state Q estimate. This tends to lead to overestimates, because regression to the mean effects mean that the highest value Q estimates are disproportionately likely to be noisy (possibly because they correspond to an action with little data in the demonstrator dataset). In on-policy Q learning, this is less problematic, because the agent can take the action associated with their noisily inaccurate estimate, and as a result get more data for that action, and get an estimate that is less noisy in future. But when we're in a fully offline setting, all our learning is completed before we actually start taking actions with our policy, so taking high-uncertainty actions isn't a valuable source of new information, but just risky. \n\nThe approach suggested by this DeepMind paper - Critic Regularized Regression, or CRR - is essentially a synthesis of these two possible approaches. The method learns a Q function as normal, using temporal difference methods. The distinction in this method comes from how to get a policy, given a learned Q function. Rather than simply taking the action your Q estimate says is highest-value at a particular point, CRR optimizes a policy according to the formula shown below. The f() function is a stand-in for various potential functions, all of which are monotonic with respect to the Q function, meaning they increase when the Q function does. \n\nhttps://i.imgur.com/jGmhYdd.png\n\nThis basically amounts to a form of a behavioral cloning loss (with the part that maximizes the probability under your policy of the actions sampled from the demonstrator dataset), but weighted or, as the paper terms it, filtered, by the learned Q function. The higher the estimated q value for a transition, the more weight is placed on that transition from the demo dataset having high probability under your policy. Rather than trying to mimic all of the actions of the demonstrator, the policy preferentially tries to mimic the demonstrator actions that it estimates were particularly high-quality. Different f() functions lead to different kinds of filtration. The `binary`version is an indicator function for the Advantage of an action (the Q value for that action at that state minus some reference value for the state, describing how much better the action is than other alternatives at that state) being greater than zero. Another, `exp`, uses exponential weightings which do a more \"soft\" upweighting or downweighting of transitions based on advantage, rather than the sharp binary of whether an actions advantage is above 1. \n\nThe authors demonstrate that, on multiple environments from three different environment suites, CRR outperforms other off-policy baselines - either more pure behavioral cloning, or more pure RL - and in many cases does so quite dramatically. They find that the sharper binary weighting scheme does better on simpler tasks, since the trade-off of fewer but higher-quality samples to learn from works there. However, on more complex tasks, the policy benefits from the exp weighting, which still uses and learns from more samples (albeit at lower weights), which introduces some potential mimicking of lower-quality transitions, but at the trade of a larger effective dataset size to learn from.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2006.15134"
    },
    "204": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2006-06936",
        "transcript": "This paper is ultimately relatively straightforward, for all that it's embedded in the somewhat new-to-me literature around graph-based Neural Architecture Search - the problem of iterating through options to find a graph representing an optimized architecture. The authors want to understand whether in this problem, as in many others in deep learning, we can benefit from building our supervised models off of representations learned during an unsupervised pretraining step. In this case, the unsupervised pretraining is conceptually simple - a variational autoencoder - even though the components of the VAE are more complex by dint of being made up of graph networks. This autoencoder, termed arch2vec, is trained on a simple reconstruction loss, and uses the Graph Isomorphism Network (or, GIN) architecture in its encoder, rather than a more typical Graph Convolutional Network. I don't feel like I fully follow the intuitive difference between these two structures, but have the general sense that GIN architectures are simpler; calculating a weighted sum of current central node features with the features of neighboring nodes, rather than learning a function of the full concatenated (current_node, sum_of_neighbors) vector. \n\nFirst, the authors investigate the quality of their embedding space, compared to the representation implicitly learned by doing end-to-end supervised (i.e. with accuracies as labels) NAS. They show that (1) distances in their continuous embedding space correlate more strongly with the edit distance between graphs, compared to the embedding learned by the supervised model, and that (2) their embedding fills more of the space (as would be expected from the KL regularization term) and leads to high-performing networks being naturally concentrated within the space. \n\nhttps://i.imgur.com/SavZnce.png\n\nLooking into their representation as an initialization point, they demonstrate that their initializations do lead to lower long-term regret over the course of the architecture search process, particularly differentiating themselves from random initializations at the end of training. \n\nhttps://i.imgur.com/4DG7lZd.png\nThe authors argue that this is because the representations learned by the supervised methods are \"biased towards weight-free operations, which are often preferred in the early stage of the search process, resulting in lower final accuracies.\" I admit I don't fully understand why this would be true, though they do cite a few papers they say demonstrate it. My initial thought was that weight-free architectures would overperform early in the training of each individual network, but my understanding was that the dataset used here is a labeled static dataset of architectures and accuracies, so the within-training-run dynamics wouldn't obviously play a role. Nevertheless, there does seem to be empirical benefit that comes from using these pretrained representations, even if I don't follow the intuition behind it fully.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2006.06936"
    },
    "205": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2006.12433",
        "transcript": "This is a nice little empirical paper that does some investigation into which features get learned during the course of neural network training. To look at this, it uses a notion of \"decodability\", defined as the accuracy to which you can train a linear model to predict a given conceptual feature on top of the activations/learned features at a particular layer. This idea captures the amount of information about a conceptual feature that can be extracted from a given set of activations. \n\nThey work with two synthetic datasets. \n\n1. Trifeature: Generated images with a color, shape, and texture, which can be engineered to be either entirely uncorrelated or correlated with each other to varying degrees. \n2. Navon: Generated images that are letters on the level of shape, and are also composed of letters on the level of texture \n\nThe first thing the authors investigate is: to what extent are the different properties of these images decodable from their representations, and how does that change during training? In general, decodability is highest in lower layers, and lowest in higher layers, which makes sense from the perspective of the Information Processing Inequality, since all the information is present in the pixels, and can only be lost in the course of training, not gained. They find that decodability of color is high, even in the later layers untrained networks, and that the decodability of texture and shape, while much less high, is still above chance. When the network is trained to predict one of the three features attached to an image, you see the decodability of that feature go up (as expected), but you also see the decodability of the other features go down, suggesting that training doesn't just involve amplifying predictive features, but also suppressing unpredictive ones. This effect is strongest in the Trifeature case when training for shape or color; when training for texture, the dampening effect on color is strong, but on shape is less pronounced. \n\nhttps://i.imgur.com/o45KHOM.png\n\nThe authors also performed some experiments on cases where features are engineered to be correlated to various degrees, to see which of the predictive features the network will represent more strongly. In the case where two features are perfectly correlated (and thus both perfectly predict the label), the network will focus decoding power on whichever feature had highest decodability in the untrained network, and, interestingly, will reduce decodability of the other feature (not just have it be lower than the chosen feature, but decrease it in the course of training), even though it is equally as predictive. \nhttps://i.imgur.com/NFx0h8b.png\n\nSimilarly, the network will choose the \"easy\" feature (the one more easily decodable at the beginning of training) even if there's another feature that is slightly *more* predictive available. This seems quite consistent with the results of another recent paper, Shah et al, on the Pitfalls of Simplicity Bias in neural networks. The overall message of both of these experiments is that networks generally 'put all their eggs in one basket,' so to speak, rather than splitting representational power across multiple features. \n\nThere were a few other experiments in the paper, and I'd recommend reading it in full - it's quite well written - but I think those convey most of the key insights for me.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2006.12433"
    },
    "206": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=ren2020unlabeled",
        "transcript": "This paper argues that, in semi-supervised learning, it's suboptimal to use the same weight for all examples (as happens implicitly, when the unsupervised component of the loss for each example is just added together directly. Instead, it tries to learn weights for each specific data example, through a meta-learning-esque process. \n\nThe form of semi-supervised learning being discussed here is label-based consistency loss, where a labeled image is augmented and run through the current version of the model, and the model is optimized to try to induce the same loss for the augmented image as the unaugmented one. The premise of the authors argument for learning per-example weights is that, ideally, you would enforce consistency loss less on examples where a model was unconfident in its label prediction for an unlabeled example. \n\nAs a way to solve this, the authors suggest learning a vector of parameters - one for each example in the dataset - where element i in the vector is a weight for element i of the dataset, in the summed-up unsupervised loss. They do this via a two-step process, where first they optimize the parameters of the network given the example weights, and then the optimize the example weights themselves. To optimize example weights, they calculate a gradient of those weights on the post-training validation loss, which requires backpropogating through the optimization process (to determine how different weights might have produced a different gradient, which might in turn have produced better validation loss). This requires calculating the inverse Hessian (second derivative matrix of the loss), which is, generally speaking, a quite costly operation for huge-parameter nets. To lessen this cost, they pretend that only the final layer of weights in the network are being optimized, and so only calculate the Hessian with respect to those weights. They also try to minimize cost by only updating the example weights for the examples that were used during the previous update step, since, presumably those were the only ones we have enough information to upweight or downweight. With this model, the authors achieve modest improvements - performance comparable to or within-error-bounds better than the current state of the art, FixMatch. \n\nOverall, I find this paper a little baffling. It's just a crazy amount of effort to throw into something that is a minor improvement. A few issues I have with the approach: \n\n- They don't seem to have benchmarked against the simpler baseline of some inverse of using Dropout-estimated uncertainty as the weight on examples, which would, presumably, more directly capture the property of \"is my model unsure of its prediction on this unlabeled example\"\n- If the presumed need for this is the lack of certainty of the model, that's a non-stationary problem that's going to change throughout the course of training, and so I'd worry that you're basically taking steps in the direction of a moving target\n- Despite using techniques rooted in meta-learning, it doesn't seem like this models learns anything generalizable - it's learning index-based weights on specific examples, which doesn't give it anything useful it can do with some new data point it finds that it wasn't specifically trained on\n\nGiven that, I think I'd need to see a much stronger case for dramatic performance benefits for something like this to seem like it was worth the increase in complexity (not to mention computation, even with the optimized Hessian scheme)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2007.01293"
    },
    "207": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2007-14062",
        "transcript": "Transformers - powered by self-attention mechanisms - have been a paradigm shift in NLP, and are now the standard choice for training large language models. However, while transformers do have many benefits in terms of computational constraints - most saliently, that attention between tokens can be computed in parallel, rather than needing to be evaluated sequentially like in a RNN - a major downside is their memory (and, secondarily, computational) requirements. The baseline form of self-attention works by having every token attend to every other token, where \"attend\" here means that a query from each token A will take an inner product with each other token -A, and then be elementwise-multiplied with the values of every other token -A. This implies a O(N^2) memory and computation requirement, where N is your sequence length. \n\nSo, the question this paper asks is: how do you get the benefits, or most of the benefits, of a full-attention network, while reducing the number of other tokens each token attends to. \n\nThe authors' solution - Big Bird - has three components. First, they approach the problem of approximating the global graph as a graph theory problem, where each token attending to every other is \"fully connected,\" and the goal is to try to sparsify the graph in a way that keeps shortest path between any two nodes low. They use the fact that in an Erdos-Renyi graph - where very edge is simply chosen to be on or off with some fixed probability - the shortest path is known to be logN. In the context of aggregating information about a sequence, a short path between nodes means that the number of iterations, or layers, that it will take for information about any given node A to be part of the \"receptive field\" (so to speak) of node B, will be correspondingly short. Based on this, they propose having the foundation of their sparsified attention mechanism be simply a random graph, where each node attends to each other with probability k/N, where k is a tunable hyperparameter representing how many nodes each other node attends to on average. \n\nTo supplement, the authors further note that sequence tasks of interest - particularly language - are very local in their information structure, and, while it's important to understand the global context of the full sequence, tokens close to a given token are most likely to be useful in constructing a representation of it. Given this, they propose supplementing their random-graph attention with a block diagonal attention, where each token attends to w/2 tokens prior to and subsequent to itself. (Where, again, w is a tunable hyperparameter) \n\nHowever, the authors find that these components aren't enough, and so they add a final component: having some small set of tokens that attend to all tokens, and are attended to by all tokens. This allows them to theoretically prove that Big Bird can approximate full sequences, and is a universal Turing machine, both of which are true for full Transformers. I didn't follow the details of the proof, but, intuitively, my reading of this is that having a small number of these global tokens basically acts as a shortcut way for information to get between tokens in the sequence - if information is globally valuable, it can be \"written\" to one of these global aggregator nodes, and then all tokens will be able to \"read\" it from there. \n\n\n\nThe authors do note that while their sparse model approximates the full transformer well in many settings, there are some problems - like needing to find the token in the sequence that a given token is farthest from in vector space - that a full attention mechanism could solve easily (since it directly calculates all pairwise comparisons) but that a sparse attention mechanism would require many layers to calculate. \n\n\nEmpirically, Big Bird ETC (a version which adds on additional tokens for the global aggregators, rather than making existing tokens serve thhttps://i.imgur.com/ks86OgJ.pnge purpose) performs the best on a big language model training objective, has comparable performance to existing models on questionhttps://i.imgur.com/x0BdamC.png answering, and pretty dramatic performance improvements in document summarization. It makes sense for summarization to be a place where this model in particular shines, because it's explicitly designed to be able to integrate information from very large contexts (albeit in a randomly sampled way), where full-attention architectures must, for reasons of memory limitation, do some variant of a sliding window approach.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2007.14062"
    },
    "208": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2006-07710",
        "transcript": "This is an interesting paper that makes a fairly radical claim, and I haven't fully decided whether what they find is an interesting-but-rare corner case, or a more fundamental weakness in the design of neural nets. The claim is: neural nets prefer learning simple features, even if there exist complex features that are equally or more predictive, and even if that means learning a classifier with a smaller margin - where margin means \"the distance between the decision boundary and the nearest-by data\". A large-margin classifier is preferable in machine learning because the larger the margin, the larger the perturbation that would have to be made - by an adversary, or just by the random nature of the test set - to trigger misclassification. \n\nhttps://i.imgur.com/PJ6QB6h.png\n\nThis paper defines simplicity and complexity in a few ways. In their simulated datasets,  a feature is simpler when the decision boundary along that axis requires fewer piecewise linear segments to separate datapoints. (In the example above, note that having multiple alternating blocks still allows for linear separation, but with a higher piecewise linear requirement). In their datasets that concatenate MNIST and CIFAR images, the MNIST component represents the simple feature. \n\nThe authors then test which models use which features by training a model with access to all of the features - simple and complex - and then testing examples where one set of features is sampled in alignment with the label, and one set of features is sampled randomly. If the features being sampled randomly are being used by the model, perturbing them like this should decrease the test performance of the model. For the simulated datasets, a fully connected network was used; for the MNIST/CIFAR concatenation, a variety of different image classification convolutional architectures were tried. \n\nThe paper finds that neural networks will prefer to use the simpler feature to the complete exclusion of more complex features, even if the complex feature is slightly more predictive (can achieve 100 vs 95% separation). The authors go on to argue that what they call this Extreme Simplicity Bias, or Extreme SB, might actually explain some of the observed pathologies in neural nets, like relying on spurious features or being subject to adversarial perturbations. They claim that spurious features - like background color or texture - will tend to be simpler, and that their theory explains networks' reliance on them. Additionally, relying completely or predominantly on single features means that a perturbation along just that feature can substantially hurt performance, as opposed to a network using multiple features, all of which must be perturbed to hurt performance an equivalent amount. \n\nAs I mentioned earlier, I feel like I'd need more evidence before I was strongly convinced by the claims made in this paper, but they are interestingly provocative. On a broader level, I think a lot of the difficulties in articulating why we expect simpler features to perform well come from an imprecision in thinking in language around the idea - we think of complex features as inherently brittle and high-dimensional, but this paper makes me wonder how well our existing definitions of simplicity actually match those intuitions.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2006.07710"
    },
    "209": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2010-11924",
        "transcript": "Generalization is, if not the central, then at least one of the central mysteries of deep learning. We are somehow able to able to train high-capacity, overparametrized models, that empirically have the capacity to fit to random data - meaning that they have the capacity to memorize the labeled data we give them - and which yet still manage to train functions that generalize to test data. People have tried to come up with generalization bounds - that is, bounds on the expected test error of a model class - but that have all been vacuous, which here means that their upper bound is so far above the actual observed test set error that it's meaningless for the purpose of predicting which changes will enhance or detract from generalization. \n\nThis paper builds on - and somewhat critiques - an earlier paper, Jiang et al, which takes the approach of assessing generalization bounds empirically. The central approach taken by both papers is to compare the empirical test error of two networks that are identical except for one axis which is varied, and test whether the ranking of the predicted generalization errors for the two networks, resulting from a particular analytical bound, aligns with the ranking of actual, empirical test error. Said succinctly: the goal is to measure how good a generalization bound is at predicting which networks will actually generalize, across the kinds of hyperparameter changes we'd be likely to experiment with in practice. An important note here is that this kind of rank-based measurement is insensitive to whether the actual magnitude of the generalization bound is; it only cares about relative bounds for different model configurations. \n\nFor a given pair of environments (or pairs of hyperparameter settings), the experimental framework trains multiple seeds and averages the sign error across them. If the two models in the pair were close to one another in generalization error, they were downweighted in the overall average, or removed from the estimation if they were too close, to reduce noise. \n\nA difference in methodologies between the Jiang paper and this one is that this one puts a lot of emphasis on the need to rank generalization measures not just by their average performance over a suite of different hyperparameter perturbations, but also by a metric capturing how robust the measure is, for which they suggest the max error rather than average error. Their rationale is that simply looking at an average obscures cases where a measure performs poorly in a particular region of hyperparameter space, in a way that might tell us interesting things about its failure modes. \n\nFor example, beyond just being able to say that generalization bounds based on Frobenius norms performed poorly on average at predicting the effects of changes to training set size, they were able to look at the particular settings where it performed the worst, which turn out to be on small network sizes. \n\nThe plot below shows the results from all of the tested measures aggregated together. Each row represents a different axis that was being varied, and, for each measure, a number of different settings were sampled over (for the hyperparameters that were being held fixed across pairs, rather than being varied. Each distribution rectangle represents the average sign error across all of the pairs that were sampled for that measure, and that axis of variation. The measures are listed from left to right according to their average performance across all environments and all axes of variation. \n\nhttps://i.imgur.com/Tg3wdA3.png\n\nSome conclusions from this experiment were: \n\n- Generalization measures seem to not perform well on changes made to width, however, the authors note this was mostly because changes to width tended to not change the generalization performance in consistent ways, and so the difference in test error between the networks in the pair was more often within the range of noise\n- Most but not all generalization bounds correctly predict that more training data should result in better generalization\n- No bound does particularly well on predicting the generalization effects of changes in depth\n\nOverall, I found this paper delightfully well written, and a real pleasure to read. My one critique is that the authors explicitly point out that an important piece of data for comparing generalization bounds is the set of features they depend on. That is, if a generalization bound can only make predictions with access to the learned weights (in addition to the model class and data characteristics), it's a lot less practically useful, in terms of model design, than one that doesn't. I wish they had followed through on that and represented the dependencies of the different bounds on some way in their central figure, so that it was easier to compare them \"fairly,\" or accounting for the information they had access to.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2010.11924"
    },
    "210": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2006-06882",
        "transcript": " Occasionally, I come across results in machine learning that I'm glad exist, even if I don't fully understand them, precisely because they remind me how little we know about the complicated information architectures we're building, and what kinds of signal they can productively use. This is one such result. \n\nThe paper tests a method called self-training, and compares it against the more common standard of pre-training. Pre-training works by first training your model on a different dataset, in a supervised way, with the labels attached to that dataset, and then transferring the learned weights on that model model (except for the final prediction head) and using that as initialization for training on your downstream task. Self-training also uses an external dataset, but doesn't use that external data's labels. It works by \n\n1) Training a model on the labeled data from your downstream task, the one you ultimately care about final performance on \n\n2) Using that model to make label predictions (for the label set of your downstream task), for the external dataset \n\n3) Retraining a model from scratch with the combined set of human labels and predicted labels from step (2) \n\n\nhttps://i.imgur.com/HaJTuyo.png\nThis intuitively feels like cheating; something that shouldn't quite work, and yet the authors find that it equals or outperforms pretraining and self-supervised learning in the setting they examined (transferring from ImageNet as an external dataset to CoCo as a downstream task, and using data augmentations on CoCo). They particularly find this to be the case when they're using stronger data augmentations, and when they have more labeled CoCo data to train with from the pretrained starting point. They also find that self-training outperforms self-supervised (e.g. contrastive) learning in similar settings. They further demonstrate that self-training and pre-training can stack; you can get marginal value from one, even if you're already using the other. They do acknowledge that - because it requires training a model on your dataset twice, rather than reusing an existing model directly - their approach is more computationally costly than the pretrained-Imagenet alternative. \n\nThis work is, I believe, rooted in the literature on model distillation and student/teacher learning regimes, which I believe has found that you can sometimes outperform a model by training on its outputs, though I can't fully remember the setups used in those works. \n\nThe authors don't try too hard to give a rigorous theoretical account of why this approach works, which I actually appreciate. I think we need to have space in ML for people to publish what (at least to some) might be unintuitive empirical results, without necessarily feeling pressure to articulate a theory that may just be a half-baked after-the-fact justification. \n\nOne criticism or caveat I have about this paper is that I wish they'd evaluated what happened if they didn't use any augmentation. Does pre-training do better in that case? Does the training process they're using just break down? Only testing on settings with augmentations made me a little less confident in the generality of their result. Their best guess is that it demonstrates the value of task-specificity in your training. I think there's a bit of that, but also feel like this ties in with other papers I've read recently on the surprising efficacy of training with purely random labels. I think there's, in general, a lot we don't know about what ostensibly supervised networks learn in the face of noisy or even completely permuted labels.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2006.06882"
    },
    "211": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2010-02302",
        "transcript": "The thing I think is happening here: \n\nIt proposes a self-supervised learning scheme (which...seems fairly basic, but okay) to generate encodings. It then trains a Latent World Model, which takes in the current state encoding, the action, and the belief state (I think just the prior RNN state?) and predicts a next state. The intrinsic reward is the difference between this and the actual encoding of the next step. (This is dependent on a particular action and resulting next obs, it seems). I don't really know what the belief state is doing here. Is it a...  scalar rather than a RNN state? It being said to start at 0 makes it seem that way. \n\nSummary: For years, an active area of research has been the question about how to incentivize reinforcement learning agents to more effectively explore their environment. In many environments, the state space is large, and it's quite difficult to find reward just by randomly traversing it, and, in the absence of reward signal, most reinforcement learning algorithms won't learn. To remedy this, a common approach has been to attempt to define a measure of novelty, such that you can reward policies for exploring novel states. One approach for this is to tabulate counts of how often you've seen given past states, and explore in inverse proportion to those counts. However, this is complicated to scale to image-based and continuous state spaces. \n\nAnother tactic has been to use uncertainty in an agent's model of the world as an indication of that part of state space being insufficiently explored. In this way of framing the problem, exploring a part of space gives you more samples from it, and if you use those samples to train your forward predictive model - a model predicting the next state - it will increase in accuracy for that state and states like this. So, in this setting, the \"novelty reward\" for your agent comes from the prediction error; it's incentivized to explore states where its model is more incorrect. However, a problem with this, if you do simple pixel-based prediction, is that there are some inherent sources of uncertainty in an environment, that don't get reduced by you drawing more samples from those parts of space. The canonical example of this is static on a tv - it's just fundamentally noisy and unpredictable, and no amount of gathering data will reduce that fundamental noise. A lot of ways of naively incentivizing uncertainty draw you into those parts of state space again and again, even though they aren't really serving the purpose of getting you to explore interesting, uninvestigated parts of the space. \n\nThis paper argues for a similar uncertainty-based metric, but based on prediction of a particular kind of representation of the state, which they argue the pathological property described earlier, of getting stuck in regions of high inherent uncertainty. They do this by first learning a self-supervised representation that seems *kind of* like contrastive predictive coding, but slightly different. Their approach simply pushes the Euclidean distance between the representations of nearby timesteps to be smaller, without any explicit negative set to contrast again. To avoid the network learning the degenerative solution of \"always predict a constant, so everything is close to everything else\", the authors propose a \"whitening\" (or rescaling, or normalizing\" operation before the mean squared error. This involves subtracting out the mean representation, and dividing by the covariance, before doing a mean squared error. This means that, even if you've pushed your representations together in absolute space, after the whitening operation, they will be \"pulled out\" again to be in a spherical unit Gaussian, which stops the network from being able to get a benefit from falling into the collapsing solution.\n\nhttps://i.imgur.com/Psjlf4t.png\n\nGiven this pretrained encoder, the method works by:\n\n- Constructing a recurrent Latent World Model (LWM) that takes in the encoded observation, the current action, and the prior belief state of the recurrent model\n- Encoding the current observation with the pretrained encoder\n- Passing that representation into the LWM to get a predicted next representation out (prediction happens in representation space, not pixel space)\n- Using the error between the actual encoding of the next observation, and the predicted next representation, as the novelty signal\n- Training a DQN on top of the encoding\n\nSomething I'm a little confused by is whether the encoding network is exclusively trained via MSE loss, or whether it also gets gradients from the actual RL DQN task. \n\nOverall, this method makes sense, but I'm not quite sure why the proposed representation structure would be notably better than other, more canonical self-supervised losses, like CPC. They show Action-Conditioned CPC as a baseline when demonstrating the quality of the representations, but not as a slot-in replacement for the MSE representations in their overall architecture. It does seem to get strong performance on exploration-heavy tasks, but I'll admit I'm not familiar with the quality of the baselines they chose, and so don't have a great sense of whether the - admittedly quite strong! - performance shown in the table below is in fact comparing against the current state of the art.\n\nhttps://i.imgur.com/cIr2Y4w.png",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2010.02302"
    },
    "212": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1911-09071",
        "transcript": "When humans classify images, we tend to use high-level information about the shape and position of the object. However, when convolutional neural networks classify images,, they tend to use low-level, or textural, information more than high-level shape information. This paper tries to understand what factors lead to higher shape bias or texture bias. \n\nTo investigate this, the authors look at three datasets with disagreeing shape and texture labels. The first is GST, or Geirhos Style Transfer. In this dataset, style transfer is used to render the content of one class in the style of another (for example, a cat shape in the texture of an elephant). In the Navon dataset, a large-scale letter is rendered by tiling smaller letters. And, in the ImageNet-C dataset, a given class is rendered with a particular kind of distortion; here the distortion is considered to be the \"texture label\". In the rest of the paper, \"shape bias\" refers to the extent to which a model trained on normal images will predict the shape label rather than the texture label associated with a GST image. The other datasets are used in experiments where a model explicitly tries to learn either shape or texture. \n\nhttps://i.imgur.com/aw1MThL.png\n\nTo start off, the authors try to understand whether CNNs are inherently more capable of learning texture information rather than shape information. To do this, they train models on either the shape or the textural label on each of the three aforementioned datasets. On GST and Navon, shape labels can be learned faster and more efficiently than texture ones. On ImageNet-C (i.e. distorted ImageNet), it seems to be easier to learn texture than texture, but recall here that texture corresponds to the type of noise, and I imagine that the cardinality of noise types is far smaller than that of ImageNet images, so I'm not sure how informative this comparison is. Overall, this experiment suggests that CNNs are able to learn from shape alone without low-level texture as a clue, in cases where the two sources of information disagree \n\nThe paper moves on to try to understand what factors about a normal ImageNet model give it higher or lower shape bias - that is, a higher or lower likelihood of classifying a GST image according to its shape rather than texture. Predictably, data augmentations have an effect here. When data is augmented with aggressive random cropping, this increases texture bias relative to shape bias, presumably because when large chunks of an object are cropped away, its overall shape becomes a less useful feature. Center cropping is better for shape bias, probably because objects are likely to be at the center of the image, so center cropping has less of a chance of distorting them. On the other hand, more \"naturalistic\" augmentations like adding Gaussian noise or distorting colors lead to a higher shape bias in the resulting networks, up to 60% with all the modifications. However, the authors also find that pushing the shape bias up has the result of dropping final test accuracy. \n\nhttps://i.imgur.com/Lb6RMJy.png\n\nInterestingly, while the techniques that increase shape bias seem to also harm performance, the authors also find that higher-performing models tend to have higher shape bias (though with texture bias still outweighing shape) suggesting that stronger models learn how to use shape more effectively, but also that handicapping models' ability to use texture in order to incentivize them to use shape tends to hurt performance overall. \n\nOverall, my take from this paper is that texture-level data is actually statistically informative and useful for classification - even in terms of generalization - even if is too high-resolution to be useful as a visual feature for humans. CNNs don't seem inherently incapable of learning from shape, but removing their ability to rely on texture seems to lead to a notable drop in accuracy, suggesting there was real signal there that we're losing out on.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1911.09071"
    },
    "213": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2008-11687",
        "transcript": "This is an interesting - and refreshing - paper, in that, instead of trying to go all-in on a particular theoretical point, the authors instead run a battery of empirical investigations, all centered around the question of how to explain what happens to make transfer learning work. The experiments don't all line up to support a single point, but they do illustrate different interesting facets of the transfer process. \n\n- An initial experiment tries to understand how much of the performance of fine-tuned models can be explained by (higher-level, and thus larger-scale) features, and how much is driven by lower level (and thus smaller-scale) image statistics.  To start with, the authors compare the transfer performance from ImageNet onto three different datasets - clip art, sketches, and real images. As expected, transfer performance is highest with real datasets, which are the most similar to training domain. However, there still *is* positive transfer in terms of final performance across all domains, as well as benefit in optimization speed.\n- To try to further tease out the difference between the transfer benefits of high and low-level features, the authors run an experiment where blocks of pixels are shuffled around within the image on downstream tasks .  The larger the size of the blocks being shuffled, the more that large-scale features of the image are preserved. As predicted, accuracy drops dramatically when pixel block size is small, for both randomly initialized and pretrained models. In addition, the relative value added by pretraining drops, for all datasets except quickdraw (the dataset of sketches). This suggests that in most datasets, the value brought by fine-tuning was mostly concentrated in large-scale features. One interesting tangent of this experiment was the examination of optimization speed (in the form of mean training accuracy over initial epochs). Even at block sizes too small for pretraining to offer a benefit to final accuracy, it did still contribute to faster training. (See transparent bars in right-hand plot below)\nhttps://i.imgur.com/Y8sO1da.png\n\n- On a somewhat different front, the authors look into how similar pretrained + finetuned models are to one another, compared to models trained on the same dataset from random initializations. First, they look at a measure of feature similarity, and find that the features learned by two pretrained networks are more similar to each other than a pretrained network is to a randomly initalized network, and also more than two randomly initialized networks are to one another. Randomly initialized networks are closest to one another in their final-layer features, but this is still a multiple of 4 or 5 less than the similarity between the pretrained networks\n- Looking at things from the perspective of optimization, the paper measures how much performance drops when you linearly interpolate between different solutions found by both randomly initialized and pretrained networks. For randomly initialized networks, interpolation requires traversing a region where test accuracy drops to 0%. However, for pretrained networks, this isn't the case, with test accuracy staying high throughout. This suggests that pretraining gets networks into a basin of the loss landscape, and that future training stays within that basin. There were also some experiments on module criticality that I believe were in a similar vein to these, but which I didn't fully follow\n- Finally, the paper looks at the relationship between accuracy on the original pretraining task and both accuracy and optimization speed on the downstream task. They find that higher original-task accuracy moves in the same direction as higher downstream-task accuracy, though this is less true when the downstream task is less related (as with quickdraw). Perhaps more interestingly, they find that the benefits of transfer to optimization speed happen and plateau quite early in training. Clip Art and Real transfer tasks are much more similar in the optimization speed benefits they get form ImageNet training, where on the accuracy front, the real did dramatically better.\nhttps://i.imgur.com/jBCJcLc.png\n\nWhile there's a lot to dig into in these results overall, the things I think are most interesting are the reinforcing of the idea that even very random and noisy pretraining can be beneficial to optimization speed (this seems reminiscent of another paper I read from this year's NeurIPS, examining why pretraining on random labels can help downstream training), and the observation that pretraining deposits weights in a low-loss bucket, from which they can learn more efficiently (though, perhaps, if the task is too divergent from the pretraining task, this difficulty in leaving the basin becomes a disadvantage). This feels consistent with some work in the Lottery Ticket Hypothesis, which has recently suggested that, after a short duration of training, you can rewind a network to a checkpoint saved after that duration, and be successfully able to train to low loss again.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2008.11687"
    },
    "214": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2010-12050",
        "transcript": "Contrastive learning works by performing augmentations on a batch of images, and training a network to match the representations of the two augmented parts of a pair together, and push the representations of images not in a pair farther apart. Historically, these algorithms have benefitted from using stronger augmentations, which has the effect of making the two positive elements in a pair more visually distinct from one another. This paper tries to build on that success, and, beyond just using a strong augmentation, tries to learn a way to perturb images that adversarially increases contrastive loss. As with adversarial training in normal supervised setting, the thinking here is that examples which push loss up the highest are the hardest and thus most informative for the network to learn from \n\nWhile the concept of this paper made some sense, I found the notation and the explanation of mechanics a bit confusing, particularly when it came to choice to frame a contrastive loss as a cross-entropy loss, with the \"weights\" of the dot product in the the cross-entropy loss being, in fact, the projection by the learned encoder of various of the examples in the batch. \n\nhttps://i.imgur.com/iQXPeXk.png\n\nThis notion of the learned representations being \"weights\" is just odd and counter-intuitive, and the process of trying to wrap my mind around it isn't one I totally succeeded at. I think the point of using this frame is because it provides an easy analogue to the Fast Gradient Sign Method of normal supervised learning adversarial examples, even though it has the weird effect that, as the authors say \"your weights vary by batch...rather than being consistent across training,\" \n\nNotational weirdness aside, my understanding is that the method of this paper: \n\n- Runs a forward pass of normal contrastive loss (framed as cross-entropy loss) which takes augmentations p and q and runs both forward through an encoder.\n- Calculates a delta to apply to each input image in the q that will increase the loss most, taken over all the images in the p set\n    - I think the delta is per-image in q, and is just aggregated over all images in p, but I'm not fully confident of this, as a result of notational confusion. It could also be one delta applied for all all images in q.\n- Calculate the loss that results when you run forward the adversarially generated q against the normal p\n- Train a combined loss that is a weighted combination of the normal p/q contrastive part and the adversarial p/q contrastive part\n\nhttps://i.imgur.com/UWtJpVx.png\n\nThe authors show a small but relatively consistent improvement to performance using their method. Notably, this improvement is much stronger when using larger encoders (presumably because they have more capacity to learn from harder examples). One frustration I have with the empirics of the paper is that, at least in the main paper, they don't discuss the increase in training time required to calculate these perturbations, which, a priori, I would imagine to be nontrivial.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2010.12050"
    },
    "215": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2004.11362",
        "transcript": "This was a really cool-to-me paper that asked whether contrastive losses, of the kind that have found widespread success in semi-supervised domains, can add value in a supervised setting as well. In a semi-supervised context, contrastive loss works by pushing together the representations of an \"anchor\" data example with an augmented version of itself (which is taken as a positive or target, because the image is understood to not be substantively changed by being augmented), and pushing the representation of that example away from other examples in the batch, which are negatives in the sense that they are assumed to not be related to the anchor image. \n\nThis paper investigates whether this same structure - of training representations of positives to be close relative to negatives - could be expanded to the supervised setting, where \"positives\" wouldn't just mean augmented versions of a single image, but augmented versions of other images belonging to the same class. This would ideally combine the advantages of self-supervised contrastive loss - that explicitly incentivizes invariance to augmentation-based changes - with the advantages of a supervised signal, which allows the representation to learn that it should also see instances of the same class as close to one another. \n\nhttps://i.imgur.com/pzKXEkQ.png\n\nTo evaluate the performance of this as a loss function, the authors first train the representation - either with their novel supervised contrastive loss SupCon, or with a control cross-entropy loss - and then train a linear regression with cross-entropy on top of that learned representation. (Just because, structurally, a contrastive loss doesn't lead to assigning probabilities to particular classes, even if it is supervised in the sense of capturing information relevant to classification in the representation) \n\nThe authors investigate two versions of this contrastive loss, which differ, as shown below, in terms of the relative position of the sum and the log operation, and show that the L_out version dramatically outperforms (and I mean dramatically, with a top-one accuracy of 78.7 vs 67.4%). \n\nhttps://i.imgur.com/X5F1DDV.png\n\nThe authors suggest that the L_out version is superior in terms of training dynamics, and while I didn't fully follow their explanation, I believe it had to do with L_out version doing its normalization outside of the log, which meant it actually functioned as a multiplicative normalizer, as opposed to happening inside the log, where it would have just become an additive (or, really, subtractive) constant in the gradient term. Due to this stronger normalization, the authors positive the L_out loss was less noisy and more stable. \n\nOverall, the authors show that SupCon consistently (if not dramatically) outperforms cross-entropy when it comes to final accuracy. They also show that it is comparable in transfer performance to a self-supervised contrastive loss. One interesting extension to this work, which I'd enjoy seeing more explored in the future, is how the performance of this sort of loss scales with the number of different augmentations that performed of each element in the batch (this work uses two different augmentations, but there's no reason this number couldn't be higher, which would presumably give additional useful signal and robustness?)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2004.11362"
    },
    "216": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2006-10455",
        "transcript": "This is another paper that was a bit of a personal-growth test for me to try to parse, since it's definitely heavier on analytical theory than I'm used to, but I think I've been able to get something from it, even though I'll be the first to say I didn't understand it entirely. \n\nThe question of this paper is: why does it seem to be the case that training a neural network on a data distribution - but with your supervised labels randomly sampled - seems to afford some level of advantage when fine-tuning on those random-trained with correct labels.  What is it that these networks learn from random labels that gives them a head-start on future training? \n\nTo try to answer this, the authors focus on analyzing the first-layer weights of a network, and frame both the input data and the learned weights (after random training) to both be random variables, each with some mean and covariance matrix. The central argument made by the paper is: \n\nAfter training with random labels, the weights come to have a distributional form with a covariance matrix that is \"aligned\" with the covariance matrix of the data. \"Aligned\" here means alignment on the level of eigenvectors. Formally, it is defined as a situation where every eigenspace in the data covariance matrix is contained in, or is a subset of, an eigenspace of the weight matrix. Intuitively, it means that the principal components \u2014 the axes that define the principle dimensions of variation - of the weight space are being aligned, in a linear algebra sense, with those of the data, down to a difference of a scaling factor (that is, the fact that the eigenvalues may be different between the two). \n\nThey do show some empirical evidence of this being the case, by calculating the actual covariance matrices of both the data and the learned weight matrices, and showing that you see high degrees of similarity between the vector spaces of the two (though sometimes by having to add eigenspaces of the data together to be equivalent to an eigenspace of the weights). \n\nhttps://i.imgur.com/TB5JM6z.png\n\nThey also show some indication that this property drives the advantage in fine-tuning. They do this by just taking their analytical model of what they believe is happening during training - that weights are coming to be drawn from a distribution governed by a covariance matrix aligned with the data covariance matrix -  and sample weights from a normal distribution that has that property. They show, in the plot below, that this accounts for most of the advantage that has been observed in subsequent training from training on random labels (other than the previously-discovered effect of \"all training increases the scale of the weights, which helps in future training,\" which they account for by normalizing). \n\nhttps://i.imgur.com/cnT27HI.png\n\nUnfortunately, beyond this central intuition of covariance matrix alignment, I wasn't able to get much else from the paper. Some other things they mentioned, that I didn't follow were: \n\n- The actual proof for why you'd expect this property of alignment in the case of random training\n- An analysis of the way that \"the first layer [of a network] effectively learns a function which maps each eigenvalue of the data covariance matrix to the corresponding eigenvalue of the weight covariance matrix.\" I understand that their notion of alignment predicts that there should be some relationship between these two eigenvalues, but I don't fully follow which part of the first layer of a neural network will *learn* that function, or produce it as an output\n- An analysis of how this framework explains both the cases where you get positive transfer from random-label training (i.e. fine-tuned networks training better subsequently) and the cases where you get negative transfer",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2006.10455"
    },
    "217": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2007-13916",
        "transcript": "In the past year or so, contrastive learning has experienced widespread success, and has risen to be a dominant problem framing within self-supervised learning. The basic idea of contrastive learning is that, instead of needing human-generated labels to generate a supervised task, you instead assume that there exists some automated operation you can perform to a data element to generate another data element that, while different, should be considered still fundamentally the same, or at least more strongly related, and that you can contrast these related pairs against pairs constructed with the rest of the dataset, with which any given frame would not by default have this assumed relationship of sameness or quasi-similarity. \n\nOne fairly central way that \"different but still effectively similar\" has been historically defined - at least within the realm of image-based models - is through the use of data augmentations: image transformations such as cropping, color jitter, or Gaussian blur, which are used on an image to create the counterpart in its related pair. Fundamentally, what we're doing when we define these particular augmentations is saying: these transformations don't cause a meaningful change in what the image is, and so we want the representations we get with and without the transformations to be close to one another (or at least to contain enough information to predict one another). Another way you can say this is that we're defining properties of the image that we want our representation to be invariant to. \n\nThe authors of this paper make the point that, when aggressive cropping is part of your toolkit of augmentations, the crops of the image can actually contain meaningfully different content than the uncropped image. If you remove a few pixels around the edges of an image, it still fundamentally contains the same things. However, if you zoom in dramatically, you may get crops that contain different objects. From an image classification standpoint, you would expect that coding in an invariance to cropping in our representations would, in some cases, also mean coding in an invariance to object type, which would presumably be detrimental to the task of classifying objects. To explain the extent of the success that aggressive-cropping methods have had so far, they argue that ImageNet has the particular property that its images are curated to be primarily and centrally containing a single object at a time, such that, even if you zoom in, you're getting a part of the central object, rather than another object entirely. They argue that this dataset bias might explain why you haven't seen as much of this object-invariance be a problem in earlier augmentation-based contrastive work. To try to test this, they train different contrastive (MoCo v2) models on the MSCOCO dataset, which consists of pictures of rooms, and thus no longer has the property of being centrally of one object. They tried one setting where they performed contrastive loss on the images as a whole, and another where the input to the augmentation pipeline were images from the same dataset, but pre-cropped to only contain one image at a time. This was meant, as far as I can tell, to isolate the effect of \"object-centric vs not\" while holding other dataset factors constant. They then test how well these different models do on an object-centric classification task (Pascal Cropped Boxes). They find that the contrastive model that trains cropped versions of the dataset gets about 3.5 points higher mean accuracy (71.9 vs 75.3) compared to the contrastive loss done on the multi-object versions of images. \n\nThey also explicitly try to measure different forms of invariance, through a scheme where they binarize the elements of the representation vector, and calculate what proportion of them fire on average with and without a given set of transformations. They find that the main form of invariances that contrastive learning does well at is invariance to occlusion (part of the image not being visible), where both contrastive methods give ~84 percent co-firing, and supervised pre-training only gets about 80.9. However, on other important measures of invariance - viewpoint, illumination direction, and instance (that is, specific instance within a class) - contrastive representations perform notably worse than supervised pretraining. \n\nhttps://i.imgur.com/7Ghbv5A.png\n\nTo try to solve these two problems, they propose a method that learns from video, and that uses temporally separated frames (which are then augmented) as pairs. They call this Frame Temporal Invariance, and argue, reasonably, that by pushing the representations of adjacent frames which track a (presumably consistent, or at least slowly-evolving) scene closer together, you should expect better invariance to viewpoint change and image deformation, since those things naturally happen when an object is moving through the world. They also suggest using an off-the-shelf object bounding box model to find particular objects, and track them throughout the video, and to use contrastive learning specifically on the bounding boxes that the algorithm thinks track a consistent object. \n\nhttps://i.imgur.com/2GfCTog.png\n\nOverall, my take on this paper is that the analysis they do - of different kinds of invariances contrastive vs supervised loss does well on, and of the extent to which contrastive loss results might be biased by datasets - is quite interesting and a valuable contribution to our understanding of these very currently-hypey algorithms. However, I'm a bit less impressed by the novelty of their proposed solution. Temporal forms of contrastive learning have been around before - in reinforcement learning, and even in the original Contrastive Predictive Coding paper, where the related pairs were related by dint of temporal closeness. So, while using it in video is certainly a good idea, it doesn't really feel strongly novel to me. I also feel a little confused by their choice of using an off-the-shelf object detection model as a pre-requisite for a self-supervised task, since my impression was that a central goal of self-supervision was building techniques that could scale to situations where it was infeasible to get large amounts of labels, and any method that relies on a pre-existing trained object bounding box model is pretty inherently limited in that regard.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2007.13916"
    },
    "218": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2002-00632",
        "transcript": "A central problem in the domain of reinforcement learning is how to incentivize exploration and diversity of experience, since RL agents can typically only learn from states they go to, and it can often be the case that states with high reward don't have an obvious trail of high-reward states leading to them, meaning that algorithms that are naively optimizing for reward will be relatively unlikely to discover them. One potential way to promote exploration is to train an ensemble of agents, and have part of your loss that incentivizes diversity in the behavior of those agents. Intuitively, an incentive for diversity will push policies away from one another, which will force them to behave differently, and thus reach a wider range of different states. \n\nCurrent work in diversity tends to penalize, for each agent, the average pairwise distance between it and every other agent. The authors of this paper have two critiques with this approach:\n\n1. Pure pairwise distance doesn't fully capture the notion of diversity we might want, since if you have policies that are clustered together into multiple clusters, average pairwise distance can be increased by moving the clusters farther apart, without having to break up the clusters themselves\n2. Having the diversity term be calculated for each policy independently can lead to \"cycling\" behavior, where policies move in ways that increase distance at each step, but don't do so when each agent's independent steps are taken into account \n\nAs an alternative, they propose calculating the pairwise Kernel function similarity between all policies (where each policy is represented as the average action probabilities it returns across a sample of states), and calculating the determinate of that matrix. The authors claim that this represents a better measure of full population diversity. I can't fully connect the dots on this intuitively, but what I think they're saying is: the determinant of the kernel matrix tells you  something about the effective dimensionality spanned by the different policies. In the same way that, if a matrix is low-rank, it tells you that some vectors within it can be nearly represented by linear combinations of others, a low value of the Kernel determinant means that some policies can be sufficiently represented by combinations of other policies, such that it isn't really adding diversity value to the ensemble. \n\nhttps://i.imgur.com/CmlGsNP.png\n\nAnother contribution of this paper is to propose an interesting bandit-based way of determining when to incentivize diversity vs focus on pure reward. The diversity term in the loss is governed by a lambda parameter, and the paper's model sets up Thompson sampling to determine what the value of the parameter should be at each training iteration. This bandit setup works by starting out uncertain over whether to include diversity in the loss, and building a model of whether reward tends to increase during steps where diversity is used. Over time, if diversity consistently doesn't produce benefits, the sampler will tend more towards excluding it from the loss. This is just a really odd, different idea, that I've never heard of before in the context of hyperparameter scheduling during training. I will say that I'm somewhat confused about the window of time that the bandit uses for calculating rewards. Is it a set of trajectories used in a single training batch, or longer than that? Questions aside, they do so fairly convincingly that the adaptive parameter schedule outperforms over a fixed schedule, though they don't test it against simpler forms of annealing, so I'm less clear on whether it would outperform those. \n\nOverall, I still have some confusions about the method proposed by this paper, but it seems to be approaching the question of exploration from an interesting direction, and I'd enjoy trying to understand it further in future.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2002.00632"
    },
    "219": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2007.08794",
        "transcript": "This work attempts to use meta-learning to learn an update rule for a reinforcement learning agent. In this context, \"learning an update rule\" means learning the parameters of an LSTM module that takes in information about the agent's recent reward and current model and outputs two values - a scalar and a vector - that are used to update the agent's model. I'm not going to go too deep into meta-learning here, but, at a high level, meta learning methods optimize parameters governing an agent's learning, and, over the course of many training processes over many environments, optimize those parameters such that the reward over the full lifetime of training is higher. \n\nTo be more concrete, the agent in a given environment learns two things: \n\n- A policy, that is, a distribution over predicted action given a state.\n- A \"prediction vector\". This fits in the conceptual slot where most RL algorithms would learn some kind of value or Q function, to predict how much future reward can be expected from a given state. However, in this context, this vector is *very explicitly* not a value function, but is just a vector that the agent-model generates and updates. The notion here is that maybe our human-designed construction of a value function isn't actually the best quantity for an agent to be predicting, and, if we meta-learn, we might find something more optimal. I'm a little bit confused about the structure of this vector, but I think it's *intended* to be a categorical 1-of-m prediction\n\nAt each step, after acting in the environment, the agent passes to an LSTM: \n\n- The reward at the step\n- A binary of whether the trajectory is done\n- The discount factor\n- The probability of the action that was taken from state t\n- The prediction vector evaluated at state t\n- The prediction vector evaluated at state t+1\n\nGiven that as input (and given access to its past history from earlier in the training process), the LSTM predicts two things: \n\n- A scalar, pi-hat\n- A prediction vector, y-hat\n\nThese two quantities are used to update the existing policy and prediction model according to the rule below.\n\nhttps://i.imgur.com/xx1W9SU.png\n\n Conceptually, the scalar governs whether to increase or decrease probability assigned to the taken action under the policy, and y-hat serves as a target for the prediction vector to be pulled towards.  An important thing to note about the LSTM structure is that none of the quantities it takes as input are dependent on the action or observation space of the environment, so, once it is learned it can (hopefully) generalize to new environments. \n\nGiven this, the basic meta learning objective falls out fairly easily - optimize the parameters of the LSTM to maximize lifetime reward, taken in expectation over training runs.  However, things don't turn out to be quite that easy. The simplest version of this meta-learning objective is wildly unstable and difficult to optimize, and the authors had to add a number of training hacks in order to get something that would work. (It really is dramatic, by the way, how absolutely essential these are to training something that actually learns a prediction vector). These include: \n\n- A entropy bonus, pushing the meta learned parameters to learn policies and prediction vectors that have higher entropy (which is to say: are less deterministic)\n- An L2 penalty on both pi-hat and y-hat\n- A removal of the softmax that had originally been originally taken over the k-dimensional prediction vector categorical, and switching that target from a KL divergence to a straight mean squared error loss. As far as I can tell, this makes the prediction vector no longer actually a 1-of-k categorical, but instead just a continuous vector, with each value between 0 and 1, which makes it make more sense to think of k separate binaries? This I was definitely confused about in the paper overall\n\nhttps://i.imgur.com/EL8R1yd.png\n\nWith the help of all of these regularizers, the authors were able to get something that trained, and that appeared to be able to perform comparably to or better than A2C - the human-designed baseline - across the simple grid-worlds it was being trained in. However, the two most interesting aspects of the evaluation were: \n\n1. The authors showed that, given the values of the prediction vector, you could predict the true value of a state quite well, suggesting that the vector captured most of the information about what states were high value. However, beyond that, they found that the meta-learned vector was able to be used to predict the value calculated with discount rates different that than one used in the meta-learned training, which the hand-engineered alternative, TD-lambda, wasn't able to do (it could only well-predict values at the same discount rate used to calculate it). This suggests that the network really is learning some more robust notion of value that isn't tied to a specific discount rate. \n\n2. They also found that they were able to deploy the LSTM update rule learned on grid worlds to Atari games, and have it perform reasonably well - beating A2C in a few cases, though certainly not all. This is fairly impressive, since it's an example of a rule learned on a different, much simpler set of environments generalizing to more complex ones, and suggests that there's something intrinsic to Reinforcement Learning that it's capturing",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2007.08794"
    },
    "220": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2006-04635",
        "transcript": "This paper focuses on an effort by a Deepmind team to train an agent that can play the game Diplomacy - a complex, multiplayer game where players play as countries controlling units, trying to take over the map of Europe. Some relevant factors of this game, for the purposes of this paper, are: \n\n1) All players move at the same time, which means you need to model your opponent's current move, and play a move that succeeds in expectation over that predicted move distribution. This also means that, in order to succeed, your policy can't be easily predictable, since, if it is, you're much easier to exploit, since your opponents can more easily optimize their response to what they predict you'll do \n\n2) The action space is huge, even compared to something like Chess \n\n3) The game has significant multiagent complexity: rather than being straightforwardly zero-sum in its reward structure, like Chess or Go, it's designed to require alliances between players in order to succeed \n\nPrior work - DipNet -  had been able to outperform other hand-coded models through a deep network that imitated human actions, but the authors hadn't been able to get RL to successfully learn on top of that imitation baseline. \n\nThe basic approach this model takes is one that will probably feel familiar if you've read Deepmind's prior work on Chess or Go: an interplay between a fast-to-evaluate neural net component, and a slower, more explicit, strategically designed component. The slower \"expert\" component uses the fast network component as part of its evaluation of different moves' consequences, and then, once the slow expert has generated a series of decisions, the neural net policy learns to imitate those decisions. \n\nIn this case, the slower expert tries to explicitly calculate a Best Response strategy, given some belief about what your opponents will do at the state you're in. Since the action space is so large, it isn't possible to calculate a full best response (that is, your best *possible* action given the actions you expect your opponents to take), so this paper instead lays out a Sampled Best Response algorithm. It takes as input a state, as well as an opponent policy, a candidate policy, and a value network. (More on how those come to be layer). In the simplest case, the SBR algorithm works by: \n\n1. Sampling some number (B) of actions from the opponent policy given the state. These represent a sample distribution of what moves you think your opponents are likely to play\n2. Sampling some number (C) of candidate actions from your candidate policy. \n3. For each candidate action, evaluating - for each opponent action - the state you'd reach if you took the candidate action and the opponent took the opponent action, according to your value network. \n4. This gives you an estimated Q value for each of your candidate actions; if you pick the action with the highest Q value, this approximates your best response action to the opponent policy distribution you pass in \n\nOnce you have this SBR procedure, you can use it to bootstrap a better policy and a value network by starting with a policy and value learned from pure human-imitation, and then using SBR - with your current policy as both the opponent and candidate policy - to generate a dataset of trajectories (where each action in the trajectory is chosen according to the SBR procedure). With that trajectory dataset, you can train your policy and value networks to be able to better approximate the actions that SBR would have taken. \n\nThe basic version of this procedure is called IBR - Iterated Best Response. This is because, at each stage, SBR will return a policy that tries to perform well against the current version of the opponent policy. This kind of behavior is potentially troublesome, since you can theoretically have it lead to cycles, like those in Rock Paper Scissors where paper beats rock, scissors beat paper, and then rock beats scissors again. At each stage, you find the best response to what your opponent is doing right now, but you don't actually make transitive progress. A common way of improving along the axis of this particular failure mode is to learn via a \"fictitious play\" rather than \"self play\". Putting aside the name - which I don't find very usefully descriptive - this translates to simulating playing against, not the current version of your opponent, but a distribution made up of past versions of the opponent. This helps prevent cycling, because choosing a strategy that only defeats the current version but would lose to a prior version is no longer a rewarding option. \n\nThe Fictitious Play-based approach this paper proposes - FPPI2 - tries to resolve this problem. Instead of sampling actions in step (1) from only your current opponent policy, it uses a sampling procedure where, for each opponent action, it first samples a point in history, and then samples the opponent policy vector at that point in history (so the multiple opponent moves collectively represent a coherent point in strategy space). Given this action profile, the final version of the algorithm continues to use the most recent/updated timestep of the policy and value network for the candidate policy and value network used in SBR, so that you're (hopefully) sampling high quality actions, and making accurate assessments of states, even while you use the distribution of (presumably worse) policies to construct the opponent action distribution that you evaluate your candidate actions against. \n\nhttps://i.imgur.com/jrQAAQW.png\n\nThe authors don't evaluate against human players, but do show that their FPPI2 approach consistently outperforms the prior DipNet state of the art, and performed the best overall, though Iterated Best Response performs better than they say they expected.\n\nSome other thoughts: \n\n- This system is still fairly exploitable, and doesn't become meaningfully less so during training, though it does become more difficult to exploit quickly\n- This does seem like a problem overall where you do a lot of modeling of what you expect your opponents strategies to be, and it seems hard to be robust, especially to collusion of your opponents\n- I was a bit disappointed that the initial framing of the paper put a lot of emphasis on the alliance-focused nature of the game, but then neither suggested mechanisms targeting that aspect of the game, nor seemed to do any specific analysis of\n- I would have expected this game to benefit from some explicit modeling of different agents having different policies; possibly this just isn't something they could have had be the case under their evaluation scheme, which played against copies of a given policy?\n\nOverall, my sense is that this is still a pretty early-stage checkpoint in the effort of playing Diplomacy, and that we've still got a ways to go, but it is interesting early work, and I'm curious where it leads.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2006.04635"
    },
    "221": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iclr/RendaFC20",
        "transcript": "This is an interestingly pragmatic paper that makes a super simple observation. Often, we may want a usable network with fewer parameters, to make our network more easily usable on small devices. It's been observed (by these same authors, in fact), that pruned networks can achieve comparable weights to their fully trained counterparts if you rewind and retrain from early in the training process, to compensate for the loss of the (not ultimately important) pruned weights. This observation has been dubbed the \"Lottery Ticket Hypothesis\", after the idea that there's some small effective subnetwork you can find if you sample enough networks. \n\nGiven these two facts - the usefulness of pruning, and the success of weight rewinding - the authors explore the effectiveness of various ways to train after pruning. Current standard practice is to prune low-magnitude weights, and then continue training remaining weights from values they had at pruning time, keeping the final learning rate of the network constant. The authors find that: \n1. Weight rewinding, where you rewind weights to *near* their starting value, and then retrain using the learning rates of early in training, outperforms fine tuning from the place weights were when you pruned \n\nbut, also \n2. Learning rate rewinding, where you keep weights as they are, but rewind learning rates to what they were early in training, are actually the most effective for a given amount of training time/search cost \n\nTo me, this feels a little bit like burying the lede: the takeaway seems to be that when you prune, it's beneficial to make your network more \"elastic\" (in the metaphor-to-neuroscience sense) so it can more effectively learn to compensate for the removed neurons. So, what was really valuable in weight rewinding was the ability to \"heat up\" learning on a smaller set of weights, so they could adapt more quickly. And the fact that learning rate rewinding works better than weight rewinding suggests that there is value in the learned weights after all, that value is just outstripped by the benefit of rolling back to old learning rates. \n\nAll in all, not a super radical conclusion, but a useful and practical one to have so clearly laid out in a paper.",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=S1gSj0NKvB"
    },
    "222": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2004.13649",
        "transcript": "One of the most notable flaws of modern model-free reinforcement learning is its sample inefficiency; where humans can learn a new task with relatively few examples, model that learn policies or value functions directly from raw data need huge amounts of data to train properly. Because the model isn't given any semantic features, it has to learn a meaningful representation from raw pixels using only the (often sparse, often noisy) signal of reward. Some past approaches have tried learning representations separately from the RL task (where you're not bottlenecked by agent actions), or by adding more informative auxiliary objectives to the RL task. \n\nInstead, the authors of this paper, quippily titled \"Image Augmentation Is All You Need\", suggest using data augmentation of input observations through image modification (in particular, by taking random different crops of an observation stack),  and integrating that augmentation into the native structure of a RL loss function (in particular, the loss term for Q learning). There are two main reasons why you might expect image augmentation to be useful. \n\n1. On the most simplistic level, it's just additional data for your network\n2. But, in particular, it's additional data designed to exhibit ways an image observation can be different on a pixel level, but still not be meaningfully different in terms of its state within the game. You'd expect this kind of information to make your model robust to overfitting. \n\nThe authors go into three different ways they could add image augmentation to a Q Learning model, and show that each one provides additional marginal value. \n\nThe first, and most basic, is to just add augmented versions of observations to your training dataset. The basic method being used, Soft Actor Critic, uses a replay buffer of old observations, and this augmentation works by simply applying a different crop transformation each time an observation is sampled from a replay buffer. This is a neat and simple trick, that effectively multiplies the number of distinct observations your network sees by the number of possible crops, making it less prone to overfitting. \n\nThe next two ways involve integrating transformed versions of an observation into the structure of the Q function itself. As a quick background, Q learning is trained using a Bellman consistency loss, and Q tries to estimate the value of a (state, action) pair, assuming that you do the best possible thing at every point after you take the action at the state. The consistency loss tries to push your Q estimate of the value of a (state, action) pair closer to the sum of reward you got by taking that action and your current max Q estimate for the next state. The second term in this loss, the combined reward and next-step Q value, is called the target, since it's what you push your current-step Q value closer towards. \n\nThis paper suggests both: \n\n- Averaging your current-step Q estimate over multiple different crops the observation stack at the current state\n- Averaging the next-step Q estimate used in the target over multiple different crops (that aren't the ones used in the current-step averaging)\n\nThis has the nice side effect that, in addition to telling your network about image transformations (like small crops) that shouldn't impact its strategic assessment, it also makes your Q learning process overall lower variance, because both the current step and next step quantities are averages rather than single-sample values. \n\nhttps://i.imgur.com/LactlFq.png\n\nOperating in a lower data regime, the authors found that simply adding augmentations to their replay buffer sampling (without the two averaging losses) gave them a lot of gains in how efficiently they could learn, but all three combined gave the best performance.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2004.13649"
    },
    "223": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1912-05500",
        "transcript": "This paper out of DeepMind is an interesting synthesis of ideas out of the research areas of meta learning and intrinsic rewards. The hope for intrinsic reward structures in reinforcement learning - things like uncertainty reduction or curiosity -  is that they can incentivize behavior like information-gathering and exploration, which aren't incentivized by the explicit reward in the short run, but which can lead to higher total reward in the long run. So far, intrinsic rewards have mostly been hand-designed, based on heuristics or analogies from human intelligence and learning. \n\nThis paper argues that we should use meta learning to learn a parametrized intrinsic reward function that more directly succeeds our goal of facilitating long run reward. They do this by: \n\n- Creating agents that have multiple episodes within a lifetime, and learn a policy network to optimize Eta, a neural network mapping from the agent's life history to scalars, that serves as an intrinsic reward. The learnt policy is carried over from episode to episode.\n- The meta learner then optimizes the Eta network to achieve higher lifetime reward according to the *true* extrinsic reward, which the agent itself didn't have access to\n- The learned intrinsic reward function is then passed onto the next \"newborn\" agent, so that, even though its policy is reinitialized, it has access to past information in the form of the reward function\n\nThis neatly mimics some of the dynamics of human evolution, where our long term goals of survival and reproduction are distilled into effective short term, intrinsic rewards through chemical signals. The idea is, those chemical signals were evolved over millennia of human evolution to be ones that, if followed locally, would result in the most chance of survival. \n\nThe authors find that they're able to learn intrinsic rewards that \"know\" that they agent they're attached to will be dropped in an environment with a goal, but doesn't know the location, and so learns to incentivize searching until a goal is found, and then subsequently moving towards it. This smooth tradeoff between exploration and exploitation is something that can be difficult to balance between intrinsic exploration-focused reward and extrinsic reward.\n\nWhile the idea is an interesting one, an uncertainty I have with the paper is whether it'd be likely to scale beyond the simple environments it was trained on. To really learn a useful reward function in complex environments would require huge numbers of timesteps, and it seems like it'd be difficult to effectively assign credit through long lifetimes of learning, even with the lifetime value function used in the paper to avoid needing to mechanically backpropogate through entire lifetimes. It's also worth saying that the idea seems quite similar to a 2017 paper written by Singh et al; having not read that one in detail, I can't comment on the extent to which this work may just build incrementally on that one.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1912.05500"
    },
    "224": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2001.04451",
        "transcript": "The Transformer architecture - which uses a structure entirely based on key-value attention mechanisms to process sequences such as text - has taken over the worlds of language modeling and NLP in the past three years. However, Transformers at the scale used for large language models have huge computational and memory requirements. \n\nThis is largely driven by the fact that information at every step in the sequence (or, in the so-far-generated sequence during generation) is used to inform the representation at every other step. Although the same *parameters* are used for each of these pairwise calculation between keys and queries at each step, this is still a pairwise, and thus N^2, calculation, which can get very costly when processing long sequences on the scale of tens of thousands of tokens. This cost comes from both computation and memory, with memory being the primary focus of this paper, because the max memory requirements of a network step dictate the hardware it can be run on, in a way that the pure amount of computation that needs to be performed doesn't. A L^2 attention calculation, as naively implemented in a set of matrix multiplies, not only has to perform N^2 calculations, but has to be able to hold N^2 values in memory while performing the softmax and weighted sum that is the attention aggregation process. Memory requirements in Transformers are also driven by \n- The high parameter counts of dense layers within the network, which have less parameter use per calculation than attention does, and \n- The fact that needing to pass forward one representation per element in the list at each layer necessitates cumulatively keeping all the activations from each layer in the forward pass, so that you can use them to calculate derivatives in the backward pass. \n\nThis paper, and the \"Reformer\" architecture they suggest, is less a single idea, and more a suite of solutions targeted to make Transformers more efficient in use of both compute and memory. \n\n1. The substitution of Locality Sensitive Hashing for normal key-query attention is a strategy for reducing the L^2 compute and memory requirements of the raw attention calculation. The essential idea of attention is \"calculate how well the query at position i is matched by the key at every other position, and then use those matching softmax weights to calculate a weighted sum of the representations at each other position\". If you consider keys and queries to be in the same space, you can think of this as a similarity calculation between positions, where you want to most highly weight the most similar positions to you in the calculation of your own next-layer value. In this spirit, the authors argue that this weighted sum will be mostly influenced by the highest-similarity positions within the softmax, and so, instead of performing attention over the whole sequence, we can first sort positions into buckets based on similarity of their key/query vector for a given head, and perform attention weighting within those buckets. \n\nhttps://i.imgur.com/tQJkfGe.png\n\nThis has the advantage that the first step, of assigning a position's key/query vector to a bucket, can be done for each position individually, rather than with respect to the value at another position. In this case, this bucketing is performed by a Locality Sensitive Hashing algorithm, which works by projecting each position's vector into a lower-dimensional space, and then taking the index of that vector which has the max value. This is then used as a bucket ID for performing full attention within. This shifts the time complexity of attention from O(L^2) to O(LlogL), since for each position in the length, you only need to calculate explicit pairwise similarity for the log(L) other elements in its bucket \n\n2. Reversible layers.  This addresses the problem of needing to keep activations from each layer around for computing the backward-pass derivatives. It takes an idea used in RevNets, which proposes a reversible alternative to the commonly used ResNet architecture. In a normal ResNet scenario, Y = X + F(X), where F(X) is the computation of a single layer or block, and Y are the resulting activations passed to the next layer. In this setup, you can't go back from Y to get the value of X if you discard X for memory reasons, because the difference between the two is a function of X, which you don't have. As an alternative, RevNets define a sort of odd crosswise residual structure, that starts by partitioning X into two components, X1 and X2, and the output Y into Y1 and Y2, and performing the calculation shown below. \n\nhttps://i.imgur.com/EK2vBkK.png\n\nThis allows you to work backward, getting X2 from Y1 and Y2 (both of which you have as outputs), and then get X1 from knowing the other three parts.\n\nhttps://i.imgur.com/uLTrdyf.png\n\n This means that as soon as you have the activations at a given layer, you can discard earlier layer activations, which makes things a lot more memory efficient.  \n\n3. There's also a proposal to do (what I *think* is) a pretty basic chunking of feed forward calculations across sequence length, and performing feedforward calculations on parts of the sequence rather than the whole thing. The latter would be faster with vectorized computing, for parallelization reasons, but the former is more memory efficient",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2001.04451"
    },
    "225": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1909-11655",
        "transcript": "I found this paper a bit difficult to fully understand. Its premise, as far as I can follow, is that we may want to use genetic algorithms (GA), where we make modifications to elements in a population, and keep elements around at a rate proportional to some set of their desirable properties. In particular we might want to use this approach for constructing molecules that have properties (or predicted properties) we want. However, a downside of GA is that its easy to end up in local minima, where a single molecule, or small modifications to that molecule, end up dominating your population, because everything else gets removed for having less-high reward. The authors proposed fix for this is by training a discriminator to tell the difference between molecules from the GA population and those from a reference dataset, and then using that discriminator loss, GAN-style, as part of the \"fitness\" term that's used to determine if elements stay in the population. The rest of the \"fitness\" term is made up of chemical desiderata - solubility, how easy a molecule is to synthesize, binding efficacy, etc. I think the intuition here is that if the GA produces the same molecule (or similar ones) over and over again, the discriminator will have an easy time telling the difference between the GA molecules and the reference ones.  \n\nOne confusion I had with this paper is that it only really seems to have one experiment supporting its idea of using a discriminator as part of the loss - where the discriminator wasn't used at all unless the chemical fitness terms plateaued for some defined period (shown below). \n\nhttps://i.imgur.com/sTO0Asc.png\n\nThe other constrained optimization experiments in section 4.4 (generating a molecule with specific properties, improving a desired property while staying similar to a reference molecule, and drug discovery). They also specifically say that they'd like to be the case that the beta parameter - which controls the weight of the discriminator relative to the chemical fitness properties - lets you smoothly interpolate between prioritizing properties and prioritizing diversity/realness of images, but they find that's not the case, and that, in fact, there's a point at which you move beta a small amount and switch sharply to a regime where chemical fitness values are a lot lower. Plots of eventual chemical fitness found over time seem to be the highest for models with beta set to 0, which isn't what you'd expect if the discriminator was in fact useful for getting you out of plateaus and into long-term better solutions.\n\nOverall, I found this paper an interesting idea, but, especially since it was accepted into ICLR, found it had confusingly little empirical support behind it.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1909.11655"
    },
    "226": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1101/2020.03.03.972133",
        "transcript": "This preprint is a bit rambling, and I don't know that I fully followed what it was doing, but here's my best guess: \n\nhttps://i.imgur.com/xC2ryzp.png\n\n- We think it's probably the case that SARS-COV2 (COVID19) uses a protease (enzyme involved in its reproduction) that isn't available and co-optable in the human body, and is also quite similar to the comparable protease protein in the original SARS virus. Therefore, it is hoped that we might be able to take inhibitors that bind to SARS, and modify them in small ways to make them bond to SARS-COV2\n- The paper notes that it's specifically interested in targeted covalent inhibitors. These are drugs that inhibit the function of a protein by actually covalently binding with the relevant binding pocket, as opposed to most drugs, which by default just fit neatly inside the pocket and occupy it much of the time in equilibrium, but don't actually form permanent, stable covalent bonds with the protein. This class of drugs can be more effective, because its binding is stronger and more permanent, but it can also be more dangerous, because its potential side effects if it binds with a non-intended protein pocket can be more severe.\n- In order to get a covalently-binding drug that fits with the pocket of SARS-COV2, the authors start with a known inhibitor from SARS, and then use reinforcement learning to make modifications to it. The allowed modification actions consist of adding or removing \"fragments\" rather than atoms, where \"fragments\" here refers to coherent subcomponents of other drugs from similar families that were broken up according to hand-coded chemical rules. This approach is more stable than just adding on molecules, because at every stage in generation, the generated molecule will be coherent and chemically sound.\n- The part I don't fully follow is what they use for the reward function for compounds that are  in the process of being made. They specify that they do reward intermediate compounds, rather than just ones at the end of generation, but don't specify what goes into the reward. If I had to guess, I'd imagine it consists of (1) a molecular docking simulation that can't be differentiated through, and thus can't be used directly as a loss function, and/or (2) hand-coded heuristics from chemists for what makes a stable binding",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1101/2020.03.03.972133"
    },
    "227": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-2003-03123",
        "transcript": "This paper, presented this week at ICLR 2020, builds on existing applications of message-passing Graph Neural Networks (GNN) for molecular modeling (specifically: for predicting quantum properties of molecules), and extends them by introducing a way to represent angles between atoms, rather than just distances between them, as current methods are limited to. \n\nThe basic version of GNNs on molecule data works by creating features attached to atoms at each level (starting at level 0 with the element-specific embedding of that atom), and constructing \"messages\" between neighboring atoms that are a function of the neighbor atom's feature vector and the distance between the two neighbors. (This is the minimal version; some methods also include the bond type along with the distance as part of the edge-specific features). At a given layer, an atom's features are updated by applying an update function to both its own prior value and the sum of all the messages it receives from neighbors.\n\nThe trouble with this method is that it doesn't account for angular relationships between sets of atoms, which are physically important to quantum properties of a molecule. The naive way you might imagine representing angle is by situating the molecule in a 2D grid, and applying spherical convolutions, so your contribution to a neighbor's features would be based on your spherical distance away. However, this doesn't work, because molecules don't have a canonical frame of reference - there is no fixed left or right, or up and down, and operating in this way would mean that a molecule and its horizontal flip would have different representations. \n\nInstead, the authors propose an interesting inversion of the existing approaches, where feature vectors are attached to atoms, and are updated using the features of other atoms. In this model, features live on \"messages\" between pairs of atoms, and are updated by incorporating information from all messages within some local distance window. Importantly, each pair of atoms has a vector associated with their relationship in the molecule, and so when you combine two such messages together, you can calculate the angle between the associated vectors. This angle is invariant to flipping or rotation, because it's defined based on reference points internal to the molecule, which move together when the molecule is moved.\n \nhttps://i.imgur.com/mw46gWz.png\n\nMessages are updated from other messages using a combination of the distance between the non-shared endpoints of the messages (that is, if both message vectors share an endpoint i, and go to j and k respectively, this would be the distance between j and k), and the angle between the (i-j) vector and the (i-j) vector. For physics-based reasons I admit I didn't fully follow, these two pieces of information are embedded in a spherical basis function, so messages will update from each other differently based on their relative positions in a sphere.  \n\nhttps://i.imgur.com/Tvc7Gex.png\n\nThe representation of a given atom is then simply the sum of all its incoming messages, conditioned by the distance between the reference atom and the paired neighbor for which the message is defined. A concatenation of atom representations across layers is used to create a final atom representation, which is used for final quantum property prediction. \n\nThe authors tested on two datasets, and found dramatic improvements, with an average of 31% relative gain on the prior state of the art over different quantum property targets.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/2003.03123"
    },
    "228": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1908-06760",
        "transcript": "In the last three years, Transformers, or models based entirely on attention for aggregating information from across multiple places in a sequence, have taken over the world of NLP. In this paper, the authors propose using a Transformer to learn a molecular representation, and then building a model to predict drug/target interaction on top of that learned representation. A drug/target interaction model takes in two inputs - a protein involved in a disease pathway, and a (typically small) molecule being considered as a candidate drug - and predicts the binding affinity they'll have for one another. If binding takes place between the two, that protein will be prevented from playing its role in the chain of disease progression, and the drug will be effective.\n\nThe mechanics of the proposed Molecular Transformer DTI (or MT-DTI) model work as follows: \nhttps://i.imgur.com/ehfjMK3.png\n\n- Molecules are represented as SMILES strings, a character-based representation of atoms and their structural connections. Proteins are represented as sequences of amino acids.\n- A Transformer network is constructed over the characters in the SMILES string, where, at each level, the representation of each character comes from taking an attention-weighted average of the representations at other positions in the character string at that layer. At the final layer, the aggregated molecular representation is taken from a special \"REP\" token.\n- The molecular transformer is pre-trained in BERT-like way: by taking a large corpus (97M molecules) of unsupervised molecular representations, masking or randomizing tokens within the strings, and training the model to predict the true correct token at each point. The hope is that this task will force the representations to encode information relevant to the global structures and chemical constraints of the molecule in question\n- This pre-trained Transformer is then plugged into the DTI model, alongside a protein representation model in the form of multiple layers convolutions acting on embedded representations of amino acids. The authors noted that they considered a similar pretrained transformer architecture for the protein representation side of the model, but that they chose not to because (1) the calculations involved in attention are N^2 in the length of the sequence, and proteins are meaningfully longer than the small molecules being studied, and (2) there's no comparably large dataset of protein sequences that could be effectively used for pretraining\n- The protein and molecule representations are combined with multiple dense layers, and then produce a final affinity score. Although the molecular representation model starts with a set of pretrained weights, it also fine tunes on top of them.\n\nhttps://i.imgur.com/qybLKvf.png\n\nWhen evaluated empirically on two DTI datasets, this attention based model outperforms the prior SOTA, using a convolutional architecture, by a small but consistent amount across all metrics. Interestingly, their model is reasonably competitive even if they don't fine-tune the molecular representation for the DTI task, but only pretraining and fine-tuning together get the MT-DTI model over the threshold of prior work.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1908.06760"
    },
    "229": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1038/s41586-019-1923-7",
        "transcript": "In January of this year (2020), DeepMind released a model called AlphaFold, which uses convolutional networks atop sequence-based and evolutionary features to predict protein folding structure. In particular, their model was designed to predict a distribution for how far away each pair of amino acids will be from one another in the final folded structure. Given such a trained model, you can score a candidate structure according to how likely it is under the model, and - if your process for generating candidates is differentiable, as it is in this case - you can directly optimize the structure to increase its probability.  \n\nhttps://i.imgur.com/9ZBhqRo.png\n\nThe distance-prediction model takes as input two main categories of feature: \n\n1. Per-residue features characterizing which amino acid that residue is based on different techniques that produce one-hot amino acid type, or a distribution over amino acids. \n2. Residue pair features based on parameters of Multiple Sequence Alignment (MSA) models. I don't deeply understand the details of how the specific models here work, but at a high level: MSA features are based on the evolutionary intuition that residues that make contact within  a protein will likely evolve in a correlation with one another, and that you can simulate these evolutionary timestep correlations by comparing highly similar proteins (which were likely close in evolutionary time). \n\nhttps://i.imgur.com/h16lPwU.png\n\nThese features are stacked in a LxL grid, with the per-residue-pair features differing at each point in the grid, and the per-residue features staying constant for a full row or column (since they correspond to a given i for all j). One relevant note here is that proteins can be hundreds or thousands of residues long, and, so you can't actually construct a full LxL matrix, either on the input or output end. Instead, the notional full LxL grid is subdivided into a courser grid of 64-residue square regions, and a single one of these 64x64 regions (which could be either adjacent or far away in the protein) is passed into the model at a time. \n\nGiven these 64x64x<features> input, the model performs several layers of dilated convolutions - which allow features at a given point in the grid to be informed by information farther away - still in a 2D arrangement. The model then outputs a 64x64 grid (one element for each [i, j] amino acid pair), where each element in the grid is a 64-deep discretized probability distribution over the distance between those two residues. When I say \"discretized probability distribution,\" what I actually mean is \"histogram\". This discretization of an output distribution, where you predict how much probability mass will be in each possible distance bin, allows for flexible and finer-grained predicted distributions than, for example, you could get with a continuous Gaussian model centered around a single point. Amusingly, because the model predicts distance histograms for each residue pair, the authors term the output a \"distogram\". During training, the next-to-last layer of the model is also used to predict per-residue auxiliary features: the accessible surface area of the residue in the folded structure, and the secondary structure type (helix, strand, etc) that the residue will be a part of. However, these are just used to provide more signal during training, and aren't used for either protein structure optimization or calculation of test scores. \n\nTo actually generated predicted fold structures, the authors construct a generative model of fold structure where each amino acid is assigned two torsion angles that govern its connection to its neighbor. By setting these torsion angles to different values, you can twist and reshape the protein as a whole. Given this generative model, things proceed as you might suspect: you generate a candidate, calculate the resulting inter-residue distances, calculate a likelihood of those distances under the model you've learned, and send back a gradient to change your torsion angles to make that likelihood higher. \n\nEmpirically, the Deepmind authors evaluated on a competition dataset, and specifically compared themselves against other approaches that (like theirs) didn't make predictions for a new protein by comparing against similar templates (Template Modeling, or TM) but instead modeled from raw features (Free Modeling, or FM). AlphaFold was able to achieve high accuracy on 24 out of the 43 test domains (where a domain is a cluster of highly related proteins) compared to the next best method, which only got 14 out of the 43. Definitely still not perfect, since almost half of the test proteins were out of its reach, but fairly compelling evidence that there's value to DeepMind's approach.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1038/s41586-019-1923-7"
    },
    "230": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1093/bioinformatics/bty573",
        "transcript": "Most of the interesting mechanics within living things are mediated by interactions between proteins, making it important and useful to have good predictive models of whether proteins will interact with one another, for validating possible interaction graph structures. \n\nPrior methods for this problem - which takes as its input sequence representations of two proteins, and outputs a probability of interaction - have pursued different ideas for how to combine information from the two proteins. On a basic level, you need your method to be independent to the ordering of the proteins, since the property we care about is a property of a pair of proteins, not a property of a particular ordering of proteins. Some examples of those have included: \n\n- A kernel function between some representation of proteins\n- Representing a protein pair according to whether and how often given k-mer sequences co-occur in both proteins\n\nThis paper's DPPI method is built on a Siamese network, which applies a single shared set of convolutional layers to each of the two proteins, and then calculates a \"binding score,\" that structurally acts a bit like a similarity score, but with allowances for proteins to be complementary, rather than just similar. In more detail: \n\nhttps://i.imgur.com/8ruY9es.png\n\n1. Crop each protein into multiple overlapping subsequences of length 512 amino acids. Perform all following steps for every combination of cropped subsequences between the two proteins. (If A is divided into A1 and A2, you'd do the following steps for A1 x B and A2 x B and take the max score out of the two) \n2. Each cropped protein is represented as a probabilistic sequence. Since we can't get fully certain sequences of what amino acid is at each point in the chain, we instead pass in a 20x512 representation, where at each of the 512 locations we have a distribution over 20 possible amino acids. This tensor is passed into multiple layers of convolutional network, with the same network weights applied to each of the two proteins. \n3. A random projection is applied to the outputs of the convolutional network. The features that come out of the projection are conceptually similar to feature maps that might come out of a neural network layer, except that the weights aren't learned. This random projection has a specialized structure, in that it's composed of two (randomly-weighted) networks, A and B, each of which result in feature maps A1...K and B1...K. For protein 1, the outputs of the network are ordered A1...AK B1...BK, whereas for protein 2, the weights are swapped, and so the outputs are ordered B1...BK A1...AK. \n4. A Hadamard product between the two random projection outputs. This is basically a dot product, but for matrices (you multiply each element in the matrix by the corresponding element in the other matrix). This is basically like calculating a similarity score between the feature values of the randomly projected features. One benefit of doing the odd reordering in the prior step is that it breaks symmetry: if we took a dot product between features calculated by a fully shared-weight network, then we'd be looking explicitly for similarity between sequence features, which might not be sufficient to know if proteins interact in a complementary way. Another benefit is that it makes the final fully connected layer (which predicts interaction) agnostic to the order of inputs. (Caveat: I only about 70% follow the logic of this) In the example above, the 1st element will end up being A1(Prot1) x B1(ProtB), and the K+1th element will end up being B1(Prot1) x A1(ProtB). Since multiplication is order-independent,  both values 1 and K+1 represent the similarity between the proteins according to features A/B1. \n5. Once you have this final representation, feed it into a fully connected layer \n\nhttps://i.imgur.com/3LsgZNn.png\n\nThe authors show superior performance to past methods, and even show that they can get 96% accuracy on protein interactions within humans from training on a non-human species, showing that a lot of the biomechanical logic transfers. \n\nhttps://i.imgur.com/REoU3Ab.png\n\nThey did an ablation test and showed that the random projection layer added value, but also that it was better to have the weights be random than learned, which was surprising to me, and suggests the model as a whole is prone to overfit.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1093/bioinformatics/bty573"
    },
    "231": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1802-04364",
        "transcript": "Prior to this paper, most methods that used machine learning to generate molecular blueprints did so using SMILES representations - a string format with characters representing different atoms and bond types. This preference came about because ML had existing methods for generating strings that could be built on for generating SMILES (a particular syntax of string). However, an arguably more accurate and fundamental way of representing molecules is as graphs (with atoms as nodes and bonds as edges). Dealing with molecules as graphs avoids the problem of a given molecule having many potential SMILES representations (because there's no canonical atom to start working your way around the molecule on), and, hopefully, would have an inductive bias that somewhat more closely matches the actual biomechanical interactions within a molecule. \n\nOne way you could imagine generating a graph structure is by adding on single components (atoms or bonds) at a time. However, the authors of this paper argue that this approach is harder to constrain to only construct valid molecular graphs, since, in the course of sampling out a molecule, you'd have to go through intermediate stages that you expect to be invalid (for example, bonds with no attached atoms), making it hard to add in explicit validity checks. The alternate approach proposed here works as follows: \n\n- Atoms within molecules are grouped into valid substructures, based on a combination of biologically-motivated rules (like treating aromatic rings as a single substructure) and computational heuristics. For the purpose of this paper, substructures are generally either 1) a ring, 2) two particular atoms on either end of a bond, or 3) a \"tail\" with a bond and an atom. Importantly, these substructures are designed to be overlapping - if you had a N bonded with O, and then O with C (this example are entirely made up, and I expect chemically incoherent), then you could have \"N-O\" as one substructure, and \"O-C\" as another.\n\nhttps://i.imgur.com/yGzRPjT.png\n\n- Using these substructures (or clusters), you can form a simplified representation of a molecule, as a connected, non-cyclic junction tree of clusters connected together. This doesn't give you all the information you'd need to construct the molecule - since there could be multiple different ways, on a molecular level, to connect two substructures, but it does give a blueprint of what the molecule will look like.\n- Given these two representations, the paper proposes a two-step encoding and decoding process. For a given molecule, we encode both the full molecular graph and the simplified junction tree, getting out vectors Zg and Zt respectively.\n- The first step of decoding generates a tree given the Zt representation. This generation process works via graph message-passing, taking in the Zt vector in addition to whatever part of the tree exists, and predicting a probability for whether that node has a child, and, if it exists, a probability for what cluster is at that child node. Given this parametrized set of probabilities, we can calculate the probability of the junction tree representation of whatever ground truth molecule we're decoding, and train the tree decoder to increase that model likelihood. (Importantly, although we frame this step as \"reconstruction,\" during training, we're not actually sampling discrete nodes and edges, because we couldn't backprop through that, we're just defining a probability distribution and trying to increase the probability of our real data under it).\n- The second step of decoding takes in a tree - which at this point is a set of cluster labels with connections specified between one another - as well as the Zg vector, and generates a full, atom-level graph. This is done by enumerating all the ways that two substructures could be attached (this number is typically small, \u22644), and learning a parametrized function that scores each possible type of connection, based on the full tree \"blueprint\", the Zg embedding, and the molecule that has been generated so far.\n- When you want to sample a new molecule, you can draw a sample from the prior distributions of Zg and Zt, and run the decoding process in a sampling mode, actually making discrete draws from the distributions given by your model\n\nhttps://i.imgur.com/QdSY25u.png\n\nThe authors perform three empirical tests: ability to successfully sample-reconstruct a given molecule, ability to optimize for a desired chemical property by finding a Z that scores high on that property according to an auxiliary predictive model, and ability to optimize for a property while staying within a given similarity radius to an original anchor molecule. The Junction Tree approach outperforms on all three tasks. On reconstruction, it matches the best alternative method on reconstruction reliability, but with 100% valid molecules, rather than 43.5% on the competing method. \n\nOverall, I found this paper really enjoyable and satisfying to read. Occasionally, ML-for-bio papers err on the side of too little domain thought (just throwing the most generic-for-images model structure at a problem), or too little machine learning thought (take hand-designed features and throw them at a whole range of models), where I think this one struck a nice balance of some amount of domain knowledge (around what makes for valid substructures) but embedded in a complex and thoughtfully designed neural network framework.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.04364"
    },
    "232": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GuimaraesSFA17",
        "transcript": "This paper's proposed method, the cleverly named ORGAN, combines techniques from GANs and reinforcement learning to generate candidate molecular sequences that incentivize desirable properties while still remaining plausibly on-distribution. \n\nPrior papers I've read on molecular generation have by and large used approaches based in maximum likelihood estimation (MLE) - where you construct some distribution over molecular representations, and maximize the probability of your true data under that distribution. However, MLE methods can't be less powerful when it comes to incentivizing your model to precisely conform with structural details of your target data distribution. Generative Adversarial Networks (GANs) on the other hand, use a discriminator loss that directly penalizes your generator for being recognizably different from the true data. However, GANs have historically been difficult to use on data like the string-based molecular representations used in this paper. That's because strings are made up of discrete characters, which need to be sampled from underlying distributions, and we don't naively have good ways of making sampling from discrete distributions a differentiable process. SeqGAN was proposed to remedy this: instead of using the discriminator loss directly as the generator's loss - which would require backpropogating through the sampling operation - the generator is trained with reinforcement learning, using the discriminator score as a reward function. Each addition of an element to the sequence - or, in our case, each addition of a character to our molecular representation string -  represents an action, and full sequences are rewarded based on the extent to which they resemble true sequences. \n\nhttps://i.imgur.com/dqtcJDU.png\n\nThis paper proposes taking that model as a base, but then adding a more actually-reward-oriented reward onto it, incentivizing the model to produce molecules that have certain predicted properties, as determined by a (also very not differentiable) external molecular simulator. So, just taking a weighted sum of discriminator loss and reward, and using that as your RL reward. After all, if you're already using the policy gradient structures of RL to train the underlying generator, you might as well add on some more traditional-looking RL rewards. The central intuition behind using RL in both of these cases is that it provides a way of using training signals that are unknown or - more to the point - non-differentiable functions functions of model output. In their empirical tests focusing on molecules, the authors target the RL to optimize for one of solubility, synthesizability, and druggability (three well-defined properties within molecular simulator RDKit), as well as for uniqueness, penalizing any molecule that had been generated already. \n\nhttps://i.imgur.com/WszVd1M.png\n\nFor all that this is an interesting mechanic, the empirical results are more equivocal. Compared to Naive RL, which directly optimizes for reward without the discriminator loss, ORGAN (Or, ORWGAN, the better-performing method using a Wasserstein GAN) doesn't have notably better rates of how often its generated strings are valid, and (as you would expect) performs comparably or slightly worse when it comes to optimizing the underlying reward. It does exhibit higher diversity than naive RL on two of the three tasks, but it's hard to get an intuition for the scales involved, and how much that scale of diversity increase would impact real results.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.10843"
    },
    "233": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jcheminf/OlivecronaBEC17",
        "transcript": "Over the past few days, I've been reading about different generative neural networks being tried out for molecular generation. So far this has mostly focused on latent variable space models like autoencoders, but today I shifted attention to a different approach rooted in reinforcement learning. The goal of most of these methods is 1) to build a generative model that can sample plausible molecular structures, but more saliently 2) specifically generate molecules optimized to exhibit some property of interest. The two autoencoder methods I read about did this by building a model to predict properties from latent space, and then optimizing the latent space vector to push up the value of those predictions. A central difficulty of this, and something that was a challenge for the autoencoder methods I read about, was the difficulty of explicitly incentivizing and promoting structurally valid molecular representations when going \"off distribution\" in search of molecules not in your training set that you predict will be better along some axis, since optimizing any direction - particularly a direction governed by a imperfect predictive model - without constraints is likely to lead to models that find the easy route of finding edge cases of your property-prediction model, rather than more difficult, truly valid and novel structures. \n\nhttps://i.imgur.com/NafoeDr.png\n\nAn advantage of using reinforcement learning as a framework here is that, because your loss doesn't need to be a continuous analytic function of your outputs, you can explicitly add molecular validity, as calculated by some external program, as part of your reward signal. This allows you to penalize a model for optimizing away from valid outputs. \n\nThe specific approach proposed by the authors of this paper has two phases of training. \n\n1) A RNN sequence model trained to do character-by-character prediction of SMILES strings (a character-based molecular representation). This is just a probability distribution over SMILES strings, with no RL involved yet, and is referred to as the Prior.\n\n2) Taking that pretrained sequence model, caching it, and then fine-tuning on top with a hybrid RL and maximum likelihood loss. As seen in the equation below, this loss creates a hybrid, posterior-esque likelihood that combines the probability of an action sequence (where an action is \"the choice of next character given currently generated string\") under the prior with the reward (or \"score\", S(A)) of the action sequence, and tries to push the probability under the learned policy to be closer to that hybrid likelihood.\n\nhttps://i.imgur.com/U4ZvKsJ.png\nhttps://i.imgur.com/b28Ea7m.png\n\nThe notion here is that by including the prior in your RL loss, you keep your generated molecules closer to the learned molecular distribution, rather than letting it push towards edge cases that are improbable, but not in ways you were able to directly disincentivize with your reward function. This is an interesting framing of this problem as having two failure modes: generating molecules that are structurally invalid, in that they don't correspond to syntactically feasible representations, and generating molecules that are technically feasible but are unlikely under the actual distribution of molecules, which captures more nuanced and hard-to-hardcode facts about energetic feasibility. \n\nThe authors experiment with three tasks: \n\n- Learning to avoid structures that contain sulphur (with a reward function that penalizes both invalid molecules and the presence of sulphur). On this task, they show that methods that make use of the prior (compared to ablations that are only incentivized to increase reward, or that are pulled towards the prior in a less direct way) do a better job of solving the problem in realistic ways rather than overly simplified ones.\n- Learning to generate structures with high similarity to a particular reference molecule. Here, they perform an interesting test where they remove the reference molecule and things similar to it from the training set of the Prior, which leads to the model not immediately falling into the easy solution of just generating exact copies of the reference molecule, but instead more interesting similar-but-not-identical analogues\n- Learning to generate structures that are predicted - by a separate predictive model - to be active against a target of interest.  A similar Prior-limitation test was performed, where all the true positives from the bioactivity model are removed from sequence training, and this led to a more diverse set of solutions that did less of just mimicking the existing known positives\n\nOverall, while this paper was relatively straightforward from a machine learning perspective, I enjoyed it, thought the methods were a sensible improvement over prior work I'd read, and that the evaluations performed were an interesting test of some of the paper's ideas.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q42766915"
    },
    "234": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Gomez-Bombarelli16",
        "transcript": "I'll admit that I found this paper a bit of a letdown to read, relative to  expectations rooted in its high citation count, and my general excitement and interest to see how deep learning could be brought to bear on molecular design. But before a critique, let's first walk through the mechanics of how the authors' approach works. \n\nThe method proposed is basically a very straightforward Variational Auto Encoder, or VAE. It takes in a textual SMILES string representation of a molecular structure, uses an encoder to map that into a continuous vector representation, a decoder to map the vector representation back into a a SMILES string, and an auxiliary predictor to predict properties of a molecule given the continuous representation. So, the training loss is a combination of the reconstruction loss (log probability of the true molecule under the distribution produced by the decoder) and the semi-supervised predictive loss. The hope with this model is that it would allow you to sample from a space of potential molecules by starting from an existing molecule, and then optimizing the the vector representation of that molecule to make it score higher on whatever property you want to optimize for. \n\nhttps://i.imgur.com/WzZsCOB.png\n\nThe authors acknowledge that, in this setup, you're just producing a probability distribution over characters, and that the continuous vectors sampled from the latent space might not actually map to valid SMILES strings, and beyond that may well not correspond to chemically valid molecules. Empirically, they said that the proportion of valid generated molecules ranged between 1 and 70%. But they argue that it'd be too difficult to enforce those constraints, and instead just sample from the model and run the results through a hand-designed filter for molecular validity. In my view, this is the central weakness of the method proposed in this paper: that they seem to have not tackled the question of either chemical viability or even syntactic correctness of the produced molecules. I found it difficult to nail down from the paper what the ultimate percentage of valid molecules was from points in latent space that were off of the training . A table reports \"percentage of 5000 randomly-selected latent points that decode to valid molecules after 1000 attempts,\" but I'm confused by what the 1000 attempts means here - does that mean we draw 1000 samples from the distribution given by the decoder, and see if *any* of those samples are valid? That would be a strange metric, if so, and perhaps it means something different, but it's hard to tell. \n\nhttps://i.imgur.com/9sy0MXB.png\n\nThis paper made me really curious to see whether a GAN could do better in this space, since it would presumably be better at the task of incentivizing syntactic correctness of produced strings (given that any deviation from correctness could be signal for the discriminator), but it might also lead to issues around mode collapse, and when I last checked the literature, GANs on text data in particular were still not great.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1610.02415"
    },
    "235": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GilmerSRVD17",
        "transcript": "In the years before this paper came out in 2017, a number of different graph convolution architectures - which use weight-sharing and order-invariant operations to create representations at nodes in a graph that are contextualized by information in the rest of the graph - had been suggested for learning representations of molecules. The authors of this paper out of Google sought to pull all of these proposed models into a single conceptual framework, for the sake of better comparing and testing the design choices that went into them. All empirical tests were done using the QM9 dataset, where 134,000 molecules have predicted chemical properties attached to them, things like the amount of energy released if bombs are sundered and the energy of electrons at different electron shells. \n\nhttps://i.imgur.com/Mmp8KO6.png\n\nAn interesting note is that these properties weren't measured empirically, but were simulated by a very expensive quantum simulation, because the former wouldn't be feasible for this large of a dataset. However, this is still a moderately interesting test because, even if we already have the capability to computationally predict these features, a neural network would do much more quickly. And, also, one might aspirationally hope that architectures which learn good representations of molecules for quantum predictions are also useful for tasks with a less available automated prediction mechanism.\n\nThe framework assumes the existence of \"hidden\" feature vectors h at each node (atom) in the graph, as well as features that characterize the edges between nodes (whether that characterization comes through sorting into discrete bond categories or through a continuous representation). The features associated with each atom at the lowest input level of the molecule-summarizing networks trained here include: the element ID, the atomic number, whether it accepts electrons or donates them, whether it's in an aromatic system, and which shells its electrons are in.  \n\nhttps://i.imgur.com/J7s0q2e.png\n\nGiven these building blocks, the taxonomy lays out three broad categories of function, each of which different architectures implement in slightly different ways. \n\n1. The Message function, M(). This function is defined with reference to a node w, that the message is coming from, and a node v, that it's being sent to, and is meant to summarize the information coming from w to inform the node representation that will be calculated at v. It takes into account the feature vectors of one or both nodes at the next level down, and sometimes also incorporates feature vectors attached to the  edge connecting the two nodes. In a notable example of weight sharing, you'd use the same Message function for every combination of v and w, because you need to be able to process an arbitrary number of pairs, with each v having a different number of neighbors.  The simplest example you might imagine here is a simple concatenation of incoming node and edge features; a more typical example from the architectures reviewed is a concatenation followed by a neural network layer. The aggregate message being sent to the receiver node is calculated by summing together the messages from each incoming vector (though it seems like other options are possible; I'm a bit confused why the paper presented summing as the only order-invariant option). \n2. The Update function, U(). This function governs how to take the aggregated message vector sent to a particular node, and combine that with the prior-layer representation at that node, to come up with a next-layer representation at that node. Similarly, the same Update function weights are shared across all atoms. \n3. The Readout function, R(), which takes the final-layer representation of each atom node and aggregates the representations into a final graph-level representation an order-invariant way \n\nRather than following in the footsteps of the paper by describing each proposed model type and how it can be described in this framework, I'll instead try to highlight some of the more interesting ways in which design choices differed across previously proposed architectures. \n\n- Does the message function being sent from w to v depend on the feature value at both w and v, or just v? To put the question more colloquially, you might imagine w wanting to contextually send different information based on different values of the feature vector at node v, and this extra degree of expressivity (not present in the earliest 2015 paper), seems like a quite valuable addition (in that all subsequent papers include it)\n- Are the edge features static, categorical things, or are they feature vectors that get iteratively updated in the same way that the node vectors do? For most of the architectures reviewed, the former is true, but the authors found that the highest performance in their tests came from networks with continuous edge vectors, rather than just having different weights for different category types of edge\n- Is the Readout function something as simple as a summation of all top-level feature vectors, or is it more complex? Again, the authors found that they got the best performance by using a more complex approach, a Set2Set aggregator, which uses item-to-item attention within the set of final-layer atom representations to construct an aggregated grap-level embedding\n\nThe empirical tests within the paper highlight a few more interestingly relevant design choices that are less directly captured by the framework. The first is the fact that it's quite beneficial to explicitly include Hydrogen atoms as part of the graph, rather than just \"attaching\" them to their nearest-by atoms as a count that goes on that atom's feature vector. The second is that it's valuable to start out your edge features with a continuous representation of the spatial distance between atoms, along with an embedding of the bond type. \n\nThis is particularly worth considering because getting spatial distance data for a molecule requires solving the free-energy problem to determine its spatial conformation, a costly process. We might ideally prefer a network that can work on bond information alone. The authors do find a non-spatial-information network that can perform reasonably well - reaching full accuracy on 5 of 13 targets, compared to 11 with spatial information. However, the difference is notable, which, at least from my perspective, begs the question of whether it'd ever be possible to learn representations that can match the performance of spatially-informed ones without explicitly providing that information.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.01212"
    },
    "236": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Altae-TranRPP16",
        "transcript": "The goal of one-shot learning tasks is to design a learning structure that can perform a new task (or, more canonically, add a new class to an existing task) using only one a small number of examples of the new task or class. So, as an example: you'd want to be able to take one positive and one negative example of a given task and correctly classify subsequent points as either positive or negative. A common way of achieving this, and the way that the paper builds on, is to learn a parametrized function projecting both your labeled points (your \"support set\") and your unlabeled point (your \"query\") into an embedding space, and then assigning a class to your query according to how close it is to the support set points associated with each label. The hope is that, in the course of training on different but similar tasks, you've learned a metric space where nearby things tend to be of similar classes. This method is called a \"matching network\". This paper has the specific objective of using such one-shot methods for drug discovery, and evaluates on tasks drawn from that domain, but most of the mechanics of the paper can be understood without reference to molecular dat in particular. \n\nIn the simplest version of such a network, the query and support set points are embedded unconditionally - meaning that the query would be embedded in the same way regardless of the values in the support set, and that each point in the support set would be embedded without knowledge of each other. However, given how little data we're giving our model to work with, it might be valuable to allow our query embedder (f(x)) and support set embedder (g(x)) to depend on the values within the support set. Prior work had achieved this by: \n\n1) Creating initial f'(x) and g'(x) query and support embedders. \n\n2) Concatenating the embedded support points g'(x) into a single vector and running a bidirectional LSTM over the concatenation, which results in a representation g(x) of each input that incorporates information from g'(x_i) for all other x_i (albeit in a way that imposes a concatenation ordering that may not correspond to a meaningful order) \n\n3) Calculating f(x) of your embedding point by using an attention mechanism to combine f'(x) with the contextualized embeddings g(x) \n\nThe authors of the current paper argue that this approach is suboptimal because of the artificially imposed ordering, and because it calculated g(x) prior to f(x) using asymmetrical model structures (though it's not super clear why this latter point is a problem). Instead, they propose a somewhat elaborate and difficult-to-follow attention based mechanism. As best as I can understand, this is what they're suggesting: \n\nhttps://i.imgur.com/4DLWh8H.png\n\n1) Update the query embedding f(x) by calculating an attention distribution over the vector current embeddings of support set points (here referred to as bolded <r>), pooling downward to a single aggregate embedding vector r, and then using a LSTM that takes in that aggregate vector and the prior update to generate a new update. This update, dz, is added to the existing query embedding estimate to get a new one \n\n2) Update the vector of support set embeddings by iteratively calculating an attention mapping between the vector of current support set embeddings and the original features g'(S), and using that attention mapping to create a new <r>, which, similar to the above, is fed into an LSTM to calculate the next update. \n\nSince the model is evaluated on molecular tasks, all of the embedding functions are structured as graph convolutions. \n\nOther than the obvious fact that attention is a great way of aggregating information in an order-independent way, the authors give disappointingly little justification of why they would expect their method to work meaningfully better than past approaches. \n\nEmpirically, they do find that it performs slightly better than prior contextualized matching networks on held out tasks of predicting toxicity and side effects with only a small number from the held out task. However, neither this paper's new method nor previous one-shot learning work is able to perform very well on the challenging MUV dataset, where held-out binding tasks involve structurally dissimilar molecules from those seen during training, suggesting that whatever generalization this method is able to achieve doesn't quite rise to the task of making inferences based on molecules with different structures.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1611.03199"
    },
    "237": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/WuRFGGPLP17",
        "transcript": "This is a paper released by the creators of the DeepChem library/framework, explaining the efforts they've put into facilitating straightforward and reproducible testing of new methods. They advocate for consistency between tests on three main axes. \n\n1. On the most basic level, that methods evaluate on the same datasets\n2. That they use canonical train/test splits\n3. That they use canonical metrics. \n\nTo that end, they've integrated a framework they call \"MoleculeNet\" into DeepChem, containing standardized interfaces to datasets, metrics, and test sets.\n\n**Datasets** \n\nMoleculeNet contains 17 different datasets, where \"dataset\" here just means a collection of data labeled for a certain task or set of tasks. The tasks fall into one of four groups: \n\n- quantum mechanical prediction (atomization energy, spectra)\n- prediction of properties of physical chemistry (solubility, lipophilicity)\n- prediction of biophysical interactions like bonding affinity\n- prediction of human-level physiological properties (toxicity, side effects, whether it passes the blood brain barrier)\n\nAn interesting thing to note here is that only some datasets contain 3D orientations of molecules, because spatial orientations are properties of *a given conformation* of a molecule, and while some output measures (like binding geometry) depend on 3D arrangement, others (like solubility) don't. \n\n**Metrics**\n\nThe metrics chosen were pretty straightforward - Root Mean Squared Error or Absolute Error for continuous prediction tasks, and ROC-AUC or PRC-AUC for prediction ones. The only notable nuance was that the paper argued for PRC-AUC as the standard metric for datasets with a low number of positives, since that metric is the strictest on false positives. \n\n**Test/Train Split** \n\nMost of these were fairly normal - random split and time-based split - but I found the idea of a scaffold split (where you cluster molecules by similarity, and assign each cluster to either train or test), interesting. The idea here is that if molecules are similar enough to one another, seeing one of a pair during training might be comparable to seeing an actual shared example between training and test, and have the same propensity for overconfident results. \n\n**Models** \n\nDeepChem has put together implementations of a number of standard machine learning methods (SVM, Random Forest, XGBoost, Logistic Regression) on molecular features, as well as a number of molecule-specific graph-structured methods. At a high level, these are: \nhttps://i.imgur.com/x4yutlp.png\n\n- Graph Convolutions, which update atom representations by combining transformations of the features of bonded neighbor atoms\n- DAGs, which create an \"atom-centric\" graph for each atom in the molecule and \"pull\" information inwards from farther away nodes (for the record, I don't fully follow how this one works, since I haven't read the underlying paper)\n- Weave Model, which maintains both atom representations and pair representations between all pairs of atoms, not just ones bonded to one another, and updates each in a cross-cutting way: updating an atom representation from all of its pairs (as well as itself), and then updating a pair representation from the atoms in its pairing (as well as itself). This has the benefit of making information from far-away molecules available immediately, rather than having to propagate through a graph, but is also more computationally taxing\n- Message Passing Neural Network, which operates like Graph Convolutions except that the feature transform used to pull in information from neighboring atoms changes depending on the type of the bond between atoms\n- Deep Tensor Neural Network - Instead of bonds, this approach represents atoms in 3D space, and pulls in information based on other atoms nearby in spatial distance\n\n**Results** \n\nAs part of creating its benchmark, MoleculeNet also tested its implementations of its models on all its datasets. It's interesting the extent to which the results form a narrative, in terms of which tasks benefit most from flexible structure-based methods (like graph approaches) vs hand-engineered features. \n\nhttps://i.imgur.com/dCAdJac.png\n\nPredictions of quantum mechanical properties and properties of physical chemistry do consistently better with graph-based methods, potentially suggesting that the features we've thought to engineer aren't in line with useful features for those tasks. By contrast, on biophysical tasks, hand-engineered features combined with traditional machine learning mostly comes out on top, a fact I found a bit surprising, given the extent to which I'd read about deep learning methods claiming strong results on prediction of things like binding affinity. This was a useful pointer of things I should do some more work to resolve clarity on. And, when it came to physiological properties like toxicity and side effects, results are pretty mixed between graph-based and traditional methods.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.00564"
    },
    "238": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/KearnesMBPR16",
        "transcript": "This paper was published after the 2015 Duvenaud et al paper proposing a differentiable alternative to circular fingerprints of molecules: substituting out exact-match random hash functions to identify molecular structures with learned convolutional-esque kernels. As far as I can tell, the Duvenaud paper was the first to propose something we might today recognize as graph convolutions on atoms. I hoped this paper would build on that one, but it seems to be coming from a conceptually different direction, and it seems like it was more or less contemporaneous, for all that it was released later. \n\nThis paper introduces a structure that allows for more explicit message passing along bonds, by calculating atom features as a function of their incoming bonds, and then bond features as a function of their constituent atoms, and iterating this procedure, so information from an atom can be passed into a bond, then, on the next iteration, pulled in by another atom on the other end of that bond, and then pulled into that atom's bonds, and so forth. This has the effect of, similar to a convolutional or recurrent network, creating representations for each atom in the molecular graph that are informed by context elsewhere in the graph, to different degrees depending on distance from that atom. \n\nMore specifically, it defines: \n\n- A function mapping from a prior layer atom representation to a subsequent layer atom representation, taking into account only information from that atom (Atom to Atom)\n- A function mapping from a prior layer bond representation (indexed by the two atoms on either side of the bond), taking into account only information from that bond at the prior layer (Bond to Bond)\n- A function creating a bond representation by applying a shared function to the atoms at either end of it, and then combining those representations with an aggregator function (Atoms to Bond)\n- A function creating an atom representation by applying a shared function all the bonds that atom is a part of, and then combining those results with an aggregator function (Bonds to Atom)\n\nAt the top of this set of layers, when each atom has had information diffused into it by other parts of the graph, depending on the network depth, the authors aggregate the per-atom representations into histograms (basically, instead of summing or max-pooling feature-wise, creating course distributions of each feature), and use that for supervised tasks. \n\nOne frustration I had with this paper is that it doesn't do a great job of highlighting its differences with and advantages over prior work; in particular, I think it doesn't do a very good job arguing that its performance is superior to the earlier Duvenaud work. That said, for all that the presentation wasn't ideal, the idea of message-passing is an important one in graph convolutions, and will end up becoming more standard in later works.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.00856"
    },
    "239": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/DuvenaudMAGHAA15",
        "transcript": "If you read modern (that is, 2018-2020) papers using deep learning on molecular inputs, almost all of them use some variant of graph convolution. So, I decided to go back through the citation chain and read the earliest papers that thought to apply this technique to molecules, to get an idea of lineage of the technique within this domain. \n\nThis 2015 paper, by Duvenaud et al, is the earliest one I can find. It focuses the entire paper on comparing differentiable, message-passing networks to the state of the art standard at the time, circular fingerprints (more on that in a bit). I really appreciated this approach, which, rather than trying to claim an unrealistic level of novelty, goes into detail on the prior approach, and carves out specific areas of difference. At a high level, the authors' claim is: our model is, in its simplest case, a more flexible and super version of existing work. The unspoken corollary, which ended up being proven true, is that the flexibility of the neural network structure makes it easy to go beyond this initial level of simplicity. \n\nCircular Fingerprinting (or, more properly, Extended-Connectivity Circular Fingerprints), is a fascinating algorithm that captures many of the elements of convolution: shared weights, a hierarchy of kernels that match patterns at different scales, and a clever way of aggregating information across an arbitrary number of input nodes. Mechanistically, Circular Fingerprints work by: \n1) Taking each atom, and creating a concatenated vector of its basic features, along with the basic features of each atom it's bonded to (with bonded neighbors quasi-randomly)\n\n2) Calculating next-level features by applying some number of hash functions (roughly equivalent to convolutional kernels) to the neighborhood feature vector at the lower level to produce an integer \n\n3) For each feature, setting the value of the fingerprint vector to 1 at the index implied by the integer in step (2) \n\n4) Iterating this process at progressively higher layers, using the hash \n\nThe effect of this is to assign each index of the vector to an binary feature (modulo hash collisions), where that feature is activated if an exact match is found to a structure within a given atom.  Its main downside is that (a) its \"kernel\" equivalents are fixed and not trainable, since they're just random hashes, and (b) its features represent *exact* matches to lower-level feature patterns, which means you can't have one feature activated to different degrees by variations on a pattern it's identifying.\n\nhttps://i.imgur.com/V8FpfVE.png\n\nDuvenaud et al present their alternative in terms of keeping a similar structure, but swapping out fixed and binary components for trainable (because differentiable) and continuous ones. Instead of concatenating a random sorting of atom neighbors to enforce invariance to sorting, they simply sum feature vectors across neighbors, which is also an order-invariantoperation. Instead of applying hash functions, they apply parametrized kernel functions, with the same parameters used across all aggregated neighborhood vectors . This will no longer look for exact matches, but will activate to the extent a structure within an atom matches against a kernel pattern. Then, these features are put into a softmax, which instead setting an index of a vector to a sharp one value, activates different feature indices in the final vector to differing degrees. The final fingerprint is simply the sum of these softmax feature activations for each atom. \n\nThe authors do a few tests to confirm their substitution is working well, including starting out with a random network (to better approximate the random hash functions), comparing distances between atoms according to either the circular or neural footprint (which had a high correlation), and confirming that that performs similarly to circular fingerprints on a set of supervised learning tasks on modules. When they trained weights to be better than random on three such supervised tasks, they found that their model was comparable or better than circular fingerprints on all three (to break that down: it was basically equivalent on one, and notably better on the other two, according to mean squared error) \n\nThis really is the simplest possible version of a message-passing or graph convolutional network (it doesn't use edge features, it doesn't calculate features of a neighbor-connection according to the features of each node, etc), but it's really satisfying to see it laid out as a next-step alternative that offered value just by stepping away from exact-match feature dynamics and non-random functions, even without all the sophisticated additions that would later be added to such models.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1509.09292"
    },
    "240": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1608.04844",
        "transcript": "My objective in reading this paper was to gain another perspective on, and thus a more well-grounded view of, machine learning scoring functions for docking-based prediction of ligand/protein binding affinity. As quick background context, these models are useful because many therapeutic compounds act by binding to a target protein, and it can be valuable to prioritize doing wet lab testing on compounds that are predicted to have a stronger binding affinity. Docking systems work by predicting the pose in which a compound (or ligand) would bind to a protein, and then scoring prospective poses based on how likely such a pose would be to have high binding affinity. It's important to note that there are two predictive components in such a pipeline, and thus two sources of potential error: the searching over possible binding poses, done by physics-based systems, and scoring of the affinity of a given pose, assuming that were actually the correct one. Therefore, in the second kind of modeling, which this paper focuses on, you take in features *of a particular binding pose*, which includes information like which atoms of the compound are nearby to which atoms of the protein. \n\nThe actual neural network structure used here was admittedly a bit underwhelming (though, to be fair, many of the ideas it seems to be gesturing at wouldn't be properly formalized until Graph Convolutional Networks came around). I'll describe the network mechanically first, and then offer some commentary on the design choices. \nhttps://i.imgur.com/w9wKS10.png\n\n1. For each atom (a) in the compound, a set of neighborhood features are defined. The neighborhood is based on two hyperparameters, one for \"how many atoms from the protein should be included,\" and one for \"how many atoms from the compound should be included\". In both cases, you start by adding the closest atom from either the compound or protein, and as hyperparameter values of each increase, you add in farther-away atoms. The neighborhood features here are (i) What are the types of the atoms? (ii) What are the partial charges of the atoms? (iii) How far are the atoms from the reference atom? (iiii) What amino acid within the protein do the protein atoms come? \n2. All of these features are turned into embeddings. Yes, all of them, even the ones (distance and charge) that are continuous values. Coming from a machine learning perspective, this is... pretty weird as a design choice. The authors straight-up discretize the distance values, and then use those as discrete values for the purpose of looking up embeddings. (So, you'd have one embedding vector for distance (0.25-0.5, and a different one for 0.0-0.25, say). \n3. The embeddings are concatenated together into a single \"atom neighborhood vector\" based on a predetermined ordering of the neighbor atoms and their property vectors. We now have one atom neighborhood vector for each atom in the compound. \n4. The authors then do what they call a convolution over the atom neighborhood vectors. But it doesn't act like a normal convolution in the sense of mixing information from nearby regions of atom space. It just is basically a fully connected layer that's applied to atom neighborhood vector separately, but with shared weights, so the same layer is applied to each neighborhood vector. They then do a feature-wise max pool across the layer-transformed version of neighborhood vectors, getting you one vector for the full compound \n5. This single vector is then put into a softmax, which predicts whether this ligand (in in this particular pose) will have strong binding with the protein \n\nSome thoughts on what's going on here. First, I really don't have a good explanation for why they'd have needed to embed a discretized version of the continuous variables, and since they don't do an ablation test of that design choice, it's hard to know if it mattered. Second, it's interesting to see, in their \"convolution\" (which I think is more accurately described as a Siamese Network, since it's only convolution-like insofar as there are shared weights), the beginning intuitions of what would become Graph Convolutions. The authors knew that they needed methods to aggregate information from arbitrary numbers of atoms, and also that they need should learn representations that have visibility onto neighborhoods of atoms, rather than single ones, but they do so in an entirely hand-engineered way: manually specifying a fixed neighborhood and pulling in information from all those neighbors equally, in a big concatenated vector. By contrast, when Graph Convolutions come along, they act by defining a \"message-passing\" function for features to aggregate across graph edges (here: molecular bonds or binaries on being \"near enough\" to another atom), which similarly allows information to be combined across neighborhoods. And, then, the 'convolution' is basically just a simple aggregation: necessary because there's no canonical ordering of elements within a graph, so you need an order-agnostic aggregation like a sum or max pool. \n\nThe authors find that their method is able to improve on the hand-designed scoring functions within the docking programs. However, they also find (similar to another paper I read recently) that their model is able to do quite well without even considering structural relationships of the binding pose with the protein, which suggests the dataset (DUD - a dataset of 40 proteins with ~4K correctly binding ligands, and ~35K ligands paired with proteins they don't bind to) and problem given to the model is too easy.  It's also hard to tell how I should consider AUCs within this problem - it's one thing to be better than an existing method, but how much value do you get from a given unit of AUC improvement, when it comes to actually meaningfully reducing wetlab time used on testing compounds?  \n\nI don't know that there's much to take away from this paper in terms of useful techniques, but it is interesting to see the evolution of ideas that would later be more cleanly formalized in other works.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1608.04844v2"
    },
    "241": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1910.02845",
        "transcript": "This paper focuses on the application of deep learning to the docking problem within rational drug design. The overall objective of drug design or discovery is to build predictive models of how well a candidate compound (or \"ligand\") will bind with a target protein, to help inform the decision of what compounds are promising enough to be worth testing in a wet lab. Protein binding prediction is important because many small-molecule drugs, which are designed to be small enough to get through cell membranes, act by binding to a specific protein within a disease pathway, and thus blocking that protein's mechanism. \n\nThe formulation of the docking problem, as best I understand it, is: \n\n1. A \"docking program,\" which is generally some model based on physical and chemical interactions, takes in a (ligand, target protein) pair, searches over a space of ways the ligand could orient itself within the binding pocket of the protein (which way is it facing, where is it twisted, where does it interact with the protein, etc), and ranks them according to plausibility \n2. A scoring function takes in the binding poses (otherwise known as binding modes) ranked the highest, and tries to predict the affinity strength of the resulting bond, or the binary of whether a bond is \"active\". \n\nThe goal of this paper was to interpose modern machine learning into the second step, as alternative scoring functions to be applied after the pose generation . Given the complex data structure that is a highly-ranked binding pose, the hope was that deep learning would facilitate learning from such a complicated raw data structure, rather than requiring hand-summarized features. They also tested a similar model structure on the problem of predicting whether a highly ranked binding pose was actually the empirically correct one, as determined by some epsilon ball around the spatial coordinates of the true binding pose. Both of these were binary tasks, which I understand to be \n\n1. Does this ranked binding pose in this protein have sufficiently high binding affinity to be \"active\"? This is known as the \"virtual screening\" task, because it's the relevant task if you want to screen compounds in silico, or virtually, before doing wet lab testing. \n2. Is this ranked binding pose the one that would actually be empirically observed? This is known as the \"binding mode prediction\" task\n\nThe goal of this second task was to better understand biases the researchers suspected existed in the underlying dataset, which I'll explain later in this post. \n\nThe researchers used a graph convolution architecture. At a (very) high level, graph convolution works in a way similar to normal convolution - in that it captures hierarchies of local patterns, in ways that gradually expand to have visibility over larger areas of the input data. The distinction is that normal convolution defines kernels over a fixed set of nearby spatial coordinates, in a context where direction (the pixel on top vs the pixel on bottom, etc) is meaningful, because photos have meaningful direction and orientation. By contrast, in a graph, there is no \"up\" or \"down\", and a given node doesn't have a fixed number of neighbors (whereas a fixed pixel in 2D space does), so neighbor-summarization kernels have to be defined in ways that allow you to aggregate information from 1) an arbitrary number of neighbors, in 2) a manner that is agnostic to orientation. Graph convolutions are useful in this problem because both the summary of the ligand itself, and the summary of the interaction of the posed ligand with the protein, can be summarized in terms of graphs of chemical bonds or interaction sites. \n\nUsing this as an architectural foundation, the authors test both solo versions and ensembles of networks: \n\nhttps://i.imgur.com/Oc2LACW.png\n\n1. \"L\" - A network that uses graph convolution to summarize the ligand itself, with no reference to the protein it's being tested for binding affinity with \n2. \"LP\" - A network that uses graph convolution on the interaction points between the ligand and protein under the binding pose currently being scored or predicted\n3. \"R\" - A simple network that takes into account the rank assigned to the binding pose by the original docking program (generally used in combination with one of the above). \n\nThe authors came to a few interesting findings by trying different combinations of the above model modules. First, they found evidence supporting an earlier claim that, in the dataset being used for training, there was a bias in the positive and negative samples chosen such that you could predict activity of a ligand/protein binding using *ligand information alone.* This shouldn't be possible if we were sampling in an unbiased way over possible ligand/protein pairs, since even ligands that are quite effective with one protein will fail to bind with another, and it shouldn't be informationally possible to distinguish the two cases without protein information. Furthermore, a random forest on hand-designed features was able to perform comparably to deep learning, suggesting that only simple features are necessary to perform the task on this (bias and thus over-simplified)\n\nSpecifically, they found that L+LP models did no better than models of L alone on the virtual screening task. However, the binding mode prediction task offered an interesting contrast, in that, on this task, it's impossible to predict the output from ligand information alone, because by construction each ligand will have some set of binding modes that are not the empirically correct one, and one that is, and you can't distinguish between these based on ligand information alone, without looking at the actual protein relationship under consideration. In this case, the LP network did quite well, suggesting that deep learning is able to learn from ligand-protein interactions when it's incentivized to do so. Interestingly, the authors were only able to improve on the baseline model by incorporating the rank output by the original docking program, which you can think of an ensemble of sorts between the docking program and the machine learning model. \n\nOverall, the authors' takeaways from this paper were that (1) we need to be more careful about constructing datasets, so as to not leak information through biases, and (2) that graph convolutional models are able to perform well, but (3) seem to be capturing different things than physics-based models, since ensembling the two together provides marginal value.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1910.02845v1"
    },
    "242": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-05243",
        "transcript": "This paper is a bit provocative (especially in the light of the recent DeepMind MuZero paper), and poses some interesting questions about the value of model-based planning. I'm not sure I agree with the overall argument it's making, but I think the experience of reading it made me hone my intuitions around why and when model-based planning should be useful. \n\nThe overall argument of the paper is: rather than learning a dynamics model of the environment and then using that model to plan and learn a value/policy function from, we could instead just keep a large replay buffer of actual past transitions, and use that in lieu of model-sampled transitions to further update our reward estimators without having to acquire more actual experience. In this paper's framing, the central value of having a learned model is this ability to update our policy without needing more actual experience, and it argues that actual real transitions from the environment are more reliable and less likely to diverge than transitions from a learned parametric model. It basically sees a big buffer of transitions as an empirical environment model that it can sample from, in a roughly equivalent way to being able to sample transitions from a learnt model. \n\nAn obvious counter-argument to this is the value of models in being able to simulate particular arbitrary trajectories (for example, potential actions you could take from your current point, as is needed for Monte Carlo Tree Search). Simply keeping around a big stock of historical transitions doesn't serve the use case of being able to get a probable next state *for a particular transition*, both because we might not have that state in our data, and because we don't have any way, just given a replay buffer, of knowing that an available state comes after an action if we haven't seen that exact combination before. (And, even if we had, we'd have to have some indexing/lookup mechanism atop the data). I didn't feel like the paper's response to this was all that convincing. It basically just argues that planning with model transitions can theoretically diverge (though acknowledges it empirically often doesn't), and that it's dangerous to update off of \"fictional\" modeled transitions that aren't grounded in real data. While it's obviously definitionally true that model transitions are in some sense fictional, that's just the basic trade-off of how modeling works: some ability to extrapolate, but a realization that there's a risk you extrapolate poorly. \n\nhttps://i.imgur.com/8jp22M3.png\n\nThe paper's empirical contribution to its argument was to argue that in a low-data setting, model-free RL (in the form of the \"everything but the kitchen sink\" Rainbow RL algorithm) with experience replay can outperform a model-based SimPLe system on Atari. This strikes me as fairly weak support for the paper's overall claim, especially since historically Atari has been difficult to learn good models of when they're learnt in actual-observation pixel space. Nonetheless, I think this push against the utility of model-based learning is a useful thing to consider if you do think models are useful, because it will help clarify the reasons why you think that's the case.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.05243"
    },
    "243": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1905-12506",
        "transcript": "Arguably, the central achievement of the deep learning era is multi-layer neural networks' ability to learn useful intermediate feature representations using a supervised learning signal. In a supervised task, it's easy to define what makes a feature representation useful: the fact that's easier for a subsequent layer to use to make the final class prediction. When we want to learn features in an unsupervised way, things get a bit trickier. There's the obvious problem of what kinds of problem structures and architectures work to extract representations at all. But there's also a deeper problem: when we ask for a good feature representation, outside of the context of any given task, what are we asking for? Are there some inherent aspects of a representation that can be analyzed without ground truth labels to tell you whether the representations you've learned are good are not? \n\nThe notion of \"disentangled\" features is one answer to that question: it suggests that a representation is good when the underlying \"factors of variation\" (things that are independently variable in the underlying generative process of the data) are captured in independent dimensions of the feature representation. That is, if your representation is a ten-dimensional vector, and it just so happens that there are ten independent factors along which datapoints differ (color, shape, rotation, etc), you'd ideally want each dimension to correspond to each factor. \n\nThis criteria has an elegance to it, and it's previously been shown useful in predicting when the representations learned by a model will be useful in predicting the values of the factors of variation. This paper goes one step further, and tests the value representations for solving a visual reasoning task that involves the factors of variation, but doesn't just involve predicting them. In particular, the authors use learned representations to solve a task patterned on a human IQ test, where some factors stay fixed across a row in a grid, and some vary, and the model needs to generate the image that \"fits the pattern\". \n\nhttps://i.imgur.com/O1aZzcN.png\n\nTo test the value of disentanglement, they looked at a few canonical metrics of disentanglement, including scores that represent \"how many factors are captured in each dimension\" and \"how many dimensions is a factor spread across\". They measured the correlation of these metrics with task performance, and compared that with the correlation between simple autoencoder reconstruction error and performance.\n\nThey found that at early stages of training on top of the representations, the disentanglement metrics were more predictive of performance than reconstruction accuracy. This distinction went away as the model learning on top of the representations had more time to train. It makes reasonable sense that you'd mostly see value for disentangled features in a low-data regime, since after long enough the fine-tuning network can learn its own features regardless. But, this paper does appear to contribute to evidence that disentangled features are predictive of task performance, at least when that task directly involves manipulation of specific, known, underlying factors of variation.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.12506"
    },
    "244": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-02768",
        "transcript": "Summary: An odd thing about machine learning these days is how far you can get in a line of research while only ever testing your method on image classification and image datasets in general. This leads one occasionally to wonder whether a given phenomenon or advance is a discovery of the field generally, or whether it's just a fact about the informatics and learning dynamics inherent in image data. \n\nThis paper, part of a set of recent papers released by Facebook centering around the Lottery Ticket Hypothesis, exists in the noble tradition of \"lets try <thing> on some non-image datasets, and see if it still works\". This can feel a bit silly in the cases where the ideas or approaches do transfer, but I think it's still an important impulse for the field to have, lest we become too captured by ImageNet and its various descendants.  \n\nThis paper test the Lottery Ticket Hypothesis - the idea that there are a small subset of weights in a trained network whose lucky initializations promoted learning, such that if you reset those weights to their initializations and train only them you get comparable or near-comparable performance to the full network - on reinforcement learning and NLP datasets. In particular, within RL, they tested on both simple continuous control (where the observation state is a vector of meaningful numbers) and Atari from pixels (where the observation is a full from-pixels image). In NLP, they trained on language modeling and translation, with both a LSTM and a Transformer respectively. (Prior work had found that Transformers didn't exhibit lottery ticket like phenomenon, but this paper found a circumstance where they appear to. )\n\nSome high level interesting results: \n\nhttps://i.imgur.com/kd03bQ4.png\n\nhttps://i.imgur.com/rZTH7FJ.png\n\n- So as to not bury the lede: by and large, \"winning\" tickets retrained at their original initializations outperform random initializations of the same size and configuration on both NLP and Reinforcement Learning problems\n- There is wide variability in how much pruning in general (a necessary prerequisite operation) impacts reinforcement learning. On some games, pruning at all crashes performance, on others, it actually improves it. This leads to some inherent variability in results\nhttps://i.imgur.com/4o71XPt.png\n- One thing that prior researchers in this area have found is that pruning weights all at once at the end of training tends to crash performance for complex models, and that in order to find pruned models that have Lottery Ticket-esque high-performing properties, you need to do \"iterative pruning\". This works by training a model for a period, then pruning some proportion of weights, then training again from the beginning, and then pruning again, and so on, until you prune down to the full percentage you want to prune. The idea is that this lets the model adapt gradually to a drop in weights, and \"train around\" the capacity reduction, rather than it just happening all at once. In this paper, the authors find that this is strongly necessary for Lottery Tickets to be found for either Transformers or many RL problems. On a surface level, this makes sense, since Reinforcement Learning is a notoriously tricky and non-stationary learning problem, and Transformers are complex models with lots of parameters, and so dramatically reducing parameters can handicap the model. A weird wrinkle, though, is that the authors find that lottery tickets found without iterative pruning actually perform worse than \"random tickets\" (i.e. initialized subnetworks with random topology and weights). This is pretty interesting, since it implies that the topology and weights you get if you prune all at once are actually counterproductive to learning. I don't have a good intuition as to why, but would love to hear if anyone does.\nhttps://i.imgur.com/9LnJe6j.png\n- For the Transformer specifically, there was an interesting divergence in the impact of weight pruning between the weights of the embedding matrix and the weights of the rest of the network machinery. If you include embeddings in the set of weights being pruned, there's essentially no difference in performance between random and winning tickets, whereas if you exclude them, winning tickets exhibit the more typical pattern of outperforming random ones. This implies that whatever phenomenon that makes winning tickets better is more strongly (or perhaps only) present in weights for feature calculation on top of embeddings, and not very present for the embeddings themselves",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.02768"
    },
    "245": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-02773",
        "transcript": "In my view, the Lottery Ticket Hypothesis is one of the weirder and more mysterious phenomena of the last few years of Machine Learning. We've known for awhile that we can take trained networks and prune them down to a small fraction of their weights (keeping those weights with the highest magnitudes) and maintain test performance using only those learned weights. That seemed somewhat surprising, in that there were a lot of weights that weren't actually necessary to encoding the learned function, but, the thinking went, possibly having many times more weights than that was helpful for training, even if not necessary once a model is trained. The authors of the original Lottery Ticket paper came to the surprising realization that they could take the weights that were pruned to exist in the final network, re-initialize them (and only them) to the values they had during initial training, and perform almost as well as the final pruned model that had all weights active during training. And, performance using the specific weights and their particular initialization values is much higher than training a comparable topology of weights with random initial values. \n\nThis paper out of Facebook AI adds another fascinating experiment to the pile of off evidence around lottery tickets: they test whether lottery tickets transfer *between datasets*, and they find that they often do (at least when the dataset on which the lottery ticket is found is more complex (in terms of in size, input complexity, or number of classes) than the dataset the ticket is being transferred to. Even more interestingly, they find that for sufficiently simple datasets, the \"ticket\" initialization pattern learned on a more complex dataset actually does *better* than ones learned on the simple dataset itself. They also find that tickets by and large transfer between SGD and Adam, so whatever kind of inductive bias or value they provide is general across optimizers in addition to at least partially general across datasets. \n\nhttps://i.imgur.com/H0aPjRN.png\n\nI find this result fun to think about through a few frames. The first is to remember that figuring out heuristics for initializing networks (as a function of their topology) was an important step in getting them to train at all, so while this result may at first seem strange and arcane, in that context it feels less surprising that there are still-better initialization heuristics out there, possibly with some kind of interesting theoretical justification to them, that humans simply haven't been clever enough to formalize yet, and have only discovered empirically through methods like this. \n\nThis result is also interesting in terms of transfer: we've known for awhile that the representations learned on more complex datasets can convey general information back to smaller ones, but it's less easy to think about what information is conveyed by the topology and connectivity of a network. This paper suggests that the information is there, and has prompted me to think more about the slightly mind-bending question of how training models could lead to information compressed in this form, and how this information could be better understood.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.02773"
    },
    "246": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-00446",
        "transcript": "VQ-VAE is a Variational AutoEncoder that uses as its information bottleneck a discrete set of codes, rather than a continuous vector. That is: the encoder creates a downsampled spatial representation of the image, where in each grid cell of the downsampled image, the cell is represented by a vector. But, before that vector is passed to the decoder, it's discretized, by (effectively) clustering the vectors the network has historically seen, and substituting each vector with the center of the vector it's closest to. This has the effect of reducing the capacity of your information bottleneck, but without just pushing your encoded representation closer to an uninformed prior. (If you're wondering how the gradient survives this very much not continuous operation, the answer is: we just pretend that operation didn't exist, and imagine that the encoder produced the cluster-center \"codebook\" vector that the decoder sees). \n\nThe part of the model that got a (small) upgrade in this paper is the prior distribution model that's learned on top of these latent representations. The goal of this prior is to be able to just sample images, unprompted, from the distribution of latent codes. Once we have a trained decoder, if we give it a grid of such codes, it can produce an image. But these codes aren't one-per-image, but rather a grid of many codes representing features in different part of the image. In order to generate a set of codes corresponding to a reasonable image, we can either generate them all at once, or else (as this paper does) use an autoregressive approach, where some parts of the code grid are generated, and then subsequent ones conditioned on those. In the original version of the paper, the autoregressive model used was a PixelCNN (don't have the space to fully explain that here, but, at a high level: a model that uses convolutions over previously generated regions to generate a new region). In this paper, the authors took inspiration from the huge rise of self-attention in recent years, and swapped that operation in in lieu of the convolutions. Self-attention has the nice benefit that you can easily have a global receptive range (each region being generated can see all other regions) which you'd otherwise need multiple layers of convolutions to accomplish. \n\nIn addition, the authors add an additional layer of granularity: generating both a 32x32 and 64x64 grid, and using both to generate the decoded reconstruction. They argue that this allows one representation to focus on more global details, and the other on more precise ones. \n\nhttps://i.imgur.com/zD78Pp4.png\n\nThe final result is the ability to generate quite realistic looking images, that at least are being claimed to be more diverse than those generated by GANs (examples above). I'm always a bit cautious of claims of better performance in the image-generation area, because it's all squinting at pixels and making up somewhat-reasonable but still arbitrary metrics. That said, it seems interesting and useful to be aware of the current relative capabilities of two of the main forms of generative modeling, and so I'd recommend this paper on that front, even if it's hard for me personally to confidently assess the improvements on prior art.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.00446"
    },
    "247": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1904-00760",
        "transcript": "When talking about modern machine learning, particularly on images, it can feel like deep neural networks are a world unto themselves when it comes to complexity. On one hand, there are straightforward things like hand-designed features and linear classifiers, and then on the other, there are these deep, heavily-interacting networks that dazzle us with their performance but seem almost unavoidably difficult to hold in our heads or interpret. This paper, from ICLR 2019 earlier this year, investigates another point along this trade-off curve of complexity: a model that uses deep layers of convolutions, but limits the receptive field of those convolutions so that each feature is calculated using only a small spatial area of the image. \n\nhttps://i.imgur.com/NR0vFbN.png\n\nThis approach, termed BagNet, essentially predicts class logits off of a small area of the image, without using information from anywhere else. Then, to aggregate the local predictions, a few simple and linear steps are performed: the predictions from each spatial area are averaged together into one vector containing the \"aggregate information\" for each class, and then that class information vector is passed into a linear (non-interacting!) model to predict final class probabilities. \n\nThis is quite nice for interpretability, because you can directly identify the areas of the image that contributed evidence to the prediction, and you can know that the impact of those areas wasn't in fact amplified by feature values elsewhere, because there are no interaction effects outside of these small regions\n\nNow, it's fairly obvious that you're not going to get any state-of-the-art results off of this: the entire point is to handicap a network in ways believed to make it more interpretable. So the interesting question is instead what degree of performance loss comes from such a (fairly drastic) limitation of model capacity and receptive field? And the answer of the paper is: less than you might think. (Or, at least, less than *they* think you think). If you only use features calculated from 33x33 pixel chunks of image net, and aggregate their evidence together in a purely linear way, you can get to 87.6% top-5 image accuracy on ImageNet, which is about where we were with AlexNet in 2012. \n\nThe authors also do some comparisons of their network to more common neural networks, to try to argue that even fully nonlinear neural nets don't use spatial information very much in their predictions. One way they did this was by masking different areas of the image, and comparing the effect of masking each individually to the effect of masking all areas together. In a purely linear model like BagNet, where the effects of different areas are just aggregated together, these would sum together perfectly, and the performance loss of all areas at once would be equal to the sum of each individually. To measure \"effective spatial linearity\" of each network, they measured the correlation between the sum of the individual effects and the joint effect. For VGG, they found a correlation of 0.75 here (compared to 1.0 for BagNet), which they use to argue that VGG doesn't use very much spatial information. I found this result hard to really get a grounding on, since I don't have a good intuitive grasp for what differences in this correlation value would mean. Is a difference of 0.25 a small difference, or a dramatic one? \n\nhttps://i.imgur.com/hA58AKM.png\n\nThat aside, I found this paper interesting, and I'm quite pleased it was written. On one hand, you can say: well, obviously, we've done a lot of work in 7 years to build ResNet and DenseNet and whatnot, so of course if you apply those more advanced architectures, even on a small region of image space, you'll get good performance. That said, I still think this is an interesting finding, because it helps us understand how much of the added value in recent research requires a high (and uninterpretable)  interaction complexity, and what proportion of the overall performance can be achieved with a simpler-to-understand model. Machine learning is used in a lot of settings, and it always practically exists on a trade-off curve, where performance is important, but it's often worth trading off performance to do better on other considerations, and this paper does a good job of illustrating that trade-off curve more fully.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1904.00760"
    },
    "248": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1911.08265",
        "transcript": "The successes of deep learning on complex strategic games like Chess and Go have been largely driven by the ability to do tree search: that is, simulating sequences of actions in the environment, and then training policy and value functions to more speedily approximate the results that more exhaustive search reveals. However, this relies on having a good simulator that can predict the next state of the world, given your action. In some games, with straightforward rules, this is easy to explicitly code, but in many RL tasks like Atari, and in many contexts in the real world, having a good model of how the world responds to your actions is in fact a major part of the difficulty of RL. \n\nA response to this within the literature has been systems that learn models of the world from trajectories, and then use those models to do this kind of simulated planning. Historically these have been done by designing models that predict the next observation, given past observations and a passed-in action. This lets you \"roll out\" observations from actions in a way similar to how a simulator could. However, in high-dimensional observation spaces it takes a lot of model capacity to accurately model the full observation, and many parts of a given observation space will often be irrelevant. \n\nhttps://i.imgur.com/wKK8cnj.png\n\nTo address this difficulty, the MuZero architecture uses an approach from Value Prediction Networks, and learns an internal model that can predict transitions between abstract states (which don't need to match the actual observation state of the world) and then predict a policy, value, and next-step reward from the abstract state. So, we can plan in latent space, by simulating transitions from state to state through actions, and the training signal for that space representation and transition model comes from being able to accurately predict the reward, the empirical future value at a state (discovered through Monte Carlo rollouts) and the policy action that the rollout search would have taken at that point. If two observations are identical in terms of their implications for these quantities, the transition model doesn't need to differentiate them, making it more straightforward to learn. (Apologies for the long caption in above screenshot; I feel like it's quite useful to gain intuition, especially if you're less recently familiar with the MCTS deep learning architectures DeepMind typically uses) \n\nhttps://i.imgur.com/4nepG6o.png\n\nThe most impressive empirical aspect of this paper is the fact that it claims (from what I can tell credibly) to be able to perform as well as planning algorithms with access to a real simulator in games like Chess and Go, and as well as model-free models in games like Atari where MFRL has typically been the state of the art (because world models have been difficult to learn). I feel like I've read a lot recently that suggests to me that the distinction between model-free and model-based RL is becoming increasingly blurred, and I'm really curious to see how that trajectory evolves in future.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1911.08265"
    },
    "249": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/OhSL17",
        "transcript": "Recently, DeepMind released a new paper showing strong performance on board game tasks using a mechanism similar to the Value Prediction Network one in this paper, which inspired me to go back and get a grounding in this earlier work. \n\nA goal of this paper is to design a model-based RL approach that can scale to complex environment spaces, but can still be used to run simulations and do explicit planning. Traditional, model-based RL has worked by learning a dynamics model of the environment - predicting the next observation state given the current one and an action, and then using that model of the world to learn values and plan with. In addition to the advantages of explicit planning, a hope is that model-based systems generalize better to new environments, because they predict one-step changes in local dynamics in a way that can be more easily separated from long-term dynamics or reward patterns. \n\nHowever, a downside of MBRL is that it can be hard to train, especially when your observation space is high-dimensional, and learning a straight model of your environment will lead to you learning details that aren't actually unimportant for planning or creating policies. \n\nThe synthesis proposed by this paper is the Value Prediction Network. Rather than predicting observed state at the next step, it learns a transition model in latent space, and then learns to predict next-step reward and future value from that latent space vector. Because it learns to encode latent-space state from observations, and also learns a transition model from one latent state to another, the model can be used for planning, by simulating multiple transitions between latent state. However, unlike a normal dynamics model, whose training signal comes from a loss against observational prediction, the signal for training both latent \u2192 reward/value/discount predictions, and latent \u2192 latent transitions comes from using this pipeline to predict reward values. This means that if an aspect of the environment isn't useful for predicting reward, it won't generally be encoded into latent state, meaning you don't waste model capacity predicting irrelevant detail. \n\nhttps://i.imgur.com/4bJylms.png\n\nOnce this model exists, it can be used for generating a policy through a tree-search planning approach: simulating future trajectories and aggregating the predicted reward along those trajectories, and then taking the highest-value one. \n\nThe authors find that their model is able to do better than both model-free and model-based methods on the tasks they tested on. In particular, they find that it has many of the benefits of a model that predicts full observations, but that the Value Prediction Network learns more quickly, and is more robust to stochastic environments where there's an inherent ceiling on how well a next-step observation prediction can work. \n\nMy main question coming into this paper is: how is this different from simply a value estimator like those used in DQN or A2C, and my impression is that the difference comes from this model's ability to do explicit state simulation in latent space, and then predict a value off of the *latent* state, whereas a value network predicts value from observational state.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1707.03497"
    },
    "250": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1907-00456",
        "transcript": "Given the tasks that RL is typically used to perform, it can be easy to equate the problem of reinforcement learning with \"learning dynamically, online, as you take actions in an environment\". And while this does represent most RL problems in the literature, it is possible to learn a reinforcement learning system in an off-policy way (read: trained off of data that the policy itself didn't collect), and there can be compelling reasons to prefer this approach. In this paper, which seeks to train a chatbot to learn from implicit human feedback in text interactions, the authors note prior bad experiences with Microsoft's Tay bot, and highlight the value of being able to test and validate a learned model offline, rather than have it continue to learn in a deployment setting. This problem, of learning a RL model off of pre-collected data, is known as batch RL. In this setting, the batch is collected by simply using a pretrained language model to generate interactions with a human, and then extracting reward from these interactions to train a Q learning system once the data has been collected. \n\nIf naively applied, Q learning (a good approach for off-policy problems, since it directly estimates the value of states and actions rather than of a policy) can lead to some undesirable results in a batch setting. An interesting one, that hadn't occurred to me, was the fact that Q learning translates its (state, action) reward model into a policy by taking the action associated with the highest reward. This is a generally sensible thing to do if you've been able to gather data on all or most of a state space, but it can also bias the model to taking actions that it has less data for, because high-variance estimates will tend make up a disproportionate amount of maximum values of any estimated distribution. One approach to this is to learn two separate Q functions, and take the minimum over them, and then take the max of that across actions (in this case: words in a sentence being generated). The idea here is that low-data, high-variance parts of state space might have one estimate be high, but might have the other be low, because high variance. However, it's costly to train and run two separate models. Instead, the authors here propose the simpler solution of training a single model with dropout, and using multiple \"draws\" from that model to simulate a distribution over Q value estimates. This will have a similar effect of penalizing actions whose estimate varies across different dropout masks (which can be hand-wavily thought of as different models). \n\nThe authors also add a term to their RL training that penalizes divergence from the initial language model that they used to collect the data, and also that is the initialization point for the parameters of the model. This is done via KL-divergence control: the model is penalized for outputting a distribution over words that is different in distributional-metric terms from what the language model would have output. This makes it costlier for the model to diverge from the pretrained model, and should lead to it only happening in cases of convincing high reward.\n\nOut of these two approaches, it seems like the former is more convincing to me as a general-purpose method to use in batch RL settings. The latter is definitely something I would have expected to work well (and, indeed, KL-controlled models performed much better in empirical tests in the paper!), but more simply because language modeling is hard, and I would expect it to be good to constrain a model to be close to realistic outputs, since the sentiment-based reward signal won't reward realism directly. This seems more like something generally useful for avoiding catastrophic forgetting when switching from an old task to a new one (language modeling to sentiment modeling), rather than a particularly batch-RL-centric innovation. \n\nhttps://i.imgur.com/EmInxOJ.png\n\n An interesting empirical observation of this paper is that models without language-model control end up drifting away from realism, and repeatedly exploit part of the reward function that, in addition to sentiment, gave points for asking questions. By contrast, the KL-controlled models appear to have avoided falling into this local minimum, and instead generated realistic language that was polite and empathetic. (Obviously this is still a simplified approximation of what makes a good chat bot, but it's at least a higher degree of complexity in its response to reward). \n\nOverall, I quite enjoyed this paper, both for its thoughtfulness and its clever application of engineering to use RL for a problem well outside of its more typical domain.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1907.00456"
    },
    "251": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1910-10683",
        "transcript": "At a high level, this paper is a massive (34 pgs!) and highly-resourced study of many nuanced variations of language pretraining tasks, to see which of those variants produce models that transfer the best to new tasks. As a result, it doesn't lend itself *that* well to being summarized into a central kernel of understanding. So, I'm going to do my best to pull out some high-level insights, and recommend you read the paper in more depth if you're working particularly in language pretraining and want to get the details. \n\nThe goals here are simple: create a standardized task structure and a big dataset,  so that you can use the same architecture across a wide range of objectives and subsequent transfer tasks, and thus actually compare tasks on equal footing. To that end, the authors created a huge dataset by scraping internet text, and filtering it according to a few common sense criteria. This is an important and laudable task, but not one with a ton of conceptual nuance to it. \n\nhttps://i.imgur.com/5z6bN8d.png\n\nA more interesting structural choice was to adopt a unified text to text framework for all of the tasks they might want their pretrained model to transfer to. This means that the input to the model is always a sequence of tokens, and so is the output. If the task is translation, the input sequence might be \"translate english to german: build a bed\" and the the desired output would be that sentence in German. This gets particularly interesting as a change when it comes to tasks where you're predicting relationships of words within sentences, and would typically have a categorical classification loss, which is changed here to predicting the word of the correct class. This restructuring doesn't seem to hurt performance, and has the nice side effect that you can directly use the same model as a transfer starting point for all tasks, without having to add additional layers. Some of the transfer tasks include: translation, sentiment analysis, summarization, grammatical checking of a sentence, and checking the logical relationship between claims. \n\nAll tested models followed a transformer (i.e. fully attentional) architecture. The authors tested performance along many different axes. A structural variation was the difference between an encoder-decoder architecture and a language model one. \n\nhttps://i.imgur.com/x4AOkLz.png\n\nIn both cases, you take in text and predict text, but in an encoder-decoder, you have separate models that operate on the input and output, whereas in a language model, it's all seen as part of a single continuous sequence. They also tested variations in what pretraining objective is used. The most common is simple language modeling, where you predict words in a sentence given prior or surrounding ones, but, inspired by the success of BERT, they also tried a number of denoising objectives, where an original sentence was corrupted in some way (by dropping tokens and replacing them with either masks, nothing, or random tokens,  dropping individual words vs contiguous spans of words) and then the model had to predict the actual original sentence. \n\nhttps://i.imgur.com/b5Eowl0.png\n\nFinally, they performed testing as to the effect of dataset size and number of training steps. Some interesting takeaways: \n\n- In almost all tests, the encoder-decoder architecture, where you separately build representations of your input and output text, performs better than a language model structure. This is still generally (though not as consistently) true if you halve the number of parameters in the encoder-decoder, suggesting that there's some structural advantage there beyond just additional parameter count.\n- A denoising, BERT-style objective works consistently better than a language modeling one. Within the set of different kinds of corruption, none work obviously and consistently better across tasks, though some have a particular advantage at a given task, and some are faster to train with due to different lengths of output text.\n- Unsurprisingly, more data and bigger models both lead to better performance. Somewhat interestingly, training with less data but the same number of training iterations (such that you see the same data multiple times) seems to be fine up to a point. This potentially gestures at an ability to train over a dataset a higher number of times without being as worried about overfitting.\n- Also somewhat unsurprisingly, training on a dataset that filters out HTML, random lorem-ipsum web text, and bad words performs meaningfully better than training on one that doesn't",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1910.10683"
    },
    "252": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=igl2019generalization",
        "transcript": "Coming from the perspective of the rest of machine learning, a somewhat odd thing about reinforcement learning that often goes unnoticed is the fact that, in basically all reinforcement learning, performance of an algorithm is judged by its performance on the same environment it was trained on. In the parlance of ML writ large: training on the test set. In RL, most of the focus has historically been on whether automatic systems would be able to learn a policy from the state distribution of a single environment, already a fairly hard task. But, now that RL has had more success in the single-environment case, there comes the question: how can we train reinforcement algorithms that don't just perform well on a single environment, but over a range of environments. One lens onto this question is that of meta-learning, but this paper takes a different approach, and looks at how straightforward regularization techniques pulled from the land of supervised learning can (or can't straightforwardly) be applied to reinforcement learning. \n\nIn general, the regularization techniques discussed here are all ways of reducing the capacity of the model, and preventing it from overfitting. Some ways to reduce capacity are: \n\n- Apply L2 weight penalization\n- Apply dropout, which handicaps the model by randomly zeroing out neurons\n- Use Batch Norm, which uses noisy batch statistics, and increases randomness in a way that, similar to above, deteriorates performance\n- Use an information bottleneck: similar to a VAE, this approach works by learning some compressed representation of your input, p(z|x), and then predicting your output off of that z, in a way that incentivizes your z to be informative (because you want to be able to predict y well) but also penalizes too much information being put in it (because you penalize differences between your learned p(z|x) distribution and an unconditional prior p(z) ). This pushes your model to use its conditional-on-x capacity wisely, and only learn features if they're quite valuable in predicting y\n\nHowever, the paper points out that there are some complications in straightforwardly applying these techniques to RL. The central one is the fact that in (most) RL, the distribution of transitions you train on comes from prior iterations of your policy. This means that a noisier and less competent policy will also leave you with less data to train on. Additionally, using a noisy policy can increase variance, both by making your trained policy more different than your rollout policy (in an off-policy setting) and by making your estimate of the value function higher-variance, which is problematic because that's what you're using as a target training signal in a temporal difference framework. \n\nThe paper is a bit disconnected in its connection between justification and theory, and makes two broad, mostly distinct proposals: \n\n1. The most successful (though also the one least directly justified by the earlier-discussed theoretical difficulties of applying regularization in RL) is an information bottleneck ported into a RL setting. It works almost the same as the classification-model one, except that you're trying to increase the value of your actions given compressed-from-state representation z, rather than trying to increase your ability to correctly predict y. The justification given here is that it's good to incentivize RL algorithms in particular to learn simpler, more compressible features, because they often have such poor data and also training signal earlier in training \n2. SNI (Selective Noise Injection) works by only applying stochastic aspects of regularization (sampling from z in an information bottleneck, applying different dropout masks, etc) to certain parts of the training procedure. In particular, the rollout used to collect data is non-stochastic, removing the issue of noisiness impacting the data that's collected. They then do an interesting thing where they calculate a weighted mixture of the policy update with a deterministic model, and the update with a stochastic one. The best performing of these that they tested seems to have been a 50/50 split.  This is essentially just a knob you can turn on stochasticity, to trade off between the regularizing effect of noise and the variance-increasing-negative effect of it. \n\nhttps://i.imgur.com/fi0dHgf.png\n\nhttps://i.imgur.com/LLbDaRw.png\n\nBased on my read of the experiments in the paper, the most impressive thing here is how well their information bottleneck mechanism works as a way to improve generalization, compared to both the baseline and other regularization approaches. It does look like there's some additional benefit to SNI, particularly in the CoinRun setting, but very little in the MultiRoom setting, and in general the difference is less dramatic than the difference from using the information bottleneck.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1910.12911"
    },
    "253": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1908-01517",
        "transcript": "Domain translation - for example, mapping from a summer to a winter scene, or from a photorealistic image to an object segmentation map - is often performed by GANs through something called cycle consistency loss. This model works by having, for each domain, a generator to map domain A into domain B, and a discriminator to differentiate between real images from domain B, and those that were constructed through the cross-domain generator. With a given image in domain A, training happens by using the A\u2192B generator to map it into domain B, and then then B\u2192 A generator to map it back the original domain. These generators are then trained using two losses: one based on the B-domain discriminator, to push the generated image to look like it belongs from that domain, and another based on the L2 loss between the original domain A image, and the image you get on the other end when you translate it into B and back again. \n\nThis paper addresses an effect (identified originally in an earlier paper) where in domains with a many to one mapping between domains (for example, mapping a realistic scene into a domain segmentation map, where information is inherently lost by translating pixels to object outlines), the cycle loss incentivizes the model to operate in a strange, steganographic way, where it saves information about the that would otherwise be lost in the form of low-amplitude random noise in the translated image. This low-amplitude information can't be isolated, but can be detected in a few ways. First, we can simply examine images and notice that information that could not have been captured in the lower-information domain is being perfectly reconstructed. Second, if you add noise to the translation in the lower-information domain, in such a way as to not perceptibly change the translation to human eyes, this can cause the predicted image off of that translation to deteriorate considerably, suggesting that the model was using information that could be modified by such small additions of noise to do its reconstruction. \n\nhttps://i.imgur.com/08i1j0J.png\n\nThe authors of this paper ask whether it's possible to train models that don't perform this steganographic information-storing (which they call \"self adversarial examples\"). A typical approach to such a problem would be to train generators to perform translations with and without the steganographic information, but even though we can prove the existence of the information, we can't isolate it in a way that would allow us to remove it, and thus create these kinds of training pairs. The two tactics the paper uses are: \n\n1) Simply training the generators to be able to translate a domain-mapped image with noise as well as one without noise, in the hope that this would train it not use information that can be interfered with by the application of such noise. \n\n2) In addition to a L2 cycle loss, adding a discriminator to differentiate between the back-translated image and the original one. I believe the idea here is that if both of the encoders are adding in noise as a kind of secret signal, this would be a way for the discriminator to distinguish between the original and reconstructed image, and would thus be penalized.  \n\nThey find that both of these methods reduce the use of steganographic information, as determined both by sensitivity to noise (where less sensitivity of reconstruction to noise means less use of coded information) and reconstruction honesty (which constrains accuracy of reconstruction in many to one domains to be no greater than the prediction that a supervised predictor could make given the image from the compressed domain",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1908.01517"
    },
    "254": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1910-04744",
        "transcript": "In Machine Learning, our models are lazy: they're only ever as good as the datasets we train them on. If a task doesn't require a given capability in order for a model to solve it, then the model won't gain that capability. This fact motivates a desire on the part of researchers to construct new datasets, to provide both a source of signal and a not-yet-met standard against which models can be measured. This paper focuses on the domain of reasoning about videos and the objects within them across frames. It observes that, on many tasks that ostensibly require a model to follow what's happening in a video, models that simply aggregate some set of features across frames can do as well as models that actually track and account for temporal evolution from one frame for another. They argue that this shows that, on these tasks, which often involve real-world scenes, the model can predict what's happening within a frame simply based on expectations of the world that can be gleaned from single frames - for example, if you see a swimming pool, you can guess that swimming is likely to take place there. As an example of the kind of task they'd like to get a model to solve, they showed a scene from the Godfather where a character leaves the room, puts a gun in his pocket, and returns to the room. Any human viewer could infer that the gun is in his pocket when it returns, but there doesn't exist any single individual frame that could give evidence of that, so it requires reasoning across frames. \nhttps://i.imgur.com/F2Ngsgw.png\n\nTo get around this inherent layer of bias in real-world scenes, the authors decide to artificially construct their own dataset, where objects are moved, and some objects are moved to be contained and obscured within others, in an entirely neutral environment, where the model can't generally get useful information from single frames. This is done using the same animation environment as is used in CLEVR, which contains simple objects that have color, texture, and shape, and that can be moved around a scene. Within this environment, called CATER, the benchmark is made up of three tasks: \n\n- Simply predicting what action (\"slide cone\" or \"pick up and place box\") is happening in a given frame. For actions like sliding, where in a given frame a sliding cone is indistinguishable from a static one, this requires a model to actually track prior position in order to correctly predict an action taking place\n- Being able to correctly identify the order in which a given pair of actions occurs\n- Watching a single golden object that can be moved and contained within other objects (entertainingly enough, for Harry Potter fans, called the snitch), and guessing what frame it's in at the end of the scene. This is basically just the \"put a ball in a cup and move it around\" party trick, but as a learning task\n\nhttps://i.imgur.com/bBhPnFZ.png\n\nThe authors do show that the \"frame aggregation/pooling\" methods that worked well on previous datasets don't work well on this dataset - which accords with both expectations and the authors goals. Obviously, it's still a fairly simplified environment, but they hope CATER can still be a useful shared benchmark for people working in the space to solve a task that is known to require more explicit spatiotemporal reasoning.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1910.04744"
    },
    "255": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-02530",
        "transcript": "A common critique of deep learning is its brittleness off-distribution, combined with its tendency to give confident predictions for off-distribution inputs, as is seen in the case of adversarial examples. In response to this critique, a number of different methods have cropped up in recent years, that try to capture a model's uncertainty as well as its overall prediction. This paper tries to do a broad evaluation of uncertainty methods, and, particularly, to test how they perform on out of distribution data, including both data that is perturbed from its original values, and fully OOD data from ground-truth categories never seen during training. Ideally, we would want an uncertainty method that is less confident in its predictions as data is made more dissimilar from the distribution that the model is trained on. Some metrics the paper uses for capturing this are: \n\n- Brier Score (The difference between predicted score and ground truth 0/1 label, averaged over all examples)\n- Negative Log Likelihood\n- Expected Calibration Error (Within a given bucket, this is calculated as the difference between accuracy to ground truth labels, and the average predicted score in that bucket, capturing that you'd ideally want to have a lower predicted score in cases where you have low accuracy, and vice versa)\n- Entropy - For labels that are fully out of distribution, and don't map to any of the model's categories, you can't directly calculate ground truth accuracy, but you can ideally ask for a model that has high entropy (close to uniform) probabilities over the classes it knows about when the image is drawn from an entirely different class\n\nThe authors test over image datasets small (MNIST) and large (ImageNet and CIFAR10), as well as a categorical ad-click-prediction dataset. They came up with some interesting findings. \n\nhttps://i.imgur.com/EVnjS1R.png\n\n1. More fully principled Bayesian estimation of posteriors over parameters, in the form of Stochastic Variational Inference, works well on MNIST, but quite poorly on either categorical data or higher dimensional image datasets \n\nhttps://i.imgur.com/3emTYNP.png\n\n2. Temperature scaling, which basically performs a second supervised calibration using a hold-out set to push your probabilities towards true probabilities, performs well in-distribution but collapses fairly quickly off-distribution (which sort of makes sense given that it too is just another supervised method that can do poorly when off-distribution) \n3. In general, ensemble methods, where you train different models on different subsets of the data and take their variance as uncertainty, perform the best across the bigger image models as well as the ad click model, likely because SVI (along with many other Bayesian methods) is too computationally intensive to get to work well on higher-dimensional data \n4. Overall, none of the methods worked particularly well, and even the best-performing ones were often confidently wrong off-distribution \n\nI think it's fair to say that we're far from where we wish we were when it comes to models that \"know when they don't know,\" and this paper does a good job of highlighting that in specific fashion.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.02530"
    },
    "256": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-05838",
        "transcript": "This paper combines imitation learning algorithm GAIL with recent advances in goal-conditioned reinforcement learning, to create a combined approach that can make efficient use of demonstrations, but can also learn information about a reward that can allow the agent to outperform the demonstrator. \n\nGoal-conditioned learning is a form of reward-driven reinforcement learning where the reward is a defined to be 1 when an agent reaches a particular state, and 0 otherwise. This can be a particularly useful form of learning for navigation tasks, where, instead of only training your agent to reach a single hardcoded goal (as you would with a reward function) you teach it to reach arbitrary goals when information about the goal is passed in as input. A typical difficulty with this kind of learning is that its reward is sparse: for any given goal, if an agent never reaches it, it won't ever get reward signal it can use to learn to find it again. A clever solution to this, proposed by earlier method HER (Hindsight Experience Replay), is to perform rollouts of the agent trajectory, and then train your model to reach all the states it actually reached along that trajectory. Said another way, even if your agent did a random, useless thing with respect to one goal, if you retroactively decided that the goal was where it ended up, then it'd be able to receive reward signal after all. In a learning scenario with a fixed reward, this trick wouldn't make any sense, since you don't want to train your model to only go wherever it happened to initially end up. But because the policy here is goal-conditioned, we're not giving our policy wrong information about how to go to the place we want, we're incentivizing it to remember ways it got to where it ended up, in the hopes that it can learn generalizable things about how to reach new places. \n\nThe other technique being combined in this paper is imitation learning, or learning from demonstrations. Demonstrations can be highly useful for showing the agent how to get to regions of state space it might not find on its own.  The authors of this paper advocate creating a goal-conditioned version of one particular imitation learning algorithm (Generative Adversarial Imitation Learning, or GAIL), and combining that with an off-policy version of Hindsight Experience Replay. In their model, a discriminator tries to tell the behavior of the demonstrator from that of the agent, given some input goal, and uses that as loss, combined with the loss of a more normal Q learning loss with a reward set to 1 when a goal is achieved. Importantly, they amplify both of these methods using the relabeling trick mentioned before: for both the demonstrators and the actual agent trajectories, they take tuples of (state, next state, goal) and replace the intended goal with another state reached later in the trajectory. For the Q learner, this performs its normal role as a way to get reward in otherwise sparse settings, and for the imitation learner, it is a form of data amplification, where a single trajectory + goal can be turned into multiple trajectories    \"successfully\" reaching all of the intermediate points along the observed trajectory. The authors show that their method learns more quickly (as a result of the demonstrations), but also is able to outperform demonstrators, which it wouldn't generally be able to do without an independent, non-demonstrator reward signal",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.05838"
    },
    "257": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1909-11764",
        "transcript": "Adversarial examples and defenses to prevent them are often presented as a case of inherent model fragility, where the model is making a clear and identifiable mistake, by misclassifying a label humans would classify correctly. But, another frame on the adversarial examples research is that they're a way of imposing a certain kind of prior requirement on our models: that they be sensitive to certain scales of perturbation to their inputs. One reason to want to do this is because you believe the model might reasonably need to interact with such perturbed inputs in future. But, another is that smoothness of model outputs, in the sense of an output that doesn't change sharply in the immediate vicinity of an example, can be a useful inductive bias that improves generalization. In images, this is often not the case, as training on adversarial examples empirically worsens performance on normal examples.  In text, however, it seems like you can get more benefit out of training on adversarial examples, and this paper proposes a specific way of doing that. \n\nAn interesting up-front distinction is the one between generating adversarial examples in embeddings vs raw text. Raw text is generally harder: it's unclear how to permute sentences in ways that leave them grammatically and meaningfully unchanged, and thus mean that the same label is the \"correct\" one as before, without human input. So the paper instead works in embedding space: adding a delta vectors of adversarial noise to the learned word embeddings used in a text model. One salient downside of generating adversarial examples to train on is that doing so is generally costly: it requires calculating the gradients with respect to the input to calculate the direction of the delta vector, which requires another backwards pass through the network, in addition to the ones needed to calculate the parameter gradients to update those. It happens to be the case that once you've calculated gradients w.r.t inputs, doing so for parameters is basically done for you for free, so one possible solution to this problem is to do a step of parameter gradient calculation/model training every time you take a step of perturbation generation. However, if you're generating your adversarial examples via multi-step Projected Gradient Descent, doing a step of model training at each of the K steps in multi-step PGD means that by the time you finish all K steps and are ready to train on the example, your perturbation vector is out of sync with with your model parameters, and so isn't optimally adversarial. To fix this, the authors propose actually training on the adversarial example generated by each step in the multi-step generation process, not just the example produced at the end. So, instead of training your model on perturbations of a given size, you train them on every perturbation up to and including that size. This also solves the problem of your perturbation being out of sync with your parameters, since you \"apply\" your perturbation in training at the same step where you calculate it. \n\nThe authors sole purpose in this was to make models that generalize better, and they show reasonably convincing evidence that this method works slightly better than competing alternatives on language modeling tasks. More saliently, in my view, they come up with a straightforward and clever solution to a problem, which could potentially be used in other domains.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1909.11764"
    },
    "258": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-02403",
        "transcript": "An interesting category of machine learning papers - to which this paper belongs - are papers which use learning systems as a way to explore the incentive structures of problems that are difficult to intuitively reason about the equilibrium properties of. In this paper, the authors are trying to better understand how different dynamics of a cooperative communication game between agents, where the speaking agent is trying to describe an object such that the listening agent picks the one the speaker is being shown, influence the communication protocol (or, to slightly anthropomorphize, the language) that the agents end up using.\n\nIn particular, the authors experiment with what happens when the listening agent is frequently replaced during training with a untrained listener who has no prior experience with the agent. The idea of this experiment is that if the speaker is in a scenario where listeners need to frequently \"re-learn\" the mapping between communication symbols and objects, this will provide an incentive for that mapping to be easier to quickly learn. \n\nhttps://i.imgur.com/8csqWsY.png\n\nThe metric of ease of learning that the paper focuses on is \"topographic similarity\", which is a measure of how compositional the communication protocol is. The objects they're working with have two properties, and the agents use a pair of two discrete symbols (two letters) to communicate about them. A perfectly compositional language would use one of the symbols to represent each of the properties. To mathematically measure this property, the authors calculate (cosine) similarity between the two objects property vectors, and the (edit) distance between the two objects descriptions under the emergent language, and calculate the correlation between these quantities. In this experimental setup, if a language is perfectly compositional, the correlation will be perfect, because every time a property is the same, the same symbol will be used, so two objects that share that property will always share that symbol in their linguistic representation. \nhttps://i.imgur.com/t5VxEoX.png\nThe premise and the experimental setup of this paper are interesting, but I found the experimental results difficult to gain intuition and confidence from. The authors do show that, in a regime where listeners are reset, topographic similarity rises from a beginning-of-training value of .54 to an end of training value of .59, whereas in the baseline, no-reset regime, the value drops to .51. So there definitely is some amount of support for their claim that listener resets lead to higher compositionality. But given that their central quality is just a correlation between similarities, it's hard to gain intuition for whether the difference is a meaningful. It doesn't naively seem particularly dramatic, and it's hard to tell otherwise without more references for how topographic similarity would change under a wider range of different training scenarios.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.02403"
    },
    "259": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1910-14033",
        "transcript": "If you've been at all aware of machine learning in the past five years, you've almost certainly seen the canonical word2vec example demonstrating additive properties of word embeddings: \"king - man + woman = queen\". This paper has a goal of designing embeddings for agent plans or trajectories that follow similar principles, such that a task composed of multiple subtasks can be represented by adding the vectors corresponding to the subtasks. For example, if a task involved getting an ax and then going to a tree, you'd want to be able to generate an embedding that corresponded to a policy to execute that task by summing the embeddings for \"go to ax\" and \"go to tree\". \n\nhttps://i.imgur.com/AHlCt76.png\n\nThe authors don't assume that they know the discrete boundaries between subtasks in multiple-task trajectories, and instead use a relatively simple and clever training structure in order to induce the behavior described above. They construct some network g(x) that takes in information describing a trajectory (in this case, start and end state, but presumably could be more specific transitions), and produces an embedding. Then, they train a model on an imitation learning problem, where, given one demonstration of performing a particular task (typically generated by the authors to be composed of multiple subtasks), the agent needs to predict what action will be taken next in a second trajectory of the same composite task. At each point in the sequence of predicting the next action, the agent calculates the embedding of the full reference trajectory, and the embedding of the actions they have so far performed in the current stage in the predicted trajectory, and calculates the difference between these two values. This embedding difference is used to condition the policy function that predicts next action. At each point, you enforce this constraint, that the embedding of what is remaining to be done in the trajectory be close to the embedding of (full trajectory) - (what has so far been completed), by making the policy that corresponds with that embedding map to the remaining part of the trajectory. In addition to this core loss, they also have a few regularization losses, including: \n\n1. A loss that goes through different temporal subdivisions of reference, and pushes the summed embedding of the two parts to be close to the embedding of the whole \n2. A loss that simply pushes the embeddings of the two paired trajectories performing the same task closer together \n\nThe authors test mostly on relatively simple tasks - picking up and moving sequences of objects with a robotic arm, moving around and picking up objects in a simplified Minecraft world - but do find that their central partial-conditioning-based loss gives them better performance on demonstration tasks that are made up of many subtasks. \n\nOverall, this is an interesting and clever paper: it definitely targeted additive composition much more directly, rather than situations like the original word2vec where additivity came as a side effect of other properties, but it's still an idea that I could imagine having interesting properties, and one I'd be interested to see applied to a wider variety of tasks.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1910.14033"
    },
    "260": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1910-08210",
        "transcript": "Reinforcement learning is notoriously sample-inefficient, and one reason why is that agents learn about the world entirely through experience, and it takes lots of experience to learn useful things. One solution you might imagine to this problem is the ones humans by and large use in encountering new environments: instead of learning everything through first-person exploration, acquiring lots of your knowledge by hearing or reading condensed descriptions of the world that can help you take more sensible actions within it. This paper and others like it have the goal of learning RL agents that can take in information about the world in the form of text, and use that information to solve a task. This paper is not the first to propose a solution in this general domain, but it claims to be unique by dint of having both the dynamics of the environment and the goal of the agent change on a per-environment basis, and be described in text. \n\nThe precise details of the architecture used are very much the result of particular engineering done to solve this problem, and as such, it's a bit hard to abstract away generalizable principles that this paper showed, other than the proof of concept fact that tasks of the form they describe - where an agent has to learn which objects can kill which enemies, and pursue the goal of killing certain ones - can be solved. \n\nArguably the most central design principle of the paper is aggressive and repeated use of different forms of conditioning architectures, to fully mix the information contained in the textual and visual data streams. This was done in two main ways: \n\n- Multiple different attention summaries were created, using the document embedding as input, but with queries conditioned on different things (the task, the inventory, a summarized form of the visual features). This is a natural but clever extension of the fact that attention is an easy way to generate conditional aggregated versions of some input\nhttps://i.imgur.com/xIsRu2M.png\n- The architecture uses FiLM (Featurewise Linear Modulation), which is essentially a many-generations-generalized version of conditional batch normalization in which the gamma and lambda used to globally shift and scale a feature vector are learned, taking some other data as input. The canonical version of this would be taking in text input, summarizing it into a vector, and then using that vector as input in a MLP that generates gamma and lambda parameters for all of the convolutional layers in a vision system. The interesting innovation of this paper is essentially to argue that this conditioning operation is quite neutral, and that there's no essential way in which the vision input is the \"true\" data, and the text simply the auxiliary conditioning data: it's more accurate to say that each form of data should conditioning the process of the other one. And so they use Bidirectional FiLM, which does just that, conditioning vision features on text summaries, but also conditioning text features on vision summaries.\nhttps://i.imgur.com/qFaH1k3.png\n- The model overall is composed of multiple layers that perform both this mixing FiLM operation, and also visually-conditioned attention. The authors did show, not super surprisingly, that these additional forms of conditioning added performance value to the model relative to the cases where they were ablated",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1910.08210"
    },
    "261": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1910-13038",
        "transcript": "Reinforcement Learning is often broadly separated into two categories of approaches: model-free and model-based. In the former category, networks simply take observations and input and produce predicted best-actions (or predicted values of available actions) as output. In order to perform well, the model obviously needs to gain an understanding of how its actions influence the world, but it doesn't explicitly make predictions about what the state of the world will be after an action is taken. In model-based approaches, the agent explicitly builds a dynamics model, or a model in which it takes in (past state, action) and predicts next state. In theory, learning such a model can lead to both interpretability (because you can \"see\" what the model thinks the world is like) and robustness to different reward functions (because you're learning about the world in a way not explicitly tied up with the reward). \n\nThis paper proposes an interesting melding of these two paradigms, where an agent learns a model of the world as part of an end-to-end policy learning. This works through something the authors call \"observational dropout\": the internal model predicts the next state of the world given the prior one and the action, and then with some probability, the state of the world that both the policy and the next iteration of the dynamics model sees is replaced with the model's prediction. This incentivizes the network to learn an effective dynamics model, because the farther the predictions of the model are from the true state of the world, the worse the performance of the learned policy will be on the iterations where the only observation it can see is the predicted one. So, this architecture is model-free in the sense that the gradient used to train the system is based on applying policy gradients to the reward, but model-based in the sense that it does have an internal world representation. \n\nhttps://i.imgur.com/H0TNfTh.png\n\nThe authors find that, at a simple task, Swing Up Cartpole, very low probabilities of seeing the true world (and thus very high probabilities of the policy only seeing the dynamics model output) lead to world models good enough that a policy trained only on trajectories sampled from that model can perform relatively well. This suggests that at higher probabilities of the true world, there was less value in the dynamics model being accurate, and consequently less training signal for it. (Of course, policies that often could only see the predicted world performed worse during their original training iteration compared to policies that could see the real world more frequently).\n\n On a more complex task of CarRacing, the authors looked at how well a policy trained using the representations of the world model as input could perform, to examine whether it was learning useful things about the world. \nhttps://i.imgur.com/v9etll0.png\nThey found an interesting trade-off, where at high probabilities (like before) the dynamics model had little incentive to be good, but at low probabilities it didn't have enough contact with the real dynamics of the world to learn a sensible policy.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1910.13038"
    },
    "262": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1905-10650",
        "transcript": "In the last two years, the Transformer architecture has taken over the worlds of language modeling and machine translation. The central idea of Transformers is to use self-attention to aggregate information from variable-length sequences, a task for which Recurrent Neural Networks had previously been the most common choice. Beyond that central structural change, one more nuanced change was from having a single attention mechanism on a given layer (with a single set of query, key, and value weights) to having multiple attention heads, each with their own set of weights. The change was framed as being conceptually analogous to the value of having multiple feature dimensions, each of which focuses on a different aspect of input; these multiple heads could now specialize and perform different weighted sums over input based on their specialized function. This paper performs an experimental probe into the value of the various attention heads at test time, and tries a number of different pruning tests across both machine translation and language modeling architectures to see their impact on performance. \n\nIn their first ablation experiment, they test the effect of removing (that is, zero-masking the contribution of) a single head from a single attention layer, and find that in almost all cases (88 out of 96) there's no statistically significant drop in performance. Pushing beyond this, they ask what happens if, in a given layer, they remove all heads but the one that was seen to be most important in the single head tests (the head that, if masked, caused the largest performance drop). This definitely leads to more performance degradation than the removal of single heads, but the degradation is less than might be intuitively expected, and is often also not statistically significant. \n\nhttps://i.imgur.com/Qqh9fFG.png\n\nThis also shows an interesting distribution over where performance drops: in machine translation, it seems like decoder-decoder attention is the least sensitive to heads being pruned, and encoder-decoder attention is the most sensitive, with a very dramatic performance dropoff observed if particularly the last layer of encoder-decoder attention is stripped to a single head. This is interesting to me insofar as it shows the intuitive roots of attention in these architectures; attention was originally used in encoder-decoder parts of models to solve problems of pulling out information in a source sentence at the time it's needed in the target sentence, and this result suggests that a lot of the value of multiple heads in translation came from making that mechanism more expressive. \n\nFinally, the authors performed an iterative pruning test, where they ordered all the heads in the network according to their single-head importance, and pruned starting with the least important. Similar to the results above, they find that drops in performance at high rates of pruning happen eventually to all parts of the model, but that encoder-decoder attention suffers more quickly and more dramatically if heads are removed. \n\nhttps://i.imgur.com/oS5H1BU.png\n\nOverall, this is a clean and straightforward empirical paper that asks a fairly narrow question and generates some interesting findings through that question. These results seem reminiscent to me of the Lottery Ticket Hypothesis line of work, where it seems that having a network with a lot of weights is useful for training insofar as it gives you more chances at an initialization that allows for learning, but that at test time, only a small percentage of the weights have ultimately become important, and the rest can be pruned. In order to make the comparison more robust, I'd be interested to see work that does more specific testing of the number of heads required for good performance during training and also during testing,  divided out by different areas of the network. (Also, possibly this work exists and I haven't found it!)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.10650"
    },
    "263": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1903-11780",
        "transcript": "Self-Supervised Learning is a broad category of approaches whose goal is to learn useful representations by asking networks to perform constructed tasks that only use the content of a dataset itself, and not external labels. The idea with these tasks is to design tasks such that solving them requires the network to have learned useful  Some examples of this approach include predicting the rotation of rotated images, reconstructing color from greyscale, and, the topic of this paper, maximizing mutual information between different areas of the image. The hope behind this last approach is that if two areas of an image are generated by the same set of underlying factors (in the case of a human face: they're parts of the same person's face), then a representation that correctly captures those factors for one area will give you a lot of information about the representation of the other area. Historically, this conceptual desire for representations that are mutually informative has been captured by mutual information. If we define the representation distribution over the data of area 1 as p(x) and area 2 as q(x), the mutual information is the KL divergence between the joint distribution of these two distributions and the product of their marginals. This is an old statistical intuition: the closer the joint is to the product of marginals, the closer the variables are to independent; the farther away, the closer they are to informationally identical. \n\nhttps://i.imgur.com/2SzD5d5.png\n\nThis paper argues that the presence of the KL divergence in this mutual information formulation impedes the ability of networks to learn useful representations. This argument is theoretically based on a result from a recent paper (which for the moment I'll just take as foundation, without reading it myself) that empirical lower-bound measurements of mutual information, of the kind used in these settings, are upper bounded by log(n) where n is datapoints. Our hope in maximizing a lower bound to any quantity is that the bound is fairly tight, since that means that optimizing a network to push upward a lower bound actually has the effect of pushing the actual value up as well. If the lower bound we can estimate is constrained to be far below the actual lower bound in the data, then pushing it upward doesn't actually require the value to move upward. The authors identify this as a particular problem in areas where the underlying mutual information of the data is high, such as in videos where one frame is very predictive of the next, since in those cases the constraint imposed by the dataset size will be small relative to the actual possible maximum mutual information you could push your network to achieve.\n\nhttps://i.imgur.com/wm39mQ8.png\n\nTaking a leaf out of the GAN literature, the authors suggest keeping replacing the KL divergence component of mutual information and replacing it with the Wasserstein Distance; otherwise known as the \"earth-mover distance\", the Wasserstein distance measures the cost of the least costly way to move probability mass from one distribution to another, assuming you're moving that mass along some metric space. A nice property of the Wasserstein distance, in both GANs and in this application) is that they don't saturate quite as quickly: the value of a KL divergence can shoot up if the distributions are even somewhat different, making it unable to differentiate between distributions that are somewhat and very far away,  whereas a Wasserstein distance continues to have more meaningful signal in that regime. In the context of the swap for mutual information, the authors come up with the \"Wasserstein Dependency Measure\", which is just the Wasserstein Distance between the joint distributions and the product of the marginals.\n\nhttps://i.imgur.com/3s2QRRz.png\n\nIn practice, they use the dual formulation of the Wasserstein distance, which amounts to applying a (neural network) function f(x) to values from both distributions, optimizing f(x) so that the values are far apart, and using that distance as your training signal. Crucially, this function has to be relatively smooth in order for the dual formulation to work: in particular it has to have a small Lipschitz value (meaning its derivatives are bounded by some value). Intuitively, this has the effect of restricting the capacity of the network, which is hoped to incentivize it to use its limited capacity to represent true factors of variation, which are assumed to be the most compact way to represent the data. Empirically, the authors found that their proposed Wasserstein Dependency Measure (with a slight variation applied to reduce variance) does have the predicted property of performing better for situations where the native mutual information between two areas is high. \n\nI found the theoretical points of this paper interesting, and liked the generalization of the idea of Wasserstein distances from GANs to a new area. That said, I wish I had a better mechanical sense for how it ground out in actual neural network losses: this is partially just my own lack of familiarity with how e.g. mutual information losses are actually formulated as network objectives, but I would have appreciated an appendix that did a bit more of that mapping between mathematical intuition and practical network reality.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.11780"
    },
    "264": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-07983",
        "transcript": "In response to increasing calls for ways to explain and interpret the predictions of neural networks, one major genre of explanation has been the construction of salience maps for image-based tasks. These maps assign a relevance or saliency score to every pixel in the image, according to various criteria by which the value of a pixel can be said to have influenced the final prediction of the network. This paper is an interesting blend of ideas from the saliency mapping literature with ones from adversarial examples: it essentially shows that you can create adversarial examples whose goal isn't to change the output of a classifier, but instead to keep the output of the classifier fixed, but radically change the explanation (their term for the previously-described pixel saliency map that results from various explanation-finding methods) to resemble some desired target explanation. This is basically a targeted adversarial example, but targeting a different property of the network (the calculated explanation) while keeping an additional one fixed (keeping the output of the original network close to the original output, as well as keeping the input image itself in a norm ball around the original image. This is done in a pretty standard way: by just defining a loss function incentivizing closeness to the original output and also closeness of the explanation to the desired target, and performing gradient descent to modify pixels until this loss was low. \n\nhttps://i.imgur.com/N9uReoJ.png\n\nThe authors do a decent job of showing such targeted perturbations are possible: by my assessment of their results their strongest successes at inducing an actual targeted explanation are with Layerwise Relevance Propogation and Pattern Attribution (two of the 6 tested explanation methods). With the other methods, I definitely buy that they're able to induce an explanation that's very unlike the true/original explanation, but it's not as clear they can reach an arbitrary target. This is a bit of squinting, but it seems like they have more success in influencing propogation methods (where the effect size of the output is propogated backwards through the network, accounting for ReLus) than they do with gradient ones (where you're simply looking at the gradient of the output class w.r.t each pixel. \n\nIn the theory section of the paper, the authors do a bit of differential geometry that I'll be up front and say I did not have the niche knowledge to follow, but which essentially argues that the manipulability of an explanation has to do with the curvature of the output manifold for a constant output. That is to say: how much can you induce a large change in the gradient of the output, while moving a small distance along the manifold of a constant output value. They then go on to argue that ReLU activations, because they are by definition discontinuous, induce sharp changes in gradient for points nearby one another, and this increase the ability for networks to be manipulated. They propose a softplus activation instead, where instead of a sharp discontinuity, the ReLU shape becomes more curved, and show relatively convincingly that at low values of Beta (more curved) you can mostly eliminate the ability of a perturbation to induce an adversarially targeted explanation. \n\nhttps://i.imgur.com/Fwu3PXi.png\n\nFor all that I didn't have a completely solid grasp of some of the theory sections here, I think this is a neat proof of concept paper in showing that neural networks can be small-perturbation fragile on a lot of different axes: we've known this for a while in the area of adversarial examples, but this is a neat generalization of that fact to a new area.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.07983"
    },
    "265": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1908-07644",
        "transcript": "If your goal is to interpret the predictions of neural networks on images, there are a few different ways you can focus your attention. One approach is to try to understand and attach conceptual tags to learnt features, to form a vocabulary with which models can be understood. However, techniques in this family have to content with a number of challenges, from the difficulty in attaching clear concepts to the sheer number of neurons to interpret. An alternate approach, and the one pursued by this paper, is to frame interpretability as a matter of introspecting on *where in an image* the model is pulling information from to make its decision.\n\nThis is the question for which hard attention provides an answer: identify where in an image a model is making a decision by learning a meta-model that selects small patches of an image, and then makes a classification decision by applying a network to only those patches which were selected. By definition, if only a discrete set of patches were used for prediction, those were the ones that could be driving the model's decision. This central fact of the model only choosing a discrete set of patches is a key complexity, since the choice to use a patch or not is a binary, discontinuous action, and not something through which one can back-propogate gradients. Saccader, the approach put forward by this paper, proposes an architecture which extracts features from locations within an image, and uses those spatially located features to inform a stochastic policy that selects each patch with some probability. Because reinforcement learning by construction is structured to allow discrete actions, the system as a whole can be trained via policy gradient methods. \n\nhttps://i.imgur.com/SPK0SLI.png\n\nDiving into a bit more detail: while I don't have a deep familiarity with prior work in this area, my impression is that the notion of using policy gradient to learn a hard attention policy isn't a novel contribution of this work, but rather than its novelty comes from clever engineering done to make that policy easier to learn. The authors cite the problem of sparse reward in learning the policy, which I presume to mean that if you start in more traditional RL fashion by just sampling random patches, most patches will be unclear or useless in providing classification signal, so it will be hard to train well. The Saccader architecture works by extracting localized features in an architecture inspired by the 2019 BagNet paper, which essentially applies very tall and narrow convolutional stacks to spatially small areas of the image. This makes it the case that feature vectors for different overlapping patches can be computed efficiently: instead of rerunning the network again for each patch, it just combined the features from the \"tops\" of all of the small column networks inside the patch, and used that aggregation as a patch-level feature. These features from the \"representation network\" were then used in an \"attention network,\" which uses larger receptive field convolutions to create patch-level features that integrated the context of things around them. Once these two sets of features were created, they were fed into the \"Saccader cell\", which uses them to calculate a distribution over patches which the policy then samples over. The Saccader cell is a simplified memory cell, which sets a value to 1 when a patch has been sampled, and applies a very strong penalization on that patch being sampled on future \"glimpses\" from the policy (in general, classification is performed by making a number of draws and averaging the logits produced for each patch). \n\nhttps://i.imgur.com/5pSL0oc.png\n\nI found this paper fairly conceptually clever - I hadn't thought much about using a reinforcement learning setup for classification before -  though a bit difficult to follow in its terminology and notation. It's able to perform relatively well on ImageNet, though I'm not steeped enough in that as a benchmark to have an intuitive sense for the paper's claim that their accuracy is meaningfully in the same ballpark as full-image models. One interesting point the paper made was that their system, while limited to small receptive fields for the patch features, can use an entirely different model for mapping patches to logits once the patches are selected, and so can benefit from more powerful generic classification models being tacked onto the end.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1908.07644"
    },
    "266": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1710-11248",
        "transcript": "The fundamental unit of Reinforcement Learning is the reward function, with a core assumption of the area being that actions induce rewards, with some actions being higher reward than others. But, reward functions are just artificial objects we design to induce certain behaviors; the universe doesn\u2019t hand out \u201ctrue\u201d rewards we can build off of.  Inverse Reinforcement Learning as a field is rooted in the difficulty of designing reward functions, and has the aspiration of, instead of requiring a human to hard code a reward function, inferring rewards from observing human behavior. The rough idea is that if we imagine a human is (even if they don\u2019t know it) operating so as to optimize some set of rewards, we might be able to infer that set of underlying incentives from their actions, and, once we\u2019ve extracted a reward function, use that to train new agents. This is a mathematically quite tricky problem, for the basic reason that a human\u2019s actions are often consistent with a wide range of possible underlying  \u201cpolicy\u201d parameters, and also that a given human policy could be an optimal for a wide range of underlying reward functions. \n\nThis paper proposes using an adversarial frame on the problem, where you learn a reward function by trying to make reward higher for the human demonstrations you observe, relative to the actions the agent itself is taking. This has the effect of trying to learn an agent that can imitate human actions. However, it specifically designs its model structure to allow it to go beyond just imitation. The problem with learning a purely imitative policy is that it\u2019s hard for the model to separate out which actions the human is taking because they are intrinsically high reward (like, perhaps, eating candy), versus actions which are only valuable in a particular environment (perhaps opening a drawer if you\u2019re in a room where that\u2019s where the candy is kept). If you didn\u2019t realize that the real reward was contained in the candy, you might keep opening drawers, even if you\u2019re in a room where the candy is laying out on the table. \n\nIn mathematical terms, separating out intrinsic vs instrumental (also known as \"shaped\") rewards is a matter of making sure to learn separate out the reward associated with a given state from value of taking a given action at that state, because the value of your action is only born out based on assumptions about how states transition between each other, which is a function of the specific state to state dynamics of the you\u2019re in. The authors do this by defining a g(s) function, and a h(s) function. They then define their overall reward of an action as (g(s) + h(s\u2019)) - h(s), where s\u2019 is the new state you end up in if you take an action. \n\nhttps://i.imgur.com/3ENPFVk.png\n\nThis follows the natural form of a Bellman update, where the sum of your future value at state T should be equal to the sum of your future value at time T+1 plus the reward you achieve at time T. \n\nhttps://i.imgur.com/Sd9qHCf.png\n\nBy adopting this structure, and learning a separate neural network to capture the h(s) function representing the value from here to the end, the authors make it the case that the g(s) function is a purer representation of the reward at a state, regardless of what we expect to happen in the future. Using this, they\u2019re able to use this learned reward to bootstrap good behavior in new environments, even in contexts where a learned value function would be invalid because of the assumptions of instrumental value. They compare their method to the baseline of GAIL, which is a purely imitation-learning approach, and show that theirs is more able to transfer to environments with similar states but different state-to-state dynamics. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1710.11248"
    },
    "267": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1807.08024",
        "transcript": "In the area of explaining model predictions over images, there are two main strains of technique: methods that look for pixels that have the highest gradient effect on the output class, and assign those as the \u201creason\u201d for the class, and approaches that ask which pixel regions are most responsible for a given classification, in the sense that the classification would change the most if they were substituted with some uninformative reference value. \n\nThe tricky thing about the second class of methods is that you need to decide what to use as your uninformative fill-in value. It\u2019s easy enough to conceptually pose the problem of \u201cwhat would our model predict if it couldn\u2019t see this region of pixels,\u201d but as a practical matter, these models take in full images, and you have to put *something* to give the classifier in a region, if you\u2019re testing what the score would be if you removed the information contained in the pixels in that region. What should you fill in instead? The simplest answers are things like \u201czeros\u201d, or \u201ca constant value\u201d or \u201cwhite noise\u201d. But all of these are very off-distribution for the model; it wouldn\u2019t have typically seen images that resemble white noise, or all zeros, or all a single value. So if you measure the change in your model score from an off-distribution baseline to your existing pixels, you may not be getting the marginal value of the pixels, so much as the marginal disutility of having something so different from what the model has previously seen. There are other, somewhat more sensible approaches, like blurring out the areas around the pixel region of interest, but these experience less intense forms of the same issue. \n\nThis paper proposes instead, using generative models to fill in the regions conditioned on the surrounding pixels, and use that as a reference. The notion here is that a conditioned generative model, like a GAN or VAE, can take into account the surrounding pixels, and \u201cimagine\u201d a fill-in that flows smoothly from the surrounding pixels, and looks generally like an image, but which doesn\u2019t contain the information from the pixels in the region being tested, since it wasn\u2019t conditioned on that. \n\nhttps://i.imgur.com/2fKnY0M.png\n\nUsing this approach, the authors run two types of test: one where they optimize to find the smallest region they can remove from the image, and have it switch class (Smallest Deletion Region, or SDR), and also the smallest informative region that can be added to an otherwise uninformative image, and have the model predict the class connected to that region. They find that regions calculated using their generative model fill in, and specifically with GANs, find a smaller and more compact pixel region as their explanation for the prediction, which is consistent with both human intuitions and also with a higher qualitative sensibleness of the explanations found. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1807.08024"
    },
    "268": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=shallue2018measuring",
        "transcript": "This paper tries to do an exhaustive empirical evaluation of the question of how effectively you can reduce the number of training steps needed to train your model, by increasing the batch size. This is an interesting question because it\u2019s becoming increasingly the case that computational costs scale very slowly with additional datapoints added to a batch, and so your per-example cost will be lower, the larger a batch you do your gradient calculation in. \n\nIn the most ideal world, we might imagine that there\u2019s a perfect trade-off relationship between batch size and training steps. As a simplistic example, if it were the case that your model only needed to see each observation in the dataset once in order to obtain some threshold of accuracy, and there was an unbounded ability to trade off batch size against training steps, then one might imagine that you could just take one large step based on the whole dataset (in which case you\u2019d then be doing not Stochastic Gradient Descent, but just Batch Gradient Descent). However, there\u2019s reason to suspect that this won\u2019t be possible; for one thing, it seems like having multiple noisier steps is better for optimization than taking one single step of training. \n\nhttps://i.imgur.com/uwCfBJR.png\n\nThis paper set out to do a large-scale evaluation of what this behavior looks like over a range of datasets. They did so by setting a target test error rate, and then measuring how many training steps were necessary to reach that error rate, for a given batch size. For fairness, they trained hyperparameters separately for each batch size. They found that, matching some theoretical predictions, at small to medium batch sizes, your increase in batch size pays off 1:1 in fewer needed training steps. As batch size increases more, the tradeoff curve diverges from 1:1, and eventually goes flat, meaning that, even if you increase your batch size more, you can no longer go any lower in terms of training steps. This seems to me connected to the idea that having a noisy, multi-step search process is useful for the non-convex environments that neural net optimizers are working in. \n\nhttps://i.imgur.com/ycigYVX.png\n\nA few other notes from the paper:\n- Different model architectures can extend 1:1 scaling to higher batch sizes, and thus plateau at a lower number of training steps\n- Momentum also has the effect of plateauing at a lower number of needed training steps\n- It\u2019s been previously suggested that you need to scale optimal learning rate linearly or according to the square root of the batch size, in order to maintain best performance. The authors find that there are different learning rates across batch size, but that they aren\u2019t well-approximated by a linear or square-root relationship\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1811.03600"
    },
    "269": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1806-03107",
        "transcript": "This was definitely one of the more conceptually nuanced and complicated papers I\u2019ve read recently, and I\u2019ve only got about 60% confidence that I fully grasp all of its intuitions. However, I\u2019m going to try to collect together what I did understand. \n\nThere is a lot of research into generative models of text or image sequences, and some amount of research into building \u201cmodels\u201d in the reinforcement learning sense, where your model can predict future observations given current observations and your action. There\u2019s an important underlying distinction here between model-based RL (where you learn a model of how the world evolves, and use that to optimize reward) and model-free RL (where you leave don\u2019t bother explicitly learning a world model, and just directly try to optimize rewards)  However, this paper identifies a few limitations of this research. \n\n1) It\u2019s largely focused on predicting observations, rather than predicting *state*. State is a bit of a fuzzy concept, and corresponds to, roughly, \u201cthe true underlying state of the game\u201d. An example I like to use is a game where you walk in one door, and right next to it is a second door, which requires you to traverse the space and find rewards and a key before you can open. Now, imagine that the observation of your agent is it looking at the door. If the game doesn\u2019t have any on-screen representation of the fact that you\u2019ve found the key, it won\u2019t be present in your observations, and you\u2019ll observe the same thing at the point you have just entered and once you found the key. However, the state of the game at these two points will be quite different, in that in the latter case, your next states might be \u201copening the door\u201d rather than \u201cgoing to collect rewards\u201d. Scenarios like this are referred to broadly as Partially Observable games or environments. This paper wants to build a model of how the game evolves into the future, but it wants to build a model of *state-to-state* evolution, rather than observation-to-observation evolution, since observations are typically both higher-dimensionality and also more noisy/less informative. \n\n2) Past research has typically focused on predicting each next-step observation, rather than teaching models to be able to directly predict a state many steps in the future, without having to roll out the entire sequence of intermediate predictions.  This is arguably quite valuable for making models that can predict the long term consequences of their decision \n\nThis paper approaches that with an approach inspired by the Temporal Difference framework used in much of RL, in which you update your past estimate of future rewards by forcing it to be consistent with the actual observed rewards you encounter in the future. Except, in this model, we sample two a state (z1) and then a state some distance into the future (z2), and try to make our backwards-looking prediction of the state at time 1, taking into account observations that happened in between, match what our prediction was with only the information at time one. \n\nAn important mechanistic nuance here is the idea of a \u201cbelief state\u201d, something that captures all of your knowledge about game history up to a certain point. We can then directly sample a state Zt given the belief state Bt at that point. This isn\u2019t actually possible with a model where we predict a state at time T given the state at time T-1, because the state at time Z-1 is itself a sample, and so in order to get a full distribution of Zt, you have to sample Zt over the distribution of Zt-1, and in order to get the distribution of Zt-1 you have to sample over the distribution of Zt-2, and so on and so on. Instead, we have a separate non-state variable, Bt that is created conditional on all our past observations (through a RNN). \n\nhttps://i.imgur.com/N0Al42r.png\n\nAll said and done, the mechanics of this model look like: \n1) Pick two points along the sequence trajectory\n2) Calculate the belief state at each point, and, from that, construct a distribution over states at each timestep using p(z|b) \n3) Have an additional model that predicts z1 given z2, b1, and b2 (that is, the future beliefs and states), and push the distribution over z1 from this model to be close to the distribution over z1 given only the information available at time t1\n4) Have a model that predicts Z2 given Z1 and the time interval ahead that we\u2019re jumping, and try to have this model be predictive/have high likelihood over the data \n5) And, have a model that predicts an observation at time T2 given the state Z2, and train that so that we can convert our way back to an observation, given a state \n\nThey mostly test it on fairly simple environments, but it\u2019s an interesting idea, and I\u2019d be curious to see other people develop it in future. \n\n(A strange aspect of this model is that, as far as I can tell, it\u2019s non-interventionist, in that we\u2019re not actually conditioning over agent action, or trying to learn a policy for an agent. This is purely trying to learn the long term transitions between states) \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1806.03107"
    },
    "270": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1903-04933",
        "transcript": "Current work in image generation (and generative models more broadly) can be split into two broad categories: implicit models, and likelihood-based models. Implicit models is a categorically that predominantly creates GANs, and which learns how to put pixels in the right places without actually learning a joint probability model over pixels. This is a detriment for applications where you do actually want to be able to calculate probabilities for particular images, in addition to simply sampling new images from your model. Within the class of explicit probability models, the auto-encoder and the autoregressive model are the two most central and well-established. \n\nAn auto-encoder works by compressing information about an image into a central lower-dimensional \u201cbottleneck\u201d code, and then trying to reconstruct the original image using the information contained in the code. This structure works well for capturing global structure, but is generally weaker at local structure, because by convention images are generated through stacked convolutional layers, where each pixel in the image is sampled separately, albeit conditioned on the same latent state (the value of the layer below). This is in contrast to an auto-regressive decoder, where you apply some ordering to the pixels, and then sample them in sequence: starting the prior over the first pixel, and then the second conditional on the first, and so on. In this setup, instead of simply expecting your neighboring pixels to coordinate with you because you share latent state, the model actually has visibility into the particular pixel sampled at the prior step, and has the ability to condition on that. This leads to higher-precision generation of local pixel structure with these models .\n\nIf you want a model that can get the best of all of these worlds - high-local precision, good global structure, and the ability to calculate probabilities - a sensible approach might be to combine the two: to learn a global-compressed code using an autoencoder, and then, conditioning on that autoencoder code as well as the last sampled values, generate pixels using an autoregressive decoder. However, in practice, this has proved tricky. At a high level, this is because the two systems are hard to balance with one another, and different kinds of imbalance lead to different failure modes. If you try to constrain the expression power of your global code too much, your model will just give up on having global information, and just condition pixels on surrounding (past-sampled) pixels. But, by contrast, if you don\u2019t limit the capacity of the code, then the model puts even very local information into the code and ignores the autoregressive part of the model, which brings it away from playing our desired role as global specifier of content. \n\nThis paper suggests a new combination approach, whereby we jointly train an encoder and autoregressive decoder, but instead of training the encoder on the training signal produced by that decoder, we train it on the training signal we would have gotten from decoding the code into pixels using a simpler decoder, like a feedforward network. The autoregressive network trains on the codes from the encoder as the encoder trains, but it doesn\u2019t actually pass any signal back to it. Basically, we\u2019re training our global code to believe it\u2019s working with a less competent decoder, and then substituting our autoregressive decoder in during testing. \n\nhttps://i.imgur.com/d2vF2IQ.png\n\nSome additional technical notes: \n- Instead of using a more traditional continuous-valued bottleneck code, this paper uses the VQ-VAE tactic of discretizing code values, to be able to more easily control code capacity. This essentially amounts to generating code vectors as normal, clustering them, passing their cluster medians forward, and then ignoring the fact that none of this is differentiable and passing back gradients with respect to the median \n- For their auxiliary decoders, the authors use both a simple feedforward network, and also a more complicated network, where the model needs to guess a pixel, using only the pixel values outside of a window of size of that pixel. The goal of the latter variant is to experiment with a decoder that can\u2019t use local information, and could only use global\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.04933"
    },
    "271": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1804.00222",
        "transcript": "Unsupervised representation learning is a funny thing: our aspiration in learning representations from data is typically that they\u2019ll be useful for future tasks, but, since we (by definition) don\u2019t have access to labels, our approach has historically been to define heuristics, such as representing the data distribution in a low-dimensional space, and hope that those heuristics translate to useful learned representations. And, to a fair extent, they have. However, this paper\u2019s goal is to attach this problem more directly, by explicitly meta-learning an unsupervised update rule so that performs well in future tasks. They do this by: \n\nhttps://i.imgur.com/EEkpW9g.png\n\n1) Defining a parametrized weight update function, to slot into the role that Stochastic Gradient Descent on a label-defined loss function would play in a supervised network. This function calculates a \u201chidden state\u201d, is defined for each neuron in each layer, and takes in the pre and post-nonlinearity activations for that batch, the hidden state of the next layer, and a set of learned per-layer \u201cbackwards weights\u201d. The weight update for that neuron is then calculated using the current hidden state, the last batch's hidden state, and the current value of the weight. In the traditional way of people in this field who want to define some generic function, they instantiate these functions as a MLP. \n\n2) Using that update rule on the data from a new task, taking the representing resulting from applying the update rule, and using it in a linear regression with a small number of samples. The generalization performance from this k-shot regression, taken in expectation over multiple tasks, acts as our meta training objective. By back-propagating from this objective, to the weight values of the representation, and from there to the parameters of the optimization step, they incentivize their updater to learn representations that are useful across some distribution of tasks. \n\n\nA slightly weird thing about this paper is that they train on image datasets, but shuffle the pixels and use a fully connected network rather than a conv net. I presume this has to do with the complexities of defining a weight update rule for a convolution, but it does make it harder to meaningfully compare with other image-based unsupervised systems, which are typically done using convolution. An interesting thing they note is that, early in meta-training on images, their update rules generalize fairly well to text data. However, later in training the update rules seem to have specialized to images, and generalize more poorly to images. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1804.00222"
    },
    "272": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1905.09418",
        "transcript": "In the two years since it\u2019s been introduced, the Transformer architecture, which uses multi-headed self-attention in lieu of recurrent models to consolidate information about input and output sequences, has more or less taken the world of language processing and understanding by storm. It has become the default choice for language for problems like translation and questioning answering, and was the foundation of OpenAI\u2019s massive language-model-trained GPT. In this context, I really appreciate this paper\u2019s work to try to build our collective intuitions about the structure, specifically by trying to understand how the multiple heads that make up the aforementioned multi-head attention divide up importance and specialize function. \n\nAs a quick orientation, attention works by projecting each value in the sequence into query, key, and value vectors. Then, each element in the sequence creates its next-layer value by calculating a function of query and key (typically dot product) and putting that in a softmax against the query results with every other element. This weighting distribution is then used as the weights of a weighted average, combining the values together. By default this is a single operation, with a single set of projection matrices, but in the Transformer approach, they use multi-headed attention, which simply means that they learn independent parameters for multiple independent attention \u201cfilters\u201d, each of which can then notionally specialize to pull in and prioritize a certain kind of information. \nhttps://i.imgur.com/yuC91Ja.png\n\nThe high level theses of this paper are: \n- Among attention heads, there\u2019s a core of the most crucial and important ones, and then a long tail of heads that can be pruned (or, have their weight in the concatenation of all attention heads brought to nearly zero) and have a small effect on performance \n- It\u2019s possible and useful to divide up the heads according to the kinds of other tokens that it is most consistently pulling information from. The authors identify three: positional, syntactic, and rare.  Positional heads consistently (>90% of the time) put their maximum attention weight on the same position relative to the query word. Syntactic heads are those that recurringly in the same grammatical relation to the query, the subject to its verb, or the adjective to its noun, for example. Rare words is not a frequently used head pattern, but it is a very valuable head within the first layer, and will consistently put its highest weight on the lowest-frequency word in a given sentence. \n\nAn interesting side note here is that the authors tried at multiple stages in pruning to retrain a network using only the connections between unpruned heads, and restarting from scratch. However, in a effect reminiscent of the Lottery Ticket Thesis, retraining from scratch cannot get quite the same performance. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1905.09418"
    },
    "273": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1905.08233",
        "transcript": "https://i.imgur.com/JJFljWo.png\n\nThis paper follows in a recent tradition of results out of Samsung: in the wake of StyleGAN\u2019s very impressive generated images, it uses a lot of similar architectural elements, combined with meta-learning and a new discriminator framework, to generate convincing \u201ctalking head\u201d animations based on a small number of frames of a person\u2019s face. Previously, models that generated artificial face videos could only do so by training by a large number of frames of each individual speaker that they wanted to simulate. This system instead is able to generate video in a few-shot way: where they only need one or two frames of a new speaker to do convincing generation. \n\nThe structure of talking head video generation as a problem relies on the idea of \u201clandmarks,\u201d explicit parametrization of where the nose, the eyes, the lips, the head, are oriented in a given shot. The model is trained to be able to generate frames of a specified person (based on an input frame), and in a specific pose (based on an input landmark set). \n\nWhile the visual quality of the simulated video generated here is quite stunning, the most centrally impressive fact about this paper is that generation was only conditioned on a few frames of each target person. This is accomplished through a combination of meta-learning (as an overall training procedure/regime) and adaptive instance normalization, a way of dynamically parametrizing models that was earlier used in a StyleGAN paper (also out of the Samsung lab). Meta-learning works by doing simulated few-shot training iterations, where a model is trained for a small number of steps on a given \u201ctask\u201d (where here a task is a given target face), and then optimized on the meta-level to be able to get good test set error rates across many such target faces. \n\nhttps://i.imgur.com/RIkO1am.png\n\nThe mechanics of how this meta-learning approach actually work are quite interesting: largely a new application of existing techniques, but with some extensions and innovations worked in. \n- A convolutional model produces an embedding given an input image and a pose. An average embedding is calculated by averaging over different frames, with the hopes of capturing information about the video, in a pose-independent way. This embedding, along with a goal set of landmarks (i.e. the desired facial expression of your simulation)  is used to parametrize the generator, which is then asked to determine whether the generated image looks like it came from the sequence belonging to the target face, and looks like it corresponds to the target pose \n- Adaptive instance normalization works by having certain parameters of the network (typically, per the name, post-normalization rescaling values) that are dependent on the properties of some input data instance. This works by training a network to produce an embedding vector of the image, and then multiplying that embedding by per-layer, per-filter projection matrices to obtain new parameters. This is in particular a reasonable thing to do in the context of conditional GANs, where you want to have parameters of your generator be conditioned on the content of the image you\u2019re trying to simulate \n- This model structure gives you a natural way to do few-shot generation: you can train your embedding network, your generator, and your projection matrices over a large dataset, where they\u2019ve hopefully learned how to compress information from any given target image, and generate convincing frames based on it, so that you can just pass in your new target image, have it transformed into an embedding, and have it contain information the rest of the network can work with\n- This model uses a relatively new (~mid 2018) formulation of a conditional GAN, called the projection discriminator. I don\u2019t have time to fully explain this here, but at a high level, it frames the problem of a discriminator determining whether a generated image corresponds to a given conditioning class by projecting both the class and the image into vectors, and calculating a similarity-esque dot product.\n- During few-shot application of this model, it can get impressively good performance without even training on the new target face at all, simply by projecting the target face into an embedding, and updating the target-specific network parameters that way. However, they do get better performance if they fine-tune to a specific person, which they do by treating the embedding-projection parameters as an initialization, and then taking a few steps of gradient descent from there\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1905.08233"
    },
    "274": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1905.07830",
        "transcript": "[Machine learning is a nuanced, complicated, intellectually serious topic...but sometimes it\u2019s refreshing to let ourselves be a bit less serious, especially when it\u2019s accompanied by clear, cogent writing on a topic. This particular is a particularly delightful example of good-natured silliness - from the dataset name HellaSwag to figures containing cartoons of BERT and ELMO representing language models - combined with interesting science.] \n\nhttps://i.imgur.com/CoSeh51.png\n\nThis paper tackles the problem of natural language comprehension, which asks: okay, our models can generate plausible looking text, but do they actually exhibit what we would consider true understanding of language? One natural structure of task for this is to take questions or \u201ccontexts\u201d, and, given a set of possible endings or completion, pick the correct one. Positive examples are relatively easy to come by: adjacent video captions and question/answer pairs from WikiHow are two datasets used in this paper. However, it\u2019s more difficult to come up with *negative* examples. Even though our incorrect endings won\u2019t be a meaningful continuation of the sentence, we want them to be \u201cclose enough\u201d that we can feel comfortable attributing a model\u2019s ability to pick the correct answer as evidence of some meaningful kind of comprehension. As an obvious failure mode, if the alternative multiple choice options were all the same word repeated ten times, that would be recognizable as being not real language, and it would be easy for a model to select the answer with the distributional statistics of real language, but it wouldn\u2019t prove much. Typically failure modes aren\u2019t this egregious, but the overall intuition still holds, and will inform the rest of the paper: your ability to test comprehension on a given dataset is a function of how contextually-relevant and realistic your negative examples are.\n\nPrevious work (by many of the same authors as are on this paper), proposed a technique called Adversarial Filtering to try to solve this problem. In Adversarial Filtering, a generative language model is used to generate possible many endings conditioned on the input context, to be used as negative examples. Then, a discriminator is trained to predict the correct ending given the context. The generated samples that the discriminator had the highest confidence classifying as negative are deemed to be not challenging enough comparisons, and they\u2019re thrown out and replaced with others from our pool of initially-generated samples. Eventually, once we\u2019ve iterated through this process, we have a dataset with hopefully realistic negative examples. The negative examples are then given to humans to ensure none of them are conceptually meaningful actual endings to the sentence. The dataset released by the earlier paper, which used as it\u2019s generator a relatively simple LSTM model, was called Swag. \n\nHowever, the authors came to notice that the performance of new language models (most centrally BERT) on this dataset might not be quite what it appears: its success rate of 86% only goes down to 76% if you don\u2019t give the classifier access to the input context, which means it can get 76% (off of a random baseline of 25%, with 4 options) simply by evaluating which endings are coherent as standalone bits of natural language, without actually having to understand or even see the context. Also, shuffling the words in the words in the possible endings had a similarly small effect: the authors are able to get BERT to perform at 60% accuracy on the SWAG dataset with no context, and with shuffled words in the possible answers, meaning it\u2019s purely selecting based on the distribution of words in the answer, rather than on the meaningfully-ordered sequence of words. \n\nhttps://i.imgur.com/f6vqJWT.png\n\nThe authors overall conclusion with this is: this adversarial filtering method is only as good as the generator, and, more specifically, the training dynamic between the generator that produces candidate endings, and the discriminator that filters them. If the generator is too weak, the negative examples can be easily detected as fake by a stronger model, but if the generator is too strong, then the discriminator can\u2019t get good enough to usefully contribute by weeding samples out. They demonstrate this by creating a new version of Swag, which they call HellaSwag (for the expected acronym-optimization reasons), with a GPT generator rather than the simpler one used before: on this new dataset, all existing models get relatively poor results (30-40% performance). However, the authors\u2019 overall point wasn\u2019t \u201cwe\u2019ve solved it, this new dataset is the end of the line,\u201d but rather a call in the future to be wary, and generally aware that with benchmarks like these, especially with generated negative examples, it\u2019s going to be a constantly moving target as generation systems get better. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1905.07830"
    },
    "275": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1804-00168",
        "transcript": "This paper out of DeepMind used a Google StreetView dataset and set out to train a network capable of navigating to a given goal destination, without knowing where it was on any birds-eye map, and with its only input being photographic viewpoint images of its current location and orientation. This was done through a framework of reinforcement learning, where the model is conditioned on a representation of its goal, and given the image features of its current view of the world, and has to take actions like \u201cturn left,\u201d \u201cturn sharply left\u201d, \u201cgo forward\u201d, etc, in order to navigate. Rather than lat-long, goals are specified in city-specific ways, in terms of the distance between the goal position and a reference set of landmarks. I don\u2019t entirely understand the motivation behind this approach; the authors say it\u2019s more scalable, but it wasn\u2019t obvious to me why that would be the case. \n\nhttps://i.imgur.com/V3UATsK.png\n\n- The authors construct different architectures that combine these two fundamental pieces of input data - current image and the goal you\u2019re trying to reach - in different ways. \nIn the simplest model, called GoalNav, there\u2019s a single LSTM that combines the goal information with the output of a convolutional encoder processing images of your current viewpoint.\n- In the next most complex, CityNav, there are two LSTMs: one for processing your goal, and the other for combining the output of the goal network with your convolutional inputs, in order to decide on an action. Notionally, this separation of tasks corresponds to \u201cfigure out what absolute to go in, given your goal\u201d, and \u201cfigure out how to go in that absolute direction from where you are now\u201d. As a way to support training, the goal network is trained with an auxiliary loss function where it needs to predict how far its current orientation is from North. Note that this does pass some amount of information about current location into the model (since the network gets to know its actual orientation relative to true north), but this is only available during training, with the hope that the model will have gotten good enough at predicting orientation to perform well.  \n- The final model, similar to above, is called MultiCityNav, and is explicitly designed for transfer learning. Instead of training multiple cities on a single shared network, only the convolutional encoder and policy network (the \u201chow do I go in the absolute direction needed to reach my goal\u201d parts) are shared between cities, and the goal processing LSTM (the \u201cwhich direction should I be going in\u201d part) is re-trained per city. This is designed to allow for transfer in the parts of learning you would expect to generalize, but allow the network to learn a city-specific approach for converting between goal specifications (in terms of city landmarks) and direction. \n\nIn order to get over the fact that reward in this setting is very sparse (i.e. you only get reward when you reach the goal), the authors (1) train in a curriculum fashion, starting with tasks very nearby the model\u2019s starting point, and gradually getting longer, and (2) add a small amount of reward shaping, where you get rewarded for moving in the direction of the goal, but only if you\u2019re within 200m of it. This last is a bit of a concession on the realism front, and the authors say as much, but it\u2019s just quite hard to train RL with purely dense rewards, and it makes sense that reward shaping would help here. \n\nUltimately, they were able to get performance (in terms of goal-reaching rewards) around \u00be as strong as an Oracle model, who had access to the full map and could calculate the true shortest path.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1804.00168"
    },
    "276": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=zhou2019deconstructing",
        "transcript": "The Lottery Ticket Hypothesis is the idea that you can train a deep network, set all but a small percentage of its high-magnitude weights to zero, and retrain the network using the connection topology of the remaining weights, but only if you re-initialize the unpruned weights to the the values they had at the beginning of the first training. This suggests that part of the value of training such big networks is not that we need that many parameters to use their expressive capacity, but that we need many \u201cdraws\u201d from the weight and topology distribution to find initial weight patterns that are well-disposed for learning. \n\nThis paper out of Uber is a refreshingly exploratory experimental work that tries to understand the contours and contingencies of this effect. Their findings included: \n- The pruning criteria used in the original paper, where weights are kept according to which have highest final magnitude, works well. However, an alternate criteria, where you keep the weights that have increased the most in magnitude, works just as well and sometimes better. This makes a decent amount of sense, since it seems like we\u2019re using magnitude as a signal of \u201cdid this weight come to play a meaningful role during training,\u201d and so weights whose influence increased during training fall in that category, regardless of their starting point\nhttps://i.imgur.com/wTkNBod.png\n- The authors\u2019 next question was: other than just re-initialize weights to their initial values, are there other things we can do that can capture all or part of the performance effect? The answer seems to be yes; they found that the most important thing seems to be keeping the sign of the weights aligned with what it was at its starting point. As long as you do that, redrawing initial weights (but giving them the right sign), or re-setting weights to a correctly signed constant value, both work nearly as well as the actual starting values\n\nhttps://i.imgur.com/JeujUr3.png\n\n- Turning instead to the weights on the pruning chopping block, the authors find that, instead of just zero-ing out all pruned weights, they can get even better performance if they zero the weights that moved towards zero during training, and re-initialize (but freeze) the weights that moved away from zero during training. The logic of the paper is \u201cif the weight was trying to move to zero, bring it to zero, otherwise reinitialize it\u201d. This performance remains high at even lower levels of training than does the initial zero-masking result \n- Finally, the authors found that just by performing the masking (i.e. keeping only weights with large final values), bringing those back to their values, and zeroing out the rest, *and not training at all*, they were able to get 40% test accuracy on MNIST, much better than chance. If they masked according to \u201clarge weights that kept the same sign during training,\u201d they could get a pretty incredible 80% test accuracy on MNIST. Way below even simple trained models, but, again, this model wasn\u2019t *trained*, and the only information about the data came in the form of a binary weight mask \n\nThis paper doesn\u2019t really try to come up with explanations that wrap all of these results up neatly with a bow, and I really respect that. I think it\u2019s good for ML research culture for people to feel an affordance to just run a lot of targeted experiments aimed at explanation, and publish the results even if they don\u2019t quite make sense yet. I feel like on this problem (and to some extent in machine learning generally), we\u2019re the blind men each grabbing at one part of an elephant, trying to describe the whole. Hopefully, papers like this can bring us closer to understanding strange quirks of optimization like this one \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.01067"
    },
    "277": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.05101",
        "transcript": "A few years ago, a paper came out demonstrating that adaptive gradient methods (which dynamically scale gradient updates in a per-parameter way according to the magnitudes of past updates) have a tendency to generalize less well than non-adaptive methods, even they adaptive methods sometimes look more performant in training, and are easier to hyperparameter tune. The 2017 paper offered a theoretical explanation for this fact based on Adam learning less complex solutions than SGD; this paper offers a different one, namely that Adam performs poorly because it is typically implemented alongside L2 regularization, which has importantly different mechanical consequences than it does in SGD. \n\nSpecifically, in SGD, L2 regularization, where the loss includes both the actual loss and a L2 norm of the weights, can be made equivalent to weight decay, by choosing the right parameters for each. (see proof below). \n\nhttps://i.imgur.com/79jfZg9.png\nHowever, for Adam, this equivalence doesn\u2019t hold. This is true because, in SGD, all the scaling factors are just constants, and for each learning rate value and regularization parameter, a certain weight decay parameter is implied by that. However, since Adam scales its parameter updates not by a constant learning rate but by a matrix, it\u2019s not possible to pick other hyperparameters in a way that could get you something similar to constant-parameter weight decay. \n\nTo solve this, the authors suggest using an explicit weight decay term, rather than just doing implicit weight decay via L2 regularization. This is salient because the L2 norm is added to the *loss function*, and it makes up part of the gradient update, and thus gets scaled down by Adam by the same adaptive mechanism that scales down historically large gradients. When weight decay is moved outside of the form of being a norm calculation inside a loss function, and just something applied to the final update but not actually part of the adaptive scaling calculation, the authors find that 1) Adam is able to get comparable performance on image and sequence tasks (where it has previously had difficult), and 2) that even for SGD, where it was possible to find a optimal parameter setting to reproduce weight decay, having an explicit and decoupled weight decay parameter made parameters that were previously dependent on one another in their optimal values (regularization and learning rate) more independent. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.05101"
    },
    "278": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1905.01320",
        "transcript": "Meta learning, or, the idea of training models on some distribution of tasks, with the hope that they can then learn more quickly on new tasks because they have \u201clearned how to learn\u201d similar tasks, has become a more central and popular research field in recent years. Although there is a veritable zoo of different techniques (to an amusingly literal degree; there\u2019s an emergent fad of naming new methods after animals), the general idea is: have your inner loop consist of training a model on some task drawn from a distribution over tasks (be that maze learning with different wall configurations, letter identification from different languages, etc), and have the outer loop that updates some structural part of your model be based on improving generalization error on each task within the distribution. It\u2019s been demonstrated that meta-learned systems can in fact learn more quickly (at least when their tasks are \u201cin distribution\u201d relative to the distribution they were trained on, which is an important point to be cognizant of), but this paper is less interested with how much better or faster they\u2019re learning, and more interested in whether there are qualitative differences in the way normal learning systems and meta-trained learning systems go about learning a new task. \n\nThe author (oddly for DeepMind, which typically goes in for super long author lists, there\u2019s only the one on this paper) goes about this by studying simple learning tasks where it\u2019s easier for us to introspect into what each model is learning over time. \n\nhttps://i.imgur.com/ceycq46.png\n\nIn the first test, he looks at linear regression in a simple setting: where for each individual \u201ctask\u201d data is generated according a known true weight matrix (sampled from a prior over weight matrices), with some noise added in. Given this weight matrix, he takes the singular value decomposition (think: PCA), and so ends up with a factorized representation of the weights, where higher eigenvalues on the factors, or \u201cmodes\u201d, represent that those factors represent larger-scale patterns that explain more variance, and lower eigenvalues are smaller scale refinements on top of that. He can apply this same procedure to the weights the network has learned at any given point in training, and compare, to see how close the network is to having correctly captured each of these different modes. When normal learners (starting from a raw initialization) approach the task, they start by matching the large scale (higher eigenvalue) factors of variation, and then over the course of training improve performance on the higher-precision factors. By contrast, meta learners, in addition to learning faster, also learn large scale and small scale modes at the same rate.  Similar analysis was performed and similar results found for nonlinear regression, where instead of PCA-style components, the function generating data were decomposed into different Fourier frequencies, and the normal learner learned the broad, low-frequency patterns first, where the meta learner learned them all at the same rate. \n\nThe paper finds intuition for this by showing that the behavior of the meta learners matches quite well against how a Bayes-optimal learner would update on new data points, in the world where that learner had a prior over the data-generating weights that matched the true generating process. So, under this framing, the process of meta learning is roughly equivalent to your model learning a prior correspondant with the task distribution it was trained on. This is, at a high level, what I think we all sort of thought was happening with meta learning, but it\u2019s pretty neat to see it laid out in a small enough problem where we can actually validate against an analytic model. \n\nA bit of a meta (heh) point: I wish this paper had more explanation of why the author chose to use the specific eigenvalue-focused metrics of progression on task learning that he did. They seem reasonable, but I\u2019d have been curious to see an explication of what is captured by these, and what might be captured by alternative metrics of task progress. \n\n(A side note: the paper also contained a reinforcement learning experiment, but I both understood that one less well and also feel like it wasn\u2019t really that analogous to the other tests) \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1905.01320"
    },
    "279": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=berthelot2019mixmatch",
        "transcript": "As per the \u201cholistic\u201d in the paper title, the goal of this work is to take a suite of existing work within semi-supervised learning, and combine many of its ideas into one training pipeline that can (with really impressive empirical success) leverage the advantages of those different ideas. \n\nThe core premise of supervised learning is that, given true-label training signal from a small number of labels, you can leverage large amounts of unsupervised data to improve your model. A central intuition of many of these methods is that, even if you don\u2019t know the class of a given sample, you know it *has* a class, and you can develop a loss by pushing your model to predict the class for an example and a modified or perturbed version of that example, since, if you have a prior belief that that modification should not change your true class label, then your unlabeled data point should have the same class prediction both times. Entropy minimization is built off similar notions: although we don\u2019t know a point\u2019s class, we know it must have one, and so we\u2019d like our model to make a prediction that puts more of its weight on a single class, rather than be spread out, since we know the \u201ccorrect model\u201d will be a very confident prediction of one class, though we don\u2019t know which it is. These methods will give context and a frame of mind for understanding the techniques merged together into the MixMatch approach. \n\nAt its very highest level, MixMatch\u2019s goal is to take in a dataset of both labeled and unlabeled data, and produce a training set of inputs, predictions, and (occasionally constructed or modified labels) to calculate a model update loss from. \nhttps://i.imgur.com/6lHQqMD.png\n- First, for each unlabeled example in the dataset, we produce K different augmented versions of that image (by cropping it, rotating it, flipping it, etc). This is in the spirit of the consistency loss literature, where you want your model to make the same prediction across augmentations\n- Do the same augmentation for each labeled example, but only once per input, rather than k times \n- Run all of your augmented examples through your model, and take the average of their predictions. This is based on the idea that the average of the predictions will be a lower variance, more stable pseudo-target to pull each of the individual predictions towards. Also in the spirit of making something more shaped like a real label, they undertake a sharpening step, turning down the temperature of the averaged distribution. This seems like it would have the effect of more confidently pulling the original predictions towards a single \u201cbest guess\u201d label \n- At this point, we have a set of augmented labeled data, with a true label, and also a set of augmented unlabeled data, with a label based off of an averaged and sharpened best guess from the model over different modifications. At this point, the pipeline uses something called \u201cMixUp\u201d (on which there is a previous paper, so I won\u2019t dive into it too much here), which takes pairs of data points, calculates a convex combination of the inputs, runs it through the model, and uses as the loss-function target a convex combination of the outputs. So, in the simple binary case, if you have a positive and negatively labeled image and sample a combination parameter of 0.75, you have an image that is 0.75 positive, 0.25 negative, and the new label that you\u2019re calculating cross entropy loss against is 0.75. \n- MixMatch generates pairs for its MixUp calculation by mixing (heh) labeled and unlabeled data together, and pairing each labeled and unlabeled pair with one observation from the merged set. \nAt this point, we have combined inputs, and we have combined labels, and we can calculate loss between them \n\nWith all of these methods combined, this method takes the previous benchmark of 38% error, for a CIFAR dataset with only 250 labels, and drops that to 11%, which is a pretty astonishing improvement in error rate. After performing an ablation study, they find that MixUp itself, temperature sharpening, and calculating K>1 augmentations of unlabeled data rather than K=1 are the strongest value-adds; it doesn\u2019t appear like there\u2019s that much difference that comes from mixing between unlabeled and labeled for the MixUp pairs.  \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.02249"
    },
    "280": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1904-01033",
        "transcript": "This paper blends concepts from variational inference and hierarchical reinforcement learning, learning skills or \u201coptions\u201d out of which master policies can be constructed, in a way that allows for both information transfer across tasks and specialization on any given task. \n\nThe idea of hierarchical reinforcement learning is that instead of maintaining one single policy distribution (a learned mapping between world-states and actions), a learning system will maintain multiple simpler policies, and then learn a meta-policy for transitioning between these object-level policies. The hope is that this setup leads to both greater transparency and compactness (because skills are compartmentalized), and also greater ability to transfer across tasks (because if skills are granular enough, different combinations of the same skills can be used to solve quite different tasks). \n\nThe differentiating proposal of this paper is that, instead of learning skills that will be fixed with respect to the master, task-specific policy, we instead learning cross-task priors over different skills, which can then be fine-tuned for a given specific task. Mathematically, this looks like a reward function that is a combination of (1) actual rewards on a trajectory, and (2) the difference in the log probability of a given trajectory under the task-specific posterior and under the prior. \n\nhttps://i.imgur.com/OCvmGSQ.png\n\nThis framing works in two directions: it allows a general prior to be pulled towards task-specific rewards, to get more specialized value, but it also pulls the per-task skill towards the global prior. This is both a source of transfer knowledge and general regularization, and also an incentive for skills to be relatively consistent across tasks, because consistent posteriors will be more locally clustered around their prior. The paper argues that one advantage of this is a symmetry-breaking effect, avoiding a local minimum where two skills are both being used to solve subtask A, and it would be better for one of them to specialize on subtask B, but in order to do so the local effect would be worse performance of that skill on subtask A, which would be to the overall policy\u2019s detriment because that skill was being actively used to solve that task. Under a prior-driven system, the model would have an incentive to pick one or the other of the options and use that for a given subtask, based on whichever\u2019s prior was closest in trajectory-space. \n\nhttps://i.imgur.com/CeFQ9PZ.png\n\nOn a mechanical level, this set of priors is divided into a few structural parts: \n1) A termination distribution, which chooses whether to keep drawing actions from the skill/subpolicy you\u2019re currently on, or trade it in for a new one. This one has a prior set at a Bernoulli distribution with some learned alpha \n2) A skill transition distribution,  which chooses, conditional on sampling a \u201cterminate\u201d, which skill to switch to next. This has a prior of a uniform distribution over skills, which incentivizes the learning system to not put all its sampling focus on one policy too early \n3) A distribution of actions given a skill choice, which, as mentioned before, has both a cross-task prior and a per-task learned posterior\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1904.01033"
    },
    "281": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1810-09536",
        "transcript": "This paper came on my radar after winning Best Paper recently at ICLR, and all in all I found it a clever way of engineering a somewhat complicated inductive bias into a differentiable structure. The empirical results weren\u2019t compelling enough to suggest that this structural shift made a regime-change difference in performing, but it does seem to have some consistently stronger ability to do syntactic evaluation across large gaps in sentences. \n\nThe core premise of this paper is that, while language is to some extent sequence-like, it is in a more fundamental sense tree-like: a recursive structure of modified words, phrases, and clauses, aggregating up to a fully complete sentence. In practical terms, this cashes out to parse trees, labels akin to the sentence diagrams that you or I perhaps did once upon a time in grade school. \n\nhttps://i.imgur.com/GAJP7ji.png\n\nGiven this, if you want to effectively model language, it might be useful to have a neural network structure explicitly designed to track where you are in the tree. To do this, the authors of this paper use a clever activation function scheme based on the intuition that you can think of jumping between levels of the tree as adding information to the stack of local context, and then removing that information from the stack when you\u2019ve reached the end of some local phrase. In the framework of a LSTM, which has explicit gating mechanisms for both \u201cforgetting\u201d (removing information from cell memory) and input (adding information to the representation within cell memory) this can be understood as forcing a certain structure of input and forgetting, where you have to sequentially \u201cclose out\u201d or add nodes as you move up or down the tree. \n\n\nTo represent this mathematically, the authors use a new activation function they developed, termed cumulative max or cumax. In the same way that the softmax is a differentiable (i.e. \u201csoft\u201d) version of an argmax, the cumulative max is a softened version of a vector that has zeros up to some switch point k, and ones thereafter. If you had such a vector as your forget mask, then \u201cclosing out\u201d a layer in your tree would be equivalent to shifting the index where you switch from 0 to 1 up by one, so that a layer that previously had a \u201cremember\u201d value of 1.0 now is removing its content from the stack.  However, since we need to differentiate, this notional 0/1 vector is instead represented as a cumulative sum of a softmax, which can be thought of as the continuous-valued probability that you\u2019ve reached that switch-point by any given point in the vector. Outside of the abstractions of what we\u2019re imagining this cumax function to represent, in a practical sense, it does strictly enforce that you monotonically remember or input more as you move along the vector. This has the practical fact that the network will be biased towards remembering information at one end of the representation vector for longer, meaning it could be a useful inductive bias around storing information that has a more long-term usefulness to it. One advantage that this system has over a previous system that, for example, had each layer of the LSTM operate on a different forgetting-decay timescale, is that this is a soft approximation, so, up to the number of neurons in the representation, the model can dynamically approximate whatever number of tree nodes it likes, rather than being explicitly correspondent with the number of layers. \n\nBeyond being a mathematically clever idea, the question of whether it improves performance is a little mixed. It does consistently worse at tasks that require keeping track of short term dependency information, but seems to do better at more long-term tasks, although not in a perfectly consistent or overly dramatic way. My overall read is that this is a neat idea, and I\u2019m interested to see if it gets built on, as well as interested to see later papers that do some introspective work to validate whether the model is actually using this inductive bias in the tree-like way that we\u2019re hoping and imagining it will. \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1810.09536"
    },
    "282": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1905.03670",
        "transcript": "It\u2019s possible I\u2019m missing something here, but my primary response to reading this paper is just a sense of confusion: that there is an implicit presenting of an approach as novel, when there doesn\u2019t seem to me to be a clear mechanism that is changed from prior work. The premise of this paper is that self-supervised learning techniques (a subcategory of unsupervised learning, where losses are constructed based on reconstruction or perturbation of the original image) should be made into supervised learning by learning on a loss that is a weighted combination of the self-supervised loss and the supervised loss, making the overall method a semi-supervised one. \n\nThe self-supervision techniques that they identify integrating into their semi-supervised framework are: \n- Rotation prediction, where an image is rotated to one of four rotation angles, and then a classifier is applied to guess what angle \n- Exemplar representation invariance, where an imagenet is cropped, mirrored, and color-randomized in order to provide inputs, whose representations are then pushed to be closer to the representation for the unmodified image \n\nMy confusion is due to the fact that the I know that I\u2019ve read several semi-supervised learning papers that do things of this ilk (insofar as combining unsupervised and supervised losses together) and it seems strange to identify it as a novel contribution. That said, this paper does give an interesting overview of self-supervisation techniques, I found it valuable to read for that purpose. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1905.03670"
    },
    "283": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=ilyas2019adversarial",
        "transcript": "It didn\u2019t hit me how much this paper was a pun until I finished it, and in retrospect, I say, bravo. \n\nThis paper focuses on adversarial examples, and argues that, at least in some cases, adversarial perturbations aren\u2019t purely overfitting failures on behalf of the model, but actual features that generalize to the test set. This conclusion comes from a set of two experiments: \n\n- In one, the authors create a dataset that only contains what they call \u201crobust features\u201d. They do this by taking a classifier trained to be robust using adversarial training (training on adversarial examples), and doing gradient descent to modify the input pixels until the final-layer robust model activations of the modified inputs match the final layer activations when the unmodified inputs are passed in. Operating under the premise that features identified by a robust model are themselves robust, because by definition they don\u2019t change in the presence of an adversarial perturbation, creating a training set that matches these features means that you\u2019ve created some kind of platonic, robust version of the training set, with only robust features present. They then take this dataset, and train a new model on it, and show that it has strong test set performance, in both normal settings, and adversarial ones. This is not enormously surprising, since the original robust classifier performed well, but still interesting. \n\n- The most interesting and perhaps surprising experiment is where the authors create a dataset by taking normal images, and layering on top an adversarial perturbation. They then label these perturbed images with the label corresponding to the perturbation class, and train a model off of that. They then find that this model, which is trained on images which correspond to their labeled class only in their perturbation features, and not in the underlying visual features a human would recognize, achieves good test set performance under normal conditions. However, it performs poorly on adversarial perturbations of the test set. \n\nhttps://i.imgur.com/eJQXb0i.png\nOverall, the authors claim that the perturbations that are \u201ctricking\u201d models are features that can genuinely provide some amount of test set generalization, due to real but unintuitive regularities in the data, but that these features are non-robust, in that small amounts of noise can cause them to switch sign. \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.02175"
    },
    "284": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/WilsonRSSR17",
        "transcript": "In modern machine learning, gradient descent has diversified into a zoo of subtly distinct techniques, all designed, analytically, heuristically, or practically, to ease or accelerate our model\u2019s path through multidimensional loss space.  A solid contingent of these methods are Adaptive Gradient methods, which scale the size of gradient updates according to variously calculated historical averages or variances of the vector update, which has the effect of scaling down the updates along feature dimensions that have experienced large updates in the past. The intuition behind this is that we may want to effectively reduce the learning rate (by dividing by a larger number) along dimensions where there have been large or highly variable updates. These methods are commonly used because, as the name suggests, they update to the scale of your dataset and particular loss landscape, avoiding what might otherwise be a lengthy process of hyperparameter tuning. But this paper argues that, at least on a simplified problem, adaptive methods can reach overly simplified and overfit solutions that generalize to test data less well than a non-adaptive, more standard gradient descent method. \n\nThe theoretical core of the paper is a proof showing limitations of the solution reached by adaptive gradient on a simple toy regression problem, on linearly separable data. It\u2019s a little dodgy to try to recapitulate a mathematical proof in verbal form, but I\u2019ll do my best, on the understanding that you should really read the fully thing to fully verify the logic. The goal of the proof is to characterize the solution weight vector learned by different optimization systems. In this simplified environment, a core informational unit of your equations is X^T(y), which (in a world where labels are either -1 or 1), goes through each feature, and for each feature, takes a dot product between that feature vector (across examples) and the label vector, which has the effect of adding up a positive sum of all the feature values attached to positive examples, and then subtracting out (because of the multiply by -1) all the feature values attached to positive examples. When this is summed, we get a per feature value that will be positive if positive values of the feature tend to indicate positive labels, and negative if the opposite is true, in each case with a magnitude relating to the strength of that relationship. The claim made by the paper, supported by a lot of variable transformations, is that the solution learned by Adaptive Gradient methods reduces to a sign() operation on top of that vector, where magnitude information is lost. This happens because the running gradients that you divide out happen to correspond to the absolute value of this vector, and dividing a vector (which would be the core of the solution in the non-adaptive case) by its absolute value gives you a simple sign. The paper then goes on to show that this edge case can lead to cases of pathological overfitting in cases of high feature dimensionality relative to data points. (I wish I could give more deep insight on why this is the case, but I wasn\u2019t really able to translate the math into intuition, outside of this fact of scaling by gradient magnitudes having the effect of losing potentially useful gradient information.\n\nThe big question from all this is...does this matter? Does it matter, in particular, beyond a toy dataset, and an artificially simple problem? The answer seems to be a pretty strong maybe. The authors test adaptive methods against hyperparameter-optimized SGD and momentum SGD (a variant, but without the adaptive aspects), and find that, while adaptive methods often learn more quickly at first, SGD approaches pick up later in training, first in terms of test set error at a time when adaptive methods\u2019 training set error still seems to be decreasing, and later even in training set error. So there seems to be evidence that solutions learned by adaptive methods generalize worse than ones learned by SGD, at least on some image recognition and language-RNN models. (Though, interestingly, RMS-Prop comes close to the SGD test set levels, doing the best out of the adaptive methods). Overall, this suggests to me that doing fully hyperparameter optimized SGD might be a stronger design choice, but that adaptive methods retain popularity because of their (very appealing, practically) lack of need for hyperparameter tuning to at least to a *reasonable* job, even if their performance might have more of a ceiling than that of vanilla SGD. \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.08292"
    },
    "285": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1802-06070",
        "transcript": "[I do occasionally wonder if people will look back on the \u201cIs All You Need\u201d with genuine confusion in a few years. \u201cReally\u2026all you need?\u201d]\n\nThis paper merges the ideas of curiosity-based learning and hierarchical reinforcement learning, to propose an architecture for learning distinctive skills based solely on an incentive to make those skills distinguishable from one another and relatively internally random, rather than because they\u2019re directly useful in achieving some reward. \n\nThe notion of hierarchical reinforcement learning is that, instead of learning a single joint policy, we learn some discrete number of subpolicies, and then treat the distribution over those subpolicies as you would a distribution over actions in a baseline RL policy. In order to achieve a reward, a model jointly optimizes the action distribution of the subpolicies, and also the distribution over subpolicies. One issue with this approach, which is raised by this paper (though I don\u2019t really have strong enough domain background here to know how much of a problem this is in practice) is that this joint optimization process means that, early in the process, we choose subpolicies that are doing the best, and sample more from and thus improve those. This \u201cearly exploitation\u201d problem (in the explore vs exploit frame) means that we might not learn skills that would be valuable to know later on, but that don\u2019t give us any reward until we\u2019ve developed them further. \n\nTo address this, this paper proposes DIAYN, an algorithm which (1) samples discrete latent skill vectors according to a uniform, high-entropy prior, rather than according to how useful we think they are now, and (2) doesn\u2019t even have a direct notion of usefulness, but instead incentivizes shaping of skills to be more distinct from one another, in terms of the states that are visited by each skill\u2019s policy. The network then learns policies conditioned on each skill vector, and at each point operates according to whichever has been sampled. This idea of distinctiveness is encapsulated by saying \u201cwe want to have high mutual information between the states visited by a skill, and the discrete ID of that skill,\u201d or, in more practical terms, \u201cwe want to be able to train a discriminator to do a good job predicting which skill we\u2019re sampling from, based on the states it sees. (I swear, every time I read a paper where someone uses mutual information these days, it\u2019s actually a discriminator under the hood).\n\nhttps://i.imgur.com/2a378Bo.png\n\nThis incentivizes the model to take actions associated with each skill that will get it to states that are unlikely to occur in any of the existing skills. Depending on what set of observations you give the discriminator to work with, you can shape what axes your skills are incentivized to vary on; if you try to discriminate skills based solely on an agent\u2019s center of mass, you\u2019ll end up with policies that vary their center of mass more wildly. The paper shows that, at least on simple environments, agents can learn distinctive clusters of skills based on this objective. \n\nAn interesting analogy here is to unsupervised pretraining of e.g. large language models and other similar settings, where we first train a model without (potentially costly) explicit reward, and this gives us a starting point set of representations that allow us to reach good performance more quickly once we start training on supervised reward signal. There is some evidence that this pretraining effect could be captured by this kind of purely-exploratory approach, as suggested by experiments done to take the learned skills or subpolicies, hold them fixed, and train a meta-controller to pick subpolicies according to an external reward, where the \u201cpretrained\u201d policy reaches high reward more quickly. \n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.06070"
    },
    "286": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=burda2018exploration",
        "transcript": "Reward functions are a funny part of modern reinforcement learning: enormously salient from the inside, if you\u2019re coding or working with RL systems, yet not as clearly visible from the outside perspective, where we just see agents playing games in what seem to be human-like ways. Just seeing things from this angle, it can be easy to imagine that the mechanisms being used to learn are human-like as well. And, it\u2019s true that some of the Atari games being examined are cases where there is in fact a clear, explicit reward in the form of points, that human players would also be trying to optimize. But in most cases, the world isn\u2019t really in the habit of producing clear reward signals, and it definitely doesn\u2019t typically do so on time scales that account for most of the learning humans do. \n\nSo, it\u2019s generally hypothesized that in addition to updating on (sparse) environmental rewards, humans also operate according to certain pre-coded, possibly evolutionarily-engineered heuristics, of which one is curiosity. The intuition is: it sure seems like, especially early in life, humans learn by interacting with objects purely driven by curiosity, and we\u2019d love to somehow harness that same drive to allow our learning systems to function in environments lacking dense, informative reward signals. One such environment is the video game Montezuma\u2019s Revenge, which in addition to being amusingly difficult to search for, is a game with sparse, long-range rewards, on which typical reward-based agents have historically performed poorly, and on which this current paper focuses. \n\nA strong existing tradition of curiosity objectives focuses on incentivizing agents to be able to better predict the next observation, given the current observation and their action within it. Intuitively, by training such a network on historical observations, and giving agents a bonus according to that prediction\u2019s error on a given observation. The theory behind this is that if an agent isn\u2019t able to predict the observation-transition dynamics at a given state, that probably means it hasn\u2019t visited many nearby states, and so we want to incentivize it doing so to gain information. If this sounds familiar to the classic \u201cexplore vs exploit\u201d trade-off, it\u2019s very much a similar idea: in cases of clear reward, we should take the reward, but in cases of low or uncertain reward, there\u2019s value to exploration. \n\nOne difficulty of systems like the one described above is that they reward the agent for being in environments where the next observation is difficult to predict from the current one. And while that could describe novel states about which the agent needs to gain information, it can also describe states that are inherently stochastic; the canonical example being random static on a TV screen. The agent has a lot of trouble predicting the next observation because it\u2019s fundamentally non-deterministic to a greater degree than even the random-but-causal dynamics of most games. The proposed alternative of this paper is a little strange, but makes more sense in the context of responding to this stochasticity problem. The authors propose to create a random mapping, in the form of an initialized but untrained neural network, taking in observations and spitting out embedding vectors. Then, they incentivize their agent to go to places that have high prediction error on a network designed to predict these random embeddings. Since the output is just a function mapping, it\u2019s deterministic with respect to observations. The idea here is that if you\u2019ve seen observations similar to your current observation, you\u2019ll be more able to predict the corresponding embedding, even if there\u2019s no meaningful relationship that you\u2019re learning. \n\nhttps://i.imgur.com/Ds5gHDE.png\n\nThe authors found that this performed well on Montezuma\u2019s Revenge and Private Eye, but only middlingly-well on other environments. I\u2019m a bit torn on this paper overall. On one hand, it seems like a clever idea, and I\u2019m in general interested in seeing more work on curiosity. It does clearly seem to be capturing something that corresponds to novelty-seeking, and the agent trained using it explores a higher number of rooms than alternative options. On the other, I\u2019m a little skeptical of the fact that it only has consistent performance in two environments, and wish there had been more comparisons to simpler forms of observation similarity, since this really does just seem like a metric of \u201chow similar of observation vectors to this have you seen before\u201d. I find myself wondering if some sort of density modeling could even be effective here, especially if (as may be the case, I\u2019m unsure) the input observations are metadata rather than pixels. \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1810.12894"
    },
    "287": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1903-05168",
        "transcript": "Language seems obviously useful to humans in coordinating on complicated tasks, and, the logic goes, you might expect that if you gave agents in a multi-agent RL system some amount of shared interest, and the capacity to communicate, that they would use that communication channel to coordinate actions. This is particularly true in cases where some part of the environment is only visible to one of the agents. A number of papers in the field have set up such scenarios, and argued that meaningful communication strategies developed, mostly in the form of one agent sending a message to signal its planned action to the other agent before both act. This paper tries to tease apart the various quantitative metrics used to evaluate whether informative message are being sent, and tries to explain why they can diverge from each other in unintuitive ways. The experiments in the paper are done in quite simple environments, where there are simple one-shot actions and a payoff matrix, as well as an ability for the agents to send messages before acting. \n\nSome metrics identified by the paper are: \n- Speaker Consistency: There\u2019s high mutual information shown between the message a speaker sends, and what action it takes. Said another way, you could use a speaker\u2019s message to predict their action at a rate higher than random, because it contains information about the action \n- Heightened reward/task completion under communication: Fairly straightforward, this metric argues that informative communication happened when pairs of agents do better in the presence of communication channels than when they aren\u2019t available\n- Instantaneous coordination: Measures the mutual information between the message sent by agent A and the action of agent B, in a similar way to Speaker Consistency.\n\nThis work agrees that it\u2019s important to measure the causal impact of messages on other-agent actions, but argues that instantaneous communication is flawed because the mutual information metric between messages and response actions doesn\u2019t properly condition on the state of the game under whcih the message is being sent. Even if you successfully communicate your planned action to me, the action I actually take in response will be conditioned on my personal payoff matrix, and may average out to seeming unrelated or random if you take an expectation over every possible state the message could be recieved in. Instead, they suggest doing an explicit causal causal approach, where for each configuration of the game (different payoff matrix), they sample different messages, and calculate whether you see messages driving more consistent actions when you condition on other factors in the game. \n\nAn interesting finding of this paper is that, at least in these simple environments, you\u2019re able to find cases where there is Speaker Consistency (SC; messages that contain information about the speaker\u2019s next action), but no substantial Causal Influence of Communication (CIC). This may seem counterintuitive, since, why would you as an agent send a message containing information about your action, if not because you\u2019re incentivized to communicate with the other agent? It seems like the answer is that it\u2019s possible to have this kind of shared information *on accident,* as a result of the shared infrastructure between the action network and the messaging network. Because both use a shared set of early-layer representations, you end up having one contain information about the other as an incidental fact; if the networks are fully separated with no shared weights, the Speaker Consistency values drop. \n\nAn important caveat to make here is that this paper isn\u2019t, or at least shouldn\u2019t be, arguing that agents in multi-agent systems don\u2019t actually learn communication. The environments used here are quite simple, and just might not plausibly be difficult enough to incentivize communication. However, it is a fair point that it\u2019s valuable to be precise in what exactly we\u2019re measuring, and test how that squares with what we actually care about in a system, to try to avoid cases like these where we may be liable to be led astray by our belief about how the system *should* be learning, rather than how it actually is \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.05168"
    },
    "288": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1903-01611",
        "transcript": "In 2018, a group including many of the authors of this updated paper argued for a theory of deep neural network optimization that they called the \u201cLottery Ticket Hypothesis\u201d. It framed itself as a solution to what was otherwise a confusing seeming-contradiction: that you could prune or compress trained networks to contain a small percentage of their trained weights without loss of performance, but also that if you tried to train a comparably small network (comparable to the post-training pruned network) from initialization, you wouldn\u2019t be able to achieve similar performance. They showed that, at least for some set of tasks, you could in fact train a smaller network to equivalent performance, but only if you kept the same connectivity patterns as in the pruned network, and if you re-initialized those neurons to the same values they were initialized at during the initial training. These lucky initialization patterns are the lottery tickets being referred to in the eponymous hypothesis: small subnetworks of well-initialized weights that are set up to be able to train well. This paper assesses whether and under what conditions the LTH holds on larger problems, and does a bit of a meta-analysis over different alternate theories in this space. \n\nOne such alternate theory, from Liu et al, proposes that, in fact, there is no value in re-initializing to the specific initial values, and that you can actually get away with random initialization if you keep the connectivity patterns of the pruned weights. The \u201cAt Scale\u201d paper compares the two methods over a wide range of pruning percentages, and convincingly shows that while random initialization with the same connectivity can perform well up to 80% of the weights being removed, after 80%, the performance of the random initialization drops, whereas the performance of the \u201cwinning ticket\u201d approach remains comparable with full network training up to 99% of the weights being pruned. This seems to provide support for the theory that there is value in re-initializing the weights to how they were, especially when you prune to very small subnetworks. \n\nhttps://i.imgur.com/9O2aAIT.png\n\nThe core of the current paper focuses on a difficulty in the original LTH paper: that the procedure of iterative pruning (train, then prune some weights, then train again) wasn\u2019t able to reliably find \u201cwinning tickets\u201d for deep networks of the type needed to solve ImageNet or CIFAR. To be precise, re-initializing pruned networks to their original values did no better than initializing them randomly in these networks. In order to actually get these winning tickets to perform well, the original authors had to do a somewhat arcane process of of starting the learning rate very small and scaling it up, called \u201cwarmup\u201d. Neither paper gave a clear intuition as to why this would be the case, but the updated paper found that they could avoid the need for this approach if, instead of re-initializing weights to their original value, they set them to the values they were at after some small number of iterations into training. They justify this by showing that performance under this new initialization is related to something they call \u201cstability to pruning,\u201d which measures how close the learned weights after re-initialization are to the original learned weights in the full model training. And, while the weights of deeper networks are unstable (by this metric) when first initialized, they become stable fairly early on. I was a little confused by this framing, since it seemed fairly tautological to me, since you\u2019re using \u201chow stably close are the weights to the original weights\u201d as a way to explain \u201cwhen can you recover performance comparable to original performance.\u201d This was framed as being a mechanistic explanation of why you can see a lottery ticket phenomenon to some extent, but only if you do a \u201clate reset\u201d to several iterations after initialization, but it didn\u2019t feel quite mechanistically satisfying enough to me. \n\nThat said, I think this is overall an intriguing idea, and I\u2019d love to see more papers discuss it. In particular, I\u2019d love to read more qualitative analysis about whether there are any notable patterns shared by \u201cwinning tickets\u201d. \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.01611"
    },
    "289": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1904-10509",
        "transcript": "The Transformer, a somewhat confusingly-named model structure that uses attention mechanisms to aggregate information for understanding or generating data, has been having a real moment in the last year or so, with GPT-2 being only the most well-publicized tip of that iceberg. It has lots of advantages: the obvious attractions of strong performance, as well as the ability to train in parallel across parts of a sequence, which RNNs can\u2019t do because of the need to build up and maintain state. However, a problematic fact about the Transformer approach is how it scales to large sequences of input data. Because attention is based on performing pairwise queries between each point in the data sequence and each other point, to allow for aggregation of information from places throughout the sequence, it scales as O(N^2), because every new element in the sequence needs to be queried by N other ones. This makes it resource-intensive to run transformer models on large architectures. \n\nThe Sparse Transformer design proposed in this OpenAI paper tries to cut down on this resource cost by loosening the requirement that, in every attention operation, each element directly pulls information from every other element. In this new system, each point doesn\u2019t get information about each other point in a single operation, but, having two operations such limited operations being chained in a row provides that global visibility. This is done in one of two ways. \n(1) The first, called the \u201cstrided\u201d version, performs two operations in a row, one masked attention that only looks at the last k timesteps (for example, the last 7), and then a second masked attention that only looks at every kth timestep. So, at the end of the second operation, each point has pulled information from points at checkpoints 7, 14, 21 steps ago, and each of these has pulled information from the window between it and its preceding checkpoint, giving visibility into a full global receptive frame in the course of two operations\n(2) The second, called the \u201cfixed\u201d version, uses a similar sort of logic, but instead of having the \u201cwindow accumulation points\u201d be defined in reference to the point doing the querying, you instead have fixed accumulation points responsible for gathering information from the windows around them. So, using the example given in the paper, if you imagine a window of size 128, and an \u201caccumulation points per window\u201d of 8, then the points in indices 120-128 (say) would have visibility into points 0-128. That represents the first operation, and in the second one, all other points in the sequence pull in information by querying the designated accumulation points for all the windows that aren\u2019t masked for it. \n\nThe paper argues that, between these two systems, the Strided system should work best when the data has some inherent periodicity, but I don\u2019t know that I particularly follow that intuition. I have some sense that the important distinction here is that in the strided case you have many points of accumulation, each with not much context, whereas in the fixed case you have very few accumulation points each with a larger window, but I don\u2019t know what performance differences exactly I\u2019d expect these mechanical differences to predict.  \n\nThis whole project of reducing access to global information seems initially a little counterintuitive, since the whole point of a transformer design, in some sense, was its ability to gain global context in a single layer, as opposed to a convnet needing multiple layers to build receptive field, or a RNN needing to maintain state throughout the sequence. However, I think this paper makes the most sense as a way of interpolating the space between something like a CNN and a full attention design, for the sake of efficiency. With a CNN, you have a fixed kernel, and so as your sequence gets longer, you need to add more and more layers in order for any given point to be able to incorporate into its representation information from the complete other side of the sequence. With a RNN, as your sequence gets longers, you pay the cost of needing to backpropogate state farther. So, by contrast, even though the Sparse Transformer seems to be giving up its signal advantage, it\u2019s instead just trading one constant number of steps to achieve global visibility (1), for another (2, in this paper, but conceptually could be more), but still in a way that\u2019s constant with respect to the length of the data. And, in exchange for this trade, they get very sparse, very masked operations, where many of the multiplications involved in these big query calculations can be ignored, making for faster computation speeds. \n\nOn the datasets tried, the Sparse Transformer increased speed, and in fact in I think all cases increased performance - not by much, the performance gain by itself isn\u2019t that dramatic, but in the context of expecting if anything worse performance as a result of limiting model structure, it\u2019s encouraging and interesting that it either stays about the same or possible improves. \n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1904.10509"
    },
    "290": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1904-12848",
        "transcript": "This paper focuses on taking advances from the (mostly heuristic, practical) world of data augmentation for supervised learning, and applying those to the unsupervised setting, as a way of inducing better performance in a semi-supervised environment (with many unlabeled points, and few labeled ones) \n\nData augmentation has been a mostly behind-the-scenes implementation detail in modern deep learning: minor modifications like shifting a dataset by a few pixels, rotating it slightly, or flipping it horizontally, to generate additional pseudoexamples for the model to train on. The core idea motivating such approaches is that the tactics of data augmentation are modifications that should not change the class label in a platonic, ground truth sense, which allows us to use them as training examples of the same class label as the original image from which the perturbations were made. In addition to just giving then network generically more data, this approach communicates to the network the specific kinds of modifications that can be made to an image and have it still be of the same class. \n\nThe Unsupervised Data Augmentation (UDA) tactic from this paper notices two things: \n(1) Within the sphere of supervised learning, there has been dataset-specific innovation in generating augmented data that will be particularly helpful to a given dataset. Inlanguage modeling, an example of this is having a sentence go into another language and back again through two well-trained translation networks, and use the resulting sentence as another example of the same class. For ImageNet, there\u2019s an approach called AutoAugment that uses reinforcement learning on a validation set to learn a policy of image operations (rotate, shear, change color) in order to increase validation accuracy. [I am a little confused about this as an approach, since I worry about essentially overfitting to the validation set. That said, I don\u2019t have time to delve specifically into the AutoAugment paper today, so I\u2019ll just leave this here as a caveat] \n\n(2) Within semi-supervised learning, there\u2019s a growing tendency to use consistency loss as a way of making use of unlabeled data. The basic idea of consistency loss is that, even if you don\u2019t know the class of a given datapoint, if you modify it in some small way, you can be confident that the model\u2019s prediction should be consistent between the point and its perturbation, even if you don\u2019t have knowledge of what the actual ground truth is. Often, systems like this have been designed using simple Gaussian noise on top of original unlabeled images. The key proposal of this paper is to substitute this more simplified perturbation procedure with the augmentation approaches being iterated on in supervised learning, since the goal of both - to modify inputs so as to capture ways in which they might differ but still be notionally of the same class - is nearly identical \n\nOn top of this core idea, the UDA paper also proposes an additional clever training tactic: in cases where you have many unlabeled examples and few labeled ones, you may need a large model to capture the information from the unlabeled examples, but this may result in overfitting on the labeled ones. To avoid this, they use an approach called \u201cTraining Signal Annealing,\u201d where at each point in training they remove from the loss calculation any examples that the model is particularly confident about: where the prediction of the true class is above some threshold eta. As training goes on, the network is gradually allowed to see more of the training signals. In this kind of a framework, the model can\u2019t overfit as easily, because once it starts getting the right answer on supervised examples, they drop out of the loss calculation. \n\nIn terms of empirical results, the authors find that in UDA they\u2019re able to improve on many semi-supervised benchmarks with quite small numbers of labeled examples. At one point, they use as a baseline a BERT model that was fine-tuned in an unsupervised way prior to its semi-supervised training, and show that their augmentation method can add value even on top of the value that the unsupervised pre-training adds. \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1904.12848"
    },
    "291": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1903-07227",
        "transcript": "The Magenta group at Google is a consistent source of really interesting problems for machine learning to solve, in the vein of creative generation of art and music,  as well as mathematically creative ways to solve those problem. In this paper, they tackle a new problem with some interesting model-structural implications: generating Bach chorales composed of polyphonic multi-instrument arrangements. On one layer, this is similar to music generation problems that have been studied before, in that generating a musically coherent sequence requires learning both local and larger-scale structure between time steps in the music sequence. However, an additional element here is that there\u2019s dependence of multiple instruments\u2019 notes on one another at a given time step, so, in addition to generating time steps conditional on one another, you ideally want to learn how to model certain notes in a given harmony conditional on the other notes already present there. \n\nUnderstanding the specifics of the approach was one of those scenarios where the mathematical arguments were somewhat opaque, but the actual mechanical description of the model gave a lot of clarity. I find this frequently the case with machine learning, where there\u2019s this strange set of dual incentives between the engineering impulse towards designing effective system, and the academic need to connect the approach to a more theoretical mathematical foundation.\n\nThe approach taken here has a lot in common with the autoregressive model structures used in PixelCNN or WaveNet. These are all based, theoretically speaking, on the autoregressive property of joint probability distributions, that they can sampled from by sampling first from the prior over the first variable (or pixel, or wave value), and then the second conditional on the first, then the third conditional on the first two, and so on. In practice, autoregressive models don\u2019t necessary condition on the *entire* previous rest of the input in generating a conditional distribution for a new point (for example, because they use a convolutional structure that doesn\u2019t have a receptive field big enough to reach back through the entire previous sequence), but they are based on that idea. A unique aspect of this model is that, instead of defining one specific conditional dependence relationship (where pixel J is conditioned on wave values J-5 through J, or some such), they argue that they instead learn conditional relationships over any possible autoregressive ordering of both time steps and instrument IDs. This is a bit of a strange idea, that, like I mentioned, is simplified by going through the mechanics. \n\n\nThe model works in a way strikingly similar to recent large scale language modeling: by, for each sample, masking some random subset of the tokens, and asking the model to predict the masked values given the unmasked ones. In this case, an interesting nuance is that the values to be masked are randomly sampled across both time step and instrument, such that in some cases you\u2019ll have a prior time step but no other instruments at your time step, or other instruments at your time step but no prior time steps to work from, and so on. The model needs to flexibly use various kinds of local context to predict the notes that are masked. (As an aside, in addition to the actual values, the network is given the actual 0/1 mask, so it can better distinguish between \u201c0, no information\u201d and \u201c0 because in the actual data sample there wasn\u2019t a pitch here\u201d.) The model refers to these unmasked points as \u201ccontext points\u201d. \n\nAn interesting capacity that this gives the model, and which the authors use as their sampling technique, is to create songs that are hybrids of existing chorales by randomly keeping some chunks and dropping out others, and using the model to interpolate through the missing bits. \n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.07227"
    },
    "292": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1902-10186",
        "transcript": "Attention mechanisms are a common subcomponent within language models, initially as a part of recurrent models, and more recently as their own form of aggregating information over sequences, independent from the recurrence structure. Attention works by taking as input some sequence of inputs, in the most typical case embedded representations of words in a sentence, and learning a distribution of weights over those representations, which allows the network to aggregate the representations, typically by taking a weighted sum. One effect of using an attention mechanism is that, for each instance being predicted, the network produces this weight distribution over inputs, which intuitively feels like it\u2019s the network demonstrating which input words were most important in constructing its decision. As a result, uses of attention have often been accompanied by examples that show attention distributions for examples, implicitly using them as a form of interpretability or model explanation. This paper has the goal of understanding whether attention distributions can be seen as a valid form of feature importance, and takes the position that they shouldn\u2019t be. At a high level, I think the paper makes some valid criticisms, but ultimately I didn\u2019t find the evidence it presented quite as strong as I would have liked. \n\nThe paper performs two primary analyses of the attention distributions produced by a trained LSTM model: \n\n(1) It calculates the level of correlation between the importance that would be implied by attention weights and the importance as calculated using more canonical gradient-based methods (generally things in the shape of \u201cwhich words contributed the most towards the prediction being what it was). It finds correlation values that range across random seeds, but are generally centered around 0.5. The paper frames this as a negative result, implying that, in the case where attention was a valid form of importance, the correlation with existing metrics would be higher. I definitely follow the intuition that you would expect there be a significant and positive correlation between methods in this class, but it\u2019s unclear to me what a priori reasoning chooses to draw the threshold on \u201csignificant\u201d in a way that makes 0.5 fall below it. It just feels like one of those cases where I could imagine someone showing the same plots and coming to a different interpretation, and it\u2019s not clear to me what criteria support one threshold of magnitude vs another \n\n\n(2) It measures how much it can permute the weights of an attention distribution, and have the prediction made by the network not change in a meaningful way. It does this both by random tests, and also by measuring the maximum adversarial perturbation: the farthest-away distribution (in terms of Jenson-Shannon distance) that still produces a prediction within some epsilon of the original prediction. There are a few concerns I have about this as an analysis. First off, it makes a bit of an assumption that attention can only be a valid form of explanation if it\u2019s a causal mechanism within the model. You could imagine that attention distributions still give you information about the internal state of the model, even if they are just reporting that state rather than directly influencing it. Secondly, it seems possible to me that you could get a relatively high Jenson-Shannon distance from an initial distribution just by permuting the indexes of the low-value weights, and shifting distributional weight between them in a way that doesn\u2019t fundamentally change what the network is primarily attending to. Even if this is not the case in this paper, I\u2019d love to see an example or some kind of quantitative measure showing that the J-S Shannon distances they demonstrate require a substantive change in weight priorities, rather than a trivial one. \n\nAnother general critique is that the experiments in this paper only focused on attention within a LSTM structure, where the embedding associated with each word isn\u2019t really strictly an embedding of that specific word, but also contains a lot of information about things before and after, because of the nature of a BiLSTM. So, there is some specificity in the embedding corresponding to just that word, but a lot less than in a pure attention model, like some being used in NLP these days, where you\u2019re learning an attention distribution over the raw, non-LSTM-ed representations. In this case, it makes sense that attention would be blurry, and not map exactly to our notions of which words are more important, since the word level representations are themselves already aggregations. I think it\u2019s totally fair to only focus on the LSTM case, but would prefer the paper scoped its claims in better accordance with its empirical results. \n\nI feel a bit bad: overall, I really approve of papers like this being done to put a critical empirical frame on ML\u2019s tendency to get conceptually ahead of itself. And, I do think that the evidentiary standard for \u201cprove that X metric isn\u2019t a form of interpretability\u201d shouldn\u2019t be that high, becuase on priors, I would expect most things not to be. I think that they may well be right in their assessment, I would just like a more surefooteded set of analyses and interpretation behind it. \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1902.10186"
    },
    "293": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1809.11044",
        "transcript": "One of the dominant narratives of the deep learning renaissance has been the value of well-designed inductive bias - structural choices that shape what a model learns. The biggest example of this can be found in convolutional networks, where models achieve a dramatic parameter reduction by having features maps learn local patterns, which can then be re-used across the whole image. This is based on the prior belief that patterns in local images are generally locally contiguous, and so having feature maps that focus only on small (and gradually larger) local areas is a good fit for that prior. This paper operates in a similar spirit, except its input data isn\u2019t in the form of an image, but a graph: the social graph of multiple agents operating within a Multi Agent RL Setting. In some sense, a graph is just a more general form of a pixel image: where a pixel within an image has a fixed number of neighbors, which have fixed discrete relationships to it (up, down, left, right), nodes within graphs have an arbitrary number of nodes, which can have arbitrary numbers and types of attributes attached to that relationship. \n\nThe authors of this paper use graph networks as a sort of auxiliary information processing system alongside a more typical policy learning framework, on tasks that require group coordination and knowledge sharing to complete successfully. For example, each agent might be rewarded based on the aggregate reward of all agents together, and, in the stag hunt, it might require collaborative effort by multiple agents to successfully \u201ccapture\u201d a reward. Because of this, you might imagine that it would be valuable to be able to predict what other agents within the game are going to do under certain circumstances, so that you can shape your strategy accordingly. \n\nThe graph network used in this model represents both agents and objects in the environment as nodes, which have attributes including their position, whether they\u2019re available or not (for capture-able objects), and what their last action was. As best I can tell, all agents start out with directed connections going both ways to all other agents, and to all objects in the environment, with the only edge attribute being whether the players are on the same team, for competitive environments. Given this setup, the graph network works through a sort of \u201cdiffusion\u201d of information, analogous to a message passing algorithm. At each iteration (analogous to a layer), the edge features pull in information from their past value and sender and receiver nodes, as well as from a \u201cglobal feature\u201d. Then, all of the nodes pull in information from their edges, and their own past value. Finally, this \u201cglobal attribute\u201d gets updated based on summations over the newly-updated node and edge information. (If you were predicting attributes that were graph-level attributes, this global attribute might be where you\u2019d do that prediction. However, in this case, we\u2019re just interested in predicting agent-level actions). \n\nhttps://i.imgur.com/luFlhfJ.png\n\nAll of this has the effect of explicitly modeling agents as entities that both have information, and have connections to other entities. One benefit the authors claim of this structure is that it allows them more interpretability: when they \u201cplay out\u201d the values of their graph network, which they call a Relational Forward Model or RFM, they observe edge values for two agents go up if those agents are about to collaborate on an action, and observe edge values for an agent and an object go up before that object is captured. Because this information is carefully shaped and structured, it makes it easier for humans to understand, and, in the tests the authors ran, appears to also help agents do better in collaborative games.\n\nhttps://i.imgur.com/BCKSmIb.png\n\nWhile I find graph networks quite interesting, and multi-agent learning quite interesting, I\u2019m a little more uncertain about the inherent \u201cgraphiness\u201d of this problem, since there aren\u2019t really meaningful inherent edges between agents.  One thing I am curious about here is how methods like these would work in situations of sparser graphs, or, places where the connectivity level between a node\u2019s neighbors, and the average other node in the graph is more distinct. Here, every node is connected to every other node, so the explicit information localization function of graph networks is less pronounced. I might naively think that - to whatever extent the graph is designed in a way that captures information meaningful to the task - explicit graph methods would have an even greater comparative advantage in this setting. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1809.11044"
    },
    "294": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1811.06272",
        "transcript": "It is a fact universally acknowledged that a reinforcement learning algorithm not in possession of a model must be in want of more data. Because they generally are. Joking aside, it is broadly understood that model-free RL takes a lot of data to train, and, even when you can design them to use off-policy trajectories, collecting data in the real environment might still be too costly. Under those conditions, we might want to learn a model of the environment and generate synthesized trajectories, and train on those. This has the advantage of not needing us to run the actual environment, but the obvious disadvantage that any model will be a simplification of the true environment, and potentially an inaccurate one. These authors seek to answer the question of: \u201cis there a way to generate trajectories that has higher fidelity to the true environment.\u201d As you might infer from the fact that they published a paper, and that I\u2019m now writing about it, they argue that, yes, there is, and it\u2019s through explicit causal/counterfactual modeling. \n\nCausal modeling is one of those areas of statistics that seems straightforward at its highest level of abstraction, but tends to get mathematically messy and unintuitive when you dive into the math. So, rather than starting with equations, I\u2019m going to try to verbally give some intuitions for the way causal modeling is framed here. Imagine you\u2019re trying to understand what would happen if a person had gone to college. There\u2019s some set of information you know about them, and some set of information you don\u2019t, that\u2019s just random true facts about them and about the universe. If, in the real world, they did go to college, and you want to simulate what would have happened if they didn\u2019t, it\u2019s not enough to just know the observed facts about them, you want to actually isolate all of the random other facts (about them, about the world) that weren\u2019t specifically \u201cthe choice to go to college\u201d, and condition on those as well. Obviously, in the example given here, it isn\u2019t really practically possible to isolate all the specific unseen factors that influence someone\u2019s outcome. But, conceptually, this quantity, is what we\u2019re going to focus on in this paper. \n\nNow, imagine a situation where a RL agent has been dropped into a maze-like puzzle. It has some set of dynamics, not immediately visible to the player, that make it difficult, but ultimately solvable.  The best kind of simulated data, the paper argues, would be to keep that state of the world (which is partially unobservable) fixed, and sample different sets of actions the agent might take in that space. Thus, \u201ccounterfactual modeling\u201d: for a given configuration of random states in the world, sampling different actions within it. To do this, you first have to infer the random state the agent is experiencing. In the normal model-based case, you\u2019d have some prior over world states, and just sample from it. However, if you use the experience of the agent\u2019s trajectory, you can make a better guess as to what world configuration it was dropped into. If you can do this, which is, technically speaking, sampling from the posterior over unseen context, conditional on an agent\u2019s experience, then the paper suggests you\u2019ll be able to generate data that\u2019s more realistic, because the trajectories will be direct counterfactuals of \u201creal world\u201d scenarios, rather than potentially-unsolvable or unrealistic draws from the prior. \n\nThis is, essentially, the approach proposed by the paper: during training, they make this \u201cworld state\u201d visible to the agent, and let it learn a model predicting what state it started with, given some trajectory of experience. They also learn a model that predicts the outcome and ultimately the value of actions taken, conditioned on this random context (as well as visible context, and the agent\u2019s prior actions). They start out by using this as a tool for policy evaluation, which is a nice problem setup because you can actually check how well you\u2019re doing against some baseline: if you want to know how good your simulated data is at replicating the policy reward on real data, you can just try it out on real data and see. The authors find that they reduce policy reward estimation error pretty substantially by adding steps of experience (in Bayesian terms, bit of evidence moving them from the prior, towards the posterior). \n\nhttps://i.imgur.com/sNAcGjZ.png\nThey also experiment with using this for actual policy search, but, honestly, I didn\u2019t quite follow the intuitions behind Guided Policy Search, so I\u2019m just going to not dive into that for now, since I think a lot of the key contributions of the paper are wrapped up in the idea of \u201cestimate the reward of a policy by simulating data from a counterfactual trajectory\u201d\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1811.06272"
    },
    "295": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.10485",
        "transcript": "This paper feels a bit like watching a 90\u2019s show, and everyone\u2019s in denim and miniskirts, except it\u2019s a 2017 ML paper, and everything uses attention. (I\u2019ll say it again, ML years are like dog years, but more so). That said, that\u2019s not a critique of the paper: finding clever ways to cobble together techniques for your application can be an important and valuable contribution. This paper addresses the problem of text to image generation: how to take a description of an image and generate an image that matches it, and it makes two main contributions: 1) a GAN structure that seems to merge insights from Attention and Progressive GANs in order to select areas of the sentence to inform details in specific image regions, and 2) a novel discriminator structure to evaluate whether a sentence matches an image. \n\nhttps://i.imgur.com/JLuuhJF.png\n\nFocusing on the first of these first: their generation system works by an iterative process, that gradually builds up image resolution, and also pulls specific information from the sentence to inform details in each region. The first layer of the network generates a first \u201chidden state\u201d based on a compressed representation of the sentence as a whole (the final hidden state of a LSTM text encoder, I believe), as well as random noise (typical input to a GAN). Subsequent \u201chidden states\u201d are calculated by calculating attention weightings between each region of the image, and each word in the sentence, and pulling together a per-region context vector based on that attention map. (As far as I understand it, \u201cregion\u201d here refers to the fact that when you\u2019re at lower spatial scales of what is essentially a progressive generation process, 64x64 rather than 256x256, for example, each \u201cpixel\u201d actually represents a larger region of the image). I\u2019m using quotes around \u201chidden state\u201d in the above paragraph because I think it\u2019s actually pretty confusing terminology, since it suggests a recurrent structure, but this model isn\u2019t actually recurrent: there\u2019s a specific set of weights for resolution block 0, and 1, and 2. This whole approach, of calculating a specific attention-weighted context vector over input words based on where you are in the generation process is very conceptually similar to the original domain of attention, where the attention query would be driven by the hidden state of the LSTM generating the translated version of some input sentence, except, here, instead of translating between languages, you\u2019re translating across mediums. \n\nThe loss for this model is a combination of per-layer loss, and a final, special, full-resolution loss. At each level of resolution, there exists a separate discriminator, which seems to be able to take in both 1) only an image, and judge whether it thinks that image looks realistic on it\u2019s own, and 2) an image and a global sentence vector, and judge whether the image matches the sentence. It\u2019s not fully clear from the paper, but it seems like this is based on just feeding in the sentence vector as additional input? \nhttps://i.imgur.com/B6qPFax.png\nFor each non-final layer\u2019s discriminator, the loss is a combination of both of these unconditional and conditional losses. \n\nThe final contribution of this paper is something they call the DAMSM loss: the Deep Attention Multimodal Similarity Model. This is a fairly complex model structure, whose ultimate goal is to assess how closely a final generated image matches a sentence. The whole structure of this loss is based on projecting region-level image features (from an intermediate, 17x17 layer of a pretrained Inception Net) and word features into the same space, and then calculating dot product similarities between them, which are then used to build \u201cvisual context vectors\u201d for each word (for each word, created a weighted sum of visual vectors, based on how similar each is to the word). Then, we take each word\u2019s context vector, and see how close it is to the original word vector. If we, again, imagine image and word vectors as being in a conceptually shared space, then this is basically saying \u201cif I take a weighted average of all the things that are the most similar to me, how ultimately similar is that weighted average to me\u201d. This allows there to be a \u201cconcept representation\u201d match found when, for example, a particular word\u2019s concept, like \u201cbeak\u201d, is only present in one region, but present there very strongly: the context vector will be strongly weighted towards that region, and will end up being very close, in cosine similarity terms, to the word itself. By contrast, if none of the regions are a particularly good match for the word\u2019s concept, this value will be low. DAMSM then aggregates up to an overall \u201crelevance\u201d score between a sentence and image, that\u2019s simply a sum over a word\u2019s \u201cconcept representation\u201d, for each word in a sentence. It then calculates conditional probabilities in two directions: what\u2019s the probability of the sentence, given the image (relevance score of (Sent, Imag), divided by that image\u2019s summed relevance with all possible sentences in the batch), and, also, what\u2019s the probability of the image, given the sentence (relevance score of the pair, divided by the sentence\u2019s summed relevance with all possible images in the batch). \n\nIn addition to this word-level concept modeling, DAMSM also has full sentence-level versions, where it simply calculates the relevance of each (sentence, image) pair by taking the cosine similarity between the global sentence and global image features (the final hidden state of an encoder RNN, and the final aggregated InceptionNet features, respectively). All these losses are aggregated together, to get one that uses both global information, and information as to whether specific words in a sentence are represented well in an image. \n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.10485"
    },
    "296": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1807.03247",
        "transcript": "This is a paper where I keep being torn between the response of \u201cthis is so simple it\u2019s brilliant; why haven\u2019t people done it before,\u201d and \u201cthis is so simple it\u2019s almost tautological, and the results I\u2019m seeing aren\u2019t actually that surprising\u201d. The basic observation this paper makes is one made frequently before, most recently to my memory by Geoff Hinton in his Capsule Net paper: sometimes the translation invariance of convolutional networks can be a bad thing, and lead to worse performance. In a lot of ways, translation invariance is one of the benefits of using a convolutional architecture in the first place: instead of having to learn separate feature detectors for \u201ca frog in this corner\u201d and \u201ca frog in that corner,\u201d we can instead use the same feature detector, and just move it over different areas of the image. However, this paper argues, this makes convolutional networks perform worse than might naively be expected at tasks that require them to remember or act in accordance with coordinates of elements within an image. \n\nFor example, they find that normal convolutional networks take nearly an hour and 200K worth of parameters to learn to \u201cpredict\u201d the one-hot encoding of a pixel, when given the (x,y) coordinates of that pixel as input, and only get up to about 80% accuracy. Similarly, trying to take an input image with only one pixel active, and predict the (x,y) coordinates as output, is something the network is able to do successfully, but only when the test points are sampled from the same spatial region as the training points: if the test points are from a held-out quadrant, the model can\u2019t extrapolate to the (x, y) coordinates there, and totally falls apart. \n\nhttps://i.imgur.com/x6phN4p.png\n\nThe solution proposed by the authors is a really simple one: at one or more layers within the network, in addition to the feature channels sent up from the prior layer, add two addition channels: one with a with deterministic values going from -1 (left) to 1 (right), and the other going top to bottom. This essentially adds two fixed \u201cfeatures\u201d to each pixel, which jointly carry information about where it is in space. Just by adding this small change, we give the network the ability to use spatial information or not, as it sees fit. If these features don\u2019t prove useful, their weights will stay around their initialization values of expectation-zero, and the behavior should be much like a normal convolutional net. However, if it proves useful, convolution filters at the next layer can take position information into account. It\u2019s easy to see how this would be useful for this paper\u2019s toy problems: you can just create a feature detector for \u201cif this pixel is active, pass forward information about it\u2019s spatial position,\u201d and predict the (x, y) coordinates out easily. You can also imagine this capability helping with more typical image classification problems, by having feature filters that carry with them not only content information, but information about where a pattern was found spatially.  \n\nThe authors do indeed find comparable performance or small benefits to ImageNet, MNIST, and Atari RL, when applying their layers in lieu of normal convolutional layer. On GANs in particular, they find less mode collapse, though I don\u2019t yet 100% follow the intuition of why this would be the case. \nhttps://i.imgur.com/wu7wQZr.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1807.03247"
    },
    "297": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1712.09913",
        "transcript": "This paper was a real delight to read, and even though I\u2019m summarizing it here, I\u2019d really encourage you, if you\u2019re reading this, to read the paper itself, since I found it to be unusually clearly written. It tackles the problem of understanding how features of loss functions - these integral, yet arcane, objects defined in millions of parameter-dimensions - impact model performance. Loss function analysis is generally a difficult area, since the number of dimensions and number of points needed to evaluate to calculate loss are both so high. The latter presents computational challenges, the former ones of understanding: human brains and many-dimensional spaces are not a good fit. Overall, this paper contributes by 1) arguing for a new way of visualizing loss functions, 2) demonstrating how and in what cases \u201cflatness\u201d of loss function contributes to performance and trainability, and 3)) \n\nThe authors review a few historically common ways of visualizing loss functions, before introducing their variant. The simplest, one-dimensional visualization technique, 1-D Linear Interpolation, works by taking two parameter settings (say, a random initialization, and the final network minimum), and smoothly interpolating between the two, by taking a convex combination mediated by alpha. Then, you can plot the value of the loss at all of these parameter configurations as a function of alpha. If you want to plot in 2D, with a contour plot, you can do so in a pretty similar manner, by picking two random \u201cdirection vectors\u201d of the same size as the parameter vector, and then adding amounts of those directions, weighted by alpha and beta, to your starting point. These random directions become your axes, and you get a snapshot of the change in your loss function as you move along them. \n\n\nThe authors then make the observation that these techniques can\u2019t natively be used to compare two different models, if the parameters of those models are on different scales. If you take a neural net, multiply one layer by 10, and then divide the next layer by 10, you\u2019ve essentially done a no-op that won\u2019t impact the outcome. However, if you\u2019re moving by a fixed amount along your random direction in parameter space, you\u2019ll have to move much farther to go the commensurate amount of distance in the network that\u2019s been multiplied by 10. To address this problem, they suggest a simple fix: after you\u2019ve selected each of your random directions, scale the value in each direction vector by the norm of the filter that corresponds to that value. This gets rid of the sensitivity of your plots to the scale of weights. (One thing I admit I\u2019m a little confused by here is the fact that each value in the direction vector corresponds to a filter, rather than to a weight; I would have natively thought theta, and all the direction vectors, are of length number-of-model-parameters, and each value is a single weight. I think I still broadly grasp the intuition, but I\u2019d value having a better sense of this). \n\nTo demonstrate the value of their normalization system, they compare the interpolation plots for a model with small and large batch size, with and without weight decay. Small batches are known to increase flatness of the loss function around the eventual minimum, which seems co-occurrent with good generalization results. And, that bears out in the original model\u2019s linear interpolation (figs a, b, c), where the small model has the wider solution basin, and also better performance. However, once weight decay is applied (figs d, e, f), the small-batch basin appears to shrink to be very narrow, although small-batch still has dominant performance. At first glance, this would seem to be a contradiction of the \u201cflatter solutions mean more generalization\u201d rule.  \n\nhttps://i.imgur.com/V0H13kK.png\n\nBut this is just because weight decay hits smaller models more strongly, because they have more distinct updates during which they apply the weight decay penalty. This means that when weight decay is applied, the overall scale of weights in the small-batch network is lower, and so it\u2019s solution looked \u201csharp\u201d when plotted on the same weight scale as the large-batch network. When normalization was used, this effect by and large went away, and you once again saw higher performance with flatter loss functions. (batch size and performance with and without weight decay, shown normalized below)\n\nhttps://i.imgur.com/vEUIgo0.png\n\nA few other, more scattered observations from the paper:\n\n- I\u2019ve heard explanations of skip connections in terms of \u201cgiving the model shorter gradient paths between parameters and output,\u201d but haven\u2019t really seen an argument for why skip connections lead to smoother loss functions, even they empirically seem to\n\nhttps://i.imgur.com/g3QqRzh.png\n\n- The authors also devise a technique for visualizing the change in loss function along the trajectory taken by the optimization algorithm, so that different ones can be compared.  The main problem in previous methods for this has been that optimization trajectories happen in a low-dimensional manifold within parameter space, so if you just randomly select directions, you won\u2019t see any interesting movement along the trajectory. To fix this, they choose as their axes the principal components you get from making a matrix out of the parameter values at each epoch: this prioritizes the parameters that had the most variance throughout training. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1712.09913"
    },
    "298": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1809.02861",
        "transcript": "This paper focuses on the well-known fact that adversarial examples are often transferable: that is, that an adversarial example created by optimizing loss on a surrogate model trained on similar data can often still induce increased loss on the true target model, though typically not to the same magnitude as an example optimized against the target itself. Its goal is to come up with clearer theoretical formulation for transferred examples, and more clearly understand what kinds of models transfer better than others. \n\nThe authors define their two scenarios of interest as white box (where the parameters of the target model are known), and limited knowledge, or black box, where only the data type and feature representation is known, but the exact training dataset is unknown, as well as the parameters of the target model. Most of the mathematics of this paper revolve around this equation, which characterizes how to find a delta to maximize loss on the surrogate model: \n\nhttps://i.imgur.com/Y0mD35x.png\n\nIn words: you\u2019re finding a delta (perturbations of each input value) such that the p-norm of delta is less than some radius epsilon, and such that delta maximizes the dot product between delta and the model gradient with respect to the inputs. The closer two vectors are to one another, the higher their dot product. So, having your delta just *be* the model gradient w.r.t inputs maximizes that quantity. However, we also need to meet the requirement of having our perturbation\u2019s norm be less than epsilon, so we in order to find the actual optimal value, we divide by the norm of the gradient (to get ourselves a norm of 1), and multiply by epsilon (to get ourselves a norm of epsilon). This leads to the optimal value of delta being, for a norm of 2: \n\nhttps://i.imgur.com/Op0H7KL.png\n\nAn important thing to remember is that all of the above has been using w-hat, meaning it\u2019s been an examination of what the optimal delta is when we\u2019re calculating against the surrogate model. But, if we plug in the optimal transfer value of delta we found above, how does this compare to the increase in loss if we were able to optimize against the true model? \n\nhttps://i.imgur.com/RHILZK1.png\n\nLoss on the true model is, as above, calculated as the dot product of the delta perturbation with the gradient w.r.t inputs of the true model. Using the same logic as above, this quantity is maximized when our perturbation is as close as possible to the target model\u2019s gradient vector. So, the authors show, the degree to which adversarial examples calculated on one model transfer to another is mediated by the cosine distance between surrogate model\u2019s gradient vector and the target model\u2019s one. The more similar these gradients w.r.t the input are to one another, the closer surrogate-model loss increase will be to target-model loss increase. This is one of those things that makes sense once it\u2019s laid out, but it\u2019s still useful to have a specific conceptual quality to point to when predicting whether adversarial examples will transfer, rather than just knowing that they do, at least some of the time, to at least some extent. \n\nAnother interesting thing to notice from the above equation, though not directly related to transfer examples, is the right hand of the equation, the upper bound on loss increase, which is the p-norm of the gradient vector of the target model. In clearer words, this means that the amount of loss that it\u2019s possible to induce on a model using a given epsilon of perturbation is directly dependent on the norm of that model\u2019s gradient w.r.t inputs. This suggests that more highly regularized models, which are by definition smoother and have smaller gradients with respect to inputs, will be harder to attack. This hypothesis is borne out by the authors\u2019 experiments. However, they also find, consistent with my understanding of prior work, that linear models are harder to attack than non-linear ones. This draws a line between two ways we\u2019re used to thinking about model complexity/simplicity: having a less-smooth function with bigger gradients increases your vulnerability, but having nonlinear model structure seems to decrease it.\n\nhttps://i.imgur.com/mw9exLU.png\n\nOne final intriguing empirical finding of this paper is that, in addition to being the hardest models to attack when they are the target, highly regularized models work the best as surrogate models. There\u2019s a simplistic way in which this makes sense, in that if you create your examples against a \u201charder\u201d adversary to begin with, they\u2019ll be in some sense stronger, and transfer better. However, I\u2019m not sure that intuition is a correct one here. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1809.02861"
    },
    "299": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1806.11146",
        "transcript": "In the literature of adversarial examples, there\u2019s this (to me) constant question: is it the case that adversarial examples are causing the model to objectively make a mistake, or just displaying behavior that is deeply weird, and unintuitive relative to our sense of what these models \u201cshould\u201d be doing. A lot of the former question seems to come down to arguing over about what\u2019s technically \u201cout of distribution\u201d, which has an occasional angels-dancing-on-a-pin quality, but it\u2019s pretty unambiguously clear that the behavior displayed in this paper is weird, and beyond what I naively expected a network to be able to be manipulated to do.  \n\nThe goal these authors set for themselves is what they call \u201creprogramming\u201d of a network; they want the ability to essentially hijack the network\u2019s computational engine to perform a different task, predicting a different set of labels, on a different set of inputs than the ones the model was trained on. For example, one task they perform is feeding in MNIST images at the center of a bunch of (what appear to be random, but are actually carefully optimized) pixels, and getting a network that can predict MNIST labels out the other end. Obviously, it\u2019s not literally possible to change the number of outputs that a network produces once it\u2019s trained, so the authors would arbitrarily map ImageNet outputs to MNIST categories (like, \u201cwhen this model predicts Husky, that actually means the digit 7\u201d)  and then judge how well this mapped output performs as a MNIST classifier. I enjoyed the authors\u2019 wry commentary here about the arbitrariness of the mapping, remarking that \u201ca \u2018White Shark\u2019 has nothing to do with counting 3 squares in an image, and an \u2018Ostrich\u2019 does not at all resemble 10 squares\u201d. \n\nhttps://i.imgur.com/K02cwK0.png\n\nThis paper assumes a white box attack model, which implies visibility of all of the parameters, and ability to directly calculate gradients through the model. So, given this setup of a input surrounded by modifiable pixel weights, and a desire to assign your \u201cMNIST Labels\u201d correctly, this becomes a straightforward optimization problem: modify the values of your input weights so as to maximize your MNIST accuracy. An important point to note here is that the same input mask of pixel values is applied for every new-task image, and so these values are optimized over a full training set of inserted images, the way that normal weights would be. One interesting observation the authors make is that, counter to the typical setup of adversarial examples, this attack would not work with a fully linear model, since you actually need your \u201cweights\u201d to interact with your \u201cinput\u201d, which is different each time, but these are both just different areas of your true input. This need to have different regions of input determine how other areas of input are processed isn\u2019t possible in a linear model where each input has a distinct impact on the output, regardless of other input values. By contrast, when you just need to optimize a single perturbation to get the network to jack up the prediction for one class, that can be accomplished by just applying a strong enough bias everywhere in the input, all pointing in the same direction, which can be added together linearly and still get the job done. \n\nThe authors are able to perform MNIST and the task of \u201ccount the squares in this small input\u201d to relatively high levels of accuracy. They perform reasonably on CIFAR (as well as a fully connected network, but not as well as a convnet). They found that performance was higher when using a pre-trained ImageNet, relative to just random weights. There\u2019s some suggestion made that this implies there\u2019s a kind of transfer learning going on, but honestly, this is weird enough that it\u2019s hard to say. \nhttps://i.imgur.com/bj2MUnk.png\n\nThey were able to get this reprogramming work on different model structures, but, fascinatingly, saw distinctive patterns to the \"weight pixels\" they needed to add to each model structure, with ResNet easily differentiable from Inception. \n\nOne minor quibble I have with the framing of this paper - which I overall found impressive, creative, and well-written - is that I feel like it\u2019s stretching the original frame of \u201cadversarial example\u201d a bit too far, to the point of possible provoking confusion. It\u2019s not obvious that the network is making a mistake, per se, when it classifies this very out-of-distribution input as something silly. I suppose, in an ideal world, we may want our models to return to something like a uniform-over-outputs state of low confidence when predicting out of distribution, but that\u2019s a bit different than seeing a gibbon in a picture of a panda. I don\u2019t dispute the authors claim that the behavior they\u2019re demonstrating is a vulnerability in terms of its ability to let outside actors \u201chijack\u201d networks compute, but I worry we might be overloading the \u201cadversarial example\u201d to cover too many types of network failure modes. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1806.11146"
    },
    "300": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1807.09341",
        "transcript": "This paper tries to solve the problem of how to learn systems that, given a starting state and a desired target, can earn the set of actions necessary to reach that target. The strong version of this problem requires a planning algorithm to learn a full set of actions to take the agent from state A to B. However, this is a difficult and complex task, and so this paper tries to address a relaxed version of this task: generating a set of \u201cwaypoint\u201d observations between A and B, such that each successive observation is relatively close to one another in terms of possible actions (the paper calls this \u2018h-reachable\u2019, if observations are reachable from one another in h timesteps). With these checkpoint observations in hand, the planning system can them solve many iterations of a much shorter-time-scale version of the problem. \n\nHowever, the paper asserts, applying pre-designed planning algorithms in observation space (sparse, high-dimensional) is difficult, because planning algorithms apparently do better with denser representations. (I don\u2019t really understand, based on just reading this paper, *why* this is the case, other than the general fact that high dimensional, sparse data is just hard for most things). Historically, a typical workflow for applying planning algorithms to an environment would have been to hand-design feature representations where nearby representations were close in causal decision space (i.e. could be easily reached from one another). This paper\u2019s goal is to derive such representations from data, rather than hand-designing them. \n\nThe system they design to do this is a little unwieldy to follow, and I only have about 80% confidence that I fully understand all the mechanisms. One basic way you might compress high-dimensional space into a low-dimensional code is by training a Variational Autoencoder, and pulling the latent code out of the bottleneck in the middle. However, we also want to be able to map between our low-dimensional code and a realistic observation space, once we\u2019re done planning and have our trajectory of codes, and VAE typically have difficulty generating high-dimensional observations with high fidelity. If what you want is image-generation fidelity, the natural step would be to use a GAN. \n\nHowever, GANs aren\u2019t really natively designed to learn an informative representation; their main goal is generation, and there\u2019s no real incentive for the noise variables used to seed generation to encode any useful information. One GAN design that tries to get around this is the InfoGAN, which gets its name from the requirement that there be high mutual information between (some subset of) the noise variables used to seed the generator, and the actual observation produced. I\u2019m not going to get into the math of the variational approximation, but what this actually mechanically shakes out to is: in addition to generating an observation from a code, an InfoGAN also tries to predict the original code subset given the observation. Intuitively, this requirement, for the observation to contain information about the code, also means the code is forced to contain meaningful information about the image generated from it. \n\nHowever, even with this system, even if each code separately corresponds to a realistic observation, there\u2019s no guarantee that closeness in state space corresponds to closeness in \u201ccausality space\u201d. This feature is valuable for planning, because it means that if you chart out a trajectory through state space, it actually corresponds to a reasonable trajectory through observation space. In order to solve this problem, the authors added their final, and more novel, modification to the InfoGAN framework: instead of giving the GAN one latent code, and having it predict one observation, they would give two at a time, and have the GAN try to generate a pair of temporally nearby (i.e. less than h actions away) observations. Importantly, they\u2019d also define some transition or sampling function within state space, so that there would be a structured or predictable way that adjacent pairs of states looked. So, if the GAN were able to learn to map adjacent points in state space to adjacent points in observation space, then you\u2019d be able to plan out trajectories in state space, and have them be realistic in observation space. \nhttps://i.imgur.com/oVlVc0x.png\nThey do some experiments and do show that both adding the \u201cInfo\u201d structure of the InfoGAN, and adding the paired causal structure, lead to states with improved planning properties.They also compared the clusters derived from their Causal InfoGAN states to the clusters you\u2019d get from just naively assuming that nearness in observation space meant nearness in causality space. \n\nhttps://i.imgur.com/ddQpIdH.png\n\nThey specifically tested this on an environment divided into two \u201crooms\u201d, where there were many places where there were two points, nearby in Euclidean space, but far away (or mutually inaccessible) in action space. They showed that the Causal InfoGAN (b) was successfully able to learn representations such that points nearby in action space clustered together, whereas a Euclidean representation (c) didn't have this property. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1807.09341"
    },
    "301": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.08647",
        "transcript": "This paper builds very directly on the idea of \u201cempowerment\u201d as an intrinsic reward for RL agents. Where empowerment incentivizes agents to increase the amount of influence they\u2019re able to have over the environment, \u201csocial influence,\u201d this paper\u2019s metric, is based on the degree which the actions of one agent influence the actions of other agents, within a multi-agent setting. The goals between the two frameworks are a little different. The notion of \u201cempowerment\u201d is built around a singular agent trying to figure out a short-term proxy for likelihood of long-term survival (which is a feedback point no individual wants to hit). By contrast, the problems that the authors of this paper seek to solve are more explicitly multi-agent coordination problems: prisoner\u2019s dilemma-style situations where collective reward requires cooperation. However, they share a mathematical basis: the idea that an agent\u2019s influence on some other element of its environment (be it the external state, or another agent\u2019s actions) is well modeled by calculating the mutual information between its agents and that element. \n\nWhile this is initially a bit of an odd conceptual jump, it does make sense: if an action can give statistical information to help you predict an outcome, it\u2019s likely (obviously not certain, but likely) that that action influenced that outcome. In a multi-agent problem, where cooperation and potentially even communication can help solve the task, being able to influence other agents amounts to \u201cfinding ways to make oneself useful to other agents\u201d, because other agents aren\u2019t going to change behavior based on your actions, or \u201clisten\u201d to your \u201cmessages\u201d (in the experiment where a communication channel was available between agents) if these signals don\u2019t help them achieve *their* goals. So, this incentive, to influence the behavior of other (self-interested) agents, amounts to a good proxy for incentivizing useful cooperation. \n\nZooming in on the exact mathematical formulations (which differ slightly from, though they\u2019re in a shared spirit with, the empowerment math): the agent\u2019s (A\u2019s) Causal Influence reward is calculated by taking a KL divergence between the action distribution of the other agent (B) conditional on the action A took, compared to other actions A might have taken. (see below. Connecting back to empowerment: Mutual Information is just the expected value of this quantity, taken over A\u2019s action distribution). \n\nhttps://i.imgur.com/oxXCbdK.png\n\nOne thing you may notice from the above equation is that, because we\u2019re working in KL divergences, we expect agent A to have access to the full distribution of agent B\u2019s policy conditional on A\u2019s action, not just the action B actually took. We also require the ability to sample \u201ccounterfactuals,\u201d i.e. what agent B would have done if agent A had done something differently. Between these two requirements. If we take a realistic model of two agents interacting with each other, in only one timeline, only having access to the external and not internal parameters of the other, it makes it clear that these quantities can\u2019t be pulled from direct experience. Instead, they are calculated by using an internal model: each agent builds its own MOA (Model of Other Agents), where they build a predictive model of what an agent will do at a given time, conditional on the environment and the actions of all other agents. It\u2019s this model that is used to sample the aforementioned counterfactuals, since that just involves passing in a different input. I\u2019m not entirely sure, in each experiment, whether the MOAs are trained concurrent with agent policies, or in a separate prior step. \n\nhttps://i.imgur.com/dn2cBg4.png\n\nTesting on, again, Prisoner\u2019s Dilemma style problems requiring agents to take risky collaborative actions, the authors did find higher performance using their method, compared to approaches where each agent just maximizes its own external reward (which, it should be said, does depend on other agents\u2019 actions), with no explicit incentive towards collaboration. Interestingly, when they specifically tested giving agents access to a \u201ccommunication channel\u201d (the ability to output discrete signals or \u201cwords\u201d visible to other agents), they found that it was able to train just as effectively with only an influence reward, as it was with both an influence and external reward. \n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.08647"
    },
    "302": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1703.04908",
        "transcript": "This paper performs a fascinating toy experiment, to try to see if something language-like in structure can be effectively induced in a population of agents, if they are given incentives that promote it. In some sense, a lot of what they find \u201cjust makes sense,\u201d but it\u2019s still a useful proof of concept to show that it can be done. \n\nThe experiment they run takes place in a simple, two-dimensional world, with a fixed number of landmarks (representing locations goals need to take place), and agents, and actions. In this construction, each agent has a set of internal goals, which can either be actions (like \u201cgo to green landmark\u201d) they themselves need to perform, or actions that they want another agent to perform. Agents\u2019 goals are not visible to other agents, but all agents\u2019 reward is defined to be the aggregated reward of all agents together, so if agent A has a goal involving an action of agent B\u2019s, it\u2019s in B\u2019s \u201cinterest\u201d to do that action, if it can be communicated to them. In order to facilitate other agents performing goals, at each step, each agent both takes an action, and also emits an \u201cutterance\u201d, which is just a discrete symbolic \u201cword\u201d out of some some fixed vocabulary of words (Note that applying \u201cword\u201d here is a but fuzzy; the agents do not pronounce or spell a character-based word, they just pick a discrete symbol that is playing the role of a word\u201d. Even though other agents cannot see a given agent\u2019s goals, they can see its public utterances, and so agents learn that communication is a way to induce other agents to perform desired actions. \n\nAs a mathematically interesting aside: this setup, of allowing each agent to sample a single discrete word out of a small vocabulary at each setting, takes the deployment of some interesting computational tricks to accomplish. First off, in general, sampling a discrete single symbol out of a set of possible symbols is not differentiable, since it\u2019s a discrete rather than continuous action, and derivatives require continuous functions. However, a paper from 2016 proposed a (heuristic) solution to this problem by means of the Gumbel Softmax Trick. This derives from the older \u201cGumbel Max Trick\u201d, which is the mathematical fact that if you want to sample from a categorical distribution, a computationally easy way to do so is to add a variable sampled from a (0,1) Gumbel distribution to the log probability of each category, and then take the argmax of this as the index of the sample category (I\u2019m not going to go another level down into why this is true, since I think it\u2019s too far afield of the scope of this summary). Generally, argmax functions are also not differentiable. However, they can be approximated with softmaxes, which interpolate between a totally uniform and very nearly discrete-sample distribution based on a temperature parameter. In practice, or, at least, if this paper does what the original Gumbel Softmax paper did, during training, a discrete sample is taken, but a low-temperature continuous approximation is used for actual gradient calculation (i.e. for gradients, the model pretends that it used the continuous approximation rather than the discrete sample). \n\nhttps://i.imgur.com/0RpRJG2.png\n\nComing back to the actual communication problem, the authors do find that under these (admittedly fairly sanitized and contrived) circumstances, agents use series of discrete symbols to communicate goals to other agents, which ends up looking a lot like a very simple language. \n\nhttps://i.imgur.com/ZF0EbN4.png\n\nAs one might expect, in environments where there were only two agents, there was no symbol that ended up corresponding to \u201cred agent\u201d or \u201cblue agent\u201d, since each could realize that the other was speaking to it. However, in three-agent environments, the agents did develop symbols that clearly mapped to these categories, to specify who directions were being given to. The authors also tried cutting off verbal communication; in these situations, the agents used gaze and movement to try to signal what they wanted other agents to do. Probably most entertainingly, when neither verbal nor visual communication was allowed, agents would move to and \u201cphysically\u201d push other agents to the location where their action needed to be performed. \n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.04908v2"
    },
    "303": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.12162",
        "transcript": "This paper continues in the tradition of curiosity-based models, which try to reward models for exploring novel parts of their environment, in the hopes this can intrinsically motivate learning. However, this paper argues that it\u2019s insufficient to just treat novelty as an occasional bonus on top of a normal reward function, and that instead you should figure out a process that\u2019s more specifically designed to increase novelty. Specifically: you should design a policy whose goal is to experience transitions and world-states that are high novelty. \n\nIn this setup, like in other curiosity-based papers, \u201chigh novelty\u201d is defined in terms of a state being unpredictable given a prior state, history, and action. However, where other papers saw novelty reward as something only applied when the agent arrived at somewhere novel, here, the authors build a model (technically, an ensemble of models) to predict the state at various future points. The ensemble is important here because it\u2019s (quasi) bootstrapped, and thus gives us a measure of uncertainty. States where the predictions of the ensemble diverge represent places of uncertainty, and thus of high value to explore. I don\u2019t 100% follow the analytic specification of this idea (even though the heuristic/algorithmic description makes sense). The authors frame the Utility function of a state and action as being equivalent to the Jenson Shannon Divergence (~distance between probability distributions) shown below. \n\nhttps://i.imgur.com/YIuomuP.png\n\n\nHere, P(S | S, a, T) is the probability of a state given prior state and action under a given model of the environment (Transition Model), and P(gamma) is the distribution over the space of possible transition models one might learn. A \u201cmodel\u201d here is one network out of the ensemble of networks that makes up our bootstrapped (trained on different sets) distribution over models. Conceptually, I think this calculation is measuring \u201chow different is each sampled model/state distribution from all the other models in the distribution over possible models\u201d. If the models within the distribution diverge from one another, that indicates a location of higher uncertainty. \n\nWhat\u2019s important about this is that, by building a full transition model, the authors can calculate the expected novelty or \u201cutility\u201d of future transitions it might take, because it can make a best guess based on this transition model (which, while called a \u201cprior\u201d, is really something trained on all data up to this current iteration). My understanding is that these kinds of models function similarly to a Q(s,a) or V(s) in a pure-reward case: they estimate the \u201cutility reward\u201d of different states and actions, and then the policy is updated to increase that expected reward. \n\nI\u2019ve recently read papers on ICM, and I was a little disappointed that this paper didn\u2019t appear to benchmark against that, but against Bootstrapped DQN and Exploration Bonus DQN, which I know less well and can less speak to the conceptual differences from this approach. Another difficulty in actually getting a good sense of results was that the task being tested on is fairly specific, and different from RL results coming out of the world of e.g. Atari and Deep Mind Labs. All of that said, this is a cautiously interesting idea, if the results generate to beat more baselines on more environments. \n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.12162"
    },
    "304": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.02274",
        "transcript": "This paper proposes a new curiosity-based intrinsic reward technique that seeks to address one of the failure modes of previous curiosity methods. The basic idea of curiosity is that, often, exploring novel areas of an environment can be correlated with gaining reward within that environment, and that we can find ways to incentivize the former that don\u2019t require a hand-designed reward function. This is appealing because many useful-to-learn environments either lack inherent reward altogether, or have reward that is very sparse (i.e. no signal until you reach the end, at which point you get a reward of 1). In both of these cases, supplementing with some kind of intrinsic incentive towards exploration might improve performance. The existing baseline curiosity technique is called ICM, and works based on \u201csurprisal\u201d: asking the agent to predict the next state as a function of its current state, and incentivizing exploration of areas where the gap between these two quantities is high, to promote exploration of harder-to-predict (and presumably more poorly sampled) locations. However, one failure mode of this approach is something called the \u201cnoisy TV\u201d problem, whereby if the environment contains something analogous to a television where one can press a button and go to a random channel, that is highly unpredictable, and thus a source of easy rewards, and thus liable to distract the agent from any other actions. \n\nAs an alternative, the authors here suggest a different way of defining novelty: rather than something that is unpredictable, novelty should be seen as something far away from what I as an agent have seen before. This is more direct than the prior approach, which takes \u2018hard to predict\u2019 as a proxy for \u2018somewhere I haven\u2019t explored\u2019, which may not necessary be a reasonable assumption. \n\nhttps://i.imgur.com/EfcAOoI.png\n\nThey implement this idea by keeping a memory of past (embedded) observations that the agent has seen during this episode, and, at each step, check whether the current observation is predicted to be more than K steps away than any of the observations in memory (more on that in a moment). If so, a bonus reward is added, and this observation is added to the aforementioned memory. (Which, waving hands vigorously, kind of ends up functioning as a spanning set of prior experience). \n\nhttps://i.imgur.com/gmHE11s.png\n\nThe question of \u201chow many steps is observation A from observation B\u201d is answered by a separate Comparator network which is trained in pretty straightforward fashion: a random-samplling policy is used to collect trajectories, which are then turned into pairs of observations as input, and a 1 if they occurred > k + p steps apart, and a 0 if they occurred < k steps apart. Then, these paired states are passed into a shared-weight convolutional network, which creates an embedding, and, from that embedding, a prediction is made as to whether they\u2019re closer than the thresholds or farther away. This network is pre-trained before the actual RL training starts. (Minor sidenote: at RL-training time, the network is chopped into two, and the embedding read out and stored, and then input as a pair with each current observation to make the prediction). \n\nhttps://i.imgur.com/1oUWKyb.png\n\nOverall, the authors find that their method works better than both ICM and no-intrinsic-reward for VizDoom (a maze + shooting game), and the advantage is stronger in situations more sparse settings of the external reward. \n\nhttps://i.imgur.com/4AURZbX.png\n\nOn DeepMind Lab tasks, they saw no advantage on tasks with already-dense extrinsic rewards, and little advantage on the \u201cnormally sparse\u201d, which they suggest may be due to it actually being easier than expected. They added doors to a maze navigation task, to ensure the agent couldn\u2019t find the target right away, and this situation brought better performance of their method. They also tried a fully no-extrinsic-reward situation, and their method strongly performed both the ICM baseline and (obviously) the only-extrinsic-reward baseline, which was basically an untrained random policy in this setting. Regarding the poor performance of the ICM baseline in this environment, \u201cwe hypothesise that the agent can most significantly change its current view when it is close to the wall \u2014 thus increasing one-step prediction error \u2014 so it tends to get stucknear \u201cinteresting\u201d diverse textures on the walls.\u201d. \n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.02274"
    },
    "305": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1808.04355",
        "transcript": "I really enjoyed this paper - in addition to being a clean, fundamentally empirical work, it was also clearly written, and had some pretty delightful moments of quotable zen, which I\u2019ll reference at the end. The paper\u2019s goal is to figure out how far curiosity-driven learning alone can take reinforcement learning systems, without the presence of an external reward signal. \u201cIntrinsic\u201d reward learning is when you construct a reward out of internal, inherent features of the environment, rather than using an explicit reward function. In some ways, intrinsic learning in RL can be thought of as analogous to unsupervised learning in classification problems, since reward functions are not inherent to most useful environments, and (when outside of game environments that inherently provide rewards), frequently need to be hand-designed. Curiosity-driven learning is a subset of intrinsic learning, which uses as a reward signal the difference between a prediction made by the dynamics model (predicting next state, given action) and the true observed next state. Situations where the this prediction area are high generate high reward for the agent, which incentivizes it to reach those states, which allows the dynamics model to then make ever-better predictions about them. \n\nTwo key questions this paper raises are: \n\n\n1) Does this approach even work when used on it\u2019s own? Curiosity had previously most often been used as a supplement to extrinsic rewards, and the authors wanted to know how far it could go separately. \n\n\n2) What is the best feature to do this \u201csurprisal difference\u201d calculation in? Predicting raw pixels is a high-dimensional and noisy process, so naively we might want something with fewer, more informationally-dense dimensions, but it\u2019s not obvious which methods that satisfy these criteria will work the best, so the paper empirically tried them. \n\nThe answer to (1) seems to be: yes, at least in the video games tested. Impressively, when you track against extrinsic reward (which, again, these games have, but we\u2019re just ignoring in a curiosity-only setting), the agents manage to increase it despite not optimizing against it directly. There were some Atari games where this effect was stronger than others, but overall performance was stronger than might have been naively expected. One note the authors made, worth keeping in mind, is that it\u2019s unclear how much of this is an artifact of the constraints and incentives surrounding game design, which might reflect back a preference for gradually-increasing novelty because humans find it pleasant. \n\nhttps://i.imgur.com/zhl39vo.png\n\nAs for (2), another interesting result of this paper is that random features performed consistently well as a feature space to do these prediction/reality comparisons in. Random features here is really just as simple as \u201cdesign a convolutional net that compresses down to some dimension, randomly initialize it, and then use those randomly initialized weights to run forward passes of the network to get your lower-dimensional state\u201d. This has the strong disadvantage of (presumably) not capturing any meaningful information about the state, but also has the advantage of being stable: the other techniques tried, like pulling out the center of a VAE bottleneck, changed over time as they were being trained on new states, so they were informative, but non-stationary. \n\nMy two favorite quotable moments from this paper were: \n1) When the authors noted that they had removed the \u201cdone\u201d signal associated with an agent \u201cdying,\u201d because it is itself a sort of intrinsic reward. However, \u201cin practice, we do find that the agent avoids dying in the games since that brings it back to the beginning of the game, an area it has already seen many times and where it can predict the dynamics well.\u201d. Short and sweet: \u201cAvoiding death, because it\u2019s really boring\u201d \n\nhttps://i.imgur.com/SOfML8d.png\n\n2) When they noted that an easy way to hack the motivation structure of a curiosity-driven agent was through a \u201cnoisy tv\u201d, which, every time you pressed the button, jumped to a random channel. As expected, when they put this distraction inside a maze, the agent spent more time jacking up reward through that avenue, rather than exploring. Any resemblance to one\u2019s Facebook feed is entirely coincidental. \n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1808.04355"
    },
    "306": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1809.04474",
        "transcript": "This paper posits that one of the central problems stopping multi-task RL - that is, single models trained to perform multiple tasks well - from reaching better performance, is the inability to balance model resources and capacity between the different tasks the model is being asked to learn. Empirically, prior to this paper, multi-task RL could reach ~50% of human accuracy on Atari and Deepmind Lab tasks. The fact that this is lower than human accuracy is actually somewhat less salient than the fact that it\u2019s quite a lot lower than single-task RL - how a single model trained to perform only that task could do. \n\nWhen learning a RL model across multiple tasks, the reward structures of the different tasks can vary dramatically. Some can have high-magnitude, sparse rewards, some can have low magnitude rewards throughout. If a model learns it can gain what it thinks is legitimately more reward by getting better at a game with an average reward of 2500 than it does with an average reward of 15, it will put more capacity into solving the former task. Even if you apply normalization strategies like reward clipping (which treats all rewards as a binary signal, regardless of magnitude, and just seeks to increase the frequency of rewards), that doesn\u2019t deal with some environments having more frequent rewards than others, and thus more total reward when summed over timestep. \n\n\nThe authors here try to solve this problem by performing a specific kind of normalization, called Pop Art normalization, on the problem. PopArt normalization (don\u2019t worry about the name) works by adaptively normalizing both the target and the estimate of the target output by the model, at every step. In the Actor-Critic case that this model is working on, the target and estimate that are being normalized are, respectively, 1) the aggregated rewards of the trajectories from state S onward, and 2) the value estimate at state S. If your value function is perfect, these two things should be equivalent, and so you optimize your value function to be closer to the true rewards under your policy. And, then, you update your policy to increase probability of actions with higher advantage (expected reward with that action, relative to the baseline Value(S) of that state).  The \u201cadaptive\u201d part of that refers to correcting for the fact when you\u2019re estimating, say, a Value function to predict the total future reward of following a policy at a state, that V(S) will be strongly non-stationary, since by improving your policy you are directly optimizing to increase that value. This is done by calculating \u201cscale\u201d and \u201cshift\u201d parameters off of a recent data. \n\nThe other part of the PopArt algorithm works by actually updating the estimate our model is producing, to stay normalized alongside the continually-being-re-normalized target. \n\nhttps://i.imgur.com/FedXTfB.png\nIt does this by taking the new and old versions of scale (sigma) and shift (mu) parameters (which will be used to normalize the target) and updates the weights and biases of the last layer, such that the movement of the estimator moves along with the movement in the target. \n\nUsing this toolkit, this paper proposes learning one *policy* that\u2019s shared over all task, but keeping shared value estimation functions for each task. Then, it normalizes each task\u2019s values independently, meaning that each task ends up contributing equal weight to the gradient updates of the model (both for the Value and Policy updates). In doing this, the authors find dramatically improved performance at both Atari and Deepmind, relative to prior IMPALA work\nhttps://i.imgur.com/nnDcjNm.png\nhttps://i.imgur.com/Z6JClo3.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1809.04474"
    },
    "307": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.01561",
        "transcript": "This reinforcement learning paper starts with the constraints imposed an engineering problem - the need to scale up learning problems to operate across many GPUs - and ended up, as a result, needing to solve an algorithmic problem along with it. \n\nIn order to massively scale up their training to be able to train multiple problem domains in a single model, the authors of this paper implemented a system whereby many \u201cworker\u201d nodes execute trajectories (series of actions, states, and reward) and then send those trajectories back to a \u201clearner\u201d node, that calculates gradients and updates a central policy model. However, because these updates are queued up to be incorporated into the central learner, it can frequently happen that the policy that was used to collect the trajectories is a few steps behind from the policy on the central learner to which its gradients will be applied (since other workers have updated the learner since this worker last got a policy download). This results in a need to modify the policy network model design accordingly. \n\nIMPALA (Importance Weighted Actor Learner Architectures) uses an \u201cActor Critic\u201d model design, which means you learn both a policy function and a value function. The policy function\u2019s job is to choose which actions to take at a given state, by making some higher probability than others. The value function\u2019s job is to estimate the reward from a given state onward, if a certain policy p is followed. The value function is used to calculate the \u201cadvantage\u201d of each action at a given state, by taking the reward you receive through action a (and reward you expect in the future), and subtracting out the value function for that state, which represents the average future reward you\u2019d get if you just sampled randomly from the policy from that point onward. The policy network is then updated to prioritize actions which are higher-advantage. If you\u2019re on-policy, you can calculate a value function without needing to explicitly calculate the probabilities of each action, because, by definition, if you take actions according to your policy probabilities, then you\u2019re sampling each action with a weight proportional to its probability. However, if your actions are calculated off-policy, you need correct for this, typically by calculating an \u201cimportance sampling\u201d ratio, that multiplies all actions by a probability under the desired policy divided by the probability under the policy used for sampling. This cancels out the implicit probability under the sampling policy, and leaves you with your actions scaled in proportion to their probability under the policy you\u2019re actually updating. IMPALA shares the basic structure of this solution, but with a few additional parameters to dynamically trade off between the bias and variance of the model. \n\nThe first parameter, rho, controls how much bias you allow into your model, where bias here comes from your model not being fully corrected to \u201cpretend\u201d that you were sampling from the policy to which gradients are being applied. The trade-off here is that if your policies are far apart, you might downweight its actions so aggressively that you don\u2019t get a strong enough signal to learn quickly. However, the policy you learn might be statistically biased. Rho does this by weighting each value function update by: \n\nhttps://i.imgur.com/4jKVhCe.png\n\nwhere rho-bar is a hyperparameter. If rho-bar is high, then we allow stronger weighting effects, whereas if it\u2019s low, we put a cap on those weights. \n\nThe other parameter is c, and instead of weighting each value function update based on policy drift at that state, it weights each timestep based on how likely or unlikely the action taken at that timestep was under the true policy. \n\nhttps://i.imgur.com/8wCcAoE.png\n\nTimesteps that much likelier under the true policy are upweighted, and, once again, we use a hyperparameter, c-bar, to put a cap on the amount of allowed upweighting. Where the prior parameter controlled how much bias there was in the policy we learn, this parameter helps control the variance - the higher c-bar, the higher the amount of variance there will be in the updates used to train the model, and the longer it\u2019ll take to converge. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.01561"
    },
    "308": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1811.02549",
        "transcript": "This paper\u2019s high-level goal is to evaluate how well GAN-type structures for generating text are performing, compared to more traditional maximum likelihood methods. In the process, it zooms into the ways that the current set of metrics for comparing text generation fail to give a well-rounded picture of how models are performing. \n\nIn the old paradigm, of maximum likelihood estimation, models were both trained and evaluated on a maximizing the likelihood of each word, given the prior words in a sequence. That is, models were good when they assigned high probability to true tokens, conditioned on past tokens. However, GANs work in a fundamentally new framework, in that they aren\u2019t trained to increase the likelihood of the next (ground truth) word in a sequence, but to generate a word that will make a discriminator more likely to see the sentence as realistic. Since GANs don\u2019t directly model the probability of token t, given prior tokens, you can\u2019t evaluate them using this maximum likelihood framework. \n\nThis paper surveys a range of prior work that has evaluated GANs and MLE models on two broad categories of metrics, occasionally showing GANs to perform better on one or the other, but not really giving a way to trade off between the two. \n- The first type of metric, shorthanded as \u201cquality\u201d, measures how aligned the generated text is with some reference corpus of text: to what extent your generated text seems to \u201ccome from the same distribution\u201d as the original. BLEU, a heuristic frequently used in translation, and also leveraged here, measures how frequently certain sets of n-grams occur in the reference text, relative to the generated text. N typically goes up to 4, and so in addition to comparing the distributions of single tokens in the reference and generated, BLEU also compares shared bigrams, trigrams, and quadgrams (?) to measure more precise similarity of text. \n- The second metric, shorthanded as \u201cdiversity\u201d measures how different generated sentences are from one another. If you want to design a model to generate text, you presumably want it to be able to generate a diverse range of text - in probability terms, you want to fully sample from the distribution, rather than just taking the expected or mean value. Linguistically, this would be show up as a generator that just generates the same sentence over and over again. This sentence can be highly representative of the original text, but lacks diversity. One metric used for this is the same kind of BLEU score, but for each generated sentence against a corpus of prior generated sentences, and, here, the goal is for the overlap to be as low as possible \n\nThe trouble with these two metrics is that, in their raw state, they\u2019re pretty incommensurable, and hard to trade off against one another. The authors of this paper try to address this by observing that all models trade off diversity and quality to some extent, just by modifying the entropy of the conditional token distribution they learn. If a distribution is high entropy, that is, if it spreads probability out onto more tokens, it\u2019s likelier to bounce off into a random place, which increases diversity, but also can make the sentence more incoherent. By contrast, if a distribution is too low entropy, only ever putting probability on one or two words, then it will be only ever capable of carving out a small number of distinct paths through word space. \nThe below table shows a good example of what language generation can look like at high and low levels of entropy \nhttps://i.imgur.com/YWGXDaJ.png\n\nThe entropy of a softmax distribution be modified, without changing the underlying model, by changing the *temperature* of the softmax calculation. So, the authors do this, and, as a result, they can chart out that model\u2019s curve on the quality/diversity axis. Conceptually, this is asking \u201cat a range of different quality thresholds, how good is this model\u2019s diversity,\u201d and vice versa. I mentally analogize this to a ROC curve, where it\u2019s not really possible to compare, say, precision of models that use different thresholds, and so you instead need to compare the curve over a range of different thresholds, and compare models on that.  \n\nhttps://i.imgur.com/C3zdEjm.png\n\nWhen they do this for GANs and MLEs, they find that, while GANs might dominate on a single metric at a time, when you modulate the temperature of MLE models, they\u2019re able to achieve superior quality when you tune them to commensurate levels of diversity. \n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1811.02549"
    },
    "309": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.05473",
        "transcript": "GANs for images have made impressive progress in recent years, reaching ever-higher levels of subjective realism. It\u2019s also interesting to think about domains where the GAN architecture is less of a good fit. An example of one such domain is natural language. \n\nAs opposed to images, which are made of continuous pixel values, sentences are fundamentally sequences of discrete values: that is, words. In a GAN, when the discriminator makes its assessment of the realness of the image, the gradient for that assessment can be backpropagated through to the pixel level. The discriminator can say \u201cmove that pixel just a bit, and this other pixel just a bit, and then I\u2019ll find the image more realistic\u201d. However, there is no smoothly flowing continuous space of words, and, even if you use continuous embeddings of words, it\u2019s still the case that if you tried to apply a small change to a embedding vector, you almost certainly wouldn\u2019t end up with another word, you\u2019d just be somewhere in the middle of nowhere in word space. In short: the discrete nature of language sequences doesn\u2019t allow for gradient flow to propagate backwards through to the generator. \n\nThe authors of this paper propose a solution: instead of trying to treat their GAN as one big differentiable system, they framed the problem of \u201cgenerate a sequence that will seem realistic to the discriminator\u201d as a reinforcement learning problem? After all, this property - of your reward just being generated *somewhere* in the environment, not something analytic, not something you can backprop through -  is one of the key constraints of reinforcement learning. Here, the more real the discriminator finds your sequence, the higher the reward. One approach to RL, and the one this paper uses, is that of a policy network, where your parametrized network produces a distribution over actions. You can\u2019t update your model to deterministically increase reward, but you can shift around probability in your policy such that your expected reward of following that policy is higher. \n\nThis key kernel of an idea - GANs for language, but using a policy network framework to get around not having backprop-able loss/reward- gets you most of the way to understanding what these authors did, but it\u2019s still useful to mechanically walk through specifics. \n\nhttps://i.imgur.com/CIFuGCG.png\n\nAt each step, the \u201cstate\u201d is the existing words in the sequence, and the agent\u2019s \u201caction\u201d the choosing of its next word\n- The Discriminator can only be applied to completed sequences, since it's difficult to determine whether an incoherent half-sentence is realistic language. So, when the agent is trying to calculate the reward of an action at a state, it uses Monte Carlo Tree Search: randomly \u201crolling out\u201d many possible futures by randomly sampling from the policy, and then taking the average Discriminator judgment of all those futures resulting from each action as being its expected reward\n- The Generator is a LSTM that produces a softmax over words, which can be interpreted as a policy if it\u2019s sampled from randomly \n- One of the nice benefits of this approach is that it can work well for cases where we don't have a hand-crafted quality assessment metric, the way we have BLEU score for translation ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1609.05473"
    },
    "310": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1811.01778",
        "transcript": "I should say from the outset: I have a lot of fondness for this paper. It goes upstream of a lot of research-community incentives: It\u2019s not methodologically flashy, it\u2019s not about beating the State of the Art with a bigger, better model (though, those papers certainly also have their place). The goal of this paper was, instead, to dive into a test set used to evaluate performance of models, and try to understand to what extent it\u2019s really providing a rigorous test of what we want out of model behavior. Test sets are the often-invisible foundation upon which ML research is based, but like real-world foundations, if there are weaknesses, the research edifice built on top can suffer. \n\nSpecifically, this paper discusses the Winograd Schema, a clever test set used to test what the NLP community calls \u201ccommon sense reasoning\u201d. An example Winograd Schema sentence is: \nThe delivery truck zoomed by the school bus because it was going so fast.\nA model is given this task, and asked to predict which token the underlined \u201cit\u201d refers to. These cases are specifically chosen because of their syntactic ambiguity - nothing structural about the order of the sentence requires \u201cit\u201d to refer to the delivery truck here. However, the underlying meaning of the sentence is only coherent under that parsing. This is what is meant by \u201ccommon-sense\u201d reasoning: the ability to understand meaning of a sentence in a way deeper than that allowed by simple syntactic parsing and word co-occurrence statistics.\n\nTaking the existing Winograd examples (and, when I said tiny, there are literally 273 of them) the authors of this paper surface some concerns about ways these examples might not be as difficult or representative of \u201ccommon sense\u201d abilities as we might like. \n\n- First off, there is the basic, previously mentioned fact that there are so few examples that it\u2019s possible to perform well simply by random chance, especially over combinatorially large hyperparameter optimization spaces. This isn\u2019t so much an indictment of the set itself as it is indicative of the work involved in creating it. \n- One of the two distinct problems the paper raises is that of \u201cassociativity\u201d. This refers to situations where simple co-occurance counts between the description and the correct entity can lead the model to the correct term, without actually having to parse the sentence. An example here is:\n\u201cI\u2019m sure that my map will show this building; it is very famous.\u201d \nTreasure maps aside, \u201cfamous buildings\u201d are much more generally common than \u201cfamous maps\u201d, and so being able to associate \u201cit\u201d with a building in this case doesn\u2019t actually require the model to understand what\u2019s going on in this specific sentence. The authors test this by creating a threshold for co-occurance, and, using that threshold, call about 40% of the examples \u201cassociative\u201d\n- The second problem is that of predictable structure - the fact that the \u201chinge\u201d adjective is so often the last word in the sentence, making it possible that the model is brittle, and just attending to that, rather than the sentence as a whole \n\nThe authors perform a few tests - examining results on associative vs non-associative examples, and examining results if you switch the ordering (in cases like \u201cEmma did not pass the ball to Janie although she saw that she was open,\u201d where it\u2019s syntactically possible), to ensure the model is not just anchoring on the identity of the correct entity, regardless of its place in the sentence. Overall, they found evidence that some of the state of the art language models perform well on the Winograd Schema as a whole, but do less well (and in some cases even less well than the baselines they otherwise outperform) on these more rigorous examples. Unfortunately, these tests don\u2019t lead us automatically to a better solution - design of examples like this is still tricky and hard to scale - but does provide valuable caution and food for thought. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1811.01778"
    },
    "311": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.06682",
        "transcript": "For solving sequence modeling problems, recurrent architectures have been historically the most commonly used solution, but, recently, temporal convolution networks, especially with dilations to help capture longer term dependencies, have gained prominence. RNNs have theoretically much larger capacity to learn long sequences, but also have a lot of difficulty propagating signal forward through long chains of recurrent operations. This paper, which suggests the approach of Trellis Networks, places itself squarely in the middle of the debate between these two paradigms. TrellisNets are designed to be a theoretical bridge between between temporal convolutions and RNNs - more specialized than the former, but more generalized than the latter. \nhttps://i.imgur.com/J2xHYPx.png\nThe architecture of TrellisNets is very particular, and, unfortunately, somewhat hard to internalize without squinting at diagrams and equations for awhile. Fundamentally: \n- At each layer in a TrellisNet, the network creates a \u201ccandidate pre-activation\u201d by combining information from the input and the layer below, for both the current and former time step. \n- This candidate pre-activation is then non-linearly combined with the prior layer, prior-timestep hidden state \n- This process continues for some desired number of layers. \n\nhttps://i.imgur.com/f96QgT8.png\n\nAt first glance, this structure seems pretty arbitrary: a lot of quantities connected together, but without a clear mechanic for what\u2019s happening. However, there are a few things interesting to note here, which will help connect these dots, to view TrellisNet as either a kind of RNN or a kind of CNN: \n- TrellisNet uses the same weight matrices to process prior and current timestep inputs/hidden states, no matter which timestep or layer it\u2019s on. This is strongly reminiscent of a recurrent architecture, which uses the same calculation loop at each timestep \n- TrellisNets also re-insert the model\u2019s input at each layer. This also gives it more of a RNN-like structure, where the prior layer\u2019s values act as a kind of \u201chidden state\u201d, which are then combined with an input value  \n- At a given layer, each timestep only needs access to two elements of the prior layer (in addition to the input); it does not require access to all the prior-timestep values of it\u2019s own layer. This is important because it means that you can calculate an entire layer\u2019s values at once, given the values of the prior layer: this means these models can be more easily parallelized for training \n\nSeeing TrellisNets as a kind of Temporal CNN is fairly straightforward: each timestep\u2019s value, at a given layer, is based on a \u201cfilter\u201d of the lower-layer value at the current and prior timestep, and this filter is shared across the whole sequence. Framing them as a RNN is certainly trickier, and anyone wanting to understand it in full depth is probably best served by returning to the paper\u2019s equations. At at high level, the authors show that TrellisNets can represent a specific kind of RNN: a truncated RNN, where each timestep only uses history from the prior M time steps, rather than the full sequence. This works by sort of imagining the RNN chains as existing along the diagonals of a TrellisNet architecture diagram: as you reach higher levels, you can also reach farther back in time. Specifically, a TrellisNet that wants to represent a depth K truncated RNN, which is allowed to unroll through M steps of history, can do so using M + K - 1 layers. Essentially, by using a fixed operation across layers and timesteps, the TrellisNet authors blur the line between layer and timestep: any chain of operations, across layers, is fundamentally a series of the same operation, performed many times, and is in that way RNN-like.\n\nThe authors have not yet taken a stab at translation, but tested their model on a number of word and character-level language modeling tasks (predicting the next word or character, given prior ones), and were able to successfully beat SOTA on many of them. I\u2019d be curious to see more work broadly in this domain, and also gain a better understanding of areas in which a fixed, recurrently-used layer operation, like the ones used in RNNs and this paper, is valuables, and areas (like a \u201cnormal\u201d CNN)  where having specific weights for different levels of the hierarchy is valable.  \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.06682"
    },
    "312": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1808.04891",
        "transcript": "This paper is, on the whole, a refreshing jaunt into the applied side of the research word. It isn\u2019t looking to solve a fundamental machine learning problem in some new way, but it does highlight and explore one potential beneficial application of a common and widely used technique: specifically, combining word embeddings with context-free grammars (such as: regular expressions), to make the latter less rigid. \n\nRegular expressions work by specifying specific hardcoded patterns of symbols, and matching against any strings in some search set that match those patterns. They don\u2019t need to specify specific characters - they can work at higher levels of generality, like \u201cuppercase alphabetic character\u201d or \u201cany character\u201d, but they\u2019re still fundamentally hardcoded, in that the designer of the expression needs to create a specification that will affirmatively catch all the desired cases. This can be a particular challenging task when you\u2019re trying to find - for example - all sentences that match the pattern of someone giving someone else a compliment. You might want to match against \u201cI think you\u2019re smart\u201d and also \u201cI think you\u2019re clever\u201d. However, in the normal use of regular expressions, something like this would be nearly impossible to specify, short of writing out every synonym for \u201cintelligent\u201d that you can think of. \n\nThe \u201cEmbedding Grammars\u201d paper proposes a solution to this problem: instead of enumerating a list of synonyms, simply provide one example term, or, even better, a few examples, and use those term\u2019s word embedding representation to define a \u201csynonym bubble\u201d (my word, not theirs) in continuous space around those examples. This is based on the oft-remarked-upon fact that, because word embedding systems are generally trained to push together words that can be used in similar contexts, closeness in word vector space frequently corresponds to words being synonyms, or close in some other sense. So, if you \u201cmatch\u201d to any term that is sufficiently nearby to your exemplar terms, you are performing something similar to the task of enumerating all of a term\u2019s syllables. Once this general intuition is in hand, the details of the approach are fairly straightforward: the authors try a few approaches, and find that constructing a bubble of some epsilon around each example\u2019s word vector, and matching to anything inside that bubble, works the best as an approach. \n\nhttps://i.imgur.com/j9OSNuE.png\n\nOverall, this seems like a clever idea; one imagines that the notion of word embeddings will keep branching out into ever more far-flung application as time goes on. There are reasons to be skeptical of this paper, though. Fundamentally, word embedding space is a \u201chere there be dragons\u201d kind of place: we may be able to observe broad patterns, and might be able to say that \u201cnearby words tend to be synonyms,\u201d but we can\u2019t give any kind of guarantee of that being the case. As an example of this problem, often the nearest thing to an example, after direct synonyms, are direct antonyms, so if you set too high a threshold, you\u2019ll potentially match to words exactly the opposite of what you expect. We are probably still a ways away from systems like this one being broady useful, for this and other reasons, but I do think it\u2019s valuable to try to understand what questions we\u2019d want answered, what features of embedding space we\u2019d want more elucidated, before applications like these would be more stably usable. \n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1808.04891"
    },
    "313": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.13409",
        "transcript": "I admit it - the title of the paper pulled me in, existing as it does in the chain of weirdly insider-meme papers, starting with Vaswani\u2019s 2017 \u201cAttention Is All You Need\u201d. That paper has been hugely influential, and the domain of machine translation as a whole has begun to move away from processing (or encoding) source sentences with recurrent architectures, to instead processing them using self-attention architectures. (Self-attention is a little too nuanced to go into in full depth here, but the basic idea is: instead of summarizing varying-length sequences by feeding each timestep into a recurrent loop and building up hidden states, generate a query, and weight the contribution of each timestep to each \u201chidden state\u201d based on the dot product between that query and each timestep\u2019s representation). There has been an overall move in recent years away from recurrence being the accepted default for sequence data, and towards attention and (often dilated) convolution taking up more space. I find this an interesting set of developments, and had hopes that this paper would address that arc. \n\nHowever, unfortunately, the title was quite out of sync with the actual focus of the paper - instead of addressing the contribution of attention mechanisms vs recurrence, or even directly addressing any of the particular ideas posed in the \u201cAttention is All You Need\u201d paper, this YMNNA instead takes aim at a more fundamental structural feature of translation models: the encoder/decoder structure. The basic idea of an encoder/decoder approach, in a translation paradigm, is that you process the entire source sentence before you start generating the tokens of the predicted, other-language target sentence. Initially, this would work by running a RNN over the full sentence, and using the final hidden state of that RNN as a compressed representation of the full sentence. More recently, the norm has been to use multiple layers of RNN, and to represent the source sentence via the hidden states at each timestep (so: as many hidden states as you have input tokens), and then at each step in the decoding process, calculate an attention-weighted average over all of those hidden states. But, fundamentally, both of these structures share the fact that some kind of global representation is calculated and made available to the decoder before it starts predicting words in the output sentence. \n\nThis makes sense for a few reasons. First, and most obviously, languages aren\u2019t naturally aligned with one another, in the sense of one word in language X corresponding to one word in language Y. It\u2019s not possible for you to predict a word in the target sentence if its corresponding source sentence token has not yet been processed. For another, there can be contextual information from the sentence as a whole that can disambiguate between different senses of a word, which may have different translations - think Teddy Bear vs Teddy Roosevelt. However, this paper poses the question: how well can you do if you throw away this structure, and build a model that continually emits tokens of the target sequence as it reads in the source sentence? Using a recurrent model, the YMNNA model takes, at each timestep, the new source token, the previous target token, and the prior hidden state from the last time step of the RNN, and uses that to predict a token.\n\nHowever, that problem mentioned earlier - of languages not natively being aligned such that you have the necessary information to predict a word by the time you get to its point in the target sequence - hasn\u2019t gone away, and is still alive and kicking. This paper solves it in a pretty unsatisfying way - by relying on an external tool, fast-align, that does the work of guessing which source tokens correspond to which target tokens, and inserting buffer tokens into the target, so that you don\u2019t need to predict a word until it\u2019s already been seen by the source-reading RNN; until then you just predict the buffer. This is fine and clever as a practical heuristic, but it really does make their comparisons against models that do alignment and translation jointly feel a little weak. \n\nhttps://i.imgur.com/Gitpxi7.png\n\nAn additional heuristic that makes the overall narrative of the paper less compelling is the fact that, in order to get comparable performance to their baselines, they padded the target sequences with between 3 and 5 buffer tokens, meaning that the models learned that they could process the first 3-5 tokens of the sentence before they need to start emitting the target. Again, there\u2019s nothing necessarily wrong with this, but, since they are consuming a portion of the sentence before they start emitting translations, it does make for a less stark comparison with the \u201cread the whole sentence\u201d encoder/decoder framework. \n\nA few other frustrations, and notes from the paper\u2019s results section: \nAs earlier mentioned, the authors don\u2019t actually compare their work against the \u201cAttention is All You Need\u201d paper, but instead to a 2014 paper. This is confusing both in terms of using an old baseline for SOTA, and also in terms of their title implicitly arguing they are refuting a paper they didn\u2019t compare to \nComparing against their old baseline, their eager translation model performs worse on all sentences less than 60 tokens in length (which makes up the vast majority of all the sentences there are), and only beats the baseline on sentences > 60 tokens in length \nAdditionally, they note as a sort of throwaway line that their model took almost three times as long to train as the baseline, with the same amount of parameters, simply because it took so much longer to converge. \n\nBeing charitable, it seems like there is some argument that an eager translation framework performs well on long sentences, and can do so while only keeping a hidden state in memory, rather than having to keep the hidden states for each source sequence element around, like attention-based decoders require. However, overall, I found this paper to be a frustrating let-down, that used too many heuristics and hacks to be a compelling comparison to prior work. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.13409"
    },
    "314": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.04805",
        "transcript": "The last two years have seen a number of improvements in the field of language model pretraining, and BERT - Bidirectional Encoder Representations from Transformers - is the most recent entry into this canon. The general problem posed by language model pretraining is: can we leverage huge amounts of raw text, which aren\u2019t labeled for any specific classification task, to help us train better models for supervised language tasks (like translation, question answering, logical entailment, etc)? Mechanically, this works by either 1) training word embeddings and then using those embeddings as input feature representations for supervised models, or 2) treating the problem as a transfer learning problem, and fine-tune to a supervised task - similar to how you\u2019d fine-tune a model trained on ImageNet by carrying over parameters, and then training on your new task. Even though the text we\u2019re learning on is strictly speaking unsupervised (lacking a supervised label), we need to design a task on which we calculate gradients in order to train our representations. For unsupervised language modeling, that task is typically structured as predicting a word in a sequence given prior words in that sequence. Intuitively, in order for a model to do a good job at predicting the word that comes next in a sentence, it needs to have learned patterns about language, both on grammatical and semantic levels. A notable change recently has been the shift from learning unconditional word vectors (where the word\u2019s representation is the same globally) to contextualized ones, where the representation of the word is dependent on the sentence context it\u2019s found in. All the baselines discussed here are of this second type. \n\nThe two main baselines that the BERT model compares itself to are OpenAI\u2019s GPT, and Peters et al\u2019s ELMo. The GPT model uses a self-attention-based Transformer architecture, going through each word in the sequence, and predicting the next word by calculating an attention-weighted representation of all prior words. (For those who aren\u2019t familiar, attention works by multiplying a \u201cquery\u201d vector with every word in a variable-length sequence, and then putting the outputs of those multiplications into a softmax operator, which inherently gets you a weighting scheme that adds to one). ELMo uses models that gather context in both directions, but in a fairly simple way: it learns one deep LSTM that goes from left to right,  predicting word k using words 0-k-1, and a second LSTM that goes from right to left, predicting word k using words k+1 onward. These two predictions are combined (literally: just summed together) to get a representation for the word at position k. \n\nhttps://i.imgur.com/2329e3L.png\n\nBERT differs from prior work in this area in several small ways, but one primary one: instead of representing a word using only information from words before it, or a simple sum of prior information and subsequent information, it uses the full context from before and after the word in each of its multiple layers. It also uses an attention-based Transformer structure, but instead of incorporating just prior context, it pulls in information from the full sentence. To allow for a model that actually uses both directions of context at a time in its unsupervised prediction task, the authors of BERT slightly changed the nature of that task: it replaces the word being predicted with the \u201cmask\u201d token, so that even with multiple layers of context aggregation on both sides, the model doesn\u2019t have any way of knowing what the token is. By contrast, if it weren\u2019t masked, after the first layer of context aggregation, the representations of other words in the sequence would incorporate information about the predicted word k, making it trivial, if another layer were applied on top of that first one, for the model to directly have access to the value it\u2019s trying to predict. This problem can either be solved by using multiple layers, each of which can only see prior context (like GPT), by learning fully separate L-R and R-L models, and combining them at the final layer (like ELMo) or by masking tokens, and predicting the value of the masked tokens using the full remainder of the context. \n\nThis task design crucially allows for a multi-layered bidirectional architecture, and consequently a much richer representation of context in each word\u2019s pre-trained representation. BERT demonstrates dramatic improvements over prior work when fine tuned on a small amount of supervised data, suggesting that this change added substantial value.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.04805"
    },
    "315": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.02334",
        "transcript": "This recent paper, a collaboration involving some of the authors of MAML, proposes an intriguing application of techniques developed in the field of meta learning to the problem of unsupervised learning - specifically, the problem of developing representations without labeled data, which can then be used to learn quickly from a small amount of labeled data. As a reminder, the idea behind meta learning is that you train models on multiple different tasks, using only a small amount of data from each task, and update the model based on the test set performance of the model. \n\nThe conceptual advance proposed by this paper is to adopt the broad strokes of the meta learning framework, but apply it to unsupervised data, i.e. data with no pre-defined supervised tasks. The goal of such a project is, as so often is the case with unsupervised learning, to learn representations, specifically, representations we believe might be useful over a whole distribution of supervised tasks. However, to apply traditional meta learning techniques, we need that aforementioned distribution of tasks, and we\u2019ve defined our problem as being over unsupervised data.  How exactly are we supposed to construct the former out of the latter? This may seem a little circular, or strange, or definitionally impossible: how can we generate supervised tasks without supervised labels? \n\nhttps://i.imgur.com/YaU1y1k.png\n\nThe artificial tasks created by this paper are rooted in mechanically straightforward operations, but conceptually interesting ones all the same: it uses an off the shelf unsupervised learning algorithm to generate a fixed-width vector embedding of your input data (say, images), and then generates multiple different clusterings of the embedded data, and then uses those cluster IDs as labels in a faux-supervised task. It manages to get multiple different tasks, rather than just one - remember, the premise of meta learning is in models learned over multiple tasks - by randomly up and down-scaling dimensions of the embedding before clustering is applied. Different scalings of dimensions means different points close to one another, which means the partition of the dataset into different clusters. \n\nWith this distribution of \u201csupervised\u201d tasks in hand, the paper simply applies previously proposed meta learning techniques - like MAML, which learns a model which can be quickly fine tuned on a new task, or prototypical networks, which learn an embedding space in which observations from the same class, across many possible class definitions are close to one another.\n\nhttps://i.imgur.com/BRcg6n7.png\n\nAn interesting note from the evaluation is that this method - which is somewhat amusingly dubbed \u201cCACTUs\u201d - performs best relative to alternative baselines in cases where the true underlying class distribution on which the model is meta-trained is the most different from the underlying class distribution on which the model is tested. Intuitively, this makes reasonable sense: meta learning is designed to trade off knowledge of any given specific task against the flexibility to be performant on a new class division, and so it gets the most value from trade off where a genuinely dissimilar class split is seen during testing. \n\nOne other quick thing I\u2019d like to note is the set of implicit assumptions this model builds on, in the way it creates its unsupervised tasks. First, it leverages the smoothness assumptions of classes - that is, it assumes that the kinds of classes we might want our model to eventually perform on are close together, in some idealized conceptual space. While not a perfect assumption (there\u2019s a reason we don\u2019t use KNN over embeddings for all of our ML tasks) it does have a general reasonableness behind it, since rarely are the kinds of classes very conceptually heterogenous. Second, it assumes that a truly unsupervised learning method can learn a representation that, despite being itself sub-optimal as a basis for supervised tasks, is a well-enough designed feature space for the general heuristic of \u201cnearby things are likely of the same class\u201d to at least approximately hold. I find this set of assumptions interesting because they are so simplifying that it\u2019s a bit of a surprise that they actually work: even if the \u201cclasses\u201d we meta-train our model on are defined with simple Euclidean rules, optimizing to be able to perform that separation using little data does indeed seem to transfer to the general problem of \u201cseparating real world, messier-in-embedding-space classes using little data\u201d.  \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.02334"
    },
    "316": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1505.05770",
        "transcript": "This paper argues for the use of normalizing flows - a way of building up new probability distributions by applying multiple sets of invertible transformations to existing distributions - as a way of building more flexible variational inference models. \n\nThe central premise of a variational autoencoder is that of learning an approximation to the posterior distribution of latent variables - p(z|x) - and parameterizing that distribution according to values produced by a neural network. In typical practice, this has meant that VAEs are limited in terms of the complexity of latent variable distributions they can encode, since using an analytically specified distribution tends to limit you to simpler distributional shapes - Gaussians, uniform, and the like. Normalizing flows are here proposed as a way to allow for the model to learn more complex forms of posterior distribution. \n\nNormalizing flows work off of a fairly simple intuition: if you take samples from a distribution p(x), and then apply a function f(x) to each x in that sample, you can calculate the expected value of your new distribution f(x) by calculating the expectation of f(x) under the old distribution p(x). That is to say: \nhttps://i.imgur.com/NStm7zN.png\nThis mathematical transformation has a pretty delightful name - The Law of the Unconscious Statistician - that came from the fact that so many statisticians just treated this identity as a definitional fact, rather than something actually in need of proving (I very much fall into this bucket as well). The implication of this is that if you apply many transformations in sequence to the draws from some simple distribution, you can work with that distribution without explicitly knowing its analytical formulation, just by being able to evaluate - and, importantly - invert the function. The ability to invert the function is key, because of the way you calculate the derivative: by taking the inverse of the determinant of the derivative of your function f(z) with respect to z. (Note here that q(z) is the original distribution you sampled under, and q\u2019(z) is the implicit density you\u2019re trying to estimate, after your function has been applied). \n\nhttps://i.imgur.com/8LmA0rc.png\n\nCombining these ideas together: a variational flow autoencoder works by having an encoder network define the parameters of a simple distribution (Gaussian or Uniform), and then running the samples from that distribution through a series of k transformation layers. This final transformed density over z is then given to the decoder to work with. Some important limitations are in place here, the most salient of which is that in order to calculate derivatives, you have to be able to calculate the determinant of the derivative of a given transformation. Due to this constraint, the paper only tests a few transformations where this is easy to calculate analytically - the planar transformation and radial transformation. If you think about transformations of density functions as fundamentally stretching or compressing regions of density, the planar transformation works by stretching along an axis perpendicular to some parametrically defined plane, and the radial transformation works by stretching outward in a radial way around some parametrically defined point. Even though these transformations are individually fairly simple, when combined, they can give you a lot more flexibility in distributional space than a simple Gaussian or Uniform could. \n\nhttps://i.imgur.com/Xf8HgHl.png\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1505.05770"
    },
    "317": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1806.10474",
        "transcript": "This paper draws from two strains of recent work: the hierarchical music modeling of MusicVAE - which intentionally model musical structure at both local and more global levels - , and the discrete autoencoder approaches of Vector Quantized VAEs - which seek to maintain the overall structure of a VAE, but apply a less aggressive form of regularization.\n\nThe goal of this paper is to build a model that can generate music, not from that music\u2019s symbolic representation - lists of notes - but from actual waveform audio. This is a more difficult task because the model now has to learn mappings between waveforms and symbolic notes, but confers the advantage of being able to model expressive dimensions of music that are difficult to capture in a pure symbolic representation. Models of pure waveform data have been used before - Wavenet is a central example - but typically they are learned alongside some kind of text conditioning structure, which is to say, you tell the model to say \u201cHello there, world\u201d and the model is only responsible for building local mappings between those phonemes and waveforms, not actually modeling coherent words to follow after \u201cHello\u201d. To try to address this problem, the authors of the paper propose the solution of learning an autoencoded representation over the full music sample, to try to capture global structure. Each predicted value of the global structure sequence then represents some number of timesteps of the generated sequence: say, 20. The idea here is: learn a global model that produces 1/N (1/20, in this case) fewer sequence points, whose job is ensuring long term consistency. Then, the authors also suggest the use of a lower level decoder model that uses the conditioning information from the autoencoder, and, in a similar fashion to a text to speech wavenet, captures a high fidelity mapping between that conditioning and the output waveform. \n\nThis overall structure has a lot in common with the recently released MusicVAE paper. The most salient architectural change proposed by this paper is that of Argmax VAEs, rather than VQ VAEs. Overall, the reason for training discrete autoencoders is to have a more easily adjustable way of regularizing the bottlenecked representation, to avoid the fact that for some challenging problems, excessively strong VAE regularization can lead to that high level representational space just not being used. To understand the difference, it\u2019s worth understanding that VQ VAEs work by generating a continuous encoding vector (the same as a typical VAE) but then instead of passing that continuous vector itself directly on to the decoder, the VQ VAE instead fits what is basically a K means operation: it maps the continuous vector to one of it\u2019s \u201cprototypical\u201d or \u201ccodebook\u201d vectors based on closeness in Euclidean distance (these codebook vectors are learned in a separate trading loop, in a K Means style algorithm). The Argmax VAE is similar, but instead of needing to take that alternating step of learning the codebook vectors via K Means, it performs a much simpler quantization operation: just taking the argmax of indices across the continuous vector, so that the output is the one-hot vector closest to the continuous input. While this reduces the capacity of the model, it also limits the problem of \u201ccodebook collapse\u201d, which is a failure mode that can happen during the K Means iteration (I\u2019m actually not entirely clear on the prototypical example of codebook collapse, or exactly why it happens). \n\nhttps://i.imgur.com/H5YqSZG.png\n\nCombining these ideas together: this paper\u2019s model works by learning an Argmax VAE over a larger and courser timeframe of the model, and then learning a local, high resolution decoder - similar to Wavenet - over the smaller time scales, conditioned on the output of the Argmax VAE making high level decisions. This combination balances the needs of coherent musical structure and local fidelity, and allows for different weighing of those trade-offs in a fairly flexible way, by changing the frequency at which you produce Argmax VAE conditioning output. \n \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1806.10474v1"
    },
    "318": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1709.04326",
        "transcript": "A central question of this paper is: under what circumstances will you see agents that have been trained to optimize their own reward implement strategies - like tit for tat - that are are more sophisticated and higher overall reward than each agent simply pursuing its dominant strategy. The games under consideration here are \u201cgeneral sum\u201d games like Iterated Prisoner\u2019s Dilemma, where each agent\u2019s dominant strategy is to defect, but with some amount of coordination or reciprocity, better overall outcomes are possible. Previously, models have achieved this via explicit hardcoding, but this paper strove to use a simpler, more general approach: allowing each agent A to optimize its reward not only with regard to a fixed opponent, but with regard to an opponent that will make a predictable update move in response to the action A is about to take. \n\nSpecifically, this model  - shorthanded as LOLA, Learning with Opponent-Learning Awareness -  maximizes a given agent\u2019s expected discount reward, but looks at reward *conditional on* the ways the opponent will update to a given action. In a simplified world where the explicit reward function is known, it\u2019s possible to literally take the derivative through the opponent\u2019s expected update step, taking into account the ways your expected reward is changed by the response you expect of your opponent. Outside of this simplified framework, in the world of policy gradients, there\u2019s no analytic loss function; you can no longer directly differentiate your reward function with respect to your opponent\u2019s actions, but you can differentiate your expected reward estimator with respect to them. This concept is quite similar to a 2016 paper, Metz et al, that used this concept to train a more effective GAN, by allowing each network in the adversarial pair to \u201clook ahead\u201d to their opponent\u2019s expected response as a way to avoid getting stuck in repetitive action/response cycles. In circumstances where the parameters of the opponent are not known - obviously closer to realistic for an adversarial scenario - the paper demonstrates proof of concept ability to model an opponent\u2019s strategy based on their past actions, and use that to conduct response-step estimates. \n\nhttps://i.imgur.com/5xddJRj.png\n\nIt should of course be said in all this: even though this setup did produce results closer to what we would expect in rational reciprocity, it\u2019s still very simplified. In most of the experiments, each agent had perfect knowledge of the opponent\u2019s priorities and likely responses; in most game theory scenarios, constructing a model of your opponent is a nontrivial part of the difficulty. Nonetheless, I found it a \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1709.04326v3"
    },
    "319": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1806.05759",
        "transcript": "The overall goal of the paper is measure how similar different layer activation profiles are to one another, in hopes of being able to quantify the similarity of the representations that different layers are learning. If you had a measure that captured this, you could ask questions like: \u201chow similar are the representations that are learned by different networks on the same task\u201d, and \u201cwhat is the dynamic of representational change in a given layer throughout training\u201d? \n\nCanonical Correlation Analysis is one way of approaching this question, and the way taken by this paper. The premise of CCA is that you have two multidimensional variable sets, where each set is made up of vectors representing dimensions within that variable set. Concretely, in this paper, the sets under examination are the activation profiles of two layers (either the same layer at different points in training, or different layers in the same network, or layers in different networks). An activation profile is thought of in terms of multiple vectors, where each vector represents a given neuron\u2019s activation value, evaluated over some observation set X. Importantly, for the two layers that you\u2019re comparing, the set of observations X needs to be of the same length, but the layers can have different number of neurons (and, consequently, different numbers of vectors making up that layer\u2019s multivariate set). \n\nGiven this setup, the goal of CCA is to find vectors that are linear combinations of the basis vectors of each set, to satisfy some constraint. In that broad sense, this is similar to the project of PCA, which also constructs linear-combination principal components to better represent the underlying data space. However, in PCA, the constraints that define these combinations are based on one multidimensional feature space, not two. In CCA, instead of generating k principal components, you generate k *pairs* of canonical correlates. Each canonical correlate pair, (U1, V1) is a linear combination of the activation vectors of sets L1 and L2  respectively, and is chosen with the goal of minimizing the the angle (cosine) distance between the correlates in each pair. \n\nIf you think about L1 and L2 each only having two activations (that is: if you think about them as being two-dimensional spaces) then the goal of CCA is to find the cosine distance between the planes defined by the two activation spaces. An important intuition here is that in this framing, vector sets that are just linear transformations of one another (scalings, rotations, swaps in the arbitrary order of activations) will look the same, which wouldn\u2019t be the case if you just looked at raw correlations between the individual activations. This is connected to the linear algebra idea that, if you have two vectors, and a third that is just a linear combination of the first two, the span of those vectors is still just that two-dimensional space. This property is important for the analysis of neural network representations because it means it will be able to capture similarities between representational spaces that have fundamental geometric similarities, even if they\u2019re different on a more surface level. \n\nIn prior papers, CCA had been used by calculating the CCA vectors between varying sets of layers, and then taking the mean CCA value over all of the pairs of vectors. This paper argues against that approach, on the theory that network layers are probably not using the full representational capacity of their activation dimensions (think, as analogy: a matrix with three columns, that only actually spans two), and so including in your average very low-order correlations is mostly adding uninformative noise to your similarity measure. Instead, this paper weights the correlation coefficients according to the magnitudes of the correlate vectors in the pair; as best I can tell, this is roughly analogous to weighting according to eigenvalues, in a PCA setting. \n\nUsing this weighted-average similarity measure, the authors do some really interesting investigations into learning dynamics. These include: \n* Comparing the intermediate-layer representations learned by networks that achieve low train error via memorization vs via actually-generalizing solutions, and show that, during training, the intermediate representations of generalizing networks are more similar to one another than memorizing networks are to one another. Intuitively, this aligns with the idea that there are many ways to noisily memorize, but a more constrained number of ways to actually learn meaningful information about a dataset. A super interesting implication of this is the idea that representational similarity *on the training set* across multiple bootstrapped or randomized trainings could be used as a proxy for test set performance, which could be particularly valuable in contexts where test data is limited \n\nhttps://i.imgur.com/JwyHFmN.png\n\n* Across networks, lower layers tend to be more similar to one another than layers closer to the output; said another way, the very simple (e.g. edge detectors) tend to be quite similar across networks, but the higher level representations are more divergent and influenceable by smaller quirks of the training set. \n* Within a given dataset, you can cluster learned internal representations across many training sets and recover groups trained with the same learning rate, even though the final layer softmax is inherently similar across models that achieve the same training error. This implies that metrics like this can give us some idea of the different minima that the optimization algorithm finds, as a function of different learning rates. \n\nOverall, I found this paper a great example of a straightforward idea used to clearly answer important and interesting questions, which is always refreshing amidst a sea of \u201ctiny hack for an extra 0.05 accuracy\u201d. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1806.05759"
    },
    "320": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1703.05175",
        "transcript": "This paper describes an architecture designed for generating class predictions based on a set of features in situations where you may only have a few examples per class, or, even where you see entirely new classes at test time. Some prior work has approached this problem in ridiculously complex fashion, up to and including training a network to predict the gradient outputs of a meta-network that it thinks would best optimize loss, given a new class. The method of Prototypical Networks prides itself on being much simpler, and more intuitive, so I hope I\u2019ll be able to convey that in this explanation.  In order to think about this problem properly, it makes sense to take a few steps back, and think about some fundamental assumptions that underly machine learning. \nhttps://i.imgur.com/Q45w0QT.png\n\nOne very basic one is that you need some notion of similarity between observations in your training set, and potential new observations in your test set, in order to properly generalize.  To put it very simplistically, if a test example is very similar to examples of class A that we saw in training, we might predict it to be of class A at testing. But what does it *mean* for two observations to be similar to one another? If you\u2019re using a method like K Nearest Neighbors, you calculate a point\u2019s class identity based on the closest training-set observations to it in Euclidean space, and you assume that nearness in that space corresponds to likelihood of two data points having come the same class. This is useful for the use case of having new classes show up after training, since, well, there isn\u2019t really a training period: the strategy for KNN is just carrying your whole training set around, and, whenever a new test point comes along, calculating it\u2019s closest neighbors among those training-set points. If you see a new class in the wild, all you need to do is add the examples of that class to your group of training set points, and then after a few examples, if your assumptions hold, you\u2019ll be able to predict that class by (hopefully) finding those two or three points as neighbors. But what if some dimensions of your feature space matter much more than others for differentiating between classes? In a simplistic example, you could have twenty features, but, unbeknownst to you, only one is actually useful for separating out your classes, and the other 19 are random. If you use the naive KNN assumption, you wouldn\u2019t expect to perform well here, because you will have distances in these 19 meaningless directions spreading out your points, due to randomness, more than the meaningful dimension spread them out due to belonging to different classes. And what if you want to be able to learn non-linear relationships between your features, which the composability of multi-layer neural networks lends itself well to? In cases like those, the features you were handed may be a woefully suboptimal metric space in which to calculate a kind of similarity that corresponds to differences in class identity, so you\u2019ll just have to strike out for the territories and create a metric space for yourself. \n\nThat is, at a very high level, what this paper seeks to do: learn a transformation between input features and some vector space, such that distances in that vector space correspond as well as possible to probabilities of belonging to a given output class. You may notice me using \u201cvector space\u201d and \u201cembedding\u201d similarity; they are the same idea: the result of that learned transformation, which represents your input observations as dense vectors in some p-dimensional space, where p is a chosen hyperparameter. \n\nWhat are the concrete learning steps this architecture goes through?\n \n1. During each training episode, sample a subset of classes, and then divide those classes into training examples, and query examples \n2. Using a set of weights that are being learned by the network, map the input features of each training example into a vector space. \n3. Once all training examples are mapped into the space, calculate a \u201cmean vector\u201d for class A by averaging all of the embeddings of training examples that belong to class A. This is the \u201cprototype\u201d for class A, and once we have it, we can forget the values of the embedded examples that were averaged to create it. This is a nice update on the KNN approach, since the number of parameters we need to carry around to evaluate is only (num-dimensions) * (num-classes), rather than (num-dimensions) * (num-training-examples). \n4. Then, for each query example, map it into the embedding space, and use a distance metric in that space to create a softmax over possible classes. (You can just think of a softmax as a network\u2019s predicted probability, it\u2019s a set of floats that add up to 1). \n5. Then, you can calculate the (cross-entropy) error between the true output and that softmax prediction vector in the same way as you would for any classification network \n6. Add up the prediction loss for all the query examples, and then backpropogate through the network to update your weights \n\nThe overall effect of this process is to incentivize your network to learn, not necessarily a good prediction function, but a good metric space. The idea is that, if the metric space is good enough, and the classes are conceptually similar to each other (i.e. car vs chair, as opposed to car vs the-meaning-of-life), a space that does well at causing similar observed classes to be close to one another will do the same for classes not seen during training. \n\nI admit to not being sufficiently familiar with the datasets used for testing to have a sense for how well this method compares to more fully supervised classification schemes; if anyone does, definitely let me know! But the paper claims to get state of the art results compared to other approaches in this domain of few-shot learning (matching networks, and the aforementioned meta-learning). \n\nOne interesting note is that the authors found that squared Euclidean distance, when applied within the embedded space, worked meaningfully better than cosine distance (which is a more standard way of measuring distances between vectors, since it measures only angle, rather than magnitude). They suspect that this is because Euclidean distance, but not cosine distance belongs to a category of divergence/distance metrics (called Bregman Divergences) that have a special set of properties such that the point closest on aggregate to all points in a cluster is the average of all those points. If you want to dive way deep into the minutia on this point, I found this blog post quite good: http://mark.reid.name/blog/meet-the-bregman-divergences.html\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1703.05175"
    },
    "321": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.04087",
        "transcript": "The core goal of this paper is to perform in an unsupervised (read: without parallel texts) way what other machine translation researchers had previously only effectively performed in a supervised way: the creation of a word-to-word translational mapping between natural languages. To frame the problem concretely: the researchers start with word embeddings learned in each language independently, and their desired output is a set of nearest neighbors for a source word that contains the true target (i.e. translated) word as often a possible. \n\nAn interesting bit of background for this paper is that Mikilov, who was the initial progenitor of the word embedding approach, went on to posit, based on experiments he\u2019d conducted, that the embeddings produced by different languages share characteristics in vector space, such that one could expect a linear translation (i.e. taking a set of points and rotating, shifting, and/or scaling them) to be able to map from one language to another. This assumption is relied on heavily in this paper. A notional note: when I refer to \u201ca mapped source embedding\u201d or \u201cmapped source\u201d, that just means that a matrix transformation, captured in a weight matrix W, is being used to do some form of rotation, scaling, or shifting, to \u201cmap\u201d between the source embedding space and the shared space. \n\nThe three strategies this paper employs are: \n1. Using adversarial training to try to force the distributions of the embeddings in source and target languages to be similar to one another \n2. Taking examples where method (1) has high confidence, and borrowing a method from supervised word-to-word translation, called the Procrustes method, to further optimize the mapping into the shared vector space \n3. Calculating the nearest neighbors of a source word using an approach they develop called \u201cCross-Domain Similarity Local Scaling\u201d. At a high level, this conducts nearest neighbors, but \u201cnormalizes\u201d for density, so that, on an intuitive level, it\u2019s basically scaling distances up in dense regions of the space, and scaling them down in sparse regions \n\nFocusing on (1) first, the notion here goes back to that assumption I mentioned earlier: that internal relationships within embedding space are similar across languages, such that if you able to align the overall distributions of target embedding with a mapped source embedding, then you might - if you take Mikilov\u2019s assumption seriously -  reasonably expect this to push words in the mapped-source space close to their corresponding words in target space. And this does work, to some degree, but the researchers found that this approach on it\u2019s own didn\u2019t get them to where they wanted to be in terms of accuracy. \n\nTo further refine the mapping created by the adversarial training, the authors use something called the \u201cProcrustes Method\u201d. They go into it in more detail in the paper, but at a high level, it turns out that if you\u2019re trying to solve the problem of minimizing the sum of squared distances between a mapped-source embedding and a target embedding, assuming that that mapping is linear, and that you want the weight matrix to be orthogonal, that problem reduces to doing the singular value decomposition of the matrix of source embeddings multiplied by the (transposed) matrix of target embeddings, for a set of ground truth shared words. Now, you may reasonably note: this is an unsupervised method, we don\u2019t have access to ground truth embeddings across languages. And you would be correct. So, here, what the authors do is take words that are *mutual* nearest neighbors (according to the CSLS metric of nearest neighbors I\u2019ll describe in (3)  ) after conducting their adversarially-learned rotation, and take that mutual-nearest-neighbor-dom as a marker of high confidence in that word pair. They took these mutually-nearest-neighbor pairs, and used those as \u201cground truth\u201d to conduct this singular value decomposition, which was applied on top of the adversarially-learned rotation to get to their final mapping. \n\n(3) is described well in equation form in the paper itself, and is just a way of constructing a similarity metric between a mapped-source embedding and a target embedding that does some clever normalization. Specifically, it takes two times the (cosine) distance between Ws (mapped source) and t (target), and subtracts out the average (cosine) distance of Ws to its k nearest target words, as well as the (average) cosine distance of t to its k nearest source words. In this way, it normalizes the distance between Ws and t based on how dense each of their neighborhoods is. \n\nUsing all of these approaches together, the authors really do get quite impressive performance. For EN-ES, ES-EN, EN-FR, FR-EN, EN-DE, DE-EN, and EO (Esperanto)-EN, the performance of the adversarial method is within 0.5 accuracy score of the supervised method, with the adversarial method being higher in 5 of those 7 cases (note: I read this as \"functionally equivalent\"). Interestingly, though, for EN-RU, RU-EN, EN-CHN, and CHN-EN, the adversarial method was dramatically less effective, with accuracy deltas ranging from 5 to 10 points between the adversarial and the supervised method, with the supervised method prevailing in all cases. This suggests that the assumption of a simple linear mapping between the vector spaces of different languages may be a more valid one when the languages are more closely related, and thus closer in their structure. I'd be really interested in any experiments that try to actually confirm this by testing on a wider array of languages, or testing on subgroups of languages that are closer or farther (i.e. you would expect ES-FR to do even better than EN-FR, and you would expect ES-DE to do worse than EN-DE). \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1710.04087"
    },
    "322": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1805.04770",
        "transcript": "A finding first publicized by Geoff Hinton is the fact that, when you train a simple, lower capacity module on the probability outputs of another model, you can often get a model that has comparable performance, despite that lowered capacity. Another, even more interesting finding is that, if you take a trained model, and train a model with identical structure on its probability outputs, you can often get a model with better performance than the original teacher, with quicker convergence.\n\nThis paper addresses, and tries to specifically test, a few theories about why this effect might be observed. One idea is that the \"student\" model can learn more quickly because getting to see the full probability distribution over a well-trained models outputs gives it a more valuable signal, specifically because the trained model is able to better rank the classes that aren't the true class. For example, if you're training on Imagenet, on an image of a huskies, you're only told \"this is a husky (1), and not one of 100 other classes, which are all 0\". Whereas a trained model might say \"'this is most likely a husky, but the probability of wolf is way higher than that of teapot\". This inherently gives you more useful signal to train on, because you\u2019re given a full distribution of classes that an image is most like. This theory goes by the name of the \u201cDark Knowledge\u201d theory (a truly delightful name), because it pulls all of this knowledge that is hidden in a 0/1 label into the light. An alternative explanation for the strong performance of distillation techniques is that the student model is just benefitting from the implicit importance weighting of having a stronger gradient on examples where the teacher model is more confident. You could think of this as leading the student towards examples that are the most clear or unambiguous examples of a class, rather than more fuzzy and uncertain ones.\n\nAlong with a few other tests (which I won\u2019t address here, for sake of time and focus), the authors design a few experiments to test these possible mechanisms of action. The first test involved doing an explicit importance weighting of examples according to how confident the teacher model is, but including no information about the incorrect classes. The second was similar, but instead involved perturbing the probabilities of the classes that weren\u2019t the max probability. In this situation, the student model gets some information in terms of the overall magnitudes of the not-max class, but can\u2019t leverage it as usefully because it\u2019s been randomized.\n\nIn both situations, they found that there still was some value - in other words, that the student outperformed the teacher - but it outperformed by less than the case where the teacher could see the full probability distribution. This supports the case that both the inclusion of probabilities for the less probable classes, as well as the \u201cconfidence weighting\u201d effect of weighting the student to learn more from examples on which the \u201cteacher\u201d model was more confident.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1805.04770"
    },
    "323": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.05751",
        "transcript": "Last year, a machine translation paper came out, with an unfortunately un-memorable name (the Transformer network) and a dramatic proposal for sequence modeling that eschewed both Recurrent NNN and Convolutional NN structures, and, instead, used self-attention as its mechanism for \u201cremembering\u201d or aggregating information from across an input. Earlier this month, the same authors released an extension of that earlier paper, called Image Transformer, that applies the same attention-only approach to image generation, and also achieved state of the art performance there. \n\nThe recent paper offers a framing of attention that I find valuable and compelling, and that I\u2019ll try to explicate here. They describe attention as being a middle ground between the approaches of CNNs and RNNs, and one that, to use an over-abused cliche, gets the best of both worlds.  CNNs are explicitly local: each convolutional filter only gathers information from the cells that fall in specific locations along some predefined grid. And, because convolutional filters have a unique parameter for every relative location in the grid they\u2019re applied to, increasing the size of any given filter\u2019s receptive field would engender an exponential increase in parameters: to go from a 3x3 grid to a 4x4 one, you go from 9 parameters to 16.  Convolutional networks typically increase their receptive field through the mechanism of adding additional layers, but there is still this fundamental limitation that for a given number of layers, CNNs will be fairly constrained in their receptive field. \n\nOn the other side of the receptive field balance, we have RNNs. RNNs have an effectively unlimited receptive field, because they just apply one operation again and again: take in a new input, and decide to incorporate that information into the hidden state. This gives us the theoretical ability to access things from the distant past, because they\u2019re stored somewhere in the hidden state. However, each element is only seen once and needs to be stored in the hidden state in a way that sort of \u201caverages over\u201d all of the ways it\u2019s useful for various points in the decoding/translation process. (My mental image basically views RNN hidden state as packing for a long trip in a small suitcase: you have to be very clever about what you decide to pack, averaging over all the possible situations you might need to be prepared for. You can\u2019t go back and pull different things into your suitcase as a function of the situation you face; you had to have chosen to add them at the time you encountered them). All in all, RNNs are tricky both because they have difficulty storing information efficiently over long time frames, and also because they can be monstrously slow to train, since you have to run through the full sequence to built up hidden state, and can\u2019t chop it into localized bits the way you can with CNNs. \n\nSo, between CNN - with its locally-specific hidden state - and RNN - with its large receptive field but difficulty in information storage - the self-attention approach interposes itself. Attention works off of three main objects: a query, and a set of keys, each one is attached to a value. In general, all of these objects take the form of vectors. For a given query, you calculate its similarity with each key, and then normalize those into a distribution (a set of weights, all of which sum to 1) that is used as the weights in calculating a weighted average of the values. As a motivating example, think of a model that is \u201cunrolling\u201d or decoding a translated sentence. In order to translate a sentence properly, the model needs to \u201cremember\u201d not only the conceptual content of the sentence, but what it has already generated. So, at each given point in the unrolling, the model can \u201cquery\u201d the past and get a weighted distribution over what\u2019s relevant to it in its current context. In the original Transformer, and also in the new one, the models use \u201cmulti-headed attention\u201d, which I think is best compared to convolution filters: in the same way that you learn different convolution filters, each with different parameters, to pick up on different features, you learn different \u201cheads\u201d of the attention apparatus for the same purpose. \n\nTo go back to our CNN - Attention - RNN schematic from earlier: Attention makes it a lot easier to query a large receptive field, since you don\u2019t need an additional set of learned parameters for each location you expand to; you just use the same query weights and key weights you use for every other key and query. And, it allows you to contextually extract information from the past, depending on the needs you have right now. That said, it\u2019s still the case that it becomes infeasible to make the length of the past you calculate your attention distribution over excessively long, but that cost is in terms of computation, not additional parameters, and thus is a question of training time, rather than essential model complexity, the way additional parameters is. \n\nJumping all the way back up the stack, to the actual most recent image paper, this question of how best to limit the receptive field is one of the more salient questions, since it still is the case that conducting attention over every prior pixel would be a very large number of calculations. The Image Transformer paper solves this in a slightly hacky way: by basically subdividing the image into chunks, and having each chunk operate over the same fixed memory region (rather than scrolling the memory region with each pixel shift) to take better advantage of the speed of batched big matrix multiplies. \n\nOverall, this paper showed an advantage for the Image Transformer approach relevative to PixelCNN autoregressive generation models, and cited the ability for a larger receptive field during generation - without explosion in number of parameters - as the most salient reason why. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.05751"
    },
    "324": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.02827",
        "transcript": "\nIt\u2019s a commonly understood problem in Reinforcement Learning: that it is difficult to fully specify your exact reward function for an agent you\u2019re training, especially when that agent will need to operate in conditions potentially different than those it was trained in. The canonical example of this, used throughout the Inverse Rewards Design paper, is that of an agent trained on an environment of grass and dirt, that now encounters an environment with lava. In a typical problem setup, the agent would be indifferent to passing or not passing over the lava, because it was never disincentivized from doing so during training. \n\nThe fundamental approach this paper takes is to explicitly assume that there exists a program designer who gave the agent some proxy reward, and that that proxy reward is a good approximation of the true reward on training data, but might not be so on testing. This framing, of the reward as a noisy signal, allows the model to formalize its uncertainty about scenarios where the proxy reward might be a poor mapping to the real one. \n\nThe way the paper tests this is through a pretty simplified model. In the example, the agent is given a reward function expressed by a weighting of different squares it could navigate into: it has a strong positive weight on dirt, and a strong negative one on grass. The agent then enters an environment where there is lava, which, implicitly, it has a 0 penalty for in its rewards function. However, it\u2019s the case that, if you integrate over all possible weight values for \u201clava\u201d, none of them would have produced different behavior over the training trajectories. Thus, if you assume high uncertainty, and adopt a risk-averse policy where under cases of uncertainty you assume bad outcomes, this leads to avoiding values of the environment feature vector that you didn\u2019t have data weighting against during training. \n\nOverall, the intuition of this paper makes sense to me, but it\u2019s unclear to me if the formulation it uses generalizes outside of a very trivial setting, where your reward function is an explicit and given function of your feature vectors, rather than (as is typical) a scalar score not explicitly parametrized by the states of game prior to the very last one. It\u2019s certainly possible that it might, but, I don\u2019t feel like I quite have the confidence to say at this point. \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.02827v1"
    },
    "325": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1704.06960",
        "transcript": "This paper has an unusual and interesting goal, compared to those I more typically read: it wants to develop a \u201ctranslation\u201d between the messages produced by a model, and natural language used by a human. More specifically, the paper seeks to do this in the context of an two-player game, where one player needs to communicate information to the other. A few examples of this are: \n- Being shown a color, and needing to communicate to your partner so they can choose that color \n- Driving, in an environment where you can\u2019t see the other car, but you have to send a coordinating message so that you don\u2019t collide \n\n\nRecently, people have started training multi-agent that play games like these, where they send \u201cmessage\u201d vectors back and forth, in a way fully integrated with the rest of the backpropogation procedure. From just observing the agents\u2019 actions, it\u2019s not necessarily clear which communication strategy they\u2019re using. That\u2019s why this paper poses as an explicit problem: how can we map between the communication vectors produced by the agents and the words that would be produced by a human in a similar environment? \n\nInterestingly, the paper highlights two different ways you could think about structuring a translation objective. The first is \u201cpragmatic interpretation,\u201d under which you optimize what you communicate about something according to the operation that needs to be performed afterwards. To make that more clear, take a look at the attached picture. Imagine that player one is shown a shape, and needs to use a phrase from the bottom language (based on how many sides the shape has) to describe it to player two, who then needs to guess the size of the shape (big or small), and is rewarded for guessing correctly. Because \u201cmany\u201d corresponds to both a large and a small shape, the strategy that optimizes the action that player two takes, conditional on getting player one\u2019s message, is to lie and describe a hexagon as \u201cfew\u201d, since that will lead to correct inference about the size of the shape, which is what\u2019s most salient here. This example shows how, if you optimize a translation mapping by trying to optimize the reward that the post-translation agent can get, you might get a semantically incorrect translation. That might be good for the task at hand, but, because it leaves you with incorrect beliefs about the true underlying mapping, it will generalize poorly to different tasks. \n\nThe alternate approach, championed by the paper, is to train a translation such that the utterances in both languages are similar insofar as, conditional on hearing them, and having some value for their own current state, the listening player arrives at similar beliefs about the current state of the player sending the message. This is mathematically framed as by defining a metric q, representing the quality of the translation between two z vectors, as: \u201ctaking an expectation over all possible contextual states of (player 1, player 2), what is the difference between the distribution of beliefs about the state of player 1 (the sending player) induced in player 2 by hearing each of the z vectors. Because taking the full expectation over this joint distribution is intractable, the approach is instead done by sampling. \n\nThese equations require that you have reasonable models of human language, and understanding of human language, in the context of games. To do this, the authors used two types of datasets: \n1. Linguistic descriptions of objects of things, like the xkcd color dataset. Here, the player\u2019s hidden state is the color that they are trying to describe using some communication scheme. \n2. Mechanical turk game runs playing the aforementioned driver game, where they have to communicate to the other driver. Here, the player\u2019s \u201chidden state\u201d represents a combination of its current location and intentions. \nFrom these datasets, they can train simple emulator models that learn \u201cwhat terms is a human most likely to use for a given color\u201d [p(z|x)], and \u201cwhat colors will a human guess, conditional on those terms\u201d. \n\nThe paper closes by providing a proof as to how much reward-based value is lost by optimizing for the true semantic meaning, rather than the most pragmatically useful translation. They find that there is a bound on the gap, and that, in many empirical cases, the observed gap is quite small. \n\nOverall, this paper was limited in scope, but provided an interesting conceptual framework for thinking about how you might structure a translation, and the different implications that structure might have on your results. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1704.06960"
    },
    "326": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1805.11604",
        "transcript": "At NIPS 2017, Ali Rahimi was invited on stage to give a keynote after a paper he was on received the \u201cTest of Time\u201d award. While there, in front of several thousand researchers, he gave an impassioned argument for more rigor: more small problems to validate our assumptions, more visibility into why our optimization algorithms work the way they do. The now-famous catchphrase of the talk was \u201calchemy\u201d; he argued that the machine learning community has been effective at finding things that work, but less effective at understanding why the techniques we use work. A central example he used in his talk is that of Batch Normalization: a now nearly-universal step in optimizing deep nets, but one where our accepted explanation of \u201creducing internal covariate shift\u201d is less rigorous than one might hope. \n\nWith apologies for the long preamble, this is the context in which today\u2019s paper is such a welcome push in the direction of what Rahimi was advocating for - small, focused experimentation that tries to build up knowledge from principles, and, specifically, asks the question: \u201cDoes Batch Norm really work via reducing covariate shift\u201d. \n\nTo answer the question of whether internal covariate shift is a likely mechanism of the - empirically very solid - improved performance of Batch Norm, the authors do a few simple experience. First, and most straightforwardly, they train a basic convolutional net with and without BatchNorm, pick a layer, and visualize the activation distribution of that layer over time, both in the Batch Norm and non-Batch Norm case. While they saw the expected performance boost, the Batch Norm case didn\u2019t seem to be meaningfully more stable over time, relative to the normal case. Second, the authors tested what would happen if they added non-zero-mean random noise *after* Batch Norm in the network. The upshot of this was that they were explicitly engineering internal covariate shift, and, if control thereof was the primary useful purpose of Batch Norm, you would expect that to neutralize BN\u2019s good performance. In this experiment, while the authors did indeed see noisier, less stable activation distributions in the noise + BN case (in particular: look at layer 13 activations in the attached image), but noisy BN performed nearly as well as non-noisy, and meaningfully better than the standard model without noise, but also without BN. \n\nAs a final test, they approached the idea of \u201cinternal covariate shift\u201d from a different definitional standpoint. Maybe a better way of thinking about it is in terms of stability of your gradients, in the face of updates made by lower layers of the network. That is to say: each parameter of the network pushes itself in the direction of lower loss all else held equal, but in practice, you change lower-level parameters simultaneously, which could cause the directional change the higher-layer parameter thought it needed to be off. So, the authors calculated the \u201cgradient delta\u201d between the gradient the model trains on, and what the gradient would be if you estimated it *after* all of the lower layers of the model had updated, such that the distribution of inputs to that layer has changed. Although the expectation would be that this gradient delta is smaller for batch norm, in fact, the authors found that, if anything, the opposite was true. \n\nSo, in the face of none of these ideas panning out, the authors then introduce the best idea they\u2019ve found for what motivates BN\u2019s improved performance: a smoothing out of the loss function that SGD is optimizing. A smoother curve means, generally speaking, that the magnitudes of your gradients will be smaller, and also that the value of the gradient will change more slowly (i.e. low second derivative). As support for this idea, they show really different results for BN vs standard models in terms of, for example, how predictive a gradient at one point is of a gradient taken after you take a step in the direction of the first gradient. BN has meaningfully more predictive gradients, tied to lower variance in the values of the loss function in the direction of the gradient. The logic for why the mechanism of BN would cause this outcome is a bit tied up in math that\u2019s hard to explain without LaTeX visuals, but basically comes from the idea that Batch Norm decreases the magnitude of the gradient of each layer output with respect to individual weight parameters, by averaging out those magnitudes over the batch. \n\nAs Rahimi said in his initial talk, a lot of modern modeling is \u201capplying brittle optimization techniques to loss surfaces we don\u2019t understand.\u201d And, by and large, that is in fact true: it\u2019s devilishly difficult to get a good handle on what loss surfaces are doing when they\u2019re doing it in several-million-dimensional space. But, it being hard doesn\u2019t mean we should just give up on searching for principles we can build our understanding on, and I think this paper is a really fantastic example of how that can be done well. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1805.11604"
    },
    "327": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1803.08494",
        "transcript": "If you were to survey researchers, and ask them to name the 5 most broadly influential ideas in Machine Learning from the last 5 years, I\u2019d bet good money that Batch Normalization would be somewhere on everyone\u2019s lists. Before Batch Norm, training meaningfully deep neural networks was an unstable process, and one that often took a long time to converge to success. When we added Batch Norm to models, it allowed us to increase our learning rates substantially (leading to quicker training) without the risk of activations either collapsing or blowing up in values. It had this effect because it addressed one of the key difficulties of deep networks: internal covariate shift. \n\nTo understand this, imagine the smaller problem, of a one-layer model that\u2019s trying to classify based on a set of input features. Now, imagine that, over the course of training, the input distribution of features moved around, so that, perhaps, a value that was at the 70th percentile of the data distribution initially is now at the 30th. We have an obvious intuition that this would make the model quite hard to train, because it would learn some mapping between feature values and class at the beginning of training, but that would become invalid by the end. This is, fundamentally, the problem faced by higher layers of deep networks, since, if the distribution of activations in a lower layer changed even by a small amount, that can cause a \u201cbutterfly effect\u201d style outcome, where the activation distributions of higher layers change more dramatically. Batch Normalization - which takes each feature \u201cchannel\u201d a network learns, and normalizes [normalize = subtract mean, divide by variance] it by the mean and variance of that feature over spatial locations and over all the observations in a given batch - helps solve this problem because it ensures that, throughout the course of training, the distribution of inputs that a given layer sees stays roughly constant, no matter what the lower layers get up to. \n\nOn the whole, Batch Norm has been wildly successful at stabilizing training, and is now canonized - along with the likes of ReLU and Dropout - as one of the default sensible training procedures for any given network. However, it does have its difficulties and downsides. One salient one of these comes about when you train using very small batch sizes - in the range of 2-16 examples per batch. Under these circumstance, the mean and variance calculated off of that batch are noisy and high variance (for the general reason that statistics calculated off of small sample sizes are noisy and high variance), which takes away from the stability that Batch Norm is trying to provide. \n\nOne proposed alternative to Batch Norm, that didn\u2019t run into this problem of small sample sizes, is Layer Normalization. This operates under the assumption that the activations of all feature \u201cchannels\u201d within a given layer hopefully have roughly similar distributions, and, so, you an normalize all of them by taking the aggregate mean over all channels, *for a given observation*, and use that as the mean and variance you normalize by. Because there are typically many channels in a given layer, this means that you have many \u201csamples\u201d that go into the mean and variance. However, this assumption - that the distributions for each feature channel are roughly the same - can be an incorrect one. \n\nA useful model I have for thinking about the distinction between these two approaches is the idea that both are calculating approximations of an underlying abstract notion: the in-the-limit mean and variance of a single feature channel, at a given point in time. Batch Normalization is an approximation of that insofar as it only has a small sample of points to work with, and so its estimate will tend to be high variance. Layer Normalization is an approximation insofar as it makes the assumption that feature distributions are aligned across channels: if this turns out not to be the case, individual channels will have normalizations that are biased, due to being pulled towards the mean and variance calculated over an aggregate of channels that are different than them. \n\nGroup Norm tries to find a balance point between these two approaches, one that uses multiple channels, and normalizes within a given instance (to avoid the problems of small batch size), but, instead of calculating the mean and variance over all channels, calculates them over a group of channels that represents a subset. The inspiration for this idea comes from the fact that, in old school computer vision, it was typical to have parts of your feature vector that - for example - represented a histogram of some value (say: localized contrast) over the image. Since these multiple values all corresponded to a larger shared \u201cgroup\u201d feature. If a group of features all represent a similar idea, then their distributions will be more likely to be aligned, and therefore you have less of the bias issue. \n\nOne confusing element of this paper for me was that the motivation part of the paper strongly implied that the reason group norm is sensible is that you are able to combine statistically dependent channels into a group together. However, as far as I an tell, there\u2019s no actually clustering or similarity analysis of channels that is done to place certain channels into certain groups; it\u2019s just done so semi-randomly based on the index location within the feature channel vector. So, under this implementation, it seems like the benefits of group norm are less because of any explicit seeking out of dependant channels, and more that just having fewer channels in each group means that each individual channel makes up more of the weight in its group, which does something to reduce the bias effect anyway.\n\nThe upshot of the Group Norm paper, results-wise, is that Group Norm performs better than both Batch Norm and Layer Norm at very low batch sizes. This is useful if you\u2019re training on very dense data (e.g. high res video), where it might be difficult to store more than a few observations in memory at a time. However, once you get to batch sizes of ~24, Batch Norm starts to do better, presumably since that\u2019s a large enough sample size to reduce variance, and you get to the point where the variance of BN is preferable to the bias of GN.  \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1803.08494"
    },
    "328": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1804.04849",
        "transcript": "I have a lot of fondness for this paper as a result of its impulse towards clear explanations, simplicity, and pushing back against complexity for complexity\u2019s sake. The goal of the paper is pretty straightforward. Long Short Term Memory networks (LSTM) work by having a memory vector, and pulling information into and out of that vector through a gating system. These gates take as input the context of the network at a given timestep (the prior hidden state, and the current input), apply weight matrices and a sigmoid activation, and produce \u201cmask\u201d vectors with values between 0 and 1. A typical LSTM learns three separate gates: a \u201cforget\u201d gate that controls how much of the old memory vector is remembered, an \u201cinput\u201d gate that controls how much new contextual information is added to the memory, an \u201coutput\u201d gate that controls how much of the output (a sum of the gated memory information, and the gated input information) is passed outward into a hidden state context that\u2019s visible to the rest of the network. Note here that \u201chidden\u201d is an unfortunate word here, since this is actually the state that is visible to the rest of the network, whereas the \u201cmemory\u201d vector is only visible to the next-step memory updating calculations. Also note that \u201cforget gate\u201d is an awkward name insofar as the higher the value of the forget gate, the more that the model *remembers* of its past memory. This is confusing, but we appear to be stuck with this terminology \n\nThe Gated Recurrent Unit, or GRU, did away with the output gate. In this system, the difference between \u201chidden\u201d and \u201cmemory\u201d vectors is removed, and so the network no longer has separate information channels for communicating with subsequent layers, and simple memory passed to future timesteps. On a wide range of problems, the GRU has performed comparably to the LSTM. This makes the authors ask: if a two-gate model can do as well, can a single gate model? In particular: how well does a LSTM-style model perform, if it only has a forget gate. \n\nThe answer, to not bury the probably-obvious lede, is: quite well. Models that only have a forget gate perform comparably to or better than traditional LSTM models for the tasks at which they were tried. On a mechanical level, not having an input gate means that, instead of having individual scaling for \u201chow much old memory do you remember\u201d and \u201chow much new context do you take in\u201d, so that those values could be, for example, 0.2 and 0.15, these numbers are defined as a convex combination of a single value, which is the forget gate. That\u2019s a fancy way of saying: we calculate some x between 0 and 1, and that\u2019s the weight on the forget gate, and then (1-x) is the weight on the input gate. This model, for reasons that are entirely unjustified, and obviously the result of some In Joke, is called JANET, because with a single gate, it\u2019s Just Another NETwork. Image is attached to prove I\u2019m Not Making This Shit Up. \n\nThe authors go down a few pathways of explaining why this forget-only model performs well, of which the most compelling is that it gives the model an easier and more efficient way to learn a skip connection, where information is passed down more or less intact to a future point in the model. It\u2019s more straightforward to learn because the \u201cskip-ness\u201d of the connection, or, how strongly the information wants to propogate into the future, is just controlled by one set of parameters, and not a complex interaction of input, forget, and output. An interesting side investigation they perform is how the initialization of the bias term in the forget gate (which is calculated by applying weights to the input and former hidden state, and then adding a constant bias term) effects a model\u2019s ability to learn long term dependencies. In particular, they discuss the situation where the model gets some signal, and then a long string of 0 values. If the bias term of the model is quite low, then all of those 0 values being used to calculate the forget gate will mean that only the bias is left, and the more times the bias is multiplied by itself, the smaller and closer to 0 it gets. The paper suggests initializing the bias of the forget gate according to the longest dependencies you expect the model to have, with the idea that you should more strongly bias your model towards remembering old information, regardless of what new information comes in, if you expect long term dependencies to be strongly relevant. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1804.04849"
    },
    "329": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.04821",
        "transcript": "The general goal of meta-learning systems is to learn useful shared structure across a broad distribution of tasks, in such a way that learning on a new task can be faster. Some of the historical ways this has been done have been through initializations (i.e. initializing the network at a point such that it is easy to further optimize on each individual task, drawn from some distribution of tasks), and recurrent network structures (where you treat the multiple timesteps of a recurrent network as the training iterations on a single task, and train the recurrent weights of the network based on generalization performance on a wide range of tasks). This paper proposes a different approach: a learned proxy loss function. The idea here is that, often, early in the learning process, handcoded rewards aren\u2019t the best or most valuable signal to use to guide a network, both because they may be high variance, and because they might not natively incentivize things like exploration rather than just exploitation. A better situation would be if we had some more far-sighted loss function we could use, that had proved to be a good proxy over a variety of different rewards. \n\nThis is exactly what this method proposes to give us. Training consists of an inner loop, and an outer loop. Each instantiation of the inner loop corresponds to a single RL task, drawn from a distribution over tasks (for example, all tasks involving the robot walking to a position, with a single instantiated task being the task of walking to one specific position). Within the inner loop, we apply a typical policy gradient loop of optimizing the parameters of our policy, except, instead of expected rewards, we optimize our policy parameters according to a loss function we specifically parametrize. Within the outer loop, we take as signal the final reward on the trained policy on this task, and use that to update our parametrized loss. This parametrized loss is itself a neural network, that takes in the agent\u2019s most recent set of states, actions, and rewards at a rolling window of recent timesteps, and performs temporal convolutions on those, to get a final loss value out the other side. In short, this auxiliary network takes in information about the agent\u2019s recent behavior, and outputs an assessment of how well the agent is doing according to this longer-view loss criteria. Because it\u2019s not possible to directly formulate the test performance of a policy in terms of the loss function that was used to train the policy (which would be necessary for backprop), the weights of this loss-calculating network are instead learned via evolutionary strategies. At a zoomed-out level of complexity, this means: making small random perturbations to the current parameters of the network, and moving in the direction of the random change that works the best. \n\nSo, ultimately, you end up with a loss network that takes in recent environmental states and the behavior of the agent, and returns an estimate of the proxy loss value, that has hopefully been trained such that it captures environmental factors that indicate progress on the task, over a wide variety of similar tasks. Then, during testing, the RL agent can use that loss function to adapt its behavior. An interesting note here is that for tasks where the parameters of the task being learned are inferable from the environment - for example, where the goal is \u201cmove towards the green dot\u201d, you don\u2019t actually need to give the agent the rewards from a new task; ideally, it will have learned how to infer the task from the environment. One of the examples they use to prove their method has done something useful is train their model entirely on tasks where an ant-agent\u2019s goal is to move towards various different targets on the right, and then shift it to a scenario where its target is towards the left.  In the EPG case, the ant was able to quickly learn to move left, because it\u2019s loss function was able to adapt to the new environment where the target had moved. By contrast, RL^2 (a trained learning algorithm implemented as a recurrent network) kept on moving right as its initial strategy, and seemed unable to learn the specifics of a task outside its original task distribution of \u201calways move right\u201d. \n\nI think this paper could benefit from being a little bit more concrete about what it\u2019s expected use cases are (like: what kinds of environments lend themselves to having proxy loss functions inferred from environmental data? Which don\u2019t?), but overall, I find the kernel of idea this model introduces interesting, and will be interested to see if other researchers run with it.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.04821"
    },
    "330": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1804.02464",
        "transcript": "Meta learning is an area sparking a lot of research curiosity these days. It\u2019s framed in different ways: models that can adapt, models that learn to learn, models that can learn a new task quickly. This paper uses a somewhat different lens: that of neural plasticity, and argues that applying the concept to modern neural networks will give us an effective, and biologically inspired way of building adaptable models. The basic premise of plasticity from a neurobiology perspective (at least how it was framed in the paper: I\u2019m not a neuroscientist myself, and may be misunderstanding) is that plasticity performs a kind of gating function on the strength of a neural link being upregulated by experience. The most plastic a connection is, the more quickly it can get modified by new data; the less plastic, the more fixed it is. \n\nIn concrete terms, this is implemented by subdividing the weight on each connection in the network into two parts: the \u201cfixed\u201d component, and the \u201cplastic\u201d component. (see picture). The fixed component acts like a typical weight: it gets modified during training, but stays fixed once training is done. The plastic component is composed of an alpha weight, multiplied by a term H. H is basically a decaying running average of the past input*output activations of this weight. Activations that are high in magnitude, and the same sign, for both the input and the output will lead to H being pushed higher. Note that that this H can continue to be updated even after the model is done training, because it builds up information whenever you pass a new input X through the network. The plastic component\u2019s learned weight, alpha, controls how strong the influence of this is on the model. If alpha is near zero, then the connection behaves basically identically to a \u201ctypical\u201d neural network, with weights that don\u2019t change as a function of activation values. If alpha is positive, that means that strong co-activation within H will tend to make the connection weight higher. If alpha is negative, the opposite is true, and strong co-activation will make the connection weight more negative. (As an aside, I\u2019d be really interested to see the distribution over alpha values in a trained model, relative to the weight values, and look at how often they go in the same direction as the weights, and increase magnitude, and how often they have the opposite direction and attenuate the weight towards zero).\n\nThese models are trained by running them for fixed size \u201cepisodes\u201d during which the H value gets iteratively changed, and then the alpha parameters of H get updated in the way that would have reduced error over the episode.  One area in which they seem to show strong performance is that of memorization (where the network is shown an image once, and needs to reconstruct it later). The theory for why this is true is that the weights are able to store short-term information about which pixels are in the images it sees by temporarily boosting themselves higher for inputs and activations they\u2019ve recently seen. \n\nThere are definitely some intuitional gaps for me in this paper. The core one is: this framework just makes weights able to update themselves as a function of the values of their activations, not as a function of an actual loss function. That is to say: it seems like a potentially better analogy to neural plasticity is just a network that periodically gets more training data, and has some amount of connection plasticity to update as a result of that. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1804.02464"
    },
    "331": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.03641",
        "transcript": "DeepMind\u2019s recently released paper (one of a boatload coming out in the wake of ICLR, which just finished in Vancouver) addresses the problem of building an algorithm that can perform well on tasks that don\u2019t just stay fixed in their definition, but instead evolve and change, without giving the agent a chance to re-train in the middle. An example of this, is one used at various points in the paper: of an agent trying to run East, that finds two of its legs (a different two each time) slowly less functional. The theoretical framework they use to approach this problem is that of meta learning. Meta Learning is typically formulated as: how can I learn to do well on a new task, given only a small number of examples of that task? That\u2019s why it\u2019s called \u201cmeta\u201d: it\u2019s an extra, higher-level optimization loop applied around the process of learning. Typical learning learns parameters of some task, meta learning learns longer-scale parameters that make the short-scale, typical learning work better. Here, the task that evolves and changes over time (i.e. a nonstationary task) is seen as a close variant of the the multi-task problem. And, so, the hope is that a model that can quickly adapt to arbitrary new tasks can also be used to learn the ability to adapt to a gradually changing task environment. \n\nThe meta learning algorithm that got most directly adapted for this paper is MAML: Model Agnostic Meta Learning. This algorithm works by, for a large number of tasks, initializing the model at some parameter set theta, evaluating the loss for a few examples on that task, and moving the gradients from the initialization theta, to a task-specific parameter set phi. Then, it  calculating the \u201ctest set\u201d performance of the one-step phi parameters, on the task. But then - the crucial thing here - the meta learning model updates its initialization parameters, theta. So, the meta learning model is learning a set of parameters that provides a good jumping off point for any given task within the distribution of tasks the model is trained on. In order to do this well, the theta parameters need to both 1) learn any general information, shared across all tasks, and 2) position the parameters such that an initial update step moves the model in the profitable direction. \n\nThey adapted this idea, of training a model that could quickly update to multiple tasks, to the environment of a slowly/continuously changing environment, where certain parameters of the task the agent is facing. In this formulation, our set of tasks is no longer random draws from the distribution of possible tasks, but a smooth, Markov-walk gradient over tasks. The main change that the authors made to the original MAML algorithm was to say that each general task would start at theta, but then, as that task gradually evolved, it would perform multiple updates: theta to phi1, phi1 to phi2, and so on. The original theta parameters would then be updated according to a similar principle as the MAML parameters: so as to make the loss, summed over the full non-stationary task (notionally composed of many little sub-tasks) is as low as possible. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1710.03641"
    },
    "332": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.00179",
        "transcript": "The problem setting of the paper is the desire to perform translation in a monolingual setting, where datasets exist of each language independently, but little or no paired sentence data (paired here meaning that you know you have the same sentence or text in both languages). The paper outlines the prior methods in this area as being, first, training a single-language language model (i.e. train a model to take in a sentence, and return how coherent of a sentence it is in a given language) and using that to supplement a machine translation system. The authors honestly don\u2019t go into this much, so I can\u2019t tell exactly what they mean by it. The second baseline they talk about is bootstrapping themselves additional training data, by training a model using a small amount of training data, then using that mediocre model to translate additional sentences, which they use as additional training data to train the mediocre model to a higher performance. It doesn\u2019t seem like this should work, but I\u2019ve seen this or similar approaches used in a few cases, and it typically does add benefit. But, the authors claim, they can do better. \n\nThe core intuition of this paper is pretty simple, and will be familiar to anyone who read my summary of CycleGAN, lo these many weeks ago.  Their approach rests on the idea that, even if you can\u2019t push translation models to be objectively correct in a paired sense, you can push translation models to be symmetric with one another, insofar as translating from language A to B (let\u2019s say English to French), and then back from French to English, gets you something in English that looks like your original input. This forces the model to maintain an informative mapping, so that enough information about the English sentence is stored to allow it to be reconstructed.However, unconstrained, the model could just develop a 1:1 word mapping that gives you information about the English input, but doesn\u2019t actually map to the translation in French. If you can additionally confirm that the translation into French looks like a coherent French sentence (which, recall, we can do with a language model trained on French independently), we can get closer to to generating a mapping that is hopefully more coherent. \n\nOne interesting aspect of this paper is the fact that the model they describe is trained with reinforcement learning. Typically, reinforcement learning is used for scenarios where you don\u2019t have direct visibility into how the actions you take impact your loss function. Compare this to a supervised network (where you can take the derivative of your loss with respect to the last layer, and backpropogate that back through to your inputs), or even a GAN, where you can take the derivative of the discriminator-created loss back through the input the discriminator, and through into the GAN that created it. This model treats the translation models that are learned as policies; that is, probability distributions over sets of words. It samples multiple A -> B translations using something called beam search, which, as it samples the sequence of words, samples several at each timestep, and then keeps that chain alive by continuing to sample along it. This helps the sequential translation not fall into a pit where it samples one highly probable word, but then, as you add more words, it doesn\u2019t lead towards a good sentence. This ultimately results in taking multiple (k=12, in this case) samples from each translation distribution, and so the model uses as its objective the expected rewards over these samples, where the rewards is constructed as a combination of in-language coherence loss (scored by using the log likelihood of a trained single-language model) and reconstruction loss (scored by close the A -> B -> A\u2019 is to the original A). \n\nMy confusion about the use of reinforcement loss here mostly comes from the question of whether it just wasn\u2019t possible to build an end to end model, where, like a GAN, you backpropogated through a constructed input, back to the model that constructed it (in this case, through both translator models). Is the issue just that a sequence of words is fundamentally discrete, in a way that images aren\u2019t, and in a way that impedes backprop? That seems possible, but also, I think it\u2019s the case that a typical encoder-decoder model, that outputs softmaxes over words, is able to be backpropogated through. Overall, it\u2019s hard for me to tell if I\u2019m missing something basic about why reinforcement learning is the obvious choice here, or if other more GAN-like approaches were an option, but that meme hadn\u2019t spread into the literature yet, and RL was the more historically canonical choice.  \n\nOne other minor disappointing note: it looks like their results are based on a scenario that does use a small number of bilingual training pairs, as a way to pretrain the translation models to a reasonable, non-random starting point. It\u2019s not clear whether this method would have worked with an actual cold start, i.e. a translation model that has no idea what it\u2019s doing, and is using only this as signal. That said, they used a much smaller number of bilingual pairs than a true supervised method, and so even with a need for a warm start, a method like this could still give you leverage over language pairs where there exists some paired data, but not enough to build a full, sophisticated model on top of. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.00179"
    },
    "333": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.05365",
        "transcript": "This paper builds on the paper \"Learned in Translation: Contextualized Word Vectors\" , which learned contextualized word representations by using the sequence of encodings generated by a Bidirectional LSTM as the representation of the sequence of input words. This paper says \u201cif we're learning a deep LSTM, ie one with more than one layer, why should we use only the last layer that it produces as the representation of the word?\u201d. This paper instead suggests that it could be valuable for transfer learning if each task can learn a weighting of layer encodings that is most valuable for that task. In a prime example of \u201cyour model is a special case of my model,\u201d they note that this framework can easily learn the approach of only using the final encoding layer by just only giving that layer a non-zero weight. As intuition for why this might be a valuable thing to do: different layers tend to capture different levels of meaning, with lower layers more likely to capture part of speech information, and higher layers more likely to capture more rich semantic context. \n\nhttps://i.imgur.com/s8Qn6YY.png\n\nOne difficulty in comparing this paper directly to the \u201ctake the top layer encoding from a LSTM\u201d paper is that they were trained on different problems: the top layer paper learned using a machine translation objective, where, by contrast, this one learns by using a much simpler language model. Here, a simple language model means a RNN that is trained to predict the next word, given the hidden state built up over all prior words. Because we want to pull in word context from both directions, this is\u2019t just a LSTM but a bidirectional LSTM, which - surprise surprise - tries to pick the word *before* a word in a sentence by using all of the words that come after it. This has the advantage of not requiring parallel data, the way machine translation does, but also makes it difficult to make direct comparisons to prior work that isolate the effect of multi-layer combination, as separate from the switch between machine translation and direct language modeling. \n\nAlthough this is likely also a benefit you see with just top-layer contextual vectors, it is interesting to examine the attached table, and look at how effectively the model is able to learn different representations of the word \u201cplay\u201d depending on the context in which it appears; in each case, the nearest neighbor of a context of \u201cplay\u201d is a sentence in which the word is used in the same context. \n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.05365"
    },
    "334": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.04606",
        "transcript": "This paper is a clever but conceptually simple idea to improve the vectors learned for individual words. In this proposed approach, instead of learning a distinct vector per word in the word, the model instead views a word as being composed of overlapping n-grams, which are combined to make the full word. \n\nRecall: in the canonical skipgram approach to learning word embeddings, each word is represented by a single vector. The word might be tokenized first (for example, de-pluralized), but, fundamentally, there isn\u2019t any way for the network to share information about the meanings of \u201cliberty\u201d and \u201cliberation\u201d; even though a human could see that they share root structure, for a skipgram model, they are two totally distinct concepts that need to be separately learned. The premise of this paper is that this approach leaves valuable information on the table, and that by learning vectors for subcomponents of words, and combining them to represent the whole word, we can more easily identify and capture shared patterns. \n\nOn a technical level, this is done by: \n- For each n value in the range selected (typically 3-6 inclusive), representing each input word as a set of overlapping windows of that n, with special characters for Start and End. For example, if n=3, and the word is \u201cwhere\u201d, it could be represented as [\u201c<wh\u201d, \u201cwhe\u201d, \u201cher\u201d, \u201cere>\u201d] \n- In addition to the set of ngrams, additionally representing each word through its full-word representation of \u201c<where>\u201d, to \u201ccatch\u201d any leftover meaning not captured in the smaller ngrams. \n- When you\u2019re calculating loss, representing each word as simply being the sum of all of its component ngrams \n\n\nhttps://i.imgur.com/NP5qFEV.png\nThis has some interesting consequences. First off, the perplexity of the model, which you can think of as a measure of unsupervised goodness of fit, is equivalent or improved by this approach relative to baselines on all but one model. Intriguingly, and predictably once you think about it, the advantage of the subword approach is much stronger for languages like German, Russian, and Arabic, which have strong re-use and aggregation of root words, and also strong patterns of morphological mutation of words. Additionally, the authors found that the subword model got to its minimum loss value using much less data than the canonical approach. This makes decent sense if you think about the fact that subcomponent re-use means there are fewer meaningful word subcomponents than their are unique words, and seeing a subcomponent used across many words means that you need fewer words to learn the patterns it corresponds to. \n\nhttps://i.imgur.com/EmN167L.png\n\nA lot of the benefit of this approach seems to be through better representation of syntax; when tested on an analogy task, embeddings trained with subword information did meaningfully better on syntactic analogies (\u201cswim is to swum as ran is to <>\u201d) but equivalent or worse on semantic analogies (\u201cmother is to girl as father is to <>\u201d). One theory about this is that focusing on the subword elements does a better job of more quickly getting the representation of the word to be close to it\u2019s exact meaning, but has a harder time learning precise semantics, relative to a full-word model. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.04606"
    },
    "335": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1708.00107",
        "transcript": "This paper\u2019s approach goes a step further away from the traditional word embedding approach - of training embeddings as the lookup-table first layer of an unsupervised monolingual network - and proposes a more holistic form of transfer learning that involves not just transferring over learned knowledge contained in a set of vectors, but a fully trained model. \n\nTransfer learning is the general idea of using part or all of a network trained on one task to perform a different task. The most common kind of transfer learning is in the image domain, where models are first trained on the enormous ImageNet dataset, and then several of the lower layers of the network (where more local, small-pixel-range patterns are detected) are transferred, with their weights fixed in place to a new network. The modeler then attaches a few more layers to the top, connects it to a new target, and then is able to much more quickly learn their new target, because the pre-training has gotten them into a useful region of parameter-space. \n\nhttps://i.imgur.com/wjloHdi.png\nWithin NLP, the most common form of transfer learning is initializing the lookup table of vectors that\u2019s used to convert discrete words in to vectors (also known as an embedding) with embeddings pre-trained on huge unsupervised datasets, like GloVe, trained on all of English Wikipedia. Again, this makes your overall task easier to train, because you\u2019ve already converted words from their un-useful binary representation (where the word cat is just as far from Peru as it is from kitten) to a meaningful real-valued representation.\n\nThe approach suggested in this paper goes beyond simply learning the vector input representation of words. Instead, the authors suggest using as word vectors the sequence of encodings produced by an encoder-decoder bi-directional recurrent model. An encoder-decoder model means that you have one part of the network that maps from input sentence to an \u201cencoded\u201d representation of the sentence, and then another part that maps that encoded representation into the proper tokens in the target language. Historically, this encoding had been a single vector for the whole sentence, which tried to conceptually capture all of the words into one vector. More recently, a different approach has grown popular, where the RNN produces a number of encodings equal to the number of input words. Then, when the decoder is producing words in the target sentence, it uses something called \u201cattention\u201d to select a weighted combination of these encodings at each point in time. Under this scheme, the decoder might pull out information about verbs when its own hidden state suggests it needs a verb, and might pull out information about pronoun referents when its own hidden state asks for that. The upshot of all of this is that you end up with a sequence of encoded vectors equal in length to your number of inputs. Because the RNN is bidirectional, which means the encoding is a concatenation of the forward RNN and backward RNN, that means that each of these encodings captures both information about its corresponding word, and contextual information about the rest of the sentence. \n\nThe proposal of the authors is to train the encoder-decoder outlined above, and, once it is trained, lop off the decoder, and use the encoded sequence of words as your representation of the input sequence of words. An important note in all this is that recurrent encoder-decoder model was itself trained using a lookup table initialized with learned GloVe vectors, so in a sense they\u2019re not substituting for the unsupervised embeddings so much as learning marginal information on top of them.\n\nThe authors went on to test this approach on a few problems - question answering, logical entailment, and sentiment classification. They compared their use of the RNN encoded word vectors (which they call Context Vectors, or CoVE) with models initialized just using the fixed GloVE word vectors. One important note here is that, because each word vector is learned fully in context, the same word will have a different vector in each sentence it appears in. That\u2019s why you can\u2019t transfer one single vector per word, but instead have to transfer the recurrent model that can produce the vectors. All in all, the authors found that concatenating CoVe vectors to GloVe vectors, and using the concatenated version as input, produced sizable gains on the problems where it was tried. \n\nThat said, it\u2019s a pretty heavy lift to integrate someone else\u2019s learned weights into your own model, just in terms of getting all the code to play together nicely. I\u2019m not sure if this is a compelling enough result, a la ImageNet pretraining, for practitioners to want to go to the trouble of tacking a non-training RNN onto the bottom of all their models. If I ever get a chance, I\u2019d be interested to play with the vectors you get out of this model, and look at how much variance you see in the vectors learned for different words across different sentences. Do you see clusters that correspond to sense disambiguation, (a la state of mind, vs a rogue state)? And, how does this contextual approach to the paper I reviewed yesterday, that also learns embeddings on a machine translation task, but does so in terms of training a lookup table, rather than using trained encodings? All in all, I enjoyed this paper: it was a simple idea, and I\u2019m not sure whether it was a compelling one, but it did leave me with some interesting questions. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1708.00107"
    },
    "336": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1412.6448",
        "transcript": "If you\u2019ve been paying any attention to the world of machine learning in the last five years, you\u2019ve likely seen everyone\u2019s favorite example for how Word2Vec word embeddings work: king - man + woman = queen. Given the ubiquity of Word2Vec, and similar unsupervised embeddings, it can be easy to start thinking of them as the canonical definition of what a word embedding *is*. But that\u2019s a little oversimplified. In the context of machine learning, an embedding layer simply means any layer structured in the form of a lookup table, where there is some pre-determined number of discrete objects (for example: a vocabulary of words), each of which corresponds to a d-dimensional vector in the lookup table (where d is the number of dimensions you as the model designer arbitrarily chose). These embeddings are initialized in some way, and trained jointly with the rest of the network, using some kind of objective function. \n\nUnsupervised, monolingual word embeddings are typically learned by giving a model as input a sample of words that come before and after a given target word in a sentence, and then asking it to predict the target word in the center. Conceptually, if there are words that appear in very similar contexts, they will tend to have similar word vectors. This happens because scores are calculated using the dot product of the target vector with each of the context words, and if two words are to both score highly in that context, the dot product with their common-context vectors must be high for both, which pushes them towards similar values. For the last 3-4 years, unsupervised word vectors like these - which were made widely available for download - became a canonical starting point for NLP problems; this starting representation of words made it easier to learn from smaller datasets, since knowledge about the relationships between words was being transferred from the larger original word embedding training set, through the embeddings themselves. \n\nThis paper seeks to challenge the unitary dominance of monolingual embeddings, by examining the embeddings learned when the objective is, instead, machine translation, where given a sentence in one language, you must produce it in another.  Remember: an embedding is just a lookup table of vectors, and you can use it as the beginning of a machine translation model just as you can the beginning of a monolingual model. In theory, if the embeddings learned by a machine translation model had desirable properties, they could also be widely shared and used for transfer learning, like Word2Vec embeddings often are. \n\nWhen the authors of the paper dive into comparing the embeddings from both of these two approaches, they find some interesting results, such as: while the monolingual embeddings do a better job at analogy-based tests, machine translation embeddings do better at having similarity, within their vector space, map to true similarity of concept. Put another way, while monolingual systems push together words that appear in similar contexts (Teacher, Student, Principal), machine translation systems push words together when they map to the same or similar words in the target language (Teacher, Professor). The attached image shows some examples of this effect; the first three columns are all monolingual approaches, the final two are machine translation ones. When it comes to analogies, machine translation embeddings perform less well at semantic analogies (Ottowa is to Canada as Paris is to France) but does better at syntactic analogies (fast is to fastest as heavier is to heaviest). While I don\u2019t totally understand why monolingual would be better at semantic analogies, it does make sense that the machine translation model would do a better job of encoding syntactic information, since such information is necessarily to sensibly structure a sentence. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1412.6448"
    },
    "337": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1805.09804",
        "transcript": " This paper outlines (yet another) variation on a variational autoencoder (VAE), which is, at a high level, a model that seeks to 1) learn to construct realistic samples from the data distribution, and 2) capture meaningful information about the data within its latent space. The \u201clatent space\u201d is a way of referring to the information bottleneck that happens when you compress the input (typically for these examples: an image) into a low-dimensional vector, before trying to predict that input out again using that low-dimensional vector as a seed or conditional input. In a typical VAE, the objective function is composed of two terms: a reconstruction loss that captures how well your Decoder distribution captures the X that was passed in as input, and a regularization loss that pushes the latent z code you create to be close to some input prior distribution. Pushing your learned z codes to be closer to a prior is useful because you can then sample using that prior, and have those draws map to the coheret regions of the space, where you\u2019ve trained in the past.\n \nThe Implicit Autoencoder proposal changes both elements of this objective function, but since one - the modification of the regularization term - is actually drawn from another (Adversarial Autoencoders), I\u2019m primarily going to be focusing on the changes to the reconstruction term. In a typical variational autoencoder, the model is incentivized to perform an exact reconstruction of the input X, by using the latent code as input. Since this distance is calculated on a pixelwise basis, this puts a lot of pressure on the latent z code to learn ways of encoding this detailed local information, rather than what we\u2019d like it to be capturing, which is broader, global structure of the data. In the IAE approach, instead of incentivizing the input x to be high probability in the distribution conditioned by the z that the encoder embedded off of x, we instead try to match the joint distributions of (x, z) and (reconstructed-x, z). This is done by taking these two pairs, and running them through a GAN, which needs to tell which pair represents the reconstructed x, and which the input x. Here, the GAN takes as input a concatenation of z (the embedded code for this image), and n, which is a random vector. Since a GAN is a deterministic mapping, this random vector n is what allows for sampling from this model, rather than just pulling the same output every time.\n \nUnder this system, the model is under less pressure to recreate the details from the particular image that was input. Instead, it just needs to synchronize the use of z between the encoder and the decoder. To understand why this is true, imagine if you had an MNIST set of 1 and 2s, and a binary number for your z distribution. If you encode a 2, you can do so by setting that binary float to 0. Now, as long as your decoder realizes what the encoder was trying to do, and reconstructs a 2, then the joint distribution will be similar between the encoder and decoder, and our new objective function will be happy. An important fact here is: this doesn\u2019t require that the decoder reconstruct the *exact* 2 that was passed in, as long as it matches, in distribution, the set of images that the encoder is choosing to map to the same z code, the decoder can do well.\nA consequence of this approach is an ability to modulate how much information you actually want to pull out into your latent vector, and how much you just want to be represented by your random noise vector, which will control randomness in the GAN and, to continue the example above, allow you to draw more than one distinct 2 off of the \u20182\u201d latent code. If you have a limited set of z dimensionality, the they will represent high level concepts (for example: MNIST digits) and the rest of the variability in images will be modeled through the native GAN framework. If you have a high dimensional z, then more and more detail-level information will get encoded into the z vector, rather than just being left to the noise.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1805.09804"
    },
    "338": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1804.02476",
        "transcript": "These days, a bulk of recent work in Variational AutoEncoders - a type of generative model - focuses on the question of how to add recently designed, powerful decoders (the part that maps from the compressed information bottleneck to the reconstruction) to VAEs, but still cause them to capture high level, conceptual information within the aforementioned information bottleneck (also know as a latent code). In the status quo, it\u2019s the case that the decoder can do well enough even without conditioning on conceptual variables stored in the latent codes, that it\u2019s not worth storing information there. \n\nThe reason why VAEs typically make it costly to store information in latent codes is the typical inclusion of a term that measures the KL divergence (distributional distance, more or less) between an uninformative unit Gaussian (the prior) and distribution of latent z codes produced for each individual input x (the posterior). Intuitively, if the distribution for each input x just maps to the prior, then that gives the decoder no information about what x was initially passed in: this means the encoder has learned to ignore the latent code. The question of why this penalty term is included in the VAE has two answers, depending on whether you\u2019re asking from a theoretical or practical standpoint. Theoretically, it\u2019s because the original VAE objective function could be interpreted as a lower bound on the true p(x) distribution. Practically, pulling the individual distributions closer to that prior often has a regularizing effect, that causes z codes for individual files to be closer together, and also for closeness in z space to translate more to closeness in recreation concept. That happens because the encoder is disincentivized from making each individual z distribution that far from a prior. The upshot of this is that there\u2019s a lot of overlap between the distributions learned for various input x values, and so it\u2019s in the model\u2019s interest to make the reconstruction of those nearby elements similar as well. \n\nThe argument of this paper starts from the compression cost side. If you look at the KL divergence term with the prior from an information theory, you can see it as the \u201ccost of encoding your posterior, using a codebook developed from your prior\u201d. This is a bit of an opaque framing, but the right mental image is the morse code tree, the way that the most common character in the English language corresponds to the shortest morse symbol, and so on. This tree was optimized to make messages as short as possible, and was done so by mapping common letters to short symbols. But, if you were to encode a message in, say, Russian, you\u2019d no longer be well optimized for the letter distribution in Russian, and your messages would generally be longer. So, in the typical VAE setting, we\u2019re imagining a receiver who has no idea what message he\u2019ll be sent yes, and so uses the global prior to inform their codebook. By contrast, the authors suggest a world in which we meaningfully order the entries sent to the receiver in terms of similarity.  Then, if you use the heuristic \u201ceach message provides a good prior for the next message I\u2019ll receive, you incur a lot less coding cost than, because the \u201cprior\u201d is designed  to be a good distribution to use to encode this sample, which will hopefully be quite similar to the next one. \n\nOn a practical level, this translates to:  \n1. Encoding a z distribution \n2. Choosing one of that z code\u2019s K closest neighbors \n3. Putting that as input into a \u201cprior network\u201d that takes in the randomly chosen nearby c, and spits out distributional parameters for another distribution over zs, which we\u2019ll call the \u201cprior\u201d. \nIntuitively, a lot of the trouble with the constraint that all z encodings be close to the same global prior is that that was just too restrictive. This paper tries to impose a local prior instead, that\u2019s basically enforcing local smoothness, by pulling the z value closer to others already nearby it,but without forcing everything to look like a global prior. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1804.02476"
    },
    "339": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1512.09300",
        "transcript": "Variational Autoencoders are a type of generative model that seek to learn how to generate new data by incentivizing the model to be able to reconstruct input data, after compressing it to a low-dimensional space. Typically, the way that the reconstruction is scored against the original is by comparing the pixel by pixel values: a reconstruction gets a high score if it is able to place pixels of color in the same places that the original did. However, there are compelling reasons why this is a sub-par way of scoring images. \n\nThe central one is: it focuses on and penalizes superficial differences, so if the model accurately reproduces the focal object of the image, but does so, say, 10 pixels to the right of where it was previously, that will incur a penalty we might not actually want to apply. The flip side of this is that a direct pixel-comparison loss doesn\u2019t differentiate between pixel differences that do or don\u2019t change the fundamental substance of the image. For instance, having 100 pixels wrong around the border of a dog, making it seem very slightly larger, would be the same amount of error as having 100 pixels concentrated in a weird bulb that appears to be growing out of a dog\u2019s ear, even though the former does a better job of being recognizable as a dog.\n\nThe authors of the VAE/GAN paper have a clever approach to solving this problem, that involves taking the typical pixel loss, and breaking it up into two conceptual parts. \nThe first focuses on aligning the conceptual features of the reconstructed image with the conceptual features of the input image. It does so by running both the input and the reconstruction through a discriminative convolutional model which - in the typical way of deep learning - learns ever more abstract features at each layer of the network. These \u201cconceptual features\u201d abstract out the precise pixel values, and instead capture the higher level features of the image. So, instead of calculating the pixelwise squared loss between the specific input x, and its after-bottleneck reconstruction x~, you take the squared loss between the feature maps at some layer for both x and x~, and push them to be closer together, so that the reconstruction shares the same features as the original. \nThe second focuses on detail-level specifics of images, but, cleverly, does so in a general, rather than a observation-specific way. This is done by training a GAN-style discriminator to tell the difference between generated images* and original image, and then using that loss to train the decoder part of the VAE. The cleverness of this comes from the fact that they are still enforcing that the details and structural features of the reconstructed image are not distinguishable from real images, but doing so in a general sense, rather than requiring the details to be an exact match to the details found in a given input x. \n\nhttps://i.imgur.com/Bmtmac2.png\n\nThe authors freely admit that existing metrics of scoring images (which themselves *use* pixelwise similarity) rate their method as being worse than existing VAEs. However, they argue, that\u2019s inherently a flawed metric, that doesn\u2019t capture the aspects of clean visual quality we want in generated image. A metric they propose instead involves using an dataset where a list of attributes are attached to each image (old, black, blond, etc). They add these as additional input while training the network, so that whatever signals the decoder part of the model needs to turn someone blonde, it gets those from the externally-given attribute vector, rather than a learned representation. This means that, once the model is trained, we can set some value of the attribute vector, and have the decoder generate samples conditional on that. The metric is constructed by taking the decoded samples conditioned on some attribute set, and then taking a classifier model that is trained on the real images to detect attribute values from the images. The generated images are then scored by how closely the predictions from the classifier model match the true values of the attributes. If the generator model were working perfectly, this error rate would as low as for real data. By this metric (which: grain of salt, since they invented), the VAE/GAN model is superior to both GANs and vanilla VAEs. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1512.09300"
    },
    "340": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.00937",
        "transcript": "There are mathematicians, still today, who look at deep learning, and get real salty over the lack of convex optimization. That is to say: convex functions are ones where you have an actual guarantees that gradient descent will converge,  and mathematicians of olden times (i.e. 2006) spent reams of paper arguing that this or that function had convex properties, and thus could be guaranteed to converge, under this or that set of arcane conditions. And then, Deep Learning came along, with its huge, nonlinear, very much nonconvex objective functions, that it was nonetheless trying to optimize via gradient descent. From the perspective of an optimization theorist, this had the whiff of heresy, but exceptionally effective heresy. And, so, the field of DL has half-exploded, half-stumbled along, showcasing a portfolio of very impressive achievements, but with theory very much a secondary priority relative to performance. \n\nSomething else that gradient descent isn\u2019t supposed to be able to do is learn models that include discrete (i.e. non-continuous) operators. Without continuous gradients, the functions don\u2019t have an obvious way to \u201cpush\u201d in a certain direction, to modulate the loss at the end of the network. Discrete nodes mean that the value just jumps from being in one state, to being in the other, with no intermediate values. This has historically posed a problem for algorithms fueled by gradient descent.\n\n\nThe authors of this paper came up with a solution that is 60% cleverness, and 40% just guessing that \u201ceven if we ignore the theory, things will probably work well enough\u201d. But, first, their overall goal: to create a Variational Auto Encoder where the latent states, the compressed internal representation that is typically an array of continuous values, is instead an array of categorical values. The goal of this was 1) to have a representation type that was a better match for the discrete nature of data types like speech (which has distinct phonemes we might like to discretely capture), and, 2) to have a more compressed latent space that would (of necessity) focus on more global information, and leave local pixel-level information to be learned by the expressive PixelCNN decoder. The way they do this is remarkably simple. First, they learn a typical VAE encoder, mapping from the input pixels to a continuous z space. (An interesting sidenote here is that this paper uses spatially organized z; instead of using one single z vector to represent the whole image, they may have 32x32 spatial locations, each of which has its own z vector,  to represent at 128x128 image). Then, for each of the spatial regions, they take the continuous vector produced by the network, and compare it to a fixed set of \u201cembedding\u201d vectors, of the same shape. That spatial location is then lumped into the category of the embedding that it\u2019s closest to, meaning that you end up with a compressed layer of 32x32 (in this case) spatial regions, each of which is represented by a categorical number between 0 and max-num-categories. Then, the network passes forward the embedding that this input vector was just \u201csnapped\u201d to being, Then, the decoder uses the full spatial location set of embeddings to do its decoding.\n \nhttps://i.imgur.com/P8LQRYJ.png\n\nThe clever thing here comes when you ask how to train the encoder to produce a different embedding, when there was this discrete \u201cjump\u201d that happened. The authors choose to just avoid the problem, more or less. They do that by just taking the gradient signals that come back from the end of the network to the embedding, and just pass those directly to the vector that was used to nearest-neighbors-lookup the embedding. Basically, they pretend that they passed the vector through the rest of the network, rather than the embedding. The embeddings are then trained in a K Means Clustering kind of way; with the embeddings being iteratively updated to be closer to the points that were assigned to their embedding in each round of training. This is the \u201cVector Quantization\u201d part of VQ-VAE\n\nOverall, this seems to perform quite well: with the low capacity of the latente space meaning that it is incentivized to handle more global structure, while leaving low level pixel details to the decoder. It is also much easier to fit after-the-fact distributions over; once we\u2019ve trained a VQ-VAE, we can easily learn a global model that represents the location by location dependencies between the categories (i.e. a 1 in this corner means at 5 in this other corner is more probable). This gives us the ability to have an analytically specified distribution, in latent space, that actually represents the structure of how these \u201cconcept level categories\u201d relate to each other. By contrast, with most continuous latent spaces, it\u2019s intractable to learn an explicit density function after the fact, and thus if we want to be able to sample we need to specify and enforce a prior distribution over z ahead of time. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.00937"
    },
    "341": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1803.05428",
        "transcript": "I\u2019ve spent the last few days pretty deep in the weeds of GAN theory - with all its attendant sample-squinting and arcane training diagnosis - and so today I\u2019m shifting gears to an applied paper, that mostly showcases some clever modifications of an underlying technique. The goal of the MusicVAE is as you might expect: to make music. But the goal isn\u2019t just the ability to produce patterns of notes that sound musical, it\u2019s the ability to learn a vector space where we can modify the values along each dimension, and cause the music we produce to vary along conceptually meaningful directions. In an ideal world, we might learn a dimension that corresponds to tempo, another that corresponds to the key we\u2019re in, etc.\n\nTo achieve this goal, the modelers use the structure of a Variational AutoEncoder, a model where we pass in the input, compress it down to some latent code (read: a low-dimensional vector of continuous values), and then, starting from that latent code, use a decoder to try to recreate (or \u201creconstruct\u201d) the output. Think of this as describing a scene to a friend behind their back, and trying to describe it in a maximally informative way, so that they can draw it themselves, and get as close as possible to the original. Ideally, this set of constraints incentives you to learn an informative code, which will contain the kind of conceptually meaningful information that we want it to.\n\nOne problem this can run into is that, given certain mathematical facts about the structure of autoencoders, if you use a decoder with a lot of capacity, like a RNN, the model can \u201cdecide\u201d to use the RNN to model the data directly, storing all that conceptual information we\u2019d like to have pulled out in the latent code in the parameters of the RNN instead. And, so, to solve this, the authors of the paper came up with a clever solution: instead of generating the full piece of music at once, they would instead build a hierarchical model, with a \u201cconductor\u201d layer that prescribes what a medium-sized chunk of the reconstructed piece will sound like, and a lower level \u201cdecoder\u201d layer that takes the conductor\u2019s direction for that chunk, and unspools it into a series of notes. On a more mechanical level, when the encoder spits out a latent code for a given piece of music, we pass that to the conductor. The conductor then produces - say - 10 embeddings, with each embedding corresponding to a set of 4 measures. Each decoder only sees the embedding for its chunk, and is only responsible for mapping that embedding into a series of concrete notes. This inability of each decoder to see what the decoders before and after it are doing means that, in order for the piece to sound coherent, the network needs to learn to develop a condensed set of instructions to give to the conductor.\n\nhttps://i.imgur.com/PQKoraX.png\n\nIn practice, they come up with some really neat results: the example they show on the linked page demonstrates a learned concept-dimension that maps to \u201chow much is this piece composed of long, held notes, vs short staccato ones\u201d. They show that they can \u201cinterpolate\u201d across this dimension (that is: slowly change its value) and see that the output slowly morphs from very long held notes, to a high density of different ones.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1803.05428"
    },
    "342": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.00704",
        "transcript": "Despite their difficulties in training, Generative Adversarial Networks are still one of the most exciting recent ideas in machine learning; a way to generate data without the fuzziness and averaging of earlier methods. However, up until recently, there had been major way in which the GAN\u2019s primary competitor in the field, the Variational Autoencoder, was superior: it could do inference. \n\nIntuitively, inference is the inverse of generation. Whereas generation works by taking some source of randomness - a random vector, the setting of some latent code - and transforming that recipe into an observation, an inference process tries to work in reverse, taking in the observation as input and trying to guess what \u201crecipe\u201d was used to generate it. (As a note: in real world data, it\u2019s generally not the case that there were explicit numerical factors used to generate data; this framing is a simplified model meant to represent the way a small set of latent settings of an object jointly cause a lot of that object\u2019s feature values). The authors of this paper proposed the BiGAN to fix that deficiency in GAN literature. \n\nhttps://i.imgur.com/vZZzWH5.png\n\nThe BiGAN - short for Bidirectional GAN - works by having two generators, not one. One generator works in the typical fashion of a GAN: taking in a random vector z, and transforming that into G(z) = x. The second generator works in reverse, taking in as input data from the underlying dataset, and transforming it into a code z, E(x) = z. Once these generators are in place, the discriminators work, not by trying to differentiate the x and z values separately, but all together. That works by giving the discriminator a pair, (x, z), and asking the discriminator to decide whether that pair came from the z -> x decoder, or the x -> z encoder. If this model fully converges, it becomes the case that G(z) and E(x) are inverse transformations, giving us a way to take in a new input x, and infer its underlying factors z. This is valuable because it\u2019s been shown that, in typical GANs, changes in z often correspond to latent values we care about, and it would be useful to be able to generate z from x for purposes of representation learning.  \n\nThe authors offer quite a nice intuitive proof for why the model learns this inverse mapping. For each pair of (x, z), it\u2019s either the case that E(x) = z (if the pair came from the encoder), or that G(z) = x (if the pair came from the decoder). But if only one of those is the case, then it\u2019s easy for the discriminator to tell which generation process produced the pair. So, in order to fool the discriminator, G(z) and E(x) need to synchronize their decoding and encoding processes. \n\nThe authors also tried a method where, instead of having this bidirectional GAN structure, they instead simply built a network on top of the generated samples, that tries to predict the original z used, taking the generated x as input. They show that this performs less well on subjective quality measures of the learned representation, which they attribute to the fact that GANs notoriously only learn some modes of the data, and thus a x -> z encoder that only takes the generated z as input will not have good coverage over the full distribution of x.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.00704"
    },
    "343": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.04076",
        "transcript": "Generative Adversarial Networks (GANs) are an exciting technique, a kernel of an effective concept that has been shown to be able to overcome many of the problems of previous generative models: particularly the fuzziness of VAEs. But, as I\u2019ve mentioned before, and as you\u2019ve doubtless read if you\u2019re read any material about the topic, they\u2019re finicky things, difficult to train in a stable way, and particularly difficult to not devolve into mode collapse. Mode collapse is a phenomenon where, at each iteration, the generator places all of its mass on one single output or dense cluster of outputs, instead of representing the full distribution of output space, they way we\u2019d like it to. One proposed solution to this is the one I discussed yesterday, of explicitly optimizing the generator according to not only what the discriminator thinks about its current allocation of probability, but what the discriminator\u2019s next move will be (thus incentivizing the generator not to take indefensible strategies like \u201cput all your mass in one location the discriminator can push down next round\u201d. \n\nAn orthogonal approach to that one is the one described in LSGANs: to change the objective function of the network, away from sigmoid cross-entropy, and instead to a least squares loss. While I don\u2019t have the latex capabilities to walk through the exact mathematics in this format, what this means on a conceptual level is that instead of incentivizing the generator to put all of its mass on places that the discriminator is sure is a \u201ctrue data\u201d region, we\u2019re instead incentivizing the generator to put mass right on the true/fake data decision boundary. Likely this doesn\u2019t make very much sense yet (it didn\u2019t for me, at this point in reading). Occasionally, delving deeper into math and theory behind an idea provides you rigor, but without much intuition. I found the opposite to be true in this case, where learning more (for the first time!) about f divergences actually made this method make more sense. So, bear with me, and hopefully trust me not to take you to deep into the weeds without a good reason. \n\nOn a theoretical level, this paper\u2019s loss function means that you end up minimizing a chi squared divergence between the distributions, instead of a KL divergence. \"F divergences\" are a quantity that calculates a measure of how different two distributions are from one another, and does that by taking an average of the density q, weighted at each point by f, which is some function of the ratio of densities, p/q. (You could also think of this as being an average of the function f, weighted by the density q; they\u2019re equivalent statements). For the KL divergence, this function is x*logx. For chi squared it\u2019s (x-1)^2. \n\nAll of this starts to coalesce into meaning with the information that, typically the behavior of a typical GAN looks like the divergence FROM the generator\u2019s probability mass, TO the discriminator\u2019s probability mass. That means that we take the ratio of how much mass a generator puts somewhere to how much mass the data has there, and we plug it into the x*logx function seen below.\n\n https://i.imgur.com/BYRfi0u.png\n\nNow, look how much the function value spikes when that ratio goes over 1. Intuitively, what this means is that we heavily punish the generator when it puts mass in a place that\u2019s unrealistic, i.e. where there isn\u2019t representation from the data distribution. But - and this is the important thing - we don\u2019t symmetrically punish it when it its mass at a point is far higher than the mass put their in the real data; or when the ratio is much smaller than one. This means that we don\u2019t have a way of punishing mode collapse, the scenario where the generator puts all of its mass on one of the modes of the data; we don\u2019t do a good job of pushing the generator to have mass everywhere that the data has mass. By contrast, the Chi Squared divergence pushes the ratio of (generator/data) to be equal to 1 *from both directions*. So, if there\u2019s more generator mass than data mass somewhere, that\u2019s bad, but it\u2019s also bad for there to be more data mass than generator mass. This gives the network a stronger incentive to not learn mode collapsed solutions.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.04076"
    },
    "344": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.02163",
        "transcript": "If you\u2019ve ever read a paper on Generative Adversarial Networks (from now on: GANs), you\u2019ve almost certainly heard the author refer to the scourge upon the land of GANs that is mode collapse. When a generator succumbs to mode collapse, that means that, instead of modeling the full distribution, of input data, it will choose one region where there is a high density of data, and put all of its generated probability weight there. Then, on the next round, the discriminator pushes strongly away from that region (since it now is majority-occupied by fake data), and the generator finds a new mode. \n\nIn the view of the authors of the Unrolled GANs paper,  one reason why this happens is that, in the typical GAN, at each round the generator implicitly assumes that it\u2019s optimizing itself against the final and optimal discriminator. And, so, it makes its best move given that assumption, which is to put all its mass on a region the discriminator assigns high probability. Unfortunately for our short-sighted robot friend, this isn\u2019t a one-round game, and this mass-concentrating strategy gives the discriminator a really good way to find fake data during the next round: just dramatically downweight how likely you think data is in the generator\u2019s prior-round sweet spot, which it\u2019s heavy concentration allows you to do without impacting your assessment of other data. Unrolled GANs operate on this key question: what if we could give the generator an ability to be less short-sighted, and make moves that aren\u2019t just optimizing for the present, but are also defensive against the future, in ways that will hopefully tamp down on this running-around-in-circles dynamic illustrated above. If the generator was incentivized not only to make moves that fool the current discriminator, but also make moves that make the next-step discriminator less likely to tell it apart, the hope is that it will spread out its mass more, and be less likely to fall into the hole of a mode collapse. \n\nThis intuition was realized in UnrolledGANs, through a mathematical approach that is admittedly a little complex for this discussion format. Essentially, in addition to the typical GAN loss (which is based on the current values of the generator and discriminator), this model also takes one \u201cstep forward\u201d of the discriminator (calculates what the new parameters of the discriminator would be, if it took one update step), and backpropogates backward through that step. The loss under the next-step discriminator parameters is a function of both the current generator, and the next-step parameters, which come from the way the discriminator reacts to the current generator. When you take the gradient with respect to the generator of both of these things, you get something very like the ideal we described earlier: a generator that is trying to put its mass into areas the current discriminator sees as high-probability, but also change its parameters such that it gives the discriminator a less effective response strategy. \n\nhttps://i.imgur.com/0eEjm0g.png\n\nEmpirically: UnrolledGANs do a quite good job at their stated aim of reducing mode collapse, and the unrolled training procedure is now a common building-block technique used in other papers. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.02163"
    },
    "345": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1703.10593",
        "transcript": "Over the last five years, artificial creative generation powered by ML has blossomed. We can now imagine buildings based off of a sketch, peer into the dog-tiled \u201cdreams\u201d of a convolutional net, and, as of 2017, turn images of horses into ones of zebras. This last problem - typically termed image-to-image translation- is the one that CycleGAN focuses on. The kinds of transformations that can full under this category is pretty conceptually broad: zebras to horses, summer scenes to winter ones, images to Monet paintings. (Note: I switch between using horse/zebra as my explanatory example, and using summer/winter. Both have advantages for explaining different conceptual poinfts) However, the idea is the same: you start with image a, which belongs to set A, and you want to generate a mapping of that image into set B, where the only salient change is that it\u2019s now in set B. As a clarifying example: if you started out with a horse, and your goal was to translate it into a zebra, you would hope that the animal is in the same size, relative position, and pose, and that the only element that changed was changing the quality of \u201chorseness\u201d for the quality of \u201czebraness\u201d. \n\nhttps://i.imgur.com/NCExS7A.png\n\nThe real trick of CycleGAN is the fact that, unlike prior attempts to solve this problem, they didn\u2019t use paired data. This is understandable, given the prior example: while it\u2019s possible to take a picture of a scene in both summer and winter, you obviously can\u2019t convert a horse into a zebra so that you can take a \u201cpaired\u201d picture of it in both forms. When you have paired data, this is a reasonably well-defined problem: you want to learn some mathematical transformation to turn a specific summer image into a specific winter one, and you can use the ground truth winter image as explicit supervision. Since they lack this per-image cross-domain ground truth, the authors of this paper take what would be one question (\u201cis the winter version of this image that the network generated close to the actual known winter version of this image\u201d) and decompose it into two: \nDoes the winter version of this original summer image looks like it belongs to the set of winter images? This is enforced by a GAN-style discriminator, which takes in outputs of the summer->winter generator, and true images of winter, and tries to tell them apart. This loss component pushes generated winter images to have the quality of \u201cwinterness\u201d. This is the \u201cAdversarial Loss\u201d \nDoes the winter version of this image contain enough information about this specific original summer image to accurately reconstruct it with an inverted (winter -> summer) generator? This constraint pushes the generator to actually translate aspects of this specific image between summer and winter. Without it, as the authors of the paper showed, the model has no incentive to actually do translation, and instead just generates winter images that have nothing to do with the summer image (and, frequently experience mode collapse: only generating a single winter image over and over again). This is termed the \u201cCycle Consistency Loss\u201d \n\nIt\u2019s actually the case that there are two versions of both of the above networks; that\u2019s what puts the \u201ccycle\u201d in CycleGAN. In addition to a loss ensuring you can map summer -> winter -> summer, there\u2019s another one ensuring the other direction, winter -> summer -> winter holds as well. And, for both of those directions, we use the adversarial loss on the middle \u201ctranslated\u201d image, and a cycle consistency loss on the last \u201creconstructed\u201d image. A key point here is that, because of the inherent structure of this loss function requires mapping networks going in both directions, training a winter->summer generator gets you a summer-> winter one for free. \n\n(Note: this is a totally different model architecture than most of the \u201cstyle transfer\u201d applications you likely previously seen, though when applied to photograph -> painting translation, it can have similar results) \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1703.10593"
    },
    "346": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.00597",
        "transcript": "The paper provides derivations and intuitions about the learning dynamics for VAEs based on observations about [$\\beta$-VAEs][beta]. Using this they derive an alternative way to constrain the training of VAEs that doesn't require typical heuristics, such as warmup or adding noise to the data.\n\nHow exactly would this change a typical implementation? Typically, SGD is used to [optimize the ELBO directly](https://github.com/pytorch/examples/blob/master/vae/main.py#L91-L95). Using GECO, I keep a moving average of my constraint $C$ (chosen based on what I want the VAE to do, but it can be just the likelihood plus a tolerance parameter) and use that to calculate Lagrange multipliers, which control the weighting of the constraint to the loss. [This implementation](https://github.com/denproc/Taming-VAEs/blob/master/train.py#L83-L97) from a class project appears to be correct.\n\nWith the stabilization of training, I can't help but think of this as batchnorm for VAEs.\n\n[beta]: https://openreview.net/forum?id=Sy2fzU9gl",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.00597"
    },
    "347": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1101/2020.02.07.938852",
        "transcript": "A very simple (but impractical) discrete model of subclonal evolution would include the following events:\n\n* Division of a cell to create two cells:\n    * **Mutation** at a location in the genome of the new cells\n* Cell death at a new timestep\n* Cell survival at a new timestep\n\nBecause measurements of mutations are usually taken at one time point, this is taken to be at the end of a time series of these events, where a tiny of subset of cells are observed and a **genotype matrix** $A$ is produced, in which mutations and cells are arbitrarily indexed such that $A_{i,j} = 1$ if mutation $j$ exists in cell $i$. What this matrix allows us to see is the proportion of cells which *both have mutation $j$*.\n\nUnfortunately, I don't get to observe $A$, in practice $A$ has been corrupted by IID binary noise to produce $A'$. This paper focuses on difference inference problems given $A'$, including *inferring $A$*, which is referred to as **`noise_elimination`**. The other problems involve inferring only properties of the matrix $A$, which are referred to as:\n\n* **`noise_inference`**: predict whether matrix $A$ would satisfy the *three gametes rule*, which asks if a given genotype matrix *does not describe a branching phylogeny* because a cell has inherited mutations from two different cells (which is usually assumed to be impossible under the infinite sites assumption). This can be computed exactly from $A$.\n* **Branching Inference**: it's possible that all mutations are inherited between the cells observed; in which case there are *no branching events*. The paper states that this can be computed by searching over permutations of the rows and columns of $A$. The problem is to predict from $A'$ if this is the case.\n\nIn both problems inferring properties of $A$, the authors use fully connected networks with two hidden layers on simulated datasets of matrices. For **`noise_elimination`**, computing $A$ given $A'$, the authors use a network developed for neural machine translation called a [pointer network][pointer]. They also find it necessary to map $A'$ to a matrix $A''$, turning every element in $A'$ to a fixed length row containing the location, mutation status and false positive/false negative rate.\n\nUnfortunately, reported results on real datasets are reported only for branching inference and are limited by the restriction on input dimension. The inferred branching probability reportedly matches that reported in the literature.\n\n[pointer]: https://arxiv.org/abs/1409.0473",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1101/2020.02.07.938852"
    },
    "348": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=Kool2020Estimating",
        "transcript": "It's a shame that the authors weren't able to continue their series of [great][reinforce] [paper][attention] [titles][beams], although it looks like they thought about calling this paper **\"Put Replacement In Your Basement\"**. Also, although they don't say it in the title or abstract, this paper introduces an estimator the authors call the **\"unordered set estimator\"** which, as a name, is not the best. However, this is one of the most exciting estimators for gradients of non-differentiable expectations over structured discrete distributions (that sounds specific but includes a vast number of important problems, for example see [the first paragraph of this paper][mcts] and, less important but fun, adversarial examples or GANS on sequences).\n\nFor a distribution $p_\\theta(s)$, where $s$ can take some set of discrete states, this paper is concerned with how to compute the gradient of the expectation of a function, $f$, over $p_\\theta(s)$:\n\n$$\n\\nabla_\\theta \\mathbb{E}_{s \\sim p_\\theta(s)} \\left[ f(s) \\right]\n$$\n\nTo jump directly to the definition of the estimator I've got to define the *leave-one-out* ratio. Given a distribution $p$ over unordered sets $S^k$, the leave-one-out ratio is\n$$\nR(S^k, s) = \\frac{p^{D \\backslash \\{ s \\}} (S^k \\backslash \\{ s \\} )}{p(S^k)}.\n$$\nWhere $p^{D \\backslash \\{ s \\}}$ is the distribution over sets that don't contain $s$, which is the value of the first sampled element. For a given element $s$, it's the ratio of probability of sampling the other elements *given $s$* in $S^k$ divided by the probability of sampling $S^k$.\n\nThen the *unordered set estimator* is\n$$\n\\nabla_\\theta \\mathbb{E}_{s \\sim p_\\theta(s)} \\left[ f(s) \\right] = \\nabla_\\theta \\mathbb{E}_{s \\sim p_\\theta(s)} \\left[ \\sum_{s \\in S^k} p(s) R(S^k, s) f(s) \\right] \\\\\n=   \\mathbb{E}_{s \\sim p_\\theta(s)} \\left[ \\nabla_\\theta p_\\theta (s) R(S^k, s) f(s) \\right],\n$$\nand they proceed to show that it's a Rao-Blackwellization (so guaranteed as low or lower variance) than a single sample estimator, importance weighted estimators and the [the RB discrete estimator][rb].\n\nThe authors also show that they can use the multiple samples to compute a built-in baseline and further reduce variance. If we define the leave-one-out ratio on a distribution with one element already left out as $R^{D \\backslash \\{ s \\} } (S^k, s')$ (the *second-order* leave-one-out ratio):\n$$\n\\nabla_\\theta \\mathbb{E}_{s \\sim p_\\theta(s)} \\left[ f(s) \\right] \\\\\n=  \\mathbb{E}_{s \\sim p_\\theta(s)} \\left[ \\sum_{s \\in S^k} \\nabla_\\theta p_\\theta (s) R(S^k, s) \\left( f(s) - \\sum_{s' \\in S^k} p_{\\theta} (s') R^{D \\backslash \\{ s \\} } (S^k, s') f(s')  \\right) \\right]\n$$\n\nThanks to [stochastic beam search][beams] these sets can be sampled quickly and easily, so it all adds up to a powerful generic method. If I need gradients in an SGD setting of an expectation involving discrete variables, and I can take multiple samples, this is the most promising method I know about at the moment.\n\n[reinforce]: https://openreview.net/forum?id=r1lgTGL5DE\n[attention]: https://arxiv.org/abs/1803.08475\n[beams]: https://arxiv.org/abs/1903.06059\n[mcts]: https://arxiv.org/pdf/1910.06862.pdf\n[rb]: https://arxiv.org/abs/1810.04777",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=rklEj2EFvB"
    },
    "349": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhangM18",
        "transcript": "A common problem in phylogenetics is:\n\n1. I have $p(\\text{DNA sequences} | \\text{tree})$ and $p(\\text{tree})$.\n2. I've used these to run an MCMC algorithm and generate many (approximate) samples from $p(\\text{tree} | \\text{DNA sequences})$.\n3. I want to evaluate $p(\\text{tree} | \\text{DNA sequences})$.\n\nThe first solution you might think of is to add up how many times you saw each *tree topology* and divide by the total number of MCMC samples; referred to in this paper as *simple sample relative frequencies* (SRF). An obvious problem with this method is that if you didn't happen to sample a tree topology you will assign it zero probability.\n\nThis paper focuses on producing a better solution to this problem, by defining a distribution over trees that's easy to fit and provides support over the entire space of tree topologies.\n\nWhat is a Subsplit Bayesian Network?\n==============================\n\nPhylogenies are leaf-labeled bifurcating trees, binary trees with labeled leaves. **Clades** are nonempty subsets of the leaf labels; labeled in the following figure as $C_1 \\to C_7$:\n\nhttps://i.imgur.com/khS3uSo.png\n\nTo build a phylogeny, I can just take the full set of leaves and recursively split them into clades as described in the above diagram. This means that the distribution over phylogenies is equivalent to the distribution over clades. To make this distribution tractable, it is typically assumed that clades are conditionally independent given parent clades; called the *Conditional Clade Distribution* (CCD) assumption, attributed here to [Larget][] and [Hohna][].\n\nSo, a phylogeny may be described by its clades, this paper proposes that these clades may be described by their **subsplits**; the splitting process placing leaf nodes into one of two clades. Then, the authors note this process is a directed Bayesian network and that this Bayesian network must include all possible clade splits. It is therefore a complete binary tree with depth $N-1$ (where $N$ is the number of leaf nodes).\n\nFitting This Parameterisation to MCMC Samples\n=====================================\n\nUnder this parameterisation the likelihood of observing a given tree is:\n$$\np(\\text{tree}) = p(S_1 = s_{1,k}) \\prod_{i>1} p(S_i = s_{i,k} | S_{\\pi_i} = s_{\\pi_i, k}),\n$$\nassuming a collection of subsplits $T_k = \\{ S_i = s_{i,k}, i \\geq 1 \\}$. In this definition $S_{\\pi_i}$ is index set of parent nodes of $S_i$; ie a subsplit can depend on any of the parent nodes.\n\nThe maximum likelihood probabilities for possible subsplits can be solved in closed form; algorithmically involving counting the occurrences of subsplits and dividing by the number of trees observed. If this seems exactly the same as SRF, that's because it is; I [checked the published code to verify this][sbn_ds1].\n\nThe authors then consider the case of unrooted trees, where the log likelihood of observed trees can't be easily factorised. They then present a [simple averaging][sa] (not sure where this variational method is discussed in that paper, appears to be under a different name?) and an EM algorithm to fit SBN models over such distributions. They also discuss conditional probability sharing (parameterising conditional on parent-child relationships) immediately before this and it's not clear if this is used in the distribution fit by SA or EM.\n\nExperiments show the distributions fit using the EM algorithm perform well according to KL divergence from a \"true\" distribution defined by fitting a distribution using SRF to a larger dataset. They also show less dispersion estimating the probability of sampled trees versus that estimated by this \"ground truth\".\n\n[sa]: https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf\n\n[sbn_ds1]: https://github.com/morrislab/sbn/blob/master/experiments/DS1.ipynb\n\n[larget]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3676676/\n[hohna]: https://academic.oup.com/sysbio/article/61/1/1/1676649",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/7418-generalizing-tree-probability-estimation-via-bayesian-networks"
    },
    "350": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1811.11804",
        "transcript": "This paper compares methods to calculate the marginal likelihood, $p(D | \\tau)$, when you have a tree topology $\\tau$ and some data $D$ and you need to marginalise over the possible branch lengths $\\mathbf{\\theta}$ in the process of Bayesian inference. In other words, solving the following integral:\n\n$$\n\\int_{ [ 0, \\infty ]^{2S - 3} } p(D | \\mathbf{\\theta}, \\tau ) p( \\mathbf{\\theta} | \\tau) d \\mathbf{\\theta}\n$$\n\nThere are some details about this problem that are common to phylogenetic problems, such as an exponential prior on the branch lengths, but otherwise this is the common problem of approximate Bayesian inference. This paper compares the following methods:\n\n* ELBO (appears to be [BBVI][])\n* Gamma Laplus Importance Sampling\n* Variational Bayes Importance Sampling\n* Beta' Laplus\n* Gamma Laplus\n* Maximum un-normalized posterior probability\n* Maximum likelihood\n* Naive Monte Carlo\n* Bridge Sampling\n* Conditional Predictive Ordinates\n* Harmonic Mean\n* Stabilized Harmonic Mean\n* Nested Sampling\n* Pointwise Predictive Density\n* Path Sampling\n* Modified Path Sampling\n* Stepping Stone\n* Generalized Stepping Stone\n\nI leave the in depth description of each algorithm to the paper and appendices, although it's worth mentioning that Laplus is a Laplace approximation where the approximating distribution is constrained to be positive.\n\nSome takeaways from the empirical results:\n\n* If runtime is not a concern power posterior methods are preferred:\n\n>  The power posterior methods remain the best general-purpose tools for phylogenetic modelcomparisons, though they are certainly too slow to explore the tree space produced by PT.\n\n* Bridge sampling is the next choice, if you need something faster.\n* Harmonic Mean is a bad estimator for phylogenetic tree problems.\n* Gamma Laplus is a good fast option.\n* Naive Monte Carlo is a poor estimator, which is probably to be expected.\n* Gamma Laplus is the best option for very fast algorithms:\n\n> Empirical posterior distributions on branch lengths are clearly not point-masses, and yet simply normalizing the unnormalized posterior at the maximum outperforms 6 of the 19 tested methods.\n\nAll methods were compared on metrics important to phylogenetic inference, such as *average standard deviation of split frequencies\" (ASDSF), which is typically used to confirm whether parallel MCMC chains are sampling from the same distribution over tree topologies. Methods were also compared on KL divergence to the true posterior and RMSD (appears to be the mean squared error between CDFs?).\n\n[bbvi]: https://arxiv.org/abs/1401.0118",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1811.11804v1"
    },
    "351": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.04777",
        "transcript": "This paper approaches the problem of optimizing parameters of a discrete distribution with respect to some loss function that is an expectation over that distribution. In other words, an experiment will probably be a variational autoencoder with discrete latent variables, but there are many real applications:\n\n$$\n\\mathcal{L} (\\eta) : = \\mathbb{E}_{z \\sim q_{\\eta} (z)} \\left[ f_{\\eta} (z) \\right]\n$$\n\nUsing the [product rule of differentiation][product] the derivative of this loss function can be computed by enumerating all $1 \\to K$ possible values of $z$:\n\n$$\n\\nabla_\\eta \\mathbb{E}_{z \\sim q_{\\eta} (z)} \\left[ f_{\\eta} (z) \\right] = \\nabla_\\eta \\sum_{k=1}^{K} q_\\eta (k) f_\\eta (k) \\\\\n=  \\sum_{k=1}^{K}  f_\\eta (k) \\nabla_\\eta q_\\eta (k) + q_\\eta (k) \\nabla_\\eta  f_\\eta (k)\n$$\n\nThis expectation can also be expressed as the score function estimator (aka the REINFORCE estimator):\n\n$$\n\\nabla_\\eta \\mathbb{E}_{z \\sim q_{\\eta} (z)} \\left[ f_{\\eta} (z) \\right] = \\sum_{k=1}^{K}  \\left(f_\\eta (k) \\nabla_\\eta q_\\eta (k) + q_\\eta (k) \\nabla_\\eta  f_\\eta (k)\\right)\\frac{q_\\eta (k)}{q_\\eta (k)} \\\\\n= \\sum_{k=1}^{K} q_\\eta (k) f_\\eta (k) \\nabla_\\eta \\log q_\\eta (k) + q_\\eta (k) \\nabla_\\eta  f_\\eta (k) \\\\\n=  \\mathbb{E}_{z \\sim q_{\\eta} (z)} \\left[ f_\\eta (k) \\nabla_\\eta \\log q_\\eta (k) +  \\nabla_\\eta  f_\\eta (k) \\right] \\\\\n= \\sum_{k=1}^{K}  f_\\eta (k) \\nabla_\\eta q_\\eta (k) + q_\\eta (k) \\nabla_\\eta  f_\\eta (k) =  \\mathbb{E}_{z \\sim q_{\\eta} (z)} \\left[ g(z) \\right]\n$$\nIn other words, both can be referred to as estimators $g(z)$.\n\nThe authors note that this can be calculated over a subset of the $k$ most probable states (overloading their $k$ from possible values of $z$). Call this set $C_k$:\n\n$$\n\\nabla_\\eta \\mathbb{E}_{z \\sim q_{\\eta} (z)} \\left[ f_{\\eta} (z) \\right]  = \\mathbb{E}_{z \\sim q_{\\eta} (z)} \\left[ g(z) \\right] \\\\\n=  \\mathbb{E}_{z \\sim q_{\\eta} (z)} \\left[ g(z) \\mathbb{1}\\{ z \\in C_k\\} + g(z) \\mathbb{1} \\{ z \\notin C_k \\} \\right] \\\\\n= \\sum_{z \\in C_k} q_\\eta(z) g(z) +  \\mathbb{E}_{z \\sim q_{\\eta} (z)} \\left[  g(z) \\mathbb{1} \\{ z \\notin C_k \\} \\right]\n$$\n\nAs long as $k$ is small, it's easy to calculate the first term, and if most of the probability mass is contained in that set, then it shouldn't matter how well we approximate the second term.\n\nThe authors choose an importance-sampling for the second term, but this is where I get confused. They denote their importance weighting function $q_\\eta (z \\notin C_k)$ which *could* mean all of the probability mass *not* under the states in $C_k$? Later, they define a decision variable $b$ that expresses whether we are in this set or not, and it's sampled with probability $q_\\eta (z \\notin C_k)$, so I think my interpretation is correct. The gradient estimator then becomes:\n\n$$\n\\hat{g} (v) = \\sum_{z \\in C_k} q_\\eta (z) g(z) + q_\\eta (z \\notin C_k) g(v)\\\\\nv \\sim q_\\eta | v \\notin C_k\n$$\n\n[product]: https://en.wikipedia.org/wiki/Product_rule\n\nShowing this is Rao-Blackwellization\n----------------------------------------------\n\nAnother way to express $z$ would be to sample a Bernoulli r.v. with probability $\\sum_{j \\notin C_k} q_\\eta (j) $, then if it's $1$ sample from $z \\in C_k$ and if it's $0$ sample from $z \\notin C_k$. As long as those samples are drawn using $q_\\eta$ then:\n\n$$\nT(u,v,b) \\stackrel{d}{=} z \\\\\nT := u^{1-b} v^b\n$$\nwhere $u \\sim q_\\eta | C_k$, $v \\sim q_\\eta | v \\notin C_k$ and $b \\sim \\text{Bernoulli}(\\sum_{j \\notin C_k} q_\\eta (j))$.\n\nExpressing $z$ in this way means the gradient estimator from before can be written as:\n\n$$\n\\hat{g} (v) = \\mathbb{E} \\left[ g( T(u,v,b) ) | v \\right]\n$$\nAnd they left it as an exercise for the reader to expand that out and show it's the same as equation 6:\n$$\n\\mathbb{E} \\left[ g( T(u,v,b) ) | v \\right] = \\mathbb{E} \\left[ g( T(u,v,b)) \\mathbb{1} \\{ b=0 \\} + g( T(u,v,b)) \\mathbb{1} \\{ b=1 \\} \\right] \\\\\n=  \\mathbb{E} \\left[ g(z) \\mathbb{1} \\{ z \\in C_k \\} + g( z) \\mathbb{1} \\{ z \\notin C_k \\} \\right] =  \\mathbb{E} \\left[ g(z) \\right] \n$$\n\nWriting the estimator as a conditional expectation of some statistic of the random variables under the distribution is sufficient to show that this is an instance of Rao-Blackwellization. To be safe, the authors also apply the [conditional variance decomposition][eve] to reinforce the property that RB estimators always have lower variance:\n\n$$\nVar(Y) = E\\left[ Var (Y|X) \\right] + Var(E \\left[ Y | X \\right] ) \\\\\nVar(g (z) ) = Var (\\mathbb{E} \\left[ g( T(u,v,b) ) | v \\right] ) + E \\left[ Var ( g( T(u,v,b) ) | v ) \\right] \\\\\nVar (\\mathbb{E} \\left[ g( T(u,v,b) ) | v \\right] ) = Var (\\hat{g} (v) ) = Var(g (z) ) -  E \\left[ Var ( g( T(u,v,b) ) | v ) \\right]\n$$\n\nThey go on to show that the variance is less than or equal to $Var(g(z)) \\sum_{j \\notin C_k} q_\\eta (j)$.\n\nFinally, they note that the variance of a simple estimator can also be reduced by taking multiple samples and averaging. They then provide an equation to calculate the optimal $k$ number of elements of $z$ to evaluate depending on how concentrated the distribution being evaluated is, and a proof showing that this will have a lower variance than the naive estimator.\n\n$$\n\\hat{k} = \\underset{k \\in {0, ..., N}}{\\operatorname{argmin}} \\frac{\\sum_{j \\notin C_k} q_\\eta (j)}{N-k}\n$$\n\nI'm not very interested in the experiments right now, but skimming through them it's interesting to see that this method performs very well on a high dimensional hard attention task on MNIST. Particularly because a Gumbel-softmax estimator falls apart in the same experiment. It would be nice to see results on RL problems as were shown in the [RELAX][] paper.\n\n[eve]: https://en.wikipedia.org/wiki/Law_of_total_variance\n[relax]: https://arxiv.org/abs/1711.00123",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.04777"
    },
    "352": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.09508",
        "transcript": "This paper directly relies on understanding [Gumbel-Softmax][gs] ([Concrete][]) sampling.\n\nThe place to start thinking about this paper is in terms of a distribution over possible permutations. You could, for example, have a categorical distribution over all the permutation matrices of a given size. Of size 2 this is easy:\n\n```\np(M)   0.1    0.9\n\nM:     1, 0   0, 1\n       0, 1   1, 0\n```\n\nYou could apply the Gumbel-Softmax trick to this, and other selections of permutation matrices in small dimensions, but the number of possible permutations grows factorially with the dimension. If you want to infer the order of $100$ items, you now have a categorical over $100!$ variables, and you can't even store that number in floating point.\n\nBy breaking up and applying a normalised weight to each permutation matrix, we are effectively doing a [Naive Birkhoff-Von Neumann decomposition][naive] to return a sample from the space of Doubly Stochastic matrices. This paper proposes *much* more efficient ways to sample from this space in a way that is amenable to stochastic variational (read *SGD*) methods, such that they have an experiment working with permutations of 278 variables.\n\nThere are two ingredients to a good stochastic variational recipe:\n\n1. A differentiable sampling method; for example (most famously): $x = \\mu + \\sigma \\epsilon \\text{ where } \\epsilon \\sim \\mathcal{N}(0,1)$\n2. A density over this sampling distribution; in the preceding example: $\\mathcal{N}(\\mu, \\sigma)$\n\nThis paper presents two recipes: Stick-breaking transformations and Rounding towards permutation matrices.\n\nStick-breaking\n--------------------\n\n[Shakir Mohamed wrote a good blog post on stick-breaking methods.][sticks]\n\nOne problem, which you could guess from the categorical-over-permutations example above, is we can't possibly store a probability for each permutation matrix ($N!$ is a lot of numbers to store). Stick-breaking gives us another way to represent these probabilities implicitly through $B \\in [0,1]^{(N-1)\\times (N-1)}$. The row `x` gets recursively broken up like this:\n\n```\n    B_11    B_12*(1-x[:1].sum())     1-x[:2].sum()\nx: |-------|------------------------|---------------------|\n    x[0]    x[1]                     x[2]\n    <---------------------------------------------------->\n                               1.0\n```\n\nFor two dimensions, this becomes a little more complicated (and is actually a novel part of this paper) so I'll just refer you to the paper and say: *this is also possible in two dimensions*.\n\nOK, so now to sample from the distribution of Doubly Stochastic matrices, you just need to sample $(N-1) \\times (N-1)$ values in the range $[0,1]$. The authors sample a Gaussian and pass it through a sigmoid. Along with a temperature parameter, the values get pushed closer to 0 or 1 and the result is a permutation matrix.\n\nTo get the density, the authors *appear to* (this would probably be annoying in high dimensions) automatically differentiate through the $N^2$ steps of the stick-breaking transformation to get the Jacobian and use change of variables.\n\nRounding\n-------------\n\nThis method is more idiosyncratic, so I'll just copy the steps straight from the paper:\n\n> 1. Input $Z \\in R^{N \\times N} $, $M \\in R_+^{N \\times N}$, and $V \\in  R_+^{N \\times N}$;\n> 2. Map $M \\to \\tilde{M}$, a point in the Birkhoff polytope, using the [Sinkhorn-Knopp algorithm][sk];\n> 3. Set $\\Psi = \\tilde{M} + V \\odot Z$ where $\\odot$ denotes elementwise multiplication;\n> 4. Find $\\text{round}(\\Psi)$, the nearest permutation matrix to $\\Psi$, using the [Hungarian algorithm][ha];\n> 5. Output $X = \\tau \\Psi + (1- \\tau)\\text{round}(\\Psi)$.\n\nSo you can just schedule $\\tau \\to 0$ and you'll be moving your distribution to be a distribution over permutation matrices.\n\nThe big problem with this is we *can't easily get the density*. Step 4 is not differentiable. However, the authors argue the function is still piecewise-linear so we can just get around this. Once they've done that, it's possible to evaluate the density by change of variables again.\n\nResults\n----------\n\nOn a synthetic permutation matching problem, the rounding method gets a better match to the true posterior (the synthetic problem is small enough to enumerate the true posterior). It also performs better than competing methods on a real matching problem; matching the activations in neurons of C. Elegans to the location of neurons in the known connectome.\n\n[sticks]: http://blog.shakirm.com/2015/12/machine-learning-trick-of-the-day-6-tricks-with-sticks/\n[naive]: https://en.wikipedia.org/wiki/Doubly_stochastic_matrix#Birkhoff_polytope_and_Birkhoff.E2.80.93von_Neumann_theorem\n[gs]: http://www.shortscience.org/paper?bibtexKey=journals/corr/JangGP16\n[concrete]: http://www.shortscience.org/paper?bibtexKey=journals/corr/1611.00712\n[sk]: https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem#Sinkhorn-Knopp_algorithm\n[ha]: https://en.wikipedia.org/wiki/Hungarian_algorithm",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1710.09508"
    },
    "353": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.02907",
        "transcript": "The propagation rule used in this paper is the following:\n\n$$\nH^l = \\sigma \\left(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{l-1} W^l \\right)\n$$\n\nWhere $\\tilde{A}$ is the [adjacency matrix][adj] of the undirected graph (with self connections, so has a diagonal of 1s and is symmetric) and $H^l$ are the hidden activations at layer $l$. The $D$ matrices are performing row normalisation, $\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}$ is [equivalent to][pygcn] (with $\\tilde{A}$ as `mx`):\n\n```\nrowsum = np.array(mx.sum(1))            # sum along rows\nr_inv = np.power(rowsum, -1).flatten()  # 1./rowsum elementwise\nr_mat_inv = sp.diags(r_inv)             # cast to sparse diagonal matrix\nmx = r_mat_inv.dot(mx)                  # sparse matrix multiply\n```\n\nThe symmetric way to express this is part of the [symmetric normalized Laplacian][laplace]. This work, and the [other][hammond] [work][defferrard] on graph convolutional networks, is motivated by convolving a parametric filter over $x$. Convolution becomes easy if we can perform a *graph Fourier transform* (don't worry I don't understand this either), but that requires us having access to eigenvectors of the normalized graph Laplacian (which is expensive).\n\n[Hammond's early paper][hammond] suggested getting around this problem by using Chebyshev polynomials for the approximation. This paper takes the approximation even further, using only *first order* Chebyshev polynomial, on the assumption that this will be fine because the modeling capacity can be picked up by the deep neural network.\n\nThat's how the propagation rule above is derived, but we don't really need to remember the details. In practice $\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} = \\hat{A}$ is calculated prior and using a graph convolutional network involves multiplying your activations by this sparse matrix at every hidden layer. If you're thinking in terms of a batch with $N$ examples and $D$ features, this multiplication happens *over the examples*, mixing datapoints together (according to the graph structure). If you want to think of this in an orthodox deep learning way, it's the following:\n\n```\nactivations = F.linear(H_lm1, W_l)         # (N,D)\nactivations = activations.permute(1,0)     # (D,N)\nactivations = F.linear(activations, hat_A) # (D,N)\nactivations = activations.permute(1,0)     # (N,D)\nH_l = F.relu(activations)\n```\n\n**Won't this be really slow though, $\\hat{A}$ is $(N,N)$!** Yes, if you implemented it that way it would be very slow. But, many deep learning frameworks support sparse matrix operations ([although maybe not the backward pass][sparse]). Using that, a graph convolutional layer can be implemented [quite easily][pygcnlayer].\n\n**Wait a second, these are batches, not minibatches?** Yup, minibatches are future work.\n\n**What are the experimental results, though?** There are experiments showing this works well for semi-supervised experiments on graphs, as advertised. Also, the many approximations to get the propagation rule at the top are justified by experiment.\n\n**This summary is bad.** Fine, smarter people have written their own posts: [the author's][kipf], [Ferenc's][ferenc].\n\n[adj]: https://en.wikipedia.org/wiki/Adjacency_matrix\n[pygcn]: https://github.com/tkipf/pygcn/blob/master/pygcn/utils.py#L56-L63\n[laplace]: https://en.wikipedia.org/wiki/Laplacian_matrix#Symmetric_normalized_Laplacian\n[hammond]: https://arxiv.org/abs/0912.3848\n[defferrard]: https://arxiv.org/abs/1606.09375\n[sparse]: https://discuss.pytorch.org/t/does-pytorch-support-autograd-on-sparse-matrix/6156/7\n[pygcnlayer]: https://github.com/tkipf/pygcn/blob/master/pygcn/layers.py#L35-L68\n[kipf]: https://tkipf.github.io/graph-convolutional-networks/\n[ferenc]: http://www.inference.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1609.02907"
    },
    "354": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/BalduzziFLLMM17",
        "transcript": "Imagine you make a neural network mapping a scalar to a scalar. After you initialise this network in the traditional way, randomly with some given variance, you could take the gradient of the input with respect to the output for all reasonable values (between about - and 3 because networks typically assume standardised inputs). As the value increases, different rectified linear units in the network will randomly switch on, drawing a random walk in the gradients; another name for which is brown noise.\n\n![](http://i.imgur.com/KMzfzMZ.png)\n\nHowever, do the same thing for deep networks, and any traditional initialisation you choose, and you'll see the random walk start to look like white noise. One intuition given in the paper is that as different rectifiers in the network switch off and on the input is taking a number of different paths though the network. The number of possible paths grows exponentially with the depth of the network, so as the input varies, the gradients become increasingly chaotic. **The explanations and derivations given in the paper are much better reasoned and thorough, please read those if you are interested**.\n\nWhy should we care about this? Because the authors take the recent nonlinearity [CReLU][] (output is concatenation of `relu(x)` and `relu(-x)`) and develop an initialisation that will avoid problems with gradient shattering. The initialisation is just to take your standard initialised weight matrix $\\mathbf{W}$ and set the right half to be the negative of the left half ($\\mathbf{W}_{\\text{left}}$). As long as the input to the layer is also concatenated, the left half will be multiplied by `relu(x)` and the right by `relu(-x)`. Then:\n\n$$\n\\mathbf{W}.\\text{CReLU}(\\mathbf{x}) = \\begin{cases} \\mathbf{W}_{\\text{left}}\\mathbf{x} & \\text{ if } x > 0 \\\\ \\mathbf{W}_{\\text{left}}\\mathbf{x} & \\text{ if } x \\leq 0\\end{cases}\n$$\n\nDoing this allows them to train deep networks without skip connections, and they show results on CIFAR-10 with depths of up to 200 exceeding (slightly) a similar resnet.\n\n[crelu]: https://arxiv.org/abs/1603.05201",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v70/balduzzi17b.html"
    },
    "355": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.08591",
        "transcript": "Imagine you make a neural network mapping a scalar to a scalar. After you initialise this network in the traditional way, randomly with some given variance, you could take the gradient of the input with respect to the output for all reasonable values (between about -3 and 3 because networks typically assume standardised inputs). As the value increases, different rectified linear units in the network will randomly switch on, drawing a random walk in the gradients; another name for which is brown noise.\n\n![](http://i.imgur.com/KMzfzMZ.png)\n\nHowever, do the same thing for deep networks, and any traditional initialisation you choose, and you'll see the random walk start to look like white noise. One intuition given in the paper is that as different rectifiers in the network switch off and on the input is taking a number of different paths though the network. The number of possible paths grows exponentially with the depth of the network, so as the input varies, the gradients become increasingly chaotic. **The explanations and derivations given in the paper are much better reasoned and thorough, please read those if you are interested**.\n\nWhy should we care about this? Because the authors take the recent nonlinearity [CReLU][] (output is concatenation of `relu(x)` and `relu(-x)`) and develop an initialisation that will avoid problems with gradient shattering. The initialisation is just to take your standard initialised weight matrix $\\mathbf{W}$ and set the right half to be the negative of the left half ($\\mathbf{W}_{\\text{left}}$). As long as the input to the layer is also concatenated, the left half will be multiplied by `relu(x)` and the right by `relu(-x)`. Then:\n\n$$\n\\mathbf{W}.\\text{CReLU}(\\mathbf{x}) = \\begin{cases} \\mathbf{W}_{\\text{left}}\\mathbf{x} & \\text{ if } x > 0 \\\\ \\mathbf{W}_{\\text{left}}\\mathbf{x} & \\text{ if } x \\leq 0\\end{cases}\n$$\n\nDoing this allows them to train deep networks without skip connections, and they show results on CIFAR-10 with depths of up to 200 exceeding (slightly) a similar resnet.\n\n[crelu]: https://arxiv.org/abs/1603.05201",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1702.08591"
    },
    "356": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1705.08665",
        "transcript": "This paper described an algorithm of parametrically adding noise and applying a variational regulariser similar to that in [\"Variational Dropout Sparsifies Deep Neural Networks\"][vardrop]. Both have the same goal: make neural networks more efficient by removing parameters (and therefore the computation applied with those parameters). Although, this paper also has the goal of giving a prescription of how many bits to store each parameter with as well.\n\nThere is a very nice derivation of the hierarchical variational approximation being used here, which I won't try to replicate here. In practice, the difference to prior work is that the stochastic gradient variational method uses hierarchical samples; ie it samples from a prior, then incorporates these samples when sampling over the weights (both applied through local reparameterization tricks). It's a powerful method, which allows them to test two different priors (although they are clearly not limited to just these), and compare both against competing methods. They are comparable, and the choice of prior offers some tradeoffs in terms of sparsity versus quantization.\n\n[vardrop]: http://www.shortscience.org/paper?bibtexKey=journals/corr/1701.05369",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1705.08665"
    },
    "357": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1701.05369",
        "transcript": "The authors introduce their contribution as an alternative way to approximate the KL divergence between prior and variational posterior used in [Variational Dropout and the Local Reparameterization Trick][kingma] which allows unbounded variance on the multiplicative noise. When the noise variance parameter associated with a weight tends to infinity you can say that the weight is effectively being removed, and in their implementation this is what they do.\n\nThere are some important details differing from the [original algorithm][kingma] on per-weight variational dropout. For both methods we have the following initialization for each dense layer:\n\n```\ntheta = initialize weight matrix with shape (number of input units, number of hidden units)\nlog_alpha = initialize zero matrix with shape (number of input units, number of hidden units)\nb = biases initialized to zero with length the number of hidden units\n```\n\nWhere `log_alpha` is going to parameterise the variational posterior variance.\n\nIn the original paper the algorithm was the following:\n\n```\nmean = dot(input, theta) + b # standard dense layer\n# marginal variance over activations (eq. 10 in [original paper][kingma])\nvariance = dot(input^2, theta^2 * exp(log_alpha)) \n# sample from marginal distribution by scaling Normal \nactivations = mean + sqrt(variance)*unit_normal(number of output units) \n```\n\nThe final step is a standard [reparameterization trick][shakir], but since it is a marginal distribution this is referred to as a local reparameterization trick (directly inspired by the [fast dropout paper][fast]).\n\nThe sparsifying algorithm starts with an alternative parameterisation for `log_alpha`\n\n```\nlog_sigma2 = matrix filled with negative constant (default -8) with size (number of input units, number of hidden units)\nlog_alpha = log_sigma2 - log(theta^2)\nlog_alpha = log_alpha clipped between 8 and -8\n```\n\nThe authors discuss this in section 4.1, the $\\sigma_{ij}^2$ term corresponds to an additive noise variance on each weight with $\\sigma_{ij}^2 = \\alpha_{ij}\\theta_{ij}^2$. Since this can then be reversed to define `log_alpha` the forward pass remains unchanged, but the variance of the gradient is reduced. It is quite a counter-intuitive trick, so much so I can't quite believe it works.\n\nThey then define a mask removing contributions to units where the noise variance has gone too high:\n\n```\nclip_mask = matrix shape of log_alpha, equals 1 if log_alpha is greater than thresh (default 3)\n```\n\nThe clip mask is used to set elements of `theta` to zero, and then the forward pass is exactly the same as in the original paper.\n\nThe difference in the approximation to the KL divergence is illustrated in figure 1 of the paper; the sparsifying version tends to zero as the variance increases, which matches the true KL divergence. In the [original paper][kingma] the KL divergence would explode, forcing them to clip the variances at a certain point.\n\n[kingma]: https://arxiv.org/abs/1506.02557\n[shakir]: http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/\n[fast]: http://proceedings.mlr.press/v28/wang13a.html",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1701.05369"
    },
    "358": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1612.01936",
        "transcript": "Normally, a Deep Convolutional Network (DCN) has a conditional probabilistic model where we have some parameterised function $f_{\\theta}$, and some distribution over the targets to define the loss function (categorical for classification or Gaussian for regression). But, that conditional function is treated as a black box (probabilistically speaking, it's just fit by maximum likelihood). This paper breaks the entire network up into a number of latent factors.\n\nThe latent factors are designed to represent familiar parts of DCNs; for example, max-pooling selects from a set of activations and we can model the uncertainty in this selection with a categorical random variable. To recreate every pixel you can imagine selecting a set of paths backwards through the network to reproduce that pixel activation. That's not how their model is parameterised, all I can do is point you to the paper for the real DRMM model definition.\n\n# Inference\n\nThe big trick of this paper is that they have designed the probabilistic model so that max-sum-product message passing (also introduced in this paper) inference equates to the forward prop in a DCN. What does that mean? Well, since the network structure is now a hierarchical probabilistic model we can hope to throw better learning algorithms at it, which is what they do.\n\n# Learning\n\nUsing this probabilistic formulation, you can define a loss function that includes a reconstruction loss generating the image from responsibilities you can estimate during the forward pass. Then, you can have reconstruction gradients _and_ gradients wrt your target loss, so you can train semi-supervised or unsupervised but still work with what is practically a DCN.\n\nIn addition, it's possible to derive a full EM algorithm in this case, and the M-step corresponds to just solving a generalised Least Squares problem. Which means gradient-free and better principled training of neural networks.\n\n# Experimental Results\n\nNot all of the theory presented in this paper is covered in the experiments. They do demonstrate their method working well on MNIST and CIFAR-10 semi-supervised (SOTA with additional variational tweaks). But, there are not yet any experiments using the full EM algorithm they describe (only reporting that results _appear promising_); all experiments use gradient optimisation. They report that their network will train 2-3x faster, but only demonstrate this on MNIST (and what about variation in the chosen optimizer?).\n\nAlso, the model can be sampled from, but we don't have any visualisations of the kind of images we would get.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1612.01936"
    },
    "359": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.00712",
        "transcript": "This paper presents a way to differentiate through discrete random variables by replacing them with continuous random variables. Say you have a discrete [categorical variable][cat] and you're sampling it with the [Gumbel trick][gumbel] like this ($G_k$ is a Gumbel distributed variable and $\\boldsymbol{\\alpha}/\\sum_k \\alpha_k$ are our categorical probabilities):\n\n$$\nz = \\text{one_hot} \\left( \\underset{k}{\\text{arg max}} [ G_k + \\log \\alpha_k ] \\right)\n$$\n\nThis paper replaces the one hot and argmax with a softmax, and they add a $\\lambda$ variable to control the \"temperature\". As $\\lambda$ tends to zero it will equal the above equation.\n\n$$\nz = \\text{softmax} \\left( \\frac{  G_k + \\log \\alpha_k }{\\lambda} \\right)\n$$\n\nI made [some notes][nb] on how this process works, if you'd like more intuition.\n\nComparison to [Gumbel-softmax][gs]\n--------------------------------------------\n\nThese papers are proposed precisely the same distribution with notation changes ([noted there][gs]). Both papers also reference each other and the differences. Although, they mention differences in the variatonal objectives to the Gumbel-softmax. This paper also compares to [VIMCO][], which is probably a harder benchmark to compare against (multi-sample versus single sample).\n\nThe results in both papers compare to SOTA score function based estimators and both report high scoring results (often the best). There are some details about implementations to consider though, such as scheduling and exactly how to define the variational objective.\n\n[cat]: https://en.wikipedia.org/wiki/Categorical_distribution\n[gumbel]: https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/\n[gs]: http://www.shortscience.org/paper?bibtexKey=journals/corr/JangGP16\n[nb]: https://gist.github.com/gngdb/ef1999ce3a8e0c5cc2ed35f488e19748\n[vimco]: https://arxiv.org/abs/1602.06725",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.00712"
    },
    "360": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/JangGP16",
        "transcript": "In [stochastic computation graphs][scg], like [variational autoencoders][vae], using discrete variables is hard because we can't just differentiate through Monte Carlo estimates. This paper introduces a distribution that is a smoothed version of the [categorical distribution][cat] and has a parameter that, as it goes to zero, will make it equal the categorical distribution. This distribution is continuous and can be reparameterised.\n\nIn other words, the Gumbel trick way to sample a categorical $z$ looks like this ($g_i$ is gumbel distributed and $\\boldsymbol{\\pi}/\\sum_j \\pi_j$ are the categorical probabilties):\n\n$$\nz = \\text{one_hot} \\left( \\underset{i}{\\text{arg max}} [ g_i + \\log \\pi_i ] \\right)\n$$\n\nThis paper replaces the one hot and argmax with a [softmax][], and they introduce $\\tau$ to control the \"discreteness\":\n\n$$\nz = \\text{softmax} \\left(  \\frac{ g_i + \\log \\pi_i}{\\tau} \\right)\n$$\n\nI made a [notebook that illustrates this][nb] while looking at another paper that came out at the same time, which I should probably compare against here.\n\nComparison with [Concrete Distribution][concrete]\n---------------------------------------------------------------\n\nThe concrete and Gumbel-softmax distributions are exactly the same (notation switch: $\\tau \\to \\lambda$, $\\pi_i \\to \\alpha_k$, $G_k \\to g_i$). Both papers have structured output prediction experiments (predict one half of MNIST digits from the other half). This paper shows Gumbel-softmax always being better, but doesn't compare to VIMCO, which is sometimes better at test time in the concrete distribution paper.\n\nSidenote - blog post\n----------------------------\n\nThe authors posted a [nice blog post][blog] that is also a good short summary and explanation.\n\n[blog]: http://blog.evjang.com/2016/11/tutorial-categorical-variational.html\n[scg]: https://arxiv.org/abs/1506.05254\n[vae]: https://arxiv.org/abs/1312.6114\n[cat]: https://en.wikipedia.org/wiki/Categorical_distribution\n[softmax]: https://en.wikipedia.org/wiki/Softmax_function\n[concrete]: http://www.shortscience.org/paper?bibtexKey=journals/corr/1611.00712\n[nb]: https://gist.github.com/gngdb/ef1999ce3a8e0c5cc2ed35f488e19748",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1611.01144"
    },
    "361": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.03383",
        "transcript": "When training a [VAE][] you will have an inference network $q(z|x)$. If you have another source of information you'd like to base the approximate posterior on, like some labels $s$, then you would make $q(z|x,s)$. But $q$ is a complicated function, and it can ignore $s$ if it wants, and still perform well. This paper describes an adversarial way to force $q$ to use $s$.\n\nThis is made more complicated in the paper, because $s$ is not necessarily a label, and in fact _is real and continuous_ (because it's easier to backprop in that case). In fact, we're going to _learn_ the representation of $s$, but force it to contain the label information using the training procedure. To be clear, with $x$ as our input (image or whatever):\n\n$$\ns  = f_{s}(x)\n$$\n\n$$\n\\mu, \\sigma = f_{z}(x,s)\n$$\n\nWe sample $z$ using $\\mu$ and $\\sigma$ [according to the reparameterization trick, as this is a VAE][vae]:\n\n$$\nz \\sim \\mathcal{N}(\\mu, \\sigma)\n$$\n\nAnd then we use our decoder to turn these latent variables into images:\n\n$$\n\\tilde{x} = \\text{Dec}(z,s)\n$$\n\nTraining Procedure\n--------------------------\n\nWe are going to create four parallel loss functions, and incorporate a discriminator to train this:\n\n1. Reconstruction loss plus variational regularizer; propagate a $x_1$ through the VAE to get $s_1$, $z_1$ (latent) and $\\tilde{x}_{1}$.\n2. Reconstruction loss with a different $s$:\n    1. Propagate $x_1'$, a different sample with the __same class__ as\n $x_1$\n    2. Pass $z_1$ and $s_1'$ to your decoder.\n    3. As $s_1'$ _should_ include the label information, you should have reproduced $x_1$, so apply reconstruction loss to whatever your decoder has given you (call it $\\tilde{x}_1'$).\n3. Adversarial Loss encouraging realistic examples from the same class, regardless of $z$.\n    1. Propagate $x_2$ (totally separate example) through the network to get $s_2$.\n    2. Generate two $\\tilde{x}_{2}$ variables, one with the prior by sampling from $p(z)$ and one using $z_{1}$.\n    3. Get the adversary to classify these as fake versus the real sample $x_{2}$.\n\nThis is pretty well described in Figure 1 in the paper.\n\nExperiments show that $s$ ends up coding for the class, and $z$ codes for other stuff, like the angle of digits or line thickness. They also try to classify using $z$ and $s$ and show that $s$ is useful but $z$ is not (can only predict as well as chance). So, it works.\n\n[vae]: https://jaan.io/unreasonable-confusion/",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.03383"
    },
    "362": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SalimansGZCRC16",
        "transcript": "This is heavily derived from the top voted summary above, but I had to review this paper for a lab meeting so I expanded on it a bit. I hope this doesn't offend anyone, but this should be, at worst, redundant to the summary above.\n\nThis paper has two parts: five recommended techniques for training GANs and\na thorough evaluation with visual Turing tests and semi-supervised tasks.\nThat is more concrete than the feature extraction and visualisation in, for\nexample, Radford's [DCGAN paper][dcgan].\n\n### Feature Matching\n\nProblem: instability from overtraining on the current discriminator.\nIntuition is that the discriminator will have learnt the kind of useful\nrepresentation we see in deep image models, and there is more information\navailable by matching those than the single classifier output.\n\nSolution: Match activations at some hidden with an L2 loss. This is\nthe same as the \"content\" loss in the [neural style paper][style]:\n\n$$\n\\newcommand{\\aB}{\\mathbf{a}}\n\\newcommand{\\bB}{\\mathbf{b}}\n\\newcommand{\\cB}{\\mathbf{c}}\n\\newcommand{\\dB}{\\mathbf{d}}\n\\newcommand{\\eB}{\\mathbf{e}}\n\\newcommand{\\fB}{\\mathbf{f}}\n\\newcommand{\\gB}{\\mathbf{g}}\n\\newcommand{\\hB}{\\mathbf{h}}\n\\newcommand{\\iB}{\\mathbf{i}}\n\\newcommand{\\jB}{\\mathbf{j}}\n\\newcommand{\\kB}{\\mathbf{k}}\n\\newcommand{\\lB}{\\mathbf{l}}\n\\newcommand{\\mB}{\\mathbf{m}}\n\\newcommand{\\nB}{\\mathbf{n}}\n\\newcommand{\\oB}{\\mathbf{o}}\n\\newcommand{\\pB}{\\mathbf{p}}\n\\newcommand{\\qB}{\\mathbf{q}}\n\\newcommand{\\rB}{\\mathbf{r}}\n\\newcommand{\\sB}{\\mathbf{s}}\n\\newcommand{\\tB}{\\mathbf{t}}\n\\newcommand{\\uB}{\\mathbf{u}}\n\\newcommand{\\vB}{\\mathbf{v}}\n\\newcommand{\\wB}{\\mathbf{w}}\n\\newcommand{\\xB}{\\mathbf{x}}\n\\newcommand{\\yB}{\\mathbf{y}}\n\\newcommand{\\zB}{\\mathbf{z}}\n\\newcommand{\\Exp}{\\mathbb{E}}\n|| \\Exp_{\\xB \\sim p_{\\text{data}}} \\fB (\\xB) - \\Exp_{\\zB \\sim p_{\\zB}(\\zB)} \\fB (G(\\zB)) ||_2^2\n$$\n\nWhere $\\fB (\\xB)$ and $\\fB (\\zB)$ are the activations in some hidden layer\ncorresponding to either a real or generated image.\n\n### Minibatch Discrimination\n\nProblem: generators like to collapse to a single mode (ie just generate a\nsingle image), because it's a decent local optimum.\n\nSolution: make sure the discriminator can look at samples in combination,\nso it will know if it's getting the same (or similar) images more easily.\nJust give the discriminator features that tell it about the distance of\neach image to other images in the same batch. The diagram in the paper\ndescribes this best.\n\nThey mention this tensor $T$ in the paper, but don't really explain what it\nis. In [the code][mbcode], it appears to be basically a weight matrix,\nwhich means that it is also learnt as part of the discriminator.\n\n### Historical Averaging\n\nProblem: no guarantee with gradient descent that a two player game like\nthis won't go into extended orbits.\n\nSolution: encourage parameters to revert to their historical mean, with an\nL2 penalty:\n\n$$\n\\newcommand{\\thetaB}{\\boldsymbol{\\theta}}\n|| \\thetaB - \\frac{1}{t} \\sum_{i=1}^t \\thetaB[i] ||^2\n$$\n\nOrbits are penalised by always being far from their mean, and this is\nsupposed to correspond to a \"fictitious play\" algorithm. I'm not sure if\nthat's true, but maybe?\n\n### One-sided label smoothing\n\nProblem: vulnerability of discriminator to adversarial examples? (Not\nexplicitly stated).\n\nSolution: replace positive (ie probability that a sample is real?) labels\nwith a target _smaller than 1_.\n\n### Virtual Batch Normalisation\n\nProblem: batch normalisation is highly variable, as it is based on\nstatistics of the current minibatch (enough so that you can sometimes avoid\nusing dropout if you're using batchnorm).\n\nSolution: for every minibatch, use the statistics gathered from a\n_reference minibatch_ for your batch normalisation. For every minibatch,\nyou'll have to first propagate through the reference minibatch with your\ncurrent parameter settings, but then you can use the statistics you gather\nby doing this for the minibatch you're actually going to use for training.\n\n_Interesting sidenote_: in the [code][impcode], they are actually using\n[weight normalization][weightnorm] instead of batchnorm (not in all cases).\nProbably because both papers have Tim Salimans as first author.\n\n### Assessing Image Quality\n\nProblem: noisy labelling from mechanical turk.\n\nSolution: aim for low entropy conditional categorical distribution when\nlabelling samples with Google's inception model. The inception model gives\nyou $p(y|\\xB)$, so you want to maximise:\n\n$$\n\\Exp_{\\xB} KL (p(y|\\xB)||p(y))\n$$\n\nThen they exponentiate the resulting value for no real reason, just to make\nvalues easier to compare. Since they say this matches human judgement in\ntheir experiments, this means we can all start using this measure and just\ncite this paper!\n\n### Semi-supervised Learning\n\nProblem: standard semi-supervised, we have some data that is labelled and\nsome that isn't, how to learn a conditional model that will give us\n$p(y|\\xB)$.\n\nSolution: make \"generated\" a class in your classification problem. Now you\ncan put generated samples into your dataset, but even better you can\nproduce a loss on unlabeled samples that you just _don't want them to be\nlabeled as \"generated\"_. So we end up with the following two losses for\nsupervised and unsupervised data:\n\n$$\nL_{\\text{supervised}} = - \\Exp_{\\xB,y \\sim p_{\\text{data}} (\\xB, y)} \\log p_{\\text{model}} (y | \\xB, y < K + 1)\n$$\n\n$$\nL_{\\text{unsupervised}} = - \\{ \\Exp_{\\xB \\sim p_{\\text{data}} (\\xB)} \\log [\n1- p_{\\text{model}} (y = K+1 | \\xB) ] + \\Exp_{\\xB \\sim G}\\log [\np_{\\text{model}} (y=K+1 | \\xB)] \\}\n$$\n\nWith this method, and using feature matching but _not minibatch\ndiscrimination_, they show SOTA results for semi-supervised learning on\nMNIST, SVHN and CIFAR-10.\n\n[mbcode]: https://github.com/openai/improved-gan/blob/master/mnist_svhn_cifar10/nn.py#L132-L170\n[impcode]: https://github.com/openai/improved-gan/blob/master/mnist_svhn_cifar10/nn.py#L45-L91\n[weightnorm]: https://arxiv.org/abs/1602.07868\n[short]: http://www.shortscience.org/paper?bibtexKey=journals/corr/SalimansGZCRC16#udibr\n[improved]: https://arxiv.org/abs/1606.03498\n[dcgan]: https://arxiv.org/abs/1511.06434\n[style]: https://arxiv.org/abs/1508.06576\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.03498"
    },
    "363": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MirzaO14",
        "transcript": "# Conditional Generative Adversarial Nets\n\n## Introduction\n\n* Conditional version of [Generative Adversarial Nets (GAN)](https://gist.github.com/shagunsodhani/1f9dc0444142be8bd8a7404a226880eb) where both generator and discriminator are conditioned on some data **y** (class label or data from some other modality).\n* [Link to the paper](https://arxiv.org/abs/1411.1784)\n\n## Architecture\n\n* Feed **y** into both the generator and discriminator as additional input layers such that **y** and input are combined in a joint hidden representation.\n\n## Experiment\n\n### Unimodal Setting \n\n* Conditioning MNIST images on class labels.\n* *z* (random noise) and **y** mapped to hidden layers with ReLu with layer sizes of 200 and 1000 respectively and are combined to obtain ReLu layer of dimensionality 1200.\n* Discriminator maps *x* (input) and **y** to maxout layers and the joint maxout layer is fed to sigmoid layer.\n* Results do not outperform the state-of-the-art results but do provide a proof-of-the-concept.\n\n### Multimodal Setting\n\n* Map images (from Flickr) to labels (or user tags) to obtain the one-to-many mapping.\n* Extract image and text features using convolutional and language model.\n* Generative Model\n    * Map noise and convolutional features to a single 200 dimensional representation.\n* Discriminator Model\n    * Combine the representation of word vectors (corresponding to tags) and images.\n\n## Future Work\n\n* While the results are not so good, they do show the potential of Conditional GANs, especially in the multimodal setting.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1411.1784"
    },
    "364": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/LuongSLVZ15",
        "transcript": "# Addressing the Rare Word Problem in Neural Machine Translation\n\n## Introduction\n\n* NMT(Neural Machine Translation) systems perform poorly with respect to OOV(out-of-vocabulary) words or rare words.\n* The paper presents a word-alignment based technique for translating such rare words.\n* [Link to the paper](https://arxiv.org/abs/1410.8206)\n\n## Technique\n\n* Annotate the training corpus with information about what do different OOV words (in the target sentence) correspond to in the source sentence.\n* NMT learns to track the alignment of rare words across source and target sentences and emits such alignments for the test sentences.\n* As a post-processing step, use a dictionary to map rare words from the source language to target language.\n\n## Annotating the Corpus\n\n### Copy Model\n\n* Annotate the OOV words in the source sentence with tokens *unk1*, *unk2*,..., etc such that repeated words get the same token.\n* In target language, each OOV word, that is aligned to some OOV word in the source language, is assigned the same token as the word in the source language. \n* The OOV word in the target language, which has no alignment or is aligned with a known word in the source language. is assigned the null token.\n* Pros\n    * Very straightforward\n* Cons\n    * Misses out on words which are not labelled as OOV in the source language.\n\n### PosAll - Positional All Model\n\n* All OOV words in the source language are assigned a single *unk* token.\n* All words in the target sentences are assigned positional tokens which denote that the *jth* word in the target sentence is aligned to the *ith* word in the source sentence.\n* Aligned words that are too far apart, or are unaligned, are assigned a null token.\n* Pros\n    * Captures complete alignment between source and target sentences.\n* Cons\n    * It doubles the length of target sentences.\n\n### PosUnk - Positional Unknown Model\n\n* All OOV words in the source language are assigned a single *unk* token.\n* All OOV words in the target sentences are assigned *unk* token with the position which gives the relative position of the word in the target language with respect to its aligned source word.\n* Pros:\n    * Faster than PosAll model.\n* Cons\n    * Does not capture alignment for all words.\n\n## Experiments\n\n* Dataset  \n    * Subset of WMT'14 dataset\n* Alignment computed using the [Berkeley Aligner](https://code.google.com/archive/p/berkeleyaligner/)\n* Used architecture from [Sequence to Sequence Learning with Neural Networks paper](https://gist.github.com/shagunsodhani/a2915921d7d0ac5cfd0e379025acfb9f).\n\n## Results\n\n* All the 3 approaches (more specifically the PosUnk approach) improve the performance of existing NMTs in the order PosUnk > PosAll > Copy.\n* Ensemble models benefit more than individual models as the ensemble of NMT models works better at aligning the OOV words.\n* Performance gains are more when using smaller vocabulary.\n* Rare word analysis shows that performance gains are more when proposition of OOV words is higher.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P15/P15-1002.pdf"
    },
    "365": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LuongM16",
        "transcript": "# Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models\n\n## Introduction\n\n* The paper presents a novel open vocabulary NMT(Neural Machine Translation) system that translates mostly at word level and falls back to character level models for rare words.\n* Advantages:\n    * Faster and easier to train as compared to character models.\n    * Does not produce unknown words in the translations which need to be removed using *unk replacement* techniques.\n* [Link to the paper](https://arxiv.org/abs/1604.00788)\n\n## Unk Replacement Technique\n\n* Most NMT operate on constrained vocabulary and represent unknown words with *unk* token.\n* A post-processing step replaces *unk* tokens with actual words using alignment information.\n* Disadvantages:\n    * These systems treat words as independent entities while they are morphologically related.\n    * Difficult to capture things like name translation.\n\n## Proposed Architecture\n\n### Word-level NMT\n\n* Deep LSTM encoder-decoder.\n* Global attention mechanism and bilinear attention scoring function.\n* Similar to regular NMT system except in the way unknown words are handled.\n\n### Character-level NMT\n\n* Deep LSTM model used to generate on-the-fly representation of rare words (using final hidden state from the top layer).\n* Advantages:\n    * Simplified architecture.\n    * Efficiency through precomputation - representations for rare sources words can be computed at once before each mini-batch.\n    * The model can be trained easily in an end-to-end fashion.\n\n#### Hidden-state Initialization\n\n* For source representation, layers of the LSTM are initialized with zero hidden states and cell values.\n* For target representation, the same strategy is followed except for the hidden state of the first layer where one of the following approaches are used: \n    * **same-path** target generation approach\n        * Use the context vector just before softmax (of word-level NMT).\n    * **seperate-path** target generation approach\n        * Learn a new weight matrix **W** that will be used to generate the context vector.\n\n### Training Objective\n\n* *J = J<sub>w</sub> + \u03b1J<sub>c</sub>*\n* *J* - total loss\n* *J<sub>w</sub>* - loss in a regular word-level NMT\n* *\u03b1J<sub>c</sub>* - loss in the character-level NMT\n\n### Word Character Generation Strategy\n\n* The final hidden state from character-level decoder could be interpreted as the representation of *unk* token but this approach would not be efficient.\n* Instead, *unk* is fed to the word-level decoder as it is so as to decouple the execution for the character-level model as soon the word-level model finishes.\n* During testing, a beam search decoder is run at the word level to find the best translation using the word NMT alone. \n* Next, a character-level encoder is used to generate the words in place of *unk* to minimise the combined loss.\n\n## Experiments\n\n### Data\n\n* WMT\u201915 translation task from English into Czech with newstest2013 (3000 sentences) as dev set and newstest2015 (2656 sentences) as a test set. \n\n### Metrics\n\n* Case-sensitive NIST BLEU.\n* chrF3\n\n### Models\n\n* Purely word based \n* Purely character based\n* Hybrid (proposed model)\n\n### Observations\n\n* Hybrid model surpasses all the other systems (neural/non-neural) and establishes a new state-of-the-art result for English-Czech translation in WMT\u201915 with 19.9 BLEU.\n* Character-level models, when used as a replacement for the standard unk replacement technique in NMT, yields an improvement of up to +7.9 BLEU points.\n* Attention is very important for character-based models as the non-attentional character models perform poorly.\n* Character models with shorter time-step backpropagation perform inferior as compared to ones with longer backpropagation.\n* Separate-path strategy outperforms same-path strategy.\n\n### Rare word embeddings\n\n* Obtain representations for rare words.\n* Compare the Spearman correlation between similarity scores assigned by humans and by the model.\n* Outperforms the recursive neural network model (which also uses a morphological analyser) on this task.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.00788"
    },
    "366": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/HuangSMN12",
        "transcript": "# Improving Word Representations via Global Context and Multiple Word Prototypes\n\n## Introduction\n\n* This paper pre-dated papers like Glove and Word2Vec and proposed an architecture that\n    * combines local and global context while learning word embeddings to capture the word semantics.\n    * learns multiple embeddings per word to account for homonymy and polysemy.\n* [Link to the paper](http://www.aclweb.org/anthology/P12-1092)\n\n## Global Context-Aware Neural Language Model\n\n### Training Objective\n\n* Given a word sequence *s* (local context) and a document *d* in which the sequence occurs (global context), learn word representations while learning to discriminate the last correct word in *s* from other words.\n* *g(s, d)* - scoring function giving liklihood of correct sequence.\n* *g(s<sup>w</sup>, d)* - scoring function giving liklihood of last word in *s* repalced by a word *w*.\n* Objective - *g(s, d)* > *g(s<sup>w</sup>, d)* + 1 for any other word *w*.\n\n### Architecture\n\n* Two scoring components (neural networks) to capture:\n    \n    * Local Context\n        * Map word sequence *s* into an ordered list of vectors *x = [x<sub>1</sub>, ..., x<sub>m</sub>]*.\n        * *x<sub>i</sub>* - embedding corresponding to *i<sup>th</sup>* word in the sequence.\n        * Compute local score *score<sub>l</sub>* by using a neural network (with one hidden layer) over *x*.\n        * Preserves word order and syntactic information.\n    * Global Context\n        * Map document *d* to an ordered list of word embeddings, *d = (d<sub>1</sub>, ..., d<sub>k</sub>)*.\n        * Compute *c*, the weighted average of all word vectors in document.\n        * The paper uses *idf* score for weighting the documents.\n        * *x = * concatenation of *c* and vector of the last word in *s*.\n        * Compute global score *score<sub>g</sub>* by using a neural network (with two hidden layers) over *x*.\n        * Similar to bag-of-words features.\n    *score = score<sub>l</sub> + score<sub>g</sub>*\n    * Train the weights of the hidden layers and the word embeddings.\n\n### Multi-Prototype Neural Language Model\n\n* Words can have different meanings in different contexts which are difficult to capture when we train only one vector per word.\n* Solution - train multiple vectors per word to capture the different meanings.\n* Approach\n    \n    * Gather all the fixed-sized context windows for all occurrences of a given word.\n    * Find the context vector by performing weighted averaging of all the words in the context window.\n    * Cluster the context vectors using spherical k-means.\n    * Each word occurrence in the corpus is re-labeled to its associated cluster.\n    * To find similarity between a pair of words *(w, w')*:\n        * For each possible cluster of *i* and *j* corresponding to the words *w* and *w'*, find distance between cluster centers for *i* and *j* and weight them by the product of probabilities of *w* belonging to *i* and *w'* belonging to *j* given their respective contexts.\n        * Average the value over the *k<sup>2</sup>* pairs.\n\n## Training\n    \n* Dataset\n  * Wikipedia corpus\n\n* Parameters\n  * 10-word windows\n  * 100 hidden units\n  * No weight regularization\n  * 10 different word embeddings learnt for words having multiple meanings.\n\n## Evaluation\n\n* Dataset\n  * WordSim-353 \n      * 353 pairs of nouns\n      * words represented without context\n      * contains human similarity judgements on pair of words\n  * The paper contributed a new dataset\n      * captures human similarity judgements on pair of words in the context of a sentence\n      * consists of verbs and adjectives along with nouns\n      * for details on how the dataset is constructed, refer the paper\n\n* Performance\n  * Proposed model achieves higher correlation to human scores than models using only the local or global context.\n  * Performance can be improved by removing the stop words.\n  * Using multi-prototype approach (multiple vectors for the same word) benefits the model on the tasks where the context is also given.\n\n## Comments\n\n* This work predated the more general word embedding models like [Word2Vec](https://gist.github.com/shagunsodhani/176a283e2c158a75a0a6) and [Glove](https://gist.github.com/shagunsodhani/efea5a42d17e0fcf18374df8e3e4b3e8). While this model performs good at intrinsic evaluation tasks like word similarity, it is outperformed by the more general and recent models on downstream tasks like NER. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aclweb.org/anthology/P12-1092"
    },
    "367": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/KirosZSZUTF15",
        "transcript": "# Skip-Thought Vectors\n\n## Introduction\n\n* The paper describes an unsupervised approach to train a generic, distributed sentence encoder.\n* It also describes a vocabulary expansion method to encode words not seen at training time.\n* [Link to the paper](https://arxiv.org/abs/1506.06726)\n\n## Skip-Thoughts\n\n* Train an encoder-decoder model where the encoder maps the input sentence to a sentence vector and the decoder generates the sentences surrounding the original sentence.\n* The model is called **skip-thoughts** and the encoded vectors are called **skip-thought vectors.**\n* Similar to the [skip-gram](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) model in the sense that surrounding sentences are used to learn sentence vectors.\n\n### Architecture\n\n* Training data is in form of sentence tuples (previous sentence, current sentence, next sentence).\n* **Encoder**\n    * RNN Encoder with GRU.\n* **Decoder**\n    * RNN Decoder with conditional GRU.\n    * Conditioned on encoder output.\n    * Extra matrices introduced to bias the update gate, reset gate and hidden state, given the encoder output.\n    * **Vocabulary matrix (V)** - Weight matrix having one row (vector) for each word in the vocabulary.\n    * Separate decoders for the previous and next sentence which share only **V**.\n    * Given the decoder context **h** (at any time), encoder output, and list of words already generated for the output sentence, the probability of choosing *w* as the next word is proportional to *exp(**V(*word*)h**)*\n* **Objective**\n    * Sum of the log-probabilities for the forward and backwards sentences conditioned on the encoder output.\n\n## Vocabulary Expansion\n\n* Use a model like Word2Vec which can be trained to induce word representations and train it to obtain embeddings for all the words that are likely to be seen by the encoder.\n* Learn a matrix **W** such that *encoder(word) = cross-product(W, Word2Vec(word))* for all words that are common to both Word2Vec model and encoder model.\n* Use **W** to generate embeddings for words are not seen during encoder training.\n\n## Dataset\n\n* [BookCorpus dataset](https://arxiv.org/abs/1506.06724) having books across 16 genres.\n\n## Training\n\n* **uni-skip**\n    * Unidirectional auto-encoder with 2400 dimensions.\n* **bi-skip**\n    * Bidirectional model with forward (sentence given in correct order) and backward (sentence given in reverse order) encoders of 1200 dimensions each.\n* **combine-skip**\n    * concatenation of uni-skip and bi-skip vectors. \n* Initialization\n    * Recurrent matricies - orthogonal initialization.\n    * Non-recurrent matricies - uniform distribution in [-0.1,0.1]. \n* Mini-batches of size 128.\n* Gradient Clipping at norm = 10.\n* Adam optimizer.\n\n## Experiments\n\n* After learning skip-thoughts, freeze the model and use the encoder as feature extractor only.\n* Evaluated the vectors with linear models on following tasks:\n\n### Semantic Relatedness\n\n* Given a sentence pair, predict how closely related the two sentences are.\n* **skip-thoughts** method outperforms all systems from SemEval 2014 competition and is outperformed only by dependency tree-LSTMs.\n* Using features learned from image-sentence embedding model on COCO boosts performance and brings it at par with dependency tree-LSTMs.\n\n### Paraphrase detection\n\n* **skip-thoughts** outperforms recursive nets with dynamic pooling if no hand-crafted features are used.\n* **skip-thoughts** with basic pairwise statistics produce results comparable with the state-of-the-art systems that house complicated features and hand engineering.\n\n### Image-sentence Ranking\n\n* MS COCO dataset\n* Task\n    * Image annotation\n        * Given an image, rank the sentences on basis of how well they describe the image.\n    * Image search - Given a caption, find the image that is being described.\n* Though the system does not outperform baseline system in all cases, the results does indicate that skip-thought vectors can capture image descriptions without having to learn their representations from scratch.\n\n\n### Classification\n\n* **skip-thoughts** perform about as good as bag-of-words baselines but are outperformed by methods where sentence representation has been learnt for the task at hand.\n* Combining **skip-thoughts** with bi-gram Naive Bayes (NB) features improves the performance.\n\n## Future Work\n\n* Variants to be explored include:\n    * Fine tuning the encoder-decoder model during the downstream task instead of freezing the weights.\n    * Deep encoders and decoders.\n    * Larger context windows.\n    * Encoding and decoding paragraphs.\n    * Encoders, such as convnets.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5950-skip-thought-vectors"
    },
    "368": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.06434",
        "transcript": "# Deep Convolutional Generative Adversarial Nets\n\n## Introduction\n\n* The paper presents Deep Convolutional Generative Adversarial Nets (DCGAN) - a topologically constrained variant of conditional GAN.\n* [Link to the paper](https://arxiv.org/abs/1511.06434)\n\n## Benefits\n\n* Stable to train\n* Very useful to learn unsupervised image representations.\n\n## Model\n\n* GANs difficult to scale using CNNs.\n* Paper proposes following changes to GANs:\n    * Replace any pooling layers with strided convolutions (for discriminator) and fractional strided convolutions (for generators).\n    * Remove fully connected hidden layers.\n    * Use batch normalisation in both generator (all layers except output layer) and discriminator (all layers except input layer).\n    * Use LeakyReLU in all layers of the discriminator.\n    * Use ReLU activation in all layers of the generator (except output layer which uses Tanh).\n\n## Datasets\n    \n* Large-Scale Scene Understanding.\n* Imagenet-1K.\n* Faces dataset.\n\n## Hyperparameters\n\n* Minibatch SGD with minibatch size of 128.\n* Weights initialized with 0 centered Normal distribution with standard deviation = 0.02\n* Adam    Optimizer\n* Slope of leak = 0.2 for LeakyReLU.\n* Learning rate = 0.0002, \u03b21 = 0.5\n\n## Observations\n\n* Large-Scale Scene Understanding data\n    * Demonstrates that model scales with more data and higher resolution generation.\n    * Even though it is unlikely that model would have memorized images (due to low learning rate of minibatch SGD).\n* Classifying CIFAR-10 dataset\n    * Features\n        * Train in Imagenet-1K and test on CIFAR-10.\n        * Max pool discriminator's convolutional features (from all layers) to get 4x4 spatial grids.\n        * Flatten and concatenate to get a 28672-dimensional vector.\n        * Linear L2-SVM classifier trained over the feature vector.\n    * 82.8% accuracy, outperforms K-means (80.6%)\n* Street View House Number Classifier\n    * Similar pipeline as CIFAR-10\n    * 22.48% test error.\n* The paper contains many examples of images generated by final and intermediate layers of the network.\n* Images in the latent space do not show sharp transitions indicating that network did not memorize images.\n* DCGAN can learn an interesting hierarchy of features.\n* Networks seems to have some success in disentangling image representation from object representation.\n* Vector arithmetic can be performed on the Z vectors corresponding to the face samples to get results like `smiling woman - normal woman + normal man = smiling man` visually.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.06434"
    },
    "369": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/GoodfellowPMXWOCB14",
        "transcript": "# Generative Adversarial Nets\n\n## Introduction\n\n* The paper proposes an adversarial approach for estimating generative models where one model (generative model) tries to learn a data distribution and another model (discriminative model) tries to distinguish between samples from the generative model and original data distribution.\n* [Link to the paper](https://arxiv.org/abs/1406.2661)\n\n## Adversarial Net\n\n* Two models - Generative Model(*G*) and Discriminative Model(*D*)\n* Both are multi-layer perceptrons.\n* *G* takes as input a noise variable *z* and outputs data sample *x(=G(z))*.\n* *D* takes as input a data sample *x* and predicts whether it came from true data or from *G*.\n* *G* tries to minimise *log(1-D(G(z)))* while *D* tries to maximise the probability of correct classification.\n* Think of it as a minimax game between 2 players and the global optimum would be when *G* generates perfect samples and *D* can not distinguish between the samples (thereby always returning 0.5 as the probability of sample coming from true data).\n* Alternate between *k* steps of training *D* and 1 step of training *G* so that *D* is maintained near its optimal solution.\n* When starting training, the loss *log(1-D(G(z)))* would saturate as *G* would be weak. Instead maximise *log(D(G(z)))*\n* The paper contains the theoretical proof for global optimum of the minimax game.\n\n## Experiments\n\n* Datasets\n    * MNIST, Toronto Face Database, CIFAR-10\n* Generator model uses RELU and sigmoid activations.\n* Discriminator model uses maxout and dropout.\n* Evaluation Metric\n    * Fit Gaussian Parzen window to samples obtained from *G* and compare log-likelihood.\n\n## Strengths\n\n* Computational advantages\n    * Backprop is sufficient for training with no need for Markov chains or performing inference.\n    * A variety of functions can be used in the model.\n* Since *G* is trained only using the gradients from *D*, fewer chances of directly copying features from the true data.\n* Can represent sharp (even degenerate) distributions.\n\n## Weakness\n\n* *D* must be well synchronised with *G*.\n* While *G* may learn to sample data points that are indistinguishable from true data, no explicit representation can be obtained.\n\n## Possible Extensions\n\n* Conditional generative models.\n* Inference network to predict *z* given *x*.\n* Implement a stochastic extension of the deterministic [Multi-Prediction Deep Boltzmann Machines](https://papers.nips.cc/paper/5024-multi-prediction-deep-boltzmann-machines.pdf)\n* Using discriminator net or inference net for feature selection.\n* Accelerating training by ensuring better coordination between *G* and *D* or by determining better distributions to sample *z* from during training.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5423-generative-adversarial-nets"
    },
    "370": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=mikolov2015roadmap",
        "transcript": "# A Roadmap towards Machine Intelligence\n\n## Introduction\n\n* The paper presents some general characteristics that intelligent machines should possess and a roadmap to develop such intelligent machines in small, realistic steps.\n* [Link to the paper](https://arxiv.org/abs/1511.08130)\n\n## Ability to Communicate\n\n* The intelligent agents should be able to communicate with humans, preferably using language as the medium.\n* Such systems can be programmed through natural language and can access much of the human knowledge which is encoded using natural language.\n* The learning environment should facilitate interactive communication and the machine should have a minimalistic bit interface for IO to keep the interface simple.\n* Further, the machine should be free to use any internal representation for learning tasks.\n\n## Ability to Learn\n\n* Learning allows the machine to adapt to the external environment and correct their mistakes.\n* Users should be able to control the motivation of the machine via a communication channel. This is similar to the notion of rewards in reinforcement learning.\n\n## A simulated ecosystem to educate communication-based intelligent machines\n\n* Simulated environment to teach basic linguistic interactions and know-how to operate in the world.\n* Though the environment should be challenging enough to force the machine to \"learn how to learn\", its complexity should be manageable.\n* Unlike class AI block worlds, the simulated environment is not intended to teach an exhaustive set of functionality to the agent. The aim is to teach the machine how to learn efficiently by combining already acquired skills.\n\n### Description\n\n#### Agent\n\n* Learner or actor\n* Teacher\n    * Assigns tasks and rewards to the learner and provides helpful information.\n    * Aim is to kick start the learner's efficient learning capabilities without providing enough direct information.\n* Environment\n    * Learner explores the environment by giving orders, asking questions and receiving feedback.\n    * Environment uses a controlled language which is more explicit and restricted.\n\nThink of learner as a high-level programming language, the teacher as the programmer and the environment as the compiler.\n\n#### Interface Channels\n\n* Generic input and output channels.\n* Teacher and environment write to the input channel.\n* Reward is written to input channel.\n* Learner writes to the output channel and learns to use ambigous prefixes to address the agents and services it needs to interact with.\n\n\n#### Reward\n\n* Way to provide feedback to the learner.\n* Rewards should become sparse as the learner's intelligence grows and \"curiosity\" should be a learnt strategy.\n* Learner should maximise average reward over time so that faster strategies are preferred in case of equal rewards.\n\n#### Incremental Structure\n\n* Think of learner progressing through different levels where skills from earlier levels can be used in later levels.\n* Tasks need not be ordered within a level.\n* Learner starts by performing basic tasks like repeating characters then learns to associate linguistic strings to action sequences. Further, the learner learns to ask questions and \"read\" natural text.\n\n#### Time Off\n\n* Learner is given time to either explore the environment or to interact with the Teacher or to update its internal structure by replaying the previous experience.\n\n\n#### Evaluation\n\n* Evaluating the learning agent on only the final behaviour only is not sufficient as it overlooks the number of attempts to reach the optimal behaviour.\n* Better approach would be to conduct public competition where developers have access to preprogrammed environment for fixed amount of time and learners are evaluated on tasks that are considerably different from the tasks encountered during training.\n\n\n#### Tasks\n\nA brief overview of the type of tasks is provided [here](https://github.com/facebookresearch/CommAI-env/blob/master/TASKS.md)\n\n## Types of Learning\n\n* Concept of positive and negative rewards.\n* Discovery of algorithms.\n* Remember facts, skills, and learning strategies.\n\n\n## Long term memory\n\n* To store facts, algorithms and even ability to learn.\n\n## Compositional Learning Skills\n\n* Producing new structures by combining together known facts and skills.\n* Understanding new concepts should not always require training examples.\n\n## Computational properties of intelligent machines\n\n* Computational model should be able to represent any pattern in data (alternatively, represent any algorithm in fixed length).\n* Among the various Turning-complete computational systems available, the most natural choice would be a compositional system that can perform computations in parallel. \n* Alternatively, a non-growing model with immensely large capacity could be used. \n* In a growing model, new cells are connected to ones that spawned them leading to topological structures that can contribute to learning.\n* But it is not clear if such topological structures can arise in a large-capacity unstructured model.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.08130"
    },
    "371": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=kannan2016smart",
        "transcript": "# Smart Reply: Automated Response Suggestion for Email\n\n## Introduction\n\n* Proposes a novel, end-to-end architecture for generating short email responses.\n* Single most important benchmark of its success is that it is deployed in Inbox by Gmail and assists with around 10% of all mobile responses.\n* [Link to the paper.](https://arxiv.org/abs/1606.04870)\n\n## Challenges in deploying Smart Reply in a user-facing product\n\n* Responses must always be of high quality. Ensured by constructing a target response set to select responses from.\n* The likelihood of choosing the responses must be maximised. Ensured by normalising the responses and enforcing diversity.\n* The system should not add latency to emails. Ensured by using a triggering model to decide if the email is suitable to undergo the response generation pipeline. Computation time is further reduced by finding approximate best result instead of the best result.\n* Ensure privacy by encrypting all the data which adds challenge in verifying the model's quality and debugging the system.\n\n## Architecture\n\n## Preprocess Email\n\n* Perform actions like language detection, tokenization, sentence segmentation etc on the input email.\n\n## Triggering Model\n\n* A feed-forward neural network (with embedding layer and 3 fully connected hidden layers) to decide if the input email is suitable for suggesting responses.\n\n#### Data\n\n* Training set of pairs *(o, y)* where *o* is the incoming message and *y* is a boolean variable to indicate if the message had a response.\n\n#### Features\n\n* Unigrams, bigrams from the messages.\n* Signals like - is the recipient in the contact list of the sender.\n\n## Response Selection\n\n* LSTM network to predict the approximate best response for an incoming message *o*\n\n#### Network\n\n* Sequence to Sequence Learning.\n* Reads the input message (token by token) and encode a vector representation.\n* Compute softmax to get the probability of first output token given the input token sequence.\n* Keep feeding in the previous response tokens and the input token sequence to compute the probability of next output token.\n* During inference, approximate the most likely response greedily by taking the most likely response at each timestamp and feeding it back or by using the beam search approach.\n\n## Response Set Generation\n\n* Generate a set of high-quality responses that also capture the variability in the intent of the response.\n* Canonicalize the email response by extracting the semantic structure using a dependency parser.\n* Partition all response messages into \"semantic\" clusters.\n* These semantic clusters define the response space for scoring and selecting possible responses and for promoting diversity among the responses.\n\n## Semantic Intent Clustering\n\n* Since a large, labelled dataset is not available, a graph based, semi-supervised approach is used.\n\n#### Graph Construction\n\n* Manually define a few clusters with a small number of example responses for each cluster.\n* Construct a graph with frequent response messages (including the labelled nodes) as response nodes (V<sub>R</sub>).\n* For each response node, extract a set of feature nodes (V<sub>F</sub>) corresponding to features like skip-gram and n-grams and add an edge between the response node and the feature node.\n* Learn a semantic labelling for all response nodes by propagating semantic intent information (available because of labelled nodes) throughout the graph.\n* After some iterations, sample some of the unlabeled nodes from the graph, manually label these sample nodes and repeat this algorithm until convergence.\n* For validation, extract the top k members of each cluster and validate the quality with help of human evaluators.\n\n## Suggestion Diversity\n\n* Provide users with a varied set of response by omitting redundant response (by not selecting more than one response from any semantic cluster) and by enforcing negative (or positive) responses.\n* If the top two responses contain at least one positive (negative) response and none of the top three responses is\nnegative (positive), the third response is replaced with a negative (positive) one.\n* This is done by performing a second LSTM pass where the search is restricted to only positive (or negative) responses in the target set.\n\n## Strengths\n\n* The system is already in production and assists with around 10% of all mobile responses.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.04870"
    },
    "372": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.05328",
        "transcript": "#### Introduction\n\n* The paper explores the domain of conditional image generation by adopting and improving PixelCNN architecture.\n* [Link to the paper](https://arxiv.org/abs/1606.05328)\n\n#### Based on PixelRNN and PixelCNN\n\n* Models image pixel by pixel by decomposing the joint image distribution as a product of conditionals.\n* PixelRNN uses two-dimensional LSTM while PixelCNN uses convolutional networks.\n* PixelRNN gives better results but PixelCNN is faster to train.\n\n#### Gated PixelCNN\n\n* PixelRNN outperforms PixelCNN due to the larger receptive field and because they contain multiplicative units, LSTM gates, which allow modelling more complex interactions.\n* To account for these, deeper models and gated activation units (equation 2 in the [paper](https://arxiv.org/abs/1606.05328)) can be used respectively.\n* Masked convolutions can lead to blind spots in the receptive fields.\n* These can be removed by combining 2 convolutional network stacks:\n    * Horizontal stack - conditions on the current row.\n    * Vertical stack - conditions on all rows above the current row.\n* Every layer in the horizontal stack takes as input the output of the previous layer as well as that of the vertical stack.\n* Residual connections are used in the horizontal stack and not in the vertical stack (as they did not seem to improve results in the initial settings).\n\n#### Conditional PixelCNN\n\n* Model conditional distribution of image, given the high-level description of the image, represented using the latent vector h (equation 4 in the [paper](https://arxiv.org/abs/1606.05328))\n* This conditioning does not depend on the location of the pixel in the image.\n* To consider the location as well, map h to spatial representation $s = m(h)$ (equation 5 in the the [paper](https://arxiv.org/abs/1606.05328))\n\n#### PixelCNN Auto-Encoders\n\n* Start with a traditional auto-encoder architecture and replace the deconvolutional decoder with PixelCNN and train the network end-to-end.\n\n#### Experiments\n\n* For unconditional modelling, Gated PixelCNN either outperforms PixelRNN or performs almost as good and takes much less time to train.\n* In the case of conditioning on ImageNet classes, the log likelihood measure did not improve a lot but the visual quality of the generated sampled was significantly improved.\n* Paper also included sample images generated by conditioning on human portraits and by training a PixelCNN auto-encoder on ImageNet patches.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.05328"
    },
    "373": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/OordKK16",
        "transcript": "#### Introduction\n\n* Problem: Building an expressive, tractable and scalable image model which can be used in downstream tasks like image generation, reconstruction, compression etc.\n* [Link to the paper](https://arxiv.org/abs/1601.06759)\n\n#### Model\n\n* Scan the image, one row at a time and one pixel at a time (within each row).\n* Given the scanned content, predict the distribution over the possible values for the next pixel.\n* Joint distribution over the pixel values is factorised into a product of conditional distributions thus causing the problem as a sequence problem.\n* Parameters used in prediction are shared across all the pixel positions.\n* Since each pixel is jointly determined by 3 values (3 colour channels), each channel may be conditioned on other channels as well.\n\n##### Pixel as discrete value\n\n* The conditional distributions are multinomial (with channel variable taking 1 of 256 discrete values).\n* This discrete representation is simpler and easier to learn.\n\n#### Pixel RNN\n\n##### Row LSTM\n\n* Undirectional layer that processed image row by row.\n* Uses one-dimensional convolution (kernel of size kx1, k>=3).\n* Refer image 2 in the [paper](https://arxiv.org/abs/1601.06759).\n* Weight sharing in convolution ensures translation invariance of computed feature along each row.\n* For LSTM, the input-to-state component is computed for the entire 2-d input map and then is masked to include only the valid context.\n* For equations related to state-to-state component, refer to equation 3 in the [paper](https://arxiv.org/abs/1601.06759)\n\n##### Diagonal BiLSTM\n\n* Bidirectional layer that processes the image in the diagonal fashion.\n* Input map skewed by offsetting each row of the image by one position with respect to the previous row.\n* Refer image 3 in the [paper](https://arxiv.org/abs/1601.06759)\n* For both directions, the input-to-state component is a 1 x 1 convolution while the state-to-state recurrent component is computed with column wise convolution using kernel size 2x1.\n* Kernel size of 2x1 processes minimal information yielding a highly non-linear computation.\n* Output map is skewed back by removing the offset positions.\n* To prevent layers from seeing further pixels, the right output map is shifted down by one row and added to left output map.\n\n##### Residual Connections\n\n* Residual connections (or skip connections) are used to increase convergence speed and to propagate signals more explicitly.\n* Refer image 4 in the [paper](https://arxiv.org/abs/1601.06759)\n\n##### Masked Convolutions\n\n* Masks are used to enforce certain restrictions on the connections in the network (eg when predicting values for R channel, values of B channel can not be used).\n* Mask A is applied to first convolution layer and restricts connections to only those neighbouring pixels and colour channels that have already been seen.\n* Mask B is applied to all subsequent input-to-state convolution transactions and allows connections from a colour channel to itself.\n* Refer image 4 in the [paper](https://arxiv.org/abs/1601.06759)\n\n##### PixelCNN\n\n* Uses multiple convolution layers that preserve spatial resolution.\n* Makes receptive field large but not unbounded.\n* Mask used to avoid seeing the future context.\n* Faster that PixelRNN at training or evaluation time (as convolutions can be parallelized easily).\n\n##### Multi-Scale PixelRNN\n\n* Composed of one unconditional PixelRNN and multiple conditional PixelRNNs.\n* Unconditional network generates a smaller s x s image which is fed as input to the conditional PixelRNN. (n is a multiple of s)\n* Conditional PixelRNN is a standard PixelRNN with layers biased with an upsampled version of the s x s image.\n* For upsampling, a convolution network with deconvolution layers constructs an enlarged feature map of size c x n x n.\n* For biasing, the c x n x n map is mapped to 4hxnxn map (using 1x1 unmasked convolution) and added to input-to-state map.\n\n#### Training and Evaluation\n\n* Pixel values are dequantized using real-valued noise and log likelihood of continuous and discrete models are compared.\n* Update rule - RMSProp\n* Batch size - 16 for MNIST and CIFAR 10 and 32(or 64) for IMAGENET.\n* Residual connections are as effective as Skip connections, in fact, the 2 can be used together as well.\n* PixelRNN outperforms other models for Binary MNIST and CIFAR10.\n* For CIFAR10, Diagonal BiLSTM > Row LSTM > PixelCNN. This is also the order of receptive field for the 3 architectures and the observation underlines the importance of having a large receptive field.\n* The paper also provides new benchmarks for generative image modelling on IMAGENET dataset.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1601.06759"
    },
    "374": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SimonyanVZ13",
        "transcript": "#### Introduction\n\n* The paper presents gradient computation based techniques to visualise image classification models.\n* [Link to the paper](https://arxiv.org/abs/1312.6034)\n\n#### Experimental Setup\n\n* Single deep convNet trained on ILSVRC-2013 dataset (1.2M training images and 1000 classes).\n* Weight layer configuration is: conv64-conv256-conv256-conv256-conv256-full4096-full4096-full1000.\n\n#### Class Model Visualisation\n\n* Given a learnt ConvNet and a class (of interest), start with the zero image and perform optimisation by back propagating with respect to the input image (keeping the ConvNet weights constant).\n* Add the mean image (for training set) to the resulting image.\n* The paper used unnormalised class scores so that optimisation focuses on increasing the score of target class and not decreasing the score of other classes.\n\n#### Image-Specific Class Saliency Visualisation\n\n* Given an image, class of interest, and trained ConvNet, rank the pixels of the input image based on their influence on class scores.\n* Derivative of the class score with respect to image gives an estimate of the importance of different pixels for the class.\n* The magnitude of derivative also indicated how much each pixel needs to be changed to improve the class score.\n\n##### Class Saliency Extraction\n\n* Find the derivative of the class score with respect with respect to the input image.\n* This would result in one single saliency map per colour channel.\n* To obtain a single saliency map, take the maximum magnitude of derivative across all colour channels.\n\n##### Weakly Supervised Object Localisation\n\n* The saliency map for an image provides a rough encoding of the location of the object of the class of interest. \n* Given an image and its saliency map, an object segmentation map can be computed using GraphCut colour segmentation.\n* Color continuity cues are needed as saliency maps might capture only the most dominant part of the object in the image.\n* This weakly supervised approach achieves 46.4% top-5 error on the test set of ILSVRC-2013.\n\n#### Relation to Deconvolutional Networks\n\n* DeconvNet-based reconstruction of the $n^{th}$ layer input is similar to computing the gradient of the visualised neuron activity $f$ with respect to the input layer.\n* One difference is in the way RELU neurons are treated: \n    * In DeconvNet, the sign indicator (for the derivative of RELU) is computed on output reconstruction while in this paper, the sign indicator is computed on the layer input.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1312.6034"
    },
    "375": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.01759",
        "transcript": "#### Introduction\n\n* Introduces fastText, a simple and highly efficient approach for text classification.\n* At par with deep learning models in terms of accuracy though an order of magnitude faster in performance. \n* [Link to the paper](http://arxiv.org/abs/1607.01759v3)\n* [Link to code](https://github.com/facebookresearch/fastText)\n\n#### Architecture\n\n* Built on top of linear models with a rank constraint and a fast loss approximation.\n* Start with word representations that are averaged into text representation and feed them to a linear classifier.\n* Think of text representation as a hidden state that can be shared among features and classes.\n* Softmax layer to obtain a probability distribution over pre-defined classes.\n* High computational complexity $O(kh)$, $k$ is the number of classes and $h$ is dimension of text representation.\n\n##### Hierarchial Softmax\n\n* Based on Huffman Coding Tree\n* Used to reduce complexity to $O(hlog(k))$\n* Top T results (from the tree) can be computed efficiently $O(logT)$ using a binary heap.\n\n##### N-gram Features\n\n* Instead of explicitly using word order, uses a bag of n-grams to maintain efficiency without losing on accuracy.\n* Uses [hashing trick](https://arxiv.org/pdf/0902.2206.pdf) to maintain fast and memory efficient mapping of the n-grams.\n\n#### Experiments\n\n##### Sentiment Analysis\n\n* fastText benefits by using bigrams.\n* Outperforms [char-CNN](http://arxiv.org/abs/1502.01710v5) and [char-CRNN](http://arxiv.org/abs/1602.00367v1) and performs a bit worse than [VDCNN](http://arxiv.org/abs/1606.01781v1).\n* Order of magnitudes faster in terms of training time.\n* Note: fastText does not use pre-trained word embeddings.\n\n##### Tag Prediction\n\n* fastText with bigrams outperforms [Tagspace](http://emnlp2014.org/papers/pdf/EMNLP2014194.pdf).\n* fastText performs upto 600 times faster at test time.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.01759"
    },
    "376": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/PenningtonSM14",
        "transcript": "#### Introduction\n\n* Introduces a new global log-bilinear regression model which combines the benefits of both global matrix factorization and local context window methods.\n\n\n#### Global Matrix Factorization Methods\n\n* Decompose large matrices into low-rank approximations.\n* eg - Latent Semantic Analysis (LSA)\n\n##### Limitations\n\n* Poor performance on word analogy task\n* Frequent words contribute disproportionately high to the similarity measure.\n\n#### Shallow, Local Context-Based Window Methods\n\n* Learn word representations using adjacent words.\n* eg - Continous bag-of-words (CBOW) model and skip-gram model.\n\n##### Limitations\n\n* Since they do not operate directly on the global co-occurrence counts, they can not utilise the statistics of the corpus effectively.\n\n\n#### GloVe Model\n\n* To capture the relationship between words $i$ and $j$, word vector models should use ratios of co-occurene probabilites (with other words $k$) instead of using raw probabilites themselves.\n* In most general form:\n     * $F(w_{i}, w_{j}, w_{k}^{~} ) = P_{ik}/P_{jk}$\n* We want $F$ to encode information in the vector space (which have a linear structure), so we can restrict to the difference of $w_{i}$ and $w_{j}$\n    * $F(w_{i} - w_{j}, w_{k}^{~} ) = P_{ik}/P_{jk}$\n* Since right hand side is a scalar and left hand side is a vector, we take dot product of the arguments.\n    * $F( (w_{i} - w_{j})^{T}, w_{k}^{~} ) = P_{ik}/P_{jk}$\n* *F* should be invariant to order of the word pair $i$ and $j$.\n    * $F(w_{i}^{T}w_{k}^{~}) = P_{ik}$\n* Doing further simplifications and optimisations (refer paper), we get cost function, \n    * $J = \\sum_{\\text{over all i, j pairs in the vocabulary}}[w_{i}^{T}w_{k}^{~} + b_{i} + b_{k}^{~} - log(X_{ik})]^{2}$\n    * $f$ is a weighing function.\n    * $f(x) = min((x/x_{max})^{\\alpha}, 1)$\n    * Typical values, $x_{\\max} = 100$ and $\\alpha = 3/4$\n    * *b* are the bias terms.\n\n##### Complexity\n\n* Depends on a number of non-zero elements in the input matrix.\n* Upper bound by the square of vocabulary size\n* Since for shallow window-based approaches, complexity depends on $|C|$ (size of the corpus), tighter bounds are needed.\n* By modelling number of co-occurrences of words as power law function of frequency rank, the complexity can be shown to be proportional to $|C|^{0.8}$\n\n#### Evaluation\n\n##### Tasks\n\n* Word Analogies\n    * a is to b as c is to ___?\n    * Both semantic and syntactic pairs\n    * Find closest d to $w_{b} - w_{c} + w_{a}$ (using cosine similarity)\n\n* Word Similarity\n* Named Entity Recognition\n\n##### Datasets\n\n* Wikipedia Dumps - 2010 and 2014\n* Gigaword5\n* Combination of Gigaword5 and Wikipedia2014\n* CommonCrawl\n* 400,000 most frequent words considered from the corpus.\n\n##### Hyperparameters\n\n* Size of context window.\n* Whether to distinguish left context from right context.\n* $f$ - Word pairs that are $d$ words apart contribute $1/d$ to the total count.\n* $xmax = 100$\n* $\\alpha = 3/4$\n* AdaGrad update\n\n##### Models Compared With\n\n* Singular Value Decomposition\n* Continous Bag-Of-Words\n* Skip-Gram\n\n##### Results\n\n* Glove outperforms all other models significantly.\n* Diminishing returns for vectors larger than 200 dimensions.\n* Small and asymmetric context windows (context window only to the left) works better for syntactic tasks.\n* Long and symmetric context windows (context window to both the sides) works better for semantic tasks.\n* Syntactic task benefited from larger corpus though semantic task performed better with Wikipedia instead of Gigaword5 probably due to the comprehensiveness of Wikipedia and slightly outdated nature of Gigaword5.\n* Word2vec\u2019s performance decreases if the number of negative samples increases beyond about 10.\n* For the same corpus, vocabulary, and window size GloVe consistently achieves better results, faster.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D14/D14-1162.pdf"
    },
    "377": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=jeh2002simrank",
        "transcript": "#### Introduction\n\n* Algorithm to derive similarity between 2 nodes of a graph (or graphical model derived from any other kind of dataset).\n* [Link to the paper](http://dl.acm.org/citation.cfm?id=775126)\n\n#### SimRank\n\n* Input: A directed graph $G = (V, E)$ where $V$ represents vertices and $E$ represents edges.\n* SimRank defines similarity between 2 vertices (or nodes) $i$ and $j$ as the average of the similarity between their in-neighbours decayed by a constant factor $C$.\n* Any node is maximally similar to itself (with similarity = 1).\n* PageRank analyses the individual vertices of the graph with respect to the global structure, while SimRank analyses the relationship between a pair of vertices (edges).\n* SimRank scores are symmetric and can be defined between all pair of vertices.\n* $G^{2}$ is defined as the node pair graph such that each node in *G^{2}* corresponds to an ordered pair of nodes of $G$ and there exists an edge between node pair (a, b) and (c, d) if there exists an edge between (a, c) and (b, d).\n* In $G^{2}$, similarity flows from node to node with singleton nodes (nodes of the form (a, a)) as the source of similarity.\n\n#### Variants\n\n##### Minimax Variant\n\n* Defines similarity of nodes $i$ and $j$ as the minimum of maximum similarity between $i$ and any in-neighbour of $j$ and between $j$ and any in-neighbour of $i$.\n\n#### Computing SimRank\n\n* A naive solution can be obtained by iteration to a fixed point.\n* Space complexity is $O(n^{2})$ and time complexity is $O(kn^{2}d)$ where $k$ is the number of iterations, $n$ is the number of vertices and $d$ is the average of product of indegrees of pair of vertices.\n* Optimisations can be made by setting the similarity between far off nodes as 0 and considering only nearby nodes for an update.\n\n#### Different Interpretations\n\n##### Co-citation Score\n\n* The first iteration of SimRank produces results same as co-citation score between a pair of vertices.\n* Successive iterations improve these initial scores.\n\n##### Random Surfer-Pairs Model\n\n* SimRank $s(a, b)$ can be interpreted as the measure of how soon two random surfers are expected to meet at the same node if they start at nodes a and b and walk the graph backwards.\n* Expected Meeting Distance (EMD) between 2 nodes a and b is the expected number of steps required before 2 surfers (starting at a and b) would meet if they walked randomly in locked step.\n* Surfers are allowed to teleport with a small probability - to circumvent the infinite EMD problem.\n* Expected-f Meeting Distance (EMD) - Given length l of a tour, compute f(l) (where f is a non-negative monotonic function) to bound the expected distance to a finite interval.\n* Common choice for f is $f(z) = C^{z}$ where $C  \\in (0, 1)$\n* The SimRank score for two nodes, with parameter $C$, is the expected-f meeting distance travelling back-edges with $f(z) = C^{z}$\n\n#### Evaluation\n\n* Experiments on 2 datasets:\n    * Corpus of scientific research papers from ResearchIndex.\n    * Transcripts of undergrad students at Stanford.\n* Domain specific properties used to measure similarity and compared with SimRank scores.\n* Results show improvement over co-citation scores.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/775047.775126"
    },
    "378": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LiuLSNCP16",
        "transcript": "#### Introduction\n\n* The paper explores the strengths and weaknesses of different evaluation metrics for end-to-end dialogue systems(in unsupervised setting).    \n* [Link to the paper](https://arxiv.org/abs/1603.08023)\n\n#### Evaluation Metrics Considered\n\n##### Word Based Similarity Metric\n\n###### BLEU\n\n* Analyses the co-occurrences of n-grams in the ground truth and the proposed responses.\n* BLEU-N: N-gram precision for the entire dataset.\n* Brevity penalty added to avoid bias towards short sentences.\n\n###### METEOR\n\n* Create explicit alignment between candidate and target response (using Wordnet, stemmed token etc).\n* Compute the harmonic mean of precision and recall between proposed and ground truth.\n\n###### ROGUE\n\n* F-measure based on Longest Common Subsequence (LCS) between candidate and target response.\n\n##### Embedding Based Metric\n\n###### Greedy Matching\n\n* Each token in actual response is greedily matched with each token in predicted response based on cosine similarity of word embedding (and vice-versa).\n* Total score is averaged over all words.\n\n###### Embedding Average\n\n* Calculate sentence level embedding by averaging word level embeddings\n* Compare sentence level embeddings between candidate and target sentences.\n\n###### Vector Extrema\n\n* For each dimension in the word vector, take the most extreme value amongst all word vectors in the sentence, and use\nthat value in the sentence-level embedding.\n* Idea is that by taking the maxima along each dimension, we can ignore the common words (which will be pulled towards the origin in the vector space).\n\n#### Dialogue Models Considered\n\n##### Retrieval Models\n\n###### TF-IDF\n\n* Compute the TF-IDF vectors for each context and response in the corpus.\n* C-TFIDF computes the cosine similarity between an input context and all other contexts in the corpus and returns the response with the highest score.\n* R-TFIDF computes the cosine similarity between the input context and each response directly.\n\n###### Dual Encoder\n\n* Two RNNs which respectively compute the vector representation of the input context and response. \n* Then calculate the probability that given response is the ground truth response given the context.\n\n##### Generative Models\n\n###### LSTM language model\n\n* LSTM model trained to predict the next word in the (context, response) pair.\n* Given a context, model encodes it with the LSTM and generates a response using a greedy beam search procedure.\n\n\n###### Hierarchical Recurrent Encoder-Decoder (HRED) \n\n* Uses a hierarchy of encoders.\n* Each utterance in the context passes through an \u2018utterance-level\u2019 encoder and the output of these encoders is passed through another 'context-level' decoder. \n* Better handling of long-term dependencies as compared to the conventional Encoder-Decoder.\n\n\n#### Observations\n\n* Human survey to determine the correlation between human judgement on the quality of responses, and the score assigned by each metric.\n* Metrics (especially BLEU-4 and BLEU-3) correlate poorly with human evaluation.\n* Best performing metric:\n    * Using word-overlaps - BLEU-2 score\n    * Using word embeddings - vector average\n* Embedding-based metrics would benefit from a weighting of word saliency.\n* BLEU could still be a good evaluation metric in constrained tasks like mapping dialogue acts to natural language sentences.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.08023"
    },
    "379": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1608.03000",
        "transcript": "#### Introduction\n\n* Task of translating natural language queries into regular expressions without using domain specific knowledge.\n* Proposes a methodology for collecting a large corpus of regular expressions to natural language pairs.\n* Reports performance gain of 19.6% over state-of-the-art models.\n* [Link to the paper](http://arxiv.org/abs/1608.03000v1)\n\n#### Architecture\n\n* LSTM based sequence to sequence neural network (with attention)\n* Six layers\n    * One-word embedding layer\n    * Two encoder layers\n    * Two decoder layers\n    * One dense output layer.\n* Attention over encoder layer.\n* Dropout with the probability of 0.25.\n* 20 epochs, minibatch size of 32 and learning rate of 1 (with decay rate of 0.5)\n\n#### Dataset Generation\n\n* Created a public dataset - **NL-RX** - with 10K pair of (regular expression, natural language) \n* Two step generate-and-paraphrase approach\n    * Generate step\n        * Use handcrafted grammar to translate regular expressions to natural language.\n    * Paraphrase step\n        * Crowdsourcing the task of translating the rigid descriptions into more natural expressions.\n\n\n#### Results\n\n* Evaluation Metric\n    * Functional equality check (called DFA-Equal) as same regular expression could be written in many ways.\n* Proposed architecture outperforms both the baselines - Nearest Neighbor classifier using Bag of Words (BoWNN) and Semantic-Unify",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1608.03000"
    },
    "380": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1608.03542",
        "transcript": "#### Introduction\n\n* Large scale natural language understanding task - predict text values given a knowledge base.\n* Accompanied by a large dataset generated using Wikipedia\n* [Link to the paper](http://www.aclweb.org/anthology/P/P16/P16-1145.pdf)\n\n#### Dataset\n\n* WikiReading dataset built using Wikidata and Wikipedia.\n* Wikidata consists of statements of the form (property, value) about different items\n* 80M statements, 16M items and 884 properties.\n* These statements are grouped by items to get (item, property, answer) tuples where the answer is a set of values.\n* Items are further replaced by their Wikipedia documents to generate 18.58M statements of the form (document, property, answer).\n* Task is to predict answer given document and property.\n* Properties are divided into 2 classes:\n    * **Categorical properties** - properties with a small number of possible answers. Eg gender.\n    * **Relational properties** - properties with unique answers. Eg date of birth.\n* This classification is done on the basis of the entropy of answer distribution.\n* Properties with entropy less than 0.7 are classified as categorical properties.\n* Answer distribution has a small number of very high-frequency answers (head) and a large number of answers with very small frequency (tail).\n* 30% of the answers do not appear in the training set and must be inferred from the document.\n\n#### Models\n\n##### Answer Classification\n\n* Consider WikiReading as classification task and treat each answer as a class label.\n\n###### Baseline\n    \n* Linear model over Bag of Words (BoW) features.\n* Two BoW vectors computed - one for the document and other for the property. These are concatenated into a single feature vector.\n\n###### Neural Networks Method\n\n* Encode property and document into a joint representation which is fed into a softmax layer.\n\n* **Average Embeddings BoW**\n    \n    * Average the BoW embeddings for documents and property and concatenate to get joint representation.\n\n* **Paragraph Vectors**\n    \n    * As a variant of the previous method, encode document as a paragraph vector.\n\n* **LSTM Reader**\n\n    * LSTM reads the property and document sequence, word-by-word, and uses the final state as joint representation.\n\n* **Attentive Reader**\n\n    * Use attention mechanism to focus on relevant parts of the document for a given property.\n\n* **Memory Networks**\n\n    * Maps a property p and list of sentences x<sub>1</sub>, x<sub>2</sub>, ...x<sub>n</sub> in a joint representation by attention over the sentences in the document.\n\n##### Answer Extraction\n\n* For relational properties, it makes more sense to model the problem as information extraction than classification.\n\n* **RNNLabeler**\n    \n    * Use an RNN to read the sequence of words and estimate if a given word is part of the answer.\n\n* **Basic SeqToSeq (Sequence to Sequence)**\n\n    * Similar to LSTM Reader but augmented with a second RNN to decode answer as a sequence of words.\n\n* **Placeholder SeqToSeq**\n    \n    * Extends Basic SeqToSeq to handle OOV (Out of Vocabulary) words by adding placeholders to the vocabulary.\n    * OOV words in the document and answer are replaced by placeholders so that input and output sentences are a mixture of words and placeholders only.\n\n* **Basic Character SeqToSeq**\n\n    * Property encoder RNN reads the property, character-by-character and transforms it into a fixed length vector.\n    * This becomes the initial hidden state for the second layer of a 2-layer document encoder RNN.\n    * Final state of this RNN is used by answer decoder RNN to generate answer as a character sequence.\n\n* **Character SeqToSeq with pretraining**\n\n    * Train a character-level language model on input character sequence from the training set and use the weights to initiate the first layer of encoder and decoder.\n\n#### Experiments\n\n* Evaluation metric is F1 score (harmonic mean of precision and accuracy).\n* All models perform well on categorical properties with neural models outperforming others.\n* In the case of relational properties, SeqToSeq models have a clear edge.\n* SeqToSeq models also show a great deal of balance between relational and categorical properties.\n* Language model pretraining enhances the performance of character SeqToSeq approach.\n* Results demonstrate that end-to-end SeqToSeq models are most promising for WikiReading like tasks.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1608.03542"
    },
    "381": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/YangYM15",
        "transcript": "#### Introduction\n\n* Presents WikiQA - a publicly available set of question and sentence pairs for open-domain question answering.\n* [Link to the paper](https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/)\n\n#### Dataset\n\n* 3047 questions sampled from Bing query logs.\n* Each question associated with a Wikipedia page.\n* All sentences in the summary paragraph of the page become the candidate answers.\n* Only 1/3rd questions have a correct answer in the candidate answer set.\n* Solutions crowdsourced through MTurk like platform.\n* Answer sentences are associated with *answer phrases* (shortest substring of a sentence that answers the question) though this annotation is not used in the experiments reported by the paper.\n\n#### Other Datasets\n\n* [QASent datset](http://homes.cs.washington.edu/~nasmith/papers/wang+smith+mitamura.emnlp07.pdf)\n    * Uses questions from TREC-QA dataset (questions from both query logs and human editors) and selects sentences which share at least one non-stopword from the question. \n    * Lexical overlap makes QA task easier.\n    * Does not support evaluating for *answer triggering* (detecting if the correct answer even exists in the candidate sentences).\n\n#### Experiments\n\n##### Baseline Systems\n\n* **Word Count** - Counts the number of non-stopwords common to question and answer sentences.\n* **Weighted Word Count** - Re-weight word counts by the IDF values of the question words.\n* **[LCLR](https://www.microsoft.com/en-us/research/publication/question-answering-using-enhanced-lexical-semantic-models/)** - Uses rich lexical semantic features like WordNet and vector-space lexical semantic models.\n* **Paragraph Vectors** - Considers cosine similarity between question vector and sentence vector.\n* **Convolutional Neural Network (CNN)** - Bigram CNN model with average pooling.\n* **PV-Cnt** and **CNN-Cnt** - Logistic regression classifier combining PV (and CNN) models and Word Count models.\n\n##### Metrics\n\n* MAP and MRR for answer selection problem.\n* Precision, recall and F1 scores for answer triggering problem.\n\n#### Observations\n\n* CNN-cnt outperforms all other models on both the tasks.\n* Three additional features, namely the length of the question (QLen), the length of sentence (SLen), and the class of the question (QClass) are added to track question hardness and sentence comprehensiveness.\n* Adding QLen improves performance significantly while adding SLen (QClass) improves (degrades) performance marginally.\n* For the same model, the performance on the WikiQA dataset is inferior to that on the QASent dataset.\n* Note: The dataset is very small to train end-to-end networks.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D15/D15-1237.pdf"
    },
    "382": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HermannKGEKSB15",
        "transcript": "#### Introduction\n\n* Build a supervised reading comprehension data set using news corpus.\n* Compare the performance of neural models and state-of-the-art natural language processing model on reading comprehension task.\n* [Link to the paper](http://arxiv.org/abs/1506.03340v3)\n\n#### Reading Comprehension\n\n* Estimate conditional probability $p(a|c, q)$, where $c$ is a context document, $q$ is a query related to the document, and $a$ is the answer to that query.\n\n#### Dataset Generation\n\n* Use online newspapers (CNN and DailyMail) and their matching summaries.\n* Parse summaries and bullet points into Cloze style questions.\n* Generate corpus of document-query-answer triplets by replacing one entity at a time with a placeholder.\n* Data anonymized and randomised using coreference systems, abstract entity markers and random permutation of the entity markers.\n* The processed data set is more focused in terms of evaluating reading comprehension as models can not exploit co-occurrence.\n\n#### Models\n\n##### Baseline Models\n\n* **Majority Baseline**\n    * Picks the most frequently observed entity in the context document.\n* **Exclusive Majority**\n    * Picks the most frequently observed entity in the context document which is not observed in the query.\n\n##### Symbolic Matching Models\n\n* **Frame-Semantic Parsing**\n    * Parse the sentence to find predicates to answer questions like \"who did what to whom\".\n    * Extracting entity-predicate triples $(e1,V, e2)$ from query $q$ and context document $d$\n    * Resolve queries using rules like `exact match`, `matching entity` etc.\n\n* **Word Distance Benchmark**\n    * Align placeholder of Cloze form questions with each possible entity in the context document and calculate the distance between the question and the context around the aligned entity.\n    * Sum the distance of every word in $q$ to their nearest aligned word in $d$\n\n##### Neural Network Models\n\n* **Deep LSTM Reader**\n    * Test the ability of Deep LSTM encoders to handle significantly longer sequences.\n    * Feed the document query pair as a single large document, one word at a time.\n    * Use Deep LSTM cell with skip connections from input to hidden layers and hidden layer to output.\n\n* **Attentive Reader**\n    * Employ attention model to overcome the bottleneck of fixed width hidden vector.\n    * Encode the document and the query using separate bidirectional single layer LSTM.\n    * Query encoding is obtained by concatenating the final forward and backwards outputs.\n    * Document encoding is obtained by a weighted sum of output vectors (obtained by concatenating the forward and backwards outputs).\n    * The weights can be interpreted as the degree to which the network attends to a particular token in the document.\n    * Model completed by defining a non-linear combination of document and query embedding.\n\n* **Impatient Reader**\n    * As an add-on to the attentive reader, the model can re-read the document as each query token is read.\n    * Model accumulates the information from the document as each query token is seen and finally outputs a joint document query representation in the form of a non-linear combination of document embedding and query embedding.\n\n#### Result\n\n* Attentive and Impatient Readers outperform all other models highlighting the benefits of attention modelling.\n* Frame-Semantic pipeline does not scale to cases where several methods are needed to answer a query.\n* Moreover, they provide poor coverage as a lot of relations do not adhere to the default predicate-argument structure.\n* Word Distance approach outperformed the Frame-Semantic approach as there was significant lexical overlap between the query and the document.\n* The paper also includes heat maps over the context documents to visualise the attention mechanism.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend"
    },
    "383": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.06931",
        "transcript": "#### Introduction\n\n* The paper presents a suite of benchmark tasks to evaluate end-to-end dialogue systems such that performing well on the tasks is a necessary (but not sufficient) condition for a fully functional dialogue agent.\n* [Link to the paper](https://research.facebook.com/publications/evaluating-prerequisite-qualities-for-learning-end-to-end-dialog-systems/)\n\n#### Dataset\n\n* Created using large-scale real-world sources - OMDB (Open Movie Database), MovieLens and Reddit.\n* Consists of ~75K movie entities and ~3.5M training examples.\n\n#### Tasks\n\n##### QA Task\n\n* Answering Factoid Questions without relation to the previous dialogue.\n* KB(Knowledge Base) created using OMDB and stored as triplets of the form (Entity, Relation, Entity).\n* Question (in Natural Language Form) generated by creating templates using [SimpleQuestions](https://arxiv.org/abs/1506.02075)\n* Instead of giving out just 1 response, the system ranks all the answers in order of their relevance.\n\n##### Recommendation Task\n\n* Providing personalised responses to the user via recommendation instead of providing universal facts as in case 1.\n* MovieLens dataset with a *user x item* matrix of ratings.\n* Statements (for any user) are generated by sampling highly ranked movies by the user and forming a statement about these movies using natural language templates.\n* Like the previous case, a list of ranked responses is generated.\n\n##### QA + Recommendation Task\n\n* Maintaining short dialogues involving both factoid and personalised content.\n* Dataset consists of short conversations of 3 exchanges (3 from each participant).\n\n##### Reddit Discussion Task\n\n* Identify most likely response is discussions on Reddit.\n* Data processed to flatten the potential conversation so that it appears to be a two participant conversation.\n\n##### Joint Task\n\n* Combines all the previous tasks into one single task to test all the skills at once.\n\n#### Models Tested\n\n* **Memory Networks** - Comprises of a memory component that includes both long term memory and short term context.\n\n* **Supervised Embedding Models** - Sum the word embeddings of the input and the target independently and compare them with a similarity metric.\n\n* **Recurrent Language Models** - RNN, LSTM, SeqToSeq\n\n* **Question Answering Systems** - Systems that answer natural language questions by converting them into search queries over a KB.\n\n* **SVD(Singular Value Decomposition)** - Standard benchmark for recommendation.\n\n* **Information Retrieval Models** - Given a message, find the most similar message in the training dataset and report its output or find a most similar response to input directly.\n\n#### Result\n\n##### QA Task\n\n* QA System > Memory Networks > Supervised Embeddings > LSTM\n\n##### Recommendation Task\n\n* Supervised Embeddings > Memory Networks > LSTM > SVD\n\n##### Task Involving Dialog History\n\n* QA + Recommendation Task and Reddit Discussion Task\n* Memory Networks > Supervised Embeddings > LSTM\n\n##### Joint Task\n\n* Supervised word embeddings perform very poorly even when using a large number of dimensions (2000 dimensions).\n* Memory Networks perform better than embedding models as they can utilise the local context and the long-term memory. But they do not perform as well on standalone QA tasks.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.06931"
    },
    "384": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZarembaSV14",
        "transcript": "#### Introduction\n\n* The paper explains how to apply dropout to LSTMs and how it could reduce overfitting in tasks like language modelling, speech recognition, image caption generation and machine translation.\n* [Link to the paper](https://arxiv.org/abs/1409.2329)\n\n#### [Dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n\n* Regularisation method that drops out (or temporarily removes) units in a neural network.\nthe network, along with all its incoming and outgoing connections\n* Conventional dropout does not work well with RNNs as the recurrence amplifies the noise and hurts learning.\n\n#### Regularization\n\n* The paper proposes to apply dropout to only the non-recurrent connections.\n* The dropout operator would corrupt information carried by some units (and not all) forcing them to perform intermediate computations more robustly.\n* The information is corrupted L+1 times where L is the number of layers and is independent of timestamps traversed by the information.\n\n#### Observation\n\n* In the context of language modelling, image caption generation, speech recognition and machine translation, dropout enables training larger networks and reduces the testing error in terms of perplexity and frame accuracy.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1409.2329"
    },
    "385": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.04442",
        "transcript": "#### Introduction\n\n* Automated Theorem Proving (ATP) - Attempting to prove mathematical theorems automatically.\n* Bottlenecks in ATP:\n    * **Autoformalization** - Semantic or formal parsing of informal proofs.\n    * **Automated Reasoning** - Reasoning about already formalised proofs.\n* Paper evaluates the effectiveness of neural sequence models for premise selection (related to automated reasoning) without using hand engineered features.\n* [Link to the paper](https://arxiv.org/abs/1606.04442)\n\n#### Premise Selection\n\n* Given a large set of premises P, an ATP system A with given resource limits, and a new conjecture C, predict those premises from P that will most likely lead to an automatically constructed proof of C by A\n\n#### Dataset\n\n* Mizar Mathematical Library (MML) used as the formal corpus.\n* The premise length varies from 5 to 84299 characters and over 60% if the words occur fewer than 10 times (rare word problem).\n\n#### Approach\n\n* The model predicts the probability that a given axiom is useful for proving a given conjecture.\n* Conjecture and axiom sequences are separately embedded into fixed length real vectors, then concatenated and passed to a third network with few fully connected layers and logistic loss.\n* The two embedding networks and the joint predictor path are trained jointly.\n\n##### Stage 1: Character-level Models\n\n* Treat premises on character level using an 80-dimensional one hot encoding vector.\n* Architectures for embedding:\n    * pure recurrent LSTM and GRU Network\n    * CNN (with max pooling)\n    * Recurrent-convolutional network that shortens the sequence using convolutional layer before feeding it to LSTM.\n\n##### Stage 2: Word-level Models\n\n* MML dataset contains both implicit and explicit definitions.\n* To avoid manually encoding the implicit definitions, the entire statement defining an identifier is embedded and the definition embeddings are used as word level embeddings.\n* This is better than recursively expanding and embedding the word definition as the definition chains can be very deep.\n* Once word level embeddings are obtained, the architecture from Character-level models can be reused.\n\n#### Experiments\n\n##### Metrics\n\n* For each conjecture, the model ranks the possible premises.\n* Primary metric is the number of conjectures proved from top-k premises.\n* Average Max Relative Rank (AMMR) is  more sophisticated measure based on the motivation that conjectures are easier to prove if all their dependencies occur earlier in ranking.\n* Since it is very costly to rank all axioms for a conjecture, an approximation is made and a fixed number of random false dependencies are used for evaluating AMMR.\n\n##### Network Training\n\n* Asynchronous distributed stochastic gradient descent using Adam optimizer.\n* Clipped vs Non-clipped Gradients.\n* Max Sequence length of 2048 for character-level sequences and 500 for word-level sequences.\n\n##### Results\n\n* Best Selection Pipeline - Stage 1 character-level CNN which produces word-level embeddings for the next stage.\n* Best models use simple CNNs followed by max pooling and two-stage definition-based def-CNN outperforms naive word-CNN (where word embeddings are learnt in a single pass).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.04442v1"
    },
    "386": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/VinyalsL15",
        "transcript": "#### Introduction\n\n* The paper presents a domain agnostic approach for conversational modelling based on [Sequence to Sequence Learning Framework](https://gist.github.com/shagunsodhani/e3608ccf262d6e5a6b537128c917c92c).\n* [Link to the paper](http://arxiv.org/abs/1506.05869)\n\n#### Model\n\n* Neural Conversational Model (NCM)\n* A Recurrent Neural Network (RNN) reads the input sentence, one token at a time, and predicts the output sequence, one    token at a time.\n* Learns by backpropagation.\n* The model maximises the cross entropy of correct sequence given its context.\n* Greedy inference approach where predicted output token is used as input to predict the next output token.\n\n#### Dataset\n\n* IT HelpDesk dataset of conversations about computer related issues.\n* OpenSubtitles dataset containing movie conversations.\n\n#### Results\n\n* The paper has reported some samples of conversations generated by the interaction between human actor and the NCM.\n* NCM reports lower perplexity as compared to n-grams model.\n* NCM outperforms CleverBot in a subjective test involving human evaluators to grade the two systems.\n\n#### Strengths\n\n* Domain-agnostic.\n* End-To-End training without handcrafted rules.\n* Underlying architecture (Sequence To Sequence Framework) can be leveraged for machine translation, question answering etc.\n\n#### Weakness\n\n* The responses are simple, short and at times inconsistent.\n* The objective function of Sequence To Sequence Framework is not designed to capture the objective of conversational models.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.05869"
    },
    "387": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1212.0901",
        "transcript": "#### Introduction\n\n* Recurrent Neural Networks (RNNs) are very powerful at modelling sequences but they are not good at learning long-term dependencies.\n* The paper discusses the reasons behind this difficulty and some suggestions to mitigate it.\n* [Link to the paper.](https://arxiv.org/abs/1212.0901)\n\n#### Optimization Difficulty\n\n* RNNs form a deterministic state variable h<sup>t</sup> as function of input observation and previous state.\n* Learnable parameters to decide what will be remembered about the past sequence.\n* Using local optimisation techniques like Stochastic Gradient Descent (SGD) are unlikely to find optimal values of tunable parameters\n* When computations performed by RNN are unfolded through time, a deep Neural Network with shared weights is realised.\n* The cost function of this deep network depends on the output of hidden layers.\n* Gradient descent updates could \"explode\" (become very large) or \"vanish\" (become very small).\n\n#### Training Recurrent Networks\n\n* **Clip Gradient** - when the norm of the gradient vector ($g$) is above a threshold, update is done in direction of threshold $g/||g||$. This normalisation implements a simple form of second-order normalisation (the second-order derivate will also be large in regions of exploding gradient).\n\n* Use a **leaky integration** state-to-state map:\n    $h_{t, i} = \\alpha_{i}h_{t-1, i} + (1-\\alpha _{i})F_{i}(h_{t-1}, x_{t})$\n\n    Different values of &alpha; allow a different amount of the previous state to \"leak\" through the unfolded layers to further in time. This simply expands the time-scale of vanishing gradients and not totally remove them.\n\n* Use **output probability models** like Restricted Boltzmann Machine or NADE to capture higher order dependencies between variables in case of multivariate prediction.\n\n* By using **rectifier non-linearities**, the gradient on hidden units becomes sparse and these sparse gradients help the hidden units to specialise. The basic idea is that if the gradient is concentrated in fewer paths (in the unfolded computational graph) the vanishing gradient effect would be limited.\n\n* A **simplified Nesterov Momentum** rule is proposed to allow storing past velocities for a longer time while actually using these velocities more conservatively. The new formulation is also easier to implement.\n\n#### Results\n\n* SGD with these optimisations outperforms a vanilla SGD.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1212.0901"
    },
    "388": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.04582",
        "transcript": "#### Introduction\n\n* **Machine Comprehension (MC)** - given a natural language sentence, answer a natural language question.\n* **End-To-End MC** - can not use language resources like dependency parsers. The only supervision during training is the correct answer.\n* **Query Regression Network (QRN)** - Variant of Recurrent Neural Network (RNN).\n* [Link to the paper](http://arxiv.org/abs/1606.04582)\n\n#### Related Work\n\n* Long Short-Term Memory (LSTM) and Gated Recurrence Unit (GRU) are popular choices to model sequential data but perform poorly on end-to-end MC due to long-term dependencies.\n* Attention Models with shared external memory focus on single sentences in each layer but the models tend to be insensitive to the time step of the sentence being accessed.\n* **Memory Networks (and MemN2N)**\n    * Add time-dependent variable to the sentence representation.\n    * Summarize the memory in each layer to control attention in the next layer.\n* **Dynamic Memory Networks (and DMN+)**\n    * Combine RNN and attention mechanism to incorporate time dependency.\n    * Uses 2 GRU\n        * time-axis GRU - Summarize the memory in each layer.\n        * layer-axis GRU - Control the attention in each layer.\n* QRN is a much simpler model without any memory summarized node.\n\n#### QRN\n\n* Single recurrent unit that updates its internal state through time and layers.\n* Inputs\n    * $q_{t}$ - local query vector\n    * $x_{t}$ - sentence vector\n* Outputs\n    * $h_{t}$ - reduced query vector\n    * $x_{t}$ - sentence vector without any modifications\n* Equations\n    * $z_{t} = \\alpha (x_{t}, q_{t})$\n    * &alpha is the **update gate function** to measure the relevance between input sentence and local query.\n    * $h`_{t} = \\gamma (x_{t}, q_{t})$\n    * &gamma is the **regression function** to transform the local query into regressed query.\n    * $h_{t} = z_{t} \\* h'_{t} + (1 - z_{t}) \\* h_{t-1}$\n* To create a multi layer model, output of current layer becomes input to the next layer.\n\n#### Variants\n\n* **Reset gate function** ($r_{t}$) to reset or nullify the regressed query $h`_{t}$ (inspired from GRU).\n    * The new equation becomes $h_{t} = z_{t}\\*r_{t}\\* h`_{t} + (1 - z_{t})\\*h_{t-1}$\n* **Vector gates** - update and reset gate functions can produce vectors instead of scalar values (for finer control).\n* **Bidirectional** - QRN can look at both past and future sentences while regressing the queries.\n    * $q_{t}^{k+1} = h_{t}^{k, \\text{forward}} + h_{t}^{k, \\text{backward}}$.\n    * The variables of update and regress functions are shared between the two directions.\n\n#### Parallelization\n\n* Unlike most RNN based models, recurrent updates in QRN can be computed in parallel across time.\n* For details and equations, refer the [paper](http://arxiv.org/abs/1606.04582).\n\n#### Module Details\n\n##### Input Modules\n    \n* A trainable embedding matrix A is used to encode the one-hot vector of each word in the input sentence into a d-dimensional vector.\n* Position Encoder is used to obtain the sentence representation from the d-dimensional vectors.\n* Question vectors are also obtained in a similar manner.\n\n##### Output Module\n\n* A V-way single-layer softmax classifier is used to map predicted answer vector $y$ to a V-dimensional sparse vector $v$.\n* The natural language answer $y is the arg max word in $v$.\n\n\n#### Results\n\n* [bAbI QA](https://gist.github.com/shagunsodhani/12691b76addf149a224c24ab64b5bdcc) dataset used.\n* QRN on 1K dataset with '2rb' (2 layers + reset gate + bidirectional) model and on 10K dataset with '2rvb' (2 layers + reset gate + vector gate + bidirectional) outperforms MemN2N 1K and 10K models respectively.\n* Though DMN+ outperforms QRN with a small margin, QRN are simpler and faster to train (the paper made the comment on the speed of training without reporting the training time of the two models).\n* With very few layers, the model lacks reasoning ability while with too many layers, the model becomes difficult to train.\n* Using vector gates works for large datasets while hurts for small datasets.\n* Unidirectional models perform poorly.\n* The intermediate query updates can be interpreted in natural language to understand the flow of information in the network.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.04582"
    },
    "389": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SutskeverVL14",
        "transcript": "#### Introduction\n\n* The paper proposes a general and end-to-end approach for sequence learning that uses two deep LSTMs, one to map input sequence to vector space and another to map vector to the output sequence.\n* For sequence learning, Deep Neural Networks (DNNs) requires the dimensionality of input and output sequences be known and fixed. This limitation is overcome by using the two LSTMs.\n* [Link to the paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n\n#### Model\n\n* Recurrent Neural Networks (RNNs) generalizes feed forward neural networks to sequences.\n* Given a sequence of inputs $(x_{1}, x_{2}...x_{t})$, RNN computes a sequence of outputs $(y_1, y_2...y_t')$ by iterating over the following equation:\n\n$$h_t = sigm(W^{hx}x_t + W^{hh} h_{t-1})$$\n\n$$y^{t} = W^{yh}h_{t}$$\n\n* To map variable length sequences, the input is mapped to a fixed size vector using an RNN and this fixed size vector is mapped to output sequence using another RNN.\n* Given the long-term dependencies between the two sequences, LSTMs are preferred over RNNs.\n* LSTMs estimate the conditional probability *p(output sequence | input sequence)* by first mapping the input sequence to a fixed dimensional representation and then computing the probability of output with a standard LST-LM formulation.\n\n##### Differences between the model and standard LSTMs\n\n* The model uses two LSTMs (one for input sequence and another for output sequence), thereby increasing the number of model parameters at negligible computing cost.\n* Model uses Deep LSTMs (4 layers).\n* The words in the input sequences are reversed to introduce short-term dependencies and to reduce the \"minimal time lag\". By reversing the word order, the first few words in the source sentence (input sentence) are much closer to first few words in the target sentence (output sentence) thereby making it easier for LSTM to \"establish\" communication between input and output sentences.\n\n\n#### Experiments\n\n* WMT'14 English to French dataset containing 12 million sentences consisting of 348 million French words and 304 million English words.\n* Model tested on translation task and on the task of re-scoring the n-best results of baseline approach.\n* Deep LSTMs trained in sentence pairs by maximizing the log probability of a correct translation $T$, given the source sentence $S$\n* The training objective is to maximize this log probability, averaged over all the pairs in the training set.\n* Most likely translation is found by performing a simple, left-to-right beam search for translation.\n* A hard constraint is enforced on the norm of the gradient to avoid the exploding gradient problem.\n* Min batches are selected to have sentences of similar lengths to reduce training time.\n* Model performs better when reversed sentences are used for training.\n* While the model does not beat the state-of-the-art, it is the first pure neural translation system to outperform a phrase-based SMT baseline.\n* The model performs well on long sentences as well with only a minor degradation for the largest sentences.\n* The paper prepares ground for the application of sequence-to-sequence based learning models in other domains by demonstrating how a simple and relatively unoptimised neural model could outperform a mature SMT system on translation tasks.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks"
    },
    "390": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jmlr/ErhanMBBV09",
        "transcript": "#### Introduction\n\n* The paper explores the challenges involved in training deep networks, the effect of unsupervised pre-training on training process and visualizes the error function landscape for deep architectures.\n* [Link to the paper](http://research.google.com/pubs/pub34923.html)\n\n#### Experiments\n\n* Datasets used - Shapeset and MNIST.\n* Train deep architectures for a variable number of layers with and without pre-training.\n* Weights initialized using random sample from $[\\frac{-1}{\\sqrt(k)}, \\frac{1}{\\sqrt(k)}]$ where $k$ is fan-in value.\n\n#### Observations\n\n* Increasing depth (without pre-training) causes error rate to go up faster than the case of pre-training.\n* Pre-training also makes the network more robust to random initializations.\n* At same training cost level, the pre-trained models systematically yields a lower cost than the randomly initialized ones.\n* Pre-training seems to be most advantageous for smaller training sets.\n* Pre-training appears to have a regularizing effect - it decreases the variance (for parameter configurations) by restricting the set of possible final configurations for parameter values and introduces a bias.\n* Pre-training helps for larger layers (with a larger number of units per layer) and for deeper networks. But in the case of small networks, it can lower the performance. \n* As small networks tend to have a small capacity, this supports the hypothesis that pre-training exhibits a kind of regularizing effect.\n* Pre-training seems to provide a better marginal conditioning of the weights. Though this is not the only benefit pre-training provides as it captures more intricate dependencies.\n* Pre-training the lower layers is more important (and impactful) than pre-training the layers closer to the output.\n* Error landscape seems to be flatter for deep architectures and for the case of pre-training.\n* Learning trajectories for pre-trained and not pre-trained models start and stay in different regions of function space. Moreover, trajectories of any of the given type initially move together, but at some point, they diverge away.",
        "sourceType": "blog",
        "linkToPaper": "http://www.jmlr.org/proceedings/papers/v5/erhan09a.html"
    },
    "391": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1406.3676",
        "transcript": "#### Introduction\n\n* Open-domain Question Answering (Open QA) - efficiently querying large-scale knowledge base(KB) using natural language.\n* Two main approaches:\n    * Information Retrieval\n        * Transform question (in natural language) into a valid query(in terms of KB) to get a broad set of candidate answers.\n        * Perform fine-grained detection on candidate answers.\n    * Semantic Parsing\n        * Interpret the correct meaning of the question and convert it into an exact query.\n* Limitations:\n    * Human intervention to create lexicon, grammar, and schema.\n* This work builds upon the previous work where an embedding model learns low dimensional vector representation of words and symbols.\n* [Link](https://arxiv.org/abs/1406.3676) to the paper.\n\n#### Task Definition\n\n* Input - Training set of questions (paired with answers).\n* KB providing a structure among the answers.\n* Answers are entities in KB and questions are strings with one identified KB entity.\n* The paper has used FREEBASE as the KB.\n* Datasets\n    * WebQuestions - Built using FREEBASE, Google Suggest API, and Mechanical Turk.\n    * FREEBASE triplets transformed into questions.\n    * Clue Web Extractions dataset with entities linked with FREEBASE triplets.\n    * Dataset of paraphrased questions using WIKIANSWERS.\n\n#### Embedding Questions and Answers\n\n* Model learns low-dimensional vector embeddings of words in question entities and relation types of FREEBASE such that questions and their answers are represented close to each other in the joint embedding space.\n* Scoring function $S(q, a)$, where $q$ is a question and $a$ is an answer, generates high score if $a$ answers $q$.\n    * $S(q, a) = f(q)^{T} . g(a)$\n    * $f(q)$ maps question to embedding space. \n    * $f(q) = W \\phi (q)$\n    * $W$ is a matrix of dimension $K * N$\n    * $K$ - dimension of embedding space (hyper parameter).\n    * $N$ - total number of words/entities/relation types.\n    * $\\psi(q)$ - Sparse Vector encoding the number of times a word appears in $q$.\n* Similarly, $g(a) = W \\psi (a)$ maps answer to embedding space.\n* $\\psi(a)$ gives answer representation, as discussed below.\n\n#### Possible Representations of Candidate Answers\n\n* Answer represented as a **single entity** from FREEBASE and TBD is a one-of-N encoded vector.\n* Answer represented as a **path** from question to answer. The paper considers only one or two hop paths resulting in 3-of-N or 4-of-N encoded vectors(middle entities are not recorded).\n* Encode the above two representations using **subgraph representation** which represents both the path and the entire subgraph of entities connected to answer entity as a subgraph. Two embedding representations are used to differentiate between entities in path and entities in the subgraph.\n* SubGraph approach is based on the hypothesis that including more information about the answers would improve results.\n\n#### Training and Loss Function\n\n* Minimize margin based ranking loss to learn matrix $W$.\n* Stochastic Gradient Descent, multi-threaded with Hogwild.\n\n#### Multitask Training of Embeddings\n\n* To account for a large number of synthetically generated questions, the paper also multi-tasks the training of model with paraphrased prediction.\n* Scoring function $S_{prp} (q1, q2) = f(q1)^{T} f(q2)$, where $f$ uses the same weight matrix $W$ as before.\n* High score is assigned if $q1$ and $q2$ belong to same paraphrase cluster.\n* Additionally, the model multitasks the task of mapping embeddings of FREEBASE entities (mids) to actual words.\n\n#### Inference\n\n* For each question, a candidate set is generated.\n* The answer (from candidate set) with the highest set is reported as the correct answer.\n* Candidate set generation strategy\n    * $C_1$ - All KB triplets containing the KB entity from the question forms a candidate set. Answers would be limited to 1-hop paths.\n    * $C_2$ - Rank all relation types and keep top 10 types and add only those 2-hop candidates where the selected relations appear in the path.\n    \n#### Results\n\n* $C_2$ strategy outperforms $C_1$ approach supporting the hypothesis that a richer representation for answers can store more information.\n* Proposed approach outperforms the baseline methods but is outperformed by an ensemble of proposed approach with semantic parsing via paraphrasing model.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1406.3676"
    },
    "392": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1502.05698",
        "transcript": "#### Introduction\n\nThe [paper](http://arxiv.org/pdf/1502.05698v10) presents a framework and a set of synthetic toy tasks (classified into skill sets) for analyzing the performance of different machine learning algorithms.\n\n#### Tasks\n\n* **Single/Two/Three Supporting Facts**: Questions where a single(or multiple) supporting facts provide the answer. More is the number of supporting facts, tougher is the task.\n* **Two/Three Supporting Facts**: Requires differentiation between objects and subjects.\n* **Yes/No Questions**: True/False questions.\n* **Counting/List/Set Questions**: Requires ability to count or list objects having a certain property.\n* **Simple Negation and Indefinite Knowledge**: Tests the ability to handle negation constructs and model sentences that describe a possibility and not a certainty.\n* **Basic Coreference, Conjunctions, and Compound Coreference**: Requires ability to handle different levels of coreference.\n* **Time Reasoning**: Requires understanding the use of time expressions in sentences.\n* **Basic Deduction and Induction**: Tests basic deduction and induction via inheritance of properties.\n* **Position and Size Reasoning**\n* **Path Finding**: Find path between locations.\n* **Agent's Motivation**: Why an agent performs an action ie what is the state of the agent.\n\n#### Dataset\n\n* The dataset is available [here](https://research.facebook.com/research/-babi/) and the source code to generate the tasks is available [here](https://github.com/facebook/bAbI-tasks).\n* The different tasks are independent of each other.\n* For supervised training, the set of relevant statements is provided along with questions and answers.\n* The tasks are available in English, Hindi and shuffled English words.\n\n#### Data Simulation\n\n* Simulated world consists of entities of various types (locations, objects, persons etc) and of various actions that operate on these entities. \n* These entities have their internal state and follow certain rules as to how they interact with other entities.\n* Basic simulations are of the form: <actor> <action> <object> eg Bob go school.\n* To add variations, synonyms are used for entities and actions.\n\n#### Experiments\n\n##### Methods\n\n* N-gram classifier baseline\n* LSTMs\n* Memory Networks (MemNNs)\n* Structured SVM incorporating externally labeled data\n\n##### Extensions to Memory Networks\n\n* **Adaptive Memories** - learn the number of hops to be performed instead of using the fixed value of 2 hops.\n* **N-grams** - Use a bag of 3-grams instead of a bag-of-words.\n* **Nonlinearity** - Apply 2-layer neural network with *tanh* nonlinearity in the matching function.\n\n##### Structured SVM\n\n* Uses coreference resolution and semantic role labeling (SRL) which are themselves trained on a large amount of data.\n* First train with strong supervision to find supporting statements and then use a similar SVM to find the response.\n\n##### Results\n\n* Standard MemNN outperform N-gram and LSTM but still fail on a number of tasks.\n* MemNNs with Adaptive Memory improve the performance for multiple supporting facts task and basic induction task.\n* MemNNs with N-gram modeling improves results when word order matters.\n* MemNNs with Nonlinearity performs well on Yes/No tasks and indefinite knowledge tasks.\n* Structured SVM outperforms vanilla MemNNs but not as good as MemNNs with modifications.\n* Structured SVM performs very well on path finding task due to its non-greedy search approach.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1502.05698v10"
    },
    "393": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1602.00370",
        "transcript": "#### Introduction\n\n* LargeVis - a technique to visualize large-scale and high-dimensional data in low-dimensional space.\n* Problem relates to both information visualization and machine learning (and data mining) domain.\n* [Link to the paper](https://arxiv.org/abs/1602.00370)\n\n#### t-SNE\n\n* State of the art method for visualization problem.\n* Start by constructing a similarity structure from the data and then project the structure to 2/3 dimensional space.\n* An accelerated version proposed that uses a K-nearest Neighbor (KNN) graph as the similarity structure.\n* Limitations\n    * Computational cost of constructing KNN graph for high-dimensional data.\n    * Efficiency of graph visualization techniques breaks down for large data ($O(NlogN)$ complexity).\n    * Parameters are sensitive to the dataset.\n\n#### LargeVis\n\n* Constructs KNN graph (in a more efficient manner as compared to t-SNE).\n* Uses a principled probabilistic model for graph visualization.\n* Let us say the input is in the form of $N$ datapoints in $d$ dimensional space.\n\n##### KNN Graph Construction\n\n* Random projection tree used for nearest-neighborhood search in the high-dimensional space with euclidean distance as metric of distance.\n* Tree is constructed by partitioning the entire space and the nodes in the tree correspond to subspaces.\n* For every non-leaf node of the tree, select a random hyperplane that splits the subspace (corresponding to the leaf node) into two children.\n* This is done till the number of nodes in the subspace reaches a threshold.\n* Once the tree is constructed, each data point gets assigned a leaf node and points in the subspace of the leaf node are the candidate nearest neighbors of that datapoint.\n* For high accuracy, a large number of trees should be constructed (which increases the computational cost).\n* The paper counters this bottleneck by using the idea \"a neighbor of my neighbor is also my neighbor\" to increase the accuracy of the constructed graph.\n* Basically construct an approximate KNN graph using random projection trees and then for each node, search its neighbor's neighbors as well.\n* Edges are assigned weights just like in t-SNE.\n\n##### Probabilistic Model For Graph Visualization\n\n* Given a pair of vertices, the probability of observing an edge between them is given as a function of the distance between the projection of the pair of vertices in the lower dimensional space.\n* The probability of observing an edge with weight $w$ is given as $w_{th}$ power of probability of observing an edge.\n* Given a weighted graph, $G$, the likelihood of the graph can be modeled as the likelihood of observed edges plus the likelihood of negative edges (vertex pairs without edges).\n* To simplify the objective function, some negative edges are sampled corresponding to each vertex using a noisy distribution.\n* The edges are sampled with probability proportional to their weight and then treated as binary edges.\n* The resulting objective function can be optimized using asynchronous stochastic gradient descent (very effective on sparse graphs).\n* The overall complexity is $O(sMN)$, $s$ is the dimension of lower dimensional space, $M$ is the number of negative samples and $N$ is the number of vertices.\n\n#### Experiments\n\n* Data is first preprocessed with embedding learning techniques like SkipGram and LINE and brought down to 100-dimensional space. \n* One limitation is that the data is preprocessed to reduce the number of dimensions to 100. This seems to go against the claim of scaling for hundreds of dimensions.\n* KNN construction is fastest for LargeVis followed by random projection trees, NN Descent, and vantage point trees (used in t-SNE).\n* LargeVis requires very few iterations to create highly accurate KNN graphs.\n* KNN Classifier is used to measure the accuracy of graph visualization quantitatively.\n* Compared to t-SNE, LargeVis is much more stable with respect to learning rate across datasets.\n* LargeVis benefits from its linear complexity (as compared to t-SNE's log linear complexity) and for default learning rate, outperforms t-SNE for larger datasets.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1602.00370"
    },
    "394": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Kim14f",
        "transcript": "#### Introduction\n\n* The paper demonstrates how simple CNNs, built on top of word embeddings, can be used for sentence classification tasks.\n* [Link to the paper](https://arxiv.org/abs/1408.5882)\n* [Implementation](https://github.com/shagunsodhani/CNN-Sentence-Classifier)\n\n#### Architecture\n\n* Pad input sentences so that they are of the same length.\n* Map words in the padded sentence using word embeddings (which may be either initialized as zero vectors or initialized as word2vec embeddings) to obtain a matrix corresponding to the sentence.\n* Apply convolution layer with multiple filter widths and feature maps.\n* Apply max-over-time pooling operation over the feature map.\n* Concatenate the pooling results from different layers and feed to a fully-connected layer with softmax activation.\n* Softmax outputs probabilistic distribution over the labels.\n* Use dropout for regularisation.\n\n#### Hyperparameters\n\n* RELU activation for convolution layers\n* Filter window of 3, 4, 5 with 100 feature maps each.\n* Dropout - 0.5\n* Gradient clipping at 3\n* Batch size - 50\n* Adadelta update rule.\n\n#### Variants\n\n* CNN-rand\n    * Randomly initialized word vectors.\n* CNN-static\n    * Uses pre-trained vectors from word2vec and does not update the word vectors.\n* CNN-non-static\n    * Same as CNN-static but updates word vectors during training.\n* CNN-multichannel\n    * Uses two set of word vectors (channels).\n    * One set is updated and other is not updated.\n\n#### Datasets\n\n* Sentiment analysis datasets for Movie Reviews, Customer Reviews etc.\n* Classification data for questions.\n* Maximum number of classes for any dataset - 6\n\n#### Strengths\n\n* Good results on benchmarks despite being a simple architecture.\n* Word vectors obtained by non-static channel have more meaningful representation. \n\n#### Weakness\n\n* Small data with few labels.\n* Results are not very detailed or exhaustive.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1408.5882"
    },
    "395": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=vandermaaten2008visualizing",
        "transcript": "#### Introduction\n\n* Method to visualize high-dimensional data points in 2/3 dimensional space.\n* Data visualization techniques like Chernoff faces and graph approaches just provide a representation and not an interpretation.\n* Dimensionality reduction techniques fail to retain both local and global structure of the data simultaneously. For example, PCA and MDS are linear techniques and fail on data lying on a non-linear manifold.\n* t-SNE approach converts data into a matrix of pairwise similarities and visualizes this matrix.\n* Based on SNE (Stochastic Neighbor Embedding)\n* [Link to paper](http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)\n\n#### SNE\n\n* Given a set of datapoints $x_1, x_2, ...x_n, p_{i|j}$ is the probability that $x_i$ would pick $x_j$ as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at $x_i$. Calculation of $\\sigma_i$ is described later.\n* Similarly, define $q_{i|j}$ as conditional probability corresponding to low-dimensional representations of $y_i$ and $y_j$ (corresponding to $x_i$ and $x_j$). The variance of Gaussian in this case is set to be $1/\\sqrt{2}$\n* Argument is that if $y_i$ and $y_j$ captures the similarity between $x_i$ and $x_j$, $p_{i|j}$ and $q_{i|j}$ should be equal. So objective function to be minimized is Kullback-Leibler (KL) Divergence measure for $P_i$ and $Q_i$, where $P_i$ ($Q_i$) represent conditional probability distribution given $x_i$ ($y_i$)\n* Since KL Divergence is not symmetric, the objective function focuses on retaining the local structure.\n* Users specify a value called perplexity (measure of effective number of neighbors). A binary search is performed to find $\\sigma_i$ which produces the $P_i$ having same perplexity as specified by the user.\n* Gradient Descent (with momentum) is used to minimize objective function and Gaussian noise is added in early stages to perform simulated annealing.\n\n#### t-SNE (t-Distributed SNE)\n\n##### Symmetric SNE\n\n* A single KL Divergence between P (joint probability distribution in high-dimensional space) and Q (joint probability distribution in low-dimensional space) is minimized.\n* Symmetric because $p_{i|j} = p_{j|i}$ and $q_{i|j} = q_{j|i}$\n* More robust to outliers and has a simpler gradient expression.\n\n##### Crowding Problem\n\n* When we model a high-dimensional dataset in 2 (or 3) dimensions, it is difficult to segregate the nearby datapoints from moderately distant datapoints and gaps can not form between natural clusters.\n* One way around this problem is to use UNI-SNE but optimization of the cost function, in that case, is difficult.\n\n##### t-SNE\n\n* Instead of Gaussian, use a heavy-tailed distribution (like Student-t distribution) to convert distances into probability scores in low dimensions. This way moderate distance in high-dimensional space can be modeled by larger distance in low-dimensional space.\n* Student-t distribution is an infinite mixture of Gaussians and density for a point under this distribution can be computed very fast. \n* The cost function is easy to optimize.\n\n##### Optimization Tricks\n\n###### Early Compression\n\n* At the start of optimization, force the datapoints (in low-dimensional space) to stay close together so that datapoints can easily move from one cluster to another. \n* Implemented an L2-penalty term proportional to the sum of the squared distance of datapoints from the origin.\n\n###### Early Exaggeration\n\n* Scale all the $p_{i|j}$'s so that large $q_{i|j}$*'s are obtained with the effect that natural clusters in the data form tight, widely separated clusters as a lot of empty space is created in the low-dimensional space.\n\n##### t-SNE on large datasets\n\n* Space and time complexity is quadratic in the number of datapoints so infeasible to apply on large datasets.\n* Select a random subset of points (called landmark points) to display.\n* for each landmark point, define a random walk starting at a landmark point and terminating at any other landmark point.\n* $p_{i|j}$ is defined as fraction of random walks starting at $x_i$ and finishing at $x_j$ (both these points are landmark points). This way, $p_{i|j}$ is not sensitive to \"short-circuits\" in the graph (due to noisy data points).\n\n#### Advantages of t-SNE\n\n* Gaussian kernel employed by t-SNE (in high-dimensional) defines a soft border between the local and global structure of the data.\n* Both nearby and distant pair of datapoints get equal importance in modeling the low-dimensional coordinates.\n* The local neighborhood size of each datapoint is determined on the basis of the local density of the data.\n* Random walk version of t-SNE takes care of \"short-circuit\" problem.\n\n#### Limitations of t-SNE\n\n* It is unclear t-SNE would perform on general **Dimensionality Reduction** for more than 3 dimensions. For such higher (than 3) dimensions, Student-t distribution with more degrees of freedom should be more appropriate.\n* t-SNE reduces the dimensionality of data mainly based on local properties of the data which means it would fail in data which has intrinsically high dimensional structure (**curse of dimensionality**).\n* The cost function for t-SNE is not convex requiring several optimization parameters to be chosen which would affect the constructed solution.",
        "sourceType": "blog",
        "linkToPaper": "http://www.jmlr.org/papers/v9/vandermaaten08a.html"
    },
    "396": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1145/1553374.1553380",
        "transcript": "### Introduction\n\n* *Curriculum Learning*  - When training machine learning models, start with easier subtasks and gradually increase the difficulty level of the tasks.\n* Motivation comes from the observation that humans and animals seem to learn better when trained with a curriculum like a strategy.\n* [Link](http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf) to the paper.\n\n### Contributions of the paper\n\n* Explore cases that show that curriculum learning benefits machine learning.\n* Offer hypothesis around when and why does it happen.\n* Explore relation of curriculum learning with other machine learning approaches.\n\n### Experiments with convex criteria\n\n* Training perceptron where some input data is irrelevant(not predictive of the target class).\n* Difficulty can be defined in terms of the number of irrelevant samples or margin from the separating hyperplane.\n* Curriculum learning model outperforms no-curriculum based approach.\n* Surprisingly, in the case of difficulty defined in terms of the number of irrelevant examples, the anti-curriculum strategy also outperforms no-curriculum strategy.\n\n### Experiments on shape recognition with datasets having different variability in shapes\n\n* Standard(target) dataset - Images of rectangles, ellipses, and triangles.\n* Easy dataset - Images of squares, circles, and equilateral triangles.\n* Start performing gradient descent on easy dataset and switch to target data set at a particular epoch (called *switch epoch*). \n* For no-curriculum learning, the first epoch is the *switch epoch*.\n* As *switch epoch* increases, the classification error comes down with the best performance when *switch epoch* is half the total number of epochs.\n* Paper does not report results for higher values of *switch epoch*.\n\n### Experiments on language modeling\n\n* Standard data set is the set of all possible windows of the text of size 5 from Wikipedia where all words in the window appear in 20000 most frequent words.\n* Easy dataset considers only those windows where all words appear in 5000 most frequent words in vocabulary.\n* Each word from the vocabulary is embedded into a *d* dimensional feature space using a matrix **W** (to be learnt).\n* The model predicts the score of next word, given a window of words.\n* Expected value of ranking loss function is minimized to learn **W**.\n* Curriculum Learning-based model overtakes the other model soon after switching to the target vocabulary, indicating that curriculum-based model quickly learns new words.\n\n### Curriculum as a continuation method\n\n* Continuation methods start with a smoothed objective function and gradually move to less smoothed function.\n* Useful in the case where the objective function in non-convex.\n* Consider a family of cost functions $C_\\lambda (\\theta)$ such that  $C_0(\\theta)$ can be easily optimized and $C_1(\\theta)$ is the actual objective function.\n* Start with $C_0 (\\theta)$ and increase $\\lambda$, keeping $\\theta$ at a local minimum of $C_\\lambda (\\theta)$.\n* Idea is to move $\\theta$ towards a dominant (if not global) minima of $C_1(\\theta)$.\n* Curriculum learning can be seen as a sequence of training criteria starting with an easy-to-optimise objective and moving all the way to the actual objective.\n* The paper provides a mathematical formulation of curriculum learning in terms of a target training distribution and a weight function (to model the probability of selecting anyone training example at any step).\n\n### Advantages of Curriculum Learning\n\n* Faster training in the online setting as learner does not try to learn difficult examples when it is not ready.\n* Guiding training towards better local minima in parameter space, specifically useful for non-convex methods.\n\n### Relation to other machine learning approaches\n\n* **Unsupervised preprocessing** - Both have a regularizing effect and lower the generalization error for the same training error.\n* **Active learning** - The learner would benefit most from the examples that are close to the learner's frontier of knowledge and are neither too hard nor too easy.\n* **Boosting Algorithms** - Difficult examples are gradually emphasised though the curriculum starts with a focus on easier examples and the training criteria do not change.\n* **Transfer learning** and **Life-long learning** - Initial tasks are used to guide the optimisation problem.\n\n### Criticism\n\n* Curriculum Learning is not well understood, making it difficult to define the curriculum.\n* In one of the examples, anti-curriculum performs better than no-curriculum. Given that curriculum learning is modeled on the idea that learning benefits when examples are presented in order of increasing difficulty, one would expect anti-curriculum to perform worse.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1145/1553374.1553380"
    },
    "397": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SukhbaatarSWF15",
        "transcript": "## Introduction\n\n* Neural Network with a recurrent attention model over a large external memory.\n* Continous form of Memory-Network but with end-to-end training so can be applied to more domains.\n* Extension of RNNSearch and can perform multiple hops (computational steps) over the memory per symbol.\n* [Link to the paper](http://arxiv.org/pdf/1503.08895v5.pdf).\n* [Link to the implementation](https://github.com/facebook/MemNN).\n\n## Approach\n\n* Model takes as input $x_1,...,x_n$ (to store in memory), query $q$ and outputs answer $a$.\n\n### Single Layer\n\n* Input set ($x_i$) embedded in D-dimensional space, using embedding using matrix $A$ to obtain memory vectors ($m_i$).\n* Query is also embedded using matrix $B$ to obtain internal state $u$.\n* Compute match between each memory $m_i$ and $u$ in the embedding space followed by softmax operation to obtain probability vector $p$ over the inputs.\n* Each $x_i$ maps to an output vector $c_i$ (using embedding matrix $C$).\n* Output $o$ = weighted sum of transformed input $c_i$, weighted by $p_i$.\n* Sum of output vector, $o$ and embedding vector, $u$, is passed through weight matrix $W$ followed by softmax to produce output.\n* $A$, $B$, $C$ and $W$ are learnt by minimizing cross entropy loss.\n\n### Multiple Layers\n\n* For layers above the first layer, input $u^{k+1} = u^k + o^k$.\n* Each layer has its own $A^k$ and $C^k$ - with constraints.\n* At final layer, output $o = \\text{softmax}(W(o^K, u^K))$ \n\n### Constraints On Embedding Vectors\n\n* Adjacent\n    * Output embedding for one layer is input embedding for another ie $A^k+1 = C^k$\n    * $W = C^k$\n    * $B = A^1$\n\n* Layer-wise (RNN-like)\n    * Same input and output embeddings across layes ie $A^1 = A^2 ... = A^K$ and $C^1 = C^2 ... = C^K$.\n    * A linear mapping $H$ is added to update of $u$ between hops. \n    * $u^{k+1} = Hu^k + o^k$.\n    * $H$ is also learnt.\n    * Think of this as a traditional RNN with 2 outputs\n        * Internal output - used for memory consideration\n        * External output - the predicted result\n        * $u$ becomes the hidden state.\n        * $p$ is an internal output which, combined with $C$ is used to update the hidden state.\n\n## Related Architectures\n\n* RNN - Memory stored as the state of the network and unusable in long temporal contexts.\n* LSTM - Locks network state using local memory cells. Fails over longer temporal contexts.\n* Memory Networks - Uses global memory.\n* Bidirectional RNN - Uses a small neural network with sophisticated gated architecture (attention model) to find useful hidden states but unlike MemNN, perform only a single pass over the memory.\n\n## Sentence Representation for Question Answering Task\n\n* Bag-of-words representation\n    * Input sentences and questions are embedded as a bag of words.\n    * Can not capture the order of the words.\n\n* Position Encoding\n    * Takes into account the order of words.\n\n* Temporal Encoding\n    * Temporal information encoded by matrix $T_A$ and memory vectors are modified as \n\n    $m_i = \\text{sum}(Ax_{ij}) + T_A(i)$\n\n* Random Noise\n    * Dummy Memories (empty memories) are added at training time to regularize $T_A$.\n\n* Linear Start (LS) training\n    * Removes softmax layers when starting training and insert them when validation loss stops decreasing.\n\n## Observations\n\n* Best MemN2N models are close to supervised models in performance.\n* Position Encoding improves over bag-of-words approach.\n* Linear Start helps to avoid local minima.\n* Random Noise gives a small yet consistent boost in performance.\n* More computational hops leads to improved performance.\n* For Language Modelling Task, some hops concentrate on recent words while other hops have more broad attention span over all memory locations.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5846-end-to-end-memory-networks"
    },
    "398": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.03126",
        "transcript": "### Introduction\n\n* Knowledge Bases (KBs) are effective tools for Question Answering (QA) but are often too restrictive (due to fixed schema) and too sparse (due to limitations of Information Extraction (IE) systems).\n* The paper proposes Key-Value Memory Networks, a neural network architecture based on [Memory Networks](https://gist.github.com/shagunsodhani/c7a03a47b3d709e7c592fa7011b0f33e) that can leverage both KBs and raw data for QA.\n* The paper also introduces MOVIEQA, a new QA dataset that can be answered by a perfect KB, by Wikipedia pages and by an imperfect KB obtained using IE techniques thereby allowing a comparison between systems using any of the three sources.\n* [Link to the paper](https://arxiv.org/abs/1606.03126).\n\n### Related Work\n\n* TRECQA and WIKIQA are two benchmarks where systems need to select the sentence containing the correct answer, given a question and a list of candidate sentences. \n* These datasets are small and make it difficult to compare the  systems using different sources.\n* Best results on these benchmarks are reported by CNNs and RNNs with attention mechanism.\n\n### Key-Value Memory Networks\n\n* Extension of [Memory Networks Model](https://gist.github.com/shagunsodhani/c7a03a47b3d709e7c592fa7011b0f33e).\n* Generalises the way context is stored in memory.\n* Comprises of a memory made of slots in the form of pair of vectors $(k_{1}, v_{1})...(k_{m}, v_{m})$ to encode long-term and short-term context.\n\n#### Reading the Memory\n\n* **Key Hashing** - Question, *x* is used to preselect a subset of array $(k_{h1}, v_{h1})...(k_{hN}, v_{hN})$ where the key shares atleast one word with *x* and frequency of the words is less than 1000.\n* **Key Addresing** - Each candidate memory is assigned a relevance probability:\n    * $p_hi$ = softmax($A\u03c6_X(x).A\u03c6_K (k_{hi}))$\n    * \u03c6 is a feature map of dimension *D* and *A* is a *dxD* matrix.\n* **Value Reading** - Value of memories are read by taking their weighted sum using addressing probabilites and a vector *o* is returned.\n* $o = sum(p_{hi} A\\phi_V(v_{hi}))$\n* Memory access process conducted by \"controller\" neural network using $q = A\u03c6_X (x)$ as the query.\n* Query is updated using\n    * $q_2 = R_1 (q+o)$\n* Addressing and reading steps are repeated using new $R_i$ matrices to retrieve more pertinent information in subsequent access.\n* After a fixed number of hops, H, resulting state of controller is used to compute a final prediction.\n* $a = \\text{argmax}(\\text{softmax}(q_{H+1}^T B\\phi_Y (y_i)))$\nwhere $y_i$ are the possible candidate outputs and $B$ is a $dXD$ matrix.\n* The network is trained end-to-end using a cross entropy loss, backpropogation and stochastic gradient.\n* End-to-End Memory Networks can be viewed as a special case of Key-Value Memory Networks by setting key and value to be the same for all the memories.\n\n#### Variants of Key-Value Memories\n\n* $\u03c6_x$ and $\u03c6_y$ - feature map corresponding to query and answer are fixed as bag-of-words representation.\n\n##### KB Triple\n\n* Triplets of the form \"subject relation object\" can be represented in Key-Value Memory Networks with subject and relation as the key and object as the value.\n* In standard Memory Networks, the whole triplet would have to be embedded in the same memory slot.\n* The reversed relations \"object is_related_to subject\" can also be stored.\n\n##### Sentence Level\n\n* A document can be split into sentences with each sentence encoded in the key-value pair of the memory slot as a bag-of-words.\n\n##### Window Level\n\n* Split the document in the windows of W words and represent it as bag-of-words. \n* The window becomes the key and the central word becomes the value.\n\n##### Window + Centre Encoding\n\n* Instead of mixing the window centre with the rest of the words, double the size of the dictionary and encode the centre of the window and the value using the second dictionary.\n\n##### Window + Title\n\n* Since the title of the document could contain useful information, the word window can be encoded as the key and document title as the value.\n* The key could be augmented with features like \"_window_\" and \"_title_\" to distinguish between different cases.\n\n### MOVIEQA Benchmark\n\n#### Knowledge Representation\n\n* Doc - Raw documents (from Wikipedia) related to movies.\n* KB - Graph-based KB made of entities and relations.\n* IE - Performing Information Extraction on Wikipedia to create a KB.\n* The QA pairs should be answerable by both raw document and KB so that the three approaches can be compared and the gap between the three solutions can be closed.\n* The dataset has more than 100000 QA pairs, making it much larger than most existing datasets.\n\n### Experiments\n\n#### MOVIEQA\n\n##### Systems Compared\n\n* [Bordes et al's QA system](TBD)\n* [Supervised Embeddings](TBD)(without KB)\n* [Memory Networks](TBD)\n* Key-Value Memory Networks\n\n##### Observations\n\n* Key-Value Memory Networks outperforms all methods on all data sources.\n* KB > Doc > IE\n* The best memory representation for directly reading documents uses \"Window Level + Centre Encoding + Title\".\n\n##### KB vs Synthetic Document Analysis\n\n* Given KB triplets, construct synthetic \"Wikipedia\" articles using templates, conjunctions and coreferences to determine the causes for the gap in performance when using KB vs doc.\n* Loss in One Template sentences are due to the difficulty of extracting subject, relation and object from the artificial docs. \n* Using multiple templates does not deteriorate performance much. But conjunctions and coreferences cause a dip in performance.\n\n#### WIKIQA\n\n* Given a question, select the sentence (from Wikipedia document) that best answers the question.\n* Key-Value Memory Networks outperforms all other solutions though it is only marginally better than LDC and attentive models based on CNNs and RNNs.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.03126"
    },
    "399": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZarembaS14",
        "transcript": "## Problem Statement\n\n* Evaluating if LSTMs can express and learn short, simple programs (linear time, constant memory) in the sequence-to-sequence framework.\n* [Link to paper](http://arxiv.org/pdf/1410.4615v3.pdf)\n\n\n## Approach\n\n* Formulate program evaluation task as a sequence-to-sequence learning problem using RNNs.\n* Train on short programs that can be evaluated in linear time and constant memory - RNN can perform only a single pass over the data and its memory is limited.\n* Two parameters to control the difficulty of the program:\n    * `length` : Number of digits in the integer that appears in the program.\n    * `nesting` : Number of times operations can be combined with each other.\n* LSTM reads the input program, one character at a time and produces output, one character at a time.\n\n### Additional Learning Tasks\n\n* **Addition Task** - Given two numbers, the model learns to add them. This task becomes the baseline for comparing performance on other tasks.\n* **Memorization Task** - Give a random number, the model memorizes it and outputs it. Following techniques enhance the accuracy of the model:\n    * **Input reversing** - Reversing the order of input, while keeping the output fixed introduces many short-term dependencies that help LSTM in learning the process.\n    * **Input doubling** - Presenting the same input to the network twice enhances the performance as the model gets to look at the input twice.\n\n## Curriculum learning\n\nGradually increase the difficulty of the program fed to the system.\n\n* **No Curriculum (baseline)** - Fixed `length` and fixed `nesting` programs are fed to the system.\n* **Naive Curriculum** - Start with `length` = 1 and `nesting` = 1 and keep increasing the values iteratively.\n* **Mix Strategy** - Randomly choose `length` and `nesting` to generate a mix of easy and difficult examples.\n* **Combined Strategy** - Each training example is obtained either by Naive curriculum strategy or mix strategy.\n\n## Network Architecture\n\n* 2 layers, unrolled for 50 steps.\n* 400 cells per layer.\n* Parameters initialized uniformly in [-0.08, 0.08]\n* minibatch size 100\n* norm of gradient normalized to be less than 5\n* start with learning rate = 0.5, further decreased by 0.8 after reaching target accuracy of 95%\n\n## Observations\n\nTeacher forcing technique used for computing accuracy ie when predicting the $i_{th}$ digit, the correct first i-1 digits of the output are provided as input to the LSTM.\n\nThe general trend is (combine, mix) > (naive, baseline).\n\nIn certain cases for program evaluation, baseline performs better than naive curriculum strategy. Intuitively, the model would use all its memory to store patterns for a given size input. Now when a higher size input is provided, the model would have to restructure its memory patterns to learn the output for this new class of inputs. The process of memory restructuring may be causing the degraded performance of the naive strategy. The combined strategy combines the naive and mix strategy and hence reduces the need to restructure the memory patterns.\n\nWhile LSTMs can learn to map the character level representation of simple programs to their correct output, the idea can not extend to arbitrary programs due to the runtime limitations of conventional RNNs and LSTM. Moreover, while learning is essential, the optimal curriculum strategy needs to be understood further.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1410.4615"
    },
    "400": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1204.6078",
        "transcript": "#### Introduction\n\n* GraphLab abstraction exposes asynchronous, dynamic, graph-parallel computation model in the shared-memory setting.\n* This paper extends the abstraction to the distributed setting.\n* [Link](http://vldb.org/pvldb/vol5/p716_yuchenglow_vldb2012.pdf) to the paper.\n\n#### Characteristics of MLDM (Machine Learning and Data Mining)\n\n* Graph Structured Computation\n    * Sometimes computation requires modeling dependencies between data.\n    * eg modeling dependencies between similar users for the recommendation use case.\n* Asynchronous Iterative Computation \n    * In many cases, asynchronous procedures outperform synchronous ones.\n    * eg linear systems, belief propagation, stochastic optimization etc.\n* Dynamic Computation\n    * Iterative computation converges asymmetrically.\n    * Convergence can be accelerated by dynamic scheduling.\n    * eg do not update parameters that have already converged. \n* Serializability\n    * Ensuring that all parallel executions have an equivalent serial execution is desirable for both correctness and faster convergence. \n\n#### GraphLab Abstraction\n\n### Data Graph\n* Store program state as a directed graph.\n* **G = (V,E,D)** where D is the user defined data (model parameters, algorithm state, statistical data etc).\n* The graph data **D** is mutable but the state of the graph **(V,E)** is immutable.\n\n#### Update Function\n* Stateless procedure that modifies the data within the scope of a vertex and schedules the execution of the *update* function on other vertices.\n* **Scope** of a vertex (S) - data corresponding to the vertex, its edges and its adjacent vertices.\n* **update: $f (v, S_v) -> (S_v, T)$** where T is the set of vertices where *update* function is scheduled to be invoked.\n* Scheduling of computation id decoupled from movement of data and no message passing is required between vertices.\n\n#### Execution Model\n* Input to the model is G and T, the initial set of vertices to be updated.\n* During each step, a vertex is extracted from T, updated and a set of vertices is added to T (for future computation).\n* Vertices in T can be executed in any order with the only constraint that all vertices be eventually executed.\n\n#### Sync Operation\n* Sync operation runs in the background to maintain global aggregates concurrently.\n* These global values are read by *update* function and written by the sync operation.\n\n#### Consistency Models\n* Full consistency\n    * Full read/write access in the *scope*.\n    * Scope of concurrently updating vertices cannot overlap.\n* Edge consistency\n    * Read/write access on the vertex and the adjacent edges but only read access to adjacent vertices.\n    * Slightly overlapping scope.\n* Vertex consistency\n    * Write access to the vertex and read access to adjacent edges and vertices.\n    * All vertices can run update function simultaneously.\n\n\n### Distributed Data Graph\n\n* Two-phase partitioning process for load balancing the graph on arbitrary cluster size.\n* In the first phase, partition the graph into k parts (k >> number of machines).\n* Each part, called **atom**, is a file of graph generating commands.\n* Atom also stores information about **ghosts** (set of vertices and edges adjacent to the partition boundary).\n* Atom index file contains connectivity structure and file location for the k atoms as a meta-graph.\n* In the second phase, this meta-graph is partitioned over the physical machines.\n\n### Distributed GraphLab Engines\n\n#### Chromatic Engine\n\n* A vertex coloring (no adjacent vertices have the same color) is constructed to serialize parallel execution of dependent tasks (in our case, vertices in the graph).\n* For edge consistency model, execute all vertices of the same color before going to next color and run sync operation between color steps.\n* Changes to ghost vertices and edges are communicated asynchronously as they are made.\n* Vertex consistency is trivial - assign same color to all the vertices.\n* For full consistency, construct second-order vertex coloring (no vertex shares the same color as any of its distance two neighbors)\n\n#### Distributed Locking Engine\n\n* Associate reader-writer locks on each vertex.\n* Each machine can update only the local vertices.\n* Optimisations\n    * Ghosting system uses caching to eliminate wait on remote, unchanged data.\n    * Lock request and synchronization are pipelined to hide network latency.  \n        * Each machine maintains a pipeline of vertices for which locks have been requested but not granted.\n        * A vertex is executed once lock acquisition and data synchronization are complete.\n        * Nonblocking reader-writer locks, that work through callback functions, are used.\n\n### Fault Tolerance\n\n* Distributed checkpointing via two modes:\n* Synchronous checkpointing\n    * Suspend computation to save all modified data since the last checkpoint.\n* Asynchronous checkpointing based on Chandy-Lamport snapshot algorithm.\n    * The snapshot step becomes an *update* function in the GraphLab abstraction.\n    * Better than synchronous checkpointing.\n\n### System Design\n* One instance of GraphLab runs on each machine.\n* These processes are symmetric and communicate via RPC.\n* The first process additionally acts as the master and computes placement of atoms based on atom index.\n* Each process maintains a local scheduler (for its vertices) and a cache to access remote data.\n* Distributed consensus algorithm to decide when all the schedulers are empty.\n\n### Observations\n* The biggest strength of the paper are its extensive experiments.\n* GraphLab benefits from the use of background asynchronous communication and pipelined locking but its communication layer is not as efficient as MPI's communication layer.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1204.6078v1"
    },
    "401": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DeanCMCDLMRSTYN12",
        "transcript": "## Introduction\n\n* In machine learning, accuracy tends to increase with an increase in the number of training examples and number of model parameters.\n* For large data, training becomes slow on even GPU (due to increase CPU-GPU data transfer).\n* Solution: Distributed training and inference - DistBelief\n* [Link to paper](https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf)\n\n## DistBelief\n\n* Framework for parallel and distributed computation in neural networks.\n* Exploits both model parallelism and data parallelism.\n* Two methods implemented over DistBelief - *Downpour SGD* and *Sandblaster L-BFGS*\n\n## Previous work in this domain\n\n* Distributed Gradient Computation using relaxed synchronization requirements and delayed gradient descent.\n* Lock-less asynchronous stochastic gradient descent in case of sparse gradient.\n* Deep Learning \n    * Most focus has been on training small models on a single machine.\n    * Train multiple, small models on a GPU farm and combine their predictions.\n    * Alternate approach is to modify standard deep networks to make them more parallelizable.\n* Distributed computational tools\n    * MapReduce - Not suited for iterative computations\n    * GraphLab - Can not exploit computing efficiencies available in deep networks.\n\n## Model Parallelism\n\n* User specifies the computation at each node in each layer of the neural network and the messages to be exchanged between different nodes.\n* Model parallelism supported using multithreading (within a machine) and message passing (across machines).\n* Model can be partitioned across machines with DistBelief taking care of all communication/synchronization tasks.\n* Diminishing returns in terms of speed gain if too many machines/partitions are used as partitioning benefits as long as network overhead is small.\n\n## Data Parallelism\n\n* Distributed training across multiple model instances (or replicas) to optimize a single function.\n\n## Distributed Optimization Algorithms\n\n### Downpour SGD\n\n* Asynchronous stochastic gradient descent.\n* Divide training data into subsets and run a copy of model on each subset.\n* Centralized sharded parameter server\n    * Keeps the current state of all the parameters of the model.\n    * Used by model instances to share parameters.\n* Asynchronous approach as both model replicas and parameter server shards run independently.\n* Workflow\n    * Model replica requests an updated copy of model parameters from parameter server.\n    * Processes mini batches and sends the gradient to the server.\n    * Server updates the parameters.\n* Communication can be limited by limiting the rate at which parameters are requested and updates are pushed.\n* Robust to machine failure since multiple instances are running.\n* Relaxing consistency requirements works very well in practice.\n* Warm starting - Start training with a single replica and add other replicas later.\n\n### Sandblaster L-BFGS\n\n* Sandblaster is the distributed batch Optimization framework.\n* Provides distributed implementation of L-BFGS.\n* A single 'coordinator' process interacts with replicas and the parameter server to coordinate the batch optimization process.\n* Workflow\n    * Coordinator issues commands like dot product to the parameter server shards.\n    * Shards execute the operation, store the results locally and maintain additional information like history cache.\n    * Parameters are fetched at the beginning of each batch and the gradients are sent every few completed cycles.\n\n* Saves overhead of sending all parameters and gradients to a single server.\n* Robust to machine failure just like Downpour SGD. \n* Coordinator uses a load balancing scheme similar to \"backup tasks\" in MapReduce.\n\n## Observation\n\nDownpour SGD (with Adagrad adaptive learning rate) outperforms Downpour SGD (with fixed learning rate) and Sandblaster L-BFGS. Moreover, Adagrad can be easily implemented locally within each parameter shard. It is surprising that Adagrad works so well with Downpour SGD as it was not originally designed to be used with asynchronous SGD. The paper conjectures that \"*Adagrad automatically stabilizes volatile parameters in the face of the flurry of asynchronous updates, and naturally adjusts learning rates to the demands of different layers in the deep network.*\"\n\n## Beyond DistBelief - TensorFlow\n\n* In November 2015, [Google open sourced TensorFlow](http://googleresearch.blogspot.in/2015/11/tensorflow-googles-latest-machine_9.html) as its second-generation machine learning system. DistBelief had limitations like tight coupling with Google's infrastructure and was difficult to configure. TensorFlow takes care of these issues and is twice fast as DistBelief.Google also published a [white paper](http://download.tensorflow.org/paper/whitepaper2015.pdf) on the topic and the implementation can be found [here](http://www.tensorflow.org/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks"
    },
    "402": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=mikolov2013efficient",
        "transcript": "## Introduction\n\n* Introduces techniques to learn word vectors from large text datasets.\n* Can be used to find similar words (semantically, syntactically, etc).\n* [Link to the paper](http://arxiv.org/pdf/1301.3781.pdf)\n* [Link to open source implementation](https://code.google.com/archive/p/word2vec/)\n\n## Model Architecture\n\n* Computational complexity defined in terms of a number of parameters accessed during model training.\n* Proportional to $E*T*Q$\n* *E* - Number of training epochs\n* *T* - Number of words in training set\n* *Q* - depends on the model\n\n### Feedforward Neural Net Language Model (NNLM)\n\n* Probabilistic model with input, projection, hidden and output layer.\n* Input layer encodes N previous word using 1-of-V encoding (V is vocabulary size).\n* Input layer projected to projection layer P with dimensionality *N\\*D*\n* Hidden layer (of size *H*) computes the probability distribution over all words.\n* Complexity per training example $Q =N*D + N*D*H + H*V$\n* Can reduce *Q* by using hierarchical softmax and Huffman binary tree (for storing vocabulary).\n\n### Recurrent Neural Net Language Model (RNNLM)\n\n* Similar to NNLM minus the projection layer.\n* Complexity per training example $Q =H*H + H*V$\n* Hierarchical softmax and Huffman tree can be used here as well.\n\n## Log-Linear Models\n\n* Nonlinear hidden layer causes most of the complexity.\n* NNLMs can be successfully trained in two steps:\n  *  Learn continuous word vectors using simple models.\n  *  N-gram NNLM trained over the word vectors.\n\n\n### Continuous Bag-of-Words Model\n\n* Similar to feedforward NNLM.\n* No nonlinear hidden layer.\n* Projection layer shared for all words and order of words does not influence projection.\n* Log-linear classifier uses a window of words to predict the middle word.\n* $Q = N*D + D*\\log_2V$\n\n### Continuous Skip-gram Model\n\n* Similar to Continuous Bag-of-Words but uses the middle world of the window to predict the remaining words in the window.\n* Distant words are given less weight by sampling fewer distant words.\n* $Q = C*(D + D*log_2 V$) where *C* is the max distance of the word from the middle word.\n* Given a *C* and a training data, a random *R* is chosen in range *1 to C*.\n* For each training word, *R* words from history (previous words) and *R* words from future (next words) are marked as target output and model is trained.\n\n\n## Results\n\n* Skip-gram beats all other models for semantic accuracy tasks (eg - relating Athens with Greece).\n* Continuous Bag-of-Words Model outperforms other models for semantic accuracy tasks (eg great with greater) - with skip-gram just behind in performance.\n* Skip-gram architecture combined with RNNLMs outperforms RNNLMs (and other models) for Microsoft Research Sentence Completion Challenge.\n* Model can learn relationships like \"Queen is to King as Woman is to Man\". This allows algebraic operations like Vector(\"King\") - Vector(\"Man\") + Vector(\"Woman\").",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "403": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=hastieElasticNet",
        "transcript": "## Introduction to elastic net\n\n* Regularization and variable selection method.\n* Sparse Representation\n* Exihibits grouping effect.\n* Prticulary useful when number of predictors (*p*) >> number of observations (*n*).\n* LARS-EN algorithm to compute elastic net regularization path.\n\n## Lasso\n\n* Least square method with L1-penalty on regression coefficient.\n* Does continuous shrinkage and automatic variable selection\n \n ### Limitations\n \n * If *p >> n*, lasso can select at most *n* variables.\n * In the case of a group of variables exhibiting high pairwise correlation, lasso doesn't care about which variable is selected.\n * If *n > p* and there is a high correlation between predictors, ridge regression outperforms lasso.\n\n## Naive elastic net\n\n* Least square method.\n* Penalty on regression cofficients is a convex combination of lasso and ridge penalty.\n* *penalty = (1\u2212\u03b1)\\*|\u03b2| + \u03b1\\*|\u03b2|<sup>2</sup>* where *\u03b2* refers to the coefficient matrix.\n* *\u03b1 = 0* => lasso penalty\n* *\u03b1 = 1* => ridge penalty\n* Naive elastic net can be solved by transforming to lasso on augmeneted data.\n* Can be viewed as redge type shrinkage followed by lasso type thresholding.\n\n### Limitations\n\n* The two-stage procedure incurs double amount of shrinkage and introduces extra bias without reducing variance.\n\n## Bridge Regression\n\n* Generalization of lasso and ridge regression.\n* Can not produce sparse solutions.\n\n## Elastic net\n\n* Rescaled naive elastic net coefficients to undo shrinkage.\n* Retains good properties of the naive elastic net.\n\n## Justification for scaling\n\n* Elastic net becomes minimax optimal.\n* Scaling reverses the shrinkage control introduced by ridge regression.\n\n## LARS-EN\n\n* Based on LARS (used to solve lasso).\n* Elastic net can be transformed to lasso on augmented data so can reuse pieces of LARS algorithm.\n* Use sparseness to save on computation.\n\n## Conclusion\n\nElastic net performs superior to lasso.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "404": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/usenix/BronsonACCDDFGKLMPPSV13",
        "transcript": "# TAO\n\n* Geographically distributed, read-optimized, graph data store.\n* Favors availability and efficiency over consistency.\n* Developed by and used within Facebook (social graph).\n* Link to [paper](https://cs.uwaterloo.ca/~brecht/courses/854-Emerging-2014/readings/data-store/tao-facebook-distributed-datastore-atc-2013.pdf).\n\n## Before TAO\n\n* Facebook's servers directly accessed MySQL to read/write the social graph.\n* Memcache used as a look-aside cache.\n* Had several issue:\n  * **Inefficient edge list** - A key-value store is not a good design for storing a list of edges.\n  * **Distributed Control Logic** - In look-aside cache architecture, the control logic runs on the clients which increase the number of failure modes.\n  * **Expensive Read-After-Write Consistency** - Facebook used asynchronous master-slave replication for MySQL which introduced a time lag before latest data would reflect in the local replicas.\n\n## TAO Data Model\n\n* **Objects**\n    * Typed nodes (type is denoted by `otype`)\n    * Identified by 64-bit integers.\n    * Contains data in the form of key-value pairs.\n    * Models users and repeatable actions (eg `comments`).\n\n* **Associations** \n   * Typed directed edges between objects (type is denoted by `atype`)\n   * Identified by source object `id1`, `atype` and destination object `id2`.\n   * Contains data in the form of key-value pairs.\n   * Contains a 32-bit time field.\n   * Models actions that happen at most once or records state transition (eg `like`)\n   * Often `inverse association` is also meaningful (eg `like` and `liked by`).\n\n## Query API\n\n* Support to create, retrieve, update and delete objects and associations.\n* Support to get all associations (`assoc_get`) or their count(`assoc_count`)\n based on starting node, time, index and limit parameters.\n \n \n ## TAO Architecture\n \n ### Storage Layer\n * Objects and associations stored in MySQL.\n * TAO API mapped to SQL queries.\n * Data divided into logical shards.\n * Objects bound to the shard for their lifetime(`shard_id` is embedded in `id`).\n * Associations stored on the shard of its `id` (for faster association query).\n \n \n ### Caching Layer\n * Consists of multiple cache servers (together form a `tier`).\n * In memory, LRU cache stores objects, association lists, and association counts.\n * Write operation on association list with inverse involves writing 2 shards (for `id1` and `id2`).\n * The client sends the query to cache layer which issues inverse write query to shard2 and once that is completed, a write query is made to shard1.\n * Write failure leads to hanging associations which are repaired by an asynchronous job.\n\n### Leaders and Followers\n* A single, large tier is prone to hot spots and square growth in terms of all-to-all connections.\n* Cache split into 2 levels - one **leader tier** and multiple **follower tiers**.\n* Clients communicate only with the followers.\n* In the case of read miss/write, followers forward the request to the leader which connects to the storage layer.\n* Eventual consistency maintained by serving cache maintenance messages from leaders to followers.\n* Object update in leaders leads results in `invalidation message` to followers.\n* Leader sends `refill message` to notify about association write.\n* Leaders also serialize concurrent writes and mediates thundering herds.\n\n\n## Scaling Geographically\n\n* Since workload is read intensive, read misses are serviced locally at the expense of data freshness.\n* In the multi-region configuration, there are master-slave regions for each shard and each region has its own followers, leader, and database.\n* Database in the local region is a replica of the database in the master region.\n* In the case of read miss, the leader always queries the region database (irrespective of it being the master database or slave database).\n* In the case of write, the leader in the local region would forward the request to database in the master region.\n\n## Optimisations\n\n### Caching Server\n* RAM is partitioned into `arena` to extend the lifetime of important data types.\n* For small, fixed-size items (eg association count), a direct 8-way associative cache is maintained to avoid the use of pointers.\n* Each `atype` is mapped to 16-bit value to reduce memory footprint.\n\n### Cache Sharding and Hot Spots\n* Load is balanced among followers through `shard cloning`(reads to a shard are served by multiple followers in a tier).\n* Response to query include the object's access rate and version number. If the access rate is too high, the object is cached by the client itself. Next time when the query comes, the data is omitted in the reply if it has not changed since the previous version.\n\n### High Degree Objects\n* In the case of `assoc_count`, the edge direction is chosen on the basis of which node (source or destination) has a lower degree (to optimize reading the association list).\n* For `assoc_get` query, only those associations are searched where time > object's creation time.\n\n## Failure Detection and Handling\n\n* Aggressive network timeouts to detect (potential) failed nodes.\n\n### Database Failure\n\n* In the case of master failure, one of the slaves take over automatically.\n* In case of slave failure, cache miss are redirected to TAO leader in the region hosting the database master.\n\n### Leader Failure\n\n* When a leader cache server fails, followers route read miss directly to the database and write to a replacement leader (chosen randomly from the leader tier).\n\n### Refill and Invalidation Failures\n\n* Refill and invalidation are sent asynchronously.\n* If the follower is not available, it is stored in leader's disk.\n* These messages will be lost in case of leader failure.\n* To maintain consistency, all the shards mapping to a failed leader are invalidated.\n\n### Follower Failure\n\n* Each TAO client is configured with a primary and backup follower tier.\n* In normal mode, the request is made to primary tier and in the case of its failure, requests go to backup tier.\n* Read after write consistency may be violated if failing over between different tiers (read reaches the failover target before writer's `refill` or `invalidate`).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.usenix.org/conference/atc13/technical-sessions/presentation/bronson"
    },
    "405": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/IoffeS15",
        "transcript": "The [Batch Normalization paper](http://arxiv.org/pdf/1502.03167.pdf) describes a method to address the various issues related to training of Deep Neural Networks. It makes normalization a part of the architecture itself and reports significant improvements in terms of the number of iterations required to train the network.\n\n## Issues With Training Deep Neural Networks\n\n### Internal Covariate shift\n\nCovariate shift refers to the change in the input distribution to a learning system. In the case of deep networks, the input to each layer is affected by parameters in all the input layers. So even small changes to the network get amplified down the network. This leads to change in the input distribution to internal layers of the deep network and is known as internal covariate shift. \n\nIt is well established that networks converge faster if the inputs have been whitened (ie zero mean, unit variances) and are uncorrelated and internal covariate shift leads to just the opposite.\n\n### Vanishing Gradient \n\nSaturating nonlinearities (like tanh or sigmoid) can not be used for deep networks as they tend to get stuck in the saturation region as the network grows deeper. Some ways around this are to use:\n * Nonlinearities like ReLU which do not saturate\n * Smaller learning rates\n * Careful initializations \n\n## Normalization\n\nLet us say that the layer we want to normalize has *d* dimensions **x** $= (x_1, ... x_d)$. Then, we can normalize the $k^th$ dimension as follows:\n\n![Scaled and shifted normalized value](https://db.tt/YORi6lov)\n\nWe also need to scale and shift the normalized values otherwise just normalizing a layer would limit the layer in terms of what it can represent. For example, if we normalize the inputs to a sigmoid function, then the output would be bound to the linear region only.\n\nSo the normalized input $x^k$ is transformed to:\n\n![Scaled and shifted normalized value](https://db.tt/6vImAQoc)\n\nwhere $\u03b3$ and $\u03b2$ are parameters to be learned.\n\nMoreover, just like we use mini-batch in Stochastic Gradient Descent (SGD), we can use mini-batch with normalization to estimate the mean and variance for each activation.\n\nThe transformation from $x$ to $y$ as described above is called **Batch Normalizing Tranform**. This BN transform is differentiable and ensures that as the model is training, the layers can learn on the input distributions that exhibit less internal covariate shift and can hence accelerate the training.\n\nAt training time, a subset of activations in specified and BN transform is applied to all of them.\n\nDuring test time, the normalization is done using the population statistics instead of mini-batch statistics to ensure that the output deterministically depends on the input. \n\n\n## Batch Normalized Convolutional Networks\n\nLet us say that $x = g(Wu+b)$ is the operation performed by the layer where $W$ and $b$ are the parameters to be learned, $g$ is a nonlinearity and $u$ is the input from the previous layer.\n\nThe BN transform is added just before the nonlinearity, by normalizing $x = Wu+b$. An alternative would have been to normalize $u$ itself but constraining just the first and the second moment would not eliminate the covariate shift from $u$.\n\nWhen normalizing $Wu+b$, we can ignore the $b$ term as it would be canceled during the normalization step (*b*'s role is subsumed by \u03b2) and we have\n\n$z = g( BN(Wu) )$\n\nFor convolutional layers, normalization should follow the convolution property as well - ie different elements of the same feature map, at different locations, are normalized in the same way. So all the activations in a mini-batch are jointly normalized over all the locations and parameters (*\u03b3* and *\u03b2*) are learnt per feature map instead of per activation.\n\n## Advantages Of Batch Normalization\n\n1. Reduces internal covariant shift.\n2. Reduces the dependence of gradients on the scale of the parameters or their initial values.\n3. Regularizes the model and reduces the need for dropout, photometric distortions, local response normalization and other regularization techniques.\n4. Allows use of saturating nonlinearities and higher learning rates.\n\nBatch Normalization was applied to models trained for MNIST and Inception Network for ImageNet. All the above-mentioned advantages were validated in the experiments. Interestingly, Batch Normalization with sigmoid achieved an accuracy of 69.8% (overall best, using any nonlinearity, was 74.8%) while Inception model (sigmoid nonlinearity), without Batch Normalisation, worked only as good as a random guess.\n\n## Future Work\n\nWhile BN Transform does enhance the overall deep network training task, its precise effect on gradient propagation is still not well understood. A future extension of Batch Normalisation would be in the domain of Recurrent Neural Networks where internal covariate shift and vanishing gradients are more severe. It remains to be explored if it can also help with domain adaption by easily generalizing to new data distributions.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/ioffe15.html"
    },
    "406": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1208-4171",
        "transcript": "The [paper](http://vldb.org/pvldb/vol5/p1771_georgelee_vldb2012.pdf) presents Twitter's logging infrastructure, how it evolved from application specific logging to a unified logging infrastructure and how session-sequences are used as a common case optimization for a large class of queries.\n\n## Messaging Infrastructure\n\nTwitter uses **Scribe** as its messaging infrastructure. A Scribe daemon runs on every production server and sends log data to a cluster of dedicated aggregators in the same data center. Scribe itself uses **Zookeeper** to discover the hostname of the aggregator. Each aggregator registers itself with Zookeeper. The Scribe daemon consults Zookeeper to find a live aggregator to which it can send the data. Colocated with the aggregators is the staging Hadoop cluster which merges the per-category stream from all the server daemons and writes the compressed results to HDFS. These logs are then moved into main Hadoop data warehouse and are deposited in per-category, per-hour directory (eg /logs/category/YYYY/MM/DD/HH). Within each directory, the messages are bundled in a small number of large files and are partially ordered by time.\n\nTwitter uses **Thrift** as its data serialization framework, as it supports nested structures, and was already being used elsewhere within Twitter. A system called **Elephant Bird** is used to generate Hadoop record readers and writers for arbitrary thrift messages. Production jobs are written in **Pig(Latin)** and scheduled using **Oink**.\n\n## Application Specific Logging\n\nInitially, all applications defined their own custom formats for logging messages. While it made it easy to develop application logging, it had many downsides as well.\n\n* Inconsistent naming conventions: eg uid vs userId vs user_Id\n* Inconsistent semantics associated with each category name causing resource discovery problem.\n* Inconsistent format of log messages.\n\nAll these issues make it difficult to reconstruct user session activity. \n\n## Client Events\n\nThis is an effort within Twitter to develop a unified logging framework to get rid of all the issues discussed previously. A hierarchical, 6-level schema is imposed on all the events (as described in the table below).\n\n\n\n| Component |  Description                       |  Example                                     | \n|-----------|------------------------------------|----------------------------------------------| \n| client    |  client application                |  web, iPhone, android                          | \n| page      |  page or functional grouping       |  home, profile, who_to_follow                  | \n| section   |  tab or stream on a page           |  home, mentions, retweets, searches, suggestions | \n| component |  component object or objects       |  search_box, tweet                            | \n| element   |  UI element within the component   |  button, avatar                               | \n| action    |  actual user or application action |  impression, click, hover                      | \n\n\n**Table 1: Hierarchical decomposition of client event names.**\n\nFor example, the following event, `web:home:mentions:stream:avatar:profile_click` is logged whenever there is an image profile click on the avatar of a tweet in the mentions timeline for a user on twitter.com (read from right to left).\n\nThe alternate design was a tree based model for logging client events. That model allowed for arbitrarily deep event namespace with as fine-grained logging as required. But the client events model was chosen to make the top level aggregate queries easier.\n\nA client event is a Thrift structure that contains the components given in the table below.\n\n| Field           | Description                     | \n|-----------------|---------------------------------| \n| event initiator |  {client, server} \u00d7 {user, app} | \n| event_name      |  event name                     | \n| user_id         |  user id                        | \n| session_id      |  session id                     | \n| ip              |  user\u2019s IP address              | \n| timestamp       |  timestamp                      | \n| event_details   |  event details                  | \n\n**Table 2: Definition of a client event.**\n\nThe logging infrastructure is unified in two senses: \n\n* All log messages share a common format with clear semantics.\n* All log messages are stored in a single place.\n\n## Session Sequences\n\nA session sequence is a sequence of symbols *S = {s<sub>0</sub>, s<sub>1</sub>, s<sub>2</sub>...s<sub>n</sub>}* such that each symbol is drawn from a finite alphabet *\u03a3*. A bijective mapping is defined between \u03a3 and universe of event names. Each symbol in \u03a3 is represented by a valid Unicode point (frequent events are assigned shorter code prints) and each session sequence becomes a valid Unicode string. Once all logs have been imported to the main database, a histogram of event counts is created and is used to map event names to Unicode code points. The counts and samples of each event type are stored in a known location in HDFS. Session sequences are reconstructed from the raw client event logs via a *group-by* on *user_id* and *session_id*. Session sequences are materialized as it is difficult to work with raw client event logs for following reasons:\n* A lot of brute force scans.\n* Large group-by operations needed to reconstruct user session.\n\n#### Alternate Designs Considered\n* Reorganize complete Thrift messages by reconstructing user sessions - This solves the second problem but not the first.\n* Use a columnar storage format - This addresses the first issue but it just reduces the time taken by mappers and not the number of mappers itself.\n\nThe materialized session sequences are much smaller than raw client event logs (around 50 times smaller) and address both the issues.\n\n## Client Event Catalog\n\nTo enhance the accessibility of the client event logs, an automatically generated event data log is used along with a browsing interface to allow users to browse, search and access sample entries for the various client events. (These sample entries are the same entries that were mentioned in the previous section. The catalog is rebuilt every day and is always up to date.\n\n## Applications\n\nClient Event Logs and Session Sequences are used in following applications:\n\n* Summary Statistics - Session sequences are used to compute various statistics about sessions.\n* Event Counting - Used to understand what feature of users take advantage of a particular feature.\n* Funnel Analytics - Used to focus on user attention in a multi-step process like signup process.\n* User Modeling - Used to identify \"interesting\" user behavior. N-gram models (from NLP domain) can be extended to measure how important temporal signals are by modeling user behavior on the basis of last n actions. The paper also mentions the possibility of extracting \"activity collocations\" based on the notion of collocations.\n\n\n## Possible Extensions\n\nSession sequences are limited in the sense that they capture only event name and exclude other details. The solution adopted by Twitter is to use a generic indexing infrastructure that integrates with Hadoop at the level of InputFormats. The indexes reside with the data making it easier to reindex the data. An alternative would have been to use **Trojan layouts** which members indexing in HDFS block header but this means that indexing would require the data to be rewritten. Another possible extension would be to leverage more analogies from the field of Natural Language Processing. This would include the use of automatic grammar induction techniques to learn hierarchical decomposition of user activity. Another area of exploration is around leveraging advanced visualization techniques for exploring sessions and mapping interesting behavioral patterns into distinct visual patterns that can be easily recognized.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1208.4171"
    },
    "407": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/cacm/Domingos12",
        "transcript": "The [paper](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) presents some key lessons and \"folk wisdom\" that machine learning researchers and practitioners have learnt from experience and which are hard to find in textbooks.\n\n### 1. Learning = Representation + Evaluation + Optimization\n\nAll machine learning algorithms have three components:\n*    **Representation** for a  learner is the set if classifiers/functions that can be possibly learnt. This set is called *hypothesis space*. If a function is not in hypothesis space, it can not be learnt.\n*    **Evaluation** function tells how good the machine learning model is.\n*    **Optimisation** is the method to search for the most optimal learning model.\n\n### 2. Its Generalization That Counts\n\nThe fundamental goal of machine learning is to generalize beyond the training set. The data used to evaluate the model must be kept separate from the data used to learn the model. When we use generalization as a goal, we do not have access to a function that we can optimize. So we have to use training error as a proxy for test error.\n\n### 3. Data Alone Is Not Enough\n\nSince our ultimate goal is generalization (see point 2), there is no such thing as **\"enough\"**  data. Some knowledge beyond the data is needed to generalize beyond the data. Another way to put is \"No learner can beat random guessing over all possible functions.\" But instead of hard-coding assumptions, learners should allow assumptions to be explicitly stated, varied and incorporated automatically into the model.\n\n### 4. Overfitting Has Many Faces\n\nOne way to interpret overfitting is to break down generalization error into two components: bias and variance. **Bias** is the tendency of the learner to constantly learn the same wrong thing (in the image, a high bias would mean more distance from the centre). **Variance** is the tendency to learn random things irrespective of the signal (in the image, a high variance would mean more scattered points). \n\n![Bias Variance Diagram](https://dl.dropboxusercontent.com/u/56860240/A-Paper-A-Week/BiasVarianceDiagram.png)\n\nA more powerful learner (one that can learn many models) need not be better than a less powerful one as they can have a high variance.  While noise is not the only reason for overfitting, it can indeed aggravate the problem. Some tools against overfitting are - **cross-validation**, **regularization**, **statistical significance testing**, etc. \n\n### 5. Intuition Fails In High Dimensions\n\nGeneralizing correctly becomes exponentially harder as dimensionality (number of features) become large. Machine learning algorithms depend on similarity-based reasoning which breaks down in high dimensions as a fixed-size training set covers only a small fraction of the large input space. Moreover, our intuitions from three-dimensional space often do not apply to higher dimensional spaces. So the **curse of dimensionality** may outweigh the benefits of having more features. Though, in most cases, learners benefit from the **blessing of non-uniformity** as data points are concentrated in lower-dimensional manifolds. Learners can implicitly take advantage of this lower effective dimension or use dimensionality reduction techniques.\n\n### 6. Theoretical Guarantees Are Not What They Seem  \n\nA common type of bound common when dealing with machine learning algorithms is related to the number of samples needed to ensure good generalization. But these bounds are very loose in nature. Moreover, the bound says that given a large enough training dataset, our learner would return a good hypothesis with high probability or would not find a consistent hypothesis. It does not tell us anything about how to select a good hypothesis space.\n\nAnother common type of bound is the asymptotic bound which says \"given infinite data, the learner is guaranteed to output correct classifier\". But in practice we never have infinite data and data alone is not enough (see point 3). So theoretical guarantees should be used to understand and drive the algorithm design and not as the only criteria to select algorithm.\n\n### 7. Feature Engineering Is The Key\n\nMachine Learning is an iterative process where we train the learner, analyze the results, modify the learner/data and repeat. Feature engineering is a crucial step in this pipeline. Having the right kind of features (independent features that correlate well with the class) makes learning easier. But feature engineering is also difficult because it requires domain specific knowledge which extends beyond just the data at hand (see point 3).\n\n### 8. More Data Beats A Clever Algorithm\n\nAs a rule of thumb, a dumb algorithm with lots of data beats a clever algorithm with a modest amount of data. But more data means more scalability issues. Fixed size learners (parametric learners) can take advantage of data only to an extent beyond which adding more data does not improve the results. Variable size learners (non-parametric learners) can, in theory, learn any function given sufficient amount of data. Of course, even non-parametric learners are bound by limitations of memory and computational power. \n\n    \n### 9. Learn Many Models, Not Just One\n\nIn early days of machine learning, the model/learner to be trained was pre-determined and the focus was on tuning it for optimal performance. Then the focus shifted to trying many variants of different learners. Now the focus is on combining the various variants of different algorithms to generate the most optimal results. Such model ensembling techniques include *bagging*, *boosting* and *stacking*.\n\n### 10. Simplicity Does Not Imply Accuracy\n\nThough Occam's razor suggests that machine learning models should be kept simple, there is no necessary connection between the number of parameters of a model and its tendency to overfit. The complexity of a model can be related to the size of hypothesis space as smaller spaces allow the hypothesis to be generated by smaller, simpler codes. But there is another side to this picture - A learner with a larger hypothesis space that tries fewer hypotheses is less likely to overfit than one that tries more hypotheses from a smaller space. So hypothesis space size is just a rough guide towards accuracy. Domingos conclude in his [other paper](http://homes.cs.washington.edu/~pedrod/papers/dmkd99.pdf) that \"simpler hypotheses should be preferred because simplicity is a virtue in its own right, not because of a hypothetical connection with accuracy.\"\n\n\n### 11. Representation Does Not Imply Learnable\n\nJust because a function can be represented, does not mean that the function can actually be learnt. Restrictions imposed by data, time and memory, limit the functions that can actually be learnt in a feasible manner. For example, decision tree learners can not learn trees with more leaves than the number of training data points. The right question to ask is \"whether  a function can be learnt\" and not \"whether a function can be represented\". \n\n### 12. Correlation Does Not Imply Causation\n\nCorrelation may hint towards a possible cause and effect relationship but that needs to be investigated and validated. On the face of it, correlation can not be taken as proof of causation.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2347736.2347755"
    },
    "408": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icde/ThusooSJSCZALM10",
        "transcript": "[Hive](http://infolab.stanford.edu/~ragho/hive-icde2010.pdf) is an open-source data warehousing solution built on top of Hadoop.  It supports an SQL-like query language called HiveQL. These queries are compiled into MapReduce jobs that are executed on Hadoop. While Hive uses Hadoop for execution of queries, it reduces the effort that goes into writing and maintaining MapReduce jobs.\n\n# Data Model\n\nHive supports database concepts like tables, columns, rows and partitions. Both primitive (integer, float, string) and complex data-types(map, list, struct) are supported. Moreover, these types can be composed to support structures of arbitrary complexity. The tables are serialized/deserialized using default serializers/deserializer. Any new data format and type can be supported by implementing SerDe and ObjectInspector java interface.\n\n# Query Language\n\nHive query language (HiveQL) consists of a subset of SQL along with some extensions. The language is very SQL-like and supports features like subqueries, joins, cartesian product, group by, aggregation, describe and more. MapReduce programs can also be used in Hive queries. A sample query using MapReduce would look like this:\n\n```\nFROM ( \n    MAP inputdata USING 'python mapper.py' AS (word, count)\n    FROM inputtable\n    CLUSTER BY word\n    )\n    REDUCE word, count USING 'python reduce.py';\n```\nThis query uses `mapper.py` for transforming `inputdata` into `(word, count)` pair, distributes data to reducers by hashing on `word` column (given by `CLUSTER`) and uses `reduce.py`.\n\nNotice that Hive allows the order of `FROM` and `SELECT/MAP/REDUCE` to be changed within a sub-query. This allows insertion of different transformation results into different tables/partitions/hdfs/local directory as part of the same query and reduces the number of scans on the input data.\n\n### Limitations\n\n* Only equi-joins are supported.\n* Data can not be inserted into existing table/partitions and all inserts overwrite the data. \n\n`INSERT INTO, UPDATE`, and `DELETE` are not supported which makes it easier to handle reader and writer concurrency.\n\n# Data Storage\n\nWhile a table is the logical data unit in Hive, the data is actually stored into hdfs directories. A **table** is stored as a directory in hdfs, **partition** of a table as a subdirectory within a directory and **bucket** as a file within the table/partition directory. Partitions can be created either when creating tables or by using `INSERT/ALTER` statement. The partitioning information is used to prune data when running queries. For example, suppose we create partition for `day=monday` using the query\n\n```\nALTER TABLE dummytable ADD PARTITION (day='monday')\n```\nNext, we run the query - \n```\nSELECT * FROM dummytable WHERE day='monday'\n```\nSuppose the data in dummytable is stored in `/user/hive/data/dummytable` directory. This query will only scan the subdirectory `/user/hive/data/dummytable/day=monday` within the `dummytable` directory.\n\nA **bucket** is a file within the leaf directory of a table or a partition. It can be used to prune data when the user runs a `SAMPLE` query. \n\nAny data stored in hdfs can be queried using the `EXTERNAL TABLE` clause by specifying its location with the `LOCATION` clause. When dropping an external table, only its metadata is deleted and not the data itself.\n\n# Serialization/Deserialization\n\nHive implements the `LazySerDe` as the default SerDe. It deserializes rows into internal objects lazily so that the cost of Deserialization of a column is incurred only when it is needed. Hive also provides a `RegexSerDe` which allows the use of regular expressions to parse columns out from a row. Hive also supports various formats like `TextInputFormat`, `SequenceFileInputFormat` and `RCFileInputFormat`. Other formats can also be implemented and specified in the query itself. For example, \n\n```\nCREATE TABLE dummytable(key INT, value STRING)\n    STORED AS \n        INPUTFORMAT\n            org.apache.hadoop.mapred.SequenceFileInputFormat\n        OUTPUTFORMAT\n            org.apache.hadoop.mapred.SequenceFileOutputFormat\n```\n\n# System Architecture and Component\n\n### Components\n\n* **Metastore** - Stores system catalog and metadata about tables, columns and partitions.\n* **Driver** - Manages life cycle of a HiveQL statement as it moves through Hive.\n* **Query Compiler** - Compiles query into a directed acyclic graph of MapReduce tasks.\n* **Execution Engine** - Execute tasks produced by the compiler in proper dependency order.\n* **Hive Server** - Provides a thrift interface and a JDBC/ODBC server.\n* **Client components** - CLI, web UI, and JDBC/ODBC driver.\n* **Extensibility Interfaces** - Interfaces for SerDe, ObjectInspector, UDF (User Defined Function) and UDAF (User-Defined Aggregate Function).\n\n### Life Cycle of a query\n\nThe query is submitted via CLI/web UI/any other interface. This query goes to the compiler and undergoes parse, type-check and semantic analysis phases using the metadata from Metastore. The compiler generates a logical plan which is optimized by the rule-based optimizer and an optimized plan in the form of DAG of MapReduce and hdfs tasks is generated. The execution engine executes these tasks in the correct order using Hadoop.\n\n### Metastore\nIt stores all information about the tables, their partitions, schemas, columns and their types, etc. Metastore runs on traditional RDBMS (so that latency for metadata query is very small) and uses an open source ORM layer called DataNuclues. Matastore is backed up regularly. To make sure that the system scales with the number of queries, no metadata queries are made the mapper/reducer of a job. Any metadata needed by the mapper or the reducer is passed through XML plan files that are generated by the compiler.\n\n### Query Compiler\n\nHive Query Compiler works similar to traditional database compilers.\n* Antlr is used to generate the Abstract Syntax Tree (AST) of the query.\n* A logical plan is created using information from the metastore. An intermediate representation called query block (QB) tree is used when transforming AST to operator DAG. Nested queries define the parent-child relationship in QB tree.\n* Optimization logic consists of a chain of transformation operations such that output from one operation is input to next operation. Each transformation comprises of a **walk** on operator DAG. Each visited **node** in the DAG is tested for different **rules**. If any rule is satisfied, its corresponding **processor** is invoked. **Dispatcher** maintains a mapping fo different rules and their processors and does rule matching. **GraphWalker** manages the overall traversal process.\n* Logical plan generated in the previous step is split into multiple MapReduce and hdfs tasks. Nodes in the plan correspond to physical operators and edges represent the flow of data between operators.\n\n### Optimisations     \n\n* **Column Pruning** - Only the columns needed in the query processing are projected.\n* **Predicate Pushdown** - Predicates are pushed down to the scan so that rows are filtered as early as possible.\n* **Partition Pruning** - Predicates on partitioned columns are used to prune out files of partitions that do not satisfy the predicate. \n* **Map Side Joins** - In case the tables involved in the join are very small, the tables are replicated in all the mappers and the reducers.\n* **Join Reordering** - Large tables are streamed and not materialized in-memory in the reducer to reduce memory requirements.\n\nSome optimizations are not enabled by default but can be activated by setting certain flags. These include: \n\n* Repartitioning data to handle skew in `GROUP BY` processing.This is achieved by performing `GROUP BY` in two MapReduce stages - first where data is distributed randomly to the reducers and partial aggregation is performed. In the second stage, these partial aggregations are distributed on GROUP BY columns to different reducers.\n* Hash bases partial aggregations in the mappers to reduce the data that is sent by the mappers to the reducers which help in reducing the amount of time spent in sorting and merging the resulting data.\n\n### Execution Engine\n\nExecution Engine executes the tasks in order of their dependencies. A MapReduce task first serializes its part of the plan into a plan.xml file. This file is then added to the job cache and mappers and reducers are spawned to execute relevant sections of the operator DAG. The final results are stored to a temporary location and then moved to the final destination (in the case of say `INSERT INTO` query).\n\n# Future Work\n\nThe paper mentions the following areas for improvements:\n\n* HiveQL should subsume SQL syntax.\n* Implementing a cost-based optimizer and using adaptive optimization techniques.\n* Columnar storage to improve scan performance.\n* Enhancing JDBS/ODBC drivers for Hive to integrate with other BI tools.\n* Multi-query optimization and generic n-way join in a single MapReduce job.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/ICDE.2010.5447738"
    },
    "409": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=clauset2009powerlaw",
        "transcript": "## Introduction\n\nA variable x is said to obey a power-law if it is drawn from a probability distribution function (pdf) of the form $p(x) = Cx^{-\\alpha}$ where $C$ is called the **normalization constant** and $\\alpha$ is called **scaling parameter** or exponent. Often, the power-law applies only for values greater than some minimum $x$, called $x_\\text{min}$. The paper describes various statistical techniques to test if a given distribution follows a power-law or not.\n\nPower-law distributions come in both continuous and discrete flavor with the discrete case being more involved than the continuous one. So, the discrete power-law behavior is often approximated by continuous power-law behavior for the sake of convenience. One reliable approximation is to assume that discrete values of $x$ are generated from a continuous power-law and then rounded to nearest integer to get the discrete values.\n\nSometimes, complementary cumulative distribution function (or CDF) is also considered where $P(X) = p(x \u2265 X)$\n\n## Fitting power-laws to empirical data\n\nPower-law distribution makes a straight line on  the log-log plot. This slope can be calculated using the method of least square linear regression. But simple line fitting does not guarantee that data follows a power-law distribution. Moreover, the assumption of independent, Gaussian noise, which is a pre-requisite for linear regression, does not hold for this case.\n\n### Estimating scaling parameter\n\nAssuming that we know the value of $x_\\text{min}$, the value of $\\alpha$ can be obtained by the *method of maximum likelihood*. Maximum likelihood estimator (MLE) for continuous case is given as:\n\nhttps://github.com/shagunsodhani/powerlaw/raw/master/paper/assets/MLEContinous.png\n\nand that for the discrete case is given as:\n\nhttps://github.com/shagunsodhani/powerlaw/raw/master/paper/assets/MLEDiscrete.png\n\nThe equation for the discrete case is only an approximation as there is no exact MLE for discrete case.\n\nMLE method outperforms several linear regression based approaches like line fitting on the log-log plot, line fitting after performing logarithmic binning (done to reduce fluctuations in the tail of the distribution), line fitting to CDF with constant size bins and line fitting to CDF without any bins. But for any finite sample size $n$ and any choice of $x_\\text{min}$, there is bias present which decays as $O(1/n)$ and can be ignored for $n \u2265 50$\n\n\n### Estimating $x_\\text{min}$\n\nIf we choose a value of $x_\\text{min}$ less than the original value, then we will get a biased value of $\\alpha$ as we will be fitting power-law to non power-law region as well. If we choose a value larger than the original value, we will be losing legitimate data points (leading to statistical errors). But it is more acceptable to make a higher estimate of $x_\\text{min}$ than the original value.\n\nOne approach is to plot the PDF or CDF on the log-log plot and mark the point beyond which the distribution becomes roughly straight or to plot $\\alpha$ as a function of $x_\\text{min}$ and mark the point beyond which the value appears relatively stable. But these approaches are not objective as roughly straight and relatively stable are not quantified.\n\nThe approach proposed by Clauset et al. [Clauset, A., M. Young, and K. S. Gleditsch, 2007, Journal of Conflict Resolution 51, 58] is as follows:\n\nChoose a value of $x_\\text{min}$ such that the probability distribution of the measured data and best-fit power-law model are as similar as possible. Similarity between distributions can be measured using Kolmogorov-Smirnov (KS) statistic which is defined as:\n\nhttps://github.com/shagunsodhani/powerlaw/raw/master/paper/assets/KSStatistics.png\n\nwhere $S(x)$ is CDF of given data with values greater than or equal to $x_\\text{min}$ and *P(x)* is the CDF of power-law model that best fits the data in the region $x \u2265 x_\\text{min}$. This method works well for both continuous and discrete data and is recommended for the general case. Other measures, like weighted KS or Kuiper statistics, can also be used in place of KS statistic.\n\n## Testing the power-law hypothesis\n\nMLE and other approaches do not tell us whether power-law is a possible fit to the given data - all they do is find the best fit values of $x_\\text{min}$ and $\\alpha$ assuming the data comes from a power-law distribution. A basic approach would be to calculate the value of $x_\\text{min}$ and $\\alpha$ and use them to hypothesize a power-law distribution from which the data is drawn. We then check the validity of this hypothesis using the goodness-of-fit tests.\n\n### Goodness-of-fit tests\n\nA large number of synthetic data sets are generated from the hypothesized power-law distribution. Then each of these distributions is fitted to their own power-law model individually and the KS statistics is calculated for each distribution. The *p-* value is defined to be the fraction of synthetic datasets where the distance (KS statistic value) is greater than the distance for given dataset. A large value of *p* (close to 1) means that the fluctuations between given data and the hypothesized model could be because of statistical fluctuations alone while a small value of *p* (close to 0) means that the model is not a possible fit to the distribution.\n\n### Dataset generation\n\nThe generated dataset needs to be such that it has a distribution similar to the given data below $x_\\min$ and follows the fitted power-law above $x_\\text{min}$. Suppose the given data has *n<sub>tail</sub>* observations (where $x \u2265 x_\\text{min}$) and *n* observations in total. With a probability of $x_\\text{min}/n$, a random number $x_i$ is generated from the hypothesized power-law distribution. With a probability of $1- n_{tail}/n$, a number is picked randomly from the given dataset with $x < x_\\text{min}$. This way, the generated dataset of n elements is expected to follow powerlaw above $x \u2265 x_\\text{min}$ and same distribution as given data below $x_\\text{min}$.\n\nIf we want the *p-* values to be accurate to within about *\u03b5* of the true value, then we should generate at least *1/4 \u03b5<sup>-2</sup>* synthetic data sets. \n\nThe power law is ruled out if *p \u2264 0.1*. A large *p-* value does not mean that the power-law is the correct distribution for the data. There can be other distributions that can fit the data equally well or even better. Moreover, for small values of n, it is possible that the given distribution will follow a power law closely, and hence that the p-value will be large, even when the power law is the wrong model for the data.\n\n\n## Alternate distributions\n\n*p-* value test can only be used to reject the power-law hypothesis and not accept it. So even if *p-value > 0.1*, we can only say that power-law hypothesis is not rejected. It could be the case that some other distribution fits the data equally well or even better. To eliminate this possibility, we calculate a *p-* value for a fit to the alternate distribution and compare it with the *p-* value for the power-law. If the *p-* value for power-law is high and the *p-* value for the other distribution is low, we can say that data is more likely to be drawn from the power-law distribution (though we still can not be sure that it is **definitely** drawn from the power-law distribution).\n\n### Likelihood Ratio Test\n\nThis test can be used to directly compare two distributions against one another to see which is a better fit for the given data. The idea is to compute the likelihood of the given data under the two competing distributions. The one with the higher likelihood is taken to be the better fit. Alternatively, the ratio of the two likelihoods, or the logarithm *R* of the ratio can be used. If *R* is close enough to zero, then it could go to either side of zero, depending on statistical fluctuations. So *R* value needs to be sufficiently far from zero. To check for this, Vuong's method [Vuong, Q. H., 1989, Econometrica 57, 307] is used which gives a *p-* value that can tell if the conclusion from the value of *R* is statistically significant. If this *p-* value is small (*p < 0.1*), the result is significant. Otherwise, the result is not taken to be reliable and the test does not favor either distribution. \n\nOther than the likelihood ratio, several other tests like minimum description length (MDL) or cross-validation can also be performed.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1137/070710111"
    },
    "410": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=malewicz2010pregel",
        "transcript": "The [Pregel paper](https://kowshik.github.io/JPregel/pregel_paper.pdf) introduces a vertex-centric, large-scale graph computational model. Interestingly, the name Pregel comes from the name of the river which the Seven Bridges of K\u00f6nigsberg spanned. \n\n### Computational Model\n\nThe system takes as input a directed graph with properties assigned to both vertices and edges. The computation consists of a sequence of iterations, called supersteps. In each superstep, a user-defined function is invoked on each vertex in parallel. This function essentially implements the algorithm by specifying the behaviour of a single vertex V during a single superstep S. The function can read messages sent to the vertex V during the previous superstep (S-1), change the state of the vertex or its out-going edges', mutate the graph topology by adding/removing vertices or edges and by sending messages to other vertices that would be received in the next superstep (S+1). Since all computation during a superstep is performed locally, the model is well suited for distributed computing and synchronization is needed only between supersteps.\n\nThe computation terminates when every vertex is in the deactivated state. When computation starts, all vertices are in active state. A vertex deactivates itself by voting to halt and once deactivated, it does not take part in subsequent supersteps. But any time a deactivated vertex receives a message, it becomes activated again and takes part in subsequent supersteps. The resulting state machine is shown below:\n\n![Vertex State Machine](https://db.tt/Pdm1Rl6A)\n\nThe output of the computation is the set of values produced by the vertices.\n\nPregel adopts a pure message passing model that eliminates the need of shared memory and remote reads. Messages can be delivered asynchronously thereby reducing the latency. Graph Algorithms can also be expressed as a sequence of MapReduce jobs, but that requires passing the entire state of the graph from one stage to another. It also requires coordinating the various steps of chained MapReduce. In contrast, Pregel keeps vertices and out-going edges on machine performing the computation and only messages are transferred across. Though Pregel is similar in concept to MapReduce, it comes with a natural graph API and efficient support for running iterative algorithms over the graph.\n\n### API\n\n1. Pregel programs are written by subclassing the vertex class.\n2. The programmer overrides the compute() method. This is the user-defined function that is invoked on each vertex.\n3. Within compute(), the programmer can modify the value of vertex or out-going edges' properties and these changes are reflected immediately. These are the only per-vertex state that persists throughout each superstep.\n4. All messages sent to vertex V during superstep S are available via an iterator in superstep S+1. While it is guaranteed that each message would be delivered exactly once, there is no guarantee on the order of delivery.\n5. User defined handlers are invoked when destination vertex does not exist.\n6. Combiners are used to reduce the number of messages to be transferred by aggregating messages from different vertices (all of which are on the same machine) which have the same destination vertex. Since there is no guarantee about which messages will be aggregated and in what order, only commutative and associative operations can be used.\n7. Aggregators can be used for capturing the global state of the graph where each vertex provides a value to the aggregator during each superstep and these values are combined by the system using a reduce operation (which should be associative and commutative). The aggregated value is then available to all the vertices in the next superstep.\n\n\n### Topology Mutation\n\nSince compute() method allows the graph topology to be modified, conflicting requests can be made in the same superstep. Two mechanisms are used to handles the conflicts:\n\n* Within a super step, following order is followed when resolving conflicting operations:\n  *   Edge removal\n  *   Vertex removal\n  *   Vertex addition\n  *   Edge addition.\n*   User defined handlers are used for conflicts that can not be resolved by partial ordering alone. This would include scenarios where there are multiple requests to create a vertex with different initial values. In these cases, the user defines how to resolve the conflict.\n\nThe coordination mechanism is lazy in the sense that global mutations do not require coordination until the point they are applied.\n\n\n### Execution Steps\n\n* Several instances of the user program start executing on a cluster of machines. One of the instances becomes the master and the rest are the workers. \n* The master partitions the graph based on vertex id with each partition consisting of a set of vertices and its out-going edges. \n* These partitions are assigned to workers. Each worker executes the compute method on all its vertices and manages messages to and from other vertices.\n* The master instructs each worker to perform a superstep. The workers run the computer method on each vertex and tell the master how many vertices would be active in the next superstep. This continues as long as even one vertex is active.\n* Once all the vertices become deactivated, the master may ask the workers to save their portion of the graph.\n\n\n### Fault Tolerance\n\nFault tolerance is achieved through checkpointing where the master instructs the workers to save the state of computation to persistent storage. Master issues regular \"ping\" messages to workers and if a worker does not receive a message from the master in a specified time interval, the worker terminates itself. If the master does not hear back from the worker, the worker is marked as failed. In this case, the graph partitions assigned to the failed worker are reassigned to the active workers. All the active workers then load the computation state from the last checkpoint and may repeat some supersteps.\n\nAn alternate to this would be confined recovery where along with basic checkpointing, the workers log out-going messages from their assigned partitions during graph loading and subsequent supersteps. This way, lost partitions can be recomputed from log messages and the entire system does not have to perform a rollback. \n\n### Worker Implementation\n\nA worker contains a mapping of vertex id to vertex state for its portion of the complete graph. The vertex state would comprise of its current value, its put-going edges, the queue containing incoming messages and a flag marking whether it is in the active state. Two copies of queue and flag are maintained, one for the current superstep and one for the next superstep.\n\nWhile sending a message to another vertex, the worker checks if the destination vertex is on the same machine. If yes, it places the message directly on the receiver's queue instead of sending it via the network. In case the vertex lies on the remote machine, the messages are buffered and sent to destination worker as a single network message.\n\nIf a combiner is specified, it is applied to both the message being added to the outgoing message queue and to the message received at the incoming message queue.\n\n### Master Implementation\n\nThe master coordinates the workers by maintaining a list of currently alive workers, their addressing information and the information on the portion of graph assigned to them. The size of master's data structure depends on the number of partitions and a single master can be used for a very large graph.\n\nThe master sends the computation task to workers and waits for a response. If any worker fails, the master enters recovery mode as discussed in the section on fault tolerance. Otherwise, it proceeds to the next superstep. It also runs an internal hHTTP server to serve statistics about the graph and the state of the computation.\n\n### Aggregators\n\nWorkers combine all the values supplied to an aggregator, by all the vertices in a superstep, into a single local value. At the end of the superstep, the workers perform the tree-based reduction on the local value and deliver the global values to the master. The tree-based reduction is better than pipelining with a chain of workers as it allows fro more parallelization.\n\n### Applications/Experiments\n\nThe paper has described how to implement PageRank, ShortestPath, Bipartite Matching and Semi Clustering algorithm on top of Pregel. The emphasis is on showing how to think of these algorithms in a vertex-centric manner and not on how to implement them on Pregel in the best possible way.\n\nThe experiments were conducted with the single-source shortest paths algorithm with input as binary trees and log-normal graphs. Default partitioning strategy and naive implementation of the algorithms was used to show that satisfactory performance can be achieved with little coding effort. The runtime increases approximately linearly in the graph size.\n\n### Limitations\n\nOne obvious limitation is that the entire computation state resides in main memory. Secondly, Pregel is designed around sparse graphs and performance will take a hit in case of dense graphs where a lot of communication takes place between vertices. The paper counters this by arguing that realistic dense graphs and algorithms with dense computation are rare. Moreover, communication in such dense networks can be reduced by using aggregators and combiners. An add-on would be to support dynamic partitioning of graph based on message traffic to minimize communication over the network.\n\n\nPregel's open-source implementation, called [Giraph](http://giraph.apache.org/), adds several features beyond the basic Pregel model, including out-of-core computation, and edge-oriented input which does take away some of the original limitations. Facebook is using Giraph to analyze its social network and has [scaled it to a trillion edges](https://code.facebook.com/posts/509727595776839/scaling-apache-giraph-to-a-trillion-edges/) showing the scalability of the Pregel model itself.    ",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "411": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/XinCDGFS14",
        "transcript": "This week I read upon [GraphX](https://amplab.cs.berkeley.edu/wp-content/uploads/2014/02/graphx.pdf), a distributed graph computation framework that unifies graph-parallel and data-parallel computation. Graph-parallel systems efficiently express iterative algorithms (by exploiting the static graph structure) but do not perform well on operations that require a more general view of the graph like operations that move data out of the graph. Data-parallel systems perform well on such tasks but directly implementing graph algorithms on data-parallel systems is inefficient due to complex joins and excessive data movement. This is the gap that GraphX fills in by allowing the same data to be viewed and operated upon both as a graph and as a table.\n\n### Preliminaries\n\nLet $G = (V, E)$ be a graph where $V = \\{1, ..., n\\}$ is the set of vertices and $E$ is the set of $m$ directed edges. Each directed edge is a tuple of the form $(i, j)  \\in E$ where $i \\in V$ is the source vertex and $j \\in V$ is the target vertex. The vertex properties are represented as $P_V(i)$ where $i \\in V$ and edge properties as $P_E (i, j)$ for edge $(i, j) \\in E$. The collection of all the properties is $P = (P_V, P_E)$. The combination of graph structure and properties defines a property graph $G(P) = (V, E, P)$.\n\nGraph-Parallel Systems consist of a property graph $G = (V, E, P)$ and a vertex-program $Q$ that is instantiated simultaneously on all the vertices. The execution on vertex $v$, called $Q(v)$, interacts with execution on the adjacent vertices by message passing or shared state and can read/modify properties on the vertex, edges and adjacent vertices. $Q$ can run in two different modes:\n\n* **bulk-synchronous mode** - all vertex programs run concurrently in a sequence of super-steps. \n* **asynchronous mode** - vertex programs run as and when resources are available and impose constraints on whether neighbouring vertex-programs can run concurrently.\n\n**Gather-Apply-Scatter (GAS)** decomposition model breaks down a vertex-program into purely edge-parallel and vertex-parallel stages. The associative *gather* function collects the inbound messages on the vertices, the *apply* function operates only on the vertices and updates its value and the *scatter* function computes the message to be sent along each edge and can be safely executed in parallel.\n\nGrapX uses bulk-synchronous model and adopts the GAS decomposition model.\n\n### GraphX Data Model\n\nThe GraphX Data Model consists of immutable collections and property graphs. Collections consist of unordered tuples (key-value pairs) and are used to represent unstructured data. The property graph combines the structural information (in the form of collections of vertices and edges) with properties describing this structure. Properties are just collections of form $(i, P_V (i))$ and $((i, j), P_E (i, j))$. The collection of vertices and edges are represented using RDDs (Resilient Distributed Datasets). Edges can be partitioned as per a user defined function. Within a partition, edges are clustered by source vertex id and there is an unclustered index on target vertex id. The vertices are hash partitioned by id and stored in a hash index within a partition. Each vertex partition contains a bitmask which allows for set intersection and filtering. It also contains a routing table that logically maps a vertex id to set of edge partitions containing the adjacent edges. This table is used when constructing triplets and is stored as a compressed bitmap.\n\n### Operators\n\nOther than standard data-parallel operators like `filter`, `map`, `leftJoin`, and `reduceByKey`, GraphX supports following graph-parallel operators:\n*   `graph` - constructs property graph given a collection of edges and vertices.\n*   `vertices`, `edges` - decompose the graph into a collection of vertices or edges by extracting vertex or edge RDDs.\n*   `mapV`, `mapE` - transform the vertex or edge collection.\n*   `triplets` -returns collection of form $((i, j), (P_V (i), P_E (i, j), P_V (j)))$. The operator essentially requires a multiway join between vertex and edge RDD. This operation is optimized by shifting the site of joins to edges, using the routing table, so that only vertex data needs to be shuffled.    \n*   `leftJoin` - given a collection of vertices and a graph, returns a new graph which incorporates the property of matching vertices from the given collection into the given graph without changing the underlying graph structure.\n*   `subgraph` - returns a subgraph of the original graph by applying predicates on edges and vertices\n*   `mrTriplets` (MapReduce triplet) - logical composition of triplets followed by map and reduceByKey. It is the building block of graph-parallel algorithms.\n\nAll these operators can be expressed in terms on relational operators and can be composed together to express different graph-parallel abstractions. The paper shows how these operators can be used to construct a enhanced version of Pregel based on GAS. It also shows how to express connected components algorithm and `coarsen` operator.\n\n### Structural Index Reuse\n\nCollections and graphs, being immutable, share the structural indexes associated within each vertex and edge partition to both reduce memory overhead and accelerate local graph operations. Most of the operators preserve the structural indexes to reuse them. For operators like subgraph which restrict the graph, the bitmask is used to construct the restricted view. \n\n### Distributed Join Optimization\n\n##### Incremental View Maintenance\n\nThe number of vertices that change between different steps of iterative graph algorithms decreases as the computation converges. After each operation, GraphX tracks which vertices have been changed by maintaining a bit mask. When materializing a vertex view, it uses values from the previous view for vertices which have not changed and ships only those vertices which are changed. This also allows for another optimization when using the `mrTriplets` operation: `mrTriplets` support an optional argument called *skipStale*. when this option is enabled, the `mrTriplets` function does not apply on edges origination from vertices that have not changed since its last iteration. This optimization uses the same bitmask that incremental views were using.  \n\n##### Automatic Join elimination\n\nGraphX has implemented a JVM bytecode analyzer that determines whether source/target vertex attributes are referenced in a mrTriplet UDF (for map) or not. Since edges already contain the vertex ids, a 3-way join can be brought down to 2-way join if only source/target vertex attributes are needed (as in PageRank algorithm) or the join can be completely eliminated if none of the vertex attributes are referenced.\n\n### Sequential Scan vs Index Scan\n\nUsing structural indices, while reduces computation cost in iterative algorithms, prevents physical data from shrinking. To counter this issue, GraphX switches from sequential scan to bitmap index scan when the fraction of active vertices drops below 0.8. Since edges are clustered by source vertex id, bitmap index scan can efficiently join edges and vertexes together.\n\n### Other Optimizations\n\n* Though GraphX uses Spark's shuffle mechanism, it materializes shuffled data in memory itself, unlike Spark which materializes shuffle data in disk and relies on OS buffer cache to cache the data. The rationale behind this modification is that graph algorithms tend to be communication intensive and inability to control when buffers are flushed can lead to additional overhead.\n* When implementing join step, vertices routed to the same target are batched, converted from row-orientation to column-orientation and compressed by LZF algorithm and then sent to their destination.\n* During shuffling, integers are encoded using a variable encoding scheme where for each byte, the first 7 bits encode the value, and the highest order bit indicates if another byte is needed for encoding the value. So smaller integers can be encoded with fewer bytes and since, in most cases, vertex ids are smaller than 64 bits, the technique helps to reduce an amount of data to be moved.\n\n\n### System Evaluation\n\nGraphX was evaluated against graph algorithms implemented over Spark 0.8.1, Giraph 1.0 and GraphLab 2.2 for both graph-parallel computation tasks and end-to-end graph analytic pipelines. Key observations:\n\n\n*    GraphLab benefits from its native runtime and performs best among all the implementations for both PageRank and Connected Components algorithm.\n*    For connected components algorithm, Giraph benefits from using edge cuts but suffers from Hadoop overhead.\n*    GraphX outperforms idiomatic implementation of PageRank on Spark, benefitting from various optimizations discussed earlier. \n*    As more machines are added, GraphX does not scale linearly but it still outperforms the speedup achieved by GraphLab (for PageRank).\n*    GraphX outperforms Giraph and GraphLab for a multi-step, end-to-end graph analytics pipeline that parses Wikipedia articles to make a link graph, runs PageRank on the link graph and joins top 20 articles with their text.\n\n\nGraphX provides a small set of core graph-processing operators, implemented on top of relational operators, by efficiently encoding graphs as a collection of edges and vertices with two indexing data structures. While it does lag behind specialised systems like Giraph and GraphLab in terms of graph-parallel computation tasks, GraphX does not aim at speeding up such tasks. It instead aims to provide an efficient workflow in end-to-end graph analytics system by combining data-parallel and graph-parallel computations in the same framework. Given that it does outperform all the specialised systems in terms of end-to-end runtime for graph pipelines and makes the development process easier by eliminating the need to learn and maintain multiple systems, it does seem to be a promising candidate for the use case it is attempting to solve.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1402.2394"
    },
    "412": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=MapReduce_Google",
        "transcript": "This paper, by Yahoo, describes a new language called Pig Latin which is intended to provide a middle ground between declarative SQL-style language (which many developers find unnatural) and procedural map-reduce model (which is very low-level and rigid). It also introduces a novel, interactive debugging environment called Pig Pen.\n\n#### Overview\nA Pig Latin program is a sequence of steps\u200a\u2014\u200aeach step carrying out a single high-level transformation. Effectively the program specifies the query execution plan itself. The program then compiles into map-reduce jobs which are run on Hadoop (though other backends can also be plugged in). Pig Latin is more expressive than map-reduce which is essentially limited to use a one-input, two-stage data flow model. Moreover, since the map and reduce functions are opaque for each other, optimizations are hard to bake in the system itself. This limitation is also overcome with Pig Latin which allows the programmer to order the execution of individual steps by specifying the execution plan.\nUnlike traditional DBMS, Pig does not require data to be imported into system managed tables as it meant for offline, ad-hoc, scan-centric, read-only workloads. Pig supports User Defined Functions (UDFs) out of the box. Since it targets only parallel processing, there is no inbuilt support for operations like non-equi joins or correlated subqueries. These operations can still be performed using UDFs.\n\n#### Nested Data Model\nPig supports a flexible, nested data model with 4 supported types-\n\n1. Atom: Simple values like string or integer. eg \u2018medium\u2019\n2. Tuple: Sequence of \u2018fields\u2019 which can be of any type. eg (\u2018medium\u2019, 12)\n3. Bag: Collection of tuples. eg {(\u2018medium\u2019, 12), ((\u2018github\u2019, \u2018twitter\u2019), 12)}\n4. Map: Collection of key-value pairs where keys are atoms and values can be any type. eg [\u2018key\u2019 -> \u2018value\u2019, \u2018anotherKey\u2019 -> (\u2018another\u2019, \u2018value\u2019)]\n\n#### Inbuilt functions\n1. FOREACH\u200a\u2014\u200aSpecifies how each tuple is to be processed. The semantics require that there should be no dependence between processing of different input tuples to allow parallel processing.\n2. COGROUP\u200a\u2014\u200aSuppose we have two datasets:\nresult = (query, url) //a query and its result url\nrevenue = (query, amount) //a query and revenue generated by the query.\nCogrouping these two datasets, we get\ngrouped_data = COGROUP results BY query, revenue BY query.\ngrouped_data would contain one tuple for each group. The first field of the tuple is the group identifier and the other fields are bags\u200a\u2014\u200aone for each dataset being cogrouped.So 1st bag would correspond to results and other to revenue. A sample dataset has been shown here.\nCOGROUP is one level lower than JOIN as it only groups together tuples into nested bags. It can be followed by an aggregate operation or cross product operation (leading to the result expected from JOIN operation). GROUP is same as COGROUP on a single dataset.\n3. Nested Operations where commands can be nested within FOREACH command to process bags within tuples.\n\nOther functions include\u200a\u2014\u200aLOAD, STORE, FILTER, JOIN, CROSS, DISTINCT, UNION and ORDER and are similar in operation to equivalent SQL operations.\nIt may be argued as to how does Pig differ from SQL-style query language when it seems to be using similar operations. Compare the following queries which generate the same result. The first one is written in SQL (declarative) and other in Pig (procedural)\n\n    SELECT category, AVG(pagerank) FROM urls WHERE pagerank > 0.2 GROUP BY category HAVING COUNT(*) > 106\n    good_urls = FILTER urls BY pagerank > 0.2;\n    groups = GROUP good_urls BY category;\n    big_groups = FILTER groups BY COUNT(good_urls)>106 ;\n    output = FOREACH big_groups GENERATE category, AVG(good_urls.pagerank);\n\n#### Implementation\nThe Pig interpreter parses the commands and verifies that the referred input files and bags are valid. It then builds a logical plan for every bag that is being defined using the logical plans for the input bags, and the current command. These plans are evaluated lazily to allow for in-memory pipelining and filter reordering. The parsing and logical plan construction are independent of the execution platform while the compilation of the logical plan into a physical plan depends on the execution platform.\nFor each COGROUP command, the compiler generates a map-reduce job where the map function assigns keys for grouping and the reduce function is initially a no-op. The commands intervening between two subsequent COGROUP commands A and B are pushed into the reduce function of A to reduce the amount of data to be materialized between different jobs. The operations before the very first COGROUP operation are pushed into the map function of A. The ORDER command compiles into two map-reduce jobs. The first job samples the input data to determine quantiles of the sort key. The second job range-partitions the input data according to the quantiles to provide equal-sized partitions, followed by local sorting in the reduce phase, resulting in a globally sorted file.\n\nThe inflexibility of the map-reduce primitive results in some overheads while compiling Pig Latin into map-reduce jobs. For example, data must be materialized and replicated on the distributed file system between successive map-reduce jobs. When dealing with multiple data sets, an additional field must be inserted in every tuple to indicate which data set it came from. However, the Hadoop map-reduce implementation does provide many desired properties such as parallelism, load balancing, and fault-tolerance. Given the productivity gains to be had through Pig Latin, the associated overhead is often acceptable. Besides, there is the possibility of plugging in a different execution platform that can implement Pig Latin operations without such overheads.\n\nSince the files reside in the Hadoop distributed file system, LOAD operation can run in parallel. Similarly, parallelism is achieved for FILTER and FOREACH operation as any map-reduce job runs several map and reduce instances in parallel. COGROUP operation runs in parallel by re-partitioning the output from multiple map instances to multiple reduce instances.\n\nTo achieve efficiency when working with nested bags, Pig uses Hadoop\u2019s combiner function to achieve a two-tier tree evaluation of algebraic functions. So all UDFs, of algebraic nature, benefit from this optimization. Of course, non-algebraic functions can not take advantage of this.\n\n#### Pig Pen\nIt is the interactive debugging environment that also helps to construct Pig Latin program. Typically the programmer would write the a program and run it on a dataset, or a subset of the dataset (if running over the entire dataset is too expensive) to check for correctness and change the program accordingly. Pig Pen can dynamically create a side data set (a subset of the complete dataset), called sandbox dataset, that can be used to test the program being constructed. This dataset is aimed to be real (ie subset of actual data), concise and complete(illustrate the key semantics of each command). While the paper does not go into the depth of how this dataset is created, it does mention that it starts by taking small, random samples of base data and synthesizes additional data tuples to improve completeness.\n\nWithin Yahoo, Pig Latin has been used in a variety of scenarios like computing rollup aggregates and performing temporal and session analysis. While Pig Latin does provide a powerful nested data model and supports UDFs making it easier to write and debug map-reduce jobs, it does not deal with issues like materializing and replicating data between successive map-reduce jobs. The paper argues that this overhead is acceptable given the numerous advantages Hadoop offers. Pig has come a long way since the paper was written. A lot of new functions have been added and it now comes with an interactive shell called Grunt. Moreover, now UDFs can be written in various scripting languages and not just Java. All these changes have made Pig more powerful and accessible than before.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://static.googleusercontent.com/external_content/untrusted_dlcp/labs.google.com/de//papers/mapreduce-osdi04.pdf"
    },
    "413": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nsdi/ZahariaCDDMMFSS12",
        "transcript": "The paper describes the architecture of RDDs (Resilient Distributed Datasets), what problems they can be used to solve, how they perform on different benchmarks and how they are different from existing solutions.\n\nMany generalized cluster computing frameworks, like MapReduce and Dryad, lack in two areas:\n\n1. Iterative algorithms where intermediate results are used across multiple computations.\n2. Interactive data analysis where users run ad-hoc queries on the data.\n\nOne way around these problems is to use specialized frameworks like Pregel. But this leads to loss of generality. This is the problem that RDD intends to solve\u200a\u2014\u200aby providing a general purpose, fault tolerant, distributed memory abstraction.\n\n#### RDD Overview\nRDDs are immutable partitioned collections that are created through deterministic operations on data in stable storage or other RDDs. They keep enough information about how they are derived from other sources (this information is called lineage). This lineage ensures that RDDs can be easily reconstructed in case of failures without having to perform explicit checkpointing. In fact, a program can not reference an RDD that it can not reconstruct after a failure. RDDs are lazy and ephemeral. They are constructed on demand and discarded after use. This allows for pipelining of many operations. For example:\n\n    rawData = spark.textfile(filepath) // read data from file\n    dataAfterApplyingFirstFilter = rawData.filter(condition1)\n    dataAfterApplyingSecondFilter = dataAfterApplyingFirstFilter.filter(condition2)\n    dataAfterApplyingSecondFilter.save()\n\nThe execution will take place on line 4, and the two filter conditions can be merged into a single condition to avoid multiple passes over the data.\n\n\n#### RDD Model\nRDDs provide an interface based on fine-grained reads and coarse-grained updates. This means transformations (functions) are applied to all data items. These transformations can be logged to build lineage graph so as to provide fault tolerance. But this update nature makes RDDs unsuitable for applications like incremental web crawler that needs asynchronous fine-grained updates to a shared state. In such cases, DSM (Distributed Shared Memory) would be a better choice as it provides fine-grained reads and writes. Although RDDs offer many advantages over DSM. First, unlike DSM, RDDs do not need to incur checkpointing overhead. Second, RDDs, being immutable, can mitigate stragglers (slow nodes), by running backup tasks just like MapReduce. Third, since only bulk writes are supported, run time can schedule tasks based on data locality to enhance performance. Lastly, even if RDDs choose to take checkpoints (in cases where the lineage graph grows very big), consistency is not a concern because of the immutable nature of RDDs.\n\nRDDs have been implemented in Spark to provide a language integrated API. Details about this implementation have been discussed here separately.\n\n#### Representing RDDs\nThe paper proposes a graph-based representation of RDDs where an RDD is expressed through a common interface that exposes five functions:\n\n1. partition\u200a\u2014\u200arepresents atomic pieces of the dataset.\n2. dependencies\u200a\u2014\u200alist of dependencies that an RDD has on its parent RDDs or data sources\n3. iterator \u2014a function that computes an RDD based on its parents\n4. partitioner\u200a\u2014\u200awhether data is range/hash partitioned.\n5. preferredLocation\u200a\u2014\u200anodes where a partition can be accessed faster due to data locality.\n\nThe most interesting aspect of this representation is how dependencies are expressed. Dependencies belong to one of the two classes:\n\n1. Narrow Dependencies\u200a\u2014\u200awhere each partition of the parent node is used by at most one child partition. For example, map and filter operations.\n2. Wide Dependencies\u200a\u2014\u200awhere multiple child partitions use a single parent partition.\n\nNarrow dependencies support pipelined execution on one cluster node while wide dependencies require data from all parent partitions to be available and to be shuffled across nodes. Recovery is easier with narrow dependencies while in the case of wide dependencies, failure of a single partition may require a complete re-execution. The figure shows some examples of narrow and wide dependencies. Note that join operation defines a narrow dependency when parents are hash-partitioned and wide dependency in other cases.\n\nhttps://cdn-images-1.medium.com/max/800/1*9I0CaywrdzUpg7RKbGlMow.png\n\nFigure 1: Example of narrow and wide dependencies.\n\n#### Job Scheduler\nWhenever an \u201caction\u201d is executed, the scheduler builds a DAG (Directed Acyclic Graph) of stages based on the lineage graph. Each stage would contain pipelined transformations with narrow dependencies. The boundaries between different stages are the shuffle operation which are required by wide dependencies. Some of these stages may be precomputed (due to the persistence of previous computations). For remaining tasks, the scheduler uses delay scheduling to assign tasks to machines based on data locality. For wide dependencies, intermediate records are materialized on nodes holding the parent partition.\n\n### Evaluation\nSpark outperforms Hadoop and HadoopBinMem for following reasons:\n\n1. Minimum overhead of Hadoop Stack as Hadoop incurs around 25 seconds of overhead to complete the minimal requirements of job setup, starting tasks, and cleaning up.\n2. Overhead of HDFS while serving data as HDFS performs multiple memory copies and a checksum to serve each block.\n3. Deserialization cost to convert binary data to in-memory Java objects.\n\nNote that HadoopBinMem converts input data to low-overhead binary format and stores it in an in-memory HDFS instance.\nCase studies also show that Spark performs well for interactive data analysis and other user applications. One limitation of the experiments is that in all the cases comparing the 3 systems, the cluster had sufficient RAM to keep all the data in-memory. It would have been interesting to compare the performance of the three systems in the case where the cluster does not have sufficient RAM to keep the entire data in main memory.\n\n#### Comparison with existing systems\nRDDs and Spark learn from and improve the existing systems in many ways.\n1. Data flow models like MapReduce share data through stable storage but have to incur the cost of data replication, I/O and serialization.\n2. DryadLINQ and FlumeJava provide language integrated APIs and pipeline data across operators in the same query. But unlike Spark, they can not share data across multiple queries.\n3. Piccolo and DSM do not provide a high-level programming interface like RDDs. Moreover, they use checkpointing and roll back which are more expensive than lineage based approach.\n4. Nectar, Ceil and FlumeJava do not provide in-memory caching.\n5. MapReduce and Dryad use lineage based recovery within a computation, but this information is lost after a job ends. In contrast, RDDs persists lineage information across computations.\n\nRDDs can be used to express many existing models like MapReduce, DryadLINQ, Pregel, Batched Stream Processing, etc. This seems surprising given that RDDs offer only a limited interface due to their immutable nature and coarse-grained transformations. But these limitations have a negligible impact on many parallel applications. For example, many parallel programs prefer to apply the same operation to many records to keep the program simple. Similarly, multiple RDDs can be created to represent different versions of the same data.\n\nThe paper also offers an interesting insight on the question of why previous frameworks could not offer the same level of generality. It says previous frameworks did not observe that \u201cthe common cause of these problems was a lack of data sharing abstractions\u201d.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.usenix.org/conference/nsdi12/technical-sessions/presentation/zaharia"
    },
    "414": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=dean2008mapreduce",
        "transcript": "The paper introduced the famous MapReduce paradigm and created a huge impact in the BigData world. Many systems including Hadoop MapReduce and Spark were developed along the lines of the paradigm described in the paper. MapReduce was conceptualized to handle cases where computation to be performed was straightforward but parallelizing the computation and taking care of other aspects of distributed computing was a pain.\n\nA MapReduce computation takes as input a set of key/value pairs and outputs another set of key/value pairs. The computation consists of 2 parts:\n\n1. Map\u200a\u2014\u200aA function to process input key/value pairs to generate a set of intermediate key/value pairs. All the values corresponding to each intermediate key are grouped together and sent over to the Reduce function.\n2. Reduce\u200a\u2014\u200aA function that merges all the intermediate values associated with the same intermediate key.\n\nThe Map/Reduce primitives are inspired by similar primitives defined in Lisp and other functional programming languages.\n\nA program written in MapReduce is automatically parallelized without the programmer having to care about the underlying details of partitioning the input data, scheduling the computations or handling failures. The paper mentions many interesting applications like distributed grep, inverted index and distributed sort which can be expressed by MapReduce logic.\n\n#### Execution Overview\nWhen MapReduce function is invoked, following steps take place:\n\n1. The input data is partitioned into a set of M splits of equal size.\n2. One of the nodes in the cluster becomes the master and rest become the workers. There are M Map and R Reduce tasks.\n3. The Map invocations are distributed across multiple machines containing the input partitions.\n4. Each worker reads the content of the partition and applies the Map function to it. Intermediate results are buffered in memory and periodically written back to local disk.\n5. The locations are passed on to the master which passes it on to reduce workers. These workers read the intermediate data and sort it.\n6. Reduce worker iterates over the sorted data and for each unique intermediate key, passes the key and intermediate values to Reduce function. The output is appended to an output file.\n7. Once all map and reduce tasks are over, the control is returned to the user program.\n\nAs we saw, the map phase is divided into M tasks and reduce phase into R tasks. Keeping M and R much larger than the number of nodes helps to improve dynamic load balancing and speeds up failure recovery. But since the master has to make O(M+R) scheduling decisions and keep O(M*R) states in memory, the value of M and R can not be arbitrarily large.\n\n#### Master Node\nThe master maintains the state of each map-reduce task and the identity of each worker machine. The location of the intermediate file also moves between the map and reduce operations via the master. The master pings each worker periodically. In case, it does not her back, it marks the worker as failed and assigns its task to some other worker. If a map task fails, all the reduce workers are notified about the newly assigned worker. Master failure results in computation termination in which case the client may choose to restart the computation.\n\n#### Locality\nMapReduce takes advantage of the fact that input data is stored on the machines performing the computations. The master tries to schedule a map job on a machine that contains the corresponding input data. Thus, most of the data is read locally and network IO is saved. This locality optimization is inspired by active disks where computation is pushed to processing elements close to the disk.\n\n#### Backup Tasks\nStragglers are machines that take an unusually long time to complete one of the last few Map or Reduce tasks and can increase the overall execution time of the program. To account for these, when a MapReduce operation is close to completion, the master schedules backup executions of remaining in-progress tasks. This may lead to some redundant computation but can help to reduce the start-to-end execution time.\n\n#### Refinements\nMapReduce supports a variety of refinements including:\n1. Users can specify an optional combiner function that performs a partial merge over the data before sending it over the network. This combiner operation would be performed after the Map function and would reduce the amount of data to be sent over the network.\n2. MapReduce can be configured to detect if certain records fail deterministically and skip those records. It is very useful in scenarios where a few missing records can be tolerated.\n3. Within a given partition, the intermediate key/value pairs are guaranteed to be processed in increasing key order.\n4. Side-effect is supported in the sense that MapReduce tasks can produce auxiliary files as additional outputs from their operation.\n5. MapReduce can be run in sequential mode on the local machine to facilitate debugging and profiling.\n6. The master runs an internal HTTP Server that exports status pages showing metadata of computation and links to output and error files.\n7. MapReduce provides a counter facility to keep track of occurrence of various events like the number of documents processed. Counters like the number of key-value pairs processed are tracked by default.\n\nOther refinements include custom partitioning function and support for different input/output types.\n\nMapReduce was profiled for two computations(grep and sort) running on a large cluster of commodity machines. The results were very solid. The grep program scanned ten billion records in 150 seconds and the sort program could sort ten billion records in 891 seconds (including the startup overhead). Moreover, the profiling showed the benefit of backup tasks and that the system is resilient to node failure.\nOther than performance, MapReduce offers many more benefits. The user code tends to be small and understandable as the code taking care of the distributed aspect is now abstracted. So the user does not have to worry about issues like machine failure and networking error. Moreover the system can be easily scaled horizontally. Lastly, the performance is good enough that conceptually unrelated computations can be maintained separately instead of having to mix every thing together in the name of saving that extra pass over the data.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "415": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=Chang2008",
        "transcript": "Bigtable is the distributed, scalable, highly available and high performing database that powers multiple applications across Google\u200a\u2014\u200aeach having a different demand in terms of the size of data to be stored and latency with which results are expected. Though Bigtable is not open source itself, the paper was crucial in the development of powerful open source databases like Cassandra (borrows BigTable\u2019s data model), HBase and LevelDB. The paper Bigtable: A Distributed Storage System for Structured Data describes the story of how BigTable is designed and how it works.\n\n#### Data Model\nBigtable is a sorted, multidimensional key-value store indexed by a row key, a column key, and a timestamp. For example in the context of storing web pages, the page URL could be the row key, various attributes of the page could be the column key and the time when the page was fetched could be the timestamp. Rows are maintained in lexicographic order and row range is dynamically partitioned with each row range being referred to as a tablet. Reads over short range are very fast due to locality and all read/write operations under a row are atomic. Column keys are grouped into sets called column family. These sets are created before any data is stored. A table should have a small number of distinct column families. A column key has the following syntax\u200a\u2014\u200afamily:qualifier. For example, for family as language, the keys could be language:english or language:hindi. A row in a Bigtable can contain multiple versions of same data, indexed by timestamps, and stored in decreasing order of timestamps. Old versions can be garbage collected in the way that the client can specify that only last n entries are to be kept or entries for only last n days(hours/weeks etc) are to be kept.\n\n#### Building Blocks\nBigtable uses Google File System (GFS) for storing logs and data in SSTable file format. SSTable provides an immutable, ordered (key, value) map. Immutability provides certain advantages which will be discussed later. An SSTable consists of a sequence of blocks and a block index to locate the blocks. When the SSTable is opened, this index is loaded into the main memory for fast lookup.\nBigtable also uses a highly available and persistent distributed lock service called Chubby for handling synchronization issues. Chubby provides a namespace consisting of directories and small files which can be used as locks. Read and write access to a file is atomic. Clients maintain sessions with a Chubby service which needs to be renewed regularly. Bigtable depends on Chubby so much that if Chubby is unavailable for an extended period of time, Bigtable will also be unavailable.\n\n#### Implementation\n\nThere are 3 major components\u200a\u2014\u200aa library linked into the client, one master server and multiple tablet servers. Master server assigns tablets to tablet servers, load balances these servers and garbage collect files in GFS. But the client data does not move through the master, clients directly contact tablet servers for read and write operations. Tablet servers manage a set of tablets\u200a\u2014\u200athey handle the read/write operations directed to these tablets and also split very large tablets.\n\nA 3-level hierarchy is maintained to store tablet location information. The first level is a file stored in Chubby that has the location of the root table. Root table contains the location of all tables in a METADATA tablet. Each entry in this METADATA tablet contains the location of a set of user tablets. These tablet locations are cached in the client library and can also be fetched from the above scheme by recursively moving up the hierarchy.\n\nEach tablet is assigned to one tablet server at a time. The master server keeps track of which servers are alive, which tablet is assigned to which server and which tablets are unassigned. When a tablet server restarts, it creates and acquires an exclusive lock on a unique file in the servers directory (Chubby directory) which is monitored by the master server. If the tablet server loses its exclusive lock, it will stop serving its tablets though it will attempt to reconnect as long as the file exists in the servers directory. In case the file is deleted, the tablet server kills itself. The master tracks tablets that are not being served and reassigns them. To avoid network partition issues, the master kills itself if its Chubby session expires though its tablets are not reassigned.\n\nA master performs following tasks at startup time:\n\n1. Grab the unique master lock to prevent concurrent master instantiation.\n2. Find live servers by scanning the servers directory.\n3. Find what tablets are assigned to each server by messaging them.\n4. Find the set of all tablets by scanning the METADATA tablet.\n\nTablet creation, deletion or merge is initiated by the master server while tablet partition is handled by tablet servers who notifies the master. In case the notification is lost, the master would be notified when it asks for the split tablet to be loaded.\n\nMemtables are in-memory buffers to store recently committed updates to Bigtable. These updates are later written to GFS for persistent storage. Tablets can be recovered by reading the metadata from METADATA tablet which contains a set of Redo points and list of SSTables comprising the tablet.\n\nThere are 3 kinds of compactions in action:\n\n1. Minor compaction\u200a\u2014\u200aAs the memtable grows in size and reaches a certain threshold, it is frozen, a new memtable is created and the frozen memtable is written to GFS.\n2. Merging Compaction\u200a\u2014\u200aMinor compaction keeps increasing the count of SSTables. Merging compaction reads the contents of a few SSTables and the memtable and writes it to a new SSTable.\n3. Major Compaction\u200a\u2014\u200aIn Major compaction, all the SSTables are written into a single SSTable.\n\n#### Refinements\nAlong with the described implementation, several refinements are required to make the system reliable and high performing.\n\nClients can club together column families into locality groups for which separate SSTables are generated. This allows for more efficient reads. If SSTables are small enough, they can be kept in main memory as well.\n\nSSTables can be compressed in various formats. The compression ratio gets even better when multiple versions of same data are stored.\n\n**Caching**\u200a\u2014\u200aTablet servers employ 2 levels of caching.\n\n1. Scan Cache\u200a\u2014\u200aIt caches (key, value) pairs returned by the SSTable and is useful for applications that read same data multiple times.\n2. Block Cache\u200a\u2014\u200aIt caches SSTable blocks read from GFS and useful when the application uses locality of reference.\n\n**BloomFilters** are created for SSTables (particularly for the locality groups). They help to reduce the number of disk access by predicting if an SSTable may contain data corresponding to a particular row, column pair.\n\n**Commit Logs** are maintained at tablet server level instead of tablet level to keep the number of log file small. Each tablet server maintains 2 log writing threads\u200a\u2014\u200aeach writing to its own and separate log file and only one of the threads is active at a time. If one of the threads is performing poorly (say due to network congestion), the writing switches to other thread. Log entries have sequence numbers to allow recovery process.\n\nWe earlier saw that SSTable entries are immutable. The advantage is that no synchronization is needed during read operations. This also makes it easier to split tablets. Permanently removing deleted data is taken care of by garbage collector.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "416": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigmod/ArmbrustXLHLBMK15",
        "transcript": "This paper talks about how Spark SQL intends to integrate relational processing with Spark itself. It builds on the experience of previous efforts like Shark and introduces two major additions to bridge the gap between relational and procedural processing:\n\n1. DataFrame API that provides a tight integration between relational and procedural processing by allowing both relational and procedural operations on multiple data sources.\n2. Catalyst, a highly extensible optimizer which makes it easy to add new data sources and algorithms.\n\n#### Programming Interface\nSpark SQL uses a nested data model based on Hive and supports all major SQL data types along with complex (eg array, map, etc) and user-defined data types. It ships with a schema inference algorithm for JSON and other semistructured data. This algorithm is also used for inferring the schemas of RRDDs (Resilient Distributed Datasets) of Python objects. The algorithm attempts to infer a static tree structure of STRUCT types (which in turn may contain basic types, arrays etc) in one pass over the data. The algorithm starts by finding the most specific Spark SQL type for each record and then merges them using an associative most specific supertype function that generalizes the types of each field.\nA DataFrame is a distributed collection of rows with the same schema. It is equivalent to a table in an RDBMS. They are similar to the native RDDs of Spark as they are evaluated lazily, but unlike RDDs, they have a schema. A DataFrame represents a logical plan and a physical plan is built only when an output function like save is called. Deferring the execution in this way makes more space for optimizations. Moreover. DataFrames are analyzed eagerly to identify if the column names and data types are valid or not.\n\nDataFrames supports query using both SQL and a DSL which includes all common relational operators like select, where, join and groupBy. All these operators build up an abstract syntax tree (AST) of the expression (think of an expression as a column in a table), which is then optimized by the Catalyst. Spark SQL can cache data in memory using columnar storage which is more efficient than Spark\u2019s native cache which simply stores data as JVM objects. The DataFrame API supports User-defined functions (UDFs) which can use the full Spark API internally and can be registered easily.\nTo query native datasets, Spark SQL creates a logical data scan operator (pointing to the RDD) which is compiled into a physical operator that accesses fields of the native objects in-place, extracting only the fiel needed for a query. This is better than traditional object-relational mapping (ORM) which translates an entire object into a different format.\nSpark MLlib implemented a new API based on pipeline concept (think of a pipeline as a graph of transformations on the data) and choose DataFrame as the format to exchange data between pipeline stages. This makes is much easier to expose MLlib\u2019s algorithms in Spark SQL.\n\n#### Catalyst\nCatalyst is an extensible optimizer based on Scala\u2019s standard features. It supports both rule-based and cost-based optimizations and makes it easy to add new optimization techniques and data sources to Spark SQL. At its core, Catalyst is powered by a general library that represents and manipulates trees by applying rules to them. Tree is the main data type in Catalyst and is composed of node objects where a node has a type and zero or more children. Rules are functions to transform one tree to another. Trees offer a transform method that applies a pattern matching function recursively on all the nodes of the tree, transforming only the matching nodes. Rules can match multiple patterns in the same transform call and can contain arbitrary Scala code which removes the restriction of using a Domain Specific Language (DSL) only. Catalyst groups rules into batches and executes each batch until it reaches a fixed point (ie the tree stops changing). This means that each rule can be simple and self-contained while producing a global effect on the tree. Since both nodes and trees are immutable, optimizations can be easily performed in parallel as well.\nSpark SQL uses Catalyst in four phases:\n\n**Logical Plan Analysis** which requires resolving attribute references (one for which we do not know the type or which have not been matched to an input table). It uses a Catalog object to track the tables in all data sources to resolve references.\n\n**Logical Optimization** phase applies standard rule-based optimizations to the logical plan which include constant folding, predicate pushdown, projection pruning, null propagation, Boolean expression simplification, etc.\n\nIn **Physical Planning** phase, Spark SQL generates multiple physical plans corresponding to a single logical plan and then selects one of the plans using a cost model. It also performs some rule-based physical optimizations like the previous case.\n\nIn **Code Generation** phase, Catalyst uses quassiquotes (provided by Scala) to construct an AST that can be fed to Scala Compiler and bytecode can be generated at runtime.\n\nExtensions can be added even without understanding how Catalyst works. For example, to add a data source, one needs to implement a createRelation function that takes a set of key-value parameters and returns a BaseRelation object if successfully loaded. This BaseRelation can then implement interfaces to allow Spark SQL access to data. Similarly to add user-define types (UDTs), the UDTs are mapped to Catalyst\u2019s inbuilt types. So one needs to provide a mapping from an object of UDT to a Catalyst row of built in types and an inverse mapping back.\n\n#### Future Work\nSome recent work has also shown that it is quite easy to add special planning rules to optimize for specific use cases. For example, researchers in ADAM Project added a new rule to use interval trees instead of using normal joins for a computational genomics problem. Similarly, other works have used Catalyst to improve generality of online aggregation to support nested aggregate queries. These examples show that Spark and Spark SQL is quite easy to adapt to new use cases as well. As I mentioned previously, I am experimenting with Spark SQL and it does look promising. I have implemented some operators and it is indeed quite easy to extend. I am now looking forward to developing more concrete thing on top of it.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2723372.2742797"
    },
    "417": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/hotcloud/ZahariaCFSS10",
        "transcript": "The paper introduces a new framework called Spark which focuses on use cases where MapReudce fails. While a lot of applications fit the MapReduce\u2019s acyclic data flow model, there are use cases requiring iterative jobs where MapReduce is not greatly efficient. Many machine learning algorithms fall into this category. Secondly with MapReduce, each query incurs a significant overhead because it is effectively a different job each time. So MapReduce is not the ideal solution for doing interactive analysis. This is the space which Spark intends to fill in. Spark is implemented in Scala and now supports API in Java, Python, and R\n\n#### Programming Model\nThe model supports RDDs, parallel operations and 2 type of shared variables. A driver program implements the high-level control flow and launches different operations in parallel.\n\nResilient Distributed Datasets (RDDs) are a read-only collection of objects, partitioned across a set of machines in a cluster. A handle to RDD contains enough information to compute the RDD from data in case of partition failure. RDDs can be constructed:\n\n1. From a file in a Hadoop supported file system.\n2. By \u201cparallelizing\u201d a Scala collection.\n3. By transforming an existing RDD using operations like flatMap, map, and filter.\n4. By changing persistence of an existing RDD.\n\nflatMap, map and filter are standard operations as supported by various functional programming languages. For example map will take as input a list of items and return a new list of items after applying a function to each item of the original list.\n\nRDDs are lazy and ephemeral. They are constructed on demand and discarded after use. The persistence can be changed to cache which means they are still lazy but are kept in memory (or disk if they can not fit memory) or to save where they are saved to disk only.\n\nSome supported parallel operations(at time of writing the paper) include:\n\n1. reduce: Combines dataset elements using an associative function to produce a result at the driver program.\n2. collect: Sends all elements of the dataset to the driver program.\n3. foreach: Passes each element through a user-provided function\n\nSince the paper was authored, Spark has come a long way and support much more transformations (sample, union, distinct, groupByKey, reduceByKey, sortByKey, join, cartesian, etc), parallel operations (shuffle, count, first, take, takeSample etc), and persistence options (memory only, memory and disk, disk only, memory only serialized, etc).\n\nSpark supports 2 kinds of shared variables:\n\nBroadcast variables\u200a\u2014\u200aThese are read-only variables that are distributed to worker nodes for once and can be used multiple times (for reading). A use case of this variable would be training data which can be sent to all the worker nodes once and can be used for learning different models instead of sending the same data with each model.\n\nAccumulators\u200a\u2014\u200aThese variables are also shared with workers, the different being that the driver program can only read them and workers can perform only associative operations on them. A use case could be when we want to count the total number of entries in a data set, each worker fills up its count accumulator and sends it to the driver which adds up all the received values.\n\n#### Implementation\nThe core of Spark is the implementation of RDDs. Suppose we start by reading a file, then filtering the lines to get lines with the word \u201cERROR\u201d in them, then we cache the results and then count the number of such lines using the standard map-reduce trick. RDDs will be formed corresponding to each of these steps and these RDDs will be stored as a link-list to capture the lineage of each RDD.\n\nhttps://cdn-images-1.medium.com/max/800/1*6I7aiD2bPrsw32U76Q5q2g.png\n\nLineage chain for distributed dataset objects\nEach RDD contains a pointer to its parent and information about how it was transformed. This lineage information is sufficient to reconstruct any lost partition and checkpointing of any kind is not required. There is no overhead if no node fails and even if some nodes fails, only select RDDs need to be reconstructed.\n\nInternally an RDD object implements a simple interface consisting of three operations:\n\n1. getPartitions, which returns a list of partition IDs.\n2. getIterator(partition), which iterates over a partition.\n3. getPreferredLocations(partition), which is used for task scheduling to achieve data locality.\n\nSpark is similar to MapReduce\u200a\u2014\u200ait sends computation to data instead of the other way round. This requires shipping closures to workers\u200a\u2014\u200aclosures to define and process a distributed dataset. This is easy given Scala uses Java serialization. However unlike MapReduce, operations are performed on RDDs that can persist across operations.\n\nShared variables are implemented using classes with custom serialization formats. When a broadcast variable b is created with a value v, v is saved to a file in the shared file system. The serialized form of b is a path to this file. When b\u2019s value is queried, Spark checks if v is in the local cache. If not, it is read from the file system. For accumulators, each accumulator is given a unique ID upon creation and its serialized form contains its ID and the \u201czero\u201d value. On the workers, a separate copy of the accumulator is created for each thread and is reset to \u201czero\u201d value. Once the task finishes, the updated value is sent to the driver program.\n\n#### Future Work\nThe paper describes how an early stage implementation performs on Logistic Regression, Alternating Least Square, and interactive mode. The results seem to outperform MapReduce largely because of caching the results of previous computations. This makes Spark a good alternative for use cases where same data is read into memory again and again (iterative jobs fit the category.) Spark has come a long way since the paper was written. It now supports libraries for handling SQL-like queries (SparkSQL), streaming data (Spark streaming), graphs (GraphX) and machine learning (MLlib) along with more transformations and parallel operations. I came across Spark while working at Adobe Analytics and have been reading about it to learn more. The cool thing about Spark is that it supports interactive analysis and has APIs in Python, R and Java thus making it easy to adopt. While I have not done some much work around Spark, I am looking forward to making something on top of it.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.usenix.org/conference/hotcloud-10/spark-cluster-computing-working-sets"
    },
    "418": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1210.7350",
        "transcript": "Twitter\u2019s Real-Time Related Query Suggestion Architecture. The paper tells the story behind architecture to support Twitter\u2019s real-time related query suggestion, why the architecture had to be designed twice and what lessons can be learned from this exercise. It does not talk much about the algorithms, rather it talks about the different design decisions that lead to the current architecture\u200a\u2014\u200athe focus is not on how things were done but why they were done a certain way.\n\nTwitter has an interesting use case\u200a\u2014\u200asearch assistance\u200a\u2014\u200awhich boils down to things like a user searching for \u201cObama\u201d and getting results for related queries like \u201cWhite House\u201d as well. Spelling correction is also a part of search assistance. The problem is quite well researched from volume perspective of data but in Twitter\u2019s context, velocity mattered as much as volume. The results had to adapt to rapidly evolving global conversations in real-time\u200a\u2014\u200awhere real-time loosely translates to a target latency of 10 minutes. The real-time sense is important in Twitter\u2019s context where \u201crelevance\u201d has a temporal aspect as well. For example, after the Nepal Earthquake in 2015, the query \u201cNepal\u201d led to results related to \u201cearthquake\u201d as well. Then when the new constitution was passed in Nepal, the query \u201cNepal\u201d led to results related to \u201cconstitution\u201d. The time frame in which suggestions have maximum impact is very narrow. A suggestion made too early would seem irrelevant while a suggestion made too late would seem obvious and hence less impactful. These fast moving signals have to be mixed with slow moving ones for insightful results. Sometimes the query volume is very low in which case longer observation period is needed before suggestions can be made. Twitter noticed that 17% of top 1000 query terms churn over an hourly basis which means they are no longer in top 1000 after an hour. Similarly, around 13% of top 1000 query terms are churned out every day. This suggested that a fine-grained tracking of search terms was needed.\n\nTwitter started with a basic idea: if two queries are seen in the same context, then they are related. Now they had a large open design space. For example, context can be defined by user\u2019s search session or tweet or both. Measures like log likelihood, chi-square test etc can be used to quantify how often 2 queries appear together. To consider the temporal effect, counts are decayed time. Finally, Twitter has to combine these factors, and some more factors, together to come up with a ranking mechanism. This paper does not focus on what algorithms were chosen for these tasks, it focuses on how an end-to-end system was created.\n\n#### Hadoop Solution\nTwitter has a powerful petabyte-scale Hadoop-based analytics platform. Both real-time and batch processes write data to the Hadoop Distributed File System (HDFS). These include bulk exports from databases, application logs, and many other sources. Contents are serialized using either Protocol Buffers or Thrift, and LZOcompressed. There is a work-flow manager, called Oink, which schedules recurring jobs and handles dataflow dependencies between jobs. For example, if job B requires data generated by job A, A will be scheduled first.\n\nTwitter wanted to take advantage of this stack and the first version was deployed in form of a Pig script that aggregated user search sessions to compute term and cooccurrence statistics and ranked related queries on top of the existing stack. While the results were pretty good, the latency was too high and results were not available until several hours.\n\n#### Bottleneck #1 Log Imports\nTwitter uses Scribe to aggregate streaming log data in an efficient manner. These logs are rich with user interaction and are used by the search assistant. A Scribe daemon is running on each production server where it collects and sends local log data (consisting of category and message) to a cluster of aggregators which are co-located with a staging Hadoop cluster. This cluster merges per-category streams from the server daemons and writes the results to HDFS of the staging cluster. These logs are then transformed and moved to the main Hadoop data warehouse in chunks of data for an hour. These log messages are put in per-category, per-hour directories and are bundled in a small number of large files. Only now can the search assistant start its computations. The hierarchical aggregation is required to \u201croll up\u201d data into few, large files as HDFS is not good at handling large numbers of small files. As a result, there is a huge delay from when the logs are generated to when they are available for processing. Twitter estimated that they could bring down the latency to tens of minutes by re-engineering their stack though even that would be too high.\n\n#### Bottleneck #2 Hadoop:\nHadoop is not meant for latency sensitive jobs. For example, a large job could take tens of seconds to just startup\u200a\u2014\u200airrespective of the amount of data crunched. Moreover, the Hadoop cluster was a shared resource across Twitter. Using a scheduler (in this case, FairScheduler) is not the ideal solution as the focus is on predictable end-to-end latency bound and not resource allocation. Lastly, the job completion time depending on stragglers. For some scenarios, a simple hash partitioning scheme created chunks of \u201cwork\u201d with varying size. This lead to large varying running times for different map-reduce jobs. For scripts that chain together Hadoop jobs, the slowest task becomes the bottleneck. Just like with log imports, Twitter estimated the best case scenario for computing query suggestions to be of the order of ten minutes.\n\nStarting with the Hadoop stack had many advantages like a working prototype was built quickly and ad hoc analysis could be easily done. This also helped them to understand the query churn and make some important observations about factors to use in search assistant. For example, Twitter discovered that only 2 sources of context\u200a\u2014\u200asearch sessions and tweets\u200a\u2014\u200awere good enough for an initial implementation.But due to high latency, Twitter had to restrict this solution to the experimental stage itself.\n\n#### Current Architecture\nFirehose is the streaming API that provides access to all tweets in real time and the frontend, called Blender, brokers all requests and provides a streaming API for queries\u200a\u2014\u200aalso called query hose. These two streams are used by EarlyBird, the inverted indexing engine, and search assistant engine. Now client logs are not needed as Blender has all search sessions. Twitter search assistance is an in-memory processing engine comprising of two decoupled components:\n\n1. Frontend Nodes\u200a\u2014\u200aThese are lightweight in-memory caches which periodically read fresh results from HDFS. They are implemented as a Thrift service, and can be scaled out to handle increased query load.\n2. Backend Nodes\u200a\u2014\u200aThese nodes perform the real computations. The backend processing engine is replicated but not sharded. Every five minutes, computed results are persisted to HDFS and every minute, the frontend caches poll a known HDFS location for updated results.\n\nRequest routing to the replicas is handled by a ServerSet, which provides client-side load-balanced access to a replicated service, coordinated by\nZooKeeper for automatic resource discovery and robust failover.\n\nEach backend instance is a multi-threaded application that consisting of:\n1. Stats collector: Reads the firehose and query hose\n2. In-memory stores: Hold the most up-to-date statistics\n3. Rankers: Periodically execute one or more ranking algorithm by getting raw features from the in-memory stores.\n\nThere are three separate in-memory stores to keep track\nof relevant statistics:\n1. Sessions store: Keeps track of (anonymized) user sessions observed in the query hose, and for each session, the history of the queries issued in a linked list. Sessions older than a threshold are discarded. Metadata is also tracked separately.\n2. Query statistics store: Retains up-to-date statistics, like session count, about individual queries. These also include a weighted count based on a custom scoring function. This function captures things like association is more between 2 consecutively typed queries vs 2 consecutively clicked hash-tags. These weights are periodically decayed to reflect decreasing importance over time. It also keeps additional metadata about the query like its language.\n3. Query cooccurrence statistics store: Holds data about\npairs of co-occurring queries. Weighting and decaying are applied like in the case of query statistics store.\n\n**Query Flow**\u200a\u2014\u200aAs a user query flows through the query hose, query statistics are updated in the query statistics store, it is added to the sessions store and some old queries may be removed. For each previous query in the session, a query cooccurrence is formed with the new query and statistics in the query cooccurrence statistics store are also updated.\n\n**Tweet Flow**\u200a\u2014\u200aAs a tweet flows through the firehose, its n-grams are checked to determine whether they are query-like or not. All matching n-grams are processed just like the query above except that the \u201csession\u201d is the tweet itself.\n\n**Decay/Prune cycles**\u200a\u2014\u200aPeriodically, all weights are decayed and queries or co-occurrences with scores below predefined thresholds are removed\nto control the overall memory footprint of the service. Even user sessions with no recent activity are pruned.\n\n**Ranking cycles**\u200a\u2014\u200aRankers are triggered periodically to generates suggestions for each query based on the various accumulated statistics. Top results are then persisted to HDFS.\n\n#### Scalability\n\n1. Since there is no sharding, each instance of the backend processing engine must consume the entire firehose and query hose to keep up with the upcoming data.\n\n2. The memory footprint for retaining various statistics, without any pruning, is very large. But if the footprint is reduced, by say pruning, the quality and coverage of results may be affected. Another approach could be to store less session history and decay the weights more aggressively though it may again affect the quality of the results.\n\n#### Lessons Learnt\nTwitter managed to solve the problem of fast moving big data but their solution is far from ideal. It works well but only for scenario it is fine-tuned for. What is needed is a unified data platform to process for big and fast moving data with varying latency requirements. Twitter\u2019s current implementation is an in-memory engine which mostly uses tweets and search sessions to build the context. Rich parameters like clicks, impressions etc are left out for now to keep the latency under check. Twitter described it as \u201ca patchwork of different processing paradigms\u201d.\n\nThough not-so-complete, it is still an important step in the direction of unifying big data and fast data processing systems. A lot of systems exists which solve the problem is pieces eg message queues like Kafka for moving data in real time and Facebook\u2019s ptail for running Hadoop operations in real time but there is no end-to-end general data platform which can adapt itself to perform analytics on both short and long term data and combine their results as per latency bound in different contexts.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1210.7350v1"
    },
    "419": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nsdi/NishtalaFGKLLMPPSSTV13",
        "transcript": " The paper captures the story of how Facebook created a high-performing, distributed key-value store on top of Memcache (which was back then a simple, in-memory cache) and scaled it to support the world\u2019s largest social network.\n\nIn Facebook\u2019s context, users consume much more content than they create. So the workload is read intensive and caching help to reduce the workload.MORE ON IT. Facebook uses Memcache Clusters where each Memcache instance is a demand-filled, look-aside cache. This means if a client requests data from Memcache and data is not available, the client would fetch the data from the database and would populate the cache for further requests. Memcache, not being a loading cache, does not have to worry about the logic of retrieving data from the database and can be easily plugged with multiple databases. In case of write requests, the client issues an update command to the database and a delete command is sent to Memcache. Deletion, being idempotent, is preferred over updation. While the overview seems pretty simple, there are more details in actual implementation. Facebook considers and optimizes for 3 deployments scales\u200a\u2014\u200aa single cluster of servers, data replication between different clusters and spreading clusters across the world.\n\n#### A single cluster of servers\nThese are characterized by a highly read intensive workload with requests having a wide fan out. Around 44% of requests contact more than 20 Memcache servers. For popular pages, this number spans well over 100 distinct servers. One reason for this is that each request returns a very small amount of data. In case of get requests, UDP performs better than TCP and get errors are treated as cache miss though insertion is not performed. This design choice seems practical as only .25% of requests fail due to late/ dropped or out of order packets. Though the response size is very small, the variation is quite large with mean = 954 bytes and median = 135 bytes. Set and delete operations are still performed over TCP (for reliability) though the connections are coalesced to improve efficiency.\n\nWithin a cluster, data is distributed across hundreds of servers through consistent hashing. A very high request rate combined with large fanout leads to an all to all communication between Memcache servers and clients and even a single server can become a bottleneck for many requests. Clients construct a DAG representing the dependency between data so that more independent requests are fired concurrently. Facebook also provides a standalone proxy called mcrouter that acts as an interface to Memcache server interface and routes the requests/replies to/from other servers. Along with these, flow control mechanisms in the form of sliding window mechanism are provided to limit incast congestion.\n\n#### Lease\nLeases are used to address stale sets (when web server writes a stale value in the cache) and thundering herds (when a key undergoes heavy read and write operations). When a client experiences a cache-miss, Memcache gives it a lease (a 64-bit token bound to the requested key). This lease is verified by Memcache when client tries to set the value. If Memcache receives a delete request for the key, the lease is invalidated and the value can not be set. To mitigate thundering herds, Memcache returns a token only once every 10 seconds per key. If a read request comes within 10 seconds of a token issue, the client is notified to retry after a short time, by which the updated value is expected to be set in the cache. In situations where returning stale data is not much problem, the client does not have to wait and stale data (at most 10 second old) is returned.\n\n#### Memcache Pools\nSince different workloads can have different access patterns and requirements, Memcache clusters are partitioned into pools. The default pool is called wildcard and then there are separate pools for keys that can not reside in the wildcard pool. A small pool can be provisioned for keys for which cache miss is not expensive. Data is replicated within a pool when request rate is quite high and data can easily fit into a few machines. In Facebook\u2019s context, replication seems to work better than sharding though it has the additional overhead of maintaining consistency.\nIn case a few Memcache servers fail, a small set of machines, called gutters, take over. In case of more widespread failures, requests are diverted to alternate clusters.\nRegions\n\nMultiple front end clusters (web and Memcache clusters) along with a storage cluster (database) defines a region. The storage cluster has the authoritative copy of the data which is replicated across the frontend clusters. To handle data modifications, invalidation daemons called mcsqueal are deployed on each database which parse the queries, extract and group delete statements and broadcast them to all the front end cluster in the region. The batched delete operations are sent to mcrouter instances in each frontend cluster, which then unpack the individual deletes and route them to the concerned Memcache server. As an optimisation, a web server which modifies its data also sends invalidations to its own cluster to ensure read-after-write semantics for a single user request.\n\n#### Cold cluster Warmup\nWhen a new cluster is brought online, it takes time to get populated and initially the cache hit rate is very low. So a technique called Cold Cluster Warmup is employed where a new cold cluster is populated with data from a warm cluster instead of the database cluster. This way the cold cluster comes to full capacity within few hours. But additional care is needed to account for race conditions. One example could be: a client in cold cluster makes an update and before this update reaches the warm cluster, another request for the same key is made by the cold cluster then the item in the cold cache would be indefinitely inconsistent. To avoid this, Memcache rejects add operations for 2 seconds (called holdoff time)once a delete operation is taken place. So if a value is updated in a cold cluster and a subsequent request is made within 2 seconds, the add operation would be rejected indicating that the data has been updated. 2 seconds is chosen as most updates seems to propagate within this time.\n\n#### Across Region consistency\nClusters comprising a storage cluster and several front end clusters are deployed in different regions. Only one of the regions contains the master database and rest act as replicas. This adds the challenge of synchronisation.\n\nWrites from a master region send invalidations only within the master region. Sending invalidations outside may lead to a race situation where deletes reach before data is replicated. Facebook uses mcsequal daemon helps to avoid that at the cost of serving stale data for some time.\n\n'Writes from a non-master region are handled differently. Suppose a user updates his data from a non-master region with a large replication lag. A cache refill from a replica database is allowed only after the replication stream has caught up. A remote marker mechanism is used to minimise the probability if reading stale data. The presence of a marker indicates that data in the replica is stale and the query is redirected to the master region. When a webserver updates a key k, it sets a remote marker rk in the region, performs the write to the master database having key k and deletes k in the local cluster. When it tries to read k next time, it will experience a cache miss, will check if rk exists and will redirect its query to the master cluster. Had rk not been set, the query would have gone to the local cluster itself. Here latency is introduced to make sure most updated data is read.\n\n#### Single Server Improvements\nFacebook introduced many improvements for Memcache servers running as single instances as well. This includes extending Memcache to support automatic expansion of the hash table without the look-up time drifting to $O(n)$, making the server multi-threaded using a global lock and giving each thread its own UDP port to reduce contention.\n\nMemcache uses an Adaptive Slab Allocator which organizes memory into slab classes\u200a\u2014\u200apre-allocated, uniformly sized chunks of memory. Items are stored in smallest possible slab class which can fit the data. Slab sizes start at 64 bytes and reach up to 1 Mb. Each slab class maintains a free-list of available chunks and requests more memory in 1MB in case the free-list is empty. If no more free memory can be allocated, eviction takes place in Least Recently Used (LR) fashion. The allocator periodically rebalances slab assignments to keep up with the workload. Memory is freed in less used slabs and given to more heavily used slabs.\n\nMost of the items are evicted lazily from the memory though some are evicted as soon as they are expired. Short lived items are placed into a circular buffer of linked lists (indexed by seconds until expiration)\u200a\u2014\u200acalled Transient Item Cache\u200a\u2014\u200abased on the expiration time of the item. Every second, all of the items in the bucket at the head of the buffer are evicted and the head advances by one. By adding a short expiration time to heavily used set of keys whose items with short useful lifespans, the proportion of Memcache pool used by this key family was reduced from 6% to 0.3% without affecting the hit rate.\n\n#### Lessons Learnt\nMemcache\u2019s design decisions are driven by data and not just intuition. Facebooks seems to have experimented with a lot of configurations before arriving on decisions like using UDP for read operations and choosing the value for parameters like holdoff time. This is how it should be\u200a\u2014\u200adata-driven decisions. In the entire process of developing Memcache, Facebook focused on optimizations which affect a good number of users and usecases instead of optimizing for each possbile use case.\n\nFacebook separated caching layer from persistence layer which makes it easier to mould the 2 layers individually. By handling the cache insertion logic in the application itself, Facebook made it easier to plug Memcache with different data stores. By modeling the probability of reading the stale data as a parameter, they allowed the latency and hit rate on the persistence store to be configurable to some extent thus broadening the scope for Memcache. Facebook breaks down a single page load request into multiple requests, thereby allowing for different kind of stale data tolerance for each component. For example, the servers serving the profile picture can cache content longer than the servers serving messages. This helps to lower the response latency and reduce the load on the data layer.\n\nFacebook\u2019s Memcache is one of the many solutions aimed to solve a rather common problem\u200a\u2014\u200aa scalable, distributed key-value store. Amazon\u2019s Dynamo solves is well for a write-intensive workload for small data sizes and Facebook solves it for a read intensive workload. Other contenders in the list are Cassandra, Voldemort, Redis, Twitter\u2019s Memcache and more. The sheer number of possible, well known solutions suggests that there are many dimensions to the problem, some of which are yet unknown and that we still do not have a single solution that can be used for all use cases.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.usenix.org/conference/nsdi13/technical-sessions/presentation/nishtala"
    },
    "420": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sosp/DeCandiaHJKLPSVV07",
        "transcript": "Amazon\u2019s platform is built upon different techniques working together to provide a single powerful, highly-available system. One of the core components powering this system is Dynamo. There are many services in the Amazon ecosystem which store and retrieve data by primary key. Take the example of the shopping cart service. Customers should be able to view and update their cart anytime. For these services, sophisticated solutions like RDBMS, GFS, etc are an overkill as these services do not need a complex query and data management system. Instead, they need a service which only supports read and write operations on a (key, value) store where the value is a small object (less than 1Mb in size) uniquely identified by the key. The service should be scalable and highly available with a well-defined consistency window. This is what Dynamo is: a scalable, distribute, highly available key-value store that provides an \u201calways-on\u201d experience.\n\n#### Design Considerations\nDynamo achieves high availability at the cost of weaker consistency. Changes propagate to the replicas in the background and conflict resolution is done at read time to make sure none of the write operations can fail. Dynamo uses simple policies like \u201clast write win\u201d for conflict resolution though applications using Dynamo may override these techniques with their own methods. Eg the cart application may choose to add items across all versions to make sure none of the items is lost.\n\nA service could depend on multiple services to get its results. To guarantee that a service returns its results in a bounded time, each dependency in the service has to return its results with even tighter bounds. As a result clients enter into a contract with servers regarding service-related characteristics like expected request distribution rate, expected latency and so on. Such an agreement is called Service Level Agreement(SLA) and must be met to ensure efficiency. SLA apply in the context of Dynamo as well.\n\nDynamo supports incremental scaling where the system is able to scale out one node at a time. Moreover, all the nodes are symmetrical in the sense they have the same set of responsibilities. Since Dynamo is used only by Amazon\u2019s internal applications, there are no security related requirements like authentication and authorization .\n\n#### Architecture\nDynamo exposes two operations: get() and put(). get(key) returns value or list of values along with context objects corresponding to the key. put(key, context, value) stores the value and the context corresponding to the key. context objects are used for conflict resolution.\n\nTo support incremental scaling, Dynamo uses consistent hashing for its partitioning scheme. In consistent hashing, the output range of a hash function is treated as a fixed circular space. Each node and data object is assigned a random value or position within this space. A data object is mapped to the first node which is placed clockwise to the position of the data object. Every data item is replicated at N hosts. So every time a data item is assigned to a node, it is replicated to N-1 clockwise successor nodes as well. The list of nodes storing a data item is called its preference list. Generally preference list contains more than N nodes to account for system and network failures. An example case is shown with N = 3. Any key between A and B would be mapped to B (by consistent hashing logic) and to C and D (by replication logic).\n\nhttps://cdn-images-1.medium.com/max/800/1*66VMYcQfvG3Z2acQD7aeYQ.png\n\nEach time data is created/updated, a new version of data is created. So for a given key, several versions of data (or value) can exist. For versioning, Dynamo uses vector clocks. A vector clock is a list of (node, counter) pairs. When a put operation reaches node X, the node uses the context from the put request to know which version it is updating. If there is an entry corresponding to X in vector clock, the counter is incremented else a new entry is created for node X with counter = 1. When retrieving value corresponding to a key, the node will resolve conflicts among all versions based on Dynamo\u2019s logic or client\u2019s logic. A likely issue with this approach is that the vector clock list may grow very large. To mitigate this, Amazon keeps evicting pairs from the list in ascending order of the time when the entry was created till the size reaches below a threshold. Amazon has not faced any issues related to loss of accuracy with this approach. They also observed that the % of data with at least 2 versions is about 0.06%\n\nDynamo uses a quorum system to maintain consistency. For a read (or write) operation to be successful R (or W) number of replicas out of N replicas must participate in the operation successfully with the condition that R+W > N. If some of the first N replicas are not available, say due to network failure, the read and write operations are performed on the first N healthy nodes. eg if node A is down then node B can be included in its place for the quorum. In this case, B would keep track of data it received on behalf of A and when A comes online, B would hand over this data to A. This way a sloppy quorum is achieved.\n\nIt is possible that B itself becomes unavailable before it can return the data to A. In this case, anti-entropy protocols are used to keep replicas synchronized. In Dynamo, each node maintains a Merkle tree for each key range it hosts. Nodes A and B exchange the roots of Merkle trees corresponding to set of keys they both host. Merkle tree is a hash tree whose leaves are hash values of individual keys and parents are hash values of children. This allows branches to be checked for replication without having to traverse the entire tree. A branch is traversed only when the hash values at the top of the branch differ. This way the amount of data to be transferred for synchronization is minimized.\n\nThe nodes in a cluster communicate as per a gossip-based protocol in which each node contacts a random peer and then the two nodes reconcile their persisted membership history. This ensures an eventually consistent membership view. Apart from this, some nodes are marked as seed nodes which are known to all nodes including the ones that join later. Seed nodes ensure that logical partitions are not created within the network even when new nodes are added. Since consistent hashing is used, the overhead of key reallocation when adding a new node is quite low.\n\n#### Routing\nThere are 2 modes of routing requests in Dynamo. In the first mode, servers route the request. The node fulfilling the request is called coordinator. If it is a read request, any node can act as the coordinator. For a write request, the coordinator is one of the nodes from the key\u2019s current preference list. So if the write request reaches a node which is not in the preference list, it routes the request to one of the nodes in the preference list.\n\nAn alternate approach would be where the client downloads the current membership state from any Dynamo node and determine which node to send the write request to. This approach saves an extra hop within the server cluster but it assumes the membership state to be fresh.\n\n#### Optimizations\nApart from the architecture described above, Dynamo uses optimizations like read-repair where, during quorum, if a node returns a stale response for a read query, it is updated with the latest version of data. Similarly, since writes follow reads, the coordinator for read operation is the node that replies fastest to the previous read operation. This increases the chances of having read you write consistency.\nTo further reduce the latency, each node maintains an object buffer in its main memory where write operations are stored and written to disk by a separate thread. The read operations also first refer the in-memory buffer before checking the disks. There is an added risk of the node crashing before writing the objects from the buffer to the disk. To mitigate this, one of the N replicas performs a durable write\u200a\u2014\u200athat is, the data is written to the disk. Since the quorum requires only W responses, latency due to one node does not affect the performance.\n\nAmazon also experimented with different partitioning schemes to ensure uniform load distribution and adopted the scheme where hash space is divided into Q equally sized partitions and placement of partition is decoupled from the partitioning scheme.\n\n#### Lessons Learnt\nAlthough Dynamo is primarily designed as a write intensive data store, N, R and W provides ample control to modify its behavior for other scenarios as well. For example, setting R = 1 and W = N makes it a high performance read engine. Services maintaining product catalog and promotional items can use this mode. Similarly setting W = 1 means a write request is never rejected as long as at least one server is up though this increases the risk of inconsistency. Given that Dynamo allows the clients to override the conflict resolution methods, it becomes a general solution for many more scenarios than it was originally intended for.\n\nOne limitation is the small size of data for which it is designed. The choice makes sense in the context of Amazon but it would be interesting to see how storing larger values affects its performance. The response time would obviously increase as more data needs to be transferred and in-memory buffers would be able to store lesser data. But using caching and larger in memory buffers, the response time may be brought down to the limit that Dynamo can be used with somewhat larger data objects as well.\nDynamo scales well for a few hundred of nodes but it will not scale equally well for tens of thousands of nodes because of the large overhead of maintaining and distributing the routing table whose size increases with the number of nodes. Another problem that Amazon did not have to face was a high conflict resolution rate. They observed that around 99.94% requests saw exactly one version. Had this number been higher, the latency would have been more.\n\nAll in all, Dynamo is not a universal solution for a distributed key-value store. But it solves one problem and it solves it very well.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/1294261.1294281"
    },
    "421": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/osdi/SubramanianLRHLLPSVTK14",
        "transcript": "#### What are BLOBs\n\nBinary Large OBjects (BLOBs) refer to immutable binary data. A BLOB is created once, read many times, never modified, and sometimes deleted. In Facebook\u2019s context, this would include photos, videos, documents, traces, heap dumps, and source code. Facebook was originally using Haystack as its BLOB storage system. But it is designed for IO bound workloads and not storage efficiency.\n\nTo take a more specific example, when we post a new picture, a BLOB is created. Its data is written to the BLOB storage system and a handle corresponding to this data is stored in a Graph Store for quick retrieval later on. Now when someone clicks on that picture, a read request is fired and the handle is fetched from the graph store. Using this handle, the data (in this case the picture) can be retrieved. A CDN is in place to cache frequently accessed BLOBs.\n\n#### BLOB Storage Design\nBLOBs are aggregated into logical volumes. These volumes are initially unlocked and support reads, writes, and deletes. Once a volume is full, it gets locked and no more creates are allowed. Each volume consists of three files: a data file to hold BLOBs and their metadata, an index file which is a snapshot of the in-memory index of the storage machine, and a journal file to track which BLOBs have been deleted. For fault tolerance and performance, each logical volume maps into multiple physical volumes with each volume living entirely on one Haystack host. Haystack has fault tolerance to disk, host, rack, and data center failure, at an effective replication-factor of 3.6.\n\nAll seems good so far but the problem is with the large and ever increasing storage footprint of the BLOBs. Facebook made an interesting observation in this regard\u200a\u2014\u200athere is a large and increasing number of BLOBs with low request rates. So for these BLOBs, triple replication is an overkill. If only there was a separate BLOB storage system for these BLOBs. This is where f4 comes into the picture.\n\n#### Warm vs Hot\n\nFacebook benchmarked their existing implementation with a 2 week trace and observed that a kind of temperature zone exists where some BLOBs have a very high request rate (these are called the hot BLOBs) and some have a low request rate (these are called the warm BLOBs). f4 is proposed as an implementation for warm BLOB storage while Haystack would be used for hot BLOBs. Another observation was that age and temperature of BLOB are correlated. New BLOBs are queried and deleted at a much higher rate. Lastly, they observed that warm content is large and growing which furthers the need for f4.\n\n#### Design goals\nf4 was designed to reduce the effective-replication-factor without compromising on reliability and performance.\n\n#### Overview\nf4 is comprised of a number of cells, each cell residing completely in one data center. A cell is responsible for reliably storing a set of locked volumes and uses Reed-Solomon coding to store these volumes with a lower storage overhead. The downside is an increased rebuild and recovery time under failure and lower maximum read throughput.\n\nSince f4 cells support only read and delete operations, only data and index files are needed and both are read-only. For tracking deletes, all BLOBs are encrypted with keys stored in an external database. Deleting the key for a BLOB in f4 logically deletes it. The index files use triple replication within a cell and the data file is encoded and stored via a Reed-Solomon (n,k) code. The file is logically partitioned into contiguous sequences of n blocks, each of size b. For each such sequence of n blocks, k parity blocks are generated, making the size of the stripe n + k blocks. For a given block in a stripe, the other blocks in the stripe are called its companion blocks. BLOBs can be read directly from their data block. If a block is unavailable it can be recovered by decoding any n of its companion and parity blocks.\nThe block-size for encoding is kept quite large (around 1 Gb) as it decreases the number of BLOBs that span multiple blocks, thereby reducing I/O operations to read and it reduces the amount of per-block metadata that f4 needs to maintain. But a very large size would mean a larger overhead for rebuilding the blocks.\n\n#### Components\n1. Name Node: This node maintains the mapping between data blocks and parity blocks and the storage nodes that hold the actual blocks.\n2. Storage Nodes: These nodes expose two APIs - an Index API that provides existence and location information for volumes, and a File API that provides access to data.\n3. Backoff Nodes: These nodes are storage-less, CPU-heavy nodes that handle the online reconstruction of request BLOBs (not the entire block).\n4. Rebuilder Nodes: They are similar to Backoff nodes, but they handle the offline reconstruction of entire blocks.\n5. Coordinator Nodes: These nodes are storage-less, CPU-heavy nodes that handle maintenance task, such as scheduling block rebuilding and ensuring an optimal data layout.\n\nA separate geo-replicated XOR coding scheme is also used to tolerate data center or geographic region failure. In this, each volume/stripe/block is paired with a buddy volume/stripe/block in a different geographic region. Then an XOR of the buddy blocks (called as XOR Block) is stored in a third region. The effective replication factor turns out to be 2.1\n\n#### Are the results good?\nResults are amazing! With a corpus of 65PB, f4 will save Facebook over 39 PB and 87 PB of storage at effective-replication-factor of 2.8 and 2.1 respectively. All this comes with low latency and fault tolerance.\n\n#### Lessons learnt\nThe existence of temperature zones is not unique to Facebook. Such zones would be present in all kind of data at a large-enough scale with the line seperating these zones depending on the request and delete rates. Since older data is likely to have a different query rate than newer data, efficient migration from hot to warm storage before putting to cold storage needs to be explored more. This also suggests that one single data management system can not handle data of all ages as the constraints on the data start to change. In Facebook\u2019s context, any data written to Haystack was constrained by at-most-one-writer requirement while data on f4 was free of this constraint. So 2 different data models, each optimized for its own use case could be used. Till now we have seen data management systems based on nature of data (relational or NoSQL), or based on nature of queries (read vs write-oriented). But this case study opens the door for a new kind of system which migrates data from one data model to another based on temperature zones. This is what Facebook ended up creating for this particular scenario.\n\nThis case study also reinforces the superiority of modular architecture. Facebook has a clear need of separate data storage mechanism but what made it possible was its modular architecture which allowed for easy migration of data from Haystack to f4. Apparently Facebook\u2019s overall architecture is designed to enable warm storage. For example, the caching stack would cache the results related to the most popular content which is expected to be newer content. Haystack can handle most of the reads and deletes thereby reducing the load on f4 and so on.\n\nI would be looking out for similar case studies from other data giants as well. Probably they are tackling this problem from an entirely different perspective. Whatever their approaches may be, it would be interesting to compare them with f4.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.usenix.org/conference/osdi14/technical-sessions/presentation/muralidhar"
    },
    "422": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/dimacs/Johnson99",
        "transcript": "It presents an informal discussion about do\u2019s and dont\u2019s of research\u200a\u2014\u200amore specifically experimental study of algorithms. The author presents 10 important principles backed by examples from his long experience. I am listing points from the reading.\n\n#### 1. Solve a problem worth solving.\nTake some time to find a good problem. Decide what questions you want to answer before you actually start finding an answer. Finding a new solution would take up time and resources. So think before you compute. Do exploratory research and experimentation. But do not get into the endless loop of exploration.\n\n#### 2. Tie your paper to the literature.\nKnow the prior art for your problem. Find what are the existing solutions. Think if another solution is actually needed and if yes, what needs to be improved. Tell how your work relates to existing literature. This will help you and your readers. It will make your experiments newsworthy.\n\n#### 3. Use instance testbeds that can support general conclusions.\nWhen doing research, especially experimental research, you are highly likely to use data\u200a\u2014\u200afor making some hypothesis, for validating results and so on. More general your data set is, more general or applicable your results would be. Applying conclusions drawn from a specific data set on a general problem would be disastrous.\n\n#### 4. Document everything.\nDocument your code. Document your data sets. Document your experimental setup, your results, your mistakes, your conclusions. Document everything that some other person would need to replicate your experiments. This will help your future-self save some time and he would be grateful to you. Read #6. When using randomly generated data sets, use the same data set for benchmarking all approaches. It would reduce time and variance. Use sampling and bootstrapping.\n\n#### 5. Use reasonably efficient implementations.\nSeems obvious. But efficiency requires effort and we may be tempted especially when execution time in not a metric of interest. More efficient code means you can perform more experiments and on larger data sets. But do not over-optimise. Remember\u200a\u2014\u200aPremature optimization is the root of all evil.\n\n#### 6. Ensure Reproducibility.\nProvide information on how to reproduce your experiments. Read #4. See gitxiv. Share your code and data set with others. If the data set is generated using some algorithm, share its details. Use reproducible standards of comparison. Do not say \u201cresults after running the algorithm for 1 hour\u201d. Machines are evolving every day and hence doing more operations than before for the same amount of time.\n\n#### 7. Ensure Compatibility.\nComparability goes beyond reproducibility and documentation. Benchmark the machine speed and the other experimental setup that you use so that other researchers can accurately compare their result with yours. Take a full backup of your code and data.\n\n#### 8. Report the full story.\nReport the complete story, including the parts you do not like. Do not hide anomalous results. Instead, try to explain them. It could lead to more insights to the problem and to the solution. Report the full running time, even if time is not a metric of interest from the problem\u2019s point of view. Many times readers want to see how your solution performs on the scale of time.\n\n#### 9. Draw well-justified conclusions and look for explanations.\nSummarize trends, infer implications and provide conclusions. If there are no conclusions and inferences, then you fail the newsworthiness test. Probably you picked the wrong question or your solution is incomplete. Justify your conclusions with data. If needed, use techniques like profiling.\n\n#### 10. Present Your Data in Informative Ways.\nYour data representation should highlight what you want the reader to infer from the data. This can include things like appropriate ordering of columns in a table or choice of variables to be shown along the axis. If you want the reader to compare percentage rise in time, provide a column for that. Do not make the reader do the arithmetic. Take care of not overwhelming your readers with data alone. Put additional data in appendix.",
        "sourceType": "blog",
        "linkToPaper": "http://dimacs.rutgers.edu/Volumes/Vol59.html"
    },
    "423": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=reference/algo/Pagh16",
        "transcript": "The structure of the dictionary is quite simple. Let us say that we have to maintain a set $S$, of size $n$, and keys are taken from a universe $U$. The dictionary uses two hash tables, $T$ and $T\u2019$, each consisting of $r$ slots (where $r$ is slightly bigger than $n$). We have two hash functions $h$, $h\u2019$: $U \u2192 [r]$. Every key $x \\in S$ will either be stored in cell $h(x)$ of T or in cell $h\u2019(x)$ of $T\u2019$. This means lookup takes $O(1)$ time as only 2 slots needs to be checked. Deletion and updation also take $O(1)$ time as they first lookup the key and then delete or update the corresponding value.\n\nInsertion is more interesting and involved than other operations because it uses, what the paper calls, \u201cthe cuckoo approach\u201d where a key kicks other keys to it finds a nest for itself. To insert $x$, we check cell $h(x)$ of $T$. If it is free then we are done else we update the value of this cell which would make the previous key \u201cnestless\u201d. Then we insert this newly displaced key in $T\u2019$ in the same manner iteratively till all keys find a nest. The number of iterations is bound by a constant, MaxLoop, to avoid closed loops. After MaxLoop number of iterations, a rehash is performed. A rehash will also be performed once $r^2$ insertions have been performed since the last rehash. This is all one needs to know to implement their own cuckoo-hash. Simple and straightforward.\n\n### Digging Deeper\nThe approach may, at first, sound like shifting the overhead of lookup to insertion, but there is more to it. We first need to understand what is a universal family.\n\nA family of functions, where each function is defined from $U \u2192 R$, is (c, k)-universal if, for any $k$ distinct $x$\u2019s $\\in U$ , and any $k y$\u2019s $\\in R$, probability that any randomly chosen function from this family maps these $x$\u2019s to $y$\u2019s \u2264 c/|R|^k . In effect, it puts a limit on the probability of collisions when randomly picking hashing functions from this family. The paper cites a paper by Alan Siegel to explain how to construct a hash family such that probability of collisions with a set of $r^2$ keys is $O(1/n^2)$.\n\nThe paper also uses a clever trick to determine when a rehash would be needed before reaching MaxLoop iterations. If the insertion operation returns to a previously visited cell, it checks if a close loop is going to be formed. Suppose we are inserting key $x_1$ and let $x_1$, $x_2$, \u2026, $x_p$ be the sequence of nestless keys. One of these keys, say xi, becomes nestless for the second time. So it will be put back to its original position. This will cause all the the previous keys in the sequence to be moved back to their original position and $x_1$ would be nestless again. So it will be put to the other table this time. This would cause some keys to becomes nestless in the second table. If one of those keys say $x_k$ moves to a previously visited position, we are guaranteed to have a closed loop. In that case, we do not wait for MaxLoop iterations to be reached and perform rehash.\n\nNow we can work out the probability of insertion loop running for at least t iterations. We already know that t can not exceed MaxLoop. Adding up the probabilities for all possible values of t and accounting for the cost of rehash and taking n to be very large, we get the amortized insertion time to be $O(1)$. For a detailed understanding of this part, refer the original paper.\n\n### My thoughts\nThe paper has very strong mathematical and experimental backing. It is full of references related to both theoretical work and experimental evaluation. The analysis of insertion is clean and meticulous and all the maths work out beautifully. I particularly liked the elaborate and well-documented experiments. Authors have experimented with a variety of hashing algorithms along with variants of cuckoo hashing. They have highlighted the instances where their implementation differs from the reference and have provided justifications for the same. They have also studied the role of cache in all the hashing techniques in their experiments. They also tested with the dictionary tests of DIMACS implementation challenge. In all experiments, Linear Probing performed better than all other approaches with Cuckoo-Hashing lagging just 20\u201330% behind. Even then paper presents a strong case for Cuckoo-Hashing.\n\nA few things can still be explored more like how to go about choosing a family of good hash functions as per the constraints. Secondly cache miss rate tends to increase the fastest for cuckoo hashing as load factor increases. Also, it is not possible to have a load factor of more than 50% for cuckoo hashing. So in some cases, the dictionary will have to be resized. The experiments have focused on situations where the size does not vary greatly. Also insertion is expected to be $O(1)$ only for very large values of n though there is no estimate of how big n needs to be. This could be persuaded further.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-1-4939-2864-4_97"
    },
    "424": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/aaai/MitchellCHTBCMG15",
        "transcript": "A never-ending learning problem has 2 components\u200a\u2014\u200aa set of learning tasks and a set of coupling constraints. A learning task in this paradigm is same as the learning task in any other paradigm\u200a\u2014\u200aimproving the system\u2019s performance, as measured by some metric P, over a task T given some experience E. Each coupling constraint can be thought of as a function, defined over 2 or more learning tasks, which specifies the degree of satisfaction of the constraint. Given such a learning problem, a never-ending learning agent A produces a sequence of solutions to the individual learning tasks such that, over time, the quality of the individual learning functions as well as the degree to which each coupling constraint is satisfied both increases. To take a simplified example, consider the problem of Google classifying emails in our inbox. Let us say we have 2 learning tasks going on\u200a\u2014\u200aone which learns whether to put a mail in spam or not and another weather to mark a mail important or not. An obvious constraint here would be that any mail which is marked as spam must not be marked important (though not the other way round). Think of them as the set of rules that you do not want to see violated. What makes coupling constraints important and powerful is that the learning agent can improve its learning of one function by successfully learning other functions.\n\nThe paper also presents the case study of a program called the Never-Ending Language Learner (NELL). NELL implements some of the features of this new paradigm and has been in action 24X7 since January 2010. Every day it extracts beliefs from the web and updates its knowledge base by removing incorrect beliefs and adding the ones which, it believes, are correct.\n\nNELL started with an initial ontology of categories and some labelled examples and relations and has got over 80 million interconnected beliefs in its Knowledge Base (KB) so far. It is performing over 2500 learning tasks which include:\n\n- Category classification: eg Apple is a food and a company.\n- Relationship classifications: eg (Apple, Company, \u201cIsA\u201d).\n- Entity Resolution: e.g \u201cNYC\u201d and \u201cBig Apple\u201d can refer to the same entity.\n- Inferring Rules among belief triples.\n\nFor all these tasks, it uses a variety of classification methods. The coupling constraints include:\n- Multi-view co-training coupling: Different classifiers should give same output on the same input.\n- Subset/superset coupling: If a category A is a subset of category B then each phrase belonging to A must belong to B.\n- Multi-label mutual exclusion coupling: When 2 categories are declared to be mutually exclusive, none of the noun phrases should lie in both categories.\n- Coupling relations to their argument types: In constraints like LivesIn(A, B), A should be a person and B should be a place.\n- Horn clause coupling.\n\nEach reading and inference module (based on above classifications and constraints) sends proposed updates in the KB to a Knowledge Integrator (KI) which makes a final decision on all these updates. Once the updates are made, all the modules are re-trained based on the updated KB. Due to sheer size of the KB, it is not possible to consider each and every candidate belief so the KI would consider only high confidence candidate beliefs and would re-assess confidence using a limited subgraph of consistency constraints and beliefs. This means many iterations are required for the effect of constraints to propagate throughout the graph. The paper mentions a more effective algorithm to achieve this propagation.\n\nTo add learning tasks and ontology extension, it extracts sentences mentioning different categories, build a context by context co-occurrence matrix and then cluster the related contexts together. Now each cluster corresponds to a candidate relation. These candidates go through a trained classifier and manual filtering before they are added to the ontology.\n\nAn empirical analysis of the systems performance shows that:\n\n- The system is improving its reading competence over time as was expected and desired.\n\n- Its KB is increasing but at a decreasing rate as it gets difficult to extract new knowledge from less frequently mentioned beliefs.\n\nThe paper mentions some places for improvement:\n\n- Adding a self-reflection capability to NELL so that it can detect where it is doing well and where it is doing poor and allocate its resources more intelligently. This feature is a part of the paradigm itself.\n- Broaden the scope of NELL by using other data sources as well eg Never-Ending Image Learner (NEIL) uses image data.\n- Merge other ontologies like DBPedia to boost to its ontology.\n- Use \u201dmicro-reading\u201d methods to NELL perform deep semantic analysis\n\nThe never-ending learning paradigm raises 2 fundamental questions:\n\n1. Under what conditions is an increasingly consistent learning agent also an increasingly correct agent?\u200a\u2014\u200aThis is important because an autonomous agent can perceive consistency but not correctness.\n\n2. Under what conditions is convergence guaranteed in principle and in practice?\u200a\u2014\u200aThe architecture may not have sufficient self-modification operations to ensure never-ending learning or these operations may not be practical due to limits of computation and/or training experience.\n\n### My thoughts\n\nWhat makes never-ending learning different, and in some cases more powerful, from conventional paradigms are the concepts of coupling constraints and never ending learning. As a learning model, it seems closer to the human learning model. We try to learn and take actions by relating different scenarios. Our actions and decisions are constrained by both variables and our other actions and decisions. Constraint coupling seems to capture this requirement.\n\nThen there are scenarios where conventional machine learning approaches would fail. Learning is not always about throwing in more data or introducing new variables. If the domain is evolving rapidly, there would always be newer data coming in and newer variables that are not accounted for. These are the kind of scenarios where this paradigm can fill the gap.\n\nAnother aspect is that all this work builds on top of existing work. All the algorithms used for the various classifications are existing ones. The paradigm does not suggest a new algorithm for any of the individual learning problems. Instead, it provides a mechanism where success in learning one function helps in learning others.\n\nThe paper has strongly put forward the case for this new paradigm. There is a case study to evaluate the model practically, they start out with a small labeled dataset, the reported metrics are behaving as desired and the web is the best choice for applying a never-ending learning algorithm as the web is the never-ending growing domain. One criticism is that the paper does not mention how resource-intensive NELL is\u200a\u2014\u200aother than mentioning about the dataset. Even time-based metrics are missing. Not that I expect such a system to be frugal, but I would none-the-less be interested in knowing about their computing infrastructure and time-based metrics.\n\nThere is still a lot to be explored about the effectiveness of this model. Two prime questions are already listed above. Other than that, the model needs firm mathematical footing. It also needs to be put to test in other domains as well. NEIL is one of the extensions of this. I would be interested to see how does this approach plays out in other domains and what kind of ontologies are obtained especially in case of social networks which are both data-rich and constantly evolving.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10049"
    },
    "425": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1412.6071",
        "transcript": "## Introduction\n\n* [Link to Paper](http://arxiv.org/pdf/1412.6071v4.pdf)\n* Spatial pooling layers are building blocks for Convolutional Neural Networks (CNNs).\n* Input to pooling operation is a $N_{in}$ x $N_{in}$ matrix and output is a smaller matrix $N_{out}$ x $N_{out}$.\n* Pooling operation divides $N_{in}$ x $N_{in}$ square into $N^2_{out}$ pooling regions $P_{i, j}$.\n* $P_{i, j}$ \u2282 $\\{1, 2, . . . , N_{in}\\}$ $\\forall$ $(i, j) \\in \\{1, . . . , N_{out} \\}^2$\n\n## MP2\n\n* Refers to 2x2 max-pooling layer.\n* Popular choice for max-pooling operation.\n\n### Advantages of MP2\n* Fast.\n* Quickly reduces the size of the hidden layer.\n* Encodes a degree of invariance with respect to translations and elastic distortions.\n\n### Issues with MP2\n* Disjoint nature of pooling regions.\n* Since size decreases rapidly, stacks of back-to-back CNNs are needed to build deep networks.\n\n## FMP\n\n* Reduces the spatial size of the image by a factor of *\u03b1*, where *\u03b1 \u2208 (1, 2)*.\n* Introduces randomness in terms of choice of pooling region.\n* Pooling regions can be chosen in a *random* or *pseudorandom* manner.\n* Pooling regions can be *disjoint* or *overlapping*.\n\n## Generating Pooling Regions\n\n* Let $a_i$ and $b_i$ be 2 increasing sequences of integers, starting at 1 and ending at $N_{in}$.\n* Increments are either 1 or 2.\n* For *disjoint regions, $P = [a_{i\u22121}, a_{i \u2212 1}] \u00d7 [b_{j\u22121}, b_{j \u2212 1}]$\n* For *overlapping regions, $P = [a_{i\u22121}, a_i] \u00d7 [b_{j\u22121}, b_j 1]$\n* Pooling regions can be generated *randomly* by choosing the increment randomly at each step.\n* To generate pooling regions in a *peusdorandom* manner, choose $a_i$ = ceil($\\alpha | (i+u))$, where $\\alpha \\in (1, 2)$ with some $u \\in (0, 1)$.\n* Each FMP layer uses a different pair of sequence.\n* An FMP network can be thought of as an ensemble of similar networks, with each different pooling-region configuration defining a different member of the ensemble.\n\n## Observations  \n\n* *Random* FMP is good on its own but may underfit when combined with dropout or training data augmentation.  \n* *Pseudorandom* approach generates more stable pooling regions.   \n* *Overlapping* FMP performs better than *disjoint* FMP.    \n\n## Weakness\n\n* No justification is provided for the observations mentioned above.\n* It needs to be seen how performance is affected if the pooling layer in architectures like GoogLeNet.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1412.6071"
    },
    "426": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icra/MiliotoMS19",
        "transcript": "The paper proposes a method to perform joint instance and semantic segmentation. The method is fast as it is meant to run in an embedded environment (such as a robot). While the semantic map may seem redundant given the instance one, it is not as semantic segmentation is a key part of obtaining the instance map.\n\n# Architecture\n\n![image](https://user-images.githubusercontent.com/8659132/63187959-24cdb380-c02e-11e9-9121-77e0923e91c6.png)\n\nThe image is first put through a typical CNN encoder (specifically a ResNet derivative), followed by 3 separate decoders. The output of the decoder is at a low resolution for faster processing.\n\nDecoders:\n- Semantic segmentation: coupled with the encoder, it's U-Net-like. The output is a segmentation map.\n- Instance center: for each pixel, outputs the confidence that it is the center of an object.\n- Embedding: for each pixel, computes a 32 dimensional embedding. This embedding must have a low distance to embedding of other pixels of the same instance, and high distance to embedding of other pixels.\n\nTo obtain the instance map, the segmentation map is used to mask the other 2 decoder outputs to separate the embeddings and centers of each class. Centers are thresholded at 0.7, and centers with embedding distances lower than a set amount are discarded, as they are considered duplicates.\n\nThen for each class, a similarity matrix is computed between all pixels from that class and centers from that class. Pixels are assigned to their closest centers, which represent different instances of the class.\n\nFinally, the segmentation and instance maps are upsampled using the SLIC algorithm.\n\n# Loss\n\nThere is one loss for each decoder head.\n- Semantic segmentation: weighted cross-entropy\n- Instance center: cross-entropy term modulated by a $\\gamma$ parameter to counter the over-representation of the background over the target classes.\n![image](https://user-images.githubusercontent.com/8659132/63286485-22659680-c286-11e9-9134-f1b823a34217.png)\n\n- Embedding: composed of 3 parts, an attracting force between embeddings of the same instance, a repelling force between embeddings of different instances, and a l2 regularization on the embedding.\n![image](https://user-images.githubusercontent.com/8659132/63286399-f1856180-c285-11e9-9136-feb6c4a555e5.png)\n![image](https://user-images.githubusercontent.com/8659132/63286411-fcd88d00-c285-11e9-939f-0771579d8263.png)\n$\\hat{e}$ are the embeddings, $\\delta_a$ is an hyper-parameter defining \"close enough\", and $\\delta_b$ defines \"far enough\"\n\nThe whole model is trained jointly using a weighted sum of the 3 losses.\n\n# Experiments and results\n\nThe authors test their method on the Cityscape dataset, which is composed of 5000 annotated images and 8 instance classes. They compare their methods both for semantic segmentation and instance segmentation.\n\n![image](https://user-images.githubusercontent.com/8659132/63287573-a882dc80-c288-11e9-83e0-b352e43bdf28.png)\n\nFor semantic segmentation, their method is ok, though ENet for example performs better on average and is much faster.\n\n![image](https://user-images.githubusercontent.com/8659132/63287643-d700b780-c288-11e9-9d40-5bcaf695a744.png)\n\nOn the other hand, for instance segmentation, their method is much faster than the other while still performing well. Not SOTA on performance, but considering the real-time constraint, it's much better.\n\n# Comments\n\n- Most instance segmentation methods tend to be sluggish and overly complicated. This approach is much more elegant in my opinion.\n- If they removed the aggressive down/up sampling, I wonder if they would beat MaskRCNN and PANet.\n- I'm not sure what's the point of upsampling the semantic map given that we already have the instance map.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/ICRA.2019.8793593"
    },
    "427": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/AdebayoGMGHK18",
        "transcript": "The paper designs some basic tests to compare saliency methods. It founds that some of the most popular methods are independent of model parameters and the data, meaning they are effectively useless.\n\n## Methods compared\n\nThe paper compare the following methods: gradient explanation, gradient x input, integrated gradients, guided backprop, guided GradCam and SmoothGrad. They provide a refresher on those methods in the appendix.\n\nAll those methods can be put in the same framework. They require a classification model and an input (typically an image). The output of the method is an *explanation map* of the shape of the input where a higher value for a feature implies greater relevance in the decision of the model. \n\n## Metrics of comparison\n\nThe authors argue that visual inspection of the saliency maps can be misleading. They propose to compute the Spearman rank correlation, the structural similarity index (SSMI) and the Pearson correlation of the histogram of gradients. The authors point out that those metrics capture various notions of similarity, but it is an active area of research and those metrics are imperfect.\n\n## First test: model parameters randomization\n\nA saliency method must be dependent of model parameters, otherwise it cannot help us understand a model. In this test, the authors randomize the model parameters, layer per layer, starting from the top.\n\nSurprisingly, methods such as guided backprop and guided gradcam are completely insensitive to model parameters, as illustrated on this Inception v3 trained on ImageNet:\n\n![image](https://user-images.githubusercontent.com/8659132/61403152-b10b8000-a8a2-11e9-9f6a-cf1ed6a876cc.png)\n\nIntegrated gradients looks also dubious as the bird is still visible with a mostly fully randomized model, but the quantitative metrics reveal the difference is actually big between the two models.\n\n## Second test: data randomization\n\nIt is well-known that randomly shuffling the labels of a dataset does not prevent a neural network from getting a high accuracy on the training set, though it does prevent generalization. The model is able to learn by either memorizing the data or finding spurious patterns. As a result, saliency maps obtained from such a network should have no clearly interpretable signal.\n\nHere is the result for a ConvNet trained on MNIST and a shuffled MNIST:\n\n![image](https://user-images.githubusercontent.com/8659132/61406757-7efe1c00-a8aa-11e9-9826-a859a373cb4f.png)\n\nThe results are very damning for most methods. Only gradients and GradCam are very different between both models, as confirmed by the low correlation.\n\n## Discussion\n\n- Even though some methods do no depend on model parameters and data, they might still depend on the architecture of the models, which could be of some use in some contexts.\n- Methods that multiply the input with the gradient are dominated by the input.\n- Complex saliency methods are just fancy edge detectors.\n- Only gradient, smooth gradient and GradCam survives the sanity checks.\n\n# Comments\n\n- Why is their GradCam maps so ugly? They don't look like usual GradCam maps at all.\n- Their tests are simple enough that it's hard to defend a method that doesn't pass them.\n- The methods that are left are not very good either. They give fuzzy maps that are difficult to interpret.\n- In the case of integrated gradients (IG), I'm not convinced this is sufficient to discard the method. IG requires a \"baseline input\" that represents the absence of features. In the case of images, people usually just set the image to 0, which is not at all the absence of a feature. The authors also use the \"set the image to 0\" strategy, and I'd say their tests are damning for this strategy, not for IG in general. I'd expect an estimation of the baseline such as done in [this paper](https://arxiv.org/abs/1702.04595) would be a fairer evaluation of IG.\n\nCode: [GitHub](https://github.com/adebayoj/sanity_checks_saliency) (not available as of 17/07/19)\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps"
    },
    "428": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/WangDWK19",
        "transcript": "This paper considers \"the problem of learning logical structure [...] as expressed by satisfiability problems\". This is an attempt at incorporating symbolic AI into neural networks. The key contribution of the paper is the introduction of \"a differentiable smoothed MAXSAT solver\", that is able to learn logical relationships from examples. \n\nThe example given in the paper is Sudoku. The proposed model is able to learn jointly the rules of the game and how to solve the puzzles, **without prior on the rules**.\n\nThe core of the system is a new layer that learns satisfiability constraints while being differentiable. This layer can be embedded in a typical ConvNet:\n![satnet](https://user-images.githubusercontent.com/8659132/59875340-85ae7780-936e-11e9-946c-c8e8bcb2f869.png)\n\nPrevious attempts to solve Sudoku with a neural network were unsuccessful. The networks were able to reach a high accuracy on the training set but were completely unable to generalize on new puzzles, showing they were unable to learn the underlying logic. SATNet reaches 99% test accuracy on an encoded representation of Sudoku puzzles and 63% test accuracy on images of Sudoku puzzles. \n\n# Comments\n\nThis layer can probably be used to solve other puzzle games, but I'm not familiar enough with SAT to know what kind of practical problems can be solved with this system. Operational research problems maybe?\n\nCode: https://github.com/locuslab/SATNet\n",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v97/wang19e.html"
    },
    "429": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/NoklandE19",
        "transcript": "This paper was presented at ICML 2019.\n\nDo you remember greedy layer-wise training? Are you curious what a modern take on the idea can achieve? This is the paper for you then. And it has its own very good summary:\n\n> We use standard convolutional and fully connected network architectures, but instead of globally back-propagating errors, each weight layer is trained by a local learning signal,that is not back-propagated down the network. The learning signal is provided by two separate single-layer sub-networks, each with their own distinct loss function. One sub-network is trained with a standard cross-entropy loss, and the other with a similarity matching loss.\n\nIf it's a bit unclear, this figure might help: \n![local_error_signal](https://user-images.githubusercontent.com/8659132/59717441-ff672980-91e5-11e9-90d5-8f81c3468391.png)\n\nThe cross-entropy loss is the standard classification loss. The similarity loss is between the output of the layer and the one-hot encoded labels:\n$$\nL_{\\mathrm{sim}}=\\|\\| S(\\text { NeuralNet }(H))-S(Y) \\|\\|_{F}^{2}\n$$\n\nThe similarity is a cosine similarity matrix $S$ where the elements are:\n$$\ns_{i j}=s_{j i}=\\frac{\\tilde{\\mathbf{x}}_{i}^{T} \\tilde{\\mathbf{x}}_{j}}{\\|\\|\\widetilde{\\mathbf{x}}_{i}\\|\\|_{2}\\|\\|\\widetilde{\\mathbf{x}}_{j}\\|\\|_{2}}\n$$\n\nThe method is used to train VGG-like models on MNIST, Fashion-MNIST, CIFAR-10 and 100, SVHN and STL-10. While it gets near-SOTA up to CIFAR-10, it's not there yet for more complex datasets. It gets 80% accuracy on CIFAR-100 where SOTA is 90% accuracy. Still, this is better than a standard ResNet for example. \n\nWhy would we prefer a local loss to a global loss? A big advantage is that the weights can be updated during the forward pass, thus avoiding storing the activations in memory.\n\nThere was another paper on a similar topic, which I didn't read: [Greedy Layerwise Learning Can Scale to ImageNet](https://arxiv.org/abs/1812.11446).\n\n# Comments\n\n- While this is clearly not ready to just replace standard backprop, I find this line of work very interesting as it casts a doubt on one of the assumption of backprop: that we need a global signal to learn complex functions.\n- Though not mentioned in the paper, wouldn't a local loss naturally avoid vanishing and exploding gradients?\n",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v97/nokland19a.html"
    },
    "430": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1904.05049",
        "transcript": "Natural images can be decomposed in frequencies, higher frequencies contain small changes and details, while lower frequencies contain the global structure. We can see an example in this image:\n![image](https://user-images.githubusercontent.com/8659132/58988729-4e599b80-87b0-11e9-88e2-0ecde2cce369.png)\n\nEach filter of a convolutional layer focuses on different frequencies of the image. This paper proposes a way to group them explicitly into high and low frequency filters. \n\nTo do that, the low frequency group is reduced spatially by 2 in all dimensions (which they define as an octave), before applying the convolution. The spatial reduction, which is a pooling operation, makes sense as it is a low pass filter, small details are discarded but the global structure is kept.\n\nMore concretely, the layer takes as input two groups of feature maps, one with a higher resolution than the other. The output is also two groups of feature maps, separated as high/low frequencies. Information is exchanged between the two groups by pooling or upsampling as needed, and as is shown on this image:\n\n![image](https://user-images.githubusercontent.com/8659132/58990790-c7f38880-87b4-11e9-8bca-6a23c63963ad.png)\n\nThe proportion of high and low frequency feature maps is controlled through a single parameter, and through testing the authors found that having around 25% of low frequency features gives the best performance.\n\nOne important fact about this layer is that it can simply be used as replacement for a standard convolutional layer, and thus does not require other changes to the architecture. They test on various ResNets, DenseNets and MobileNets.\n\nIn terms of tasks, they get performance near state-of-the-art on [ImageNet top-1](https://paperswithcode.com/sota/image-classification-on-imagenet) and top-5. So why use this octave convolution? Because it reduces the amount of memory and computation required by the network. \n\n# Comments\n\n- I would have liked to see more groups of varying frequencies. Since an octave is a spatial reduction of 2^n, the authors could do the same with n > 1. I expect this will be addressed in future work.\n- While the results are not quite SOTA, octave convolutions seem compatible with EfficientNet, and I expect this would improve the performance of both.\n- Since each octave convolution layer outputs a multi-scale representation of the input, doesn't that mean that pooling becomes less necessary in a network? If so, octave convolutions would give better performances on a new architecture optimized for them.\n\nCode: [Official](https://github.com/facebookresearch/OctConv), [all implementations](https://paperswithcode.com/paper/drop-an-octave-reducing-spatial-redundancy-in)  \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1904.05049"
    },
    "431": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1803.08494",
        "transcript": "Batch Normalization doesn't work well when using small batch sizes, which is often required for memory intensive tasks such as detection or segmentation, or memory intensive data such as 3D images, videos or high-res images.\n\nGroup Normalization is a simple alternative that is independent of the batch size:\n![image](https://user-images.githubusercontent.com/8659132/57881829-3e255080-77f0-11e9-8ba0-56089c711e7b.png)\n\nIt works as BN, except with a different set of features for computing the mean and std:\n![image](https://user-images.githubusercontent.com/8659132/57882429-ab85b100-77f1-11e9-86df-2c9865d28e8b.png)\nThe $\\gamma$ and $\\beta$ are learned per group and applied as usual:\n![image](https://user-images.githubusercontent.com/8659132/57882468-c9ebac80-77f1-11e9-9d19-82b83b49ea24.png)\n\nA group is defined as a set of channels, and the mean and std is computed for that set of channels for one sample, as illustrated:\n![image](https://user-images.githubusercontent.com/8659132/57882184-200c2000-77f1-11e9-9d2c-8d3fad6d6827.png)\nBy default, there are 32 groups, but they show GN works well as long as there is more than one group but less than the number of channels.\n\nIn term of experiments, they try on ImageNet classification, detection and segmentation in COCO, and video classification in Kinetics. The conclusion is that **GN results in the same performance no matter the batch size, and that performance is the same as BN in large batches.** The most impressive result is a 10% increase in accuracy on ImageNet with a batch size of 2 over BN.\n\n# Comments\n\n- This paper got an honorable mention at ECCV 2018.\n- I don't understand how it works at the entrance of the network, when there is only 1 or 3 channels. Are we just not supposed to put GN there?\n- Also, the number of channels tends to increase in the network, but the number of groups stays fixed. Should it scale with the number of channels?\n- They tested GN on many tasks, but mostly on ResNet. There was only one experiment on VGG-16, where they found no big difference with BN. For now I'm not convinced GN is useful outside of ResNet.\n\nCode: https://github.com/facebookresearch/Detectron/tree/master/projects/GN\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1803.08494"
    },
    "432": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1803.09797",
        "transcript": "Concern about the issue of fairness (or the lack of it) in machine learning models is gaining widespread visibility among general public, the governments as well as the researchers. This is especially alarming as AI enabled systems are becoming more and more pervasive in our society as decisions are being taken by AI agents in healthcare to autonomous driving to criminal justice and so on. Bias in any dataset is, in some way or other, a reflection of the general attitude of humankind towards different activities which are typified by certain gender, race or ethnicity. As these datasets are the sources of knowledge  for these AI models (especially the multimodal end-to-end models which depend only on the human annotated training datasets for literally everything), their decision making ability also gets shadowed by the bias in the dataset. This paper makes an important observation about the image captioning models that these models not only explore the bias in the dataset but tend to exaggerate them during inference. This is definitely a shortcoming of the current supervised models which are marked by their over-reliance on image context. The related works section of the paper (Section 2 first part: \u201cUnwanted Dataset Bias\u201d) gives an extensive review of the types of bias in the dataset and of the few recent works trying to address them. Gender bias (Presence of woman in kitchen makes most of us to guess a woman in a kitchen scene in case the person is not clearly apprehensible in the scene or a male is supposed to snowboard more often than a woman) and reporting biases (over reporting less common co-occurrences, such as \u201cmale nurse\u201d or \u201cgreen banana\u201d) are two of the many present in machine learning datasets.\n\nThe paper addresses the problem of fair caption generation that would not presume a specific gender without appropriate evidence for that gender. This is done by introducing an \u2018Equalizer Model\u2019. This includes two complementary losses in addition to the normal cross entropy loss for the image captioning systems. The Appearance Confusion Loss (ACL) encourages the model to generate gender neutral words (for example \u2018person\u2019) when an image does not contain enough evidence of gender. During training, images of persons are masked out and the loss term encourages the gender words (\u201cman\u201d and \u201cwoman\u201d) to have equal probability i.e., the model is encouraged to get confused when it should get confused instead of hallucinating from the context. The loss expression is pretty much intuitive (eqn (2) and (3)). However, it is not a good idea to make a model confused only. Thus the other loss (the Confident Loss (Conf)) is introduced. This loss encourages the model to predict gender words and predict them correctly when there is enough evidence of gender in the image. The loss function (eqns. (4) and (5)) has an intelligent use of the quotient between predicted probabilities of male and female gender words. If I have to give a single take away line from the paper then it will be the following which summarizes the working principle behind the two losses very succinctly.\n> > \u201cThese complementary losses allow the Equalizer model to encourage models to be cautious in the absence of gender information and discriminative in its presence.\u201d\n\nThe experiments are also well thought out. For experimentations, 3 different versions of the MSCOCO dataset is created - MSCOCO-Bias, MSCOCO-Confident and MSCOCO-Balanced. The bias in the gender gradually decreases in these 3 datasets. Three different metrics are also used to evaluate the model - Error rate (fraction of man/woman misclassifications), gender ratio (how close the gender ratio in the predicted captions of the test set is to the ground truth gender ratio), right for right reasons (whether the visual evidence used by the model for the prediction of the gender words coincide with the person images). There are a few baseline models and ablation studies. The baselines considered a naive image captioning model (\u2018Show and Tell\u2019 approach), an approach where images corresponding to less common gender are sampled more while training and another baseline where the gender words are given higher weights in the cross-entropy loss. The ablation models considered the two losses (ACL and Conf) separately. For all the datasets, the proposed equalizer model consistently performed well according to all the 3 metrics. The experiments also show that, as the evaluation datasets become more and more balanced (i.e., the gender distribution departs more and more from the biased gender distribution in the training dataset), the performance of all the models falls away. However, the proposed model performs the best with the least inconsistency of performance among the the datasets. The qualitative examples with grad-cam and sliding window saliency maps for the gender words are also a positive point of the paper.\n\nThings I would have liked the paper to contain:\n* There are a few confusions in the expression of the conf loss in eqn. (4). Specifically, I am not sure what is the difference between $w_t$ and $\\tilde{w}_t$. It seems the first one is the ground truth word and the later is the predicted word. It would have been good to have a clarification.\n\nOverall, the paper is very new in defining the problem and in solving it. The solution strategy is very intuitive and easy to grasp. The paper is well written too. We can, sincerely, hope that this type of works addressing problems at the intersection of machine learning and societal issues would come more frequently and the discussed paper is a very significant first step towards it.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1803.09797"
    },
    "433": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eccv/HendricksARDSD16",
        "transcript": "This paper deals with an important problem where a deep classification system is made explainable. After the (continuing) success of Deep Networks, researchers are trying to open the blackbox and this work is one of the foremosts. The authors explored the strength of a deep learning method (vision-language model) to explain the performance of another deep learning model (image classification). The approach jointly predicts a class label and explains why it predicted so in natural language.\n\nThe paper starts with a very important differentiation between two basic schools of *explnation* systems - the *introspection* explanation system and the *justification* explanation system. The introspection system looks into the model to get an explanation (e.g., \"This is a Western Grebe because filter 2 has a high activation...\"). On the other hand, a justification system justifies the decision by producing sentence details on how visual evidence is compatible with the system output (e.g., \"This is a Western Grebe because it has red eyes...\"). The paper focuses on *justification* explanation system and proposes a novel one.\n\nThe authors argue that unlike a description of an image or a sentence defining a class (not necessarily in presence of an image), visual explanation, conditioned on an input image, provides much more of an explanatory text on why the image is classified as a certain category mentioning only image relevant features. The broad outline of the approach is given in Fig (2) of the paper.\nhttps://i.imgur.com/tta2qDp.png\nThe first stage consists of a deep convolutional network for classification which generates a softmax distribution over the classes. As the task handles fine-grained bird species classification, it uses a compact bilinear feature representation known to work well for the fine-grained classification tasks. The second stage is a stacked LSTM which generates natural language sentences or explanations justifying the decision of the first stage. The first LSTM of the stack receives the previously generated word. The second LSTM receives the output of the first LSTM along with image features and predicted label distribution from the classification network. This LSTM produces the sequence of output words until an \"end-of-sentence\" token is generated. The intuition behind using predicted label distribution for explanation is that it would inform the explanation generation model which words and attributes are more likely to occur in the description.\n\nTwo kinds of losses are used for the second stage *i.e.*, the language model. The first one is termed as the *Relevance Loss* which is the typical sentence generation loss that is seen in literature. This is the sum of cross-entropy losses of the generated words with respect to the ground truth words. Its role is to optimize the alignment between generated and ground truth sentences. However, this loss is not very effective in producing sentences which include class discriminative information. class specificity is a global sentence property. This is illustrated with the following example - *whereas a sentence \"This is an all black bird with a bright red eye\" is class specific to a \"Bronzed Cowbird\", words and phrases in the sentence, such as \"black\" or \"red eye\" are less class discriminative on their own.* As a result, cross entropy loss on individual words turns out to be less effective in capturing the global sentence property of which class specifity is an example. The authors address this issue by proposing an addiitonal loss, termed as the *Discriminative Loss* which is based on a reinforcement learning paradigm. Before computing the loss, a sentence is sampled. The sentence is passed through a LSTM-based classification network whose task is to produce the ground truth category $C$ given only the sampled sentence. The reward for this operation is simply the probability of the ground truth category $C$ given only the sentence. The intuition is - for the model to produce an output with a large reward, the generated sentence must include enough information to classify the original image properly. The *Discriminative Loss* is the expectation of the negative of this reward and a wieghted linear combination of the two losses is optimized during training.\n\nMy experience in reinforcement learning is limited. However, I must say I did not quite get why is sampling of the sentences required (which called for the special algorithm for backpropagation). If the idea is to see whether a generated sentence can be used to get at the ground truth category, could the last internal state of one of the stacked LSTM not be used? It would have been better to get some more intution behind the sampling operation. Another thing which (is fairly obvious but still I felt) is missing is not mentioning the loss used in the fine grained classification network.\n\nThe experimentation is rigorous. The proposed method is compared with four different baseline and ablation models - description, definition, explanation-label, explanation-discriminative with different permutation and combinations of the presence of two types losses, class precition informations etc. Also the evaluation metrics measure different qualities of the generated exlanations, specifically image and class relevances. To measure image relevance METEOR/CIDEr scores of the generated sentences with the ground truth (image based) explanations are computed. On the other hand, to measure the class relevance, CIDEr scores with class definition (not necessarily based on the images from the dataset) sentences are computed. The proposed approach has continuously shown better performance than any of the baseline or ablation methods. I'd specifically mention about one experiment where the effect of class conditioning is studies (end of Sec 5.2). The finding is quite interesting as it shows that providing or not providing correct class information has drastic effect at the generated explanations. It is seen that giving incorrect class information makes the explanation model hallucinate colors or attributes which are not present in the image but are specific to the class. This raises the question whether it is worth giving the class information when the classifier is poor on the first hand? But, I think the answer lies in the observation that row 5 (with class prediction information) in table 1 is always better than row 4 (no class prediction information). Since, row 5 is better than row 4, this means the classifier is also reasonable and this in turn implies that end-to-end training can improve all the stages of a pipeline which ultimately improves the overall performance of the system too!\n\nIn summary, the paper is a very good first step to explain intelligent systems and should encourage a lot more effort in this direction.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-46493-0_1"
    },
    "434": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1708.02696",
        "transcript": "In this race for getting that extra few % improvement for a '*brand-new'* paper, this paper brings a fresh air by posing some very pertinent questions supported by rigorous experimental analysis. Its an ICCV 2017 paper. The paper talks about understanding activities in videos \u2013 both from activity classification and detection perspective. In doing so, the authors examined several datasets, evaluation metrics, algorithms, and pointed to possible future directions worthy of exploring. The default choice in terms of the dataset is Charades. Other than this, multiTHUMOS, THUMOS and ActivityNet are used as and when required. The activity classification/detection algorithms analyzed, are two-stream, improved dense trajectories (IDT), LSTM on VGG, actionVLAD and temporalfields.\n\nThe paper starts with the very definition of action. To quote *\"When we talk about activities, we are referring to anything a person is doing, regardless of whether the person is intentionally and actively altering the environment, or simply sitting still\".* This is a complementary perspective than what the community has perceived as action so far - *\"Intentional bodily motion of biological agents\"* [1]. The paper generalizes this notion and advocates that bodily motion is not indispensable to define actionness (*e.g.*, 'watching the tv', 'Lying on a couch' hardly consist of a bodily motion). Analysis of motion\u2019s role in understanding activity has played a major role later in the paper. Let\u2019s see some of the major questions that the authors explored in this paper.\n\n1. \"Only verbs\" can make actions ambiguous. To quote, - \"Verbs such as 'drinking' and 'running' are unique on their own, but verbs such as 'take' and 'put' are ambiguous unless nouns and even prepositions are included: 'take medication', 'take shoes', take off shoes'\". The experiments involving both human (sec 3.1) and activity algorithms (sec 4.1) shows that given the verb less confusion arises when the object is mentioned ('holding a cup' vs 'holding a broom'), but given the object, confusion is more among different verbs ('holding a cup' vs 'drinking from a cup'). All the current algorithms are shown to have significant confusion among similar action categories, both in terms of verbs and objects. In fact, for a given category, the more categories share the object or verb, the worse is the accuracy.\n2. The next study, to me, is the most important one. It\u2019s about the long-standing concern of whether activities have clear and universal boundaries. The human study shows that, in fact, it is ambiguous. Average human agreement with ground truth is only 72.5% IOU for Charades and 58.7% IOU for MultiTHUMOS. In a natural course of action, the authors wanted to see if this ambiguity is affecting the evaluation performance of the algorithms. For this purpose, they relaxed the ground truth boundary to be more flexible (sec 3.2) and then evaluated the performance of the algorithms. The surprising fact is that this relaxation did not improve the performance much. The authors opined that despite boundary ambiguity current datasets allow current algorithms to understand and learn from the temporal extent of activities. I must say, I did not expect that ambiguity in temporal boundary will have this insignificant effect on the localization performances. In addition to the conclusion as drawn by the authors, this can be caused by another issue. The (bad) effect of other things are so large that the correction due to boundary ambiguity can't change the performance much. What I mean is - it may not be that the datasets are sufficient but the algorithms are suffering from other flaws much more than they are suffering from the boundary ambiguity.\n3. Another important question that the authors dealt with is \u2013 how does the amount of labeled training data affect the performance. The broad finding goes with the common knowledge of - \"more data means better performance\". However, there are a plethora of finer equally important insights that the authors pointed out. The amount of data does not affect all categories equally, especially for a dataset with long-tailed distribution of classes. Smaller categories are more affected. In addition, activities with more similar categories (that share the same object/verb) also get affected much more than their counter parts. The authors end the subsection (sec 4.2) with an observation that improvement can be made by designing algorithms that are better able to make use of the wealth of data in small categories than in large ones.\n4. The authors did a thorough analysis of the role of temporal reasoning (motion, continuity, and temporal context) for activity understanding. The very first finding is that current methods are doing better for longer activities than shorter ones. Another common notion (naive temporal smoothing of the predictions helps improve localization and classification) is also verified.\n5. An action is almost invariably related to persons. So, the authors tried to see if person based reasoning helps. For that, they experimented with removing the person from the scene, keeping nothing but the person etc. They also examined how diverse are the datasets in terms of human pose and if injecting human pose information helps the current approaches. The conclusion was that person based reasoning helps and the nature of the videos require the activity understanding approaches to harness pose information for improved performance.\n6. Finally, the authors try to see what aspects help most if that aspect is solved perfectly with an oracle. The oracles include perfect object detection, perfect verb identification and so on.  It varies for datasets to some extent but, in general, the finding was that all the oracles help, some more some less.\n\nI think this is a much-needed work that would help the community to ponder over different avenues of activity understanding in videos to design better systems.\n\n[1]. Wei Chen, Caiming Xiong, Ran Xu, Jason J. Corso, Actionness Ranking with Lattice Conditional Ordinal Random Fields, CVPR 2014.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1708.02696"
    },
    "435": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/BafnaMV18",
        "transcript": "Bafna et al. show that iterative hard thresholding results in $L_0$ robust Fourier transforms. In particular, as shown in Algorithm 1, iterative hard thresholding assumes a signal $y = x + e$ where $x$ is assumed to be sparse, and $e$ is assumed to be sparse. This translates to noise $e$ that is bounded in its $L_0$ norm, corresponding to common adversarial attacks such as adversarial patches in computer vision. Using their algorithm, the authors can provably reconstruct the signal, specifically the top-$k$ coordinates for a $k$-sparse signal, which can subsequently be fed to a neural network classifier. In experiments, the classifier is always trained on sparse signals, and at test time, the sparse signal is reconstructed prior to the forward pass. This way, on MNIST and Fashion-MNIST, the algorithm is able to recover large parts of the original accuracy.\n\nhttps://i.imgur.com/yClXLoo.jpg\nAlgorithm 1 (see paper for details): The iterative hard thresholding algorithm resulting in provable robustness against $L_0$ attack on images and other signals.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/8211-thwarting-adversarial-examples-an-l_0-robust-sparse-fourier-transform"
    },
    "436": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1809.08758",
        "transcript": "Guo et al. propose to augment black-box adversarial attacks with low-frequency noise to obtain low-frequency adversarial examples as shown in Figure 1. To this end, the boundary attack as well as the NES attack are modified to sample from a low-frequency Gaussian distribution instead from Gaussian noise directly. This is achieved through an inverse discrete cosine transform as detailed in the paper.\n\nhttps://i.imgur.com/fejvuw7.jpg\nFigure 1: Example of a low-frequency adversarial example.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1809.08758"
    },
    "437": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/cvprw.2018.00212",
        "transcript": "Hosseini and Poovendran propose semantic adversarial examples by randomly manipulating hue and saturation of images. In particular, in an iterative algorithm, hue and saturation are randomly perturbed and projected back to their valid range. If this results in mis-classification the perturbed image is returned as the adversarial example and the algorithm is finished; if not, another iteration is run. The result is shown in Figure 1. As can be seen, the structure of the images is retained while hue and saturation changes, resulting in mis-classified images.\n\nhttps://i.imgur.com/kFcmlE3.jpg\nFigure 1: Examples of the computed semantic adversarial examples.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/cvprw.2018.00212"
    },
    "438": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/KarmonZG18",
        "transcript": "Karmon et al. propose a gradient-descent based method for obtaining adversarial patch like localized adversarial examples. In particular, after selecting a region of the image to be modified, several iterations of gradient descent are run in order to maximize the probability of the target class and simultaneously minimize the probability in the true class. After each iteration, the perturbation is masked to the patch and projected onto the valid range of [0,1] for images. On ImageNet, the authors show that these adversarial examples are effective against a normal, undefended network.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v80/karmon18a.html"
    },
    "439": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1904-00759",
        "transcript": "Li et al. propose camera stickers that when computed adversarially and physically attached to the camera leads to mis-classification. As illustrated in Figure 1, these stickers are realized using circular patches of uniform color. These individual circular stickers are computed in a gradient-descent fashion by optimizing their location, color and radius. The influence of the camera on these stickers is modeled realistically in order to guarantee success.\n\nhttps://i.imgur.com/xHrqCNy.jpg\nFigure 1: Illustration of adversarial stickers on the camera (left) and the effect on the taken photo (right).\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1904.00759"
    },
    "440": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/wacv.2019.00143",
        "transcript": "Naseer et al. propose to smooth local gradients as defense against adversarial patches. In particular, as illustrated in Figure 1, the local image gradient is computed through convolution. Then, in local, overlapping windows, the gradients are set to zero if the total sum of absolute gradient values exceeds a specific threshold. The remaining gradient map is supposed to indicate regions where it is likely that adversarial patches can be found. Using this gradient map, the image is smoothed, i.e., blurred, afterwards. In experiments, the authors show that this reduces the impact of adversarial patches.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/wacv.2019.00143"
    },
    "441": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/raid/ZuoYL019",
        "transcript": "Zuo et al. propose a two-stage system for detecting $L_0$ adversarial examples. Their system is based on the following two observations: (a) $L_0$ adversarial examples often result in very drastic changes of individual pixels and (b) these pixels are usually isolated and scattered over the image. Thus, they propose to train a siamese network to detect adversarial examples. To this end, they use a pre-processor and train the network to detect adversarial examples by taking the input and the pre-processed input. The pre-processing is assumed to influence benign images only slightly. In their case, an inpainting mechanism is used. Specifically, pixels where one color channel exhibits extremely small or large values are inpainted using any state-of-the-art approach, as shown in Figure 1. The siamese network learns to detect adversarial examples based on the differences in input images and inpainted images.\n\nhttps://i.imgur.com/gsgWuin.jpg\nFigure 1: Examples of inpainted $L_0$ adversarial examples.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.usenix.org/conference/raid2019/presentation/zuo"
    },
    "442": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iclr/LeeAJ19",
        "transcript": "Lee et al. propose a regularizer to increase the size of linear regions of rectified deep networks around training and test points. Specifically, they assume piece-wise linear networks, in its most simplistic form consisting of linear layers (fully connected layers, convolutional layers) and ReLU activation functions. In these networks, linear regions are determined by activation patterns, i.e., a pattern indicating which neurons have value greater than zero. Then, the goal is to compute, and later to increase, the size $\\epsilon$ such that the $L_p$-ball of radius $\\epsilon$ around a sample $x$, denoted $B_{\\epsilon,p}(x)$ is contained within one linear region (corresponding to one activation pattern). Formally, letting $S(x)$ denote the set of feasible inputs $x$ for a given activation pattern, the task is to determine\n\n$\\hat{\\epsilon}_{x,p} = \\max_{\\epsilon \\geq 0, B_{\\epsilon,p}(x) \\subset S(x)} \\epsilon$.\n\nFor $p = 1, 2, \\infty$, the authors show how $\\hat{\\epsilon}_{x,p}$ can be computed efficiently. For $p = 2$, for example, it results in\n\n$\\hat{\\epsilon}_{x,p} = \\min_{(i,j) \\in I} \\frac{|z_j^i|}{\\|\\nabla_x z_j^i\\|_2}$.\n\nHere, $z_j^i$ corresponds to the $j$th neuron in the $i$th layer of a multi-layer perceptron with ReLU activations; and $I$ contains all the indices of hidden neurons. This analytical form can then used to add a regularizer to encourage the network to learn larger linear regions:\n\n$\\min_\\theta \\sum_{(x,y) \\in D} \\left[\\mathcal{L}(f_\\theta(x), y) - \\lambda \\min_{(i,j) \\in I} \\frac{|z_j^i|}{\\|\\nabla_x z_j^i\\|_2}\\right]$\n\nwhere $f_\\theta$ is the neural network with paramters $\\theta$. In the remainder of the paper, the authors propose a relaxed version of this training procedure that resembles a max-margin formulation and discuss efficient computation of the involved derivatives $\\nabla_x z_j^i$ without too many additional forward/backward passes.\n\nhttps://i.imgur.com/jSc9zbw.jpg\nFigure 1: Visualization of locally linear regions for three different models on toy 2D data.\n\nOn toy data and datasets such as MNIST and CalTech-256, it is shown that the training procedure is effective in the sense that larger linear regions around training and test points are learned. For example, on a 2D toy dataset, Figure 1 visualizes the linear regions for the optimal regularizer as well as the proposed relaxed version.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=SylCrnCcFX"
    },
    "443": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/aaai/LiuYLSCL19",
        "transcript": "Liu et al. propose DPatch, adversarial patches against state-of-the-art object detectors. Similar to existing adversarial patches, where a patch with fixed pixels is placed in an image in order to evade (or change) classification, the authors compute their DPatch using an optimization procedure. During optimization, the patch to be optimized is placed in random locations on all images of, e.g. on PASCAL VOC 2007, and the pixels are updated in order to maximize the loss of the classifier (either in a targeted setting or in an untargeted setting). In experiments, this approach is able to fool several different detectors. Using small $40\\times40$ pixel patches as illustrated in Figure 1.\n\nhttps://i.imgur.com/ma6hGNO.jpg\nFigure 1: Illustration of the use case of DPatch.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/ws.org/Vol-2301/paper_5.pdf"
    },
    "444": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SalmanLRZZBY19",
        "transcript": "Salman et al. combined randomized smoothing with adversarial training based on an attack specifically designed against smoothed classifiers. Specifically, they consider the formulation of randomized smoothing by Cohen et al. [1]; here, Gaussian noise around the input (adversarial or clean) is sampled and the classifier takes a simple majority vote. In [1], Cohen et al. show that this results in good bounds on robustness. In this paper, Salman et al. propose an adaptive attack against randomized smoothing. Essentially, they use a simple PGD attack to attack a smoothed classifier, i.e., maximize the cross entropy loss of the smoothed classifier. To make the objective tractable, Monte Carlo samples are used in each iteration of the PGD optimization. Based on this attack, they do adversarial training, with adversarial examples computed against the smoothed (and adversarially trained) classifier. In experiments, this approach outperforms the certified robustness by Cohen et al. on several datasets.\n\n[1] Jeremy M. Cohen, Elan Rosenfeld and J. Zico Kolter. Certified Adversarial Robustness via Randomized Smoothing. ArXiv, 1902.02918, 2019.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/9307-provably-robust-deep-learning-via-adversarially-trained-smoothed-classifiers"
    },
    "445": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/ccs/LambVKB19",
        "transcript": "Lamb et al. propose interpolated adversarial training to increase robustness against adversarial examples. Particularly, a $50\\%/50\\%$ variant of adversarial training is used, i.e., in each iteration the batch consists of $50\\%$ clean and $50\\%$ adversarial examples. The loss is then computed on these both parts, encouraging the network to predict the correct labels on the adversarial examples, and averaged afterwards. In interpolated adversarial training, the loss is adapted according to the Mixup strategy. Here, instead of computing the loss on the selected input-output pair, a second input-output pair is selected at random from the dataset. Then, a random linear interpolation between both inputs is considered; this means that the loss is computed as\n\n$\\lambda \\mathcal{L}(f(x\u2019), y_i) + (1 - \\lambda)\\mathcal{L}(f(x\u2019), y_j)$\n\nwhere $f$ is the neural network, $x\u2019$ the interpolated input $x\u2019 = \\lambda x_i + (1 - \\lambda)x_j$ corresponding to the two input-output pairs $(x_i, y_i)$ and $(x_j, y_j)$. In a variant called Manifold Mixup, the interpolation is performed within a hidden layer instead of the input space. This strategy is applied on both the clean and the adversarial examples and leads, accoridng to the experiments, to the same level of robustness while improving the test accuracy.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3338501.3357369"
    },
    "446": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/Bartlett96",
        "transcript": "Barlett shows that lower generalization bounds for multi-layer perceptrons with limited sizes of the weights can be found using the so-called fat-shattering dimension. Similar to the classical VC dimensions, the fat shattering dimensions quantifies the expressiveness of hypothesis classes in machine learning. Specifically, considering a sequence of points $x_1, \\ldots, x_d$, a hypothesis class $H$ is said to shatter this sequence if, for any label assignment $b_1, \\ldots, b_d \\in \\{-1,1\\}$, a function $h \\in H$ exists that correctly classifies the sequence, i.e. $\\text{sign}(h(x_i)) = b_i$. The VC dimension is the largest $d$ for which this is possible. The VC dimension has been studied for a wide range of machine learning models (i.e., hypothesis classes). Thus, it is well known that multi-layer perceptrons with at least two layers have infinite VC dimension \u2013 which seems natural as two-layer perceptrons are universal approximators. As a result, most bounds on the generalization performance of multi-layer networks (and, thus, also of more general deep networks) do not apply as the VC dimension is infinite.\n\nThe fat-shattering dimension, in contrast, does not strictly require the sequence $x_1,\\ldots, x_d$ to be correctly classified into the labels $b_1,\\ldots, b_d$. Instead, the sequence is said to be $\\gamma$-shattered if real values $r_1,\\ldots,r_d$ exist such that for every labeling, $b_1,\\ldots,b_d$, some some $h \\in H$ satisfies $(h(x_i) \u2013 r_i)b_i \\geq \\gamma$. Note that the values $r_i$ are fixed across labelings, i.e., are chosen \u201cbefore\u201d knowing the labels. The fat-shattering dimension is the largest $d$ for which this is possible. As a result, the fat-shattering dimension relaxes the VC dimension in that the models in $H$ are allowed some \u201cslack\u201d (in lack of a better word). Note that $H$ contains real-valued functions.\n\nBased on this definition, Barlett shows that multi-layer perceptrons in which all layers have weights $w$ constrained as $\\|w\\|_1 \\leq A$ scales with $A^{l(l + 1)}$. More importantly, however, the fat-shattering dimension is finite. Thus, generalization bounds based on the fat-shattering dimensions apply and are discussed by Barlett; I refer to the paper for details on the bound.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://nips.djvuzone.org/djvu/nips09/0134.djvu"
    },
    "447": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=duesterwald2019exploring",
        "transcript": "Duesterwald et al. study the influence of hyperparameters on adversarial training and its robustness as well as accuracy. As shown in Figure 1, the chosen parameters, the ratio of adversarial examples per batch and the allowed perturbation $\\epsilon$, allow to control the trade-off between adversarial robustness and accuracy. Even for larger $\\epsilon$, at least on MNIST and SVHN, using only few adversarial examples per batch increases robustness significantly while only incurring a small loss in accuracy.\n\nhttps://i.imgur.com/nMZNpFB.jpg\nFigure 1: Robustness (red) and accuracy (blue) depending on the two hyperparameters $\\epsilon$ and ratio of adversarial examples per batch. Robustness is measured in adversarial accuracy.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.03837"
    },
    "448": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1901-09878",
        "transcript": "Marchisio et al. propose a black-box adversarial attack on Capsule Networks. The main idea of the attack is to select pixels based on their local standard deviation. Given a window of allowed pixels to be manipulated, these are sorted based on standard deviation and possible impact on the predicted probability (i.e., gap between target class probability and maximum other class probability). A subset of these pixels is then manipulated by a fixed noise value $\\delta$. In experiments, the attack is shown to be effective for CapsuleNetworks and other networks.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1901.09878"
    },
    "449": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/TramerPGBM17",
        "transcript": "Tramer et al. study adversarial subspaces, subspaces of the input space that are spanned by multiple, orthogonal adversarial examples. This is achieved by iteratively searching for orthogonal adversarial examples, relative to a specific test example. This can, for example, be done using classical second- or first-order optimization methods for finding adversarial examples with the additional constraint of finding orthogonal adversarial examples. However, the authors also consider different attack strategies that work on discrete input features. In practice, on MNIST, this allows to find, on average, 44 orthogonal directions per test example. This finding indicates that adversarial examples indeed span large adversarial subspaces. Additionally, adversarial examples from the subspaces seem to transfer reasonably well to other models. The remainder of the paper links this ease of transferability to a similarity in decision boundaries learnt by different models from the same hypotheses set.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.03453"
    },
    "450": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-05419",
        "transcript": "Englesson and Azizpour propose an adapted knowledge distillation version to improve confidence calibration on out-of-distribution examples including adversarial examples. In contrast to vanilla distillation, they make the following changes: First, high capacity student networks are used, for example, by increasing depth or with. Then, the target distribution is \u201csharpened\u201d using the true label by reducing the distributions overall entropy. Finally, for wrong predictions of the teacher model, they propose an alternative distribution with maximum mass on the correct class, while not losing the information provided on the incorrect label.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.05419"
    },
    "451": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iclr/HendrycksD19",
        "transcript": "Hendrycks and Dietterich propose ImageNet-C and ImageNet-P benchmarks for corruption and perturbation robustness evaluation. Both datasets come in various sizes, and corruptions always come in different difficulties. The used corruptions include many common, realistic noise types such as various types of blur and random noise, brightness changes and compression artifacts. ImageNet-P differs from ImageNet-C in that sequences of perturbations are generated. This means, for a specific perturbation type, 30 different frames are generated; thus, less corruption types in total are used. The remainder of the paper introduces various evaluation metrics; these are usually based on the fact that the label of the corrupted image did not change. Finally, they also highlight some approaches to obtain more \u201crobust\u201d models against these corruptions. The list includes a variant of histogram equalization that is used to normalize the input images, the use of multi-scale or feature aggregation architectures and, surprisingly, adversarial logit pairing. Examples of ImageNet-C images can be found in Figure 1.\n\nhttps://i.imgur.com/YRBOzrH.jpg\nFigure 1: Examples of images in ImageNet-C.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=HJz6tiCqYm"
    },
    "452": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1905-06455",
        "transcript": "Li et al. evaluate adversarial training using both $L_2$ and $L_\\infty$ attacks and proposes a second-order attack. The main motivation of the paper is to show that adversarial training cannot increase robustness against both $L_2$ and $L_\\infty$ attacks. To this end, they propose a second-order adversarial attack and experimentally show that ensemble adversarial training can partly solve the problem.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.06455"
    },
    "453": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-02611",
        "transcript": "Lopes et al. propose patch-based Gaussian data augmentation to improve accuracy and robustness against common corruptions. Their approach is intended to be an interpolation between Gaussian noise data augmentation and CutOut. During training, random patches on images are selected and random Gaussian noise is added to these patches. With increasing noise level (i.e., its standard deviation) this results in CutOut; with increasing patch size, this results in regular Gaussian noise data augmentation. On ImageNet-C and Cifar-C, the authors show that this approach improves robustness against common corruptions while also improving accuracy slightly.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.02611"
    },
    "454": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-02337",
        "transcript": "Mu and Gilmer introduce MNIST-C, an MNIST-based corruption benchmark for out-of-distribution evaluation. The benchmark includes various corruption types including random noise (shot and impulse noise), blur (glass and motion blur), (affine) transformations, \u201cstriping\u201d or occluding parts of the image, using Canny images or simulating fog. These corruptions are also shown in Figure 1. The transformations have been chosen to be semantically invariant, meaning that the true class of the image does not change. This is important for evaluation as model\u2019s can easily be tested whether they still predict the correct labels on the corrupted images.\n\nhttps://i.imgur.com/Y6LgAM4.jpg\nFigure 1: Examples of the used corruption types included in MNIST-C.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.02337"
    },
    "455": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/TeyeAS18",
        "transcript": "Teye et al. show that neural networks with batch normalization can be used to give uncertainty estimates through Monte Carlo sampling. In particular, instead of using the test mode of batch normalization, where the statistics (mean and variance) of each batch normalization layer are fixed, these statistics are computed per batch, as in training mode. To this end, for a specific query image, random batches from the training set are sampled, and prediction uncertainty is estimated using Monte Carlo sampling to compute mean and variance. This is summarized in Algorithm 1, depicting the proposed Monte Carlo Batch Normalization method. In the paper, this approach is further interpreted as approximate inference in Bayesian models.\n\nhttps://i.imgur.com/nRdOvzs.jpg\nAlgorithm 1: Monte Carlo approach for using batch normalization for uncertainty estimation.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v80/teye18a.html"
    },
    "456": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.06450",
        "transcript": "Ba et al. propose layer normalization, normalizing the activations of a layer by its mean and standard deviation. In contrast to batch normalization, this scheme does not depend on the current batch; thus, it performs the same computation at training and test time. The general scheme, however, is very similar. Given the $l$-th layer of a multi-layer perceptron,\n\n$a_i^l = (w_i^l)^T h^l$ and $h_i^{l + 1} = f(a_i^l + b_i^l)$\n\nwith $W^l$ being the weight matrix, the activations $a_i^l$ are normalized by mean $\\mu_i^l$ and standard deviation $\\sigma_i^l$. For batch normalization these are estimated over the current mini batch:\n\n$\\mu_i^l = \\mathbb{E}_{p(x)} [a_i^l]$ and $\\sigma_i^l = \\sqrt{\\mathbb{E}_{p(x)} [(a_i^l - \\mu_i^l)^2}$.\n\nHowever, this estimation depends heavily on the batch size; additionally, models change during training and test time (at test time, these statistics are estimated over the training set). For layer normalization, instead, these statistics are evaluated over the activations in the same layer:\n\n$\\mu^l = \\frac{1}{H}\\sum_{i = 1}^H a_i^l$ and $\\sigma^l = \\sqrt{\\frac{1}{H}\\sum_{i = 1}^H (a_i^l - \\mu^l)^2}$.\n\nThus, the normalization is not depending on the batch size anymore. Additionally, layer normalization is invariant to scaling and shifts of the weight matrix (for batch normalization, this only holds for the columns of the matrix). In experiments, this approach is shown to work well for a variety of tasks including models with attention mechanisms and recurrent neural networks. For convolutional neural networks, the authors state that layer normalization does not outperform batch normalization, but performs better than using no normalization at all.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.06450"
    },
    "457": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.08760",
        "transcript": "Novak et al. study the relationship between neural network sensitivity and generalization. Here, sensitivity is measured in terms of the Frobenius gradient of the network\u2019s probabilities (resulting in a Jacobian matrix, not depending on the true label) or based on a coding scheme of activations. The latter is intended to quantify transitions between linear regions of the piece-wise linear model. To this end, all activations are assigned either $0$ or $1$ depending on their ReLU output. Based on a path between two or more input examples, the difference in this coding scheme is an estimator of how many linear regions have been \u201ctraversed\u201d. Both metrics are illustrated in Figure 1, showing that they are low for test and training examples, or in regions within the same class, and high otherwise. The second metric is also illustrated in Figure 2. Based on these metrics, the authors show that these metrics correlate with the generalization gap, meaning that the sensitivity of the network and its generalization performance seem to be inherently connected.\n\nhttps://i.imgur.com/iRt3ADe.jpg\nFigure 1: For a network trained on MNIST, illustrations of a possible trajectory (left) and the corresponding sensitivity metrics (middle and right). I refer to the paper for details.\n\nhttps://i.imgur.com/0G8su3K.jpg\nFigure 2: Linear regions for a random 2-dimensional slice of the pre-logit space before and after training.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.08760"
    },
    "458": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.08022",
        "transcript": "In the context of stylization, Ulyanov et al. propose to use instance normalization instead of batch normalization. In detail, instance normalization does not compute the mean and standard deviation used for normalization over the current mini-batch in training. Instead, these statistics are computed per instance individually. This also has the benefit of having the same training and test procedure, meaning that normalization is the same in both cases \u2013 in contrast to batch normalization.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.08022"
    },
    "459": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1803.08494",
        "transcript": "Wu and He propose group normalization as alternative to batch normalization. Instead of computing the statistics used for normalization based on the current mini-batch, group normalization computes these statistics per instance but in groups of channels (for convolutional layers). Specifically, given activations $x_i$ with $i = (i_N, i_C, i_H, i_W)$ indexing along batch size, channels, height and width, batch normalization computes\n\n$\\mu_i = \\frac{1}{|S|}\\sum_{k \\in S} x_k$ and $\\sigma_i = \\sqrt{\\frac{1}{|S|} \\sum_{k \\in S} (x_k - \\mu_i)^2 + \\epsilon}$\n\nwith the set $S$ holds all indices for a specific channel (i.e. across samples, height and width). For group normalization, in contrast, $S$ holds all indices of the current instance and group of channels. Meaning the statistics are computed across height, width and the current group of channels. Here, all channels can be divided into groups arbitrarily. In the paper, on ImageNet, groups of $32$ channels are used. Then, Figure 1 shows that for a batch size of 32, group normalization performs en-par with batch normalization \u2013 although the validation error is slightly larger. This is attributed to the stochastic element of batch normalization that leads to regularization. Figure 2 additionally shows the influence of the batch size of batch normalization and group normalization.\n\nhttps://i.imgur.com/lwP5ycw.jpg\nFigure 1: Training and validation error for different normalization schemes on ImageNet.\n\nhttps://i.imgur.com/0c3CnEX.jpg\nFigure 2: Validation error for different batch sizes.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1803.08494"
    },
    "460": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhangS18",
        "transcript": "Zhang and Sabuncu propose a generalized cross entropy loss for robust learning on noisy labels. The approach is based on the work by Gosh et al. [1] showing that the mean absolute error can be robust to label noise. Specifically, they show that a symmetric loss, under specific assumptions on the label noise, is robust. Here, symmetry corresponds to\n\n$\\sum_{j=1}^c \\mathcal{L}(f(x), j) = C$ for all $x$ and $f$\n\nwhere $c$ is the number of classes and $C$ some constant. The cross entropy loss is not symmetric, while the mean absolute error is. The mean absolute error however, usually results in slower learning and may reach lower accuracy. As alternative, the authors propose\n\n$\\mathcal{L}(f(x), e_j) = \\frac{(1 \u2013 f_j(x)^q)}{q}$.\n\nHere, $f$ is the classifier which is assumed to contain a softmax layer at the end. For $q \\rightarrow 0$ this reduces to the cross entropy and for $q = 1$ it reduces to the mean absolute error. As shown in Figure 1, this loss (or a slightly adapted version, see paper, respectively) may obtain better performance on noisy labels. To this end, the label noise is assumed to be uniform, meaning that $p(\\tilde{y} = k|y = j, x)= 1 - \\eta$ where $\\tilde{y}$ is the perturbed label.\n\nhttps://i.imgur.com/HRQ84Zv.jpg\nFigure 1: Performance of the proposed loss for different $q$ and noise rate $\\eta$ on Cifar-10. A ResNet-34 is used.\n\n[1] Aritra Gosh, Himanshu Kumar, PS Sastry. Robust loss functions under label noise for deep neural networks. AAAI, 2017.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/8094-generalized-cross-entropy-loss-for-training-deep-neural-networks-with-noisy-labels"
    },
    "461": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1903-06293",
        "transcript": "Goodfellow motivates the use of dynamical models as \u201cdefense\u201d against adversarial attacks that violate both the identical and independent assumptions in machine learning. Specifically, he argues that machine learning is mostly based on the assumption that the data is samples identically and independently from a data distribution. Evasion attacks, meaning adversarial examples, mainly violate the assumption that they come from the same distribution. Adversarial examples computed within an $\\epsilon$-ball around test examples basically correspond to an adversarial distribution the is larger (but entails) the original data distribution. In this article, Goodfellow argues that we should also consider attacks violating the independence assumption. This means, as a simple example, that the attacker can also use the same attack over and over again. This yields the idea of correlated attacks as mentioned in the paper\u2019s title. Against this more general threat model, Goodfellow argues that dynamic models are required; meaning the model needs to change (or evolve) \u2013 be a moving target that is harder to attack.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.06293"
    },
    "462": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/ijcnn.2019.8852296",
        "transcript": "Ayinde et al. study the impact of network architecture and weight initialization on learning redundant features. To empirically estimate the number of redundant features, the authors use an agglomerative clustering approach to cluster features based on their cosine similarity. Essentially, given a set of features, these are merged as long as their (average) cosine similarity is within some threshold $\\tau$. Then, this number is compared across network architectures. Figure 1, for example, shows the number of redundant features for different depths of the network and using different activation functions on MNIST. As can be seen, ReLU activations avoid redundant features, while depth of the network usually encourages redundant features.\n\nhttps://i.imgur.com/ICcCL2u.jpg\nFigure 1: Number of redundant features $n_r$ for networks with $n\u2019 = 1000$ hidden units computed suing the given threshold $\\tau$ for computing $n_r$. Experiments with different depths and activation functions are shown.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/ijcnn.2019.8852296"
    },
    "463": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/DinhPBB17",
        "transcript": "Dinh et al. show that it is unclear whether flat minima necessarily generalize better than sharp ones. In particular, they study several notions of flatness, both based on the local curvature and based on the notion of \u201clow change in error\u201d. The authors show that the parameterization of the network has a significant impact on the flatness; this means that functions leading to the same prediction function (i.e., being indistinguishable based on their test performance) might have largely varying flatness around the obtained minima, as illustrated in Figure 1. In conclusion, while networks that generalize well usually correspond to flat minima, it is not necessarily true that flat minima generalize better than sharp ones.\n\nhttps://i.imgur.com/gHfolEV.jpg\nFigure 1: Illustration of the influence of parameterization on the flatness of the obtained minima.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v70/dinh17b.html"
    },
    "464": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1901-10513",
        "transcript": "Ford et al. show that the existence of adversarial examples can directly linked to test error on noise and other types of random corruption. Additionally, obtaining model robust against random corruptions is difficult, and even adversarially robust models might not be entirely robust against these corruptions. Furthermore, many \u201cdefenses\u201d against adversarial examples show poor performance on random corruption \u2013 showing that some defenses do not result in robust models, but make attacking the model using gradient-based attacks more difficult (gradient masking).\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1901.10513"
    },
    "465": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1905-09747",
        "transcript": "Goldblum et al. show that distilling robustness is possible, however, depends on the teacher model and the considered dataset. Specifically, while classical knowledge distillation does not convey robustness against adversarial examples, distillation with a robust teacher model might increase robustness of the student model \u2013 even if trained on clean examples only. However, this seems to depend on both the dataset as well as the teacher model, as pointed out in experiments on Cifar100. Unfortunately, from the paper, it does not become clear in which cases robustness distillation does not work. To overcome this limitation, the authors propose to combine adversarial training and distillation and show that this recovers robustness; the student model\u2019s robustness might even exceed the teacher model\u2019s robustness. This, however, might be due to the additional adversarial examples used during distillation.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.09747"
    },
    "466": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/GargSZV18",
        "transcript": "Garg et al. propose adversarially robust features based on a graph interpretation of the training data. In this graph, training points are connected based on their distance in input space. Robust features are obtained using the eigenvectors of the Laplacian of the graph. It is theoretically shown that these features are robust, based on some assumptions on the graph. For example, the bound obtained on robustness depends on the gap between second and third eigenvalue.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/8217-a-spectral-view-of-adversarially-robust-features"
    },
    "467": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LittwinW18",
        "transcript": "Littwin and Wolf propose a activation variance regularizer that is shown to have a similar, even better, effect than batch normalization. The proposed regularizer is based on an analysis of the variance of activation values; the idea is that the measured variance of these variances is low if the activation values come from a distribution with few modes. Thus, the intention of the regularizer is to encourage distributions of activations with only few modes. This is achieved using the regularizers\n\n$\\mathbb{E}[(1 - \\frac{\\sigma_s^2}{\\sigma^2})^2]$\n\nwhere $\\sigma_s^2$ is the measured variance of activation values and $\\sigma^2$ is the true variance of activation values. The estimate $\\sigma^2_s$ is mostly influenced by the mini-batch used for training. In practice, the regularizer is replaced by\n\n$(1 - \\frac{\\sigma_{s_1}^2}{\\sigma_{s_2}^2 + \\beta})^2$\n\nwhich can be estimated on two different batches, $s_1$ and $s_2$, during training and $\\beta$ is a parameter that can be learned and mainly handles the case where the variance is close to zero. In the paper, the authors provide some theoretical bounds and also make a connection to batch normalization and in which cases and why the regularizer might be a better alternative. These claims are supported by experiments on Cifar and Tiny ImageNet.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/7481-regularizing-by-the-variance-of-the-activations-sample-variances"
    },
    "468": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1007/s10994-011-5268-1",
        "transcript": "Xu and Mannor provide a theoretical paper on robustness and generalization where their notion of robustness is based on the idea that the difference in loss should be small for samples that are close. This implies that, e.g., for a test sample close to a training sample, the loss on both samples should be similar. The authors formalize this notion as follows:\n\nDefinition: Let $A$ be a learning algorithm and $S \\subset Z$ be a training set such that $A(S)$ denotes the model learned on $S$ by $A$; the algorithm $A$ is $(K, \\epsilon(S))$-robust if $Z$ can be partitioned into $K$ disjoint sets, denoted $C_i$ such that $\\forall s \\in S$ it holds:\n\n$s,z \\in C_i \\rightarrow |l(A(S), s) \u2013 l(A(S), z)| \\leq \\epsilon(S)$.\n\nIn words, this means that we can partition the space $Z$ (which is $X \\times Y$ for a supervised problem) into a finite set of subsets and whenever a sample falls into the same partition as a training sample, the learned model should have nearly the same loss on both samples. Note that this notion does not entirely match the notion of adversarial robustness as commonly referred to nowadays. The main difference is that the partition can be chosen, while for adversarial robustness, the \u201cpartition\u201d (usually in form of epsilon-balls around training and testing samples) is fixed.\n\nBased on the above notion of robustness, the authors provide PAC bounds for robust algorithms, i.e. generalization performance of $A$ is linked to its generalization. Furthermore, in several examples, common machine learning techniques such as SVMs and neural networks are shown to be robust under specific conditions. For neural networks, for example, an upper bound on the $L_1$ norm of weights and the requirement of Lipschitz continuity is enough. This actually related to work on adversarial robustness, where Lipschitz continuity and weight regularization is also studied.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/s10994-011-5268-1"
    },
    "469": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1809-03113",
        "transcript": "Li et al. propose an adversarial attack motivated by second-order optimization and uses input randomization as defense. Based on a Taylor expansion, the optimal adversarial perturbation should be aligned with the dominant eigenvector of the Hessian matrix of the loss. As the eigenvectors of the Hessian cannot be computed efficiently, the authors propose an approximation; this is mainly based on evaluating the gradient under Gaussian noise. The gradient is then normalized before taking a projected gradient step. As defense, the authors inject random noise on the input (clean example or adversarial example) and compute the average prediction over multiple iterations.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1809.03113"
    },
    "470": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.03471",
        "transcript": "Lecuyer et al. propose a defense against adversarial examples based on differential privacy. Their main insight is that a differential private algorithm is also robust to slight perturbations. In practice, this amounts to injecting noise in some layer (or on the image directly) and using Monte Carlo estimation for computing the expected prediction. The approach is compared to adversarial training against the Carlini+Wagner attack.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.03471"
    },
    "471": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=geirhos2018imagenettrained",
        "transcript": "Geirhos et al. show that state-of-the-art convolutional neural networks put too much importance on texture information. This claim is confirmed in a controlled study comparing convolutional neural network and human performance on variants of ImageNet image with removed texture (silhouettes) or on edges. Additionally, networks only considering local information can perform nearly as well as other networks. To avoid this bias, they propose a stylized ImageNet variant where textured are replaced randomly, forcing the network to put more weight on global shape information.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=Bygh9j09KX"
    },
    "472": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1904-00760",
        "transcript": "Brendel and Bethge show empirically that state-of-the-art deep neural networks on ImageNet rely to a large extent on local features, without any notion of interaction between them. To this end, they propose a bag-of-local-features model by applying a ResNet-like architecture on small patches of ImageNet images. The predictions of these local features are then averaged and a linear classifier is trained on top. Due to the locality, this model allows to inspect which areas in an image contribute to the model\u2019s decision, as shown in Figure 1. Furthermore, these local features are sufficient for good performance on ImageNet. Finally, they show, on scrambled ImageNet images, that regular deep neural networks also rely heavily on local features, without any notion of spatial interaction between them.\n\nhttps://i.imgur.com/8NO1w0d.png\nFigure 1: Illustration of the heap maps obtained using BagNets, the bag-of-local-features model proposed in the paper. Here, different sizes for the local patches are used.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1904.00760"
    },
    "473": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1906-06316",
        "transcript": "Zhang et al. combine interval bound propagation and CROWN, both approaches to obtain bounds on a network\u2019s output, to efficiently train robust networks. Both interval bound propagation (IBP) and CROWN allow to bound a network\u2019s output for a specific set of allowed perturbations around clean input examples. These bounds can be used for adversarial training. The motivation to combine BROWN and IBP stems from the fact that training using IBP bounds usually results in instabilities, while training with CROWN bounds usually leads to over-regularization.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1906.06316"
    },
    "474": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhangWCHD18",
        "transcript": "Zhang et al. propose CROWN, a method for certifying adversarial robustness based on bounding activations functions using linear functions. Informally, the main result can be stated as follows: if the activation functions used in a deep neural network can be bounded above and below by linear functions (the activation function may also be segmented first), the network output can also be bounded by linear functions. These linear functions can be computed explicitly, as stated in the paper. Then, given an input example $x$ and a set of allowed perturbations, usually constrained to a $L_p$ norm, these bounds can be used to obtain a lower bound on the robustness of networks.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/7742-efficient-neural-network-robustness-certification-with-general-activation-functions"
    },
    "475": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1901-01672",
        "transcript": "Nagarajan and Kolter show that neural networks are implicitly regularized by stochastic gradient descent to have small distance from their initialization. This implicit regularization may explain the good generalization performance of over-parameterized neural networks; specifically, more complex models usually generalize better, which contradicts the general trade-off between expressivity and generalization in machine learning. On MNIST, the authors show that the distance of the network\u2019s parameters to the original initialization (as measured using the $L_2$ norm on the flattened parameters) reduces with increasing width, and increases with increasing sample size. Additionally, the distance increases significantly when fitting corrupted labels, which may indicate that memorization requires to travel a larger distance in parameter space.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1901.01672"
    },
    "476": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.12715",
        "transcript": "Gowal et al. propose interval bound propagation to obtain certified robustness against adversarial examples. In particular, given a neural network consisting of linear layers and monotonic increasing activation functions, a set of allowed perturbations is propagated to obtain upper and lower bounds at each layer. These lead to bounds on the logits of the network; these are used to verify whether the network changes its prediction on the allowed perturbations. Specifically, Gowal et al. consider an $L_\\infty$ ball around input examples; the initial bounds are, thus, $\\underline{z}_0 = x - \\epsilon$ and $\\overline{z}_0 = x + \\epsilon$. For each layer, the bounds are defined as\n\n$\\underline{z}_{k,i} = \\min_{\\underline{z}_{k \u2013 1} \\leq z_{k \u2013 1} \\leq \\overline{z}_{k-1}} e_i^T h_k(z_{k \u2013 1})$\n\nand the analogous maximization problem for the upper bound; here, $h$ denotes the applied layer. For Linear layers and monotonic activation functions, this is easy to solve, as shown in the paper. Moreover, computing these bounds is very efficient, only needing roughly two times the computation of one forward pass. During training, a combination of a clean loss and adversarial loss is used:\n\n$\\kappa l(z_K, y) + (1 - \\kappa) l(\\hat{z}_K, y)$\n\nwhere $z_K$ are the logits of the input $x$, and $\\hat{z}_K$ are the adversarial logits computed as\n\n$\\hat{Z}_{K,y\u2019} = \\begin{cases} \\overline{z}_{K,y\u2019} & \\text{if } y\u2019 \\neq y\\\\\\underline{z}_{K,y} & \\text{otherwise}\\end{cases}$\n\nBoth $\\epsilon$ and $\\kappa$ are annealed during training. In experiments, it is shown that this method results in quite tight bounds on robustness.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.12715"
    },
    "477": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1905-02161",
        "transcript": "Galloway et al. argue that batch normalization reduces robustness against noise and adversarial examples. On various vision datasets, including SVHN and ImageNet, with popular self-trained and pre-trained models they empirically demonstrate that networks with batch normalization show reduced accuracy on noise and adversarial examples. As noise, they consider Gaussian additive noise as well as different noise types included in the Cifar-C dataset. Similarly, for adversarial examples, they consider $L_\\infty$ and $L_2$ PGD and BIM attacks; I refer to the paper for details and hyper parameters. On noise, all networks perform worse with batch normalization, even though batch normalization increases clean accuracy slightly. Against PGD attacks, the provided experiments also suggest that batch normalization reduces robustness; however, the attacks only include 20 iterations and do not manage to reduce the adversarial accuracy to near zero, as is commonly reported. Thus, it is questionable whether batch normalization makes indeed a significant difference regarding adversarial robustness. Finally, the authors argue that replacing batch normalization by weight decay can recover some of the advantage in terms of accuracy and robustness.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.02161"
    },
    "478": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/cejcs/DashBDC16",
        "transcript": "Dash et al. present a reasonably recent survey on radial basis function (RBF) networks. RBF networks can be understood as two-layer perceptrons, consisting of an input layer, a hidden layer and an output layer. Instead of using a linear operation for computing the hidden layers, RBF kernels are used; as simple example the hidden units are computed as\n\n$h_i = \\phi_i(x) = \\exp\\left(-\\frac{\\|x - \\mu_i\\|^2}{2\\sigma_i^2}\\right)$\n\nwhere $\\mu_i$ and $\\sigma_i^2$ are parameters of the kernel. In a clustering interpretation, the $\\mu_i$\u2019s correspond to the kernel\u2019s center and the $\\sigma_i^2$\u2019s correspond to the kernels bandwidth. The hidden units are then summed with weights $w_i$; for one output $y \\in \\mathbb{R}$ this can be written as\n\n$y_i = \\sum_i w_i h_i$.\n\nOriginally, RBF networks were trained in a \u201cclustering\u201d-fashion in order to find the centers $\\mu_i$; the bandwidths are often treated as hyper-parameters. Dash et al. show several alternative approaches based on clustering or orthogonal least squares; I refer to the paper for details.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1515/comp-2016-0005"
    },
    "479": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1903-11257",
        "transcript": "Ahmad and Scheinkman propose a simple sparse layer in order to improve robustness against random noise. Specifically, considering a general linear network layer, i.e.\n\n$\\hat{y}^l = W^l y^{l-1} + b^l$ and $y^l = f(\\hat{y}^l$\n\nwhere $f$ is an activation function, the weights are first initialized using a sparse distribution; then, the activation function (commonly ReLU) is replaced by a top-$k$ ReLU version where only the top-$k$ activations are propagated. In experiments, this is shown to improve robustness against random noise on MNIST.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.11257"
    },
    "480": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1812-03190",
        "transcript": "Zadeh et al. propose a layer similar to radial basis functions (RBFs) to increase a network\u2019s robustness against adversarial examples by rejection. Based on a deep feature extractor, the RBF units compute\n\n$d_k(x) = \\|A_k^Tx + b_k\\|_p^p$\n\nwith parameters $A$ and $b$. The decision rule remains unchanged, but the output does not resemble probabilities anymore. The full network, i.e., feature extractor and RBF layer, is trained using an adapted loss that resembles a max margin loss:\n\n$J = \\sum_i (d_{y_i}(x_i) + \\sum_{j \\neq y_i} \\max(0, \\lambda \u2013 d_j(x_i)))$\n\nwhere $(x_i, y_i)$ is a training examples including label. The loss essentially minimizes the output corresponding to the true class while maximizing the output for all other classes up to a specified margin. Additionally, noise examples are injected during training. For these noise examples,\n\n$\\sum_j \\max(0, \\lambda \u2013 d_j(x))$\n\nis maximized to enforce that these examples are treated as negatives in a rejection setting where samples not corresponding to the data distribution (or adversarial examples) can be rejected by the model. In experiments, the proposed method seems to be more robust against FGSM and iterative attacks (as evaluated on Foolbox).\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1812.03190"
    },
    "481": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1809.09262",
        "transcript": "De Alfaro proposes a deep radial basis function (RBF) network to obtain robustness against adversarial examples. In contrast to \u201cregular\u201d RBF networks, which usually consist of only one hidden layer containing RBF units, de Alfaro proposes to stack multiple layers with RBF units. Specifically, a Gaussian unit utilizing the $L_\\infty$ norm is used:\n\n$\\exp\\left( - \\max_i(u_i(x_i \u2013 w_i))^2\\right)$\n\nwhere $u_i$ and $w_i$ are parameters and $x_i$ are the inputs to the unit \u2013 so the network inputs or the outputs of the previous hidden layer. This unit can be understood as computing a soft AND operation; therefore, an alternative OR operation\n\n$1 - \\exp\\left( - \\max_i(u_i(x_i \u2013 w_i))^2\\right)$\n\nis used as well. These two units are used alternatingly in hidden layers in the conducted experiments. Based on these units, de Alfaro argues that the model is less sensitive to adversarial examples, compared to linear operations as commonly used in ReLU networks.\n\nFor training a deep RBF-network, pseudo gradients are used for both the maximum operation and the exponential function. This is done for simplifying training; I refer to the paper for details.\n\nIn their experiments, on MNIST, a multi-layer perceptron with the proposed RBF units is used. The network consists of 512 AND units, 512 OR units, 512 AND units and finally 10 OR units. Robustness against FGSM and I-FGSM as well as PGD attacks seems to improve. However, the used PGD attack seems to be weaker than usually, it does not manage to reduce adversarial accuracy of a normal networks to near-zero.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1809.09262"
    },
    "482": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=ilyas2019adversarial",
        "transcript": "Ilyas et al. present a follow-up work to their paper on the trade-off between accuracy and robustness. Specifically, given a feature $f(x)$ computed from input $x$, the feature is considered predictive if\n\n$\\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[y f(x)] \\geq \\rho$;\n\nsimilarly, a predictive feature is robust if\n\n$\\mathbb{E}_{(x,y) \\sim \\mathcal{D}}\\left[\\inf_{\\delta \\in \\Delta(x)} yf(x + \\delta)\\right] \\geq \\gamma$.\n\nThis means, a feature is considered robust if the worst-case correlation with the label exceeds some threshold $\\gamma$; here the worst-case is considered within a pre-defined set of allowed perturbations $\\Delta(x)$ relative to the input $x$. Obviously, there also exist predictive features, which are however not robust according to the above definition. In the paper, Ilyas et al. present two simple algorithms for obtaining adapted datasets which contain only robust or only non-robust features. The main idea of these algorithms is that an adversarially trained model only utilizes robust features, while a standard model utilizes both robust and non-robust features. Based on these datasets, they show that non-robust, predictive features are sufficient to obtain high accuracy; similarly training a normal model on a robust dataset also leads to reasonable accuracy but also increases robustness. Experiments were done on Cifar10. These observations are supported by a theoretical toy dataset consisting of two overlapping Gaussians; I refer to the paper for details.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.02175"
    },
    "483": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1903-12269",
        "transcript": "Rakin et al. introduce the bit-flip attack aimed to degrade a network\u2019s performance by flipping a few weight bits. On Cifar10 and ImageNet, common architectures such as ResNets or AlexNet are quantized into 8 bits per weight value (or fewer). Then, on a subset of the validation set, gradients with respect to the training loss are computed and in each layer, bits are selected based on their gradient value. Afterwards, the layer which incurs the maximum increase in training loss is selected. This way, a network\u2019s performance can be degraded to chance level with as few as 17 flipped bits (on ImageNet, using AlexNet).\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1903.12269"
    },
    "484": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1803.03635",
        "transcript": "Frankle and Carbin discover so-called winning tickets, subset of weights of a neural network that are sufficient to obtain state-of-the-art accuracy. The lottery hypothesis states that dense networks contain subnetworks \u2013 the winning tickets \u2013 that can reach the same accuracy when trained in isolation, from scratch. The key insight is that these subnetworks seem to have received optimal initialization. Then, given a complex trained network for, e.g., Cifar, weights are pruned based on their absolute value \u2013 i.e., weights with small absolute value are pruned first. The remaining network is trained from scratch using the original initialization and reaches competitive performance using less than 10% of the original weights. As soon as the subnetwork is re-initialized, these results cannot be reproduced though. This suggests that these subnetworks obtained some sort of \u201coptimal\u201d initialization for learning.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1803.03635"
    },
    "485": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1902.02918",
        "transcript": "Cohen et al. study robustness bounds of randomized smoothing, a region-based classification scheme where the prediction is averaged over Gaussian samples around the test input. Specifically, given a test input, the predicted class is the class whose decision region has the largest overlap with a normal distribution of pre-defined variance. The intuition of this approach is that, for small perturbations, the decision regions of classes can\u2019t vary too much. In practice, randomized smoothing is applied using samples. In the paper, Cohen et al. show that this approach conveys robustness against radii R depending on the confidence difference between the actual class and the \u201crunner-up\u201d class. In practice, the radii also depend on the number of samples used.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1902.02918"
    },
    "486": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1706.02690",
        "transcript": "Liang et al. propose a perturbation-based approach for detecting out-of-distribution examples using a network\u2019s confidence predictions. In particular, the approaches based on the observation that neural network\u2019s make more confident predictions on images from the original data distribution, in-distribution examples, than on examples taken from a different distribution (i.e., a different dataset), out-distribution examples. This effect can further be amplified by using a temperature-scaled softmax, i.e.,\n\n$ S_i(x, T) = \\frac{\\exp(f_i(x)/T)}{\\sum_{j = 1}^N \\exp(f_j(x)/T)}$\n\nwhere $f_i(x)$ are the predicted logits and $T$ a temperature parameter. Based on these softmax scores, perturbations $\\tilde{x}$ are computed using\n\n$\\tilde{x} = x - \\epsilon \\text{sign}(-\\nabla_x \\log S_{\\hat{y}}(x;T))$\n\nwhere $\\hat{y}$ is the predicted label of $x$. This is similar to \u201cone-step\u201d adversarial examples; however, in contrast of minimizing the confidence of the true label, the confidence in the predicted label is maximized. This, applied to in-distribution and out-distribution examples is illustrated in Figure 1 and meant to emphasize the difference in confidence. Afterwards, in- and out-distribution examples can be distinguished using simple thresholding on the predicted confidence, as shown in various experiment, e.g., on Cifar10 and Cifar100.\n\nhttps://i.imgur.com/OjDVZ0B.png\nFigure 1: Illustration of the proposed perturbation to amplify the difference in confidence between in- and out-distribution examples.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1706.02690"
    },
    "487": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.06807",
        "transcript": "Neelakantan et al. study gradient noise for improving neural network training. In particular, they add Gaussian noise to the gradients in each iteration:\n\n$\\tilde{\\nabla}f = \\nabla f + \\mathcal{N}(0, \\sigma^2)$\n\nwhere the variance $\\sigma^2$ is adapted throughout training as follows:\n\n$\\sigma^2 = \\frac{\\eta}{(1 + t)^\\gamma}$\n\nwhere $\\eta$ and $\\gamma$ are hyper-parameters and $t$ the current iteration. In experiments, the authors show that gradient noise has the potential to improve accuracy, especially given optimization.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.06807"
    },
    "488": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iclr/LeeLLS18",
        "transcript": "Lee et al. propose a generative model for obtaining confidence-calibrated classifiers. Neural networks are known to be overconfident in their predictions \u2013 not only on examples from the task\u2019s data distribution, but also on other examples taken from different distributions. The authors propose a GAN-based approach to force the classifier to predict uniform predictions on examples not taken from the data distribution. In particular, in addition to the target classifier, a generator and a discriminator are introduced. The generator generates \u201chard\u201d out-of-distribution examples; ideally these examples are close to the in-distribution, i.e., the data distribution of the actual task. The discriminator is intended to distinguish between out- and in-distribution. The overall algorithm, including the necessary losses, is given in Algorithm 1. In experiments, the approach is shown to allow detecting out-distribution examples nearly perfectly. Examples of the generated \u201chard\u201d out-of-distribution samples are given in Figure 1.\n\nhttps://i.imgur.com/NmF0fpN.png\nAlgorithm 1: The proposed joint training scheme of out-distribution generator $G$, the in-/out-distribution discriminator $G$ and the original classifier providing $P_\\theta$(y|x)$ with parameters $\\theta$.\n\nhttps://i.imgur.com/kAclSQz.png\nFigure 1: A comparison of a regular GAN (a and c) to the proposed framework (c and d). Clearly, the proposed approach generates out-of-distribution samples (i.e., no meaningful digits) close to the original data distribution.\n",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=ryiAv2xAZ"
    },
    "489": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1901-04684",
        "transcript": "Zhang et al. search for \u201cblind spots\u201d in the data distribution and show that blind spot test examples can be used to find adversarial examples easily. On MNIST, the data distribution is approximated using kernel density estimation were the distance metric is computed in dimensionality-reduced feature space (of an adversarially trained model). For dimensionality reduction, t-SNE is used. Blind spots are found by slightly shifting pixels or changing the gray value of the background. Based on these blind spots, adversarial examples can easily be found for MNIST and Fashion-MNIST.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1901.04684"
    },
    "490": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1612.00334",
        "transcript": "Wang et al. discuss an alternative definition of adversarial examples, taking into account an oracle classifier. Adversarial perturbations are usually constrained in their norm (e.g., $L_\\infty$ norm for images); however, the main goal of this constraint is to ensure label invariance \u2013 if the image didn\u2019t change notable, the label didn\u2019t change either. As alternative formulation, the authors consider an oracle for the task, e.g., humans for image classification tasks. Then, an adversarial example is defined as a slightly perturbed input, whose predicted label changes, but where the true label (i.e., the oracle\u2019s label) does not change. Additionally, the perturbation can be constrained in some norm; specifically, the perturbation can be constrained on the true manifold of the data, as represented by the oracle classifier. Based on this notion of adversarial examples, Wang et al. argue that deep neural networks are not robust as they utilize over-complete feature representations.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1612.00334"
    },
    "491": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1145/3128572.3140451",
        "transcript": "Munoz-Gonzalez et al. propose a multi-class data poisening attack against deep neural networks based on back-gradient optimization. They consider the common poisening formulation stated as follows:\n\n$ \\max_{D_c} \\min_w \\mathcal{L}(D_c \\cup D_{tr}, w)$\n\nwhere $D_c$ denotes a set of poisened training samples and $D_{tr}$ the corresponding clea dataset. Here, the loss $\\mathcal{L}$ used for training is minimized as the inner optimization problem. As result, as long as learning itself does not have closed-form solutions, e.g., for deep neural networks, the problem is computationally infeasible. To resolve this problem, the authors propose using back-gradient optimization. Then, the gradient with respect to the outer optimization problem can be computed while only computing a limited number of iterations to solve the inner problem, see the paper for detail. In experiments, on spam/malware detection and digit classification, the approach is shown to increase test error of the trained model with only few training examples poisened.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3128572.3140451"
    },
    "492": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/ccs/MengC17",
        "transcript": "Meng and Chen propose MagNet, a combination of adversarial example detection and removal. At test time, given a clean or adversarial test image, the proposed defense works as follows: First, the input is passed through one or multiple detectors. If one of these detectors fires, the input is rejected. To this end, the authors consider detection based on the reconstruction error of an auto-encoder or detection based on the divergence between probability predictions (on adversarial vs. clean example). Second, if not rejected, the input is passed through a reformed. The reformer reconstructs the input, e.g., through an auto-encoder, to remove potentially undetected adversarial noise.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3133956.3134057"
    },
    "493": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SarkarBMC17",
        "transcript": "Sarkar et al. propose two \u201clearned\u201d adversarial example attacks, UPSET and ANGRI. The former, UPSET, learns to predict universal, targeted adversarial examples. The latter, ANGRI, learns to predict (non-universal) targeted adversarial attacks. For UPSET, a network takes the target label as input and learns to predict a perturbation, which added to the original image results in mis-classification; for ANGRI, a network takes both the target label and the original image as input to predict a perturbation. These networks are then trained using a mis-classification loss while also minimizing the norm of the perturbation. To this end, the target classifier needs to be differentiable \u2013 i.e., UPSET and ANGRI require white-box access.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1707.01159"
    },
    "494": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1803.06959",
        "transcript": "Morcos et al. study the influence of ablating single units as a proxy to generalization performance. On Cifar10, for example, a 11-layer convolutional network is trained on the clean dataset, as well as on versions of Cifar10 where a fraction of $p$ samples have corrupted labels. In the latter cases, the network is forced to memorize examples, as there is no inherent structure in the labels assignment. Then, it is experimentally shown that these memorizing networks are less robust to setting whole feature maps to zero, i.e., ablating them. This is shown in Figure 1. Based on this result, the authors argue that the area under this ablation curve (AUC) can be used as proxy for generalization performance. For example, early stopping or hyper-parameter selection can be done based on this AUC value. Furthermore, they show that batch normalization discourages networks to rely on these so-called single-directions, i.e., single units or feature maps. Specifically, batch normalization seems to favor units holding information about multiple classes/concepts.\n\n\nhttps://i.imgur.com/h2JwLUF.png\nFigure 1: Classification accuracy (y-axis) over the number of units that are ablated (x-axis) for networks trained on Cifar10 with various degrees of corrupted labels. The same experiments (left and right) for MNIST and ImageNet.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1803.06959"
    },
    "495": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1803.06978",
        "transcript": "Xie et al. propose to improve the transferability of adversarial examples by computing them based on transformed input images. In particular, they adapt I-FGSM such that, in each iteration, the update is computed on a transformed version of the current image with probability $p$. When, at the same time attacking an ensemble of networks, this is shown to improve transferability.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1803.06978"
    },
    "496": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1712-00699",
        "transcript": "Ranjan et al. propose to constrain deep features to lie on hyperspheres in order to improve robustness against adversarial examples. For the last fully-connected layer, this is achieved by the L2-softmax, which forces the features to lie on the hypersphere. For intermediate convolutional or fully-connected layer, the same effect is achieved analogously, i.e., by normalizing inputs, scaling them and applying the convolution/weight multiplication. In experiments, the authors argue that this improves robustness against simple attacks such as FGSM and DeepFool.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1712.00699"
    },
    "497": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1701.06548",
        "transcript": "Pereyra et al. propose an entropy regularizer for penalizing over-confident predictions of deep neural networks. Specifically, given the predicted distribution $p_\\theta(y_i|x)$ for labels $y_i$ and network parameters $\\theta$, a regularizer\n\n$-\\beta \\max(0, \\Gamma \u2013 H(p_\\theta(y|x))$\n\nis added to the learning objective. Here, $H$ denotes the entropy and $\\beta$, $\\Gamma$ are hyper-parameters allowing to weight and limit the regularizers influence. In experiments, this regularizer showed slightly improved performance on MNIST and Cifar-10.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1701.06548"
    },
    "498": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1808.02651",
        "transcript": "Liu et al. propose adversarial attacks on physical parameters of images, which can be manipulated efficiently through differentiable renderer. In particular, they propose adversarial lighting and adversarial geometry; in both cases, an image is assumed to be a function of lighting and geometry, generated by a differentiable renderer. By directly manipulating these latent variables, more realistic looking adversarial examples can be generated for synthetic images as shown in Figure 1.\n\nhttps://i.imgur.com/uh2pj9w.png\nFigure 1: Comparison of the proposed attack with known attacks applied to large perturbations, $L_\\infty \\approx 0.82$.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1808.02651"
    },
    "499": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1711-05934",
        "transcript": "Liu et al. propose a white-box attack against defensive distillation. In particular, the proposed attack combines the objective of the Carlini+Wagner attack [1] with a slightly different reparameterization to enforce an $L_\\infty$-constraint on the perturbation. In experiments, defensive distillation is shown to no be robust.\n\n[1] Nicholas Carlini, David A. Wagner: Towards Evaluating the Robustness of Neural Networks. IEEE Symposium on Security and Privacy 2017: 39-57\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.05934"
    },
    "500": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1805-04613",
        "transcript": "Zhou et al. study transferability of adversarial examples against ensembles of randomly perturbed networks. Specifically, they consider randomly perturbing the weights using Gaussian additive noise. Using an ensemble of these perturbed networks, the authors show that transferability of adversarial examples decreases significantly. However, the authors do not consider adapting their attack to this defense scenario.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1805.04613"
    },
    "501": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1810-09225",
        "transcript": "Thang and Evanse propose cost-sensitive certified robustness where different adversarial examples can be weighted based on their actual impact for the application. Specifically, they consider the certified robustness formulation (and the corresponding training scheme) by Wong and Kolter. This formulation is extended by acknowledging that different adversarial examples have different impact for specific applications; this is formulized through a cost matrix which quantifies which source-target label combinations of adversarial examples are actually harmful. Based on this cost matrix, cost-sensitive certified robustness as well as the corresponding training scheme is proposed and evaluated in experiments.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1810.09225"
    },
    "502": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.11279",
        "transcript": "Kim et al. propose Concept Activation Vectors (CAV) that represent the direction of features corresponding to specific human-interpretable concepts. In particular, given a network for a classification task, a concept is defined as a set of images with that concept. A linear classifier is then trained to distinguish images with concept from random images without the concept based on a chosen feature layer. The normal of the obtained linear classification boundary corresponds to the learned Concept Activation Vector (CAV). By considering the directional derivative along this direction for a given input allows to quantify how well the input aligns with the chosen concept. This way, images can be ranked and the model\u2019 sensitivity to particular concepts can be quantified. The idea is also illustrated in Figure 1.\n\nhttps://i.imgur.com/KOqPeag.png\nFigure 1: Process of constructing Concept Activation Vectors (CAVs).\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.11279"
    },
    "503": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1804.08598",
        "transcript": "Ilyas et al. propose three query-efficient black-box adversarial example attacks using distribution-based gradient estimation. In particular, their simplest attacks involves estimating the gradient locally using a search distribution:\n\n$ \\nabla_x \\mathbb{E}_{\\pi(\\theta|x)} [F(\\theta)] = \\mathbb{E}_{\\pi(\\theta|x)} [F(\\theta) \\nabla_x \\log(\\pi(\\theta|x))]$\n\nwhere $F(\\cdot)$ is a loss function \u2013 e.g., using the cross-entropy loss which is maximized to obtain an adversarial example. The above equation, using a Gaussian noise search distribution leads to a simple approximator for the gradient:\n\n$\\nabla \\mathbb{E}[F(\\theta)] = \\frac{1}{\\sigma n} \\sum_{i = 1}^n \\delta_i F(\\theta + \\sigma \\delta_i)$\n\nwhere $\\sigma$ is the search variance and $\\delta_i$ are sampled from a unit Gaussian. This scheme can then be applied as part of the projected gradient descent white-box attacks to obtain adversarial examples.\n\nThe above attack assumes that the black-box network provides probability outputs in order to compute the loss $F$. In the remainder of the paper, the authors also generalize this approach to the label-only case, where the network only provides the top $k$ labels for each input. In experiments, the attacks is shown to be effective while rarely requiring more than $50$k queries on ImageNet.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1804.08598"
    },
    "504": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1809.02861",
        "transcript": "Demontis et al. study transferability of adversarial examples and data poisening attacks in the light of the targeted models gradients. In particular, they experimentally validate the following hypotheses: First, susceptibility to these attacks depends on the size of the model\u2019s gradients; the higher the gradient, the smaller is the perturbation needed to increase the loss. Second, the size of the gradient depends on regularization. And third, the cosine between the target model\u2019s gradients and the surrogate model\u2019s gradients (trained to compute transferable attacks) influences vulnerability. These insights hold for both evasion and poisening attacks and are motivated by a simple linear Taylor expansion of the target model\u2019s loss.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1809.02861"
    },
    "505": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/TaoMLZ18",
        "transcript": "Tao et al. propose Attacks Meet Interpretability, an adversarial example detection scheme based on the interpretability of individual neurons. In the context of face recognition, in a first step, the authors identify neurons that correspond to specific face attributes. This is achieved by constructing sets of images were only specific attributes change, and then investigating the firing neurons. In a second step, all other neurons, i.e., neurons not corresponding to any meaningful face attribute, are weakened in order to improve robustness against adversarial examples. The idea is that adversarial examples make use of these non-interpretable neurons to fool the networks. Unfortunately, this defense has been shown not to be effective in [1].\n\n[1] Nicholas Carlini. Is AmI (Attacks Meet Interpretability) Robust to Adversarial Examples? ArXiv.org, abs/1902.02322, 2019.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/7998-attacks-meet-interpretability-attribute-steered-detection-of-adversarial-samples"
    },
    "506": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/aaai/ParkPSM18",
        "transcript": "Park et al. introduce adversarial dropout, a variant of adversarial training based on adversarially computing dropout masks. Specifically, instead of training on adversarial examples, the authors propose an efficient method to compute adversarial dropout masks during training. In experiments, this approach seems to improve generalization performance in semi-supervised settings.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16322"
    },
    "507": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/raid/0017DG18",
        "transcript": "Liu et al. propose fine-pruning, a combination of weight pruning and fine-tuning to defend against backdoor attacks on neural networks. Specifically, they consider a setting where training is outsourced to a machine learning service; the attacker has access to the network and training set, however, any change in network architecture would be easily detected. Thus, the attacker tries to inject backdoors through data poisening. As defense against such attacks, the authors propose to identify and prune weights that are not used for the actual tasks but only for the backdoor inputs.  This defense can then be combined with fine-tuning and, as shown in experiments, is able to make backdoor attacks less effective \u2013 even when considering an attacker aware of this defense.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-030-00470-5_13"
    },
    "508": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1811-00525",
        "transcript": "Khoury and Hadfield-Menell provide two important theoretical insights regarding adversarial robustness: it is impossible to be robust in terms of all norms, and adversarial training is sample inefficient. Specifically, they study robustness in relation to the problem\u2019s codimension, i.e., the difference between the dimensionality of the embedding space (e.g., image space) and the dimensionality of the manifold (where the data is assumed to actually live on). Then, adversarial training is shown to be sample inefficient in high codimensions.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1811.00525"
    },
    "509": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1812-02606",
        "transcript": "Grosse et al. show that Gaussian Processes allow to reject some adversarial examples based on their confidence and uncertainty; however, attacks maximizing confidence and minimizing uncertainty are still successful. While some state-of-the-art adversarial examples seem to result in significantly different confidence and uncertainty estimates compared to benign examples, Gaussian Processes can still be fooled through particularly crafted adversarial examples. To this end, the confidence is explicitly maximized and, additionally, the uncertainty is constrained to not be larger than the uncertainty of the corresponding benign test example. In experiments, this attack is shown to successfully fool Gaussian Processes while resulting in imperceptible perturbations.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1812.02606"
    },
    "510": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1901-09035",
        "transcript": "Dong et al. study interpretability in the context of adversarial examples and propose a variant of adversarial training to improve interpretability. First the authors argue that neurons do not preserve their interpretability on adversarial examples; e.g., neurons corresponding to high-level concepts such as \u201cbird\u201d or \u201cdog\u201d do not fire consistently on adversarial examples. This result is also validated experimentally, by considering deep representations at different layers. To improve interpretability, the authors propose adversarial training with an additional regularizer enforcing similar features on true and adversarial training examples.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1901.09035"
    },
    "511": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1802-08232",
        "transcript": "Carlini et al. propose several attacks to extract secrets form trained black-box models. Additionally, they show that state-of-the-art neural networks memorize secrets early during training. Particularly on the Penn treebank, after inserting a secret of specific format, the authors validate that the secret can be identified based on the models output probabilities (i.e., black-box access). Several metrics based on the log-perplexity of the secret show that secrets are memorized early during training and memorization happens for all popular architectures and training strategies; additionally, memorization also works for multiple secrets. Furthermore, the authors propose several attacks to extract secrets, most notably through shortest path search. Here, starting with an empty secret, the characters of the secret are identified sequentially in order to minimize log-perplexity. Using this attack, secrets such as credit card numbers are extractable from popular mail datasets.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.08232"
    },
    "512": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1145/3134600.3134606",
        "transcript": "Cao and Gong introduce region-based classification as defense against adversarial examples. In particular, given an input (benign test input or adversarial example), the method samples random point in the neighborhood and classifies the test sample according to the majority vote of the obtained labels.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3134600.3134606"
    },
    "513": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.24963/ijcai.2018/520",
        "transcript": "Cai et al. propose so-called curriculum adversarial training where adversarial training is applied to increasingly strong attacks. Specifically, considering a gradient-based, iterative attack such as projected gradient descent, a common proxy for the strength of the attack is the number of iterations. To avoid issues with forgetting old adversarial examples and reduced accuracy, the authors propose to apply adversarial training with different numbers of iterations. In each turn (called lesson in the paper), the network is trained adversarially for a given number of iterations until the network has high accuracy against this attack; then, the number of iterations is increased and another \u201clesson\u201d is started. In experiments, this method is shown to outperform standard adversarial training.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.24963/ijcai.2018/520"
    },
    "514": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sp/GehrMDTCV18",
        "transcript": "Gehr et al. propose a method based on abstract interpretations in order to verify robustness guarantees of neural networks. First of all, I want to note that (in contrast to most work in adversarial robustness) the proposed method is not intended to improve robustness, but to get robustness certificates. Without going into details, abstract interpretations allow to verify conditions (e.g., robustness) of a function (e.g., a neural network) based on abstractions of the input. In particular, by abstracting a norm-ball around a test sample (as is typically considered in adversarial robustness) using box constraints or polyhedra, leading to an over-approximation of the norm-ball, and transforming these abstractions according to the layers of a network, the network\u2019s output can be checked against robustness conditions without running the network on all individual points in the norm-ball. As a result, if the proposed method certifies robustness for a given input sample and an area around it, the network s indeed robust in this area (soundness). If not, the network might indeed not be robust, or robustness could not be certified due to the method\u2019s over-approximation. For details, I refer to the paper, as well as follow-up work [1] and [2].\n\n[1] Matthew Mirman, Timon Gehr, Martin T. Vechev: Differentiable Abstract Interpretation for Provably Robust Neural Networks. ICML 2018: 3575-3583\n\n[2] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, Martin T. Vechev: Fast and Effective Robustness Certification. NeurIPS 2018: 10825-10836\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/SP.2018.00058"
    },
    "515": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/Alvarez-MelisJ18",
        "transcript": "Alvarez-Melis and Jaakkola propose three requirements for self-explainable models, explicitness, faithfulness and stability, and construct a self-explainable, generalized linear model optimizing for these properties. In particular, the proposed model has the form\n\n$f(x) = \\theta(x)^T h(x)$\n\nwhere $\\theta(x)$ are features (e.g., from a deep network) and $h(x)$ are interpretable features/concepts. In practice, these concepts are learned using an auto-encoder from the raw input while the latent code, which represents $h(x)$, is regularized to learn concept under weak supervision. Additionally, the classifier is regularized to be locally difference-bounded by the concept function $h(x)$. This means that for each point $x_0$ it holds\n\n$\\|f(x) \u2013 f(x_0)\\| \\leq L \\|h(x) \u2013 h(x_0)\\|$ for all $\\|x \u2013 x_0\\|_\\delta$\n\nfor some $\\delta$ and $L$. This condition leads to some stability of interpretations with respect to the concepts $h(x)$. In practice, this is enforced through a regularizer.\n\nIn experiments, the authors argue that this class of models has advantages regarding the following three properties of self-explainable models: explicitness, i.e., whether explanations are actually understandable, faithfulness, i.e. whether estimated importance of features reflects true relevance, and stability, i.e., robustness of interpretations against small perturbations. For some of these conditions, the authors propose quantitative metrics; robustness, for example, can be evaluated using\n\n$\\arg\\max_{\\|x\u2019 - x\\|\\leq\\epsilon} \\frac{\\|f(x) \u2013 f(x\u2019)}{\\|h(x) \u2013 h(x\u2019)\\|}$\n\nwhich is very similar to practically evaluating adversarial robustness.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks"
    },
    "516": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1145/3196494.3196517",
        "transcript": "Cao et al. propose KARMA, a method to defend against data poisening in an online learning system where training examples are obtained through crowdsourcing. The setting, however, is somewhat constrained and can be described as human-in-the-loop. In particular, there is the system, which is maintained by an administrator, and there are users \u2013 among them there might be users with malicious intents, i.e. attackers. KARMA consists of two steps: identifying (possibly polluted) training examples that cause mis-classification of samples within a small oracle set; and then correcting these problems by removing clusters of polluted samples.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3196494.3196517"
    },
    "517": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sp/HerleyO17",
        "transcript": "Herley and van Oorschot explore how to make security research more scientific. In particular, they discuss different historic notions of what \u201cscientific\u201d means and related these insights to current practices in security research. I want to discuss only two points that I found very insightful. First, there seems to be a misalignment between formal methods, and empirical methods. While some researchers argue for more mathematically verifiable security methods, others claim that attackers do not care about mathematical proofs \u2013 and even provably secure systems can be implemented insecurely. And second, security is often based on unfalsifiable claims. This is problematic, as research findings that cannot be refuted by any observable event are generally assumed to be \u201cunscientific\u201d. In security, however, it can easily be shown if a system/method is insecure, while there is no possible observation allowing to determine security.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/SP.2017.38"
    },
    "518": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1145/3243734.3243757",
        "transcript": "Ji et al. propose a model-reuse, or trojaning, attack against neural networks by deliberately manipulating specific weights. In particular, given a specific input, the attacker intends to manipulate the model into mis-classifying this input. This is achieved by first generating semantic neighbors of the input, e.g. through transformations or noise, and then identifying salient features for these inputs. These features are correlated to the classifiers output, i.e. some of them have positive impact on classification, some of them have negative impact. The model is fine-tuned by actively adapting the identifying features until the target input is mis-classified.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3243734.3243757"
    },
    "519": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1809.07802",
        "transcript": "P\u00e9rolat et al. propose a game-theoretic variant of adversarial training on universal adversarial perturbations. In particular, in each training iteration, the model is trained for a specific number of iterations on the current training set. Afterwards, a universal perturbation is found (and the corresponding test images) that fools the network. The found adversarial examples are added to the training set. In the next iteration, the network is trained on the new training set which includes adversarial examples. Overall, this leads to a network being trained on a sequence of universal adversarial perturbations corresponding to earlier versions of that network.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1809.07802"
    },
    "520": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1145/2996758.2996771",
        "transcript": "Russu et al. discuss robustness of linear and non-linear kernel machines through regularization. In particular, they show that linear classifiers can easily be regularized to be robust. In fact, robustness against $L_\\infty$-bounded adversarial examples can be achieved through $L_1$ regularization on the weights. More generally, robustness against $L_p$ attacks are countered by $L_q$ regularization of the weights, with $\\frac{1}{p} + \\frac{1}{q} = 1$. These insights are generalized to the case of non-linear kernel machines; I refer to the paper for details.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/2996758.2996771"
    },
    "521": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.04671",
        "transcript": "Rusu et al. Propose progressive networks, sets of networks allowing transfer learning over multiple tasks without forgetting. The key idea of progressive networks is very simple. Instead of fine-tuning a model (for transfer learning), the pre-trained model is taken and its weights fixed. Another network is then trained from scratch while receiving features from the pre-trained network as additional input.\n\nSpecifically, the authors consider a sequence of tasks. For the first task, a deep neural network (e.g. multi-layer perceptron) is trained. Assuming $L$ layers with hidden activations $h_i^{(1)}$ for $i \\leq L$,  each layer computes\n\n$h_i^{(1)} = f(W_i^{(1)} h_{i-1}^{(1)})$\n\nwhere $f$ is an activation function and for $i = 1$, the network input is used. After training until convergence, a second network is trained \u2013 now on a different task. The parameters of the first network is fixed, but the second network can use the features of the first one:\n\n$h_i^{(2)} = f(W_i^{(2)} h_{i-1}^{(2)} + U_i^{(2:1)}h_{i-1}^{(1)})$.\n\nThis idea can be generalized to the $k$-the network, which can use the activations from all the previous networks:\n\n$h_i^{(k)} = f(W_i^{(k)} h_{i-1}^{(k)} + \\sum_{j < k} U_i^{(k:j)} h_{i-1}^{(j)})$.\n\nFor three networks, this is illustrated in Figure 1.\n\nhttps://i.imgur.com/ndyymxY.png\nFigure 1: An illustration of the feature transfer between networks.\n\nIn practice, however, this approach results in an explosion of parameters and computation. Therefore, the authors apply a dimensionality reduction to the $h_{i \u2013 1}^{(j)}$ for $j < k$. Additionally, an individual scaling factor is used to account for different ranges used in the different networks (also depending on the input data). Then, the above equation can be rewritten as\n\n$h_i^{(k)} = f(W_i^{(k)} h_{i-1}^{(k)} + \\sum_{j < k} U_i^{(k)} f(V_i^{(k)} \\alpha_i^{(:k)} h_{i-1}^{(:k)})$.\n\n(Note that notation has been adapted slightly, as I found the original notation misleading.) Here, $h_{i \u2013 1}^{(:k)}$ denotes the concatenated features from all networks $j < k$. Similarly, for each network, one $\\alpha_i^{(j)}$ is learned to scale the features (note that the notation above would imply a element-wise multiplication of the $\\alpha_i^{(j)}$'s repeated in a vector, or equivalently a matrix-vector product). $V_i^{(k)}$ then describes a dimensionality reduction; overall, a one-layer perceptron is used to \u201ctransfer\u201d features from networks $j < k$ to the current network. The same approach can also be applied to convolutional layers (e.g. a $1 \\times 1$ convolution can be used for dimensionality reduction).\n\nIn experiments, the authors show that progressive networks allow efficient transfer learning (efficient in terms of faster training). Additionally, they study which features are actually transferred.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.04671"
    },
    "522": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1809-02104",
        "transcript": "Shafahi et al. discuss fundamental limits of adversarial robustness, showing that adversarial examples are \u2013 to some extent \u2013 inevitable. Specifically, for the unit sphere, the unit cube as well as for different attacks (e.g., sparse attacks and dense attacks), the authors show that adversarial examples likely exist. The provided theoretical arguments also provide some insights on which problems are more (or less) robust. For example, more concentrated class distributions seem to be more robust by construction. Overall, these insights lead the authors to several interesting conclusions:  First, the results are likely to extent to datasets which actually live on low-dimensional manifolds of the unit sphere/cube. Second, it needs to be differentiated between the existence adversarial examples and our ability to compute them efficiently. Making it harder to compute adversarial examples might, thus, be a valid defense mechanism. And third, the results suggest that lower-dimensional data might be less susceptible to adversarial examples.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1809.02104"
    },
    "523": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1811-11304",
        "transcript": "Shafahi et al. propose universal adversarial training, meaning training on universal adversarial examples. In contrast to regular adversarial examples, universal ones represent perturbations that cause a network to mis-classify many test images. In contrast to regular adversarial training, where several additional iterations are required on each batch of images, universal adversarial training only needs one additional forward/backward pass on each batch. The obtained perturbations for each batch are accumulated in a universal adversarial examples. This makes adversarial training more efficient, however reduces robustness significantly.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1811.11304"
    },
    "524": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/CheneySK17",
        "transcript": "Cheney et al. study the robustness of deep neural networks, especially AlexNet, with regard to randomly dropping or perturbing weights. In particular, the authors consider three types of perturbations: synapse knockouts set random weights to zero, node knockouts set all weights corresponding to a set of neurons to zero, and weight perturbations add random Gaussian noise to the weights of a specific layer. These perturbations are studied on AlexNet, considering the top-5 accuracy on ImageNet; perturbations are considered per layer. For example, Figure 1 (left) shows the influence on accuracy when knocking out synapses. As can be seen, the lower layers, especially the first convolutional layer, are impacted significantly by these perturbations. Similar observations, Figure 1 (right) are made for random perturbations of weights; although the impact is less significant. Especially high-level features, i.e., the corresponding layers, seem to be robust to these kind of perturbations. The authors also provide evidence that these results extend to the top-1 accuracy, as well as other architectures. For VGG, however, the impact is significantly less pronounced which may also be due to the employed dropout layers.\n\nhttps://i.imgur.com/78T6Gg2.png\nFigure 1: Left: Influence of setting weights in the corresponding layers to zero. Right: Influence of randomly perturbing weights of specific layers. Experiments are on ImageNet using AlexNet.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.08245"
    },
    "525": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1902-03020",
        "transcript": "Grosse et al. propose an adversarial attack on a deep neural network\u2019s weight initialization in order to damage accuracy or convergence. An attacker with access to the used deep learning library is assumed. The attack has no knowledge about the training data or the addressed task; however, the attacker has knowledge (through the library\u2019s API) about the network architecture and its initialization. The goal of the attacker is to permutate the initialized weights, without being detected, in order to hinder training. In particular, as illustrated in Figure 1 for two fully connected layers described by\n\n$y(x) = \\text{ReLU}(B \\text{ReLU}(Ax + a) + b)$,\n\nthe attack tries to force a large part of neurons to have zero activation from the very beginning. This attack assumes non-negative input, e.g., images in $[0,1]$ as well as ReLU activations in order to zero-out the selected neurons. In Figure 1, this is achieved by permutating the weights in order to concentrate its negative values in a specific part of the weight matrix. Consecutive application of both weight matrices results in most activations to be zero. This will hinder training significantly as no gradients are available, while keeping the statistics of the weights (e.g., mean and variance) unchanged. A similar strategy can be applied to consecutive convolutional layers, as discussed in detail in the paper. Additionally, by slightly shifting the weights in each weight matrix allows to control the rough number of neurons that receives zero activations; this is intended to have control over the \u201cdegree\u201d of damage, i.e. whether the network should diverge or just achieve lower accuracy.  In experiments, the authors show that the proposed attacks on weight initialization allow to force training to diverge or reach lower accuracy. However, in the majority of cases, training diverges, which makes the attack less stealthy, i.e., easier to detect by the attacked user.\n\nhttps://i.imgur.com/wqwhYFL.png\nhttps://i.imgur.com/2zZMOYW.png\nFigure 1: Illustration of the idea of the proposd attacks on two fully connected layers as described in the text. The color coding illustrates large, usually positive, weight values in black and small, often negative, weight values in light gray.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1902.03020"
    },
    "526": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/iccad.2017.8203770",
        "transcript": "Liu et al. propose slight perturbations of a deep neural network\u2019s weights in order to cause mis-classification on a specific input. Specifically, the authors propose two attacks: the single bias attack, where a single bias value is manipulated in order to cause mis-classification, and the gradient descent attack, where the network\u2019s weights of a particular layer are manipulated through gradient descent to cause mis-classification. In both cases, a specific input example is considered to be fixed. The attack is intended to change the label on this input while being \u201cstealthy\u201d, i.e. not changing accuracy too much. In experiments on MNIST and CIFAR10 it is shown that these attacks are effective in changing the input\u2019s label, however also reduce the overall accuracy of the model.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/iccad.2017.8203770"
    },
    "527": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1902.00577",
        "transcript": "Saralajew et al. evaluate learning vector quantization (LVQ) approaches regarding their robustness against adversarial examples. In particular, they consider generalized LVQ where examples are classified based on their distance to the closest prototype of the same class and the closest prototype of another class. The prototypes are learned during training; I refer to the paper for details. Robustness is compared to adversarial training and evaluated against several attacks, including FGSM, DeepFool and Boundary \u2013 both white-box and black-box attacks. Regarding $L_\\infty$, LVQ usually demonstrates poorer performance than adversarial training. Still, robustness seems to be higher than normally trained deep neural networks. One of the main explanations of the authors is that LVQ follows a max-margin approach; this max-margin idea seems to favor robust models.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1902.00577"
    },
    "528": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/ccs/ZhangGJWSHM18",
        "transcript": "Zhang et al. propose a watermarking approach to protect the intellectual property of deep neural network models. Here, the watermarking concept is generalized from multimedia; specifically, the purpose of a watermark is to uniquely identify a neural network model as the original owner\u2019s property to avoid plagiarism. The problem is illustrated in Figure 1. As watermarks, the authors consider perturbed input images. During training, these perturbations are trained to produce very specific outputs, as illustrated in Figure 2. For example, random pixels are added, or text is added to images. After training, the model can be uniquely identified by these perturbed watermark images that are unrelated to the actual task.\n\nhttps://i.imgur.com/TydqBwo.png\nFigure 1: Illustration of the problem setting for watermarking.\n\nhttps://i.imgur.com/5Zlei0z.png\nFigure 2: Example watermarks.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3196494.3196550"
    },
    "529": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1804-02485",
        "transcript": "Lamb et al. introduce fortified networks with denoising auto encoders as hidden layers. These denoising auto encoders are meant to learn the manifold of hidden representations, project adversarial input back to the manifold and improve robustness. The main idea is illustrated in Figure 1. The denoising auto encoders can be added at any layer and are trained jointly with the classification network \u2013 either on the original input, or on adversarial examples as done in adversarial training.\n\nhttps://i.imgur.com/5vaZrVk.png\nFigure 1: Illustration of a fortified layer, i.e., a hidden layer that is reconstructed through a denoising auto encoder as defense mechanism. The denoising auto encoders are trained jointly with the network.\n\nIn experiments, they show that the proposed defense mechanism improves robustness on MNIST and CIFAR, compared to adversarial training and other baselines. The improvements are, however, very marginal. Especially, as the proposed method imposes an additional overhead (in addition to adversarial training).\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1804.02485"
    },
    "530": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1805.09190",
        "transcript": "Schott et al. propose an analysis-by-synthetis approach for adversarially robust MNIST classification. In particular, as illustrated in Figure 1, class-conditional variational auto-encoders (i.e., one variational auto-encoder per class) are learned. The respective recognition models, i.e., encoders, are discarded. For classification, the optimization problem\n\n$l_y^*(x) = \\max_z \\log p(x|z) - \\text{KL}(\\mathcal{N}(z, \\sigma I)|\\mathcal{N}(0,1))$\n\nis solved for each class $z$. Here, $p(x|z)$ represents the learned generative model. The optimization problem leads a latent code $z$ corresponding to the best reconstruction of the input. The corresponding likelihood can be used for classificaiton using Bayes\u2019 theorem. The obtained posteriors $p(y|x)$ are then scaled using a modified softmax (see paper) to obtain the final decision. (Additionally, input binarization is used as defense.)\n\nhttps://i.imgur.com/ignvoHQ.png\nFigure 1: The proposed analysis by synthesis approach to MNIST classification. The depicted generators are taken from class-specific variational auto-encoders.\n\nIn addition to the proposed defense, Schott et al. also derive lower and upper bounds on the robustness of the classification procedure. These bounds can be derived from the optimization problem above, see the paper for details.\n\nIn experiments, they show that their defense outperforms state-of-the-art adversarial training and allows to estimate tight bounds. In addition, the method is robust against distal adversarial examples and the adversarial examples look more meaningful, see Figure 2.\n\nhttps://i.imgur.com/uxGzzg1.png\nFigure 2: Adversarial examples for the proposed \u201cABS\u201d method, its binary variant and related work.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1805.09190"
    },
    "531": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ShenJGZ17",
        "transcript": "Shen et al. introduce APE-GAN, a generative adversarial network (GAN) trained to remove adversarial noise from adversarial examples. In specific, as illustrated in Figure 1, a GAN is traiend to specifically distinguish clean/real images from adversarial images. The generator is conditioned on th einput image and can be seen as auto encoder. Then, during testing, the generator is applied to remove the adversarial noise.\n\nhttps://i.imgur.com/mgAbzCT.png\nFigure 1: The proposed adversarial perturbation eliminating GAN (APE-GAN), see the paper for details.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1707.05474"
    },
    "532": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SongSKE18",
        "transcript": "Song et al. propose generative adversarial examples, crafted using a generative adversarial network (GAN) from scratch. In particular a GAN is trained on the original images in order to approximate the generative data distribution. Then, adversarial examples can be found in the learned latent space by finding a latent code that minimizes a loss consisting of fooling the target classifier, not fooling an auxiliary classifier (to not change the actual class) and (optionally) staying close to some fixed random latent code. These adversarial examples do not correspond ot original images anymore, instead they are unrestricted and computed from scratch. Figure 1 shows examples.\n\nhttps://i.imgur.com/Krr9MuU.png\nFigure 1: Examples of projected gradient descent (PGD, top) to find adversarial examples in the image space, and found adversarial examples in the latent space, as proposed.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/8052-constructing-unrestricted-adversarial-examples-with-generative-models"
    },
    "533": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1007/978-3-030-01258-8_39",
        "transcript": "Su et al. present an extensive robustness study of 18 different ImageNet networks. Among these networks, popular architectures such as AlexNet, VGG, Inception or ResNet can be found. Their main result shows a trade-off between robustness accuracy. A possible explanation is that recent increases in accuracy are only possible when sacrificing network robustness. In particular, as shown in Figure 1, the robustness scales linearly in the logarithm of the classification error (note that Figure 1 shows accuracy instead). Here, robustness is measured in terms of the necessary distortion of Carlini&Wagner attacks to achieve a misclassification. However, it can also be seen, that the regressed line (red) mainly relies on the better robustness of AlexNet and VGG 16/19 compared to all other networks. Therefore, I find it questionable whether this trade-off generalizes to other tasks or deep learning in general.\n\nhttps://i.imgur.com/ss7EZwV.png\nFigure 1: $L_2$ pixel distortion of Carlini&Wagner attacks \u2013 as indicator for robustness \u2013 plotted against the top-1 accuracy on ImageNet for the 18 different architectures listed in the legend.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-030-01258-8_39"
    },
    "534": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iclr/ZhaoDS18",
        "transcript": "Zhao et al. propose a generative adversarial network (GAN) based approach to generate meaningful and natural adversarial examples for images and text. With natural adversarial examples, the authors refer to meaningful changes in the image content instead of adding seemingly random/adversarial noise \u2013 as illustrated in Figure 1. These natural adversarial examples can be crafted by first learning a generative model of the data, e.g., using a GAN together with an inverter (similar to an encoder), see Figure 2. Then, given an image $x$ and its latent code $z$, adversarial examples $\\tilde{z} = z + \\delta$ can be found within the latent code. The hope is that these adversarial examples will correspond to meaningful, naturally looking adversarial examples in the image space.\n\nhttps://i.imgur.com/XBhHJuY.png\nFigure 1: Illustration of natural adversarial examples in comparison ot regular, FGSM adversarial examples.\n\nhttps://i.imgur.com/HT2StGI.png\nFigure 2: Generative model (GAN) together with the required inverter.\n\nIn practice, e.g., on MNIST, any black-box classifier can be attacked by randomly sampling possible perturbations $\\delta$ in the random space (with increasing norm) until an adversarial perturbation is found. Here, the inverted from Figure 2 is trained on top of the critic of the GAN (although specific details are missing in the paper).\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://openreview.net/forum?id=H1BLjgZCb"
    },
    "535": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/cvpr.2016.514",
        "transcript": "Xie et al. Propose to regularize deep neural networks by randomly disturbing (i.e., changing) training labels.  In particular, for each training batch, they randomly change the label of each sample with probability $\\alpha$ - when changing a label, it\u2019s sampled uniformly from the set of labels. In experiments, the authors show that this sort of loss regularization improves generalization. However, Dropout usually performs better; in their case, only the combination with leads to noticable improvements on MNIST and SVHN \u2013 and only compared to no regularization and data augmentation at all. In their discussion, they offer two interpretations of dropping labels. First, it canbe seen as learning an ensemble of models on different noisy label sets; second, it can be seen as implicitly performing data augmentation. Both interepretation area reasonable, but do not provide a definite answer to why disturbing training labels should work well.\n\nhttps://i.imgur.com/KH36sAM.png\nFigure 1: Comparison of training testing error rate during training for no regularization, dropout and DropLabel.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/cvpr.2016.514"
    },
    "536": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/CarliniW16",
        "transcript": "Carlini and Wagner show that defensive distillation as defense against adversarial examples does not work. Specifically, they show that the attack by Papernot et al [1] can easily be modified to attack distilled networks. Interestingly, the main change is to introduce a temperature in the last softmax layer. This termperature, when chosen hgih enough will take care of aligning the gradients from the softmax layer and from the logit layer \u2013 otherwise, they will have significantly different magnitude. Personally, I found that this also aligns with the observations in [2] where Carlini and Wagner also find that attack objectives defined on the logits work considerably better.\n\n[1] N. Papernot, P. McDaniel, X. Wu, S. Jha, A. Swami. Distillation as a defense to adersarial perturbations against deep neural networks. SP, 2016.\n\n[2] N. Carlini, D. Wagner. Towards Evaluating the Robustness of Neural Networks. ArXiv, 2016.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1607.04311"
    },
    "537": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1804.03308",
        "transcript": "Galloway et al. provide a theoretical and experimental discussion of adversarial training and weight decay with respect to robustness as well as generalization. In the following I want to try and highlight the most important findings based on their discussion of linear logistic regression. Considering the softplus loss $\\mathcal{L}(z) = \\log(1 + e^{-z})$, the learning problem takes the form:\n\n$\\min_w \\mathbb{E}_{x,y \\sim p_{data}} [\\mathcal{L}(y(w^Tx + b)]$\n\nwhere $y \\in \\{-1,1\\}$. This optimization problem is also illustrated in Figure 1 (top). Now considering $L_2$ weight decay can also be seen to be equivalent to scaling the softplus loss. In particular, Galloway et al. Argue that $w^Tx + b = \\|w\\|_2 d(x)$ where $d(x)$ is the (signed) Euclidean distance to the decision boundary. (This follows directly from the fact that $d(x) = \\frac{w^Tx +b}{\\|w\\|w_2}$.) Then, the problem can be rewritten as\n\n$\\min_w \\mathbb{E}_{x,y \\sim p_{data}} [\\mathcal{L}(yd(x) \\|w\\|_2)]$\n\nThis can be understood as a scaled version of the softplus loss; adding a $L_2$ weight decay term basically controls the level of scaling. This is illustrated in Figure 1 (middle) for different levels of scaling. Finally, adversarial training means training on the worst-case example for a given $\\epsilon$. In practice, for the linear logistic regression model, this results in training on $x - \\epsilon y \\frac{w}{\\|w\\|_2}$ - which can easily be understood when considering that the attacker can cause the most disturbance when changing the samples in the direction of $-w$ for label $1$. Then,\n\n$y (w^T(x - \\epsilon y \\frac{w}{\\|w\\|_2}) + b) = y(w^Tx + b) - \\epsilon \\|w\\|_2 = \\|w\\|_2 (yd(x) - \\epsilon)$,\n\nwhich results in a shift of the data by $\\epsilon$ - as illustrated in Figure 1 (bottom). Overall, show that weight decay acts as scaling the objective and adversarial training acts as shifting the data (or equivalently the objective).\n\nIn the non-linear case, decaying weights is argued to be equivalent to decaying the logits. Effectively, this results in a temperature parameter for the softmax function resulting in smoother probability distributions. Similarly, adversarial training (in a first-order approximation) can be understood as effectively reducing the probability attributed to the correct class. Here, again, weight decay results in a scaling effect and adversarial training in a shifting effect. In conclusion, adversarial training is argued to be only effective with small perturbation sizes (i.e., if the shift is not too large), weil weight decay is also beneficial for generalization. However, from reading the paper, it is unclear what the actual recommendation on both methods is.\n\nIn the experimental section, the authors focus on two models, a wide residual network and a very constrained 4-layer convolutional neural network. Here, their discussion shifts slightly to the complexity of the employed model. While not stated very explicitly, one of the take-aways is that the simpler model might be more robust, especially for fooling images.\n\nhttps://i.imgur.com/FKT3a2O.png\nhttps://i.imgur.com/wWwFKqn.png\nhttps://i.imgur.com/oaTfqHJ.png\nFigure 1: Illustration of the linear logistic regression argument. Top: illustration of linear logistic regression where $\\xi$ is the loss $\\mathcal{L}$, middle: illustration of the impact of weight decay/scaling, bottom: illustration of the impact of shift for adversarial training.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1804.03308"
    },
    "538": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/icip.2016.7533048",
        "transcript": "Fawzi et al. propose an adaptive data augmentation scheme based on adversarial transformations similar to adversarial training. In particular, in each training iteration \u2013 and for each sample/batch \u2013 they compute an adversarial version by finding a transformation that maximizes the training loss. The transformation is usually constrained to a specific class of transformations \u2013 on MNIST, for example, they consider affine transformations. Additionally, only small transformations are considered, a constraint very similar to the perturbation constraint of adversarial examples. In order to find these adversarial transformations during training, they employ an iterative algorithm based on a local first-order approximation of the training loss on the transformed sample. This leads to a simple linear program that can be solved in each iteration. Overall, however, this results in a significant computational overhead during training (as does adversarial training, as well). Quantitative results on MNIST are encouraging; specifically, Figure 1 shows that the test loss converges faster when employing these adversarial transformations as data augmentation in contrast to random transformations.\n\nhttps://i.imgur.com/OxuQCP9.png\nFigure 1: Test loss for a model trained without data augmentation, the proposed adaptive data augmentation and random data augmentation, see paper for details.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/icip.2016.7533048"
    },
    "539": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/WangJC17",
        "transcript": "Wang et al. discuss the robustness of $k$-nearest neighbors against adversarial perturbations, providing both a theoretical analysis as well as a robust 1-nearest neighbor version. Specifically, for low $k$ it is shown that nearest neighbor is usually not robust. Here, robustness is judged in a distributional sense; so for fixed and low $k$, the lowest distance of any training sample to an adversarial sample tends to zero, even if the training set size increases. For $k \\in \\mathcal{O}(dn \\log n)$, however, it is shown that $k$/nearest neighbor can be robust \u2013 the prove, showing where the $dn \\log n$ comes from can be found in the paper. Finally, they propose a simple but robust $1$-nearest neighbor algorithm. The main idea is to remove samples from the training set that cause adversarial examples. In particular, a minimum distance between any two samples with different labels is enforced.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.03922"
    },
    "540": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/SharifBR18",
        "transcript": "Sharif et al. study the effectiveness of $L_p$ norms for creating adversarial perturbations. In this context, their main discussion revolves around whether $L_p$ norms are sufficient and/or necessary for perceptual similarity. Their main conclusion is that $L_p$ norms are neither necessary nor sufficient to ensure perceptual similarity. For example, an adversarial example might be within a specific $L_p$ bal, but humans might still identify it as not similar enough to the originally attacked sample; on the other hand, there are also some imperceptible perturbations that usually extend beyond a reasonable $L_p$ ball. Such transformatons might for example include small rotations or translation. These findings are interesting because it indicates that our current model, or approximation, or perceptual similarity is not meaningful in all cases.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPRW.2018.00211"
    },
    "541": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/RatnerEHDR17",
        "transcript": "Ratner et al. Train an adversarial generative network to learn domain-specific sequences of transformations useful for data augmentation. In particular, as indicated in Figure 1, the generator learns to predict sequences of user-specified transformations and the classifier is intended to distinguish the original images from the transformed ones. For training, the authors use reinforcement learning, because the transformations are not necessarily differentiable \u2013 which makes usage of the proposed method very convenient.\n\nhttps://i.imgur.com/hHQkhIk.png\nFigure 1: High-level illustration of the proposed method for learning data augmentation.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation"
    },
    "542": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1803.04765",
        "transcript": "Papernot and McDaniel introduce deep k-nearest neighbors where nearest neighbors are found at each intermediate layer in order to improve interpretbaility and robustness. Personally, I really appreciated reading this paper; thus, I will not only discuss the actually proposed method but also highlight some ideas from their thorough survey and experimental results.\n\nFirst, Papernot and McDaniel provide a quite thorough survey of relevant work in three disciplines: confidence, interpretability and robustness. To the best of my knowledge, this is one of few papers that explicitly make the connection of these three disciplines. Especially the work on confidence is interesting in the light of robustness as Papernot and McDaniel also frequently distinguish between in-distribution and out-distribution samples. Here, it is commonly known that deep neural networks are over-confidence when moving away from the data distribution.\n\nThe deep k-nearest neighbor approach is described in Algorithm 1 and summarized in the following. For a trained model and a training set of labeled samples, they first find k nearest neighbors for each intermediate layer of the network. The layer nonconformity with a specific label $j$, referred to as $\\alpha$ in Algorithm 1, is computed as the number of labels that in the set of nearest neighbors that do not share this label. By comparing these nonconformity values to a set of reference values (computing over a set of labeled calibration data), the prediction can be refined. In particular, the probability for label $j$ can be computed as the fraction of reference nonconformity values that are higher than the computed one. See Algorthm 1 or the paper for details.\n\nhttps://i.imgur.com/RA6q1VI.png\nhttps://i.imgur.com/CkRf8ex.png\nAlgorithm 1: The deep k-nearest neighbor algorithm and an illustration.\n\nFinally, they provide experimental results \u2013 again considering the three disciplines of confidence/credibility, interpretability and robustness. The main take-aways are that the resulting confidences are more reliable on out-of-distribution samples, which also include adversarial examples. Additioanlly, the nearest neighbor allow very basic interpretation of the predictions.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1803.04765"
    },
    "543": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1801.04693",
        "transcript": "Luo et al. Propose a method to compute less-perceptible adversarial examples compared to standard methods constrained in $L_p$ norms. In particular, they consider the local variation of the image and argue that humans are more likely to notice larger variations in low-variance regions than vice-versa. The sensitivity of a pixel is therefore defined as one over its local variance, meaning that it is more sensitive to perturbations. They propose a simple algorithm which iteratively sorts pixels by their sensitivity and then selects a subset to perturb each step. Personally, I wonder why they do not integrate the sensitivity into simple projected gradient descent attacks, where a Lagrange multiplier is used to enforce the $L_p$ norm of the sensitivity weighted perturbation. However, qualitative results show that their approach also works well and results in (partly) less perceptible changes, see Figure 1.\n\nhttps://i.imgur.com/M7Ile8Y.png\nFigure 1: Qualitative results including a comparison to other state-of-the-art attacks.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1801.04693"
    },
    "544": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1706.04599",
        "transcript": "Guo et al. study calibration of deep neural networks as post-processing step. Here, calibration means a correction of the predicted confidence scores as these are commonlz too overconfident in recent deep networks. They consider several state-of-the-art post-processing steps for calibration, but surprisingly, they show that a simple linear mapping, or even scaling, works surprisingly well. So if $z_i$ are the logits of the network, then (the network being fixed) a parameter $T$ is found such that\n\n$\\sigma(\\frac{z_i}{T})$\n\nis calibrated and minimized the NLL loss on a held-out validation set. Here, the temeratur $T$ either softens or roughens the probability distribution over classes. Interestingly, finding $T$ by optimizing the same training loss helps to reduce over-confidence.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1706.04599"
    },
    "545": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1710-10547",
        "transcript": "Ghorbani et al. Show that neural network visualization techniques, often introduced to improve interpretability, are susceptible to adversarial examples. For example, they consider common feature-importance visualization techniques and aim to find an advesarial example that does not change the predicted label but the original interpretation \u2013 e.g., as measured on some of the most important features. Examples of the so-called top-1000 attack where the 1000 most important features are changed during the attack are shown in Figure 1. The general finding, i.e., that interpretations are not robust or reliable, is definitely of relevance for the general acceptance and security of deep learning systems in practice.\n\nhttps://i.imgur.com/QFyrSeU.png\nFigure 1: Examples of changed interpretations.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1710.10547"
    },
    "546": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1801-02612",
        "transcript": "Xiao et al. propose adversarial examples based on spatial transformations. Actually, this work is very similar to the adversarial deformations of [1]. In particular, a deformation flow field is optimized (allowing individual deformations per pixel) to cause a misclassification. The distance of the perturbation is computed on the flow field directly. Examples on MNIST are shown in Figure 1 \u2013 it can clearly be seen that most pixels are moved individually and no kind of smoothness is enforced. They also show that commonly used defense mechanisms are more or less useless against these attacks. Unfortunately, and in contrast to [1], they do not consider adversarial training on their own adversarial transformations as defense.\n\nhttps://i.imgur.com/uDfttMU.png\nFigure 1: Examples of the computed adversarial examples/transformations on MNIST for three different models. Note that these are targeted attacks.\n\n[1] R. Alaifair, G. S. Alberti, T. Gauksson. Adef: an Iterative Algorithm to Construct Adversarial Deformations. ArXiv, abs/1804.07729v2, 2018.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1801.02612"
    },
    "547": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.10733",
        "transcript": "Sharma and Chen provide an experimental comparison of different state-of-the-art attacks against the adversarial training defense by Madry et al. [1]. They consider several attacks, including the Carlini Wagner attacks [2], elastic net attacks [3] as well as projected gradient descent [1]. Their experimental finding \u2013 that the defense by Madry et al. Can be broken by increasing the allowed perturbation size (i.e., epsilon) \u2013 should not be surprising. Every network trained adversarially will only defend reliable against attacks from the attacker used during training.\n\n[1] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. ArXiv, 1706.06083, 2017.\n[2] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks.InIEEE Symposiumon Security and Privacy (SP), 39\u201357., 2017.\n[3] P.Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C.J. Hsieh.  Ead:  Elastic-net attacks to deep neuralnetworks via adversarial examples. arXiv preprint arXiv:1709.04114, 2017.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1710.10733"
    },
    "548": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1802-06627",
        "transcript": "Dumont et al. Compare different adversarial transformation attacks (including rotations and translations) against common as well as rotation-invariant convolutional neural networks. On MNIST, CIFAR-10 and ImageNet, they consider translations, rotations as well as the attack of [1] based on spatial transformer networks. Additionally, they consider rotation-invariant convolutional neural networks \u2013 however, both the attacks and the networks are not discussed/introduced in detail.  The results are interesting because translation- and rotation-based attacks are significantly more successful on CIFAR-10 compared to MNIST and ImageNet. The authors, however, do not give a satisfying explanation of this observation.\n\n[1] C. Xiao, J.-Y. Zhu, B. Li, W. H, M. Liu, D. Song. Spatially-Transformed Adversarial Examples. ICLR, 2018.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.06627"
    },
    "549": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1711-09115",
        "transcript": "Kanbak et al. propose ManiFool, a method to determine a network\u2019s invariance to transformations by iteratively finding adversarial transformations. In particular, given a class of transformations to consider, ManiFool iteratively alternates two steps. First, a gradient step is taken in order to move into an adversarial direction; then, the obtained perturbation/direction is projected back to the space of allowed transformations. While the details are slightly more involved, I found that this approach is similar to the general projected gradient ascent approach to finding adversarial examples. By finding worst-case transformations for a set of test samples, Kanbak et al. Are able to quantify the invariance of a network against specific transformations. Furthermore, they show that adversarial fine-tuning using the found adversarial transformations allows to boost invariance, while only incurring a small loss in general accuracy. Examples of the found adversarial transformations are shown in Figure 1.\n\nhttps://i.imgur.com/h83RdE8.png\nFigure 1: The proposed attack method allows to consider different classes of transformations as shown in these examples.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.09115"
    },
    "550": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1712-09665",
        "transcript": "Brown et al. Introduce a universal adversarial patch that, when added to an image, will cause a targeted misclassification. The concept is illustrated in Figure 1; essentially, a \u201csticker\u201d is computed that, when placed randomly on an image, causes misclassification. In practice, the objective function optimized can be written as\n\n$\\max_p \\mathbb{E}_{x\\sim X, t \\sim T, l \\sim L} \\log p(y|A(p,x,l,t))$\n\nwhere $y$ is the target label and $X$, $T$ and $L$ are te data space, the transformation space and the location space, respectively. The function $A$ takes as input the image and the patch and places the adversarial patch on the image according to the transformation and the location $t$ and $p$. Note that the adversarial patch is unconstrained (in contrast to general adversarial examples). In practice, the computed patch might look as illustrated in Figure 1.\n\nhttps://i.imgur.com/a0AB6Wz.png\nFigure 1: Illustration of the optimization procedure to obtain adversarial patches.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1712.09665"
    },
    "551": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1802-00420",
        "transcript": "Athalye et al. propose methods to circumvent different types of defenses against adversarial example based on obfuscated gradients. In particular, they identify three types of obfuscated gradients: shattered gradients (e.g., caused by undifferentiable parts of a network or through numerical instability), stochastic gradients, and exploding and vanishing gradients. These phenomena all influence the effectiveness of gradient-based attacks. Athalye et al. Give several indicators of how to find out when obfuscated gradients occur. Personally, I find most of these points straight forward, but it is still beneficial to write these \u201cdebug strategies\u201d down. The main contribution, however, is a comprehensive evaluation of all eight ICLR\u201918 defenses against state-of-the-art attacks. As all (except adversarial training) cause obfuscated gradients, Athalye et al. Discuss several strategies to \u201cun-obfuscate\u201d the gradients to successfully compute adversarial examples. Overall, they show that seven out of eight defenses are not reliable, only adversarial training with projected gradient descent can withstand attacks limited to $\\epsilon\\approx 0.3$.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.00420"
    },
    "552": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1804-07729",
        "transcript": "Alaifari et al. propose an iterative attack to construct adversarial deformations of images. In particular, and in contrast to general adversarial perturbations, adversarial deformations are described through a deformation vector field \u2013 and the corresponding norm of this vector field may be bounded; an illustration can be found in Figure 1. The adversarial deformation is computed iteratively where the deformation itself is expressed in a differentiable manner. In contrast to very simple transformations such as rotations and translations, the computed adversarial deformations may contain significantly more subtle deformations as shown in Figure 2. The authors show that such deformations can successful attack MNIST and ImageNet models.\n\nhttps://i.imgur.com/7N8rLaK.png\nFigure 1: Illustration of the advantage of using general pixel-level deformations compared to simple transformations such as translations or rotations.\n\nhttps://i.imgur.com/dCWBoI8.png\nFigure 2: Illustration of untargeted (top) and targeted (bottom) attacks on ImageNet.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1804.07729"
    },
    "553": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1806-11146",
        "transcript": "Elsayed et al. use universal adversarial examples to reprogram neural networks in order to perform different tasks. In particular, e.g., on ImageNet, an adversarial example\n\n$\\delta = \\tanh(W \\cdot M)$\n\nis computed where $M$ is a mask image (see Figure 1, in the paper the mask image essentially embeds a smaller image into an ImageNet-sized image) and $W$ is the adversarial perturbation itself (note that the notaiton was changed slightly for simplification). The hyperbolic tangent constraints the adversarial example, also called adversarial program, to the valid range of $[-1,1]$ as used in most ImageNet networks. The adversarial program is comuted by minimizing\n\n$\\min_W (-\\log P(h(\\tilde{y}) | \\tilde{x}) + \\lambda \\|W\\|_2^2$.\n\nHere, $h$ is a function mapping the labels of the target taks (e.g., MNIST classification) to the $1000$ labels of the ImageNet classification task (e.g., using the first ten labels, or assigning multiple ImageNet labels to one MNIST label). Essentially, this means minimizing the cross entropy loss of the new task (with new labels) to solve for the adversarial program. Examples of adversarial programs for different tasks and architectures are shown in Figure 1.\n\nhttps://i.imgur.com/hPLQn9m.png\nFigure 1: Adversarial programs for different ImageNet architectures (columns) and tasks (rows). Counting refers to a simple task of counting white squares (see paper).\n\nInterestingly, these adversarial programs are able to achieve quite high accuracy on tasks such as MNIST and CIFAR10. Additionally, the authors found that this is only possible for trained networks, not for networks with random weights (although, this might also have other reasons).\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1806.11146"
    },
    "554": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1804-03286",
        "transcript": "Athalye and Carlini present experiments showing that pixel deflection [1] and high-level guided denoiser [2] are ineffective as defense against adversarial examples. In particular, they show that these defenses are not effective against the (currently) strongest first-order attack, projected gradient descent. Here, they also comment on the right threat model to use and explicitly state that the attacker would know the employed defense \u2013 which intuitively makes much sense when evaluating defenses.\n\n[1] Prakash, Aaditya, Moran, Nick, Garber, Solomon, DiLillo, Antonella, and Storer, James. Deflecting adversarial at tacks with pixel deflection. In CVPR, 2018.\n[2] Liao, Fangzhou, Liang, Ming, Dong, Yinpeng, Pang, Tianyu, Zhu, Jun, and Hu, Xiaolin.  Defense against adversarial attacks using high-level representation guided denoiser. In CVPR, 2018.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1804.03286"
    },
    "555": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/TeoGRS07",
        "transcript": "Teo et al. propose a convex, robust learning framework allowing to integrate invariances into SVM training. In particular, they consider a set of valid transformations and define the cost of a training sample (i.e., pair of data and label) as the loss under the worst case transformation \u2013 this definition is very similar to robust optimization or adversarial training. Then, a convex upper bound on this cost is derived. Given, that the worst case transformation can be found efficiently, two different algorithms for minimizing this loss are proposed and discussed. The framework is evaluated on MNIST. However, considering error rate, the approach does not outperform data augmentation significantly (here, data augmentation means adding \u201cvirtual, i.e., transformed, samples to the training set). However, the proposed method seems to find sparser solutions, requiring less support vectors.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/3218-convex-learning-with-invariances"
    },
    "556": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/DongLPS0HL18",
        "transcript": "Dong et al. introduce momentum into iterative white-box adversarial examples and also show that attacking ensembles of models improves transferability. Specifically, their contribution is twofold. First, some iterative white-box attacks are extended to include a momentum term. As in optimization or learning, the main motivation is to avoid local maxima and have faster convergence. In experiments, they show that momentum is able to increase the success rates of attacks.\n\nSecond, to improve the transferability of adversarial examples in black-box scenarios, Dong et al. propose to compute adversarial examples on ensembles of models. In particular, the logits of multiple models are summed (optionally using weights) and attacks are crafter to fool multiple models at once. In experiments, crafting adversarial examples on an ensemble of diverse networks allows higher success rate sin black-box scenarios.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00957"
    },
    "557": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1711-00851",
        "transcript": "Wong and Kolter propose a method for learning provably-robust, deep, ReLU based networks by considering the so-called adversarial polytope of final-layer activations reachable through adversarial examples. Overall, the proposed approach has some similarities to adversarial training in that the overall objective can be written as\n\n$\\min_\\theta \\sum_{i = 1}^N \\max_{\\|\\Delta\\|_\\infty \\leq  \\epsilon} L(f_\\theta(x_i + \\Delta), y_i)$.\n\nHowever, in contrast to previous work, the inner maximization problem (i.e. finding the optimal adversarial example to train on) can be avoided by considering the so-called \u201cdual network\u201d $g_\\theta$ (note that the parameters $\\theta$ are the same for $f$ and $g$):\n\n$\\min_\\theta \\sum_{i = 1}^N L(-J_\\epsilon(x_i, g_\\theta(e_{y_i}1^T \u2013 I)), y_i)$\n\nwhere $e_{y_i}$ is the one-hot vector of class $y_i$ and $\\epsilon$ the maximum perturbation allowed. Both the network $g_\\theta$ and the objective $J_\\epsilon$ are derive from the dual problem of a linear program corresponding to the adversarial perturbation objective.\n\nConsidering $z_k$ to be the activations of the final layer (e.g., the logits), a common objective for adversarial examples is\n\n$\\min_{\\Delta} z_k(x + \\Delta)_{y} \u2013 z_k(x + \\Delta)_{y_{\\text{target}}}$\n\nwith $x$ being a sample, and $y$ being true or target label. Wong and Kolter show that this can be rewritten as a linear program:\n\n$\\min_{\\Delta} c^T z_k(x + \\Delta)$.\n\nInstead of minimizing over the perturbation $\\Delta$, we can also optimize over the activations $z_k$ itself. We can even constrain the activations to a set $\\tilde{Z}_\\epsilon(x)$ around a sample $x$ in which we want the network\u2019s activations to be robust. In the paper, this set is obtained using a convex relaxation of the ReLU network, where it is assumed that upper on lowe rbounds of all activations can be computed efficiently. The corresponding dual problem then involves\n\n$\\max_\\alpha J_\\epsilon(x, g_\\theta(x, \\alpha))$\n\nwith $g_\\theta$ being the dual network. Details can be found in the paper; however, I wanted to illustrate the general idea. Because of the simple structure of the network, the dual network is almost idenitical to the true network and the required upper and lower bounds can be computed in a backward style computation.\n\nIn experiments, Wong and Kolter demonstrate that they can compute reasonable robustness guarantees for simple ReLu networks on MNIST. They also show that the classification boundaries of the learned networks are smoother; however, these experiments have only been conducted on simple 2D toy datasets (with significantly larger networks compared to MNIST).\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.00851"
    },
    "558": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1805-12152",
        "transcript": "Tsipras et al. investigate the trade-off between classification accuracy and adversarial robustness. In particular, on a very simple toy dataset, they proof that such a trade-off exists; this means that very accurate models will also have low robustness. Overall, on this dataset, they find that there exists a sweet-spot where the accuracy is 70% and the adversarial accuracy (i.e., accuracy on adversarial examples) is 70%. Using adversarial training to obtain robust networks, they additionally show that the robustness is increased by not using \u201cfragile\u201d features \u2013 features that are only weakly correlated with the actual classification tasks. Only focusing on few, but \u201crobust\u201d features also has the advantage of more interpretable gradients and sparser weights (or convolutional kernels). Due to the induced robustness, adversarial examples are perceptually significantly more different from the original examples, as illustrated in Figure 1 on MNIST.\n\nhttps://i.imgur.com/OP2TOOu.png\nFigure 1: Illustration of adversarial examples for a standard model, a model trained using $L_\\infty$ adversarial training and $L_2$ adversarial training. Especially for the $L_2$ case it is visible that adversarial examples need to change important class characteristics to fool the network.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1805.12152"
    },
    "559": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SchmidtSTTM18",
        "transcript": "Schmidt et al. theoretically and experimentally show that training adversarially robust models requires a higher sample complexity compared to regular generalization. Theoretically, they analyze two very simple families of datasets, e.g., consisting of two Gaussian distributions corresponding to a two-class problem. On such datasets, they proof that \u201crobust generalization\u201d, i.e., generalization to adversarial examples, required much higher sample complexity compared to regular generlization, i.e., generalization to the test set. These results are interesting because they suggest that the sample complexity might be even worse for more complex and realistic data distributions \u2013 as we commonly tackle in computer vision.\n\nExperimentally, they show similar result son MNIST, CIFAR-10 and SVHN. Varying the size of the training set and plotting the accuracy on adversarially computed examples results in Figure 1. As can be seen, there seems to be a clear advantage of having larger training sets. Note that these models were trained using adversarial training using an $L_\\infty$ adversary constrained by the given $\\epsilon$.\n\nhttps://i.imgur.com/SriBAt4.png\nFigure 1: Training set size plotted against the adversarial test accuracy on MNIST, CIFAR-10 and SVHN. The models were trained using adversarial training.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data"
    },
    "560": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1803-00940",
        "transcript": "Motivated by JPEG compression, Prakash et al. propose an adaptive quantization scheme as defense against adversarial attacks. They argue that JPEG experimentally reduces adversarial noise; however, it is difficult to automatically decide on the level of compression as it also influences a classifier\u2019s performance. Therefore, Prakash et al. use a saliency detector to identify background region, and then apply adaptive quantization \u2013 with coarser detail at the background \u2013 to reduce the impact of adversarial noise. In experiments, they demonstrate that this approach outperforms simple  JPEG compression as defense while having less impact on the image quality.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1803.00940"
    },
    "561": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1803-06373",
        "transcript": "Kannan et al. propose a defense against adversarial examples called adversarial logit pairing where the logits of clean and adversarial example are regularized to be similar. In particular, during adversarial training, they add a regularizer of the form\n\n$\\lambda L(f(x), f(x\u2019))$\n\nwere $L$ is, for example, the $L_2$ norm and $f(x\u2019)$ the logits corresponding to adversarial example $x\u2019$ (corresponding to clean example $x$). Intuitively, this is a very simple approach \u2013 adversarial training itself enforces the classification results of clean and corresponding adversarial examples to be the same and adversarial logit pairing enforces the internal representation, i.e., the logits, to be similar. In theory, this could also be applied to any set of activations within the network. In the paper, they conclude that\n\n\u201cWe hypothesize that adversarial logit pairing works well because it provides an additional prior that regularizes the model toward a more accurate understanding of the classes.\u201d\n\nIn experiments, they show that this approach slightly outperforms adversarial training alone on SVHN, MNIST as well as ImageNet.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1803.06373"
    },
    "562": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1802-07124",
        "transcript": "Abbasi and Gagn\u00e9 propose explicit but natural out-distribution training as defense against adversarial examples. Specifically, as also illustrated on the toy dataset in Figure 1, they argue that networks commonly produce high-confident predictions in regions that are clearly outside of the data manifold (i.e., the training data distribution). As mitigation strategy, the authors propose to explicitly train on out-of-distribution data, allowing the network to additionally classify this data as \u201cdustbin\u201d data. On MNIST, for example, this data comes from NotMNIST, a dataset of letters A-J \u2013 on CIFA-10, this data could be CIFAR-100. Experiments show that this out-of-distribution training allow networks to identify adversarial examples as \u201cdustbin\u201d and thus improve robustness.\n\nhttps://i.imgur.com/nUSDZay.png\nFigure 1: Illustration of a naive model versus an augmented model, i.e., trained on out-of-distribution data, on a toy dataset (left) and on MNIST (right).\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.07124"
    },
    "563": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/AkhtarLM18",
        "transcript": "Akhtar et al. Propose a rectification and detection scheme as defense against universal adversarial perturbations. Their overall approach is illustrated in Figure 1 an briefly summarized as follows. Given a classifier with fixed weights, a rectification network (the so-called perturbation rectifying network \u2013 PRN) is trained in order to \u201cundo\u201d the perturbations. This network can be trained on a set of clean and perturbed images using the classifier\u2019s loss. Second, based on the discrete cosine transform (DCT) of the difference between original and rectified image (both for clean and perturbed images), a SVM is trained to detect adversarially perturbed images. At test time, only images that have been identified as being perturbed are rectified. In experiments, the authors show that this setup is able to defend against adversarial attacks and does not influence the classifier\u2019s accuracy significantly.\n\nhttps://i.imgur.com/KzY7Wwr.png\nFigure 1: The proposed perturbation rectifying network (PRN) asnd the correcponding perturbation detector.\n\nOverall, the proposed approach is comparable to other work that tries to either detect adversarial perturbations, or to remove them from the test image. One advantage is that the classifier itself does not need to be re-trained. However, as the rectification network is itself a (convolutional) neural netowrk, and the detector is a SVM, both are also potential targets of attacks \u2013 althoguh attacking the whole system might be more challenging (especially crafting universal perturbations).\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00357"
    },
    "564": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1803-07994",
        "transcript": "Folz et al. propose an auto-encoder based defense against adversarial examples. In particular, they propose structure-to-signal auto-encoders, S2SNets, as defense mechanism \u2013 this auto-encoder is first trained in an unsupervised fashion to reconstruct images (which can be done independent of attack models or the classification network under attack). Then, the network\u2019s decoder is fine tuned using gradients from the classification network. Their main argumentation is that the gradients of the composite network \u2013 auto-encoder plus classification network \u2013 are not class specific anymore as only the decoder is fine-tuned but not the encoder (as the encoder is trained to encode any image independent of the classification task). Experimentally they show that the gradients are indeed less class-specific. Additionally, the authors highlight that this defense is independent of an attack model and can be applied to any pre-trained classification model. Unforutntely, the approach is not compared against other defense machenisms \u2013 while related work was mentioned, a comparison would have been useful.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1803.07994"
    },
    "565": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.02284",
        "transcript": "Huang et al. study adversarial attacks on reinforcement learning policies. One of the main problems, in contrast to supervised learning, is that there might not be a reward in any time step, meaning there is no clear objective to use. However, this is essential when crafting adversarial examples as they are mostly based on maximizing the training loss. To avoid this problem, Huang et al. assume a well-trained policy; the policy is expected to output a distribution over actions. Then, adversarial examples can be computed by maximizing the cross-entropy loss using the most-likely action as ground truth.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1702.02284"
    },
    "566": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1712.03141",
        "transcript": "Biggio and Roli provide a comprehensive survey and discussion of work in adversarial machine learning. In contrast to related work [1,2], they explicitly discuss the relation of recent developments regarding the security of deep neural networks (as primarily discussed in [1] and [2]) and adversarial machine learning in general. The latter can be traced back to early work starting in 2004, e.g. involving adversarial attacks on spam filters. As a result, terminology used by Biggio and Roli is slightly different compared to publications focusing on deep neural networks. However, it also turns out that many approaches recently discussed in the deep learning community (such as adversarial training as defense) has already been introduced earlier regarding other machine learning algorithms. They also give a concise discussion of different threat models that is worth reading.\n\n[1] N. Akhtar and A. Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. arXiv.org, abs/1801.00553, 2018.\n[2] X. Yuan, P. He, Q. Zhu, R. R. Bhat, and X. Li. Adversarial examples: Attacks and defenses for deep learning. arXiv.org, abs/1712.07107, 2017.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1712.03141"
    },
    "567": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1801.00553",
        "transcript": "Akhtar and Mian present a comprehensive survey of attacks and defenses of deep neural networks, specifically in computer vision. Published on ArXiv in January 2018, but probably written prior to August 2017, the survey includes recent attacks and defenses. For example, Table 1 presents an overview of attacks on deep neural networks \u2013 categorized by knowledge, target and perturbation measure. The authors also provide a strength measure \u2013 in the form of a 1-5 start \u201crating\u201d. Personally, however, I see this rating critically \u2013 many of the attacks have not been studies extensively (across a wide variety of defense mechanisms, tasks and datasets). In comparison to the related survey [1], their overview is slightly less detailed \u2013 the attacks, for example are described in less mathematical detail and the categorization in Table 1 is less comprehensive.\n\nhttps://i.imgur.com/cdAcivj.png\nTable 1: Overview of the discussed attacks on deep neural networks.\n\n[1] Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, Xiaolin Li:\nAdversarial Examples: Attacks and Defenses for Deep Learning. CoRR abs/1712.07107 (2017)\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1801.00553"
    },
    "568": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1712.07107",
        "transcript": "Yuan et al. present a comprehensive survey of attacks, defenses and studies regarding the robustness and security of deep neural networks. Published on ArXiv in December 2017, it includes most recent attacks and defenses. For examples, Table 1 lists all known attacks \u2013 Yuan et al. categorize the attacks according to the level of knowledge needed, targeted or non-targeted, the optimization needed (e.g. iterative) as well as the perturbation measure employed. As a result, Table 1 gives a solid overview of state-of-the-art attacks. Similarly, Table 2 gives an overview of applications reported so far. Only for defenses, a nice overview table is missing. Still, the authors discuss (as of my knowledge) all relevant defense strategies and comment on their performance reported in the literature.\n\nhttps://i.imgur.com/3KpoYWr.png\nTable 1: An overview of state-of-the-art attacks on deep neural networks.\n\nhttps://i.imgur.com/4eq6Tzm.png\nTable 2: An overview of application sof some of the attacks in Table 1.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1712.07107"
    },
    "569": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.01775",
        "transcript": "Rozsa et al. propose PASS, an perceptual similarity metric invariant to homographies to quantify adversarial perturbations. In particular, PASS is based on the structural similarity metric SSIM [1]; specifically\n\n$PASS(\\tilde{x}, x) = SSIM(\\psi(\\tilde{x},x), x)$\n\nwhere $\\psi(\\tilde{x}, x)$ transforms the perturbed image $\\tilde{x}$ to the image $x$ by applying a homography $H$ (which can be found through optimization). Based on this similarity metric, they consider additional attacks which create small perturbations in terms of the PASS score, but result in larger $L_p$ norms; see the paper for experimental results.\n\n[1] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004.\n\nAlso see this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.01775"
    },
    "570": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.07262",
        "transcript": "Bastani et al. propose formal robustness measures and an algorithm for approximating them for piece-wise linear networks. Specifically, the notion of robustness is similar to related work:\n\n$\\rho(f,x) = \\inf\\{\\epsilon \\geq 0 | f \\text{ is not } (x,\\epsilon)\\text{-robust}$\n\nwhere $(x,\\epsilon)$-robustness demands that for every $x'$ with $\\|x'-x\\|_\\infty$ it holds that $f(x') = f(x)$ \u2013 in other words, the label does not change for perturbations $\\eta = x'-x$ which are small in terms of the $L_\\infty$ norm and the constant $\\epsilon$. Clearly, a higher $\\epsilon$ implies a stronger notion of robustness. Additionally, the above definition is essentially a pointwise definition of robustness.\n\nIn order to measure robustness for the whole network (i.e. not only pointwise), the authors introduce the adversarial frequency:\n\n$\\psi(f,\\epsilon) = p_{x\\sim D}(\\rho(f,x) \\leq \\epsilon)$.\n\nThis measure measures how often $f$ failes to be robust in the sense of $(x,\\epsilon)$-robustness. The network is more robust when it has low adversarial frequency. Additionally, they introduce adversarial severity:\n\n$\\mu(f,\\epsilon) = \\mathbb{E}_{x\\sim D}[\\rho(f,x) | \\rho(f,x) \\leq \\epsilon]$\n\nwhich measures how severly $f$ fails to be robust (if it fails to be robust for a sample $x$).\n\nBoth above measures can be approximated by counting given that the robustness $\\rho(f, x)$ is known for all samples $x$ in a separate test set. And this is the problem of the proposed measures: in order to approximate $\\rho(f, x)$, the authors propose an optimization-based approach assuming that the neural network is piece-wise linear. This assumption is not necessarily unrealistic, dot products, convolutions, $\\text{ReLU}$ activations and max pooling are all piece-wise linear. Even batch normalization is piece-wise linear at test time. The problem, however, is that th enetwork needs to be encoded in terms of linear programs, which I believe is cumbersome for real-world networks.\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.07262"
    },
    "571": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.10925",
        "transcript": "Ulyanov et al. utilize untrained neural networks as regularizer/prior for various image restoration tasks such as denoising, inpainting and super-resolution. In particualr, the standard formulation of such tasks, i.e.\n\n$x^\\ast = \\arg\\min_x E(x, x_0) + R(x)$\n\nwhere $x_0$ is the input image and $E$ a task-dependent data term, is rephrased as follows:\n\n$\\theta^\\ast = \\arg\\min_\\theta E(f_\\theta(z); x_0)$ and $x^\\ast = f_{\\theta^\\ast}(z)$\n\nfor a fixed but random $z$. Here, the regularizer $R$ is essentially replaced by an untrained neural network $f_\\theta$ \u2013 usually in the form of a convolutional encoder. The authors argue that the regualizer is effectively $R(x) = 0$ if the image can be generated by the encoder from the fixed code $z$ and $R(x) = \\infty$ if not. However, this argument does not necessarily provide any insights on why this approach works (as demonstrated in the paper).\n\nA main question addressed in the paper is why the network $f_\\theta$ can be used as a prior \u2013 regarding the assumption that high-capacity networks can essentially fit any image (including random noise). In my opinion, the authors do not give a convincing answer to this question. Essentially, they argue that random noise is just harder to fit (i.e. it takes longer). Therefore, limiting the number of iterations is enough as regularization. Personally I would argue that this observation is mainly due to prior knowledge put into the encoder architecture and the idea that natural images (or any images with some structure) are easily embedded into low-dimensional latent spaced compared to fully I.i.d. random noise.\n\nThey provide experiments on a range of tasks including denoising, image inpainting, super-resolution and neural network \u201cinversion\u201d. Figure 1 shows some results for image inpainting that I found quite convincing. For the remaining experiments I refer to the paper.\n\nhttps://i.imgur.com/BVQsaup.png\nFigure 1: Qualitative results for image inpainting.\n\nAlso see this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.10925"
    },
    "572": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1801.02774",
        "transcript": "Gilmer et al. study the existence of adversarial examples on a synthetic toy datasets consisting of two concentric spheres. The dataset is created by randomly sampling examples from two concentric spheres, one with radius $1$ and one with radius $R = 1.3$. While the authors argue that difference difficulties of the dataset can be created by varying $R$ and the dimensionality, they merely experiment with $R = 1.3$ and a dimensionality of $500$. The motivation to study this dataset comes form the idea that adversarial examples can easily be found by leaving the data manifold. Based on this simple dataset, the authors provide several theoretical insights \u2013 see the paper for details.\n\nBeneath theoretical insights, Gilmer et al. slso discuss the so-called manifold attack, an attack using projected gradient descent which ensures that the adversarial examples stays on the data-manifold \u2013 moreover, it is ensured that the class does not change. Unfortunately (as I can tell), this idea of a manifold attack is not studied further \u2013 which is very unfortunate and renders the question while this concept was introduced in the first place.\n\nOne of the main take-aways is the suggestion that there is a trade-off between accuracy (i.e. the ability of the network to perform well) and the average distance to an adversarial example. Thus, the existence of adversarial examples might be related to the question why deep neural networks perform very well.\n\nAlso see this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1801.02774"
    },
    "573": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1608.08967",
        "transcript": "Fawzi et al. study robustness in the transition from random samples to semi-random and adversarial  samples. Specifically they present bounds relating the norm of an adversarial perturbation to the norm of random perturbations \u2013 for the exact form I refer to the paper. Personally, I find the definition of semi-random noise most interesting, as it allows to get an intuition for distinguishing random noise from adversarial examples. As in related literature, adversarial examples are defined as\n\n$r_S(x_0) = \\arg\\min_{x_0 \\in S} \\|r\\|_2$ s.t. $f(x_0 + r) \\neq f(x_0)$\n\nwhere $f$ is the classifier to attack and $S$ the set of allowed perturbations (e.g. requiring that the perturbed samples are still images). If $S$ is mostly unconstrained regarding the direction of $r$ in high dimensional space, Fawzi et al. consider $r$ to be an adversarial examples \u2013 intuitively, and adversary can choose $r$ arbitrarily to fool the classifier. If, however, the directions considered in $S$ are constrained to an $m$-dimensional subspace, Fawzi et al. consider $r$ to be semi-random noise. In the extreme case, if $m = 1$, $r$ is random noise. In this case, we can intuitively think of $S$ as a randomly chosen one dimensional subspace \u2013 i.e. a random direction in multi-dimensional space.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1608.08967"
    },
    "574": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1608.07690",
        "transcript": "Tanay and Griffin introduce the boundary tilting perspective as alternative to the \u201clinear explanation\u201d for adversarial examples. Specifically, they argue that it is not reasonable to assume that the linearity in deep neural networks causes the existence of adversarial examples. Originally, Goodfellow et al. [1] explained the impact of adversarial examples by considering a linear classifier:\n\n$w^T x' = w^Tx + w^T\\eta$\n\nwhere $\\eta$ is the adversarial perturbations. In large dimensions, the second term might result in a significant shift of the neuron's activation. Tanay and Griffin, in contrast, argue that the dimensionality does not have an impact; althought he impact of $w^T\\eta$ grows with the dimensionality, so does $w^Tx$, such that the ratio should be preserved. Additionally, they showed (by giving a counter-example) that linearity is not sufficient for the existence of adversarial examples.\n\nInstead, they offer a different perspective on the existence of adversarial examples that is, in the course of the paper, formalized. Their main idea is that the training samples live on a manifold in the actual input space. The claim is, that on the manifold there are no adversarial examples (meaning that the classes are well separated on the manifold and it is hard to find adversarial examples for most training samples). However, the decision boundary extends beyond the manifold and might lie close to the manifold such that adversarial examples leaving the manifold can be found easily. This idea is illustrated in Figure 1.\n\nhttps://i.imgur.com/SrviKgm.png\nFigure 1: Illustration of the underlying idea of the boundary tilting perspective, see the text for details.\n\n[1] Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy:\nExplaining and Harnessing Adversarial Examples. CoRR abs/1412.6572 (2014)\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1608.07690"
    },
    "575": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1801.09344",
        "transcript": "Raghunathan et al. provide an upper bound on the adversarial loss of two-layer networks and also derive a regularization method to minimize this upper bound. In particular, the authors consider the scoring functions $f^i(x) = V_i^T\\sigma(Wx)$ with bounded derivative $\\sigma'(z) \\in [0,1]$ which holds for Sigmoid and ReLU activation functions. Still, the model is very constrained considering recent, well-performng deep (convolutional) neural networks. The upper bound is then derived by considering $f(A(x))$ where $A(x)$ is the optimal attacker $A(x) = \\arg\\max_{\\tilde{x} \\in B_\\epsilon(x)} f(\\tilde{x})$. For a linear model $f(x) = (W_1 \u2013 W_2)^Tx$, an upper bound can be derived as follows:\n\n$f(\\tilde{x}) = f(x) + (W_1 \u2013 W_2)^T(\\tilde{x} \u2013 x) \\leq f(x) + \\epsilon\\|W_1 \u2013 W_2\\|_1$.\n\nFor two-layer networks a bound is derived by considering\n\n$f(\\tilde{x}) = f(x) + \\int_0^1 \\nabla f(t\\tilde{x} + (1-t)x)^T (\\tilde{x} \u2013 x) dt \\leq f(x) + \\max_{\\tilde{x}\\in B_\\epsilon(x)} \\epsilon\\|\\nabla f(\\tilde{x})\\|_1$.\n\nIn this case, Raghunathan rewrite the second term, i.e. $\\max_{\\tilde{x}\\in B_\\epsilon(x)} \\epsilon\\|\\nabla f(\\tilde{x})\\|_1$ to derive an upper bound in the form of a semidefinite program, see the paper for details. For $v = V_1 \u2013 V_2$, this semidefinite program is based on the matrix\n\n$M(v,W) = \\left[\\begin{array}0 & 0 & 1^T W^R \\text{diag}(v)\\\\0 & 0 & W^T\\text{diag}(v)\\\\ \\text{diag}(v)^T W 1 & \\text{diag}(v)^T W & 0\\end{array}\\right]$.\n\nBy deriving the dual objective, the upper bound can then be minimized by constraining the eigenvalues of $M(v, W)$ (specifically, the largest eigenvalue; note that the dual also involves dual variables \u2013 see the paper for details). Overall, the proposed regularize involves minimizing the largest eigenvalue of $M(v, W) \u2013 D$ where $D$ is a diagonal matrix based on the dual variables. In practice, this is implemented using SciPy's implementation of the Lanczos algorithm.\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1801.09344"
    },
    "576": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/CisseBGDU17",
        "transcript": "Cisse et al. propose parseval networks, deep neural networks regularized to learn orthonormal weight matrices. Similar to the work by Hein et al. [1], the mean idea is to constrain the Lipschitz constant of the network \u2013 which essentially means constraining the Lipschitz constants of each layer independently. For weight matrices, this can be achieved by constraining the matrix-norm. However, this (depending on the norm used) is often intractable during gradient descent training. Therefore, Cisse et al. propose to use a per-layer regularizer of the form:\n\n$R(W) = \\|W^TW \u2013 I\\|$\n\nwhere $I$ is the identity matrix. During training, this regularizer is supposed to ensure that the learned weigth matrices are orthonormal \u2013 an efficient alternative to regular matrix manifold optimization techniques (see the paper).\n\n[1] Matthias Hein, Maksym Andriushchenko: Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation. CoRR abs/1705.08475 (2017)\n\nAlso see this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v70/cisse17a.html"
    },
    "577": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1801.02613",
        "transcript": "Ma et al. detect adversarial examples based on their estimated intrinsic dimensionality. I want to note that this work is also similar to [1] \u2013 in both publications, local intrinsic dimensionality is used to analyze adversarial examples. Specifically, the intrinsic dimensionality of a sample is estimated based on the radii $r_i(x)$ of the $k$-nearest neighbors around a sample $x$:\n\n$- \\left(\\frac{1}{k} \\sum_{i = 1}^k \\log \\frac{r_i(x)}{r_k(x)}\\right)^{-1}$.\n\nFor details regarding the original, theoretical formulation of local intrinsic dimensionality I refer to the paper. In experiments, the authors show that adversarial examples exhibit a significant higher intrinsic dimensionality than training samples or randomly perturbed examples. This observation allows detection of adversarial examples. A proper interpretation of this finding is, however, missing. It would be interesting to investigate what this finding implies about the properties of adversarial examples.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1801.02613"
    },
    "578": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1703.00410",
        "transcript": "Feinman et al. use dropout to compute an uncertainty measure that helps to identify adversarial examples. Their so-called Bayesian Neural Network Uncertainty is computed as follows:\n\n$\\frac{1}{T} \\sum_{i=1}^T \\hat{y}_i^T \\hat{y}_i - \\left(\\sum_{i=1}^T \\hat{y}_i\\right)\\left(\\sum_{i=1}^T \\hat{y}_i\\right)$\n\nwhere $\\{\\hat{y}_1,\\ldots,\\hat{y}_T\\}$ is a set of stochastic predictions (i.e. predictions with different noise patterns in the dropout layers). Here, is can easily be seen that this measure corresponds to a variance computatin where the first term is correlation and the second term is the product of expectations. In Figure 1, the authors illustrate the distributions of this uncertainty measure for regular training samples, adversarial samples and noisy samples for two attacks (BIM and JSMA, see paper for details).\n\nhttps://i.imgur.com/kTWTHb5.png\nFigure 1: Uncertainty distributions for two attacks (BIM and JSMA, see paper for details) and normal samples, adversarial samples and noisy samples.\n\nAlso see this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1703.00410"
    },
    "579": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1705.07263",
        "transcript": "Carlini and Wagner study the effectiveness of adversarial example detectors as defense strategy and show that most of them can by bypassed easily by known attacks. Specifically, they consider a set of adversarial example detection schemes, including neural networks as detectors and statistical tests. After extensive experiments, the authors provide a set of lessons which include:\n- Randomization is by far the most effective defense (e.g. dropout).\n- Defenses seem to be dataset-specific. There is a discrepancy between defenses working well on MNIST and on CIFAR.\n- Detection neural networks can easily be bypassed.\nAdditionally, they provide a set of recommendations for future work:\n- For developing defense mechanism, we always need to consider strong white-box attacks (i.e. attackers that are informed about the defense mechanisms).\n- Reporting accuracy only is not meaningful; instead, false positives and negatives should be reported.\n- Simple datasets such as MNIST and CIFAR are not enough for evaluation.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1705.07263"
    },
    "580": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.06280",
        "transcript": "Grosse et al. use statistical tests to detect adversarial examples; additionally, machine learning algorithms are adapted to detect adversarial examples on-the-fly of performing classification. The idea of using statistics tests to detect adversarial examples is simple: assuming that there is a true data distribution, a machine learning algorithm can only approximate this distribution \u2013 i.e. each algorithm \u201clearns\u201d an approximate distribution. The ideal adversary uses this discrepancy to draw a sample from the data distribution where data distribution and learned distribution differ \u2013 resulting in mis-classification. In practice, they show that kernel-based two-sample statistics hypothesis testing can be used to identify a set of adversarial examples (but not individual one). In order to also detect individual ones, each classifier is augmented to also detect whether the input is an adversarial example. This approach is similar to adversarial training, where adversarial examples are included in the training set with the correct label. However, I believe that it is possible to again craft new examples to the augmented classifier \u2013 as is also possible with adversarial training.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1702.06280v2"
    },
    "581": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.09404",
        "transcript": "Ross and Doshi-Velez propose input gradient regularization to improve robustness and interpretability of neural networks. As the discussion of interpretability is quite limited in the paper, the main contribution is an extensive evaluation of input gradient regularization against adversarial examples \u2013 in comparison to defenses such as distillation or adversarial training. Specifically, input regularization as proposed in [1] is used:\n\n$\\arg\\min_\\theta H(y,\\hat{y}) + \\lambda \\|\\nabla_x H(y,\\hat{y})\\|_2^2$\n\nwhere $\\theta$ are the network\u2019s parameters, $x$ its input and $\\hat{y}$ the predicted output. Here, $H$ might be a cross-entropy loss. It also becomes apparent why this regularization was originally called double-backpropagation because the second derivative is necessary during training.\nIn experiments, the authors show that the proposed regularization is superior to many other defenses including distillation and adversarial training. Unfortunately, the comparison does not include other \u201cregularization\u201d techniques to improve robustness \u2013 such as Lipschitz regularization. This makes the comparison less interpretable, especially as the combination of input gradient regularization and adversarial training performs best (suggesting that adversarial training is a meaningful defense, as well). Still, I recommend a closer look on the experiments. For example, the authors also study the input gradients of defended models, leading to some interesting conclusions.\n\n[1] H. Drucket, Y. LeCun. Improving generalization performance using double backpropagation. IEEE Transactions on Neural Networks, 1992.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.09404"
    },
    "582": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HeinA17",
        "transcript": "Hein and Andriushchenko give a intuitive bound on the robustness of neural networks based on the local Lipschitz constant. With robustness, the authors refer a small $\\epsilon$-ball around each sample; this ball is supposed to describe the region where the neural network predicts a constant class. This means that adversarial examples have to compute changes large enough to leave these robust areas. Larger $\\epsilon$-balls imply higher robustness to adversarial examples.\n\nWhen considering a single example $x$, and a classifier $f = (f_1, \\ldots, f_K)^T$ (i.e. in a multi-class setting), the bound can be stated as follows. For $q$ and $p$ such that $\\frac{1}{q} + \\frac{1}{p} = 1$ and $c$ being the class predicted for $x$, the it holds\n\n$x = \\arg\\max_j f_j(x + \\delta)$\n\nfor all $\\delta$ with\n\n$\\|\\delta\\|_p \\leq \\max_{R > 0}\\min \\left\\{\\min_{j \\neq c} \\frac{f_c(x) \u2013 f_j(x)}{\\max_{y \\in B_p(x, R)} \\|\\nabla f_c(y) - \\nabla f_j(y)\\|_q}, R\\right\\}$.\n\nHere, $B_p(x, R)$ describes the $R$-ball around $x$ measured using the $p$-norm. Based on the local Lipschitz constant (in the denominator), the bound essentially measures how far we can deviate from the sample $x$ (measured in the $p$-norm) until $f_j(x) > f_c(x)$ for some $j \\neq c$. The higher the local Lipschitz constant, the smaller deviations are allowed, i.e. adversarial examples are easier to find. Note that the bound also depends on the confidence, i.e. the edge  $f_c(x)$ has in comparison to all other $f_j(x)$.\n\nIn the remaining paper, the authors also provide bounds for simple classifiers including linear classifiers, kernel methods and two-layer perceptrons (i.e. one hidden layer). For the latter, they also propose a new type of regularization called cross-Lipschitz regularization:\n\n$P(f) = \\frac{1}{nK^2} \\sum_{i = 1}^n \\sum_{l,m = 1}^K \\|\\nabla f_l(x_i) - \\nabla f_m(x_i)\\|_2^2$.\n\nThis regularization term is intended to reduce the Lipschitz constant locally around training examples. They show experimental results using this regularization on MNIST and CIFAR, see the paper for details.\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6821-formal-guarantees-on-the-robustness-of-a-classifier-against-adversarial-manipulation"
    },
    "583": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.01421",
        "transcript": "Simon-Gabriel et al. Study the robustness of neural networks with respect to the input dimensionality. Their main hypothesis is that the vulnerability of neural networks against adversarial perturbations increases with the input dimensionality. To support this hypothesis, they provide a theoretical analysis as well as experiments.\n\nThe general idea of robustness is that small perturbations $\\delta$ of the input $x$ do only result in small variations $\\delta \\mathcal{L}$ of the loss:\n\n$\\delta \\mathcal{L} = \\max_{\\|\\delta\\| \\leq \\epsilon} |\\mathcal{L}(x + \\delta) - \\mathcal{L}(x)| \\approx \\max_{\\|\\delta\\| \\leq \\epsilon} |\\partial_x \\mathcal{L} \\cdot \\delta| = \\epsilon \\||\\partial_x \\mathcal{L}\\||$\n\nwhere the approximation is due to a first-order Taylor expansion and $\\||\\cdot\\||$ is the dual norm of $\\|\\cdot\\|$. As a result, the vulnerability of networks can be quantified by considering $\\epsilon\\mathbb{E}_x\\||\\partial_x \\mathcal{L}\\||$. A natural regularizer to increase robustness (i.e. decrease vulnerability) would be $\\epsilon \\||\\partial_x \\mathcal{L}\\||$ which is a similar regularizer as proposed in [1].\n\nThe remainder of the paper studies the norm $\\|\\partial_x \\mathcal{L}\\|$ with respect to the input dimension $d$. Specifically, they show that the gradient norm increases monotonically with the input dimension. I refer to the paper for the exact theorems and proofs. This claim is based on the assumption of non-trained networks that have merely been initialized. However, in experiments, they show that the conclusion may hold true in realistic settings, e.g. on ImageNet.\n\n[1] Matthias Hein, Maksym Andriushchenko:\nFormal Guarantees on the Robustness of a Classifier against Adversarial Manipulation. NIPS 2017: 2263-2273\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.01421"
    },
    "584": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1703.09202",
        "transcript": "Nayebi and Ganguli propose saturating neural networks as defense against adversarial examples. The main observation driving this paper can be stated as follows: Neural networks are essentially based on linear sums of neurons (e.g. fully connected layers, convolutiona layers) which are then activated; by injecting a small amount of noise per neuron it is possible to shift the final sum by large values, thereby propagating the noisy through the network and fooling the network into misclassifying an example. To prevent the impact of these adversarial examples, the network should be trained in a manner to drive many neurons into a saturated regime \u2013 noisy will, so the argument, have less impact then. The authors also give a biological motivation, which I won't go into detail here.\n\nLetting $\\psi$ be the used activation function, e.g. sigmoid or ReLU, a regularizer is added to drive neurons into saturation. In particular, a penalty\n$\\lambda \\sum_l \\sum_i \\psi_c(h_i^l)$\nis added to the loss. Here, $l$ indexes the layer and $i$ the unit in the layer; $h_i^l$ then describes the input to the non-linearity computed for unit $i$ in layer $l$. $\\psi_c$ is the complementary function defined as\n$\\psi_c(z) = \\inf_{z': \\psi'(z') = 0} |z \u2013 z'|$\nIt defines the distance of the point $z$ to the nearest saturated point $z'$ where $\\psi'(z') = 0$. For ReLU activations, the complementary function is the ReLU function itself; for sigmoid activations, the complementary function is\n$\\sigma_c(z) = |\\sigma(z)(1 - \\sigma(z))|$.\nIn experiments, Nayebi and Ganguli show that training with the additional penalty yields networks with higher robustness against adversarial examples compared to adversarial training (i.e. training on adversarial examples). They also provide some insight, showing e.g. the activation and weight distribution of layers illustrating that neurons are indeed saturated in large parts. For details, see the paper.\n\nI also want to point to a comment on the paper written by Brendel and Bethge [1] questioning the effectiveness of the proposed defense strategy. They discuss a variant of the fast sign gradient method (FSGM) with stabilized gradients which is able to fool saturated networks.\n\n[1] W. Brendel, M. Behtge. Comment on \u201cBiologically inspired protection of deep networks from adversarial attacks\u201d, https://arxiv.org/abs/1704.01547.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1703.09202"
    },
    "585": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1704.01155",
        "transcript": "Xu et al. propose feature squeezing for detecting and defending against adversarial examples. In particular, they consider \u201csqueezing\u201d the bit depth of the input images as well as local and non-local smoothing (Gaussian, median filtering etc.). In experiments they show that feature squeezing preserves accuracy while defending against adversarial examples. Figure 1 additionally shows an illustration of how feature squeezing can be used to detect adversarial examples.\n\nhttps://i.imgur.com/Ixv522J.png\nFigure 1: Illustration of using squeezing for adversarial example detection.\n\nAlso find this summary on [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1704.01155"
    },
    "586": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1101/262501",
        "transcript": "Lee et al. propose a variant of adversarial training where a generator is trained simultaneously to generated adversarial perturbations. This approach follows the idea that it is possible to \u201clearn\u201d how to generate adversarial perturbations (as in [1]). In this case, the authors use the gradient of the classifier with respect to the input as hint for the generator. Both generator and classifier are then trained in an adversarial setting (analogously to generative adversarial networks), see the paper for details.\n\n[1] Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie. Generative Adversarial Perturbations. ArXiv, abs/1712.02328, 2017.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1101/262501"
    },
    "587": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.10571",
        "transcript": "Sinha et al. introduce a variant of adversarial training based on distributional robust optimization. I strongly recommend reading the paper for understanding the introduced theoretical framework. The authors also provide guarantees on the obtained adversarial loss \u2013 and show experimentally that this guarantee is a realistic indicator. The adversarial training variant itself follows the general strategy of training on adversarially perturbed training samples in a min-max framework. In each iteration, an attacker crafts an adversarial examples which the network is trained on. In a nutshell, their approach differs from previous ones (apart from the theoretical framework) in the used attacker. Specifically, their attacker optimizes\n\n$\\arg\\max_z l(\\theta, z) - \\gamma \\|z \u2013 z^t\\|_p^2$\n\nwhere $z^t$ is a training sample chosen randomly during training.\n\nOn a side note, I also recommend reading the reviews of this paper: https://openreview.net/forum?id=Hk6kPgZA-\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1710.10571"
    },
    "588": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.05432",
        "transcript": "Shaham et al. provide an interpretation of adversarial training in the context of robust optimization. In particular, adversarial training is posed as min-max problem (similar to other related work, as I found):\n\n$\\min_\\theta \\sum_i \\max_{r \\in U_i} J(\\theta, x_i + r, y_i)$\n\nwhere $U_i$ is called the uncertainty set corresponding to sample $x_i$ \u2013 in the context of adversarial examples, this might be an $\\epsilon$-ball around the sample quantifying the maximum perturbation allowed; $(x_i, y_i)$ are training samples, $\\theta$ the parameters and $J$ the trianing objective. In practice, when the overall minimization problem is tackled using gradient descent, the inner maximization problem cannot be solved exactly (as this would be inefficient). Instead Shaham et al. Propose to alternatingly make single steps both for the minimization and the maximization problems \u2013 in the spirit of generative adversarial network training.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.05432"
    },
    "589": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.03034",
        "transcript": "Huang et al. propose a variant of adversarial training called \u201clearning with a strong adversary\u201d. In spirit the idea is also similar to related work [1]. In particular, the authors consider the min-max objective\n\n$\\min_g \\sum_i \\max_{\\|r^{(i)}\\|\\leq c} l(g(x_i + r^{(i)}), y_i)$\n\nwhere $g$ ranges over expressible functions and $(x_i, y_i)$ is a training sample. In the remainder of the paper, Huang et al. Address the problem of efficiently computing $r^{(i)}$ \u2013 i.e. a strong adversarial example based on the current state of the network \u2013 and subsequently updating the weights of the network by computing the gradient of the augmented loss. Details can be found in the paper.\n\n[1] T. Miyato, S. Maeda, M. Koyama, K. Nakae, S. Ishii. Distributional Smoothing by Virtual Adversarial Training. ArXiv:1507.00677, 2015.\n\nAlso see this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.03034"
    },
    "590": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1507.00677",
        "transcript": "Miyato et al. propose distributional smoothing (or virtual adversarial training) as defense against adversarial examples. However, I think that both terms do not give a good intuition of what is actually done. Essentially, a regularization term is introduced. Letting $p(y|x,\\theta)$ be the learned model, the regularizer is expressed as\n\n$\\text{KL}(p(y|x,\\theta)|p(y|x+r,\\theta)$\n\nwhere $r$ is the perturbation that maximizes the Kullback-Leibler divergence above, i.e.\n\n$r = \\arg\\max_r \\{\\text{KL}(p(y|x,\\theta)|p(y|x+r,\\theta) | \\|r\\|_2 \\leq \\epsilon\\}$\n\nwith hyper-parameter $\\epsilon$. Essentially, the regularizer is supposed to \u201csimulate\u201d adversarial training \u2013 thus, the method is also called virtual adversarial training.\n\nThe discussed implementation, however, is somewhat cumbersome. In particular, $r$ cannot be computed using first-order methods as the gradient of $\\text{KL}$ is $0$ for $r = 0$. So a second-order method is used \u2013 for which the Hessian needs to be approximated and the corresponding eigenvectors need to be computed. For me it is unclear why $r$ cannot be initialized randomly to solve this issue \u2026 Then, the derivative of the regularizer needs to be computed during training. Here, the authors make several simplifications (such as fixing $\\theta$ in the first part of the Kullback-Leibler divergence and ignoring the derivative of $r$ w.r.t. $\\theta$).\n\nOverall, however, I like the idea of \u201cvirtual\u201d adversarial training as it avoids the need of explicitly using attacks during training to craft adversarial examples. Then, the trained model is often robust against the chosen attacks, but new adversarial examples can be found easily through novel attacks.\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1507.00677"
    },
    "591": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1707.06728",
        "transcript": "Zantedschi et al. propose Gaussian data augmentation in conjunction with bounded $\\text{ReLU}$ activations as defense strategy against adversarial examples. Here, Gaussian data augmentation refers to the practice of adding Gaussian noise to the input during training.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1707.06728"
    },
    "592": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1602.02389",
        "transcript": "Zahavy et al. introduce the concept of ensemble robustness and show that it can be used as indicator for generalization performance. In particular, the main idea is to lift he concept of robustness against adversarial examples to ensemble of networks \u2013 as trained, e.g. through Dropout or Bayes-by-Backprop. Letting $Z$ denote the sample set, a learning algorithm is $(K, \\epsilon)$ robust if $Z$ can be divided into $K$ disjoint sets $C_1,\\ldots,C_K$ such that for every training set $s_1,\\ldots,s_n  \\in Z$ it holds:\n\n$\\forall i, \\forall z \\in Z, \\forall k = 1,\\ldots, K$: if $s,z \\in C_k$, then $l(f,s_i) \u2013 l(f,z)| \\leq \\epsilon(s_1,\\ldots,s_n)$\n\nwhere $f$ is the model produced by the learning algorithm, $l$ measures the loss and $\\epsilon:Z^n \\mapsto \\mathbb{R}$. For ensembles (explicit or implicit) this definition is extended by considering the maximum generalization loss under the expectation of a randomized learning algorithm:\n\n$\\forall i, \\forall k = 1,\\ldots,K$: if $s \\in C_k$, then $\\mathbb{E}_f \\max_{z \\in C_k} |l(f,s_i) \u2013 l(f,z)| \\leq \\epsilon(s_1,\\ldots,s_n)$\n\nHere, the randomized learning algorithm computes a distribution over models given a training set.\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1602.02389"
    },
    "593": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1712.00673",
        "transcript": "Liu et al. propose randomizing neural networks, implicitly learning an ensemble of models, to defend against adversarial attacks. In particular, they introduce Gaussian noise layers before regular convolutional layers. The noise can be seen as additional parameter of the model. During training, noise is randomly added. During testing, the model is evaluated on a single testing input using multiple random noise vectors; this essentially corresponds to an ensemble of different models (parameterized by the different noise vectors).\n\nMathemtically, the authors provide two interesting interpretations. First, they argue that training essentially minimizes an upper bound of the (noisy) inference loss. Second, they show that their approach is equivalent to Lipschitz regularization [1].\n\n[1] M. Hein, M. Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. ArXiv:1705.08475, 2017.\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1712.00673"
    },
    "594": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.01768",
        "transcript": "Oh et al. propose two different approaches for whitening black box neural networks, i.e. predicting details of their internals such as architecture or training procedure. In particular, they consider attributes regarding architecture (activation function, dropout, max pooling, kernel size of convolutional layers, number of convolutionaly/fully connected layers etc.), attributes concerning optimization (batch size and optimization algorithm) and attributes regarding the data (data split and size). In order to create a dataset of models, they trained roughly 11k models on MNIST; they ensured that these models have at least 98% accuracy on the validation set and they also consider ensembles.\n\nFor predicting model attributes, they propose two models, called kennen-o and kennen-i, see Figure 1. Kennen-o takes as input a set of $100$ predictions of the models (i.e. final probability distributions) and tries to directly learn the attributes using a MLP of two fully connected layers. Kennen-i instead crafts a single input which allows to reason about a specific model attribute. An example for kennen-i is shown in Figure 2. In experiments, they demonstrate that both models are able to predict model attributes significantly better than chance. For details, I refer to the paper.\n\nhttps://i.imgur.com/YbFuniu.png\nFigure 1: Illustration of the two proposed approaches, kennen-o (top) and kennen-i (bottom).\n\nhttps://i.imgur.com/ZXj22zG.png\nFigure 2: Illustration of the images created by kennen-i to classify different attributes. See the paper for details.\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.01768"
    },
    "595": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1704.01547",
        "transcript": "Brendel et al. propose a decision-based black-box attacks against (deep convolutional) neural networks. Specifically, the so-called Boundary Attack starts with a random adversarial example (i.e. random noise that is not classified as the image to be attacked) and randomly perturbs this initialization to move closer to the target image while remaining misclassified. In pseudo code, the algorithm is described in Algorithm 1. Key component is the proposal distribution $P$ used to guide the adversarial perturbation in each step. In practice, they use a maximum-entropy distribution (e.g. uniform) with a couple of constraints: the perturbed sample is a valid image; the perturbation has a specified relative size, i.e. $\\|\\eta^k\\|_2 = \\delta d(o, \\tilde{o}^{k-1})$; and the perturbation reduces the distance to the target image $o$: $d(o, \\tilde{o}^{k-1}) \u2013 d(o,\\tilde{o}^{k-1} + \\eta^k)=\\epsilon d(o, \\tilde{o}^{k-1})$. This is approximated by sampling from a standard Gaussian, clipping and rescaling and projecting the perturbation onto the $\\epsilon$-sphere around the image. In experiments, they show that this attack is competitive to white-box attacks and can attack real-world systems.\n\nhttps://i.imgur.com/BmzhiFP.png\nAlgorithm 1: Minimal pseudo code version of the boundary attack.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1704.01547"
    },
    "596": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1708.03999",
        "transcript": "Chen et al. propose a gradient-based black-box attack to compute adversarial examples. Specifically, they follow the general idea of [1] where the following objective is optimized:\n\n$\\min_x \\|x \u2013 x_0\\|_2 + c \\max\\{\\max_{i\\neq t}\\{z_i\\} \u2013 z_t, - \\kappa\\}$.\n\nHere, $x$ is the adversarial example based on training sample $x_0$. The second part expresses that $x$ is supposed to be misclassified, i.e. the logit $z_i$ for some $i \\neq t$ distinct form the true label $t$ is supposed to be larger that the logit $z_t$ corresponding to the true label. This is optimized subject to the constraint that $x$ is a valid image.\n\nThe attack proposed in [1] assumes a white-box setting were we have access to the logits and the gradients (basically requiring access to the full model). Chen et al., in contrast want to design a black-box attacks. Therefore, they make the following changes:\n- Instead of using logits $z_i$, the probability distribution $f_i$ (i.e. the actual output of the network) is used.\n- Gradients are approximated by finite differences.\nPersonally, I find that the first point does violate a strict black-box setting. As company, for example, I would prefer not to give away the full probability distribution but just the final decision (or the decision plus a confidence score). Then, however, the proposed method is not applicable anymore. Anyway, the changed objective looks as follows: $\\min_x \\|x \u2013 x_0\\|_2 + c \\max\\{\\max_{i\\neq t}\\{\\log f_i\\} \u2013 \\log f_t, - \\kappa\\}$ where, according to the authors, the logarithm is essential for optimization. One remaining problem is efficient optimization with finite differences. To this end, they propose a randomized/stochastic coordinate descent algorithm. In particular, in each step, a ranodm pixel is chosen and a local update is performed by calculating the gradient on this pixel using finite differences and performing an ADAM step.\n\n[1] N. Carlini, D. Wagner. Towards evaluating the robustness of neural networks. IEEE Symposium of Security and Privacy, 2017.\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1708.03999"
    },
    "597": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1708.01697",
        "transcript": "Rozsa et al. describe an adersarial attack against OpenMax [1] by directly targeting the logits. Specifically, they assume a network using OpenMax instead of a SoftMax layer to compute the final class probabilities. OpenMax allows \u201copen-set\u201d networks by also allowing to reject input samples. By directly targeting the logits of the trained network, i.e. iteratively pushing the logits in a target direction, it does not matter whether SoftMax or OpenMax layers are used on top, the network can be fooled in both cases.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1708.01697"
    },
    "598": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.07528",
        "transcript": "Papernot et al. Introduce a novel attack on deep networks based on so-called adversarial saliency maps that are computed independently of a loss. Specifically, they consider \u2013 for a given network $F(X)$ \u2013 the forward derivative\n\n$\\nabla F = \\frac{\\partial F}{\\partial X} = \\left[\\frac{\\partial F_j(X)}{\\partial x_i}\\right]_{i,j}$.\n\nEssentially, this is the regular derivative of $F$ with respect to its input; Papernot et al. seem to refer to is as \u201cforward\u201d derivative as it stands in contrast with regular backpropagation where the derivative of the loss with respect to the parameters is considered. They define an adversarial saliency map by considering\n\n$S(X, t)_i = \\begin{cases}0 & \\text{ if } \\frac{\\partial F_t(X)}{\\partial X_i} < 0 \\text{ or } \\sum_{j\\neq t} \\frac{\\partial F_j(X)}{\\partial X_i} > 0\\\\ \\left(\\frac{\\partial F_t(X)}{\\partial X_i}\\right) \\left| \\sum_{j \\neq t} \\frac{\\partial F_j(X)}{\\partial X_i}\\right| & \\text{ otherwise}\\end{cases}$\n\nwhere $t$ is the target class of the attack. The intuition of this definition is the following: The partial derivative of $F_t$ with respect to $X$ at location $i$ indicates how $X_i$ can be changed in order to increase $F_t$ (which is the goal). At the same time, $F_j$ for all $t \\neq j$ is supposed to decrease for the targeted attack, this is implemented using the second (absolute) term. If, at a specific feature $X_i$, not increase of $X_i$ will lead to an increase of $F_t$, or an increase will also lead to an increase in the other $F_j$, the saliency map is zero \u2013 indicating that feature $i$ is useless. Note that here, only increases in $X_i$ are considered; Papernot et al. have a analogous formulation for considering decreases of $X_i$.\nBased on the concept of adversarial saliency maps, a simple attack is implemented as illustrated in Algorithm 1. In particular, the feature $X_i$ for which the saliency map $S(X, t)$ is maximized is chosen and increased by a fixed amount until the network $F$ changes the label to $t$ or a maximum perturbation is reached (in which case the attack fails).\n\nhttps://i.imgur.com/PvJv9yS.png\nAlgorithm 1: The proposed algorithm for generating adversarial examples, see text for details.\n\nIn experiments on MNIST they show the effectiveness of the proposed attack. Additionally, they attempt to quantify the robustness (called \u201chardness\u201d) of specific classes. In particular, they show that some classes are harder to attack than others. To this end they derive the so-called adversarial distance\n\n$A(X, t) = 1 - \\frac{1}{M}\\sum_i 1_{[S(X, t)_i > 0]}$\n\nwhich counts the number of features in the adversarial saliency map that are greater than zero (i.e. can be perturbed during the attack in Algorithm 1). Personally, I find this \u201chardness\u201d measure quite interesting because it is independent of a specific loss, but directly takes statistics of the learned model into account.\n\nAlso see this summary on [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.07528v1"
    },
    "599": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1712.02779",
        "transcript": "Engstrom et al. demonstrate that spatial transformations such as translations and rotations can be used to generate adversarial examples. Personally, however, I think that the paper does not address the question where adversarial perturbations \u201cend\u201d and generalization issues \u201cstart\u201d. For larger translations and rotations, the problem is clearly a problem of generalization. Small ones could also be interpreted as adversarial perturbations \u2013 especially when they are computed under the intention to fool the network. Still, the distinction is not clear ...\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1712.02779"
    },
    "600": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.02533",
        "transcript": "Kurakin et al. demonstrate that adversarial examples are also a concern in the physical world. Specifically, adversarial examples are crafted digitally and then printed to see if the classification network, running on a smartphone still misclassifies the examples. In many cases, adversarial examples are still able to fool the network, even after printing.\n\nhttps://i.imgur.com/tYCKv79.png\nFigure 1: Illustration of the experimental setup.\n\nAlso find this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.02533"
    },
    "601": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1707.03501",
        "transcript": "Lu et al. present experiments regarding adversarial examples in the real world, i.e. after printing them. Personally, I find it interesting that researchers are studying how networks can be fooled by physically perturbing images. For me, one of the main conclusions it that it is very hard to evaluate the robustness of networks against physical perturbations. Often it is unclear whether changed lighting conditions, distances or viewpoints to objects might cause the network to fail \u2013 which means that the adversarial perturbation did not cause this failure.\n\nAlso found this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1707.03501"
    },
    "602": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.01236",
        "transcript": "Kurakin et al. present some larger scale experiments using adversarial training on ImageNet to increase robustness. In particular, they claim to be the first using adversarial training on ImageNet. Furthermore, they provide experiments underlining the following conclusions:\n- Adversarial training can also be seen as regularizer. This, however, is not surprising as training on noisy training samples is also known to act as regularization.\n- Label leaking describes the observation that an adversarially trained model is able to defend against (i.e. correctly classify) an adversarial example which has been computed by knowing to true label while not defending against adversarial examples that were crafted without knowing the true label. This means that crafting adversarial examples without guidance by the true label might be beneficial (in terms of a stronger attack).\n- Model complexity seems to have an impact on robustness after adversarial training. However, from the experiments, it is hard to deduce how this connection might look exactly.\n\nAlso see this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.01236"
    },
    "603": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.02770",
        "transcript": "Liu et al. provide a comprehensive study on the transferability of adversarial examples considering different attacks and models on ImageNet. In their experiments, they consider both targeted and non-targeted attack and also provide a real-world example by attacking clarifai.com. Here, I want to list some interesting conclusions drawn from their experiments:\n- Non-targeted attacks easily transfer between models; targeted-attacks, in contrast, do generally not transfer \u2013 meaning that the target does not transfer across models.\n- The level of transferability does also seem to heavily really on hyperparameters of the trained models. In the experiments, the author observed this on different ResNet models which share the general architecture building blocks, but are of different depth.\n- Considering different models, it turns out that the gradient directions (i.e. the adversarial directions used in many gradient-based attacks) are mostly orthogonal \u2013 this means that different models have different vulnerabilities. However, the observed transferability suggests that this only holds for the \u201csteepest\u201d adversarial direction; the gradient direction of one model is, thus, still useful to craft adversarial examples for another model.\n- The authors also provide an interesting visualization of the local decision landscape around individual examples. As illustrated in Figure 1, the region where the chosen image is classified correctly is often limited to a small central area. Of course, I believe that these examples are hand-picked to some extent, but they show the worst-case scenario relevant for defense mechanisms.\n\nhttps://i.imgur.com/STz0iwo.png\nFigure 1: Decision boundary showing different classes in different colors. The axes correspond to one pixel differences; the used images are computed using $x' = x +\\delta_1u + \\delta_2v$ where $u$ is the gradient direction and $v$ a random direction.\n\nAlso see this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.02770"
    },
    "604": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1610.08401",
        "transcript": "Moosavi-Dezfooli et al. propose universal adversarial perturbations \u2013 perturbations that are image-agnostic. Specifically, they extend the framework for crafting adversarial examples, i.e. by iteratively solving\n\n$\\arg\\min_r \\|r \\|_2$ s.t. $f(x + r) \\neq f(x)$.\n\nHere, $r$ denotes the adversarial perturbation, $x$ a training sample and $f$ the neural network. Instead of solving this problem for a specific $x$, the authors propose to solve the problem over the full training set, i.e. in each iteration, a different sample $x$ is chosen, one step in the direction of the gradient is taken and the perturbation is updated accordingly. In experiments, they show that these universal perturbations are indeed able to fool networks an several images; in addition, these perturbations are \u2013 sometimes \u2013 transferable to other networks.\n\nAlso view this summary on [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1610.08401"
    },
    "605": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1608.04644",
        "transcript": "Carlini and Wagner propose three novel methods/attacks for adversarial examples and show that defensive distillation is not effective. In particular, they devise attacks for all three commonly used norms $L_1$, $L_2$ and $L_\\infty$ \u2013 which are used to measure the deviation of the adversarial perturbation from the original testing sample. In the course of the paper, starting with the targeted objective\n$\\min_\\delta d(x, x + \\delta)$ s.t. $f(x + \\delta) = t$ and $x+\\delta \\in [0,1]^n$,\nthey consider up to 7 different surrogate objectives to express the constraint $f(x + \\delta) = t$. Here, $f$ is the neural network to attack and $\\delta$ denotes the perturbation. This leads to the formulation\n\n$\\min_\\delta \\|\\delta\\|_p + cL(x + \\delta)$ s.t. $x + \\delta \\in [0,1]^n$\n\nwhere $L$ is the surrogate loss. After extensive evaluation, the loss $L$ is taken to be\n\n$L(x') = \\max(\\max\\{Z(x')_i : i\\neq t\\} - Z(x')_t, -\\kappa)$\n\nwhere $x' = x + \\delta$ and $Z(x')_i$ refers to the logit for class $i$; $\\kappa$ is a constant ($=0$ in their experiments) that can be used to control the confidence of the adversarial example. In practice, the box constraint $[0,1]^n$ is encoded through a change of variable by expressing $\\delta$ in terms of the hyperbolic tangent, see the paper for details. Carlini and Wagner then discuss the detailed attacks for all three norms, i.e. $L_1$, $L_2$ and $L_\\infty$ where the first and latter are discussed in more detail as they impose non-differentiability.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1608.04644v2"
    },
    "606": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1706.06083",
        "transcript": "Madry et al. provide an interpretation of training on adversarial examples as sattle-point (i.e. min-max) problem. Based on this formulation, they conduct several experiments on MNIST and CIFAR-10 supporting the following conclusions:\n- Projected gradient descent might be \u201cstrongest\u201d adversary using first-order information. Here, gradient descent is used to maximize the loss of the classifier directly while always projecting onto the set of \u201callowed\u201d perturbations (e.g. within an $\\epsilon$-ball around the samples). This observation is based on a large number of random restarts used for projected gradient descent. Regarding the number of restarts, the authors also note that an adversary should be bounded regarding the computation resources \u2013 similar to polynomially bounded adversaries in cryptography.\n- Network capacity plays an important role in training robust neural networks using the min-max formulation (i.e. using adversarial training). In particular, the authors suggest that increased capacity is needed to fit/learn adversarial examples without overfitting. Additionally, increased capacity (in combination with a strong adversary) decreases transferability of adversarial examples.\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1706.06083"
    },
    "607": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1412.6572",
        "transcript": "Goodfellow et al. introduce the fast gradient sign method (FGSM) to craft adversarial examples and further provide a possible interpretation of adversarial examples considering linear models. FGSM is a grdient-based, one step method for generating adversarial examples. In particular, letting $J$ be the objective optimized during training and $\\epsilon$ be the maximum $\\infty$-norm of the adversarial perturbation, FGSM computes\n$x' = x + \\eta = x + \\epsilon \\text{sign}(\\nabla_x J(x, y))$\nwhere $y$ is the label for sample $x$. The $\\text{sign}$ method is applied element-wise here. The applicability of this method is shown in several examples and it is commonly used in related work.\n\nIn the remainder of the paper, Goodfellow et al. discuss a linear interpretation of why adversarial examples exist. Specifically, considering the dot product\n$w^T x' = w^T x + w^T \\eta$\nit becomes apparent that the perturbation $\\eta$ \u2013 although insignificant on a per-pixel level (i.e. smaller than $\\epsilon$) \u2013 causes the activation of a single neuron to be influence significantly. What is more, this effect is more pronounced the higher the dimensionality of $x$. Additionally, many network architectures today use $\\text{ReLU}$ activations, which are essentially linear.\n\nGoodfellow et al. conduct several more experiments; I want to highlight the conclusions of some of them:\n- Training on adversarial samples can be seen as regularization. Based on experiments, it is more effective than $L_1$ regularization or adding random noise.\n- The direction of the perturbation matters most. Adversarial samples might be transferable as similar models learn similar functions where these directions are, thus, similarly effective.\n- Ensembles are not necessarily resistant to perturbations.\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1412.6572"
    },
    "608": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1705.07204",
        "transcript": "Tram\u00e8r et al. introduce both a novel adversarial attack as well as a defense mechanism against black-box attacks termed ensemble adversarial training. I first want to highlight that \u2013 in addition to the proposed methods \u2013 the paper gives a very good discussion of state-of-the-art attacks as well as defenses and how to put them into context. Tram\u00e8r et al. consider black-box attacks, focussing on transferrable adversarial examples. Their main observation is as follows: one-shot attacks (i.e. one evaluation of the model's gradient) on adversarially trained models are likely to overfit to the model's training loss. This observation has two aspects that are experimentally validated in the paper. First, the loss of the adversarially trained model increases sharply when considering adversarial examples crafted on a different model; second, the network learns to fool the attacker by, locally, misleading the gradient \u2013 this means that perturbations computed on adversarially trained models are specialized to the local loss. These observations are also illustrated in Figure 1, however, I refer to the paper for a detailed discussion.\n\nhttps://i.imgur.com/dIpRz9P.png\n\nFigure 1: Illustration of the discussed observations. On the left, the loss function of an adversarially trained model considering a sample $x = x + \\epsilon_1 x' + \\epsilon_2 x''$ where $x'$ is a perturbation computed on the adversarially trained model and $x''$ is a perturbation computed on a different model. On the right, zoomed in version where it can be seen that the loss rises sharply in the direction of $\\epsilon_1$; i.e. the model gives misleading gradients.\n\nBased on the above observations, Tram\u00e8r et al. First introduce a new one-shot attack exploiting the fact that the adversarially trained model is trained on overfitted perturbations and second introduce a new counter-measure for training more robust networks. Their attack is quite simple; they consider one Fast-Gradient Sign Method (FSGM) step, but apply a random perturbation first to leave the local vicinity of the sample first:\n$x' = x + \\alpha \\text{sign}(\\mathcal{N}(0, I))$\n$x'' = x' + (\\epsilon - \\alpha)\\text{sign}(\\nabla_{x'} J(x', y))$\nwhere $J$ is the loss function and $y$ the label corresponding to sample $x$. In experiments, they show that the attack has higher success rates on adversarially trained models.\n\nTo counter the proposed attack, they propose ensemble adversarial training. The key idea is to train the model utilizing not only adversarial samples crafted on the model itself but also transferred from pre-trained models. On MNIST, for example, they randomly select 64 FGSM samples from 4 different models (including the one in training). Experimentally, they show that ensemble adversarial training improves the defense again all considered attacks, including FGSM, iterative FGSM as well as the proposed attack.\n\nAlso view this summary at [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1705.07204"
    },
    "609": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.04508",
        "transcript": "Papernot et al. build upon the idea of network distillation [1] and propose a simple mechanism to defend networks against adversarial attacks. The main idea of distillation \u2013 originally introduced to \u201cdistill\u201d the knowledge of very deep networks into smaller ones \u2013 is to train a second, possibly smaller network, with the probability distributions of the original, possibly larger network as supervision. Papernot et al. as well as the authors of [1] argue that the probability distributions, i.e. the activations of the final softmax layer (also referred to as \u201csoft\u201d labels), contain rich information about the task in contrast to the true \u201chard\u201d labels. This allows the network to achieve similar performance while using less parameters or a different architecture.\n\nHowever, Papernot et al. do not distill a network's knowledge into a smaller one; instead they use distillation to make networks robust against adversarial attacks. They argue that most algorithms to generate adversarial examples make use of the \u201cadversarial gradient\u201d; i.e. the gradient of the network's cost w.r.t. its input. The adversarial gradient then guides perturbation of the input image in the direction of wrong classes (the authors consider a simple classification task for simplicity). Therefore, Papernot et al. Argure, the gradient around training samples needs to be reduced \u2013 in other words, the model needs to be smoothed.\n\nhttps://i.imgur.com/jXIhIGz.png\n\nThe proposed approach is very simple, they just distill the knowledge of the network into another network with same architectures and hyper parameters. By using the probability distributions as \u201csoft\u201d labels instead of the hard labels for training, the network is essentially smoothed. The full procedure is illustrated in Figure 1.\n\nDespite the simplicity of the approach, I want to highlight some additional key observations:\n- Distillation is also supposed to help generalization by avoiding overly confident networks.\n- The success rate of adversarial attacks can be reduced significantly as shown in quantitative experiments.\n- The amplitude of adversarial gradients can be reduced, which means that the network has been smoothed and is less sensitive to variations in the input samples.\n\nAlso see this summary on [davidstutz.de](https://davidstutz.de/category/reading/).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.04508v2"
    },
    "610": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1612.06299",
        "transcript": "Narodytska and Kasiviswanathan propose a local search-based black.box adversarial attack against deep networks. In particular, they address the problem of k-misclassification defined as follows:\n\nDefinition (k-msiclassification). A neural network k-misclassifies an image if the true label is not among the k likeliest labels.\n\nTo this end, they propose a local search algorithm which, in each round, randomly perturbs individual pixels in a local search area around the last perturbation. If a perturbed image satisfies the k-misclassificaiton condition, it is returned as adversarial perturbation. While the approach is very simple, it is applicable to black-box models where gradients and or internal representations are not accessible but only the final score/probability is available. Still the approach seems to be quite inefficient, taking up to one or more seconds to generate an adversarial example. Unfortunately, the authors do not discuss qualitative results and do not give examples of multiple adversarial examples (except for the four in Figure 1).\n\nhttps://i.imgur.com/RAjYlaQ.png\nFigure 1: Examples of adversarial attacks. Top: original image, bottom: perturbed image.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1612.06299"
    },
    "611": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1312.6199",
        "transcript": "Szegedy et al. were (to the best of my knowledge) the first to describe the phenomen of adversarial examples as researched today. Specifically, they described the main objective in order to obtain adversarial examples as\n\n$\\arg\\min_r \\|r\\|_2$ s.t. $f(x+r)=l$ and $x+r$ being a valid image\n\nwhere $f$ is the neural network and $l$ the target class (i.e. targeted adversarial example). In the paper, they originally headlined the section by \u201cblind spots in neural networks\u201d. While they give some explanation and provide experiments, also introducing the notion of transferability of adversarial examples and an idea of adversarial examples used as regularization during training, many questions are left open. The given conclusion, that these adversarial examples are highly unlikely and that these examples lie dense within regular training examples are controversial in the literature.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1312.6199"
    },
    "612": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1908-09791",
        "transcript": "**Summary**: The goal of this work is to propose a \"Once-for-all\u201d (OFA) network: a large network which is trained such that its subnetworks (subsets of the network with smaller width, convolutional kernel sizes, shallower units) are also trained towards the target task. This allows to adapt the architecture to a given budget at inference time while preserving performance.\n\n**Elastic Parameters.**\nThe goal is to train a large architecture  that contains several well-trained subnetworks with different architecture configurations (in terms of depth, width, kernel size, and resolution). One of the key difficulties is to ensure that each subnetwork reaches high-accuracy even though it is not trained independently but as part of a larger architecture.\nThis work considers standard CNN architectures (decreasing spatial resolution and increasing number of feature maps), which can be decomposed into units (A unit is a block of layers such that the first layer has stride 2, and the remaining ones have stride 1). The parameters of these units (depth, kernel size, input resolution, width) are denoted as *elastic parameters* in the sense that they can take different values, which defines different subnetworks, which still share the convolutional parameters.\n\n**Progressive Shrinking.**\nAdditionally, the authors consider a curriculum-style training process which they call *progressive shrinking*. First, they train the model with the maximum depth, $D$, kernel size, $K$, and width, $W$, which yields convolutional parameters . Then they progressively fine-tune this weight, with an additional distillation term from the largest network, while considering different values for the elastic parameters, in the following order:\n  * Elastic kernel size: Training for a kernel size $k < K$ is done by taking a linear transformation the center $k \\times k$ patch in the full $K \\times K$ kernels that are in . The linear transformation is useful to model the fact that different scales might be useful for different tasks.\n  * Elastic depth: To train for depth $d < D$, simply skip the last $D-d$ layers of the unit (rather than looking at every subset of dlayers)\n  * Elastic width: For a width $w < W$. First, the channels are reorganized by importance (decreasing order of the $L1$-norm of their weights), then use only the top wchannels\n  * Elastic resolution: Simply train with different image resolutions / resizing: This is actually used for all training processes.\n\n**Experiments.**\nHaving trained the Once-for-all (OFA) network, the goal is now to find the adequate architecture configuration, given a specific task/budget constraints. To do this automatically, they propose to train a small performance predictive model. They randomly sample 16K subnetworks from OFA, evaluate their accuracy on a validation set, and learn to predict accuracy based on architecture and input image resolution. (Note: It seems that this predictor is then used to perform a cheap evolutionary search, given latency constraints,  to find the best architecture config but the whole process is not  entirely clear to me. Compared to a proper neural architecture search, however it should be inexpensive).\n\nThe main experiments are on ImageNet, using MobileNetv3 as the base full architecture, with the goal of applying the model across different platforms with different inference budget constraints. Overall, the proposed model achieves comparable or higher accuracies for reduced search time, compared to neural architecture search baselines. More precisely their model has a fixed training cost (the OFA network) and a small search cost (find best config based on target latency), which is still lower than doing exhaustive neural architecture search. Furthermore, progressive shrinking does have a significant positive impact on the subnetworks accuracy (+4%).\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1908.09791"
    },
    "613": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1802-03685",
        "transcript": "The goal is to solve SAT problems with weak supervision: In that case a model is trained only to predict ***the satisfiability*** of a formula in conjunctive normal form. As a byproduct, when the formula is satisfiable, an actual  satisfying assignment can be worked out by clustering the network's activations in most cases.\n\n * **Pros (+):** Weak supervision, interesting structured architecture, seems to generalize nicely to harder problems by increasing the number  message passing iterations.\n * **Cons (-):** Limited practical applicability since it is outperfomed by classical SAT solvers.\n \n---\n \n # NeuroSAT\n \n ## Inputs\n We consider Boolean logic formulas in their ***conjunctive normal form*** (CNF), i.e. each input formula is represented as a conjunction ($\\land$) of **clauses**, which are themselves disjunctions ($\\lor$) of litterals (positive or negative instances of variables). The goal is to learn a classifier to predict whether such a formula is satisfiable.\n \n A first problem is how to encode the input formula in such a way that it preserves the CNF invariances (invariance to negating a litteral in all clauses, invariance to permutations in $\\lor$ and $\\land$ etc.). The authors use an  ***undirected graph representation*** where:\n   * $\\mathcal V$: vertices are the litterals (positive and negative form of variables, denoted as $x$ and $\\bar x$) and the clauses occuring in the input formula\n   * $\\mathcal E$: Edges are added to connect (i) the litterals with clauses they appear in and (ii) each litteral to its negative counterpart. \n   \nThe graph relations are encoded as an ***adjacency matrix***, $A$, with as many rows as there are litterals and as many columns as there are clauses. In particular, this structure does not constrain the vertices ordering, and does not make any preferential treatment between positive or negative litterals. However it still has some caveats, which can be avoided by pre-processing the formula. For instance when there are disconnected components in the graph, the averaging decision rule (see next paragraph) can lead to false positives.\n   \n## Message-passing model\nIn a high-level view, the model keeps track of an embedding for each vertex (litterals, $L^t$ and clauses, $C^t$), updated via ***message-passing on the graph***, and combined via a Multi Layer perceptrion (MLP) to output the  model prediction of the formula's satisfiability. The model updates are as follow:\n\n$$\n\\begin{align}\nC^t, h_C^t &= \\texttt{LSTM}_\\texttt{C}(h_C^{t - 1}, A^T \\texttt{MLP}_{\\texttt{L}}(L^{t - 1}) )\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)\\\\\nL^t, h_L^t &= \\texttt{LSTM}_\\texttt{L}(h_L^{t - 1}, \\overline{L^{t - 1}}, A\\ \\texttt{MLP}_{\\texttt{C}}(C^{t }) )\\ \\ \\ \\ \\ \\ (2)\n\\end{align}\n$$\n\nwhere $h$ designates a hidden context vector for the LSTMs. The operator $L \\mapsto \\bar{L}$  returns $\\overline{L}$, the  embedding matrix $L$ where the row of each litteral is swapped with the one corresponding to the litteral's negation. \nIn other words, in **(1)** each clause embedding is updated based on the litteral that composes it, while in **(2)** each litteral embedding is updated based on the clauses it appears in  and its negated counterpart.\n\nAfter $T$ iterations of this message-passing scheme, the model computes a ***logit for the satisfiability classification problem***, which is trained via sigmoid cross-entropy:\n\n$$\n\\begin{align}\nL^t_{\\mbox{vote}} &= \\texttt{MLP}_{\\texttt{vote}}(L^t)\\\\\ny^t &= \\mbox{mean}(L^t_{\\mbox{vote}})\n\\end{align}\n$$\n\n---\n\n# Training and Inference\n\n## Training Set\n\nThe training set is built such that for any satisfiable training formula $S$, it also includes an unsatisfiable counterpart $S'$ which differs from $S$ ***only by negating one litteral in one clause***. These carefully curated samples should constrain the model to pick up substantial characteristics of the formula. In practice, the model is trained on formulas containing up to ***40 variables***, and on average ***200 clauses***. At this size, the SAT problem can still be solved by state-of-the-art solvers (yielding the supervision) but are large enough they prove challenging for Machine Learning models.\n\n\n## Inferring the SAT assignment\n\nWhen a formula is satisfiable, one often also wants to know a ***valuation*** (variable assignment) that satisfies it.\nRecall that $L^t_{\\mbox{vote}}$ encodes a \"vote\" for every litteral and its negative counterpart. Qualitative experiments show that thoses scores cannot be directly used for inferring the variable assignment, however they do induce a nice clustering of the variables (once the message passing has converged). Hence an assignment can be found as follows:\n  * (1) Reshape $L^T_{\\mbox{vote}}$  to size $(n, 2)$ where $n$ is the number of litterals. \n  * (2) Cluster the litterals into two clusters with centers $\\Delta_1$ and $\\Delta_2$ using the following criterion:\n  \\begin{align}\n  \\|x_i - \\Delta_1\\|^2 + \\|\\overline{x_i} - \\Delta_2\\|^2 \\leq \\|x_i - \\Delta_2\\|^2 + \\|\\overline{x_i} - \\Delta_1\\|^2\n  \\end{align}\n  * (3) Try the two resulting assignments (set $\\Delta_1$ to true and $\\Delta_2$ to false, or vice-versa) and choose the one that yields satisfiability if any.\n \nIn practice, this method retrieves a satistifiability assignment for over 70% of the satisfiable test formulas.\n\n---\n\n# Experiments\n\nIn practice, the ***NeuroSAT*** model is trained with embeddings of dimension 128 and 26 message passing iterations using standard MLPs: 3 layers followed by ReLU activations. The final model obtains 85% accuracy in predicting a formula's satisfiability on the test set.\n\nIt also can generalize to ***larger problems***, requiring to increase the number of message passing iterations, although the classification performance decreases as the problem size grows (e.g. 25% for 200 variables).\n \nInterestingly, the model also generalizes well to other classes of problems that  were first ***reduced to SAT***, although they have different structure than the random formulas generated for training, which seems to show that the model does learn some general structural characteristics of Boolean formulas.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.03685"
    },
    "614": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.09136",
        "transcript": "CNNs predictions are known to be very sensitive to adversarial examples, which are samples generated to be wrongly classifiied with high confidence. On the other hand, probabilistic generative models such as `PixelCNN` and `VAEs` learn a distribution over the input domain hence could be used to detect ***out-of-distribution inputs***, e.g., by estimating their likelihood under the data distribution. This paper provides interesting results showing that distributions learned by generative models are not robust enough yet to employ them in this way. \n  * **Pros (+):** convincing experiments on multiple generative models, more detailed analysis in the invertible flow case, interesting negative results.\n  * **Cons (-):** It would be interesting to provide further results for different datasets / domain shifts to observe if this property can be quanitfied as a characteristics of the model or of the input data.\n  \n  \n---\n\n## Experimental negative result\nThree classes of generative models are considered in this paper:\n  * **Auto-regressive** models such as `PixelCNN` [1]\n  * **Latent variable** models, such as `VAEs` [2]\n  * Generative models with **invertible flows** [3], in particular `Glow` [4]. \n  \nThe authors train a generative model $G$ on input data $\\mathcal X$ and then use it to evaluate the likelihood on both the training domain $\\mathcal X$ and a different domain $\\tilde{\\mathcal X}$. Their main (negative) result is showing that **a model trained on the CIFAR-10 dataset yields a higher likelihood when evaluated on the SVHN test dataset than on the CIFAR-10 test (or even train) split**. Interestingly, the  converse, when training on SVHN and evaluating on CIFAR, is not true.\n\n This result was consistantly observed for various architectures including [1], [2] and [4], although it is of lesser effect in the `PixelCNN` case.\n\nIntuitively, this could come from the fact that both of these datasets contain natural images and that CIFAR-10 is strictly more diverse than SVHN in terms of semantic content. Nonetheless, these datasets vastly differ in appearance, and this result is counter-intuitive as it goes against the direction that generative models can reliably be use to detect out-of-distribution samples. Furthermore, this observation also confirms the general idea that higher likelihoods does not necessarily coincide with better generated samples [5].\n\n---\n## Further analysis for invertible flow models\nThe authors further study this phenomenon in the invertible flow models case as they provide a more rigorous analytical framework (exact likelihood inference unlike VAE which only provide a bound on the true likelihood). \n\nMore specifically invertible flow models are characterized with a ***diffeomorphism*** (invertible function),  $f(x; \\phi)$, between input space $\\mathcal X$ and latent space $\\mathcal Z$, and choice of the latent distribution $p(z; \\psi)$. The ***change of variable formula*** links the density of $x$ and $z$ as follows:\n\n$$\n\\int_x p_x(x)d_x = \\int_x p_z(f(x)) \\left| \\frac{\\partial f}{\\partial x} \\right| dx\n$$\n\n\nAnd the training objective under this transformation becomes\n\n$$\n\\arg\\max_{\\theta} \\log p_x(\\mathbf{x}; \\theta) = \\arg\\max_{\\phi, \\psi} \\sum_i \\log p_z(f(x_i; \\phi); \\psi) + \\log \\left| \\frac{\\partial f_{\\phi}}{\\partial x_i} \\right|\n$$\n\nTypically, $p_z$ is chosen to be Gaussian, and samples are build by inverting $f$, i.e.,$z \\sim p(\\mathbf z),\\  x = f^{-1}(z)$. And $f_{\\phi}$ is build such that computing the log determinant of the Jacabian in the previous equation can be done efficiently.\n\nFirst, they observe that contribution of the flow can be decomposed in a ***density*** element (left term) and a ***volume*** element (right term), resulting from the change of variables formula. Experiment results with Glow [4] show that the higher density  on SVHN mostly comes from the ***volume element contribution***.\n  \nSecondly, they try to directly analyze the difference in likelihood between two domains $\\mathcal X$ and $\\tilde{\\mathcal X}$; which can be done by a second-order expansion of the log-likelihood locally around the expectation of the distribution (assuming $\\mathbb{E} (\\mathcal X) \\sim \\mathbb{E}(\\tilde{\\mathcal X})$). For the constant volume Glow module, the resulting analytical formula indeed confirms that the log-likelihood of SVHN should be higher than CIFAR's, as observed in practice.\n  \n  \n  --- \n## References\n  * [1] Conditional Image Generation with PixelCNN Decoders, van den Oord et al, 2016\n  * [2] Auto-Encoding Variational Bayes, Kingma and Welling, 2013\n  * [3] Density estimation using Real NVP, Dinh et al., ICLR 2015\n  * [4] Glow: Generative Flow with Invertible 1x1 Convolutions, Kingma and Dhariwal\n  * [5] A Note on the Evaluation of Generative Models, Theis et al., ICLR 2016",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.09136"
    },
    "615": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/GomezRUG17",
        "transcript": "Residual Networks (ResNets)  have greatly advanced the state-of-the-art in Deep Learning by making  it possible to train much deeper networks via the addition of skip connections. However, in order to compute gradients during the backpropagation pass, all the units' activations have to be stored during the feed-forward pass, leading to high memory requirements for these very deep networks.\n\nInstead, the authors propose a **reversible architecture** based on ResNets, in which activations at one layer can be computed from the ones of the next. Leveraging this invertibility property, they design  a more efficient implementation of backpropagation, effectively trading compute power for memory storage.\n  * **Pros (+): ** The change does not negatively impact model accuracy (for equivalent number of model parameters) and it only requires a small change in the backpropagation algorithm.\n  * **Cons (-): ** Increased number of parameters, thus need to change the unit depth to match the \"equivalent\" ResNet\n\n---\n\n# Proposed Architecture\n\n## RevNet\nThis paper proposes to incorporate idea from previous reversible architectures, such as NICE [1], into a standard ResNet. The resulting model is called **RevNet** and is composed of reversible blocks, inspired from *additive coupling* [1, 2]:\n\n$\n \\begin{array}{r|r}\n\\texttt{RevNet Block} & \\texttt{Inverse Transformation}\\\\\n\\hline\n\\mathbf{input }\\  x & \\mathbf{input }\\  y \\\\\nx_1, x_2 = \\mbox{split}(x) & y1, y2 = \\mbox{split}(y)\\\\\ny_1 = x_1 + \\mathcal{F}(x_2) & x_2 = y_2 - \\mathcal{G}(y_1) \\\\\ny_2 = x_2 + \\mathcal{G}(y_1) & x_1 = y_1 - \\mathcal{F}(x_2)\\\\\n\\mathbf{output}\\ y = (y_1, y_2) & \\mathbf{output}\\ x = (x_1, x_2)\n\\end{array}\n$\n\n\nwhere $\\mathcal F$ and $\\mathcal G$ are residual functions, composed of sequences of convolutions, ReLU and Batch Normalization layers, analoguous to the ones in a standard ResNet block, although operations in the reversible blocks need to have a stride of 1 to avoid information loss and preserve invertibility. Finally, for the `split` operation, the authors consider spliting the input Tensor across the channel dimension as in [1, 2].\n\nSimilarly to ResNet, the final RevNet architecture is composed of these invertible residual blocks, as well as non-reversible subsampling operations (e.g., pooling) for which activations have to be stored. However the number of such operations is much smaller than the number of residual blocks in a typical ResNet architecture. \n\n## Backpropagation\n\n### Standard\nThe backpropagaton algorithm is derived from the chain rule and is used to compute the total gradients of the loss with respect to the parameters  in a neural network: given a loss function $L$, we want to compute the gradients of $L$ with respect to the parameters of each layer, indexed by $n \\in [1, N]$, i.e., the quantities $ \\overline{\\theta_{n}} = \\partial L /\\ \\partial \\theta_n$. (where $\\forall x, \\bar{x} = \\partial L / \\partial x$).\n\nWe roughly summarize the algorithm in the left column of **Table 1**: In order to compute the gradients for the $n$-th block, backpropagation requires the input and output activation of this block, $y_{n - 1}$ and $y_{n}$, which have been stored, and the derivative of the loss respectively to the output, $\\overline{y_{n}}$, which has been computed in the backpropagation iteration of the upper layer; Hence the name backpropagation\n\n### RevNet\nSince activations are not stored in RevNet, the algorithm needs to be slightly modified, which we describe in the right column of **Table 1**. In summary, we first need to recover the input activations of the RevNet block using its invertibility. These activations will be propagated to the earlier layers for further backpropagation.  Secondly, we need to compute the gradients of the loss with respect to the inputs, i.e. $\\overline{y_{n - 1}} = (\\overline{y_{n -1, 1}}, \\overline{y_{n - 1, 2}})$, using the fact that:\n$\n\\begin{align}\n\\overline{y_{n - 1, i}} = \\overline{y_{n, 1}}\\ \\frac{\\partial y_{n, 1}}{y_{n - 1, i}} + \\overline{y_{n, 2}}\\ \\frac{\\partial y_{n, 2}}{y_{n - 1, i}}\n\\end{align}\n$\n\nOnce again, this result will be propagated further down the network.\nFinally, once we have computed both these quantities we can obtain the gradients with respect to the parameters of this block, $\\theta_n$.\n\n\n\n\n\n$\n \\begin{array}{|c|l|l|}\n\\hline\n&  \\mathbf{ResNet} & \\mathbf{RevNet} \\\\\n\\hline\n\\mathbf{Block} & y_{n} = y_{n - 1} + \\mathcal F(y_{n - 1}) & y_{n - 1, 1}, y_{n - 1, 2} = \\mbox{split}(y_{n - 1})\\\\\n&& y_{n, 1} = y_{n - 1, 1} + \\mathcal{F}(y_{n - 1, 2})\\\\\n&& y_{n, 2} =  y_{n - 1, 2} + \\mathcal{G}(y_{n, 1})\\\\\n && y_{n} = (y_{n, 1}, y_{n, 2})\\\\\n\\hline\n\\mathbf{Params} & \\theta = \\theta_{\\mathcal F} & \\theta = (\\theta_{\\mathcal F}, \\theta_{\\mathcal G})\\\\\n\\hline\n\\mathbf{Backprop} & \\mathbf{in:}\\  y_{n - 1}, y_{n}, \\overline{ y_{n}} & \\mathbf{in:}\\ y_{n}, \\overline{y_{n }}\\\\\n& \\overline{\\theta_n} =\\overline{y_n} \\frac{\\partial y_n}{\\partial \\theta_n} &\\texttt{# recover activations} \\\\\n&\\overline{y_{n - 1}} = \\overline{y_{n}}\\ \\frac{\\partial y_{n}}{\\partial y_{n-1}} &y_{n, 1}, y_{n, 2} = \\mbox{split}(y_{n}) \\\\\n&\\mathbf{out:}\\ \\overline{\\theta_n}, \\overline{y_{n -1}} & y_{n - 1, 2} =  y_{n, 2} - \\mathcal{G}(y_{n, 1})\\\\\n&&y_{n - 1, 1} =  y_{n, 1} - \\mathcal{F}(y_{n - 1, 2})\\\\\n&&\\texttt{#  gradients wrt. inputs} \\\\\n&&\\overline{y_{n -1, 1}} = \\overline{y_{n, 1}} + \\overline{y_{n,2}} \\frac{\\partial \\mathcal G}{\\partial y_{n,1}} \\\\\n&&\\overline{y_{n -1, 2}} = \\overline{y_{n, 1}} \\frac{\\partial \\mathcal F}{\\partial y_{n,2}} + \\overline{y_{n,2}} \\left(1 + \\frac{\\partial \\mathcal F}{\\partial y_{n,2}} \\frac{\\partial \\mathcal G}{\\partial y_{n,1}} \\right) \\\\\n&&\\texttt{ gradients wrt. parameters} \\\\\n&&\\overline{\\theta_{n, \\mathcal G}} = \\overline{y_{n, 2}} \\frac{\\partial \\mathcal G}{\\partial \\theta_{n, \\mathcal G}}\\\\\n&&\\overline{\\theta_{n, \\mathcal F}} = \\overline{y_{n,1}} \\frac{\\partial F}{\\partial \\theta_{n, \\mathcal F}} + \\overline{y_{n, 2}} \\frac{\\partial F}{\\partial \\theta_{n, \\mathcal F}} \\frac{\\partial \\mathcal G}{\\partial y_{n,1}}\\\\\n&&\\mathbf{out:}\\ \\overline{\\theta_{n}}, \\overline{y_{n -1}}, y_{n - 1}\\\\\n\\hline\n\\end{array}\n$\n\n**Table 1:** Backpropagation in the standard case and for Reversible blocks\n\n\n\n\n--- \n\n## Experiments\n\n\n** Computational Efficiency.** RevNets trade off memory requirements, by avoiding storing activations, against computations. Compared to other methods that focus on improving memory requirements in deep networks, RevNet provides the best trade-off: no activations have to be stored, the spatial complexity is $O(1)$. For the computation complexity, it is linear in the number of layers, i.e. $O(L)$. \n\nOne small disadvantage is that RevNets introduces additional parameters, as each block is composed of two residuals, $\\mathcal F$ and $\\mathcal G$, and their number of channels is also halved as the input is first split into two. \n\n**Results.** In the experiments section, the author compare ResNet architectures to their RevNets \"counterparts\": they build a RevNet with roughly the same number of parameters by halving the number of residual units and doubling the number of channels.\n\nInterestingly, RevNets achieve **similar performances** to their ResNet counterparts, both in terms of final accuracy, and in terms of training dynamics. The authors also analyze the impact of floating errors that might occur when reconstructing activations rather than storing them, however it appears these errors are of small magnitude and do not seem to negatively impact the model.\n\nTo summarize, reversible networks seems like a very promising direction to efficiently train very deep networks with memory budget constraints.\n\n---\n\n## References\n  * [1] NICE: Non-linear Independent Components Estimation, Dinh et al., ICLR 2015\n  * [2] Density estimation using Real NVP, Dinh et al., ICLR 2017\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6816-the-reversible-residual-network-backpropagation-without-storing-activations"
    },
    "616": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1807.00392",
        "transcript": " Given some input data $x$ and attribute $a_p$,  the task  is to predict label $y$ from $x$ while making $a_p$ *protected*, in other words, such that the model predictions are invariant to changes in $a_p$.\n \n  * **Pros (+)**: Simple and intuitive idea, easy to train, naturally extended to protecting multiple attributes.\n  * **Cons (-)**: Comparison to baselines could be more detailed / comprehensive, in particular the comparison to ALFR [4] which also relies on adversarial training.\n---\n\n ## Proposed Method\n \n **Domain adversarial networks.**\n The proposed model builds on the *Domain Adversarial Network* [1], originally introduced for unsupervised domain adaptation. Given some labeled data $(x, y) \\sim \\mathcal X \n\\times \\mathcal Y$, and some unlabeled data $\\tilde x \\sim  \\tilde{\\mathcal X}$, the goal is to learn a network that solves both classification tasks $\\mathcal X \\rightarrow \\mathcal Y$ and $\\tilde{\\mathcal X} \\rightarrow \\mathcal Y$ while learning a shared representation between $\\mathcal X$ and $\\tilde{\\mathcal X}$.\n\nThe model is composed of a feature extractor $G_f$ which then branches off into a *target* branch, $G_t$, to predict the target label, and a *domain* branch, $G_d$, predicting whether the input data comes either from domain $\\mathcal X$ or $\\tilde{\\mathcal X}$. The model parameters are trained with the following objective:\n\n$$\n\\begin{align}\n(\\theta_{G_f}, \\theta_{G_t} ) &= \\arg\\min \\mathbb E_{(x, y) \\sim \\mathcal X \\times \\mathcal Y}\\  \\ell_t \\left( G_t \\circ G_f(x), y \\right)\\\\\n\\theta_{G_d} &= \\arg\\max \\mathbb E_{x \\sim \\mathcal X} \\ \\ell_d\\left(  G_d \\circ G_f(x), 1 \\right) + \\mathbb E_{\\tilde x \\sim \\tilde{\\mathcal X}}\\ \\ell_d \\left(G_d \\circ G_f(\\tilde x), 0\\right)\\\\\n\\mbox{where } &\\ell_t \\mbox{ and } \\ell_d \\mbox{ are classification losses}\n\\end{align}\n$$\n\nThe gradient updates for this saddle point problem can be efficiently implemented using the Gradient Reversal Layer introduced in [1]\n\n**GRAD-pred.** In **G**radient **R**eversal **A**gainst **D**iscrimination, samples come only from one domain $\\mathcal X$, and the domain classifier $G_d$ is replaced by an *attribute* classifier, $G_p$, whose goal is to predict the value of the protected attribute $a_p$. \nIn other words, the training objective strives to build a feature representation of $x$ that is good enough to predict the correct label $y$ but such that $a_p$ cannot easily be deduced from it. \n\n\n\nOn the contrary, directly learning classification network $G_y \\circ G_f$ penalized when predicting the correct value of attribute $a_p$ could instead lead to a model that learns $a_p$ and trivially outputs an incorrect value. This situation is prevented by the adversarial training scheme here.\n\n**GRAD-auto.** The authors also consider a variant of the described model where the target branch  $G_t$ instead solves the auto-encoding/reconstruction task. The features learned by the encoder $G_f$ can then later be used as entry point of a smaller network for classification or any other task.\n \n---\n\n ## Experiments\n \n **Evaluation metrics.**  The model is evaluated on four metrics to qualify both accuracy and fairness, following the protocol in [2]:\n   * *Accuracy*, the proportion of correct classifications\n   * *Discrimination*, the average score differences (logits of the ground-truth class) between samples with $a_p = + 1$ and $a_p = -1 $ (assuming a binary attribute)\n   * *Consistency*, the average difference between a sample score and the mean of its nearest neighbors' score.\n   * *Delta = Accuracy - Discrimination*, a penalized version of accuracy\n   \n   \n**Baselines.**\n * **Vanilla** CNN trained without the protected attribute protection branch\n * **LFR** [2]: A classifier with an intermediate latent code $Z \\in \\{1 \\dots K\\}$ is trained with an objective that combines a classification loss (the model should accurately classify $x$), a reconstruction loss (the learned representation should encode enough information about the input to reconstruct it accurately) and a parity loss (estimate the probability $P(Z=z | x)$ for both populations with $a_p = 1$ and $a_p = -1$ and strive to make them equal)\n * **VFA** [3]: A VAE where the protected attribute $a_p$ is factorized out of the latent code $z$, and additional invariance is imposed via a MMD objective which tries to match the moments of the posterior distributions $q(z|a_p = -1)$ and $q(z| a_p = 1)$.\n * **ALFR** [4] : As in LFR, this paper proposes a model trained with a reconstruction loss and a classification loss. Additionally, they propose to quantify the dependence between the learned representation and the protected attribute by adding an adversary classifier that tries to extract the attribute value from the representation, formulated and trained as in the Generative Adversarial Network (GAN) setting.\n    \n**Results.** GRAD always reaches highest consistency compared to baselines. For the other metrics, the results are more mitigated, although it usually achieves best or second best results. It's also not clear how to choose between GRAD-pred and GRAD-auto as there does not seem to be a clear winner, although GRAD-pred seems more intuitive when supervision is available, as it directly solves the classification task.\n\nAuthors also report a small experiment showing that protecting several attributes at the same time can be more beneficial than protecting a single attribute. This can be expected as some attributes are highly correlated or interact in meaningful way. \nIn particular, protecting several attributes at once can easily be done in the GRAD framework by making the attribute prediction branch multi-class for instance: however it is not clear in the paper how it is actually done in practice, nor whether the same  idea could also be integrated in the baselines for further comparison.\n    \n---  \n ## References\n   * [1] Domain-Adversarial Training of Neural Networks, Ganin et al, JMRL 2016\n   * [2] Learning Fair Representations,  Zemel et al, ICML 2013\n   * [3] The Variational Fair Autoencoder, Louizos et al, 2016\n   * [4] Censoring Representations with an Adversary, Edwards and Storkey, ICLR 2016",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1807.00392"
    },
    "617": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1602.04938",
        "transcript": "Although Machine learning models have been accepted widely as the next step towards simplifying complex problems, the inner workings of a machine learning model are still unclear and these details can lead to an increase in trust of the model prediction, and the model itself. \n\n**Idea: ** A good explanation system that can justify the prediction of a classifier and can lead to diagnosing the reasoning behind a model can exponentially raise one\u2019s trust in the predictive model.\n\n**Solution: ** This paper proposes a local explanation model called LIME, that approximates a linear local explanation with respect to a data point. The paper outlines desired characteristics for explainers and expounds on how LIME matches to these characteristics, the characteristics being 1) Interpretable 2) Local Fidelity 3) Model-Agnostic and 4) Provides a global perspective. This paper also explores the concept of Fidelity-Interpretability Trade-off; The more complex a model is the less interpretable a completely faithful explanation would be, thus a balance needs to be struck between interpretability and fidelity for complex models. The paper outlines in detail how the proposed LIME explanation model works, for different types of predictive classifiers. LIME works by generating random data points around a test data point and approximating a linear explanation for these randomized points. Thus, LIME works on a rather large assumption that every complex model is linear on a microscopic level. This assumption although large seems justified for most models, although this could lead to certain global issues when analyzing a complex model on the whole.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1602.04938"
    },
    "618": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.03292",
        "transcript": "**Idea:** With the growing use of visual explanation systems of machine learning models such as saliency maps, there needs to be a standardized method of verifying if a saliency method is correctly describing the underlying ML model.\n\n**Solution:** In this paper two Sanity Checks have been proposed to verify the accuracy and the faithfulness of a saliency method:\n* *Model parameter randomization test:* In this sanity check the outputs of a saliency method on a trained model is compared to that of the same method on an untrained randomly parameterized model. If these images are similar/identical then this saliency method does not correctly describe the model. In the course of this experiment it is found that certain methods such as the Guided BackProp are constant in their explanations despite alterations in the model.\n* *Data Randomization Test:*  This method explores the relationship of saliency methods to data and their associated labels. In this test, the labels of the training data are randomized thus there should be no definite pattern describing the model (Since the model is as good as randomly guessing an output label). If there is a definite pattern, this shows that the saliency methods are independent of the underlying model/training data labels. In this test as well Guided BackProp did not fare well, implying this saliency method is as good as an edge detector as opposed to a ML explainer.\n\nThus this paper makes a valid argument toward having standardized tests that an interpretation model must satisfy to be deemed accurate or faithful.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.03292"
    },
    "619": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=lundberg2019explainable",
        "transcript": "Tree-based ML models are becoming increasingly popular, but in the explanation space for these type of models is woefully lacking explanations on a local level. Local level explanations can give a clearer picture on specific use-cases and help pin point exact areas where the ML model maybe lacking in accuracy.\n\n**Idea**: We need a local explanation system for trees, that is not based on simple decision path, but rather weighs each feature in comparison to every other feature to gain better insight on the model's inner workings.\n\n**Solution**: This paper outlines a new methodology using SHAP relative values, to weigh pairs of features to get a better local explanation of a tree-based model. The paper also outlines how we can garner global level explanations from several local explanations, using the relative score for a large sample space. The paper also walks us through existing methodologies for local explanation, and why these are biased toward tree depth as opposed to actual feature importance.\n\nThe proposed explanation model titled TreeExplainer exposes methods to compute optimal local explanation, garner global understanding from local explanations, and capture feature interaction within a tree based model.\n\nThis method assigns Shapley interaction values to pairs of features essentially ranking the features so as to understand which features have a higher impact on overall outcomes, and analyze feature interaction.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.04610"
    },
    "620": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.08608",
        "transcript": "For a machine learning model to be trusted/ used one would need to be confident in its capabilities of dealing with all possible scenarios. To that end, designing unit test cases for more complex and global problems could be costly and bordering on impossible to create.\n\n**Idea**: We need a basic guideline that researchers and developers can adhere to when defining problems and outlining solutions, so that model interpretability can be defined accurately in terms of the problem statement.\n\n**Solution**: This paper outlines the basics of machine learning interpretability, what that means for different users, and how to classify these into understandable categories that can be evaluated. This paper highlights the need for interpretability, which arises from *incompleteness*,either of the problem statement, or the problem domain knowledge. This paper provides three main categories to evaluating a model/ providing interpretations:\n- *Application Grounded Evaluation*: These evaluations are more costly, and involve real humans evaluating real tasks that a model would take up. Domain knowledge is necessary for the humans evaluating the real task handled by the model.\n- *Human Grounded Evaluation:* these evaluations are simpler than application grounded, as they simplify the complex task and have humans evaluate the simplified task. Domain knowledge is not necessary in such an evaluation. \n- *Functionally Grounded Evaluation:* No humans are involved in this version of evaluation, here previously evaluated models are perfected or tweaked to optimize certain functionality. Explanation quality is measured by a formal definition of interpretability.\n\nThis paper also outlines certain issues with the above three evaluation processes, there are certain questions that need answering before we can pick an evaluation method and metric. \n-To highlight the factors of interpretability, we are provided with the Data-driven approach. Here we analyze each task and the various methods used to fulfill the task and see which of these methods and tasks are most significant to the model.\n- We are introduced to the term latent dimensions of interpretability, i.e. dimensions that are inferred not observed. These are divided into task related latent dimensions and method related latent dimensions, these are a long list of factors that are task specific or method specific.\n\nThus this paper provides a basic taxonomy for how we should evaluate our model, and how these evaluations differ from problem to problem.  The ideal scenario outlined is that researchers provide the relevant information to evaluate their proposition correctly (correctly in terms of the domain and the problem scope).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1702.08608"
    },
    "621": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.07414",
        "transcript": "Model Interpretability aims at explaining the inner workings of a model promoting transparency of any decisions made by the model, however for the sake of human acceptance or understanding, these explanations seem to be more geared toward human trust than remaining faithful to the model.\n\n**Idea**\nThere is a distinct difference and tradeoff between persuasive and descriptive Interpretations of a model, one promotes human trust while the other stays truthful to the model. Promoting the former can lead to a loss in transparency of the model.\n\n**Questions to be answered:**\n- How do we balance between a persuasive strategy and a descriptive strategy?\n- How do we combat human cognitive bias?\n\n**Solutions:**\n- *Separating the descriptive and persuasive steps: *\n    - We first generate a descriptive explanation, without trying to simplify it\n    - In our final steps we add persuasiveness to this explanation to make it more understandable\n- *Explicit inclusion of cognitive features:*\n    - We would include attributes that affect our functional measures of interpretability to our objective function.\n    - This approach has some drawbacks however:\n        - we would need to map the knowledge of the user which is an expensive process.\n        - Any features that we fail to add to the objective function would add to the human cognitive bias\n        - Increased complexity in optimizing of a multi-objective loss function.\n\n\n\n**Important terms:**\n- *Explanation Strategy*: An explanation strategy is defined as an explanation vehicle coupled with the objective function, constraints, and hyper parameters required to generate a model explanation\n- *Explanation model*: An explanation model is defined as the implementation of an explanation strategy, which is fit to a model that is to be interpreted.\n- *Human Cognitive Bias*: if an explanation model is highly persuasive or tuned toward human trust as opposed to staying true to the model, the overall evaluation of this explanation would be highly biased compared to a descriptive model. This bias can lead from commonalities between human users across a domain, expertise of the application, or the expectation of a model explanation. Such bias is known as implicit human cognitive bias. \n- *Persuasive Explanation Strategy*: A persuasive explanation strategy aim at convincing a user/ humanizing a model so that the user feels more comfortable with the decisions generated by the model. Fidelity or truthfulness to the model in such a strategy can be very low, which can lead to ethical dilemmas as to where to draw the line between being persuasive and being descriptive. Persuasive strategies do promote human understanding and cognition, which are important aspects of interpretability, however they fail to address the certain other aspects such as fidelity to the model.\n- *Descriptive Explanation Strategy*: A descriptive explanation strategy stays true to the underlying model, and generates explanations with maximum fidelity to the model. Ideally such a strategy would describe exactly what the inner working of the underlying model is, which is the main purpose of model interpretation in terms of better understanding the actual workings of the model. \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.07414v1"
    },
    "622": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=gilpin2018explaining",
        "transcript": "With growing use of ML and AI solutions to complex problems, there is a rise in need for understanding and explaining these models appropriately however these explanations vary in how well they adhere to the model/ explain the decisions in a human understandable way.\n\n**Idea** : There is no standard method of categorizing interpretation methods/ explanations, and no good working practices in the field of interpretability.\n\n**Solution** : This paper explores and categorizes different approaches to interpreting machine learning models. The three main categories this paper proposes are:\n- Processing: interpretation approach that uses surrogate models to explain complex models\n- Representation: interpretation approach that analyzes intermediate data representations in models with transferability of data/ layers \n- Explaining Producing: interpretation approach in which the trained model as part of it's processing also generates an explanation for its process.\n\nIn this paper we see different approaches to interpretation in detail, analyzing what the major component is to the interpretation, And which proposed category the explanation method would fall under. The paper goes into detail about other research papers that also deal with categorizing or exploring explanations, and the overall meaning of explainability in other domains.\n\nThis paper also touches on how \"completeness\" (defined as how close the explanation is to the underlying model) and \"interpretation\" (defined as how easily humans can understand/ trust the model) do have tradeoffs, the author argues that these tradeoffs not only exist in the final explanation, but within each category the definition of completeness would be different and the metric used to measure this would change, which makes sense when you think that different users have different viewpoints on how a model should behave, and what the desired explanation for a result is.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1806.00069"
    },
    "623": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LakkarajuKCL17",
        "transcript": "Model interpretations must be true to the model but must also promote human understanding of the working of the model. To this end we would need an interpretability model that balances the two.\n\n**Idea** : Although there exist model interpretations that balance fidelity and human cognition on a local level specific to an underlying model, there are no global model agnostic interpretation models that can achieve the same.\n\n**Solution:** \n- Break up each aspect of the underlying model into distinct compact decision sets that have no overlap to generate explanations that are faithful to the model, and also cover all possible feature spaces of the model.\n- How the solution dealt with:\n    - *Fidelity* (staying true to the model): the labels in the approximation match that of the underlying model.\n    - *Unambiguity* (single clear decision): compact decision sets in every feature space ensures unambiguity in the label assigned to it.\n    - *Interpretability* (Understandable by humans): Intuitive rule based representation, with limited number of rules and predicates.\n    - *Interactivity* (Allow user to focus on specific feature spaces): Each feature space is divided into distinct compact sets, allowing users to focus on their area of interest.\n- Details on a \u201cdecision set\u201d:\n    - Each decision set is a two-level decision (a nested if-then decision set), where the outer if-then clause specifies the sub-space, and the inner if-then clause specifies the logic of assigning a label by the model.\n    - A default set is defined to assign labels that do not satisfy any of the two-level decisions\n    - The pros of such a model is that we do not need to trace the logic of an assigned label too far, thus less complex than a decision tree which follows a similar if-then structure.\n\n**Mapping fidelity vs interpretability**\n- To see how their model handled fidelity vs interpretability, they mapped the rate of agreement (number of times the approximation label of an instance matches the blackbox assigned label) against pre-defined interpretability complexity defining terms such as:\n    - Number of predicates (sum of width of all decision sets)\n    - Number of rules (a set of outer decision, inner decision, and classifier label)\n    - Number of defined neighborhoods (outer if-then decision)\n- Their model reached higher agreement rates to other models at lower values for interpretability complexity.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1707.01154"
    },
    "624": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Kula15",
        "transcript": "The idea is to combine collaborative filtering with content-based recommenders to mitigate the user and item coldstart problems.\n\nThe author distinguishes between positive and negative interactions.\n\nThe representation of a user and of items is the sum of all their latent representations. This sounds similar to \"**Asymmetric factor models**\" as described in [the BellKor Netflix price solution](https://www.netflixprize.com/assets/ProgressPrize2007_KorBell.pdf). **The key idea is to encode the latent user (or item) vector as a sum of latent attribute vectors.**\n\nAdagrad / asynchronous stochastic gradient descent was used for optimization.\n\n\n## See also\n\n* [Code on GitHub](https://lyst.github.io/lightfm/docs/index.html#)\n* [Paper on ArXiv](https://arxiv.org/pdf/1507.08439.pdf)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1507.08439"
    },
    "625": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=koren:icdm08",
        "transcript": "This paper is about a recommendation system approach using collaborative filtering (CF) on implicit feedback datasets.\n\nThe core of it is the minimization problem\n\n$$\\min_{x_*, y_*} \\sum_{u,i} c_{ui} (p_{ui} - x_u^T y_i)^2 + \\underbrace{\\lambda \\left ( \\sum_u || x_u ||^2 + \\sum_i || y_i ||^2\\right )}_{\\text{Regularization}}$$\n\nwith\n\n* $\\lambda \\in [0, \\infty[$ is a hyper parameter which defines how strong the model is regularized\n* $u$ denoting a user, $u_*$ are all user factors $x_u$ combined\n* $i$ denoting an item, $y_*$ are all item factors $y_i$ combined\n* $x_u \\in \\mathbb{R}^n$ is the latent user factor (embedding); $n$ is another hyper parameter. $n=50$ seems to be a reasonable choice.\n* $y_i \\in \\mathbb{R}^n$ is the latent item factor (embedding)\n* $r_{ui}$ defines the \"intensity\"; higher values mean user $u$ interacted more with item $i$\n* $p_{ui} = \\begin{cases}1 & \\text{if } r_{ui} >0\\\\0 &\\text{otherwise}\\end{cases}$\n* $c_{ui} := 1 + \\alpha r_{ui}$ where $\\alpha \\in [0, \\infty[$ is a hyper parameter; $\\alpha =40$ seems to be reasonable\n\nIn contrast, the standard matrix factoriation optimization function looks like this ([example](https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture25-mf.pdf)):\n\n$$\\min_{x_*, y_*} \\sum_{(u, i, r_{ui}) \\in \\mathcal{R}} {(r_{ui} - x_u^T y_i)}^2  + \\underbrace{\\lambda \\left ( \\sum_u || x_u ||^2 + \\sum_i || y_i ||^2\\right )}_{\\text{Regularization}}$$\n\nwhere\n\n* $\\mathcal{R}$ is the set of all ratings $(u, i, r_{ui})$ - user $u$ has rated item $i$ with value $r_{ui} \\in \\mathbb{R}$\n\nThey use alternating least squares (ALS) to train this model.\n\nThe prediction then is the dot product between the user factor and all item factors ([source](https://github.com/benfred/implicit/blob/master/implicit/recommender_base.pyx#L157-L176))",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "626": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/BurgesSRLDHH05",
        "transcript": "[Learning to rank using gradient descent](https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf) is a paper published in 2005 by Burges et all from Microsoft. The paper introduced RankNet.\n\nRankNet is a neural network for recommendations.\n\nThe main use-case of the paper is ranking search results.\n\n## Key Ideas\n\n* Preprocessing: Filter results which are relevant\n* Ranking: Rank results which are relevant by RankNet\n\n## See also\n\n* [Adapting deep RankNet for personalized search](https://www.shortscience.org/paper?bibtexKey=conf/wsdm/SongWH14)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/1102351.1102363"
    },
    "627": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/wsdm/SongWH14",
        "transcript": "[Adapting Deep RankNet for Personalized Search](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/wsdm233-song.pdf) is a paper published in 2014 by Song, Wang and He from Microsoft Research. It is heavily beased on [Learning to rank using gradient descent](https://www.shortscience.org/paper?bibtexKey=conf/icml/BurgesSRLDHH05) (Burges et al from Microsoft, 2005).\n\nThey use a neural network with 5 hidden layers. They investigate regularization by trunkated gradient and limiting the depth of the back propagation.\n\n\n## See also\n\n* July 2015: [RankNet: A ranking retrospective](https://www.microsoft.com/en-us/research/blog/ranknet-a-ranking-retrospective/)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/2556195.2556234"
    },
    "628": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.3141/2291-08",
        "transcript": "[https://www.cs.uic.edu/~jakob/papers/biagioni-trr12.pdf](https://www.cs.uic.edu/~jakob/papers/biagioni-trr12.pdf) is a super nice survey where I almost feel bad to summarize it. Almost.\n\nIt is about road map inference in the presence of traces of geo coordinates. So you have many vehicles which log their positions while driving. From this data, you want to infer the latest map.\n\nProblems:\n* The map changes: What was a valid map a week ago, might not be anymore (due to construction work, breaking roads, new roads)\n* Sensor errors: GPS is not very accurate\n\n## Contributions\n\n* overview over the literature up to 2012 on map generation\n* method for the automatic evaluation of generated maps\n* an evaluation of three reference algorithms including code\n* a [118-h trace data set](https://www.cs.uic.edu/bin/view/Bits/Software) and ground truth map\n\n## How Map Inference Works\n\n* **Preprocess Geo-Traces**: check for unreasonable speed, too extreme acceleration, too abrupt changes\n* Inference:\n    * k-Means: Reduce candidates to centroid. Works on 3D - 2 coordinates and direction\n    * trace merging: Merge edges directly, without reduction\n    * kernel density estimation\n\n## Quantitative Evaluation\n\n1. Start at one location in both, the ground truth map and the generated map\n2. Follow streets from both and sample points in a fixed range (e.g. each 50cm). Only go in directions that go away from the start\n3. Try to match both, ground truth and generated map points. Only match if points are close enough (e.g. a distance of 10cm if this is an acceptable uncertainty). Then use a classification score (e.g. F1)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.3141/2291-08"
    },
    "629": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/itsc/MassowKPHPRHDH16",
        "transcript": "A high definition (HD) map in the context of highly automated vehicles is a digital map which contains precise information (error < 1m) necessary for localization and for driving behavior:\n\n* **road geometry**: Where is the road, where are lanes?\n* **[street furniture](https://en.wikipedia.org/wiki/Street_furniture)**: street signs, traffic lights\n* **dynamic data**: end of traffic jam, construction work\n\nThis kind of information can be stored in the [OpenDRIVE format](http://www.opendrive.org/docs/OpenDRIVEFormatSpecRev1.1D.pdf), which is an XML format with the extension `.xodr`.\n\nCreating and maintaining those maps is costly with dedicated cars, so we would like to have an alternative which works automatically. This paper proposes one. **The best summary of the paper is Figure 3.**\n\n## Possible Sensors\n\nThe following kind of sensors can be equipped in many cars:\n\n* stereo cameras\n* radar\n* GPS\n\n## Active Players\n\n* **Continental** Road Database: Automatic Recording and Processing of Highly Accurate Route Data\n* [HERE](https://www.here.com/en): map provider which has published a sensor interface specification: \u201cVehicle Sensor Data Cloud Ingestion Interface\nSpecification (v2.0.2)\n\n## See also\n\n* [Inferring road maps from global positioning system traces - Survey and comparative evaluation](https://www.cs.uic.edu/~jakob/papers/biagioni-trr12.pdf)\n* [Hochgenaue Fahrzeugeigenlokalisierung und kollektives Erlernen hochgenauer digitaler Karten]()",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ITSC.2016.7795794"
    },
    "630": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/JanaiGBG17",
        "transcript": "## Problems\n\n* **Computer Vision**:\n  * Detection: Given a 2D image, where are cars, pedestrians, traffic signs?\n  * Depth estimation: Given a 2D image, estimate the depth\n* **Planning**: Where do I want to go?\n* **Control**: How should I steer?\n\n## Datasets\n\n* KITTI: Street segmentation (Computer Vision)\n* ISPRS\n* MOT\n* Cityscapes\n\n\n## What I missed\n\n* GTSRB: The German Traffic Sign Recognition Benchmark dataset\n* GTSDB: The German Traffic Sign Detection Benchmark",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.05519"
    },
    "631": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/Girshick15",
        "transcript": "## See also\nRelated papers:\n* [R-CNN](http://www.shortscience.org/paper?bibtexKey=conf/iccv/Girshick15#joecohen)\n* [Fast R-CNN](http://www.shortscience.org/paper?bibtexKey=conf/iccv/Girshick15#joecohen)\n* [Faster R-CNN](http://www.shortscience.org/paper?bibtexKey=conf/nips/RenHGS15#martinthoma)\n* [Mask R-CNN](http://www.shortscience.org/paper?bibtexKey=journals/corr/HeGDG17)\n\nBlog posts:\n* Dhruv Parthasarathy: [A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN](https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ICCV.2015.169"
    },
    "632": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeGDG17",
        "transcript": "## See also\n* [R-CNN](http://www.shortscience.org/paper?bibtexKey=conf/iccv/Girshick15#joecohen)\n* [Fast R-CNN](http://www.shortscience.org/paper?bibtexKey=conf/iccv/Girshick15#joecohen)\n* [Faster R-CNN](http://www.shortscience.org/paper?bibtexKey=conf/nips/RenHGS15#martinthoma)\n* [Mask R-CNN](http://www.shortscience.org/paper?bibtexKey=journals/corr/HeGDG17)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.06870"
    },
    "633": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1007/978-3-642-15555-0_6",
        "transcript": "In this paper the authors experiment with 10,000 image classes based on ImageNet. As ImageNet is based on Wordnet, they have a semantic tree of the categories.\n\nIt should be noted that this paper is from 2010. Hence before AlexNet. They don't use CNNs in this paper.\n\n## Key findings\n\n* A relationship between visual similarity and semantic similarity exists\n* Classification can be improved by exploiting semantic hierarchy\n* Computational difficulties with 10,000 classes\n* More classes -> lower mean accuracy\n\n## See also\n\n* [What makes ImageNet good for transfer learning?](https://arxiv.org/abs/1608.08614) ([slides](https://www.dropbox.com/s/vfmncjnyh57glkc/NIPS_LSCVS_ImageNet%20Analysis.pdf?dl=0))",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-642-15555-0_6"
    },
    "634": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1703.09580",
        "transcript": "Summary from [reddit](https://www.reddit.com/r/MachineLearning/comments/623oq4/r_early_stopping_without_a_validation_set/dfjzwqq/):\n\nWe want to minimize the expected risk (loss) but that's a mean over the real distribution of the data, which we don't know. We approximate that by using a finite dataset and try to minimize the empirical risk instead.\nThe gradients for the empirical risk are an approximation to the gradients for the expected risk.\nThe idea is that the real gradients contain just information whereas the approximated gradients contain information + noise. The noise results from using a finite dataset to approximate the real distribution of the data.\nBy computing local statistics about the gradients, the authors are able to determine when the gradients have no information about the expected risk anymore and what's left is just noise. If we keep optimizing we're going to overfit.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1703.09580"
    },
    "635": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=demvsar2006statistical",
        "transcript": "Describes how to compare classifiers when they were evaluated on multiple datasets (e.g. CIFAR 10, MNIST and SVHN). Recommends Wilcoxon signed ranks test and Friedman test with the corresponding post-hoc tests. Introduce CD (critical difference) diagrams.\n\n* McNemar test and 5x2cv are good when comparing two classifiers on one dataset\n* Describes the Wilcoxon Signed-Ranks Test in section 3.1.3 in detail",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dl.acm.org/citation.cfm?id=1248547.1248548"
    },
    "636": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/neco/Dietterich98",
        "transcript": "This paper describes some statistical test very neatly. It also has a nice Figure which classifies statistical questions in machine learning.\n\n* 5x2cv: 5 iterations of 2 fold cross validation\n* Quasi-F test\n* McNemar's test: Described in detail!\n\n## See also\n\n* [On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach](http://www.shortscience.org/paper?bibtexKey=journals/datamine/Salzberg97)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1162/089976698300017197"
    },
    "637": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/datamine/Salzberg97",
        "transcript": "This paper describes common pitfalls when classifiers are compared and recommends McNemars test\n\n## Notes\n\n* t-test is simply the wrong test for such an experimental design\n\n\n## See also\n\n* Prechelt \"A quantitative study of experimental evaluations of neural network algorithms\" - most of 200 evaluated paper had flaws\n* Wolpert \"On the connection between in-sample testing and generalization error\" - No classifier is always better than another one.\n* Diettrich: [Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms](http://www.shortscience.org/paper?bibtexKey=journals/neco/Dietterich98)\n* Demsar: [Statistical Comparisons of Classifiers over Multiple Data Sets](http://www.shortscience.org/paper?bibtexKey=demvsar2006statistical)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1023/A:1009752403260"
    },
    "638": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SzegedyIV16",
        "transcript": "This paper describes the CNN architecture Inception-v4.\n\nThey basically update Inception-v3 to use residual connections (see [He et al](http://www.shortscience.org/paper?bibtexKey=journals/corr/HeZRS15)). They also simplified the architecture as they moved from DistBelief to [TensorFlow](https://www.tensorflow.org/).\n\n## Previous papers\n\n* Inception-v1: [Going deeper with Convolutions](http://www.shortscience.org/paper?bibtexKey=journals/corr/SzegedyLJSRAEVR14)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.07261"
    },
    "639": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1312.6229",
        "transcript": "\n\n## Terms\n\n* Classification: Assign one label to one image\n* Localization: Many object, there might be multiple instances of the same class.\n* Detection: One big dominant object which has to be found and to be classified. Pretty much first classification and then finding a bounding box for the class within the image\n\n## Evaluation\n\n* ILSVRC2013, localization task\n* ILSVRC2013, detection task\n* ILSVRC2013, classification task\n\n## Implementations\n\n* https://github.com/sermanet/OverFeat",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1312.6229"
    },
    "640": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.02492",
        "transcript": "Convolutional Neural Fabrics (CNFs) are a construction algorithm for CNN architectures.\n\n> Instead of aiming to select a single optimal architecture, we propose a \u201cfabric\u201d that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern.\n\n![Image](http://i.imgur.com/wlISXgo.png)\n\n* **Pooling**: CNFs don't use pooling. However, this might not be necessary as they use strided convolution.\n* **Filter size**: All convolutions use kernel size 3.\n* **Output layer**: Scale $1 \\times 1$, channels = nr of classes\n* **Activation function**: Rectified linear units (ReLUs) are used at all nodes.\n\n## Evaluation\n\n* Part Labels dataset (face images from the LFW dataset): a super-pixel accuracy of 95.6%\n* MNIST: 0.33% error (see [SotA](https://martin-thoma.com/sota/#image-classification); 0.21 %)\n* CIFAR10: 7.43% error (see [SotA](https://martin-thoma.com/sota/#image-classification); 2.72 %)\n\n\n## What I didn't understand\n\n* \"Activations are thus a linear function over multi-dimensional neighborhoods, i.e. a four dimensional\n3\u00d73\u00d73\u00d73 neighborhood when processing 2D images\"\n* \"within the first layer, channel c at scale s receives input from channels c + {\u22121, 0, 1} from scale s \u2212 1\": Why does the scale change? Why doesn't the  first layer receive input from the same scale?",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.02492"
    },
    "641": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1701.06538",
        "transcript": "A NLP paper.\n\n> \"conditional computation, achieving greater than 1000x improvements in model capacity with\nonly minor losses in computational efficiency on modern GPU clusters. We introduce\na Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to\nthousands of feed-forward sub-networks\"\n\n## Evaluation\n* 1 billion word language modeling benchmark\n* 100 billion word google news corpus",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1701.06538"
    },
    "642": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.02167",
        "transcript": "## Ideas\n\n* Find CNN topology with Q-learning and $\\varepsilon$-greedy exploration and experience replay\n\n\n## Evaluation\n\nThe authors seem not to know DenseNets\n\n* CIFAR-10: 6.92 % accuracy ([SOTA](https://martin-thoma.com/sota/#image-classification) is 3.46 % - not mentioned in the paper)\n* SVHN: 2.06 % accuracy ([SOTA](https://martin-thoma.com/sota/#image-classification) is 1.59% - not mentioned in the paper)\n* MNIST: 0.31 % ([SOTA](https://martin-thoma.com/sota/#image-classification) is 0.21 % - not mentioned in the paper)\n* CIFAR-100: 27.14 % accuracy ([SOTA](https://martin-thoma.com/sota/#image-classification) is 17.18 % - not mentioned in the paper)\n\n\n## Related Work\n\n* Google: [Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1611.01578) ([summary](http://www.shortscience.org/paper?bibtexKey=journals/corr/1611.01578#martinthoma))",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.02167"
    },
    "643": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1701.07275",
        "transcript": "This paper is about transfer learning for computer vision tasks.\n\n## Contributions\n* Before this paper, people focused on similar datasets (e.g. ImageNet-like images) or even the same dataset but a different task (classification -> segmentation). This paper, they look at extremely different dataset (ImageNet-like vs text) but only one task (classification). They show that all layers can be shared (including the last classification layer) between datasets such as MNIST and CIFAR-10\n* Normalizing information is necessary for sharing models between datasets in order to compensate for dataset-specific differences. Domain-specific scaling parameters work well.\n\n## Evaluation\n\n* Used datasets:\n  1. MNIST (10 classes: handwritten digits 0-9),\n  2. SVHN (10 classes: house number digits, 0-9),\n  3. [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) (10 classes: airplane, automobile, bird, ...)\n  4. Daimler Mono Pedestrian Classification Benchmark (18 \u00d7 36 pixels)\n  5. Human Sketch dataset (20000 human sketches of every day objects such as \u201cbook\u201d, \u201ccar\u201d, \u201chouse\u201d, \u201csun\u201d)\n  6. German Traffic Sign Recognition (GTSR) Benchmark (43 traffic signs)\n  7. Plankton imagery data (classification benchmark that contains 30336 images of various organisms ranging from the smallest single-celled protists to copepods, larval fish, and larger jellies)\n  8. Animals with Attributes (AwA): 30475 images of 50 animal species (for zero-shot learning)\n  9. Caltech-256: object classification benchmark (256 object categories and an additional background class)\n  10. Omniglot: 1623 different handwritten characters from 50 different alphabets (one shot learning)\n* images are resized to 64 \u00d7 64 pixels, greyscale ones are converted into RGB by setting the three channels to the same value\n* Each dataset is also whitened, by subtracting its mean and dividing it by its standard deviation per channel\n* **Architecture**: ResNet + Global Average Pooling + FC with Softmax\n* \"As the majority of the datasets have a different number of classes, we use a dataset-specific fully connected layer in our experiments unless otherwise stated.\"\n* **Data augmentation**: We follow the same data augmentation strategy in [[18]](http://www.shortscience.org/paper?bibtexKey=journals/corr/HeZRS15), the 64 \u00d7 64 size whitened image is padded with 8 pixels on all sides and a 64\u00d764 patch randomly sampled from the padded image or its horizontal flip (except for MNIST / Omniglot / SVHN, as those contain text)\n* **Training**: stochastic gradient descent with momentum\n\nSharing strategies:\n\n1. Baseline: Train networks for each dataset independantly\n2. Full sharing: For MNIST / SVHN / CIFAR-10, group classes randomly together so that Node 2 might be digit \"7\" for MNIST, digit \"3\" for SVHN and \"aeroplane\" for CIFAR-10. They are trained together in one network.\n3. Deep sharing: Share all layers except the last one. Use all 10 datasets for this.\n4. Partial sharing: Have a dataset-specific first part to compensate for different image statistics, but share the middle of the network.\n\nThe results seem to be inconclusive to me.\n\n\n## Follow-up / related work",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1701.07275"
    },
    "644": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.03490",
        "transcript": "This paper\n\n1. Explains why we want \"interpretability\" and hence what it can mean, depending on what we want.\n2. Properties of interpretable models\n3. Gives examples\n\nIt is easy to read. A must-read for everybody who wants to know about interpretability of models!\n\n\n## Why we want interpretability\n\n* Trust\n  * Intelligibility: Confidence in models accuracy vs\n  * Transparency: Understanding the model\n\n## How to achieve interpretability\n\n* Post-hoc explanations (saliency maps)\n* t-SNE",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.03490"
    },
    "645": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.05552",
        "transcript": "It's not clear to me where the difference between DenseNets and DelungeNets are.\n\n## Evaluation\n\n* Cifar-10: 3.76% error (DenseNet: )\n* Cifar-100: 19.02% error\n\n## See also\n\n* [reddit](https://www.reddit.com/r/MachineLearning/comments/5l0k6w/r_delugenets_deep_networks_with_massive_and/)\n* [DenseNet](http://www.shortscience.org/paper?bibtexKey=journals/corr/1608.06993)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.05552"
    },
    "646": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.01578",
        "transcript": "Find a topology by reinforcement learning. They use REINFORCE from [Williams 1992](http://www.shortscience.org/paper?bibtexKey=Williams:92).\n\n\n## Ideas\n\n* Structure and connectivity of a Neural Network can be represented by a variable-length string.\n* The RNN controller in Neural Architecture Search is auto-regressive, which means it predicts hyperparameters one a time, conditioned on previous predictions\n* policy gradient method to maximize the expected accuracy of the sampled architectures\n* In our experiments, the process of generating an architecture stops if the number of layers exceeds\na certain value.\n\n## Evaluation\n\n* Computer Vision - **CIFAR-10**: 3.65% error (State of the art are Dense-Nets with 3.46% error)\n* Language - **Penn Treebank**: a test set perplexity of 62.4 (3.6 perplexity better than the previous state-of-the-art)\n\nThey had a Control Experiment \"Comparison against Random Search\" in which they showed that they are much better than a random exploration of the data. However, the paper lacks details how exactly the random search was implemented.\n\n## Related Work\n\n* [Designing Neural Network Architectures using Reinforcement Learning](https://arxiv.org/pdf/1611.02167.pdf) ([summary](http://www.shortscience.org/paper?bibtexKey=journals/corr/1611.02167))",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.01578"
    },
    "647": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/ec/StanleyM02",
        "transcript": "This paper introduces NEAT (NeuroEvolution of Augmenting Topologies), a genetic algorithm for finding and training neural network topologies. NEAT trains the structure and the weights.\n\n> The weight space is explored through the crossover of network weight vectors and through the mutation of single networks\u2019 weights.\n\n\n## Evaluation\n\n* XOR-Problem\n* Reinforcement learning: balance two poles attached to a cart by moving the cart in appropriate directions to keep the pole from falling\n\n## Topology Encoding\n\n* Direct Encodings\n  * Bit string: A bit string encodes the connection matrix / matrices. First used in Structured Genetic Algorithm (sGA). Limitations: Crossover might not be useful; fixed size of layers.\n  * Graph encoding: First used in Parallel Distributed Genetic Programming (PDGP).\n* Indirect Encodings\n  * Cellular Encoding (CE): genomes are programs written in a specialized graph transformation language\n\n## Glossary\n\nFor people (like me) who are new to genetic algorithms (GAs):\n\n* [Neuroevolution](https://en.wikipedia.org/wiki/Neuroevolution) (NE): a form of machine learning that uses evolutionary algorithms to train artificial neural networks\n* [crossover](https://en.wikipedia.org/wiki/Crossover_(genetic_algorithm)): a genetic operator used to vary the programming of a chromosome or chromosomes from one generation to the next\n* [Speciation](https://en.wikipedia.org/wiki/Speciation): the evolutionary process by which biological populations evolve to become distinct species\n* TWEANNs: Topology and Weight Evolving Artificial Neural Networks\n\n## Realted\n\n* 2009, K. O. Stanley , D. B. D\u2019Ambrosio and J. Gauci: [A Hypercube-Based Indirect Encoding for Evolving Large-Scale Neural Networks.](http://www.shortscience.org/paper?bibtexKey=journals/alife/StanleyDG09): Introduces HyperNEAT",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1162/106365602320169811"
    },
    "648": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhaiCZL16",
        "transcript": "The paper ([arxiv](https://arxiv.org/abs/1610.09716)) introduces DCNNs (Doubly Convolutional Neural Networks). Those are CNNs which contain a new layer type which generalized convolutional layers.\n\n## Ideas\n\nCNNs seem to learn many filters which are similar to other learned filters in the same layer. The weights are only slightly shifted.\n\nThe idea of double convolution is to learn groups filters where filters within each group are translated versions of each other. To achieve this, a doubly convolutional layer allocates a set of meta filters which has filter sizes that are larger than the effective filter size. Effective filters can be then extracted from each meta filter, which corresponds to convolving the meta filters with an identity kernel. All the extracted filters are then concatenated, and convolved with the input.\n\n> We have also confirmed that replacing a convolutional layer with a doubly convolutional layer consistently improves the performance, regardless of the depth of the layer.\n\n## Evaluation\n\n* CIFAR-10+: 7.24% error\n* CIFAR-100+: 26.53% error\n* ImageNet: 8.23% Top-5 error\n\n\n## Critique\n\nThe k-translation correlation is effectively a [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). I think the authors should have mentioned that.\n\n\n## Related\n\nTODO",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6340-doubly-convolutional-neural-networks"
    },
    "649": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1512.02017",
        "transcript": "This paper is about finding naturally looking images for the analysis of machine learning models in computer vision. There are 3 techniques:\n\n* **inversion**: the aim is to reconstruct an image from its representation\n* **activation maximization**: search for patterns that maximally stimulate a representation component (deep dream). This does NOT use an initial natural image.\n* **caricaturization**: exaggerate the visual patterns that a representation detects in an image\n\nThe introduction is nice.\n\n\n## Code\n\nThe paper comes with code: [robots.ox.ac.uk/~vgg/research/invrep](http://www.robots.ox.ac.uk/~vgg/research/invrep/index.html) ([GitHub: aravindhm/deep-goggle](https://github.com/aravindhm/deep-goggle))\n\n\n## Related\n\n* 2013, Zeiler & Fergus: [Visualizing and Understanding Convolutional Networks ](http://www.shortscience.org/paper?bibtexKey=journals/corr/ZeilerF13#martinthoma)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1512.02017"
    },
    "650": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/YuYBYR14",
        "transcript": "This paper is about the analysis of CNNs. It seems to be extremely similar to what Zeiler & Fergus did. I can't see the contribution. Only cited 7 times, although it is from December 2014 -> I suggest to read the Zeiler & Fergus paper instead.\n\n## Related\n\n* 2013, Zeiler & Fergus: [Visualizing and Understanding Convolutional Networks ](http://www.shortscience.org/paper?bibtexKey=journals/corr/ZeilerF13#martinthoma)\n\n\n## Errors\n\n* \" Section ?? provides\"",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1412.6631"
    },
    "651": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.03530",
        "transcript": "This paper deals with the question what / how exactly CNNs learn, considering the fact that they usually have more trainable parameters than data points on which they are trained.\n\nWhen the authors write \"deep neural networks\", they are talking about Inception V3, AlexNet and MLPs.\n\n## Key contributions\n\n* Deep neural networks easily fit random labels (achieving a training error of 0 and a test error which is just randomly guessing labels as expected). $\\Rightarrow$Those architectures can simply brute-force memorize the training data.\n* Deep neural networks fit random images (e.g. Gaussian noise) with 0 training error. The authors conclude that VC-dimension / Rademacher complexity, and uniform stability are bad explanations for generalization capabilities of neural networks\n* The authors give a construction for a 2-layer network with $p = 2n+d$ parameters - where $n$ is the number of samples and $d$ is the dimension of each sample - which can easily fit any labeling. (Finite sample expressivity). See section 4.\n\n\n## What I learned\n\n* Any measure $m$ of the generalization capability of classifiers $H$ should take the percentage of corrupted labels ($p_c \\in [0, 1]$, where $p_c =0$ is a perfect labeling and $p_c=1$ is totally random) into account: If $p_c = 1$, then $m()$ should be 0, too, as it is impossible to learn something meaningful with totally random labels.\n* We seem to have built models which work well on image data in general, but not \"natural\" / meaningful images as we thought.\n\n\n## Funny\n\n> deep neural nets remain mysterious for many reasons\n\n> Note that this is not exactly simple as the kernel matrix requires 30GB to store in memory. Nonetheless, this system can be solved in under 3 minutes in on a commodity workstation with 24 cores and 256 GB of RAM with a conventional LAPACK call.\n\n\n## See also\n\n* [Deep Nets Don't Learn Via Memorization](https://openreview.net/pdf?id=rJv6ZgHYg)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.03530"
    },
    "652": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1301-3583",
        "transcript": "The paper 'Big Neural Networks Waste Capacity' recognizes that adding more layer / parameters does not improve accuracy. When reading this paper, one should bear in mind that it was written well before [Deep Residual Learning for Image Recognition](http://www.shortscience.org/paper?bibtexKey=journals/corr/HeZRS15) or DenseNets.\n\nIn the experiments, they applied MLPs to SIFT features of ImageNet LSVRC-2010.\n\n**Do not read this paper**. Instead, you might want to read the \"Deep Residual Learning for Image Recognition\". It makes the same point, but clearer and offers a solution to the underfitting problem.\n\n\n## Criticism\n\nI don't understand why they write about k-means.\n\n> Assuming minimal error in the human labelling of the dataset, it should be possible to reach errors close to 0%.\n\nFor ImageNet, the human labeling error is estimated at about 5% (I can't find the source for that, though)\n\n\n> Improvements on ImageNet are thought to be a good proxy for progress in object recognition (Deng et al., 2009).\n\nImageNet images are very different from \"typical web images\" like the [100 million images Flickr dataset](http://yahoolabs.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images-for).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1301.3583"
    },
    "653": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/FahlmanL89",
        "transcript": "Cascade Correlation is an algorithm to create feed-forward neural network architectures. However, those architectures are not the typical layered architectures. See [my YouTube video](https://www.youtube.com/watch?v=1E3XZr-bzZ4) for a short explanation of the constructed architecture.\n\nFor the \"correlation\" part, see [this question](http://datascience.stackexchange.com/q/9672/8820).\n\n## Related work\n\nSee [Meiosis Networks summary](http://www.shortscience.org/paper?bibtexKey=conf/nips/Hanson89#martinthoma) for many topology learning papers",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://nips.djvuzone.org/djvu/nips02/0524.djvu"
    },
    "654": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=ash:dynamic",
        "transcript": "Dynamic Node Creation (DNC) is about topology learning. DNC sequentially adds single nodes to the network until the desired accuracy is achieved.\n\nDNC uses the logistic activation function and creates layered feed-forward architectures with only one hidden layer. So basically they only added one neuron at a time to the existing hidden layer.\n\nThey expected this to be sufficient, as it was shown that networks with only one hidden layer can model \"any function of interest to an arbitrary selected precision\". However, the number of neurons might be really huge and much larger than if one used more layers.\n\n## Related Work\n\nSee [Meiosis Networks summary](http://www.shortscience.org/paper?bibtexKey=conf/nips/Hanson89#martinthoma) for many topology learning papers",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "655": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/Hanson89",
        "transcript": "This paper is about topology learning (also called *structural learning* as in contrast to *parametric learning*) for neural networks. Instead of taking deterministic weights, each weight $w_{ij}$ is normal distributed ($w_{ij} \\sim \\mathcal{N}(\\mu_{ij}, \\sigma^2_{ij})$). Hence every connection has two learned parameters: $\\mu_{ij}$ and $\\sigma^2_{ij}$.\n\nMeiosis is cell division. So meiosis networks split nodes under some conditions.\n\nThe \"topology\" being learned seems only to add single neurons to the given layers. It is not able to add new layers or add skip connections.\n\n## Chapters\n\n* 1.1 Learning and Search: The author seems to describe the [VC dimension](https://en.wikipedia.org/wiki/VC_dimension).\n* 1.2 Stochastic Delta Rule: Explains how to update the weights parameters.\n* 1.3 Meiosis: Networks variance is initialized randomly with $\\sigma_i^2 \\sim U([-10, 10])$ (negative variance???). A node $j$ is splitted, when the random part dominates the value of the sampled weights: $$\\frac{\\sum_i \\sigma_{ij}}{\\sum_i \\mu_{ij}} > 1 \\text{ and } \\frac{\\sum_k \\sigma_{jk}}{\\sum_k \\mu_{jk}} > 1$$ The mean of the new nodes is sampled around the old mean (TODO: how is it sampled?), half the variance is assigned to the new connections.\n* 1.4 Examples: XOR, 3-bit parity, blood NMR data, learning curves. Learning rate of $\\eta = 0.5$, momentum of $\\alpha = 0.75$.\n* 1.5 Conclusion:\n\n## What I don't understand\n\n1. In the present approach, weights reflect a coarse prediction history as coded by a distribution of values and parameterized in the mean and standard deviation of these weight distributions. \n2. The first formula (1)\n3. What negative variance is.\n4. How exactly the means are sampled\n\n\n## Related work\n\n* Constructive Methods\n  * 1989: [The Cascade-Correlation Learning Architecture](http://www.shortscience.org/paper?bibtexKey=conf/nips/FahlmanL89)\n  * 1989: [Dynamic Node Creation in Backpropagation Networks](http://www.shortscience.org/paper?bibtexKey=ash:dynamic): Only one hidden layer\n* Pruning methods\n  * 1989: [Optimal Brain Damage](http://www.shortscience.org/paper?bibtexKey=conf/nips/CunDS89)\n  * 1993: [Optimal Brain Surgeon](http://www.shortscience.org/paper?bibtexKey=conf/nips/HassibiS92)\n  * 2015: [Learning both Weights and Connections for Efficient Neural Networks](http://www.shortscience.org/paper?bibtexKey=journals/corr/1506.02626)\n  * 2016: [Neural networks with differentiable structure](http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1606.06216#martinthoma)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://nips.djvuzone.org/djvu/nips02/0533.djvu"
    },
    "656": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1612.08083",
        "transcript": "This paper is about a new model for language which uses a convolutional approach instead of LSTMs.\n\n\n## General Language modeling\n\nStatistical language models estimate the probability distribution of a sequence of words. They are important for ASR (automatic speech recognition) and translation. The usual approach is to embedd words into $\\mathbb{R}^n$ and then apply RNNs to the vector sequences.\n\n\n## Evaluation\n\n* [WikiText-103](http://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/): [Perplexity](https://en.wikipedia.org/wiki/Perplexity) of 44.9 (lower is better)\n* new best single-GPU result on the Google Billion Word benchmark: Perplexity of 43.9\n\n\n## Idea\n\n* uses Gated Linear Units (GLU)\n* uses pre-activation residual blocks\n* adaptive softmax\n* no tanh in the gating mechanism\n* use gradient clipping\n\n## See also\n\n* [Reddit](https://www.reddit.com/r/MachineLearning/comments/5kbsjb/r_161208083_language_modeling_with_gated/)\n* [Improving Neural Language Models with a Continuous Cache](https://arxiv.org/abs/1612.04426): Test perplexity of **40.8 on WikiText-103** ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1612.08083"
    },
    "657": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1506.02626",
        "transcript": "This paper is about pruning a neural network to reduce the FLOPs and memory necessary to use it. This method reduces AlexNet parameters to 1/9  and VGG-16 to 1/13 of the original size.\n\n## Receipt\n\n1. Train a network\n2. Prune network: For each weight $w$: if w < threshold, then w <- 0.\n3. Train pruned network\n\n## See also\n\n* [Optimal Brain Damage](http://www.shortscience.org/paper?bibtexKey=conf/nips/CunDS89)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1506.02626"
    },
    "658": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HassibiS92",
        "transcript": "Optimal Brain Surgeon (OBS) is about pruning neural networks to minimize the amount of parameters, training time and overfitting. It is very similar to [Optimal Brain Damage](http://www.shortscience.org/paper?bibtexKey=conf/nips/CunDS89#martinthoma), but claims to choose better weights. However, it does require to compute the inverse hessian. The hessian matrix of a neural network is a $n \\times n$ matrix, where $n$ is the number of parameters of the network. Typically, $n > 10^6$. This makes the approach unusable.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://nips.djvuzone.org/djvu/nips05/0164.djvu"
    },
    "659": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/CunDS89",
        "transcript": "Optimal Brain Damage (OBD) is a techique to make a network smaller by pruning small weights.\n\n\n## Idea\n\n* use second-derivative information to make tradeoff between network complexity and training error\n* do this while training to prevent overfitting / reduce the need for data / reduce training time\n* **How to choose what to delete**: Weights which have least impact on training error. This is estimated by approximating the function with a Taylor series.\n\n## Recipe\n\n(Directly copied from the paper):\n\nThe OBD procedure can be carried out as follows:\n\n1. Choose a reasonable network architecture\n2. Train the network until a reasonable solution is obtained\n3. Compute the second derivatives $h_{kk}$ for each parameter\n4. Compute the saliencies for each parameter: $s_k = h_{kk} u_k^2 /2$\n5. Sort the parameters by saliency and delete some low-saliency parameters\n6. Iterate to step 2\n\nDeleting a parameter is defined as setting it to 0 and freezing it there. Several\nvariants of the procedure can be devised, such as decreasing the \u007fvalues of the low-saliency parameters instead of simply setting them to 0, or allowing the deleted\nparameters to adapt again after they have been set to 0.\n\n## See also\n\n* 1989: Optimal Brain Damage ([original pdf](https://papers.nips.cc/paper/250-optimal-brain-damage.pdf), [nice pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf), [txt](https://github.com/NicolasEstrada/nlp/blob/master/nipstxt/nips02/0598.txt))\n* 1993: [Optimal Brain Surgeon](http://www.shortscience.org/paper?bibtexKey=conf/nips/HassibiS92) ([pdf](https://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf) and [follow-up](http://www.shortscience.org/paper?bibtexKey=conf/nips/HassibiSW93), [2](http://www.shortscience.org/paper?bibtexKey=conf/epia/EndischHS07))\n* 1998: LeNet-5\n* 2012: AlexNet\n* 2015: [Learning both Weights and Connections for Efficient Neural Networks](http://www.shortscience.org/paper?bibtexKey=journals/corr/1506.02626)\n* 2016: [Neural networks with differentiable structure](http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1606.06216#martinthoma)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://nips.djvuzone.org/djvu/nips02/0598.djvu"
    },
    "660": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1406.4729",
        "transcript": "Spatial Pyramid Pooling (SPP) is a technique which allows Convolutional Neural Networks (CNNs) to use input images of any size, not only $224\\text{px} \\times 224\\text{px}$ as most architectures do. (However, there is a lower bound for the size of the input image).\n\n## Idea\n\n* Convolutional layers operate on any size, but fully connected layers need fixed-size inputs\n* Solution:\n  * Add a new SPP layer on top of the last convolutional layer, before the fully connected layer\n  * Use an approach similar to bag of words (BoW), but maintain the spatial information. The BoW approach is used for text classification, where the order of the words is discarded and only the number of occurences is kept.\n  * The SPP layer operates on each feature map independently.\n  * The output of the SPP layer is of dimension $k \\cdot M$, where $k$ is the number of feature maps the SPP layer got as input and $M$ is the number of bins.\n\nExample: We could use spatial pyramid pooling with 21 bins:\n\n* 1 bin which is the max of the complete feature map\n* 4 bins which divide the image into 4 regions of equal size (depending on the input size) and rectangular shape. Each bin gets the max of its region.\n* 16 bins which divide the image into 4 regions of equal size (depending on the input size) and rectangular shape. Each bin gets the max of its region.\n\n## Evaluation\n\n* Pascal VOC 2007, Caltech101: state-of-the-art, without finetuning\n* ImageNet 2012: Boosts accuracy for various CNN architectures\n* ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014: Rank #2\n\n\n## Code\n\nThe paper claims that the code is [here](http://research.microsoft.com/en-us/um/people/kahe/), but this seems not to be the case any more.\n\nPeople have tried to implement it with Tensorflow ([1](http://stackoverflow.com/q/40913794/562769), [2](https://github.com/fchollet/keras/issues/2080), [3](https://github.com/tensorflow/tensorflow/issues/6011)), but by now no public working implementation is available.\n\n\n## Related papers\n\n* [Atrous Convolution](https://arxiv.org/abs/1606.00915)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1406.4729"
    },
    "661": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1602.04938",
        "transcript": "This paper describes how to find local interpretable model-agnostic explanations (LIME) why a black-box model $m_B$ came to a classification decision for one sample $x$. The key idea is to evaluate many more samples around $x$ (local) and fit an interpretable model $m_I$ to it. The way of sampling and the kind of interpretable model depends on the problem domain.\n\nFor computer vision / image classification, the image $x$ is divided into superpixels. Single super-pixels are made black, the new image $x'$ is evaluated $p' = m_B(x')$. This is done multiple times. \n\nThe paper is also explained in [this YouTube video](https://www.youtube.com/watch?v=KP7-JtFMLo4) by Marco Tulio Ribeiro.\n\nA very similar idea is already in the [Zeiler & Fergus paper](http://www.shortscience.org/paper?bibtexKey=journals/corr/ZeilerF13#martinthoma).\n\n## Follow-up Paper\n\n* June 2016: [Model-Agnostic Interpretability of Machine Learning](https://arxiv.org/abs/1606.05386)\n* November 2016:\n  * [Nothing Else Matters: Model-Agnostic Explanations By Identifying Prediction Invariance](https://arxiv.org/abs/1611.05817)\n  * [An unexpected unity among methods for interpreting\nmodel predictions](https://arxiv.org/abs/1611.07478)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1602.04938"
    },
    "662": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.06216",
        "transcript": "The paper describes a procedure for topology pruning based on L1 norm to make weights small and a threshold for deleting them alltogether.\n\nIt is similar to [Optimal Brain Damage](https://arxiv.org/abs/1606.06216) and [Optimal Brain Surgeon](http://ee.caltech.edu/Babak/pubs/conferences/00298572.pdf).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.06216"
    },
    "663": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1412.6806",
        "transcript": "A paper in the intersection for Computer Vision and Machine Learning. They simplify networks by replacing max-pooling by convolutions with higher stride.\n\n* introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches\n\n\n## Datasets\n\ncompetitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1412.6806"
    },
    "664": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1312.4400",
        "transcript": "A paper in the intersection for Computer Vision and Machine Learning. They propose a method (network in network) to reduce parameters. Essentially, it boils down to a pattern of (conv with size > 1) -> (1x1 conv) -> (1x1 conv) -> repeat\n\n## Datasets\nstate-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST\n\n## Implementations\n\n* [Lasagne](https://github.com/Lasagne/Recipes/blob/master/modelzoo/cifar10_nin.py)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1312.4400"
    },
    "665": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1502.03167",
        "transcript": "One problem of training deep networks is that the features of lower-layer networks change while the upper-layer networks have already been adjusted to the previous lower-layer features. The phenomenon of changing inputs while optimizing is called *internal covariate shift*.\n\nBatch normalization is done at training time for each mini batch.\n\n## Ideas\n\n* Training converges faster, if input is whitened (zero means, unit variances, decorrelated).\n* Normalization parameters have to be computed within the gradient calculation step to prevent the model from blowing up\n\n## What Batch Normalization is\n\nFor a layer with $d$-dimensional input $x = (x^{(1)}, \\dots, x^{(d)})$, we will normalize\neach dimension \n$$\\hat{x}^{(k)} = \\frac{x^{(k)} - \\mathbb{E}[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$$\nwhere the expectation and the variance are computed over the training data set. This does *not* decorrelate the features, though.\n\nAdditionally, for each activation $x^{(k)}$ two paramters $\\gamma^{(k)}, \\beta^{(k)}$ are introduced which scale and shift the feature:\n\n$$y^{(k)} = \\gamma^{(k)} \\cdot \\hat{x}^{(k)} + \\beta^{(k)}$$\n\nThose two parameters (per feature) are learnable!\n\n## Effect of Batch normalization\n\n* Higher learning rates can be used\n* Initialization is less important\n* Acts as a regularizer, eliminating the need for dropout in some cases\n* Faster training\n\n## Datasets\n\n* reaching 4.9% top-5 validation error (and 4.8% test error) on ImageNet classification\n\n## Used by\n\n* [Going Deeper with Convolutions](http://www.shortscience.org/paper?bibtexKey=journals/corr/SzegedyLJSRAEVR14)\n* [Deep Residual Learning for Image Recognition](http://www.shortscience.org/paper?bibtexKey=journals/corr/HeZRS15#martinthoma)\n\n## See also\n\n* [other summaries](http://www.shortscience.org/paper?bibtexKey=conf/icml/IoffeS15)\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1502.03167"
    },
    "666": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jmlr/SrivastavaHKSS14",
        "transcript": "This paper is a much better introduction to Dropout than [Improving neural networks by preventing\nco-adaptation of feature detectors](http://www.shortscience.org/paper?bibtexKey=journals/corr/1207.0580), written by the same authors two years later.\n\n## General idea of Dropout\n\nDropout is a layer type. It has a parameter $\\alpha \\in (0, 1)$. The output dimensionality of a dropout layer is equal to its input dimensionality. With a probability of $\\alpha$ any neurons output is set to 0. At testing time, the output of all neurons is multiplied with $\\alpha$ to compensate for the fact that no output is set to 0.\n\n\n## Interpretations\n\nDropout can be interpreted as training an ensemble of many networks, which share weights.\n\nIt can also be seen as a regularizer.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dl.acm.org/citation.cfm?id=2670313"
    },
    "667": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1312.6197",
        "transcript": "This paper analyses fully connected networks with dropout and ReLU activation functions.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1312.6197"
    },
    "668": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1512.00242",
        "transcript": "*Probabilistic weighted pooling* is proposed in this paper. It is based on max-pooling and dropout.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1512.00242"
    },
    "669": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/BoureauPL10",
        "transcript": "This paper analyzes max pooling and average pooling, as it is used in many convolutional neural networks (CNNs).\n\n## Why pooling is used\n\n* invariance to image transformations\n* more compact representations (- remove irrelevant information)\n* better robustness to noise and clutter\n\n\n## Max pooling or average pooling?\n\nNo clear answer to that. Sometimes one seems to be better, sometimes the other, sometimes something in between.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.icml2010.org/papers/638.pdf"
    },
    "670": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/bmvc/ParkhiVZ15",
        "transcript": "This paper is about data collection for face recognition.\n\nOne idea was to use weaker classifiers to rank the data presented to the annotators.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.5244/C.29.41"
    },
    "671": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZeilerF13",
        "transcript": "The main contribution of this paper is a new way to analyze CNNs by (a) visualizing intermediate learned features and (b) occlusion sensitivity analysis.\n\n## Analyzation techniques\n### Visualization\n\nA multi-layer deconvolutional network is used to project the feature activations back into pixel space, showing what input pattern originally caused a given activation in the feature maps. The idea is to train a network which is given the result of a layer $L_i$ and has to reconstruct the input feature map of $L_i$. This is repeated until the input image is reached.\n\nThe deconv-net has a special **unpooling layer**: The max-pooling layers have to save where an activation came from and store those to a switch variable, which is used in unpooling.\n\n### Occlusion sensitivity analysis\n\n* Occlude(I, x, y): Put a gray square centered at $(x, y)$ over a part of the image $I$. Run the classifier.\n* Create an image like this:\n    * Run Occlude(I, x, y) for all $(x, y)$ (possible with stride)\n    * At $(x, y)$, either ...\n        * (d) ... place a pixel which color-encodes the probability of the correct class\n        * (e) ... place a pixel which color-encodes the most probable class\n\nThe following image from the Zeiler & Fergus paper visualizes this pretty well:\n\nIf the dogs face is occluded, the probability of the correct class drops a lot:\n![Imgur](http://i.imgur.com/Q1Ama2z.png)\n\nIf the dogs face is occluded, the most likely class suddenly is \"tennisball\" and no longer \"Pomeranian\".\n![Imgur](http://i.imgur.com/5QYKh7b.png)\n\nSee [LIME](http://www.shortscience.org/paper?bibtexKey=journals/corr/1602.04938#martinthoma).\n\n\n## How visualization helped to construct ZF-Net\n\n* \"The first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies\" -> Lower filter size from $11 \\times 11$ to $7 \\times 7$\n* \"the 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions\" -> Lower stride from 4 to 2\n* The occlusion analysis helps to boost confidence that the kind of features being learned are actually correct. \n\n## ZF-Net\nZeiler and Fergus also created a new network for ImageNet.\n\nThe network consists of multiple interleaved layers of convolutions, non-linear activations, local response normalizations and max pooling layers.\n\nTraining setup:\n\n* **Preprocessing**: Resize smallest dimension to 256, per-pixel mean subtraction per channel, crop $224\\text{px} \\times 224\\text{px}$ region\n* **Optimization**: Mini-Batch SGD, learning rate $= 10^{-2}$, momentum = $0.9$, 70 epochs\n* **Resources**: took around 12 days on a single GTX580 GPU\n\nThe network was evaluated on\n\n* ImageNet 2012: 14.8% error\n* Caltech-101: $86.5 \\pm 0.5$ (pretrained on ImageNet)\n* Caltech-256: $74.2\\% \\pm 0.3$ (pretrained on ImageNet)\n\n## Minor errors\n\n* typo: \"goes give\" (also: something went wrong with the link there - the whole block is a link)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1311.2901"
    },
    "672": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1503.03832",
        "transcript": "FaceNet directly maps face images to $\\mathbb{R}^{128}$ where distances directly correspond to a measure of face similarity. They use a triplet loss function. The triplet is (face of person A, other face of person A, face of person which is not A). Later, this is called (anchor, positive, negative).\n\nThe loss function is learned and inspired by LMNN. The idea is to minimize the distance between the two images of the same person and maximize the distance to the other persons image.\n\n## LMNN\n\nLarge Margin Nearest Neighbor (LMNN) is learning a pseudo-metric\n\n$$d(x, y) = (x -y) M  (x -y)^T$$\n\nwhere $M$ is a positive-definite matrix. The only difference between a pseudo-metric and a metric is that $d(x, y) = 0 \\Leftrightarrow x = y$ does not hold.\n\n## Curriculum Learning: Triplet selection\n\nShow simple examples first, then increase the difficulty. This is done by selecting the triplets.\n\nThey use the triplets which are *hard*. For the positive example, this means the distance between the anchor and the positive example is high. For the negative example this means the distance between the anchor and the negative example is low.\n\nThey want to have\n\n$$||f(x_i^a) - f(x_i^p)||_2^2 + \\alpha < ||f(x_i^a) - f(x_i^n)||_2^2$$\n\nwhere $\\alpha$ is a margin and $x_i^a$ is the anchor, $x_i^p$ is the positive face example and $x_i^n$ is the negative example. They increase $\\alpha$ over time. It is crucial that $f$ maps the images not in the complete $\\mathbb{R}^{128}$, but on the unit sphere. Otherwise one could double $\\alpha$ by simply making $f' = 2 \\cdot f$.\n\n## Tasks\n\n* **Face verification**: Is this the same person?\n* **Face recognition**: Who is this person?\n\n## Datasets\n\n* 99.63% accuracy on Labeled FAces in the Wild (LFW)\n* 95.12% accuracy on YouTube Faces DB\n\n## Network\n\nTwo models are evaluated: The [Zeiler & Fergus model](http://www.shortscience.org/paper?bibtexKey=journals/corr/ZeilerF13)  and an architecture based on the [Inception model](http://www.shortscience.org/paper?bibtexKey=journals/corr/SzegedyLJSRAEVR14).\n\n## See also\n\n* [DeepFace](http://www.shortscience.org/paper?bibtexKey=conf/cvpr/TaigmanYRW14#martinthoma)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1503.03832"
    },
    "673": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1207.0580",
        "transcript": "This paper introduced Dropout, a new layer type. It has a parameter $\\alpha \\in (0, 1)$. The output dimensionality of a dropout layer is equal to its input dimensionality. With a probability of $\\alpha$ any neurons output is set to 0. At testing time, the output of all neurons is multiplied with $\\alpha$ to compensate for the fact that no output is set to 0.\n\nA much better paper, by the same authors but 2 years later, is [Dropout: a simple way to prevent neural networks from overfitting](http://www.shortscience.org/paper?bibtexKey=journals/jmlr/SrivastavaHKSS14).\n\nDropout can be interpreted as training an ensemble of many networks, which share weights.\n\nIt was notably used by [ImageNet Classification with Deep Convolutional Neural Networks](http://www.shortscience.org/paper?bibtexKey=krizhevsky2012imagenet).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1207.0580"
    },
    "674": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=Fuk80",
        "transcript": "This was an inspiration for CNNs (convolutional neural networks).\n\nSee also:\n\n* [Neocognitron](https://en.wikipedia.org/wiki/Neocognitron)",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "675": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/IandolaMAHDK16",
        "transcript": "This paper is about the reduction of model parameters while maintaining (most) of the models accuracy.\n\nThe paper gives a nice overview over some key findings about CNNs. One part that is especially interesting is \"2.4. Neural Network Design Space Exploration\".\n\n## Model compression\n\nKey ideas for model compression are:\n\n* singular value decomposition (SVD)\n* replace parameters that are below a certain threshold with zeros to form a sparse matrix\n* combining Network Pruning with quantization (to 8 bits or less)\n* huffman encoding (Deep Compression)\n\nIdeas used by this paper are\n\n* Replacing 3x3 filters by 1x1 filters\n* Decrease the number of input channels by using **squeeze layers**\n\nOne key idea to maintain high accuracy is to downsample late in the network. This means close to the input layer, the layer parameters have stride = 1, later they have stride > 1.\n\n\n## Fire module\n\nA Fire module is a squeeze convolution layer (which has only $n_1$ 1x1 filters), feeding into an expand layer that has a mix of $n_2$ 1x1 and $n_3$ 3x3 convolution filters. It is chosen \n$$n_1 < n_2 + n_3$$\n(Why?)\n\n(to be continued)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.07360"
    },
    "676": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1411.4038",
        "transcript": "## Terms\n\n* Semantic Segmentation: Traditional segmentation divides the image in visually similar patches. Semantic segmentation on the other hand divides the image in semantically meaningful patches. This usually means to classify each pixel (e.g.: This pixel belongs to a cat, that pixel belongs to a dog, the other pixel is background).\n\n\n## Main ideas\n\n* Complete neural networks which were trained for image classification can be used as a convolution. Those networks can be trained on Image Net (e.g. VGG, AlexNet, GoogLeNet)\n* Use upsampling to (1) reduce training and prediction time (2) improve consistency of output. (See [What are deconvolutional layers?](http://datascience.stackexchange.com/a/12110/8820) for an explanation.)\n\n\n## How FCNs work\n\n1. Train a neural network for image classification which is trained on input images of a fixed size ($d \\times w \\times h$)\n2. Interpret the network as a single convolutional filter for each output neuron (so $k$ output neurons means you have $k$ filters) over the complete image area on which the original network was trained.\n3. Run the network as a CNN over an image of any size (but at least $d \\times w \\times h$) with a stride $s \\in \\mathbb{N}_{\\geq 1}$\n4. If $s > 1$, then you need an upsampling layer (deconvolutional layer) to convert the coarse output into a dense output.\n\n## Nice properties\n\n* FCNs take images of arbitrary size and produce an image of the same output size.\n* Computationally efficient\n\n## See also:\n\nhttps://www.quora.com/What-are-the-benefits-of-converting-a-fully-connected-layer-in-a-deep-neural-network-to-an-equivalent-convolutional-layer\n\n> They allow you to treat the convolutional neural network as one giant filter. You can then spatially apply the neural net as a convolution to images larger than the original training image size, getting a spatially dense output.\n>\n> Let's say you train a neural net (with some loss function) with a convolutional layer (3 x 3, stride of 2), pooling layer (3 x 3, stride of 2), and a fully connected layer with 10 units, using 25 x 25 images. Note that the receptive field size of each max pooling unit is 7 x 7, so the pooling output is 5 x 5. You can convert the fully connected layer to to a set of  10 5 x 5 convolutional filters (unit strides). If you do that, the entire net can be treated as a filter with receptive field size 35 x 35 and stride of 4. You can then take that net and apply it to a 50 x 50 image, and you'd get a 3 x 3 x 10 spatially dense output.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1411.4038"
    },
    "677": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/TaigmanYRW14",
        "transcript": "## General stuff about face recognition\n\nFace recognition has 4 main tasks:\n\n* **Face detection**: Given an image, draw a rectangle around every face\n* **Face alignment**: Transform a face to be in a canonical pose\n* **Face representation**: Find a representation of a face which is suitable for follow-up tasks (small size, computationally cheap to compare, invariant to irrelevant changes)\n* **Face verification**: Images of two faces are given. Decide if it is the same person or not.\n\nThe face verification task is sometimes (more simply) a face classification task (given a face, decide which of a fixed set of people it is).\n\nDatasets being used are:\n\n* **LFW** (Labeled Faces in the Wild): 97.35% accuracy; 13 323 web photos of 5 749 celebrities\n* **YTF** (YouTube Faces): 3425 YouTube videos of 1 595 subjects\n* **SFC** (Social Face Classification): 4.4 million labeled faces from 4030 people, each 800 to 1200 faces\n* **USF** (Human-ID database): 3D scans of faces\n\n## Ideas in this paper\n\nThis paper deals with face alignment and face representation.\n\n**Face Alignment**\n\nThey made an average face with the USF dataset. Then, for each new face, they apply the following procedure:\n\n* Find 6 points in a face (2 eyes, 1 nose tip, 2 corners of the lip, 1 middle point of the bottom lip)\n* Crop according to those\n* Find 67 points in the face / apply them to a normalized 3D model of a face\n* Transform (=align) face to a normalized position\n\n**Representation**\n\nTrain a neural network on 152x152 images of faces to classify 4030 celebrities. Remove the softmax output layer and use the output of the second-last layer as the transformed representation.\n\nThe network is:\n\n* C1 (convolution): 32 filters of size $11 \\times 11 \\times 3$ (RGB-channels) (returns $142\\times 142$ \"images\")\n* M2 (max pooling): $3 \\times 3$, stride of 2  (returns $71\\times 71$ \"images\")\n* C3 (convolution): 16 filters of size $9 \\times 9 \\times 16$ (returns $63\\times 63$ \"images\")\n* L4 (locally connected): $16\\times9\\times9\\times16$ (returns $55\\times 55$ \"images\")\n* L5 (locally connected): $16\\times7\\times7\\times16$ (returns $25\\times 25$ \"images\")\n* L6 (locally connected): $16\\times5\\times5\\times16$ (returns $21\\times 21$ \"images\")\n* F7 (fully connected): ReLU, 4096 units\n* F8 (fully connected): softmax layer with 4030 output neurons\n\nThe training was done with:\n\n* Stochastic Gradient Descent (SGD)\n* Momentum of 0.9\n* Performance scheduling (LR starting at 0.01, ending at 0.0001)\n* Weight initialization: $w \\sim \\mathcal{N}(\\mu=0, \\sigma=0.01)$, $b = 0.5$\n* ~15 epochs ($\\approx$ 3 days) of training\n\n\n## Evaluation results\n\n* **Quality**:\n  * 97.35% accuracy (or mean accuracy?) with an Ensemble of DNNs for LFW\n  * 91.4% accuracy with a single network on YTF\n* **Speed**: DeepFace runs in 0.33 seconds per image (I'm not sure which size). This includes image decoding, face detection and alignment, **the** feed forward network (why only one? wasn't this the best performing Ensemble?) and final classification output\n\n## See also\n\n* Andrew Ng: [C4W4L03 Siamese Network](https://www.youtube.com/watch?v=6jfw8MuKwpI)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2014.220"
    },
    "678": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jmlr/GlorotB10",
        "transcript": "The main contribution of [Understanding the difficulty of training deep feedforward neural networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf) by Glorot et al. is a **normalized weight initialization**\n\n$$W \\sim U \\left [  - \\frac{\\sqrt{6}}{\\sqrt{n_j + n_{j+1}}}, \\frac{\\sqrt{6}}{\\sqrt{n_j + n_{j+1}}} \\right ]$$\n\nwhere $n_j \\in \\mathbb{N}^+$ is the number of neurons in the layer $j$.\n\nShowing some ways **how to debug neural networks** might be another reason to read the paper.\n\nThe paper analyzed standard multilayer perceptrons (MLPs) on a artificial dataset of $32 \\text{px} \\times 32 \\text{px}$ images with either one or two of the 3 shapes: triangle, parallelogram and ellipse. The MLPs varied in the activation function which was used (either sigmoid, tanh or softsign).\n\nHowever, no regularization was used and many mini-batch epochs were learned. It might be that batch normalization / dropout might change the influence of initialization very much.\n\nQuestions that remain open for me:\n\n* [How is weight initialization done today?](https://www.reddit.com/r/MLQuestions/comments/4jsge9)\n* Figure 4: Why is this plot not simply completely dependent on the data?\n* Is softsign still used? Why not?\n* If the only advantage of softsign is that is has the plateau later, why doesn't anybody use $\\frac{1}{1+e^{-0.1 \\cdot x}}$ or something similar instead of the standard sigmoid activation function?",
        "sourceType": "blog",
        "linkToPaper": "http://www.jmlr.org/proceedings/papers/v9/glorot10a.html"
    },
    "679": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/kdd/Domingos99",
        "transcript": "MetaCost is a meta-algorithm which makes error-based classifiers making their decision based on the cost of errors. For example, sending advertisement is cheap, so it might be worth a lot of false positives to get a single person who is actually interested in the advertisement.\n\nThe algorithm is given in pseudocode in the paper.\n\nImportant notation:\n\n* $C(i, j)$: Cost of predicting an example belongs to class $i$, where in fact it belongs to class $j$. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/312129.312220"
    },
    "680": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/RenHGS15",
        "transcript": "**Object detection** is the task of drawing one bounding box around each instance of the type of object one wants to detect. Typically, image classification is done before object detection. With neural networks, the usual procedure for object detection is to train a classification network, replace the last layer with a regression layer which essentially predicts pixel-wise if the object is there or not. An bounding box inference algorithm is added at last to make a consistent prediction (see [Deep Neural Networks for Object Detection](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf)).\n\nThe paper introduces RPNs (Region Proposal Networks). They are end-to-end trained to generate region proposals.They simoultaneously regress region bounds and bjectness scores at each location on a regular grid.\n\nRPNs are one type of fully convolutional networks. They take an image of any size as input and output a set of rectangular object proposals, each with an objectness score.\n\n## See also\n* [R-CNN](http://www.shortscience.org/paper?bibtexKey=conf/iccv/Girshick15#joecohen)\n* [Fast R-CNN](http://www.shortscience.org/paper?bibtexKey=conf/iccv/Girshick15#joecohen)\n* [Faster R-CNN](http://www.shortscience.org/paper?bibtexKey=conf/nips/RenHGS15#martinthoma)\n* [Mask R-CNN](http://www.shortscience.org/paper?bibtexKey=journals/corr/HeGDG17)",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks"
    },
    "681": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/GirshickDDM14",
        "transcript": "The [R-CNN](http://arxiv.org/abs/1311.2524) paper presents a method based on convolutional neural networks (CNNs) for object detection. It does so by region proposals (hence the \"R\"). The key insight was to train CNNs on classification tasks and use the learned features for the region proposals. The do *not* use a sliding window approach such as Overfeat. They create around 2000 category-independent region proposals. For each proposal, they crop the part of that image. Then they resize the cropped part to fit into the CNN and classify it.\n\n\nNotable follow-ups are:\n\n* [Fast R-CNN](http://www.shortscience.org/paper?bibtexKey=conf/iccv/Girshick15)\n* [Faster R-CNNs](http://www.shortscience.org/paper?bibtexKey=conf/nips/RenHGS15)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/CVPR.2014.81"
    },
    "682": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SzegedyTE13",
        "transcript": "**Object detection** is the task of drawing one bounding box around each instance of the type of object one wants to detect. Typically, image classification is done before object detection. With neural networks, the usual procedure for object detection is to train a classification network, replace the last layer with a regression layer which essentially predicts pixel-wise if the object is there or not. An bounding box inference algorithm is added at last to make a consistent prediction.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection"
    },
    "683": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=krizhevsky2012imagenet",
        "transcript": "This paper is about Convolutional Neural Networks for Computer Vision. It was the first break-through in the ImageNet classification challenge (LSVRC-2010, 1000 classes).\n\nReLU was a key aspect which was not so often used before. The paper also used Dropout in the last two layers.\n\n## Training details\n\n* Momentum of 0.9\n* Learning rate of $\\varepsilon$ (initialized at 0.01)\n* Weight decay of $0.0005 \\cdot \\varepsilon$.\n* Batch size of 128\n* The training took 5 to 6 days on two NVIDIA GTX 580 3GB GPUs.\n\n## See also\n\n* [Stanford presentation](http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf)",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "684": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeZRS15",
        "transcript": "Deeper networks should never have a higher **training** error than smaller ones. In the worst case, the layers should \"simply\" learn identities. It seems as this is not so easy with conventional networks, as they get much worse with more layers. So the idea is to add identity functions which skip some layers. The network only has to learn the **residuals**. \n\nAdvantages:\n\n* Learning the identity becomes learning 0 which is simpler\n* Loss in information flow in the forward pass is not a problem anymore\n    * No vanishing / exploding gradient\n* Identities don't have parameters to be learned\n\n## Evaluation\n\nThe learning rate starts at 0.1 and is divided by 10 when the error plateaus. Weight decay of 0.0001 ($10^{-4}$), momentum of 0.9. They use mini-batches of size 128.\n\n* ImageNet ILSVRC 2015: 3.57% (ensemble)\n* CIFAR-10: 6.43%\n* MS COCO: 59.0% mAp@0.5 (ensemble)\n* PASCAL VOC 2007: 85.6% mAp@0.5\n* PASCAL VOC 2012: 83.8% mAp@0.5\n\n## See also\n\n* [DenseNets](http://www.shortscience.org/paper?bibtexKey=journals/corr/1608.06993)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.03385"
    },
    "685": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=huang2016networks",
        "transcript": "**Dropout for layers** sums it up pretty well. The authors built on the idea of [deep residual networks](http://arxiv.org/abs/1512.03385) to use identity functions to skip layers. \n\nThe main advantages:\n\n* Training speed-ups by about 25%\n* Huge networks without overfitting\n\n## Evaluation\n\n* [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html): 4.91% error ([SotA](https://martin-thoma.com/sota/#image-classification): 2.72 %) Training Time: ~15h\n* [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html): 24.58% ([SotA](https://martin-thoma.com/sota/#image-classification): 17.18 %) Training time: < 16h\n* [SVHN](http://ufldl.stanford.edu/housenumbers/):  1.75% ([SotA](https://martin-thoma.com/sota/#image-classification): 1.59 %) - trained for 50 epochs, begging with a LR of 0.1, divided by 10 after 30 epochs and 35. Training time: < 26h",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.09382"
    },
    "686": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.03530",
        "transcript": "## Summary     \nThe broad goal of this paper is to understand how a neural network learns the underlying distribution of the input data and the properties of the network that describes its generalization power.\n\nPrevious literature tries to use statistical measures like Rademacher complexity, uniform stability and VC dimension to explain the generalization error of the model. These methods explain generalization in terms of the number of parameters in the model along with the applied regularization.  The experiments  performed in the [Section 2] of the paper show that the learning capacity of a CNN cannot be sufficiently explained by traditional statistical learning theory. Even the effect of different regularization strategies in CNN is shown to be potentially unrelated to the generalization error, which contradicts the theory behind VC dimension. \n\nThe experiments of the paper show that the model is able to learn some underlying patterns for random labels and input with different amounts of gaussian noise. When the authors gradually increase the noise in the inputs the generalization error gradually increases while the training error is still able to reach zero. The authors have concluded that big networks are able to completely memorise the complete dataset.\n                 \n## Personal Thoughts                                       \n1) Firstly we need a new theory to explain why and how CNN memorizes the inputs and generalizes itself to new data. Since the paper shows that regularization doesn't have too much effect on the generalization for big networks, maybe the network is actually memorizing the whole input space. But the memorization is very strategic in the sense that only the inputs (eg. noise) where no underlying simple features are found, are completely memorized unlike inputs with a stronger signal where patterns can be found. This may explain the discrepancy in number of training steps between \u2018true labels\u2019 and noisy inputs in [Figure 1 a.]. My very general understanding of Information Bottleneck Hypothesis [4] is that  networks compresses noisy input data as much as possible while preserving important information. For a network more time is taken to compress noise compared to strong signals in images. This may give some intuision behind the learning process taking place.                              \n\n2) CNN is highly non-linear with millions of parameters and has a very complex loss landscape. There might be multiple minima and we need a theory to explain which of these minima gives the highest generalization. Unfortunately the working of SGD is still a black box and is very difficult to characterize. There are many interesting phenomena like adversarial attacks, effect of optimizer used on the weights found (Daniel Jiwoong et al., 2016) and the actual understanding of non-linearity in CNN (Ian J. Goodfellow et al., 2015) that all point to lapses in our overall understanding of very high dimensional manifolds. This requires rigorous experimentation to study and understand the effect of the network architecture, optimizer and the actual input (Nitish Shirish et al.,2017) to the network independently on generalization.              \n           \n## References                        \n1.  Im, Daniel Jiwoong et al. \u201cAn empirical analysis of the optimization of deep network loss surfaces.\u201d arXiv: Learning (2016): n. pag.                  \n2. Goodfellow, Ian J. and Oriol Vinyals. \u201cQualitatively characterizing neural network optimization problems.\u201d CoRR abs/1412.6544 (2015): n. pag.                \n3.  Keskar, Nitish Shirish et al. \u201cOn Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.\u201d ArXiv abs/1609.04836 (2017): n. pag.                 \n4. https://www.youtube.com/watch?v=XL07WEc2TRI                   ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.03530"
    },
    "687": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/KumraK16",
        "transcript": "# **Introduction**\n\n### **Goal of the paper**\n* The goal of this paper is to use an RGB-D image to find the best pose for grasping an object using a parallel pose gripper.\n* The goal of this algorithm is to also give an open loop method for manipulation of the object using vision data.\n\n### **Previous Research**\t\n*  Even the state of the art in grasp detection algorithms fail under real world circumstances and cannot work in real time.\n* To perform grasping a 7D grasp representation is used. But usually a 5D grasping representation is used and this is projected back into 7D space.\n* Previous methods directly found the 7D pose representation using only the vision data.\n* Compared to older computer vision techniques like sliding window classifier deep learning methods are more robust to occlusion , rotation and scaling.\n* Grasp Point detection gave high accuracy (> 92%) but was helpful for only grasping cloths or towels.\n\n### **Method**\n* Grasp detection is generally a computer vision problem.\n* The algorithm given by the paper made use of computer vision to find the grasp as a 5D representation. The 5D representation is faster to compute and is also less computationally intensive and can be used in real time.\n* The general grasp planning algorithms can be divided into three distinct sequential phases ;\n    1. Grasp detection\n    1. Trajectory planning\n    1. Grasp execution\n* One of the most major tasks in grasping algorithms is to find the best place for grasping and to map the vision data to coordinates that can be used for manipulation.\n* The method makes use of three neural networks :\n    1. 50 deep neural network (ResNet 50) to find the features in RGB image. This network is pretrained on the ImageNet dataset.\n    1. Another neural network to find the feature in depth image.\n    1. The output from the two neural networks are fed into another network that gives the final grasp configuration as the output.\n* The robot grasping configuration can be given as a function of the x,y,w,h and theta where (x,y) are the centre of the grasp rectangle and theta is the angle of the grasp rectangle.\n* Since very deep networks are being used (number of layers > 20) , residual layers are used that helps in improving the loss surface of the network and reduce the vanishing gradient problems.\n* This paper gives two types of networks for the grasp detection ;\n    1. Uni-Modal Grasp Predictor\n        * These use only an RGB 2D image to extract the feature from the input image and then use the features to give the best pose.\n        * A Linear - SVM is used as the final classifier to classify the best pose for the object.\n    1. Multi-Modal Grasp Predictor \n        * This model makes use of both the 2D image and the RGB-D image to extract the grasp.\n        * RGB-D image is decomposed into an RGB image and a depth image.\n        * Both the images are passed through the networks and the outputs are the  combined together to a shallow CNN.\n        * The output of the shallow CNN is the best grasp for the object. \n\n### **Experiments and Results**\n* The experiments are done on the Cornell Grasp dataset.\n* Almost no or minimum preprocessing is done on the images except resizing the image.\n* The results of the algorithm given by this paper are compared to unimodal methods that use only RGB images.\n* To validate the model it is checked if the predicted angle of grasp is less than 30 degrees and that the Jaccard similarity is more than 25% of the ground truth label. \n\n### **Conclusion**\n* This paper shows that Deep-Convolutional neural networks can be used to predict the grasping pose for an object.\n* Another major observation is that the deep residual layers help in better extraction of the features of the grasp object from the image.\n* The new model was able to run at realtime speeds.\n* The model gave state of the art results on Cornell Grasping dataset.\n\n----\n### **Open research questions**\n* Transfer Learning concepts to try the model on real robots.\n* Try the model in industrial environments on objects of different sizes and shapes.\n* Formulating the grasping problem as a regression problem.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1611.08036"
    },
    "688": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1710-10196",
        "transcript": "\n## **Keywords**\n\nProgressive GAN , High resolution generator\n\n---\n\n## **Summary**\n\n1.  **Introduction**\n    1.  **Goal of the paper**\n        1.  Generation of very high quality images using progressively increasing size of the generator and discriminator.\n        1.  Improved training and stability of GANs.\n        1.  New metric for evaluating GAN results.\n        1.  A high quality version of CELEBA-HQ dataset.\n    1. **Previous Research**\n        1.  Generative methods help to produce new samples from higher-dimensional data distributions such as images .\n        1.  The common approaches for generative methods are :\n            1.  Autoregressive models : Produce sharp images and are slow to evaluate. eg PixelCNN\n            1.  Variational Autoencoders : Easy to train but produces blurry images.\n            1.  Generative Adversarial Neural Network : Produces sharp images at small resolutions but are highly unstable.\n1.  **Method**\n    1.  **Basic GAN architecture**\n        1.  Gan consists of two major parts :\n            1.  _Generator_ : Creates a sample image from latent code which look very close to the training images.\n            1.  _Discriminator_: Discriminator is trained to assess how close the sample image looks to the training image.\n        1.  To measure the overlap between the training and the generated distributions many methods are used like Jensen-Shannon divergence , least-squares divergence and Wasserstein Distance.\n        1.  Larger resolution generations cause problems because it becomes difficult for both the training and the generated networks amplifying the gradient problem. Larger resolutions also require large memory and can cause problems.\n        1.  A mechanism is also proposed to stop the generator from participating in escalation that causes mode collapse problem.\n\n    1.  **Progressive growing of GANs**\n        1.  The primary method for the GAN training is to start off from a low resolution image and add extra layers in each step of the training process.\n        1.  Lower resolution images are more stable as they have very less class information and as the resolution of the image increases further smaller details and features are added to the image.\n        1.  This leads to a smooth increase in the quality of image instead of the network learning lot of details in one single step.\n    1.  **Mini-batch separation**\n        1.  GANs tend to capture only a very small set of features from the image.\n        1.  \"Minibatch discrimination\" is used to generate feature vector for each individual image along with one for the the mini batch of images also.\n        \n          ![alt_text](https://i.imgur.com/dHFl5OV.png \"image_tooltip\")\n1.  **Conclusion**\n    1.  Higher resolution images are able to be generated which are robust and efficient.\n    1.  Improved quality of the generated images is given.\n    1.  Reduced training time for a comparable result and output quality and resolution.\n\n\n\n---\n\n\n\n## **Notes**\n\n\n\n*   Gradient Problem : At higher resolutions it becomes easier to tell the differences between the training and the testing images [1]. This is referred to as the gradient problem.\n*   Mode Collapse : The generator is incapable of creating a large variety of samples and get stuck.\n\n\n## **Open research questions**\n\n\n\n1.  Improved methods for a true photorealism generation of images.\n1.  Improved semantic sensibility and improved understanding of the dataset.\n\n## **References**\n\n\n\n1.  [https://blog.acolyer.org/2018/05/10/progressive-growing-of-gans-for-improved-quality-stability-and-variation/](https://blog.acolyer.org/2018/05/10/progressive-growing-of-gans-for-improved-quality-stability-and-variation/) \n1.  [https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1710.10196"
    },
    "689": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.08864",
        "transcript": "\n## **Keywords**\n\nOne pixel attack , adversarial examples , differential evolution , targeted and non-targeted attack\n\n---\n\n\n## **Summary**\n\n1.  **Introduction **\n    1.  **Basics**\n        1.  Deep learning methods are better than the traditional image processing techniques in most of the cases in computer vision domain.\n        1.  \"Adversarial examples\" are specifically modified images with imperceptible perturbations  that are classified wrong by the network.\n    1.  **Goals of the paper**\n        1.  In most of the older techniques excessive modifications are made on the images and it may become perceivable to the human eyes. The authors of the paper suggest a method to create adversarial examples by changing only one , three or five pixels of the image.\n        1.  Generating examples under constrained conditions can help in _getting insights about the decision boundaries_ in the higher dimensional space.\n    1.  **Previous Work**\n        1.  Methods to create adversarial examples :\n            1.  Gradient-based algorithms using backpropagation for obtaining gradient information \n            1.  \"fast gradient sign\" algorithm \n            1.  Greedy perturbation searching method\n            1.  Jacobian matrix to build \"Adversarial Saliency Map\"\n        1.  Understanding and visualizing the decision boundaries of the DNN input space.\n        1.  Concept of \"Universal perturbations\" , a perturbation that when added to any natural image can generate adversarial samples with high effectiveness\n1.  **Advantages of the new types of attack **\n    1.  _Effectiveness_ : One pixel modification  with efficiency ranging from 60% - 75%.\n    1.  _Semi-Black-Box attack _: Requires only black-box feedback (probability labels) , no gradient and network architecture required.\n    1.  _Flexibility_ : Can generalize between different types of network architectures.\n1.  **Methodology**\n    1.  Finding the adversarial example as an optimization problem with constraints.** **\n    1.  _Differential evolution_\n        1.  _\"Differential evolution\" _, a general kind of  evolutionary algorithms , used to solve multimodal optimization problems.\n        1.  Does Not make use of gradient information\n        1.  Advantages of DE for generating adversarial images :\n            1.  _Higher probability of finding the global optima_\n            1.  _Requires less information from the target system_\n            1.  _Simplicity_ : Independent of the classifier \n1.  **Results **\n    1.  CIFAR-10 dataset was selected with 3 types of networks architectures , all  convolution  network  ,  Network  in  Network  and  VGG16 network . 500 random images were selected to create the perturbations and run both _targeted_ and_ non-targeted attack._\n    1.  Adversarial examples were created with only one pixel change in some cases and with 3 and 5 pixel changes in other cases.\n    1.  The attack was generalized over different architectures.\n    1.  Some specific target-pair classes are more vulnerable to attack compared to the others.\n    1.  Some classes are very difficult to perturb to other classes and some cannot be changed at all.\n    1.  Robustness of the class against attack can be broken by using higher dimensional perturbations.\n    \n1.  **Conclusion**\n    1.  Few pixels are enough to fool different types of  networks.\n    1.  The properties of the targeted perturbation depends on its decision boundary.\n    1.  Assumptions made that small changes addictive perturbation on the values of many dimensions will accumulate and cause huge change to the output , might not be necessary for explaining why natural images are sensitive to small perturbation.\n\n\n\n---\n\n\n## **Notes **\n\n\n\n*   Location of data points near the decision boundaries might affect the robustness against perturbations.\n    *   If the boundary shape is wide enough it is possible to have natural images far away from the boundary such that it is hard to craft adversarial images from it.  \n    *   If the boundary shape is mostly long and thin with natural images close to the border, it is easy to craft adversarial images from them but hard to craft adversarial images to them.\n*   The data points are moved in small steps and the change in the class probabilities are observed.\n\n## **Open research questions**\n\n\n\n1.  Effect of a larger set of initial candidate solutions( Training images) to finding the adversarial image?\n1.  Generate better adversarial examples by having more iterations of Differential evolution?\n1.  Why imbalances occur when creating perturbations?\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1710.08864"
    },
    "690": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1503.03832",
        "transcript": "\n## Keywords\nTriplet-loss , face embedding , harmonic embedding\n\n---\n## Summary\n\n### Introduction\n\n**Goal of the paper**    \nA unified system is given for face verification , recognition and clustering.\nUse of a 128 float  pose and illumination invariant feature vector or embedding in the euclidean space.\n* Face Verification : Same faces of the person gives feature vectors that have a very close L2 distance between them. \n* Face recognition : Face recognition becomes a clustering task in the embedding space\n\t\n**Previous work**    \n* Previous use of deep learning made use of an bottleneck layer to represent face as an embedding of  1000s  dimension  vector.\n* Some other techniques use PCA to reduce the dimensionality of the embedding for comparison.\n\n**Method**\n* This method makes use of inception style CNN to get an embedding of each face.\n* The thumbnails of the face image are the tight crop of the face area with only scaling and translation done on them.\n\t\n**Triplet Loss**\nTriplet loss  makes use of two matching face thumbnails and a non-matching thumbnail. The loss function tries to reduce the distance between the matching pair while increasing the separation between the the non-matching pair of images.\n\n**Triplet Selection**\n* Selection of triplets is done such that samples are hard-positive or hard-negative .\n* Hardest negative can lead to local minima early in the training and a collapse model in a few cases\n* Use of semi-hard negatives help to improve the convergence speed while at the same time reach nearer to the global minimum.\n\n**Deep Convolutional Network**\n* Training is done using SGD (Stochastic gradient descent) with Backpropagation and AdaGrad\n* The training is done on two networks :\n    - Zeiler&Fergus architecture with model depth of 22 and 140 million parameters\n    - GoogLeNet style inception model with 6.6 to 7.5 million parameters.\n\n**Experiment**\n* Study of the following cases are done :\n    - Quality of the jpeg image : The validation rate of model improves with the JPEG quality upto a certain threshold.\n\n    - Embedding dimensionality : The dimension of the embedding increases from 64 to 128,256 and then gradually starts to decrease at 512 dimensions. \n\n    - No. of images in the training data set \n\n\n\n**Results classification accuracy** :\n   - LFW(Labelled faces in the wild) dataset : 98.87% 0.15\n   - Youtube Faces DB : 95.12%  .39\nOn clustering tasks the model was able to work on a wide varieties of face images and is invariant to pose , lighting and also age.\n\n**Conclusion**\n\n* The model can be extended further to improve the overall accuracy.\n* Training networks to run on smaller systems like mobile phones.\n* There is need for improving the training efficiency.\n\n---\n\n## Notes \n\n* Harmonic embedding is a set of embedding that we get from different models but are compatible to each other. This helps to improve future upgrades and transitions to a newer model\n\n* To make the embeddings compatible with different models , harmonic-triplet loss and the generated triplets must be compatible with each other\n\n## Open research questions\n\n* Better understanding of the error cases.\n* Making the model more compact for embedded and mobile use cases.\n* Methods to reduce the training times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1503.03832"
    },
    "691": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1312.6199",
        "transcript": "### Keywords\nAdversarial example ,  Perturbations \n\n------\n\n### Summary\n\n##### Introduction \n\n* Explain two properties of  neural network that cause it to misclassify images and cause  difficulty to get solid understanding of network.\n1. Theoretical understanding  of the individual high level unit of a network and a combination of these units or layers.\n2. Understanding the continuity of input - output mapping space and the stability of the output wrt. the input.\n\n* Performing a few experiments on different networks and architectures\n1. MNIST dataset - Autoencoder , Fully Connected net\n2. ImageNet - \u201cAlexNet\u201d\n3. 10M youtube images - \u201cQuocNet\u201d  \n\n##### Understanding individual units of the Network \n\n* Previous work used individual images to maximize the activation value of each feature unit.\nSimilar experiment was done by the authors on the MNIST data set.\n\n* The interpretation of the results are as following ;\n1. Random direction vector (V)  gives rise to similarly interpretable semantic properties.\n2. Each feature unit is able to generate invariance on a particular subset of input   distribution.\nhttps://i.imgur.com/SeyXJeV.png\n\n##### Blind spots in the neural network\n\n* Output layers are highly non-linear and are able to give a nonlinear generalization over the input space.\n* It is possible for the output layers to give non-significant probabilities to regions of the input space that contain no training examples in their vicinity. Ie. It is possible to obtain probability of the different viewpoints of the object without training.\n* Deep learning kernel methods can't be assumed to have smooth decision boundaries. \n* Using optimization techniques, small changes to the image can lead to very large deviations in the output\n* __\u201cAdversarial examples\u201d__ represent pockets or holes in the input-space which are difficult to find simply moving around the input images.\n\n\n##### Experimental Results\n* Adversarial examples that are indistinguishable from the actual image can be created for all networks.\n1. Cross model generalization : Adversarial images created for one network can affect the other networks also.\n2. Cross training generalization \n\nhttps://i.imgur.com/drcGvpz.png\n\n##### Conclusion \n* Neural network have a counter intuitive properties wrt. the working of the individual units and discontinuities.\n* Occurance of the adversarial examples and its properties. \n\n-----\n### Notes \n\n* Feeding adversarial examples during the model training can improve the generalization of the model.\n* The adversarial examples on the higher layers are more effective than those of input and lower layers.\n* Adversarial examples affect models trained with different hyper parameters.\n* According to the the test conducted , autoencoders are more resilient to the adversarial examples.\n* Deep learning networks which are trained from purely supervised training are unstable to a few particular types of perturbations. Small addition of perturbations to the input leads to large perturbations at the output of the last layers.\n\n### Open research questions\n\n[1] Comparing the effects of adversarial examples on lower layers to that of the higher layers.     \n[2] Dependence of the adversarial attacks on training data set of the model.     \n[3] Why the adversarial examples generalize across different hyperparameters or training sets.     \n[4] How often do adversarial example occur?      \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1312.6199"
    },
    "692": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1604.03540",
        "transcript": "The problem statement this paper tries to address  is that the training set is distinguished by a large imbalance between the number of foreground examples and background examples-To make the point concrete cases like sliding window object detectors like deformable parts model, the imbalance may be as extreme as 100,000 background examples to one annotated foreground example.\n\nBefore i proceed to give you the details of Hard Example mining, i just want to note that HEM in its essence is mostly while training you sort your losses and train your model on the most difficult examples which mostly means the ones with the most loss.(An extension to this can be found in the paper Focal Loss). This is a simple but powerful technique. \n\nSo taking this as out background,The authors propose a simple but effective method to train an Fast-RCNN.\nTheir approach is as follows,\n1. For an input image at SGD iteration t, they first compute a convolution feature map using the conv-Network\n2. The ROI Network uses this feature map and all the input ROI's to do a forward pass\n3. Hard examples are sorted by loss and taking the B/N examples for which the current network performs worse.(Here B is batch size and N is Number of examples)\n\n4. While doing this, The researchers notice that Co-located ROI's with high overlap are likely to have co-related losses. Also If you notice Overlapping ROI's will project onto the mostly the same region in the Conv-feature  map because the feature map is a denser/smaller representation of the feature map.So this might lead to loss double counting.To deal with this They use standard Non-Maximum Supression.\n\n5. Now how NMS works here is, It iteratively selects the ROI with the highest loss and removes all lower loss ROI's that have high overlap with the selected region.Here they use a IOU threshold of 0.7\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1604.03540"
    },
    "693": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1708.02002",
        "transcript": "In object detection the boost in speed and accuracy is mostly gained through network architecture changes.This paper takes a different route towards achieving that goal,They introduce a new loss function called focal loss.\n\nThe authors identify class imbalance as the main obstacle toward one stage detectors achieving results which are as good as two stage detectors.\n\nThe loss function they introduce is a dynamically scaled cross entropy loss,Where the scaling factor decays to zero as the confidence in the correct class increases.\n\nThey add a modulating factor  as shown in the image below to the cross- entropy loss  https://i.imgur.com/N7R3M9J.png\nWhich ends up looking like this https://i.imgur.com/kxC8NCB.png\nin experiments though they add an additional alpha term to it,because it gives them better results.\n\n**Retina Net**\n\nThe network consists of a single unified network which is composed of a backbone network and two task specific subnetworks.The backbone network computes the feature maps for the input images.The first sub-network helps in object classification of the backbone networks output and the second sub-network helps in bounding box regression.\nThe backbone network they use is Feature Pyramid Network,Which they build on top of ResNet.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1708.02002"
    },
    "694": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HermannKGEKSB15",
        "transcript": "https://i.imgur.com/mYFkCxk.png\nMain contributions:\nThe paper proposed a new method to provide large scale supervised reading comprehension and also developing attention based deep neural networks that can answer complex questions from real documents.\n\nImportance:\nObtaining supervised natural language comprehensive data in large scale is difficult. On the other hand, reading comprehension methods constructed based on synthetic data failed in real environment when facing real data. This work addresses lack of real supervised reading comprehension data. In addition, they build novel deep learning models for reading comprehension by incorporating attention mechanism into recurrent neural networks. Attention mechanism allows a model to focus on the parts of a document that it believes will help it answer a question.\n\nMethod:\nFirst part, two machine reading corpora is created by exploiting CNN and Daily Mail articles along with their corresponding summaries in form of the bullet points. These bullet points are abstractive and they are paraphrasing important parts of the article rather than copying sentences from the text. The bullet points turn into Cloze type questions by replacing one entity at a time with an entity marker, for example, \u201cproducer X will not press charges against ent212 ,his lawyer says.\u201d. All the entities are replaced by entity markers and using a coreference and also entity markers are permuted for each data points to avoid world knowledge and co-occurrence effects in the reading comprehension.\n\nSecond part, For the reading comprehension task, they used 2 simple base line models(A), 2 symbolic matching models(B), and 4 recurrent neural networks models(C):\nA1) Majority Baseline: It picks the most frequently observed entity in the context document.\n\nA2) Exclusive Majority: It picks the most frequently observed entity in the context document which is not observed in the query.\n\nB1) Frame-Semantic Parsing: This method parses the sentence to find \"who did what to whom\" using state-of-the-art frame semantic parser on the anonymized data points.\n\nB2) Word Distance Benchmark: It aligns placeholder of Cloze form questions with each possible entity in the context document and calculates the distance between the question and the context around the aligned entity. Then sum of the distance of every word in a query to their nearest aligned word in the document is calculated.\n\nC1) Deep LSTM Reader (2-layer LSTM)\nThis model feeds the [document | query] pair separated by a delimiter as a single large document, one word at a time. LSTM cells have skip connections from input to hidden layers and hidden layer to output.\n\nC2) Attentive Reader (bi-directional LSTM with attention)\nThis model employs attention mechanism to overcome the bottleneck of fixed width hidden vector. First, it encodes the document and the query using separate bi-directional single layer LSTM. Then, query encoding is obtained by concatenating the final forward and backwards outputs. Document encoding is obtained by a weighted sum of output vectors (obtained by concatenating the forward and backwards outputs). The weights can be interpreted as the degree to which the network attends to a particular token in the document. Finally, the model is completed by defining a non-linear combination of document and query embedding.\n\nC3) Uniform Reader (bi-directional LSTM)\nIt is Attentive Reader without attention mechanism, which is used here to see the effect of attention mechanism on the results.\n\nC4) Impatient Reader (bi-directional LSTM with attention per each query token)\nThis one is similar to Attentive Reader except that the attention weights are computed per each query token. The intuition is that for each token the model finds which part of the context document is more relevant. The model accumulates the information from the document as each query token is seen and finally outputs a joint document query representation using a non-linear combination of document embedding and query embedding.\n\nResults:\nAs expected, Attentive and Impatient Readers outperform all other models which show the benefits of attention model. Also Uniform Reader supports this hypothesis. The accuracies on two datasets (CNN, Daily Mail) are  Maximum Frequency: 33.2 / 25.5, Exclusive Frequency: 39.3 / 32.8, Frame-semantic model: 40.2 / 35.5, Word distance model: 50.9 / 55.5, Deep LSTM Reader: 57.0 / 62.2, Uniform Reader: 39.4 / 34.4, Attentive Reader: 63.0 / 69.0, Impatient Reader: 63.8 / 68.0.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend"
    },
    "695": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/ijcai/ZhouF17",
        "transcript": "https://i.imgur.com/QxHktQC.png \nThe fundamental question that the paper is going to answer is weather deep learning can be realized with other prediction model other thahttps://i.imgur.com/Wh6xAbP.pngn neural networks. The authors proposed deep forest, the realization of deep learning using random forest(gcForest). The idea is simple and was inspired by representation learning in deep neural networks which mostly relies on the layer-by-layer processing of raw features.\n\nImportance: Deep Neural Network (DNN) has several draw backs. It needs a lot of data to train. It has many hyper-parameters to tune. Moreover, not everyone has access to GPUs to build and train them. Training DNN is mostly like an art instead of a scientific/engineering task. Finally, theoretical analysis of DNN is extremely difficult. The aim of the paper is to propose a model to address these issues and at the same time to achieve performance competitive to deep neural networks.\n\n Model: The proposed model consists of two parts. First part is a deep forest ensemble with a cascade structure similar to layer-by-layer architecture in DNN. Each level is an ensemble of random forest and to include diversity a combination of completely-random random forests and typical random forests are employed (number of trees in each forest is a hyper-parameter). The estimated class distribution, which is obtained by k-fold cv from forests, forms a class vector, which is then concatenated with the original feature vector to be input to the next level of cascade. Second part is a multi-grained scanning for representational learning where spatial and sequential relationships are captured using a sliding window scan (by applying various window sizes) on raw features, similar to the convolution and recurrent layers in DNN. Then, those features are passed to a completely random tree-forest and a typical random forest in order to generate transformed features. When transformed feature vectors are too long to be accommodated, feature sampling can be performed.\n\nBenefits: gcForest has much fewer hyper-parameters than deep neural networks. The number of cascade levels can be adaptively determined such that the model complexity can be automatically set. If growing a new level does not improve the performance, the growth of the cascade terminates. Its performance is quite robust to hyper-parameter settings, such that in most cases and across different data from different domains, it is able to get excellent performance by using the default settings. gcForest achieves highly competitive performance to deep neural networks, whereas the training time cost of gcForest is smaller than that of DNN.\n\nExperimental results: the authors compared the performance of gcForest and DNN by fixing an architecture for gcForest and testing various architectures for DNN, however assumed some fixed hyper-parameters for DNN such as activation and loss function, and dropout rate. They used MNIST (digit images recognition), ORL(face recognition), GTZAN(music classification ), sEMG (Hand Movement Recognition), IMDB (movie reviews sentiment analysis), and some low-dimensional datasets. The gcForest got the best results in these experiments and sometimes with significant differences.\n\nMy Opinions: The main goal of the paper is interesting; however one concern is the amount of efforts they put to find the best CNN network for the experiments as they also mentioned that finding a good configuration is an art instead of scientific work. For instance, they could use deep recurrent layers instead of MLP for the sentiment analysis dataset, which is typically a better option for this task. For the time complexity of the method, they only reported it for one experiment not all. More importantly, the result of CIFAR-10 in the supplementary materials shows a big gap between superior deep learning method result and gcForest result although the authors argued that gcForest can be tuned to get better result. gcForest was also compared to non-deep learning methods such as random forest and SVM which showed superior results. It was good to have the time complexity comparison for them as well. In my view, the paper is good as a starting point to answer to the original question, however, the proposed method and the experimental results are not convincing enough.\n\nGithub link: https://github.com/kingfengji/gcForest",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.24963/ijcai.2017/497"
    },
    "696": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1806.07366",
        "transcript": "Summary by senior author [duvenaud on hackernews](https://news.ycombinator.com/item?id=18678078). \n\nA few years ago, everyone switched their deep nets to \"residual nets\". Instead of building deep models like this:\n\n    h1 = f1(x)\n    h2 = f2(h1)\n    h3 = f3(h2)\n    h4 = f3(h3)\n    y  = f5(h4)\n\nThey now build them like this:\n\n    h1 = f1(x)  + x\n    h2 = f2(h1) + h1\n    h3 = f3(h2) + h2\n    h4 = f4(h3) + h3\n    y  = f5(h4) + h4\n\nWhere f1, f2, etc are neural net layers. The idea is that it's easier to model a small change to an almost-correct answer than to output the whole improved answer at once.\n\nIn the last couple of years a few different groups noticed that this looks like a primitive ODE solver (Euler's method) that solves the trajectory of a system by just taking small steps in the direction of the system dynamics and adding them up. They used this connection to propose things like better training methods.\n\nWe just took this idea to its logical extreme: What if we _define_ a deep net as a continuously evolving system? So instead of updating the hidden units layer by layer, we define their derivative with respect to depth instead. We call this an ODE net.\n\nNow, we can use off-the-shelf adaptive ODE solvers to compute the final state of these dynamics, and call that the output of the neural network. This has drawbacks (it's slower to train) but lots of advantages too: We can loosen the numerical tolerance of the solver to make our nets faster at test time. We can also handle continuous-time models a lot more naturally. It turns out that there is also a simpler version of the change of variables formula (for density modeling) when you move to continuous time. ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1806.07366"
    },
    "697": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1811.04551",
        "transcript": "**Summary**: This paper presents three tricks that make model-based reinforcement more reliable when tested in tasks that require walking and balancing. The tricks are 1) are planning based on features, 2) using a recursive network that mixes probabilistic and deterministic information, and 3) looking forward multiple steps.\n\n**Longer summary**\n\nImagine playing pool, armed with a tablet that can predict exactly where the ball will bounce, and the next bounce, and so on. That would be a huge advantage to someone learning pool, however small inaccuracies in the model could mislead you especially when thinking ahead to the 2nd and third bounce. \n\nThe tablet is analogous to the dynamics model in model-based reinforcement learning (RL). Model based RL promises to solve a lot of the open problems with RL, letting the agent learn with less experience, transfer well, dream, and many others advantages. Despite the promise, dynamics models are hard to get working: they often suffer from even small inaccuracies, and need to be redesigned for specific tasks.\n\nEnter PlaNet, a clever name, and a net that plans well in range of environments. To increase the challenge the model must predict directly from pixels in fairly difficult tasks such as teaching a cheetah to run or balancing a ball in a cup.\n\n\nHow do they do this? Three main tricks.\n\n- Planning in latest space: this means that the policy network doesn't need to look at the raw image, but looks at a summary of it as represented by a feature vector.\n- Recurrent state space models: They found that probabilistic information helps describe the space of possibilities but makes it harder for their RNN based model to look back multiple steps. However mixing probabilistic information and deterministic information gives it the best of both worlds, and they have results that show a starting performance increase when both compared to just one.\n- Latent overshooting: They train the model to look more than one step ahead, this helps prevent errors that build up over time\n\nOverall this paper shows great results that tackle the shortfalls of model based RL. I hope the results remain when tested on different and more complex environments.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1811.04551"
    },
    "698": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.11910",
        "transcript": "Catastrophic forgetting is the tendency of an neural network to forget previously learned information when learning new information. This paper combats that by keeping a buffer of experience and applying meta-learning to it. They call their new module Meta Experience Replay or MER.\n\nHow does this work? At each update they compute multiple possible updates to the model weights. One for the new batch of information and some more updates for batches of previous experience. Then they apply  meta-learning using the REPTILE algorithm, here the meta-model sees each possible update and has to predict the output which combines them with the least interference. This is done by predicting an update vector that maximizes the dot product between the new and old update vectors, that way it transfers as much learning as possible from the new update without interfering with the old updates. https://i.imgur.com/TG4mZOn.png\n\nDoes it work? Yes, while it may take longer to train, the results show that it generalizes better and needs a much smaller buffer of experience than the popular approach of using replay buffers.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.11910"
    },
    "699": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1811.06032",
        "transcript": "This paper proposed three new reinforcement learning tasks which involved dealing with images.\n\n- Task 1: An agent crawls across a hidden image, revealing portions of it at each step. It must classify the image in the minimum amount of steps. For example classify the image as a cat after choosing to travel across the ears.\n- Task 2: The agent crawls across a visible image to sit on it's target. For example a cat in a scene of pets.\n- Task 3: The agent plays an Atari game where the background has been replaced with a distracting video.\n\nThese tasks are easy to construct, but solving them requires large scale visual processing or attention, which typically require deep networks. To address these new tasks, popular RL agents (PPO, A2C, and ACKTR) were augmented with a deep image processing network (ResNet-18), but they still performed poorly.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1811.06032"
    },
    "700": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1811.06521",
        "transcript": "How can humans help an agent perform at a task that has no clear reward? Imitation, demonstration, and preferences. This paper asks which combinations of imitation, demonstration, and preferences will best guide an agent in Atari games.\n\nFor example an agent that is playing Pong on the Atari, but can't access the score. You might help it by demonstrating your play style for a few hours. To help the agent further you are shown two short clips of it playing and you are asked to indicate which one, if any, you prefer.\n\nTo avoid spending many hours rating videos the authors sometimes used an automated approach where the game's score decides which clip is preferred, but they also compared this approach to human preferences. It turns out that human preferences are often worse because of reward traps. These happen, for example, when the human tries to encourage the agent to explore ladders, resulting in the agent obsessing about ladders instead of continuing the game.\n\nThey also observed that the agent often misunderstood the preferences it was given, causing unexpected behavior called reward hacking. The only solution they mention was to have someone keep an eye on it and continue giving it preferences, but this isn't always feasible. This is the alignment problem which is a hard problem in AGI research.\n\nResults: adding merely a few thousand preferences can help in most games, unless they have sparse rewards. Demonstrations, on the other hand, tend to help those games with sparse rewards but only if the demonstrator is good at the game.\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1811.06521"
    },
    "701": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.06721",
        "transcript": "This builds on the previous [\"MERLIN\"](https://arxiv.org/abs/1803.10760) paper. First they introduce the RMA agent, which is a simplified version of MERLIN which uses model based RL and long term memory. They give the agent long term memory by letting it choose to save and load the agent's working memory (represented by the LSTM's hidden state). \n\nThen they add credit assignment, similar to the RUDDER paper, to get the \"Temporal Value Transport\" (TVT) agent that can plan long term in the face of distractions. **The critical insight here is that they use the agent's memory access to decide on credit assignment**. So if the model uses a memory from 512 steps ago, that action from 512 steps ago gets lots of credit for the current reward.\n\nThey use various tasks, for example a maze with a distracting task then a memory retrieval task. For example, after starting in a maze with, say, a yellow wall, the agent needs to collect apples. This serves as a distraction, ensuring the agent can recall memories even after distraction. At the end of the maze it needs to remember that initial color (e.g. yellow) in order to choose the exit of the correct color.\n\nThey include performance graphs showing that memory or even better memory plus credit assignment are a significant help in this, and similar, tasks.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1810.06721v1"
    },
    "702": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1806.07857",
        "transcript": "[Summary by author /u/SirJAM_armedi](https://www.reddit.com/r/MachineLearning/comments/8sq0jy/rudder_reinforcement_learning_algorithm_that_is/e11swv8/).\n\nMath aside, the \"big idea\" of RUDDER is the following: We use an LSTM to predict the return of an episode. To do this, the LSTM will have to recognize what actually causes the reward (e.g. \"shooting the gun in the right direction causes the reward, even if we get the reward only once the bullet hits the enemy after travelling along the screen\"). We then use a salience method (e.g. LRP or integrated gradients) to get that information out of the LSTM, and redistribute the reward accordingly (i.e., we then give reward already once the gun is shot in the right direction). Once the reward is redistributed this way, solving/learning the actual Reinforcement Learning problem is much, much easier and as we prove in the paper, the optimal policy does not change with this redistribution.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1806.07857"
    },
    "703": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1101/225664",
        "transcript": "**TL;DR:** There are 'place cells' in the hippopotamus that are fired when passing through a location. You can take a rat and measure how its cells are activated in a maze, then monitor neurons during planning, rest or sleep. You'll see patterns that show it's thinking of locations in order and focusing on interesting locations. This paper looks at how RL agents do 'prioritized experience replay' and compare it to place cells in animals. The authors do a RL simulation and *qualitatively* compare the results to the activity observed in place cells.\n\n**Key paragraphs:**\n\n> Neural activity recorded from hippocampal place cells during spatial navigation typically represents the animal\u2019s spatial position, though it can sometimes represent locations ahead of the animal. For instance, during \u201csharp wave ripple\u201d events, activity might progress sequentially from the animal\u2019s current location towards a goal location. These \u201cforward replay\u201d \u00b4sequences predict subsequent behavior and have been suggested to support a planning mechanism that links actions to their deferred consequences along a spatial trajectory. However, analogously to the human evidence, remote activity in the hippocampus can also represent locations behind the animal, and even altogether disjoint, \u00b4remote locations (especially during rest or sleep) (Fig. 1a).\n\n...\n\n> we develop a normative theory to predict not just whether but which memories should be accessed at each time\nto enable the most rewarding future decisions.\n\n...\n\n> To test the implications of our theory, we simulate a spatial navigation task where an agent generates and stores experiences which can be later retrieved. We show that an agent that accesses memories sequentially and in order of utility\nproduces patterns of sequential state consideration that resemble place cell replay, and reproduces qualitatively and with\nno parameter fitting a wealth of empirical findings including (i) the existence and balance between forward and reverse replay; (ii) the content of replay; and (iii) effects of experience. \n\n...\n\n> we propose the unifying view that all patterns of replay during behavior, rest, and sleep reflect different instances of a more general state retrieval operation that integrates experiences across space and time to propagate value and guide decisions. \n\n**My 2 cents**: I like this paper because prioritized experience replay reminds me of how we often dream or daydream of novel good or bad events that happened or that we anticipate. This paper drills much deeper into this connection.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1101/225664"
    },
    "704": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.04865",
        "transcript": "\n## Summary\nIn a prior work 'On Calibration of Modern Nueral Networks', temperature scailing is used for outputing confidence. This is done at inference stage, and does not change the existing classifier. This paper considers the confidence at training stage, and directly outputs the confidence from the network.\n\n## Architecture\nAn additional branch for confidence is added after the penultimate layer, in parallel to logits and probs (Figure 2).\n\nhttps://i.imgur.com/vtKq9g0.png\n\n## Training\nThe network outputs the prob $p$ and the confidence $c$ which is a single scalar. The modified prob $p'=c*p+(1-c)y$ where $y$ is the label (hint). The confidence loss is $\\mathcal{L}_c=-\\log c$, the NLL is $\\mathcal{L}_t= -\\sum \\log(p'_i)y_i$.\n\n### Budget Parameter\nThe authors introduced the confidence loss weight $\\lambda$ and a budget $\\beta$. If $\\mathcal{L}_c>\\beta$, increase $\\lambda$, if $\\mathcal{L}_c<\\beta$, decrease $\\lambda$. $\\beta$ is found reasonable in [0.1,1.0].\n\n### Hinting with 50%\nSometimes the model relies on the free label ($c=0$) and does not fit the complicated structure of data. The authors give hints with 50% so the model cannot rely 100% on the hint. They used $p'$ for only half of the bathes for each epoch.\n\n### Misclassified Examples\n\nA high-capacity network with small dataset overfits well, and mis-classified samples are required to learn the confidence. The network likely assigns low confidence to samples. The paper used an aggressive data augmentation to create difficult examples.\n\n## Inference\nReject if $c\\le\\delta$.\n\nFor out-of-distribution detection, they used the same input perturbation as in ODIN (2018). ODIN used temperature scailing and used the max prob, while this paper does not need temperature scailing since it directly outputs $c$. In evaluation, this paper outperformed ODIN.\n\n\n\n\n## Reference\nODIN: [Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks](http://www.shortscience.org/paper?bibtexKey=journals/corr/1706.02690#elbaro)\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.04865"
    },
    "705": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1706.02690",
        "transcript": "## Task\nAdd '**rejection**' output to an existing classification model with softmax layer.\n\n## Method\n1. Choose some threshold $\\delta$ and temperature $T$\n2. Add a perturbation to the input x (eq 2),\nlet $\\tilde x = x - \\epsilon \\text{sign}(-\\nabla_x \\log S_{\\hat y}(x;T))$\n3. If $p(\\tilde x;T)\\le \\delta$, rejects\n4. If not, return the output of the original classifier\n\n$p(\\tilde x;T)$ is the max prob with temperature scailing for input $\\tilde x$\n\n$\\delta$ and $T$ are manually chosen.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1706.02690"
    },
    "706": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1706.04599",
        "transcript": "## Task\n\nA neural network for classification typically has a **softmax** layer and outputs the class with the max probability. However, this probability does not represent the **confidence**. If the average confidence (average of max probs) for a dataset matches the accuracy, it is called **well-calibrated**. Old models like LeNet (1998) was well-calibrated, but modern networks like ResNet (2016) are no longer well-calibrated. This paper explains what caused this and compares various calibration methods.\n\n## Figure - Confidence Histogram\n\nhttps://i.imgur.com/dMtdWsL.png\n\nThe bottom row: group the samples by confidence (max probailities) into bins, and calculates the accuracy (# correct / # bin size) within each bin.\n\n- ECE (Expected Calibration Error): average of |accuracy-confidence| of bins\n- MCE (Maximum Calibration Error): max of |accuracy-confidence| of bins\n\n## Analysis - What\nThe paper experiments how models are mis-calibrated with different factors: (1) model capacity, (2) batch norm, (3) weight decay, (4) NLL.\n\n## Solution - Calibration Methods\n\nMany calibration methods for binary classification and multi-class classification are evaluated. The method that performed the best is **temperature scailing**, which simply multiplies logits before the softmax by some constant. The paper used the validation set to choose the best constant.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1706.04599"
    },
    "707": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1707.07998",
        "transcript": "This paper solves two tasks: Image Captioning and VQA.\nThe main idea is to use Faster R-CNN to embed images (kx2048 from k bounding boxes) instead of ResNet (14x14x2048) and apply attention over k vectors.\n\nFor **VQA**, this is basically (Faster R-CNN + ShowAttendAskAnswer). SAAA(ShowAskAttendAnswer) calculates a 2D attention map from the concatenation of a text vector (2048-dim from LSTM) and image tensor (2048x14x14 from ResNet). This image feature can be thought as a collection of 2048-dim feature vectors. This paper uses Faster R-CNN to get k bounding boxes. Each bounding box is a 2048-dim vector so we have kx2048, which is fed to SAAA.\n\n\n**SAAA**:\nhttps://i.imgur.com/2FnPXi0.png\n\n**This paper (VQA)**:\nhttps://i.imgur.com/xib77Iy.png\n\nFor **Image Captioning**, it uses 2-layer LSTM. The first layer gets the average of k 2048-dim vectors. The output is used to calculate the attention weights over k vectors. The second layer gets the weight-averaged 2048-dim vector and the output of the first layer.\n\nhttps://i.imgur.com/GeXaC30.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1707.07998"
    },
    "708": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeZRS15",
        "transcript": "#### Goal:\n+ Reformulate neural network architecture to address the degratation problem due to the very large number of layers.\n\n#### Motivation:\n\n![Motivation](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/convolutional/images/He2015_motivation.png?raw=true \"Motivation\")\n\n+ Degradation problem:\n    + Increasing the depth of the network: accuracy gets saturated and then starts degrading.\n    + As number of layers increase: higher training error.\n    + In theory, such problem should not occur. Given a neural network, one can add new layers with identity mappings. In reality, optimization algorithms probably have difficulties to find (in feasible time)these solutions.\n\n\n#### Residual Block:\n\n+ Layers are reformulated as learning residual functions with reference to the layer inputs.\n\n![Motivation](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/convolutional/images/He2015_framework.png?raw=true \"Motivation\")\n\n+ Residual Mapping. \n    + Neural networks with shortcut connections to perform identity mappings.\n    + Identity shortcut connections do no add extra parameters or complexity to the network.\n    + The problem can be seen as follows. Given the activation of layer L of a neural net, a[L], one can write the activation at layer L+2 as follows.\n\n        a[L+2] = ReLu(W[L+2] * a[L+1] + b[L+2] + a[L])\n\n    where W[L+2] is the weight matrix and b[L+2] is the bias vector at layer L+2.\n\n    + The problem of learning an identity mapping is easier in this case, if weight decay is applied, W[L+2] goes to zero, as well as b[L+2].  The activation function at layer a[L+2] = ReLu(a[L]) = a[L].\n\n    + One should take take to match the dimensions. A linear projection of the previous activation function could be used before the sum.\n\n\n#### Datasets:\n\n+ For the image classification task,  two datasets were used: ImageNet and CIFAR-10\n\n|ImageNet|CIFAR-10\n----|-----|-----\nTraining images | 1.2M | 50K\nValidation images| 50K | (*)\nTesting images | 100K | 10K\nNumber of classes | 1000 | 10\n\n(*) in the experiments with CIFAR-10, the training images are split into 45K/5K training/validation sets.\n\n#### Experiments and Results\n\n**ImageNet Dataset**\n\n+ Input images:\n    + Scale jittering as in [Simonyan2015](https://github.com/tiagotvv/ml-papers/blob/master/convolutional/Very_Deep_Convolutional_Networks_for_Large_Scale_Image_Recognition.md). Image is resized with shorter size sampled to be in between [256, 480]. \n    + 224x224 crop from image is used.\n    + Data augmentation following [Krizhevsky2012](https://github.com/tiagotvv/ml-papers/blob/master/convolutional/ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.md) methodology: image flips, change RGB levels.\n+ Training\n    + Weight initialization: follows previous work by the authors.\n    + Gradient descent with batch normalization, weight decay = 0.0001, momentum = 0.9\n    + Mini-batch size = 256\n    + Learning rate starts at 0.1 and is divided by 10 when accuracy stop increasing at the validation set.\n    + Dropout is not employed\n+ Testing\n    + Multi-crop procedure from  [Krizhevsky2012](https://github.com/tiagotvv/ml-papers/blob/master/convolutional/ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.md) is employed: 10 crops.\n    + Fully connected layers are converted into convolutional layers.\n    + Average of scores at multiple scales is employed. Testing scales used: {224, 256, 384, 480, 640}. \n+ Configurations tested on ImageNet dataset\n\n ![Motivation](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/convolutional/images/He2015_architectures.png?raw=true \"Motivation\")\n\n+ Single Model Results (validation set):\n\nArchitecture | top-1 error (%) | top-5 error (%)\n----|:-----:|:-----:\nVGG (ILSVRC'14) | - | 8.43\nGoogLeNet (ILSVRC'14)  |-  | 7.89\nVGG (v5) | 24.4 | 7.1\nPReLU-net | 21.59 | 5.71\nBN-inception | 21.99 | 5.81\nResNet-34 B (projections + identitites) | 21.84 | 5.71\nResNet-34 (projections) | 21.53 | 5.60\nResNet-50 | 20.74 | 5.25\nResNet-101| 19.87 | 4.60\nResNet-152 | **19.38** | **4.49**\n\n+ Ensemble Models Results (test set):\n\nArchitecture | top-5 error (%)\n----|:-----:|\nVGG (ILSVRC'14) | 7.32 \nGoogLeNet (ILSVRC'14)  | 6.66\nVGG (v5) | 6.8\nPReLU-net | 4.94 \nBN-Inception | 4.82\nResNet (ILSVRC'15) | **3.57** \n\n\n\n**CIFAR-10 Dataset**\n\n+ Input images:\n    + Inputs: 32x32 images\n    + Configurations tested on this dataset:\n    \noutput map size | 32x32 | 16x16 | 8x8\n----------------|-------|-------|----\nnum. layers | 1+2n | 2n | 2n\nnum. filters | 16 | 32 | 64\n\n+ Shortcuts connected to pairs of 3x3 layers (3n shortcuts)  \n\n+ Training\n    + Weight initialization: follows previous work by the authors.\n    + Gradient descent with batch normalization, weight decay = 0.0001, momentum = 0.9\n    + Mini-batch size = 128, 2 GPUs.\n    + Learning rate starts at 0.1 and is divided by 10 at 32k and 48k iterations. Stopped at 64k iterations.\n    + Dropout is not employed\n+ Testing\n    + Single 32x32 image\n\n+ Results\n\n ![CIFAR-10 results](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/convolutional/images/He2015_CIFAR.png?raw=true \"CIFAR-10 results\")",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.03385"
    },
    "709": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=simonyan2014convolutional",
        "transcript": "#### Goal:\n+ Train deep convolutional neural networks with small convolutional filters to classify images into 1000 different categories.\n\n#### Dataset\n+ ImageNet Large-Scale Visual Recognition Challenge (ILSVRC): subset of ImageNet\n    + 1.2 million training images, 50000 validation images, 150000 test images.\n    + 1000 categories.\n\n#### Architecture:\n+ Convolutional layers followed by fully-connected layers and 1000-way softmax at the output.\n\n![Configurations](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/convolutional/images/Simonyan2015_architectures.png?raw=true \"Convnet configurations\")\n\n+ Convolutional Layers\n    + Convolutional filter: 3x3, stride = 1. \n    + 'Same' convolution, padding = 1.\n    + Width of convolutional layers start at 64 and increases by a factor of 2 after max-pooling until reaching 512.\n+ Max Pooling: 2x2 window, stride = 2\n+ Activation function: ReLU\n+ Number of parameters:\n![Number of parameters](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/convolutional/images/Simonyan2015_parameters.png?raw=true \"Number of parameters\")\n\n#### Discussion:\n\n+ A stack of three 3x3 convolutional layers (without max-pooling in between) is equivalent to a 7x7 convolutional layer. Why is it better?\n    + Three non-linearities instead of just one.\n    + Reduced number of parameters. A n x n convolutional layer with C channels has (nC)^2 parameters.\n\n    \nArchitecture |n | C | # of parameters\n------|-------|----|---\n1-layer CNN | 7 | 64 | 49*4096 = 200704\n3-layer CNN | 3 | 64 | 9*4096 = 36864\n\n+ The 1x1 convolution layers from configuration C aimed to increase the non-linearity of the decision function without affecting the the receptive fields of the convolutional layers.\n\n#### Methodology:\n\n+ Training\n    + Optimize the multinomial logistic regression cost function.\n    + Gradient descent. \n        + Mini batch size = 256, Momentum = 0.9, Weight decay = 0.0005\n    + Initial learing rate: 0.01\n        + Divided by 10 when the validation set accuracy stopped improving.\n        + Decreased 3 times. Learning stopped after 370K iterations (74 epochs).\n    + Weight initialization:\n        + Configuration A was trained with random initialization of weights.\n        + For the other configurations, the first convolutional nets and the fully connected nets were initialized using weights from configuration A. The other layers were randomly initialized.\n        + Random initialization: weights are sampled from a zero-mean normal distribution with 0.01 variance. Biases are initialized wirh zero.\n+ Reduce Overfitting:\n    + Data Augmentation: followed [Krizhevsky2012](https://github.com/tiagotvv/ml-papers/blob/master/convolutional/ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.md) principles with random flippings and changes in RGB levels.\n    + Dropout regularization for the first two fully-connected layers - p(keep) = 0.5\n+ Image Resolution:\n    + Models were trained at two fixed scales S=256 and S=384.\n    + Multi-scale training (randomly sampling S): minimum=256, maximum=512.\n        + Can be seen as training set augmentation by scale jittering.\n    + At test time, test scale Q is not necessarily equal to training scale S.\n\n#### Results\n\n+ Implementation derived from C++ Caffe toolbox.\n+ Training and evaluation on multiple GPUs (no information regarding training time).\n\n+ Single scale evaluation: \n    + Fixed training scale: Q=S.\n    + Jittered training scale: Q=0.5(S_min + S_max).\n    + Local Response Normalization did not improved results.\n\nConfiguration | S | Q | top-1 error (%) | top-5 error (%)\n:--------------:|:---:|---|:-----------------:|:---------------: \nA | 256 | 256 | 29.6 | 10.4  \nA-LRN | 256 | 256 | 29.7 | 10.5\nB | 256 | 256 | 28.7 | 9.9\nC | 256 | 256 | 28.1 | 9.4 \n  | 384 | 384 | 28.1 | 9.3\n  | [256;512] | 384 | 27.3 | 8.8 \nD | 256 | 256 | 27.0 | 8.8\n  | 384 | 384 | 26.8 | 8.7\n  | [256;512] | 384 | 25.6 | 8.1 \nE | 256 | 256 | 27.3 | 9.0 \n  | 384 | 384 | 26.9 | 8.7\n  | [256;512] | 384 | **25.5** | **8.0** \n\n+ Multi-scale evaluation: \n    + Fixed training scale: Q={S-32,S,S+32}. \n    + Jittered training scale: Q={S_min, 0.5(S_min + S_max), S_max}. \n\nConfiguration | S | Q | top-1 error (%) | top-5 error (%)\n:--------------:|:---:|---|:-----------------:|:---------------: \nB | 256 | 224,256,288 | 28.2 | 9.6\nC | 256 | 224,256,288 | 27.7 | 9.2 \n  | 384 | 352,384,416 | 27.8 | 9.2\n  | [256;512] | 256,384,512 | 26.3 | 8.2 \nD | 256 | 224,256,288 | 26.6 | 8.6\n  | 384 | 352,384,416 | 26.5 | 8.6\n  | [256;512] | 256,384,512 | **24.8** | **7.5** \nE | 256 | 224,256,288 | 26.9 | 8.7 \n  | 384 | 352,384,416 | 26.7 | 8.6\n  | [256;512] | 256,384,512 | **24.8** | **7.5** \n\n+ Dense versus multi-crop evaluation\n    + Dense evaluation: fully connected layers are converted to convolutional layers at test time. Scores are obtained for full uncropped image and its flipped version and then averaged.\n    + Multi-crop evaluation: average of scores obtained by passing multiple crops of the test image through the convolutional network. \n    + Combination of multi-crop and dense has best results: probably due to different treatment of convolution boundary conditions.\n\nConfiguration | Method | top-1 error (%) | top-5 error (%)\n:--------------:|:---:|:-----------------:|:---------------: \nD | dense | 24.8 | 7.5\n  | multi-crop | 24.6 | 7.5\n  | multi-crop & dense | **24.4** | **7.2** \nE | dense | 24.8 | 7.5\n  | multi-crop | 24.6 | 7.4\n  | multi-crop & dense | **24.4** | **7.1** \n \n\n+ Comparison with State of the art solutions:\n\n    + VGG (2 nets) = ensemble of 2 models trained using configurations D and E.\n    + VGG (7 nets) = ensemble of 7 models different models trained using configurations C, D, E.\n\n![Results](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/convolutional/images/Simonyan2015_results.png?raw=true \"Results\")",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1409.1556"
    },
    "710": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=krizhevsky2012imagenet",
        "transcript": "#### Goal:\n+ Train a deep convolutional neural network to classify 1.2 million images into 1000 different categories.\n\n#### Convolutional Neural Networks:\n+ Make strong and correct assumptions about the nature of the images (stationarity, pixel dependencies). \n+ Much fewer connections and parameters: easier to train than fully connected neural networks.\n\n#### Dataset\n+ ImageNet: 15 million labeled high-resolution images from 22000 categories. Labeled manually using Amazon Mechanical Turk.\n+ ImageNet Large-Scale Visual Recognition Challenge (ILSVRC): subset of ImageNet\n    + 1.2 million training images, 50000 validation images, 150000 test images.\n    + 1000 categories\n+ Variable resolution images:\n    + Images downsampled to a fixed resolution of 256 x 256.\n\n\n#### Architecture:\n+ 8 layers: 5 convolutional and 3 fully-connected, 1000-way softmax at the output.\n\n![Architecture](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/convolutional/images/Krizhevsky2012_architecture.png?raw=true \"Architecture\")\n\n**Methodology**\n\n+ ReLU activation function: train several times faster than tanh units.\n    + Faster learning had influence on the performance of large models trained on large datasets\n+ Training on Multiple GPUs\n+ Local Response Normalization\n    + mimics a form of lateral inhibition found on real neurons.\n    + applied after ReLU in the 1st and 2nd convolutional layers.\n    + improves top-1 and top-5 error rates by 1.4% and 1.2%\n+ Overlapping pooling\n    + Neighborhood z = 3 and stride s = 2.\n    + Max-pooling employed in the 1st and 2nd convolutional layers (after response normalization) and as well as after the 5th convolutinal layer.\n+ Reducing Overfitting\n    + Data Augmentation\n        + Generate image translations and horizontal reflections.\n        + Alter the intensities of RGB channels.\n    + Dropout\n        + Used in the first two fully-connected layers - p(keep) = 0.5\n+ Learning\n    + Stochastic Gradient Descent, batch size = 128, momentum = 0.9, weight decay = 0.0005\n    + Weights initialized from Gaussian distribution with mean = 0 and standard deviation = 0.01\n        + Bias in 2nd, 4th, and 5th convolutional layers initialized as 1. This accelerated learning as the ReLU was fed with positive inputs from the start.\n        + Bias in remaining layers initialized as zeros.\n    + Learning rate ($\\epsilon$)\n        + Equal for all layers\n        + Adjusted manually (divided by 10 when validation error stopped decreasing).\n        + Initialized at 0.01 and reduced 3 times during training.\n\n            ![Update equations](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/convolutional/images/Krizhevsky2012_update.png?raw=true \"Update equations\")\n\n    + Trained during 90 epochs (5-6 days on two NVIDIA GTX 580 3GB GPUs).\n\n#### Results\n\n+ Results on ILSVRC-2010 images\n    + Baselines: sparse coding and Fisher vectors\n\nModel | Top-1 | Top-5\n------|-------|-------\nSparse Coding | 47.1% | 28.2%\nSIFT + FVs | 45.7% | 25.7%\nCNN | 37.5% | 17.0%\n\n+ Results on ILSVRC-2012\n\nModel | Top-1 (val) | Top-5 (val) | Top-5 (test)\n------|-------|-------|-------\nSparse Coding | -- | -- | 26.2%\n1 CNN | 40.7% | 18.2% | --\n5 CNNs | 38.1% | 16.4% | 16.4%\n1 CNN* | 39.0% | 16.6% | --\n7 CNNs* | 36.7% | 15.4% | 15.3%\n\nCNN* are convolutional neural networks pretrained on ImageNet 2011 Fall release and fine-tuned on ILSVRC-2012 training data.\n\n+ Qualitative assessment\n    + Convolutional kernels showed *specialization*\n\n    ![Kernels](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/convolutional/images/Krizhevsky2012_weights.png?raw=true \"Convolutional kernels from 1st layer\")\n\n    + Most of top-5 labels were reasonable\n    + Image similarity based on the feature activations induced at the last fully connected layer:\n\n    ![Qualitative Assessment](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/convolutional/images/Krizhevsky2012_qualitative.png?raw=true \"Qualitative assessment\")\n\n\n\n#### Caveat:\n+ Most of the choices made in the paper were based on experimental results. There is not too much theory behind.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "711": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LiptonKEW15",
        "transcript": "#### Goal\n+ Predict 128 diagnoses for intensive pediatric care patients.\n\n#### Dataset:\n\n+ Children's Hospital LA.\n+ Episode is a multivariate time series that describes the stay of one patient in the intensive care unit.\n\nDataset properties  | Value \n---------|----------\nNumber of episodes | 10,401\nDuration of episodes | From 12h to several months\nTime series variables | Systolic blood pressure, Diastolic blood pressure,  Peripheral capillary refill rate, End tidal CO2, Fraction of inspired O2,  Glasgow coma scale,  Blood glucose,  Heart rate,  pH,  Respiratory rate, Blood O2 Saturation,  Body temperature,  Urine output.\n\n+ Resampling and missing values:\n  + Irregularly sampled time-series that is resampled to an hourly rate.\n    + Mean measurement within each hour window is taken.\n    + Forward- and back-filling are used to fill gaps created by the resampling.\n  + When variable time series is missing entirely: imputation with a clinically *normal* value defined by domain experts.\n  + This paper is followed by [Modeling Missing Data in Clinical Time Series with RNNs](http://www.shortscience.org/paper?bibtexKey=journals/corr/LiptonKW16) from the same research group.\n  \n+ Labels:\n  + Each episode is associated with 0 or more diagnoses. (in-house taxonomy, ICD-9 based).\n  + Dataset contains 429 diagnoses. The paper focuses on the 128 most frequent diagnoses that appear 50 or more times in the dataset.\n\n#### Architecture:\n\n+ LSTM with Target Replication:\n\n![Architecture](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Lipton2016a_target.png?raw=true \"Target Replication\")\n\n+ Loss function:\n  + For the model with target replication, output y is generated at every sequence step. The loss function is then a convex combination of the final loss (log-loss in the case of this paper) and the average of the losses over all steps where T is the number of sequence steps and alpha is a hyperparameter.\n\n![Loss function](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Lipton2016a_loss.png?raw=true \"Loss function\")\n\n\n#### Experiments and Results:\n\n**Methodology**:\n+ Split dataset: 80% training, 10% validation, 10% test\n+ LTSM trained for 100 epochs via gradient stochastic gradient (with momentum).\n+ Regularization L2: 1e-6, obtained via validation dataset.\n\n+ LSTM: 2 hidden layers with 64 cells or 128 cells (and 50% dropout)\n+ Multiple combinations: target replication / auxiliary target variables (trained using the other 301 diagnoses and other clinical information as a target. Inferences are made only for the 128 major diagnoses.\n\n+ Baselines for comparison:\n  + Logistic Regression - L2 regularized\n  + MLP with 3 hidden layers - ReLU - dropout 50%.\n  + Baselines tested in the raw time-series and in a feature engineering version made by domain experts.\n\n*Metrics*:\n+ Micro AUC, Micro F1: calculated by adding the TPs, FPs, TNs and FNs for the entire dataset and for all classes.\n+ Macro AUC, Macro F1: Arithmetic mean of AUCs and F1 scores for each of the classes.\n+ Precision at 10: Fraction of correct diagnoses among the top 10 predictions of the model.\n  + The upper bound for precision at 10 is 0.2281 since in the test set there are on average 2.281 diagnoses per patient.\n\n*Results*:\n\n![All Results](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Lipton2016a_allresults.png?raw=true \"Performance metrics across all labels\")\n\n*Results for selected diagnoses*:\n\n![Results for Selected Diseases](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Lipton2016a_selected.png?raw=true \"Performance for selected diagnoses\")\n\n\n#### Discussion:\n\n+ Auxiliary outputs improve performance at the expense of increased training time. Very unbalanced dataset for some of the remaining 301 labels makes it spend an entire epoch only to learn that one of the target variables can take values \u200b\u200bother than 0.\n\n+ Real-Time Predictions: In the future, the authors expect that the proposed solution could be used to make continuously updated real-time alerts and diagnoses.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.03677"
    },
    "712": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jbi/RothmanRB13",
        "transcript": "#### Goal:\n+ Development and validation of a continuous score for patient assessment (to be used both outside and inside intensive care).\n\n+ Prior work: Modified Early Warning Score (MEWS) identifies 44% of intensive care transfers occurring within the next 12 hours. Generates 69 false positives for each correctly identified event.\n\n\n#### Dataset:\n\nModel Creation | Model Validation\n---------------|------------------\n22,265 patients admitted to the *Sarasota Memorial Hospital* (SMH) between Jan/2004 and Dec/2004 | 32341 patients admitted to the SMH between Sep/2007 and Jun/2009 \n| 45,771 patients admitted to the SMH between Jan/2008 and May/2010  \n| 32,416 patients admitted to *Abigton Memorial Hospital* (AMH) between Jul/2009 and Jun/2010  \n| 19,402 patients admitted between Jul/2008 and Nov/2008 in *Hospital C*.\n\n+ ~ 7000 variables, 500 laboratory tests.\n+ Constraints:\n  + Variables should be related to the patient's condition\n  + Collected with some frequency\n  + Susceptible to variation during patient stay in the hospital\n  + Focus: \"How the patient is\" not \"Who the patient is\"\n+ The constraints reduce the number of candidate variables to 43 (13 nurse assessments, 6 vital signs and 23 laboratory tests)\n\n#### Rothman Index:\n\n+ The Rothman Index is based on the \"Excess Risk\" associated with each of the variables:\n\n+ The excess risk is determined by the increase (in percentage points) of the mortality at 1 year identified for that variable. In the best case, the \"excess\" risk is zero and the Rothman Index equals 100.\n\n![Excess Risk](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Rothman2013_excess.png?raw=true \"Excess Risk\")\n\n\n+ The excess risk somewhat resembles the *impact coding* for categorical variables. One must always be careful that there is no data snooping.\n\n+ The index consists of the 26 variables below. They were chosen from the 43 candidates using a *forward stepwise logistic regression* with the patients of the model creation dataset (criterion p-value <0.05). Note that the logistic regression is used only to choose the variables. The Rothman Index is not a regression model itself.\n\n![Variables](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Rothman2013_variables.png?raw=true \"Variables used to derive Rothman Index\")\n\n\n+ The lab tests are collected less frequently. The score is divided into two parts (one that takes into account the laboratory variables and another that does not take into account).\n\n![Formula](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Rothman2013_formula.png?raw=true \"Rothman Index Formula\")\n\n+ *TimeSinceLabs* has a maximum value of 48 hours.\n\n#### Results:\n\n+ Outcomes:\n  + Mortality in 24h\n  + Unplanned readmission in 30 days\n  + Discharge\n+ Rothman Index is correlated with discharge category (Home, Home healthcare, Rehab, Skilled Nursing Facility, Hospice, Death)\n\n|Mortality in 24h || Readmission in 30 days || Discriminates type of discharge| | \n|-------------|-|-----------------------|-|------------------------|-------------------------------|\n| Hospital|  AUC |  Hospital | AUC | Hospital | AUC|\n|SMH | 0.933  (0.915-0.930) | SMH | 0.62  (0.61-0.63) | SMH | 0.923 (0.915-0.930)|\n|AMH | 0.948  (0.960-0.970) | AMH | *|  AMH | 0.965 (0.960-0.970)|\n|C | 0.929 (0.919-0.940) | C | *| C| 0.915  (0.900-0.931) |\n\n(*) in the case of readmission in 30 days it was possible to only identify the patients of the SMH hospital.\n\n+ Tracking the Rothman Index and correlating it with events during hospital stay:\n\n![Evolution](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Rothman2013_evolution.png?raw=true \"Evolution of Rothman Index during patient stay\")\n\n\n#### Discussion:\n\n+ Choice of 1-year mortality to calculate excess risk:\n  + Instead of in-hospital death, which is relatively rare (approximately 1\u20142% of patients), the model is based on 1-year post discharge mortality, where death is far more common (approximately 10% of patients). \n  + Improve the *signal strength* to determine the relationships between clinical measures and risk.  \n  + Outcome should be sufficiently frequent and a plausible surrogate for the patient condition. In this case, the risk tries to quantify *distance from death*. \n+ The Rothman index is not designed to predict any specific outcome.\n+ Caveat: Results should have included precision/recall analysis. For risk assessment it is important to evaluate the rate of false alarms per one true positive.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/j.jbi.2013.06.011"
    },
    "713": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ChoiBS15",
        "transcript": "\n#### Goal:\n+ Diagnostic and drug code prediction on a subsequent visit using diagnostic codes, medications, procedures and date of previous visits.\n+ Predict when the next visit to the doctor will happen.\n\n#### Dataset:\n\n+ Sutter Health Palo Alto Medical Foundation - primary care - case-control study for heart failure.\n\n![Dataset](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Choi2016_dataset.png?raw=true \"Dataset information\")\n\n+ Patients with fewer than two visits were excluded.\n+ Inputs:\n  + ICD-9 codes,\n  + GPI drug codes\n  + codes for CPT procedures\n+ Records are time-stamped with the patient's visiting time.\n+ If a patient receives multiple codes on the same visit, they all receive the same timestamp.\n+ Granularity of codes - group subcategories:\n  + ICD-9 3 digits: 1183 unique codes\n  + GPI Drug class: 595 single groups\n+ Target: y = [diagnosis, drug] - vector of 1183 + 595 = 1778 dimensions.\n\n#### Architecture:\n+ Gated Recurrent Units (GRU)\n\n![GRU architecture](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Choi2016_gru.png?raw=true \"Gated Recurrent Unit (GRU)\")\n\n\n+ The input vector x is one-hot encoded and has a dimension of 40000. The first layer tries to reduce dimensionality.\n+ Two approaches to dimensionality reduction (embedding matrix W_emb)\n  + W_emb is learned together with the model.\n  + W_emb is pre-trained using techniques such as word2vec.\n+ Loss function: cross entropy for codes + quadratic error for forecasting visits.\n+ Prediction layer codes: Softmax / Prediction layer of the next time visit: ReLu.\n\n#### Experiments and Results:\n\n+ Code available on GitHub: https://github.com/mp2893/doctorai\n+ Implementation in Theano - Training with 2 Nvidia Tesla K80 GPUs\n\n*Methodology*:\n+ Dataset split: 85% training, 15% test.\n+ RNN trained for 20 epochs.\n+ L2 regularization for both the vector of coefficients of the codes and for the vector of coefficients of the next visit (lambda = 0.001) - Dropout between GRU and prediction layer (and between GRU layers if there are more than 1).\n+ 2000 neurons in the hidden layer\n\n*Baselines*:\n\n+ Frequency: The codes from the previous visit are repeated on the new visit. Good baseline for the case of patients whose condition tends to stabilize over time.\n+ Top k most frequent codes from the previous visit.\n+ Logistic Regression and Multilayer Perceptron. Uses the last 5 visits to predict the next.\n\n\n\n*Metrics*:\n\n+ top-k recall emulates the behavior of physicians when making a differential diagnosis\n\n    top-k recall = # of true positives in the top k predictions / number of true positives\n\n+ R^2 used to evaluate the performance of the next visit prediction.\n  + Predict logarithm of time duration between visits to reduce the impact of very long intervals.\n\n*Results Table*:\n\n  + RNN-1: RNN with a single hidden layer initialized with a random orthogonal matrix for W_emb.\n  + RNN-2: RNN with two hidden layers initialized with a random orthogonal matrix for W_emb.\n  + RNN-1-IR: RNN using a single hidden layer initialized embedding matrix w emb with the Skip-gram vectors trained on the entire dataset. \n  + RNN-2-IR: RNN with two hidden layers initialized embedding matrix W_emb with the Skip-gram vectors trained on the entire dataset.    \n\n![Results](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Choi2016_results1.png?raw=true \"Forecasting future medical activities\")\n\n\n+ Performance varies according to the number of patient visits:\n  + Networks learn best when they observe more records.\n  + Patients with frequent visits are sicker patients. In a way, it is easier to predict the future in these cases.\n\n![Number of visits](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Choi2016_results2.png?raw=true \"Doctor AI performance as it knows more about the patient\")\n\n\n+ Performance of Doctor AI in other datasets:\n  + Potential to transfer knowledge accross hospitals. Pre-train Doctor AI on Sutter Health dataset and fine-tuned in MIMIC II dataset.\n  \n![Transfer knowledge](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Choi2016_results3.png?raw=true \"Performance of Doctor AI in other datasets\")\n\n#### Extras\n+ There is an interview about the paper at the [Data Skeptic](https://dataskeptic.com/blog/episodes/2017/doctor-ai) podcast.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05942"
    },
    "714": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1038/srep26094",
        "transcript": "#### Goal\n\n+ Use unsupervised deep learning to obtain a low-dimensional representation of a patient from EHR data. \n+ A better representation will facilitate clinical prediction tasks.\n\n#### Architecture:\n\n+ Patient EHR is obtained from the Hospital Data Warehouse:\n  + demographic info \n  + ICD-9 codes \n  + medication, labs\n  + clinical notes: free text \n+ Use stacked denoising autoencoders (SDA) to obtain an abstract representation of the patient with lower dimensionality.\n\n![Framework](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Miotto2016_framework.png?raw=true \"Deep Patient Framework\")\n\n#### Dataset:\n\n+ Data Warehouse from Mount Sinai Hospital in NY.\n+ All patient records that had a diagnosed disease (ICD-9 code) between 1980 and 2014 - approximately 1.2 million patients with 88.9 records/patient - were initially selected.\n  + 1980-2013: training, 2014: test.\n\n*Data Cleaning*:\n+ Diseases diagnosed in fewer than 10 patients in the training dataset were eliminated.\n+ Diseases that could not be diagnosed through EHR labels were eliminated. Related to social behavior (HIV), fortuitous events (injuries, poisoning) or unspecific ('other cancers'). The final list contains 78 diseases.\n\n*Final version of the dataset (raw patient representation)*:\n\n+ Training: 704,587 patients (to obtain deep features post SDA).\n+ Validation: 5,000 patients (for the evaluation of the predictive model for diseases).\n+ Test: 76,214 patients (for the evaluation of the predictive model for diseases).\n+ 41072 columns - demographic info, ICD-9, medication, lab test, free text (LDA topic modeling dimension 300)\n+ Very high dimensional but very sparse representation\n\n#### Results:\n\n*Stacked Denoisinig Autoencoders for low-dimensional patient representation*:\n+ 3 layers of denoising autoencoders.\n+ Each layer has 500 neurons. Patient is now represented by a dense vector of 500 features.\n+ Inputs are normalized to lie in the [0, 1] interval.\n+ Inputs in each of the layers have added noise at a ratio of 5% noise (masking noise corruption - value of these features is set to '0').\n+ Sigmoid activation function.\n\n*Classifiers for disease prediction*:\n+ Random forest classifiers with 100 trees trained for each of the 78 diseases.\n\n*Baseline for comparison*:\n+ PCA with 100 components, k-means with 500 clusters, GMM with 200 mixes and ICA with 100 components. (see Discussion)\n+ RawFeat: original patient EHR features: sparse vector with 41072 features (~ 1% of non-zero entries).\n+ Threshold to rank as \"positive\": 0.6\n\n*Aggregate performance in predicting diseases*:\n\n![Aggregate performance](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Miotto2016_results1.png?raw=true \"Aggregate performance\")\n\n+ Comment: This result of F-Score = 0.181 implies a precision of 0.102 (let us assume a recall in the order of 80%), which means that with each correct diagnosis, the Deep Patient generates approximately 9 false alarms.\n\n*Performance for some particular diseases*:\n\n![Disease results](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Miotto2016_results2.png?raw=true \"Results for some diseases\")\n\n#### Discussion:\n+ DeepPatient *does not* use lab results in model building. Only the *frequency* at which the analysis is performed is taken into account.\n+ Future enhancements:\n  +  Describe a patient with a temporal sequence of vectors s instead of summarizing all data in one vector.\n  +  Add other categories of EHR data, such as insurance details, family history and social behaviors.\n  + Use PCA as a pre-processing step before SDA?\n+ Caveat: the comparisons does not seem to be fair. If the autoencoder has dimension 500, the other baselines should also have dimension 500. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1038/srep26094"
    },
    "715": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LiptonKW16",
        "transcript": "#### Motivation:\n\n+ Take advantage of the fact that missing values can be very informative about the label.\n+ Sampling a time series generates many missing values.\n\n![Sampling](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Lipton2016_motivation.png?raw=true)\n\n\n#### Model (indicator flag):\n\n+ Indicator of occurrence of missing value.\n\n![Indicator](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Lipton2016_indicator.png?raw=true)\n\n+ An RNN can learn about missing values and their importance only by using the indicator function. The nonlinearity from this type of model helps capturing these dependencies.\n+ If one wants to use a linear model, feature engineering is needed to overcome its limitations.\n  + indicator for whether a variable was measured at all\n  + mean and standard deviation of the indicator \n  + frequency with which a variable switches from measured to missing and vice-versa. \n\n#### Architecture:\n\n+ RNN with target replication following the work \"Learning to Diagnose with LSTM Recurrent Neural Networks\" by the same authors.\n\n![Architecture](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Lipton2016_architecture.png?raw=true)\n\n\n#### Dataset:\n\n+ Children's Hospital LA\n+ Episode is a multivariate time series that describes the stay of one patient in the intensive care unit\n\nDataset properties  | Value \n---------|----------\nNumber of episodes | 10,401\nDuration of episodes | From 12h to several months\nTime series variables | Systolic blood pressure, Diastolic blood pressure,  Peripheral capillary refill rate, End tidal CO2, Fraction of inspired O2,  Glasgow coma scale,  Blood glucose,  Heart rate,  pH,  Respiratory rate, Blood O2 Saturation,  Body temperature,  Urine output.\n\n\n\n#### Experiments and Results:\n\n**Goal**\n+ Predict 128 diagnoses.\n+ Multilabel: patients can have more than one diagnose.\n\n**Methodology**\n+ Split: 80% training, 10% validation, 10% test.\n+ Normalized data to be in the range [0,1].\n\n+ LSTM RNN:\n  + 2 hidden layers with 128 cells. Dropout = 0.5, L2-regularization: 1e-6\n  + Training for 100 epochs. Parameters chosen correspond to the time that generated the smallest error in the validation dataset.\n+ Baselines:\n  + Logistic Regression (L2 regularization)\n  + MLP with 3 hidden layers and 500 hidden neurons / layer (parameters chosen via validation set)\n  + Tested with raw-features and hand-engineered features.\n+ Strategies for missing values:\n  + Zeroing\n  + Impute via forward / backfilling\n  + Impute with zeros and use indicator function\n  + Impute via forward / backfilling and use indicator function\n  + Use indicator function only\n\n#### Results\n\n+ Metrics:\n  + Micro AUC, Micro F1: calculated by adding the TPs, FPs, TNs and FNs for the entire dataset and for all classes.\n  + Macro AUC, Macro F1: Arithmetic mean of AUCs and F1 scores for each of the classes.\n  + Precision at 10: Fraction of correct diagnostics among the top 10 predictions of the model.\n    + The upper bound for precision at 10 is 0.2281 since in the test set there are on average 2.281 diagnoses per patient.\n\n![Results](https://raw.githubusercontent.com/tiagotvv/ml-papers/master/clinical-data/images/Lipton2016_results.png?raw=true)\n\n\n#### Discussion:\n\n+ Predictive model based on data collected following a given routine. This routine can change if the model is put into practice. Will the model predictions in this new routine remain valid?\n\n+ Missing values in a way give an indication of the type of treatment being followed.\n\n+ Trade-off between complex models operating on raw features and very complex features operating on more interpretable models.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.04130"
    },
    "716": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.01865",
        "transcript": "#### Motivation:\n+ When sampling a clinical time series, missing values become ubiquitous due to a variety of factors such as frequency of medical events (when a blood test is performed, for example).\n+ Missing values \u200b\u200bcan be very informative about the label - *informative missingness*.\n+ The goal of the paper is to propose a deep learning model that **exploits the missingness patterns** to enhance its performance.\n\n#### Time series notation:\n\nMultivariate time series with $D$ variables of length $T$: \n+ ${\\bf X} = ({\\bf x}_1, {\\bf x}_2, \\ldots, {\\bf x}_T)^T \\in \\mathbb{R}^{T \\times D}$.\n+  ${\\bf x}_t  \\in \\mathbb{R}^{D}$ is the $t$-th measurement of all variables. \n+ $x_t^d$ is the $d$-th component of  ${\\bf x}_t$.\n\nMissing value information is incorporated using *masking* and *time-interval* concepts.\n + Masking: says which of the entries are missing values.\n  + Masking vector ${\\bf m}_t \\in \\{0, 1\\}^D$,  $m_t^d = 1$ if $x_t^d$ exists and $m_t^d = 0$ if $x_t^d$ is missing.\n + Time-interval: temporal pattern of 'no-missing' observations. Represented by time-stamps $s_t$ and time intervals $\\delta_t$ (since its last observation).\n\nExample: \n${\\bf X}$: input time series with 2 variables, \n$$ {\\bf X} = \\begin{pmatrix}\n47 & 49 & NA & 40 & NA & 43 & 55 \\\\ NA & 15 & 14 & NA & NA & NA & 15 \n\\end{pmatrix}\n$$\nwith time-stamps\n$${\\bf s} =  \\begin{pmatrix}\n0 & 0.1 & 0.6 & 1.6 & 2.2 & 2.5 & 3.1\n\\end{pmatrix}\n$$\nThe masking vectors  ${\\bf m}_t$ and time intervals ${\\delta}_t$ for each variable are computed and stacked forming the masking matrix ${\\bf M}$ and time interval matrix ${\\bf \\Delta}$ :\n$$ {\\bf M} = \\begin{pmatrix}\n1 & 1 & 0 & 1 & 0 & 1 & 1 \\\\ 0 & 1 & 1 & 0 & 0 & 0 & 1 \n\\end{pmatrix}\n$$\n$$ {\\bf \\Delta} = \\begin{pmatrix}\n0 & 0.1 & 0.5 & 1.5 & 0.6 & 0.9 & 0.6 \\\\ 0 & 0.1 & 0.5 & 1.0 & 1.6 & 1.9 & 2.5 \n\\end{pmatrix}\n$$\n\n#### Proposed Architecture:\n+ GRU (Gated Recurrent Units) with \"trainable\" decays:\n + Input decay: which causes the variable to converge to its empirical mean instead of simply filling with the last value of the variable. The decay of each input is treated independently\n + Hidden state decay: Attempts to capture richer information from missing patterns. In this case the hidden state of the network at the previous time step is decayed.\n\n\n#### Dataset:\n+ MIMIC III v1.4: https://mimic.physionet.org/\n + Input events, Output events, Lab events, Prescription events\n+ PhysioNet Challenge 2012: https://physionet.org/challenge/2012/\n\n|  \tMIMIC III |\tPhysioNet 2012 | \n-----------------------------------|--------------|---------------------\nNumber of samples ($N$) |\t19714 |\t4000\nNumber  of variables ($D$)\t|99      |\t33\nMean number of time steps\t|35.89 |\t68.91\nMaximum number of time steps|150  |\t155\nMean of variable missing rate \t|0.9621|\t0.8225\n\n\n#### Experiments and Results:\n**Methodology**\n+ Baselines:\n + Logistic Regression, SVM, Random Forest (PhysioNet sampled every 1h. MIMIC sampled every 2h). Forward / backfilling imputation. Masking vector is concatenated input to inform the models what inputs are imputed.\n + LSTM with mean imputation.\n + Variations of the proposed GRU model:\n  + GRU-mean: impute average of the training set.\n  + GRU-forward: impute last value.\n  + GRU-simple: masking vectors and time interval are inputs. There is no imputation.\n  + GRU-D: proposed model.\n+ Batch normalization and dropout (p = 0.5) applied to the regression layer.\n+ Normalized inputs to have a mean of 0 and standard deviation 1.\n+ Parameter optimization: early stopping on validation set.\n\n**Results**\n\nMortality Prediction (results in terms of AUC):\n+ Proposed GRU-D outperforms other models on both datasets: \n + AUC = 0.8527 $\\pm$ 0.003 for MIMIC-III and 0.8424 $\\pm$ 0.012 for PhysioNet\n+ Random Forest and SVM are the best non-RNN baselines.\n+ GRU-simple was the best RNN variant.\n\nMultitask Prediction (results in terms of AUC):\n+ PhysioNet: mortality, <3 days, surgery, cardiac condition.\n+ MIMIC III: 20 diagnostic categories.\n+ The proposed GRU-D outperforms other baseline models.\n\n#### Positive Aspects:\n+ Instead of performing simple mean imputation or using indicator functions, the paper exploits missing values and missing patterns in a novel way.\n+ The paper performs lengthy comparisons against baselines.\n\n#### Caveats:\n+ Clinical mortality datasets usually have very high imbalance between classes. In such cases, AUC alone is not the best metric to evaluate. It would have been interesting to see the results in terms of precision/recall.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.01865"
    },
    "717": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1705.08498",
        "transcript": "#### Goal:\n\nPredict interventions on ICU patients using LSTM and CNN. \n\n#### Dataset:\n\nMIMIC-III v.1.4 https://mimic.physionet.org/\n\n+ Patients over 15 years of age with intensive care stay between 12h and 240h. (Only the first stay is considered for each patient) - 34148 unique records.\n+ 5 static variables.\n+ 29 vital signs and test results.\n+ Clinical notes of patients (presented as time series).\n\n#### Feature Engineering:\n\n+ Topic Modeling of clinical notes: Vector of topics using Latent Dirichlet Allocation (LDA)\n+ Physiological Words: Vital / Laboratory results converted to z-scores - [integer values \u200b\u200bbetween -4 and 4] and score is one-hot encoded (each vital / lab is replaced by 9 columns). It is good idea to avoid the imputation of missing values as the physiological word in this case is the all-zero vector.\n\nFeature vector:\n+ is the concatenation of the static variables, physiological words for each vital/lab and the  topic vector.\n+ 1 feature vector / patient / hour.\n+ 6-hour slice used to predict a 4-hour window after a 6-hour gap. All the features values are normalized between 0 and 1. (static variables are replicated).\n\n#### Target Classes:\n\nFor some of the procedures to be predicted there are 4 classes:\n+ Onset: Y goes from 0 to 1 during the prediction window.\n+ Wean: Y goes from 1 to 0 during the prediction window.\n+ Stay On: Y stays at 1 throughout prediction window.\n+ Stay Off: Y stays at 0 for the entire prediction window.\n\n#### Setup of the Experiments:\n\n+ Dataset Split: 70% training, 10% validation, 20% test.\n\nLong Short Term Memory (LSTM) Networks:\n+ Dropout P(keep) = 0.8, L2 regularization.\n+ 2 hidden layers: 512 nodes in each.\n\nConvolutional Neural Networks\n+ 3 different temporal granularities (3, 4, 5 hours). 64 filters in each.\n+ Features are treated as channels. 1D temporal convolution.\n+ Dropout between fully connected layers. P (keep) = 0.5.\n\nTensorFlow 1.0.1 - Adam optimizer. Minibatches of size 128. \nValidation set used for early stopping (metric: AUC).\n\n#### Results:\n\n+ Baseline for comparison: L2-regularized Logistic Regression\n+ Metrics: \n + AUC per class.\n + AUC macro = Arithmetic mean of AUC per class.\n\n+ Proposed architectures outperforms baseline.\n+ Physiological words improve performance (especially on high class imbalance scenario).\n\n#### Model Interpretability:\n+ LSTM: feature occlusion like analysis. The feature is replaced by uniformly distributed noise between 0 and 1 and variation in AUC is computed.\n+ CNN: analysis of the maximally activating trajectories.\n\n#### Positive Aspects:\n+ Relevant work: In the healthcare domain is very important to anticipate events.\n+ Built on top of rich and heterogeneous data: It leverages large amounts of ICU data. \n+ The proposed model is not a complete black-box. Interpretability is crucial if the system is to be adopted in the future. \n\n#### Caveats:\n+ Some of the methodology is not clearly explained:\n + How the split of the dataset was performed? Was it on a patient-level?\n + When testing the logistic regression baseline it is not clear how the feature vector was built. Was it built by simply flattening the 6-hour chunk?\n + For the raw data test, it was not mentioned the way the missing values were treated. ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1705.08498"
    },
    "718": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=nakamoto2008bitcoin",
        "transcript": "This paper describes the Bitcoin peer-to-peer currency system.\n\nThe paper first describes a bitcoin as a chain of transactions, the latest of which contains the current owner's public key, and is cryptographically signed by the previous owner.  The integrity of this chain (and therefore the ownership of the coin) is maintained as each transaction contains a hash of the previous transaction.  Therefore, it is impossible to corrupt any one transaction in the chain without affecting all future transactions as well.\n\nhttps://i.imgur.com/hKbgsdi.png\n\nThe key issue with this system is *double counting* in a peer-to-peer environment.  In physical currency, this problem does not exist, as the purchase using a physical token precludes using the same token by the same owner again.  In other digital currencies, the problem of double counting is solved using a central arbiter.\n\nThe authors solve this using a timestamp service, which stamps a block of transactions.  The transaction which have the earliest timestamp is used as the true transaction.  The block of transactions is linked to a previous block, thereby forming a *blockchain*.\n\nhttps://i.imgur.com/BYlVirg.png\n\nTo prevent the integrity of the blockchain, each block contains the previous hash, thereby ensuring the integrity of the chain.  There is also a nonce value, which is an input to a computationally difficult problem which must be solved in order to place a new block in the blockchain.  This makes it difficult for an attacker to tack on a new chain.  \n\nThe problem that must be solved is to increment the nonce such that the hash of the block contains a specified number of 0 bits.  The difficulty of this problem can be adjusted by increasing the number of 0's, and grows over time to accommodate the growing power of computer hardware.  Note that verifying that this problem is easily solved.\n\nThe resulting blockchain therefore constitutes a large investment of computational resources, with the longest blockchain being used as the standard.\n\nNot all participants in bitcoin need to mine, so an incentive is given.  When a new block is mined, the individual who mines it is given some portion of bitcoin either out of thin-air or as a transaction fee.  This also deters would-be corruptors of the blockchain, as the computational resources needed may be better spent adding new blocks on the newest version of the blockchain.",
        "sourceType": "blog",
        "linkToPaper": "?name=henryzlo"
    },
    "719": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LeeS00",
        "transcript": "The paper introduces nonnegative matrix factorization, a technique which used in fields such as chemometrics.  The problem formulation is this:\n\n$$\n\\underset{W,H}{\\text{argmin}} ~ d(X, WH) \\\\\\\\\n\\text{s.t.} ~W_{ij}, H_{ij} \\ge 0\n$$\n\nWhere:\n- $X \\in \\mathbb{R}^{n \\times m}$ is a matrix of data, for example, $n$ samples of $m$ features.  Each element of $X$ is nonnegative, as are the elements of $W$ and $H$.\n- $W \\in \\mathbb{R}^{n \\times k}$ represents how each of the $n$ samples belong to each of the $k$ \"clusters\".\n- $H \\in \\mathbb{R}^{k \\times m}$ describes each of the $k$ clusters in terms of the $m$ variables.\n- $d$ is some cost function, for example, sum of squared differences.\n\nThe non-negativity constraint means the clusters (represented by the rows $W$ of $W$) describe clusters in terms of what features are present.  This may make interpretation easier in some instances, but makes the optimization problem more difficult.\n\nThe paper mentions two loss functions, sum of squared error:\n\n$$\nd(X,WH) = \\sum_{ij} |X_{ij}-(WH)_{ij}|^2\n$$\n\n and a measure similar to unnormalized Kullback-Leibler divergence:\n \n $$\nd(X,WH) = \\sum_{ij} \\left( X_{ij} \\log \\frac{X_{ij}}{(WH)_{ij}} - X_{ij}+(WH)_{ij} \\right)\n$$\n\nFor each of these objectives, multiplicative update rules are given.  For squared error:\n\n$$\nH_{ij} \\leftarrow H_{ij} \\frac{(W^TX)_{ij}}{(W^TWH)}_{ij} ~~~\nW_{ij} \\leftarrow W_{ij} \\frac{(XH^T)_{ij}}{(WHH^T)}_{ij}\n$$\n\nAnd for divergence:\n\n$$\nH_{ij} \\leftarrow H_{ij} \\frac{\\sum_a W_{ai} X_{aj} / (WH)_{aj}} { \\sum_b W_{bj}} ~~~~\nW_{ij} \\leftarrow W_{ij} \\frac{\\sum_a H_{ja} X_{ia} / (WH)_{ia}} { \\sum_b H_{jb}}\n$$\n\nThese rules are applied alternatingly; fix $W$ and update $H$, then fix $H$ and update $W$.\n\nThese multiplicative updates are essentially a diagonally rescaled gradient descent.  The authors then prove that these update rules do not increase the objective.  Future authors have pointed out that not increasing the cost does not imply convergence; e.g. the parameters could stop updating, without having reached a minima.  However, a trivial fix to the multiplicative update rules (ensuring no division by zero, by making 0 elements slightly positive) alleviates these problems.\n  ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization"
    },
    "720": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/Girshick15",
        "transcript": "This paper is awesome in that it is full of content.\n\nThey replace W with its TSVD.  When t, the reduced rank, is small, it saves computation time because you multiply smaller matrices twice rather than multiplying bigger matrices once.\n\nIn terms of units in hidden layers, they turn n->m into n->t->m\n\nThis only works for the forward pass though.  If you were to train this, you would only learn a rank t matrix.  In which case, there would be no reason to have the t->m layer.  Unless you want more nonlinearities, but less rank; haven't seen that before.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ICCV.2015.169"
    },
    "721": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/CohenL016",
        "transcript": "The proposed Randomout algorithm randomly restarts filter weights in a CNN when it has a low gradient.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.05931"
    },
    "722": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/YuZX17",
        "transcript": "This paper proposes a framework where an agent learns to navigate a 2D maze-like\nenvironment (XWORLD) from (templated) natural language commands, in the process\nsimultaneously learning visual representations, syntax and semantics of language and\nperforming navigation actions. The task is essentially VQA + navigation; at every step\nthe agent either gets a question about the environment or navigation command,\nand the output is either a navigation action or answer. Key contributions:\n\n- Grounding and recognition are tied together to be two versions of the same problem.\nIn grounding, given an image feature map and label (word), the problem is to find\nregions of the image corresponding to word semantics (attention map); and in\nrecognition, given an image feature map and attention, the problem is to assign\na word label. And thus word embeddings (for grounding) and softmax layer weights\n(for recognition) are tied together. This enables transferring concepts\nlearnt during recognition to navigation.\n\t- Further, recognition is modulated by question intent. For e.g. given an\n\tattention map that highlights an agent's west, should it be recognized as\n\t'west', 'apple' or 'red' (location, object or attribute)? It depends on what\n\tthe question asks. Thus, GRU encoding of question produces an embedding mask\n\tthat modulates recognition. The equivalent when grounding is that word embeddings\n\tare passed through fully-connected layers.\n\n- Compositionality in language is exploited by performing grounding and\nrecognition by sequentially (softly) attending to parts of a sentence and\ngrounding in image. The resulting attention map is selectively combined\nwith attention from previous timesteps for final decision.\n\n## Weaknesses / Notes\n\nAlthough the environment is super simple, it's a neat framework and it is useful\nthat the target is specified in natural language (unlike prior/concurrent work\ne.g. Zhu et al., ICRA17). The model gets to see a top-down centred view of the\nentire environment at all times, which is a little weird.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.09831"
    },
    "723": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1706.01427",
        "transcript": "This paper describes using Relation Networks (RN) for reasoning about relations between objects/entities.\nRN is a plug-and-play module and although expects object representations as input,\nthe semantics of what an object is need not be specified, so object representations\ncan be convolutional layer feature vectors or entity embeddings from text, or something else.\nAnd the feedforward network is free to discover relations between objects (as opposed to being\nhand-assigned specific relations).\n\n- At its core, RN has two parts:\n\t- a feedforward network `g` that operates on pairs of object representations,\n\tfor all possible pairs, all pairwise computations pooled via element-wise addition\n\t- a feedforward network `f` that operates on pooled features for downstream\n\ttask, everything being trained end-to-end\n\n- When dealing with pixels (as in CLEVR experiment), individual object representations are\nspatially distinct convolutional layer features (196 512-d object representations for VGG conv5 say).\nThe other experiment on CLEVR uses explicit factored object state representations with 3D coordinates,\nshape, material, color, size.\n\n- For bAbI, object representations are LSTM encodings of supporting sentences.\n\n- For VQA tasks, `g` conditions its processing on question encoding as well, as relations\nthat are relevant for figuring out the answer would be question-dependent.\n\n\n## Strengths\n\n- Very simple idea, clearly explained, performs well. Somewhat shocked that it\nhasn't been tried before.\n\n## Weaknesses / Notes\n\nFairly simple idea \u2014 let a feedforward network\noperate on all pairs of object representations and figure out relations\nnecessary for downstream task with end-to-end training. And it is fairly general in its design,\nrelations aren't hand-designed and neither are object representations \u2014 for\nRGB images, these are spatially distinct convolutional layer features, for text,\nthese are LSTM encodings of supporting facts, and so on. This module can be dropped\nin and combined with more sophisticated networks to improve performance at VQA.\n\nRNs also offer an alternative design choice to prior works on CLEVR, that have\nthis explicit notion of programs or modules with specialized roles (that need to be pre-defined),\nas opposed to letting these relations emerge, reducing dependency on hand-designing\nmodules and adding in inductive biases from an architectural point-of-view for\nthe network to reason about relations (earlier end-to-end VQA models didn't have\nthe capacity to figure out relations).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1706.01427"
    },
    "724": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1703.06029",
        "transcript": "\nThis paper proposes a conditional GAN-based image captioning model.\nGiven an image, the generator generates a caption, and given an image\nand caption, the discriminator/evaluator distinguishes between generated\nand real captions. Key ideas:\n\n- Since caption generation involves sequential sampling, which is\nnon-differentiable, the model is trained with policy gradients, with\nthe action being the choice of word at every time step, policy being\nthe distribution over words, and reward the score assigned by the\nevaluator to generated caption.\n\n- The evaluator's role assumes a completely generated caption as input\n(along with image), which in practice leads to convergence issues. Thus\nto accommodate feedback for partial sequences during training, Monte Carlo\nrollouts are used, i.e. given a partial generated sequence, n completions\nare sampled and run through the evaluator to compute reward.\n\n- The evaluator's objective function consists of three terms\n    - image-caption pairs from training data (positive)\n    - image and generated captions (negative)\n    - image and sampled captions for other images from training data (negative)\n\n- Both the generator and evaluator are pretrained with supervision / MLE, then\nfine-tuned with policy gradients. During inference, evaluator score is used as\nthe beam search objective.\n\n## Strengths\n\nThis is neat paper with insightful ideas (Monte Carlo rollouts for assigning\nrewards to partial sequences, evaluator score as beam search objective),\nand is perhaps the first work on C-GAN-based image captioning.\n\n## Weaknesses / Notes",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1703.06029"
    },
    "725": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1604.00289",
        "transcript": "This paper performs a comparitive study of recent advances in deep learning with human-like learning from a cognitive science point of view. Since natural intelligence is still the best form of intelligence, the authors list a core set of ingredients required to build machines that reason like humans.\n\n- Cognitive capabilities present from childhood in humans.  \n    - Intuitive physics; for example, a sense of plausibility of object trajectories, affordances.\n    - Intuitive psychology; for example, goals and beliefs.\n- Learning as rapid model-building (and not just pattern recognition).\n    - Based on compositionality and learning-to-learn.\n    - Humans learn by inferring a general schema to describe goals, object types and interactions. This enables learning from few examples. \n    - Humans also learn richer conceptual models.\n        - Indicator: variety of functions supported by these models: classification, prediction, explanation, communication, action, imagination and composition.\n        - Models should hence have strong inductive biases and domain knowledge built into them; structural sharing of concepts by compositional reuse of primitives.\n- Use of both model-free and model-based learning.\n    - Model-free, fast selection of actions in simple associative learning and discriminative tasks.\n    - Model-based learning when a causal model has been built to plan future actions or maximize rewards.\n- Selective attention, augmented working memory, and experience replay are low-level promising trends in deep learning inspired from cognitive psychology.\n    - Need for higher-level aforementioned ingredients.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.00289v2"
    },
    "726": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/AndreasRDK15",
        "transcript": "This paper presents an approach to visual question answering by dynamically composing networks of independent neural modules based on the semantic parsing of the question. Main contributions:\n\n- Independent neural modules that can be combined together and jointly trained.\n    - Attention: Convolutional layer, with different filters for different instances. For example, attend[dog], attend[cat], etc.\n    - Re-attention: FC-ReLU-FC-ReLU, weights are different for different instances. For example, re-attend[above], re-attend[not], etc.\n    - Combination: Stacks two attention maps, followed by conv-ReLU to map to a single attention map. For example, combine[and], combine[except], etc.\n    - Classification: Combines attention map and image, followed by FC-Softmax to map to answer. For example, classify[colors].\n    - Measurement: FC-ReLU-FC-Softmax, takes attention map as input. For example, measure[exists].\n\n- Structured representations are extracted from questions and these are then mapped to network layouts, including the connections between them.\n    - All leaves become attend modules, all internal nodes become re-attend or combine modules dependent on their arity, and root nodes become measure modules for yes/no questions and classify modules for all other question types.\n    - Networks with the same structure but different instantiations can be processed in the same batch. For example, classify[color]\\(attend[cat]\\), classify[where]\\(attend[truck]\\).\n\n- Predictions from the module network are combined with LSTM representations to get the final answer.\n    - Syntactic regularities: 'what is flying?' and 'what are flying?' get mapped to the same module network.\n    - Semantic regularities: 'green' is an implausible answer for 'what color is the bear?'.\n\n- Experiments are performed on the synthetic SHAPES dataset and VQA dataset.\n    - Performance on the SHAPES dataset is better as it is designed to benefit from compositionality.\n\n## Strengths\n\n- This model takes advantage of the inherently compositional property of language, which makes a lot of sense. VQA is an extremely complex task and breaking it up into separate functions/modules is an excellent approach.\n\n## Weaknesses / Notes\n\n- Mapping from syntactic structure to module network is hand-designed. Ideally, the model should learn this too to generalize.\n\n- Due to its compositional nature, this kind of model can possibly be used in the zero-shot learning setting, i.e. generalize to novel question types that the network hasn't seen before.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.02799"
    },
    "727": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=huang2016networks",
        "transcript": "\nThis paper presents a way to reduce the expected network depth of deep residual networks during training by randomly dropping a subset of residual blocks and bypassing them with identity connections. The 'survival' probability $p\\_l$ decreases linearly with depth (from 1.0 to 0.5 at last layer) so as to keep layers that extract low-level features with higher probability. At test time, residual block functions are scaled by the expected number of times it appears during training, i.e. $p\\_l$. This model achieves lower test errors than ResNets (with ReLU activations) on CIFAR-10, CIFAR-100 and SVHN.\n\n## Strengths\n\n- Shorter expected depth leads to faster training (>25% speedup).\n\n- Helps reduce the vanishing gradient problem as shown by the mean gradient magnitude v/s epochs plot.\n\n- Linear decay of survival probability works better than uniform survival, which supports the intuition that low-level features need to be reliably present.\n\n- Stochastic depth acts as a regularizer. The 1202-layer stochastic depth residual network shows improvements over the 110-layer network, while the original ResNets paper reports overfitting and higher test error with 1000+ layers.\n\n## Weaknesses / Notes\n\n- Test errors for the updated ResNet architecture (ReLU activation inside residual function) are missing. That should perform better. Also, numbers on ImageNet.\n\n- Stochastic depth can be interpreted as sequential ensembling as compared to parallel ensembles.\n\n- It would be interesting to look at the filters learnt by stochastic depth residual networks, and to understand whether/how these networks learn hierarchical features as compared to the conventional CNN intuitions of compositionality.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.09382"
    },
    "728": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/LiMRJGG16",
        "transcript": "This paper builds on top of a bunch of existing ideas for building neural conversational agents so as to control against generic and repetitive responses. \n\nTheir model is the sequence-to-sequence model with attention (Bahdanau et al.), first trained with the usual MLE loss and fine-tuned with policy gradients to optimize for specific conversational properties. Specifically, they define 3 rewards:\n\n1. Ease of answering \u2014 Measured as the likelihood of responding to a query with a list of hand-picked dull responses (more negative log likelihood is higher reward). \n2. Information flow \u2014 Consecutive responses from the same agent (person) should have different information, measured as negative of log cosine distance (more negative is better).\n3. Semantic coherence \u2014 Mutual information between source and target (the response should make sense wrt query). $P(a|q) + P(q|a)$ where a is answer, q is question.\n\nThe model is pre-trained with the usual supervised objective function, taking source as concatenation of two previous utterances. Then they have two stages of policy gradient training, first with just a mutual information reward and then with a combination of all three. The policy network (sequence-to-sequence model) produces a probability distribution over actions (responses) given state (previous utterances). To estimate the gradient in an iteration, the network is frozen and responses are sampled from the model, the rewards for which are then averaged and gradients are computed for first L tokens of response using MLE and remaining T-L tokens with policy gradients, with L being gradually annealed to zero (moving towards just the long-term reward).\n\nEvaluation is done based on length of dialogue, diversity (distinct unigram, bigrams) and human studies on\n\n1. Which of two outputs has better quality (single turn)\n2. Which of two outputs is easier to respond to, and\n3. Which of two conversations have better quality (multi turn).\n\n## Strengths\n\n- Interesting results\n    - Avoids generic responses\n    - 'Ease of responding' reward encourages responses to be question-like\n- Adding in hand-engineereed approximate reward functions based on conversational properties and using those to fine-tune a pre-trained network using policy gradients is neat.\n- Policy gradient training also encourages two dialogue agents to interact with each other and explore the complete action space (space of responses), which seems desirable to identify modes of the distribution and not converge on a single, high-scoring, generic response.\n\n## Weaknesses / Notes\n\n- Evaluating conversational agents is hard. BLEU / perplexity are intentionally avoided as they don't necessarily reward desirable conversational properties.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D16/D16-1127.pdf"
    },
    "729": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeZRS15",
        "transcript": "This paper introduces Residual Nets (ResNets), which was the\nwinning submission (152-layer deep) at ILSVRC 2015 and MS-COCO 2015, and achieves\na top-5 error rate of 3.57% (ensemble of two  nets). Main contributions:\n\n- The key idea is that deeper networks face the degradation problem, i.e.\nhigher training and test error than shallower nets, because they're harder\nto optimize for approximating identity mapping by multiple non-linear layers.\n    - They mitigate this problem by forcing solvers to learn residual functions\n    i.e. $f(x) = H(x) - x$, by adding shortcut connections. If identity mapping is\n    the optimal formulation, the learned weights should drive $f(x)$ to 0 (and they\n    observe that this is a suitable preconditioning as most residual function responses\n    are small).\n\n- Shortcut connections (for identity mapping) don't require additional parameters.\n    - Size transformations are done by zero-padding (no parameters) or projections. Projections\n    introduce additional parameters and perform slightly better.\n\n- Bottleneck design is used to further reduce computational complexity, i.e. 1x1 convolutional\nlayers before and after 3x3 convolutions to reduce and increase dimensions.\n\n- For detection and localization tasks, they use ResNets in the Faster-RCNN setting.\n\n## Strengths\n\n- ResNets are significantly deeper and more accurate yet computationally cheaper than VGG.\n\n- A single ResNet outperforms previous state-of-the-art ensembles. Their final winning submission\nis an ensemble of two networks.\n\n## Weaknesses / Notes\n\n- The idea of shortcut connections to force blocks to learn residual functions preconditioned\non identity mapping is neat, and more so because it doesn't require additional parameters.\n\n- A lot of results and design decisions merit further investigation and reasoning.\n    - Why do shortcuts skip 2 or 3 layers? What happens to performance if we increase the number of layers skipped?\n    - How well do shortcut connections work with Inception modules? The statistical principles\n    underlying both these architectures seem to be orthogonal, does performance further improve?\n    - 152 seems to be an arbitrary number of layers that 'worked'.\n\n- The degradation problem seen when making networks deeper by initializing\nlayers with identity weight matrices seems to be contradictory to the results\npresented in the Net2Net paper.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.03385"
    },
    "730": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.06432",
        "transcript": "This paper presents a neat method for learning spatio-temporal representations from videos. Convolutional features from intermediate layers of a CNN are extracted, to preserve spatial resolution, and fed into a modified GRU that can (in theory) learn infinite temporal dependencies. Main contributions:\n\n- Their variant of GRU (called GRU-RCN) uses convolution operations instead of fully-connected units.\n    - This exploits the local correlation in image frames across spatial locations.\n    - Features from pool2, pool3, pool4, pool5 are extracted and fed into independent GRU-RCNs. Hidden states at last time step are now feature volumes, which are average pooled to reduce to 1x1 spatially, and fed into a linear + softmax classifier. Outputs from each of these classifiers is averaged to get the final prediction.\n\n- Other variants that they experiment with are bidirectional GRU-RCNs and stacked GRU-RCNs i.e. GRU-RCNs with connections between them (with maxpool operations for dimensionality reduction).\n    - Bidirectional GRU-RCNs perform the best.\n    - Stacked GRU-RCNs perform worse than the other variants, probably because of limited data.\n\n- They evaluate their method on action recognition and video captioning, and show significant improvements on a CNN+RNN baseline, comparing favorably with other state-of-the-art methods (like C3D).\n\n## Strengths\n\n- The idea is simple and elegant. Earlier methods for learning video representations typically used 3D convolutions (k x k x T filters), which suffered from finite temporal capacity, or RNNs sitting on top of last-layer CNN features, which is unable to capture finer spatial resolution. In theory, this formulation solves both.\n\n- Changing fully-connected operations to convolutions has the additional advantage of requiring lesser parameters (n\\_input x n\\_output x input\\_width x input\\_height v/s n\\_input x n\\_output x k\\_width x k\\_height).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.06432"
    },
    "731": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.07838",
        "transcript": "This paper presents a model that can dynamically split computation across coarse, low-capacity sub-networks and fine, high-capacity sub-networks. The coarse model processes the entire input data and is typically shallow while the fine model focuses on a few important regions of the input and is deeper. For images as input, this is a hard attention mechanism that can be trained with stochastic gradient descent and doesn't require a task-specific attention policy trained by reinforcement learning. Key ideas:\n\n- A deep network h can be decomposed into bottom layers f and top layers g such that $h(x) = g(f(x))$. Further, f consists of two alternate sub-networks $f\\_c$ and $f\\_f$. $f\\_c$ is a low-capacity sub-network while $f\\_f$ is a high-capacity sub-network.\n\n- g should be able to use representations from $f\\_c$ and $f\\_f$ dynamically. $f\\_c$ processes the entire input while $f\\_f$ only a few important regions of the input.\n\n- The coarse model processes the entire input and the norm of the gradient of the entropy with respect to the coarse vector at each spatial region is computed which is a measure of saliency. The use of the entropy gradient as a saliency measure encourages selecting input regions that could affect the uncertainty in the model\u2019s predictions the most.\n\n- The top-k input regions with highest saliency values are processed by the fine model. The refined representation for input to the top layers consists of both coarse and fine vectors. During backpropagation, gradients are computed for the refined model, i.e. propagating gradients at each position into either the coarse or fine features, depending on which was used.\n\n- To make sure $f\\_c$ and $f\\_f$ representations are interchangeable and input to the top layers has smooth transitions, an additional objective term minimizes the squared distance between coarse and fine representations and this additional term is used only to optimize the coarse layers, not the fine layers.\n\n- Experiments on cluttered MNIST, SVHN and comparison with RAM, DRAW and study with various values of number of patches for fine processing.\n\n## Strengths\n\n- Neat, general way to split computation based on importance of input; a hard-attention mechanism that can be trained with SGD, unlike RAM.\n\n- Entropy gradient as a measure of saliency is an interesting idea, and it doesn't need labels i.e. can be used at test time.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.07838"
    },
    "732": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eccv/HeZRS16",
        "transcript": "This is follow-up work to the ResNets paper. It studies the propagation formulations behind the connections of deep residual networks and performs ablation experiments. A residual block can be represented with the equations $y_l = h(x_l) + F(x_l, W_l); x_{l+1} = f(y_l)$. $x_l$ is the input to the l-th unit and $x_{l+1}$ is the output of the l-th unit. In the original ResNets paper, $h(x_l) = x_l$, $f$ is ReLu, and F consists of 2-3 convolutional layers (bottleneck architecture) with BN and ReLU in between. In this paper, they propose a residual block with both $h(x)$ and $f(x)$ as identity mappings, which trains faster and performs better than their earlier baseline. Main contributions:\n\n- Identity skip connections work much better than other multiplicative interactions that they experiment with:\n    - Scaling $(h(x) = \\lambda x)$: Gradients can explode or vanish depending on whether modulating scalar \\lambda > 1 or < 1.\n    - Gating ($1-g(x)$ for skip connection and $g(x)$ for function F):\n    For gradients to propagate freely, $g(x)$ should approach 1, but\n    F gets suppressed, hence suboptimal. This is similar to highway\n    networks. $g(x)$ is a 1x1 convolutional layer.\n    - Gating (shortcut-only): Setting high biases pushes initial $g(x)$\n    towards identity mapping, and test error is much closer to baseline.\n    - 1x1 convolutional shortcut: These work well for shallower networks\n    (~34 layers), but training error becomes high for deeper networks,\n    probably because they impede gradient propagation.\n\n- Experiments on activations.\n    - BN after addition messes up information flow, and performs considerably\n    worse.\n    - ReLU before addition forces the signal to be non-negative, so the signal is monotonically increasing, while ideally a residual function should be free to take values in (-inf, inf).\n    - BN + ReLU pre-activation works best. This also prevents overfitting, due\n    to BN's regularizing effect. Input signals to all weight layers are normalized.\n\n## Strengths\n\n- Thorough set of experiments to show that identity shortcut connections\nare easiest for the network to learn. Activation of any deeper unit can\nbe written as the sum of the activation of a shallower unit and a residual\nfunction. This also implies that gradients can be directly propagated to\nshallower units. This is in contrast to usual feedforward networks, where\ngradients are essentially a series of matrix-vector products, that may vanish, as networks grow deeper.\n\n- Improved accuracies than their previous ResNets paper.\n\n## Weaknesses / Notes\n\n- Residual units are useful and share the same core idea that worked in\nLSTM units. Even though stacked non-linear layers are capable of asymptotically\napproximating any arbitrary function, it is clear from recent work that\nresidual functions are much easier to approximate than the complete function.\nThe [latest Inception paper](http://arxiv.org/abs/1602.07261) also reports\nthat training is accelerated and performance is improved by using identity\nskip connections across Inception modules.\n\n- It seems like the degradation problem, which serves as motivation for\nresidual units, exists in the first place for non-idempotent activation\nfunctions such as sigmoid, hyperbolic tan. This merits further\ninvestigation, especially with recent work on function-preserving transformations such as [Network Morphism](http://arxiv.org/abs/1603.01670), which expands the Net2Net idea to sigmoid, tanh, by using parameterized activations, initialized to identity mappings.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-46493-0_38"
    },
    "733": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ChenGS15",
        "transcript": "This paper presents a simple method to accelerate the training\nof larger neural networks by initializing them with parameters\nfrom a trained, smaller network. Networks are made wider or deeper\nwhile preserving the same output as the smaller network which\nmaintains performance when training starts, leading to faster\nconvergence. Main contributions:\n\n- Net2Deeper\n    - Initialize layers with identity weight matrices\n    to preserve the same output.\n    - Only works when activation function $f$ satisfies\n    $f(If(x)) = f(x)$ for example ReLU, but not sigmoid, tanh.\n\n- Net2Wider\n    - Additional units in a layer are randomly sampled\n    from existing units. Incoming weights are kept the same\n    while outgoing weights are divided by the number of\n    replicas of that unit so that the output at the next layer\n    remains the same.\n\n- Experiments on ImageNet\n    - Net2Deeper and Net2Wider models converge faster to the\n    same accuracy as networks initialized randomly.\n    - A deeper and wider model initialized with Net2Net from\n    the Inception model beats the validation accuracy (and\n    converges faster).\n\n## Strengths\n\n- The Net2Net technique avoids the brief period of low performance that exists in\nmethods that initialize some layers of a deeper network from a trained\nnetwork and others randomly.\n\n- This idea is very useful in production systems which essentially have to\nbe lifelong learning systems. Net2Net presents an easy way to immediately\nshift to a model of higher capacity and reuse trained networks.\n\n- Simple idea, clearly presented.\n\n\n## Weaknesses / Notes\n\n- The random mapping algorithm for different layers was done manually\nfor this paper. Developing a remapping inference algorithm should be\nthe next step in making the Net2Net technique more general.\n\n- The final accuracy that Net2Net models achieve seems to depend only\non the model capacity and not the initialization. I think this merits\nfurther investigation. In this paper, it might just be because of randomness\nin training (dropout) or noise added to the weights of the new units to\napproximately represent the same function (when not using dropout).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05641"
    },
    "734": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eccv/JohnsonAF16",
        "transcript": "This paper proposes the use of pretrained convolutional neural networks that have already learned to encode semantic information as loss functions for training networks for style transfer and super-resolution. The trained networks corresponding to selected style images are capable of performing style transfer for any content image with a single forward pass (as opposed to explicit optimization over output image) achieving as high as 1000x speedup and similar qualitative results as Gatys et al. Key contributions:\n\n- Image transformation network\n    - Convolutional neural network with residual blocks and strided & fractionally-strided convolutions for in-network downsampling and upsampling. \n    - Output is the same size as input image, but rather than training the network with a per-pixel loss, it is trained with a feature reconstruction perceptual loss.\n\n- Loss network\n    - VGG-16 with frozen weights \n    - Feature reconstruction loss: Euclidean distance between feature representations\n    - Style reconstruction loss: Frobenius norm of the difference between Gram matrices, performed over a set of layers.\n\n- Experiments\n    - Similar objective values and qualitative results as explicit optimization over image as in Gatys et al for style transfer\n    - For single-image super-resolution, feature reconstruction loss reconstructs fine details better and 'looks' better than a per-pixel loss, even though PSNR values indicate otherwise. Respectable results in comparison to SRCNN.\n\n## Weaknesses / Notes\n\n- Although fast, limited by styles at test-time (as opposed to iterative optimizer that is limited by speed and not styles). Ideally, there should be a way to feed in style and content images, and do style transfer with a single forward pass.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-46475-6_43"
    },
    "735": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/CooijmansBLC16",
        "transcript": "This paper presents a re-parameterization of the LSTM to successfully apply batch normalization, which results in faster convergence and improved generalization on a several sequential tasks. Main contributions:\n\n- Batch normalization is applied to the input to hidden and hidden to hidden projections.\n    - Separate statistics are maintained for each timestep, estimated over each minibatch during training and over the whole dataset during test.\n    - For generalization to longer sequences during test time, population statistics of time T\\_max are used for all time steps beyond it.\n    - The cell state is left untouched so as not to hinder the gradient flow.\n\n- Proper initialization of batch normalization parameters to avoid vanishing gradients.\n    - They plot norm of gradient of loss wrt hidden state at different time steps for different BN variance initializations. High variance ($\\gamma = 1$) causes gradients to die quickly by driving activations to the saturation region.\n    - Initializing BN variance to 0.1 works well.\n\n## Strengths\n\n- Simple idea, the authors finally got it to work. Proper initialization of BN parameters and maintaining separate estimates for each time step play a key role.\n\n## Weaknesses / Notes\n\n- It would be useful in practice to put down a proper formulation for using batch normalization with variable-length training sequences.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.09025"
    },
    "736": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.06431",
        "transcript": "\nThis paper introduces an interpretation of deep residual networks as implicit ensembles of exponentially many shallow networks. For a residual block $i$, there are $2^{i-1}$ paths from input to $i$, and the input to $i$ is a mixture of  $2^{i-1}$ different distributions. The interpretation is backed by a number of experiments such as removing or re-ordering residual blocks at test time and plotting norm of gradient v/s number of residual blocks the gradient signal passes through. Removing $k$ residual blocks (for k <= 20) from a network of depth n decreases the number of paths to $2^{n-k}$ but there are still sufficiently many valid paths to not hurt classification error, whereas sequential CNNs have a single viable path which gets corrupted. Plot of gradient at input v/s path length shows that almost all contributions to the gradient come from paths shorter than 20 residual blocks, which are the effective paths. The paper concludes by saying that network 'multiplicity', which is the number of paths, plays a key role in terms of the network's expressability.\n\n## Strengths\n\n- Extremely insightful set of experiments. These experiments nail down the intuitions as to why residual networks work, as well as clarify the connections with stochastic depth (sampling the network multiplicity during training i.e. ensemble by training) and highway networks (reduction in number of available paths by gating both skip connections and paths through residual blocks).\n\n## Weaknesses / Notes\n\n- Connections between effective paths and model compression.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.06431"
    },
    "737": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1608.02908",
        "transcript": "This paper introduces a modification to the ResNets architecture with multi-level shortcut connections (shortcut from input to pre-final layer as level 1, shortcut over each residual block group as level 2, etc) as opposed to single-level shortcut connections in prior work on ResNets. The authors perform experiments with multi-level shortcut connections on regular ResNets, ResNets with pre-activations and Wide ResNets. Combined with drop-path regularization via stochastic depth and exploration over optimal shortcut level number and optimal depth/width ratio to avoid vanishing gradients and overfitting, this architecture achieves state-of-the-art error rates on CIFAR-10 (3.77%), CIFAR-100 (19.73%) and SVHN (1.59%).\n\n## Strengths\n\n- Fairly exhaustive set of experiments over\n    - Shortcut level numbers.\n    - Identity mapping types: 1) zero-padding shortcuts, 2) 1x1 convolutions for projections and others identity, and 3) all 1x1 convolutions.\n    - Residual block size (2 or 3 3x3 convolutional layers).\n    - Depths (110, 164, 182, 218) and widths for both ResNets and Pre-ResNets.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1608.02908"
    },
    "738": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ReedZZL15",
        "transcript": "This paper introduces an end-to-end trainable neural model capable of performing analogical reasoning in image representations followed by decoding back to image space. Specifically, given a 4-tuple A:B::C:D, the task is to apply the transformation A:B to C. The motivation is clear \u2014 humans are excellent at generalizing to hypothetical transformations about images (\"what if this chair were rotated 30 degrees clockwise?\").\n\n- The objective function follows directly from vector addition: $MSE(d - g(f(b) - f(a) + f(c)))$ where $f$ and $g$ are convolutional neural networks.\n\n- In case of rotation, a purely additive transformation is not optimal because repeated application of this transformation to the same query image will never return to the original point. Instead, multiplicative interactions or MLPs are used to condition the transformation on $c$ as well.\n\n- Analogy-making is also performed on disentangled representations, which separate factors of variation to separate coordinates and are learnt from distinct images $a,b, c$ such that the objective is $MSE(c - g(s . f(a) + (1-s) . f(b)))$ where $s$ are switch variables to disentangle features. Disentangled image features allow the analogy-making model to traverse the manifold of a given factor or subset of factors.\n\n- Experiments on transforming shapes, generating 2D video game sprites and 3D car renderings.\n\n## Strengths\n\n- Neat idea, well-presented",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5845-deep-visual-analogy-making"
    },
    "739": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.07571",
        "transcript": "This paper introduces the task of dense captioning and proposes\na network architecture that processes an image and produce region descriptions\nin a single pass and can be trained end-to-end. Main contributions:\n\n- Dense captioning\n    - Generalization of object detection (caption consists of single word)\n    and image captioning (region consists of whole image).\n\n- Fully convolution localization network\n    - Fully differentiable, can be trained jointly with the rest of the network\n    - Consists of a region proposal network, box regression (similar to Faster R-CNN)\n    and bilinear interpolation (similar to Spatial Transformer Networks) for\n    sampling.\n\n- Network details\n    - Convolutional layer features are extracted for image\n    - For each element in the feature map, k anchor boxes of different aspect ratios\n    are selected in the input image space.\n    - For each of these, the localization layer predicts offsets and confidence.\n    - The region proposals are projected on the convolutional feature map and a sampling\n    grid is computed from output feature map to input (bilinear sampling).\n    - The computed feature map is passed through an MLP to compute representations\n    corresponding to each region.\n    - These are passed (in a batch) as the first word to an LSTM (Show and Tell) which\n    is trained to predict each word of the caption.\n\n## Strengths\n\n- Fully differentiable 'spatial attention' mechanism (bilinear interpolation)\nin place of RoI pooling as in the case of Faster R-CNN.\n    - RoI pooling is not differentiable with respect to the input proposal coordinates.\n\n- Fast, and impressive qualitative results.\n\n## Weaknesses / Notes\n\nThe model is very well engineered together from different works (Faster R-CNN +\nSpatial Transformer Networks + Show & Tell).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.07571"
    },
    "740": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/GregorDGRW15",
        "transcript": "\nThis paper introduces a neural network architecture\nthat generates realistic images sequentially. They\nalso introduce a differentiable attention mechanism\nthat allows the network to focus on local regions of the image\nduring reconstruction. Main contributions:\n\n- The network architecture is similar to other variational\nauto-encoders, except that\n    - The encoder and decoder are recurrent networks (LSTMs).\n    The encoder's output is conditioned on the decoder's\n    previous outputs, and the decoder's outputs are iteratively\n    added to the resulting distribution from which images are\n    generated.\n    - The spatial attention mechanism restricts the input region\n    observed by the encoder and available to write for the decoder.\n\n## Strengths\n\n- The spatial soft attention mechanism is effective and fully differentiable,\nand can be used for other tasks.\n\n- Images generated by DRAW look very realistic.\n\n## Weaknesses / Notes",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/gregor15.html"
    },
    "741": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BahdanauCB14",
        "transcript": "This paper introduces an attention mechanism (soft memory access)\nfor the task of neural machine translation. Qualitative and quantitative\nresults show that not only does their model achieve state-of-the-art BLEU\nscores, it performs significantly well for long sentences which was a\ndrawback in earlier NMT works. Their motivation comes from the fact that\nencoding all information from an input sentence into a single fixed length\nvector and using that in the decoder was probably a bottleneck. Instead,\ntheir decoder uses an attention vector, which is a weighted sum of the\ninput hidden states, and is learned jointly. Main contributions:\n\n- The encoder is a bidirectional RNN, in which they take the annotation\nof each word to be the concatenation of the forward and backward RNN states.\nThe idea is that the hidden state should encode information from both the\nprevious and following words.\n\n- The proposed attention mechanism is a weighted sum of the input hidden\nstates, the weights for which come from an attention function (a single-layer\nperceptron, which takes as input the previous hidden state of the decoder and\nthe current word annotation from the encoder) and are softmax-normalized.\n\n## Strengths\n\n- Incorporating the attention mechanism shows large improvements on\nlonger sentences. The attention matrix is easily interpretable as well,\nand visualizations in the paper show that higher weights are being assigned\nto input words that correspond to output words irrespective of their order\nin the sequence (unlike an attention model that uses a mixture of Gaussians\nwhich is monotonic).\n\n## Weaknesses / Notes\n\n- Their model formulation to capture long-term dependencies is far more\nprincipled than Sutskever et al's inverting the input idea. They should\nhave done a comparative study with their approach as well though.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1409.0473"
    },
    "742": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1412.6856",
        "transcript": "This paper hypothesizes that a CNN trained for scene classification automatically\ndiscovers meaningful object detectors, representative of the scene categories,\nwithout any explicit object-level supervision. This claim is backed by well-designed\nexperiments which are a natural extension of the primary insight that since scenes\nare composed of objects (a typical bedroom would have a bed, lamp; art gallery would\nhave paintings, etc), a CNN that performs reasonable well on scene recognition\nmust be localizing objects in intermediate layers.\n\n## Strengths\n\n- Demonstrates the difference in learned representations in Places-CNN and ImageNet-CNN.\n    - The top 100 images that have the largest average activation per layer are picked and it's shown that earlier layers such as pool1 prefer similar images for both networks while deeper layers tend to be more specialized to the specific task of scene or object categorization i.e. ~75% of the top 100 images that show high activations for fc7 belong to ImageNet for ImageNet-CNN and Places for Places-CNN.\n- Simplifies input images to identify salient regions for classification.\n    - The input image is simplified by iteratively removing segments that cause the least decrease in classification score until the image is incorrectly classified. This leads them to the minimal image representation (sufficient and necessary) that is needed by the network to correctly recognize scenes, and many of these contain objects that provide discriminative information for scene classification.\n- Visualizes the 'empirical receptive fields' of units.\n    - The top K images with highest activations for a given unit are identified. To identify which regions of the image lead to high unit activations, the image is replicated with occluders at different regions. The occluded images are passed through the network and large changes in activation indicate important regions. This leads them to generate feature maps and finally to empirical receptive fields after appropriate centre-calibration, which are more localized and smaller than the theoretical size.\n- Studies the visual concepts / semantics captured by units.\n    - AMT workers are surveyed on the segments that maximally activate units. They're asked to tag the visual concept, mark negative samples and provide the level of abstraction (from simple elements and colors to objects and scenes). Plot of distribution of semantic categories at each layer shows that deeper layers do capture higher levels of abstraction and Places-CNN units indeed discover more objects than ImageNet-CNN units.\n\n## Weaknesses / Notes\n\n- Unclear as to how they obtain soft, grayed out images from the iterative segmentation methodology in the first approach where they generate minimal image representations needed for accurate classification. I would assume these regions to be segmentations with black backgrounds and hard boundaries. Perez et al. (2013) might have details regarding this.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1412.6856"
    },
    "743": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/JaderbergSZK15",
        "transcript": "This paper introduces a neural networks module that can learn input-dependent\nspatial transformations and can be inserted into any neural network. It supports\ntransformations like scaling, cropping, rotations, and non-rigid deformations.\nMain contributions:\n\n- The spatial transformer network consists of the following:\n    - Localization network that regresses to the transformation parameters\n    given the input.\n    - Grid generator that uses the transformation parameters to produce a\n    grid to sample from the input.\n    - Sampler that produces the output feature map sampled from the input\n    at the grid points.\n\n- Differentiable sampling mechanism\n    - The sampling is written in a way such that sub-gradients can be defined\n    with respect to grid coordinates.\n    - This enables gradients to be propagated through the grid generator and\n    localization network, and for the network to jointly learn the spatial\n    transformer along with rest of the network.\n\n- A network can have multiple STNs\n    - at different points in the network, to model incremental transformations\n    at different levels of abstraction.\n    - in parallel, to learn to focus on different regions of interest. For example,\n    on the bird classification task, they show that one STN learns to be a head detector,\n    while the other focuses on the central part of the body.\n\n## Strengths\n\n- Their attention (and by extension transformation) mechanism is differentiable\nas opposed to earlier works on non-differentiable attention mechanisms that used\nreinforcement learning (REINFORCE). It also supports a richer variety of\ntransformations as opposed to earlier works on learning transformations, like DRAW.\n\n- State-of-the-art classification performance on distorted MNIST, SVHN, CUB-200-2011.\n\n## Weaknesses / Notes\n\nThis is a really nice way to generalize spatial transformations in a differentiable\nmanner so the model can be trained end-to-end. Classification performance, and more\nimportantly, qualitative results of the kind of transformations learnt on larger datasets\n(like ImageNet) should be evaluated.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5854-spatial-transformer-networks"
    },
    "744": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/YangHGDS16",
        "transcript": "This paper introduces a Stacked Attention Network (SAN) for visual question answering.\nSAN uses a multiple layer attention mechanism that uses the semantic question representation\nto query the image and locate relevant visual regions, and to infer the answer.\nDetails of the SAN model:\n\n- Image features are extracted from the last pooling layer of a deep CNN (like VGG-net).\n    - Input images are first scaled to 448 x 448, so at the last pooling layer, features\n    have the dimension 14 x 14 x 512 i.e. 512-dimensional vectors at each image location\n    with a receptive field of 32 x 32 in input pixel space.\n\n- Question features are the last hidden state of the LSTM.\n    - Words are one-hot encoded, transferred to a vector space by passing through an\n    embedding matrix and these word vectors are fed into the LSTM at each time step.\n\n- Image and question features are combined into a query vector to locate relevant visual regions.\n    - Both the LSTM hidden state and 512-d image feature vector at each location are transferred\n    to the same dimensionality (say k) by a fully connected layer, and added and passed through\n    a non-linearity (tanh).\n    - Each k-dimensional feature vector is then transformed down to a single scalar and\n    a softmax is taken over all image regions to get the attention distribution (say p\\_{I}).\n    - This attention distribution is used to weight the pooling layer visual features (\\sum_{i}p\\_{i}v\\_{i})\n    and added to the LSTM vector to get a new query vector.\n    - In subsequent attention layers, this updated query vector is used to repeat the same process\n    of getting an attention distribution.\n    - The final query vector is used to compute a softmax over the answers.\n\n## Strengths\n\n- The multi-layer attention mechanism makes sense intuitively and the qualitative results\nsomewhat indicate that going from the first attention layer to subsequent attention layers,\nthe network is able to focus on fine-grained visual regions as it discovers relationships\namong multiple objects ('what are sitting in the basket on a bicycle').\n\n- SAN benefits VQA, they demonstrate state-of-the-art accuracies on multiple datasets, with\nquestion-type breakdown as well.\n\n## Weaknesses / Notes\n\n- Right now, the attention distribution is learnt in an unsupervised manner by the network.\nIt would be interesting to think about adding supervisory attention signal. Another way to\nimprove accuracies would be to use deeper LSTMs.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2016.10"
    },
    "745": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1412.6806",
        "transcript": "This paper simplifies the convolutional network proposed\nby Alex Krizhevsky by replacing max-pooling with strided\nconvolutions (under the assumption that max-pooling is\nrequired only for dimensionality reduction). They also\npropose a novel technique for visualizing representations\nlearnt by intermediate layers that produces nicer visualizations\nin input pixel space than DeconvNet (Zeiler et al) and Saliency\nmap (Simonyan at al) approaches.\n\n## Strengths\n\n- Their model performs at par or better than the original AlexNet formulation.\n    - Max-pooling replaced by convolution with stride 2\n    - Fully-connected layers replaced by 1x1 convolutions and global averaging + softmax\n    - Smaller filter size (same intuition as VGGNet paper)\n- Combining the DeconvNet (Zeiler et al.) and backpropagation (Simonyan et al.) approaches\nat the ReLU operator (which is the only point of difference) by masking out values where at\nleast one of input activation or output reconstruction is negative (guided backprop) is neat\nand leads to nice visualizations.\n\n## Weaknesses / Notes\n\n- Saliency maps generated from guided backpropagation definitely look much better\nas compared to DeconvNet visualizations and saliency maps from Simonyan et al's paper.\nIt works better probably because the negative saliency values only arise from the very\nfirst convolution, since negative error signals are never propagated back through the\nnon-linearities.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1412.6806"
    },
    "746": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/RedmonDGF16",
        "transcript": "This paper models object detection as a regression problem for bounding\nboxes and object class probabilities with a single pass through the CNN. The\nmain contribution is the idea of dividing the image into a 7x7 grid, and having\neach cell predict a distribution over class labels as well as a bounding box\nfor the object whose center falls into it. It's much faster than R-CNN and\nFast R-CNN, as the additional step of extracting region proposals has been\nremoved.\n\n## Strengths\n\n- Works real-time. Base model runs at 45fps and a faster version goes up to\n150fps, and they claim that it's more than twice as fast as other works on\nreal-time detection.\n\n- End-to-end model; Localization and classification errors can be jointly\noptimized.\n\n- YOLO makes more localization errors and fewer background mistakes than\nFast R-CNN, so using YOLO to eliminate false background detections from\nFast R-CNN results in ~3% mAP gain (without much computational time as R-CNN\nis much slower).\n\n## Weaknesses / Notes\n\n- Results fall short of state-of-the-art: 57.9% v/s 70.4% mAP (Faster R-CNN).\n\n- Performs worse at detecting small objects, as at most one object per grid\ncell can be detected.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2016.91"
    },
    "747": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Kim14f",
        "transcript": "\nThis paper reports on a series of experiments with CNNs trained\non top of pre-trained word vectors for sentence-level classification\ntasks. The model achieves very good performance across datasets, and\nstate-of-the-art on a few. The proposed model has an input layer\ncomprising of concatenated 'word2vec' embeddings, followed by a single\nconvolutional layer with multiple filters, max-pooling over time,\nfully connected layers and softmax. They also experiment with static\nand non-static channels which basically implies whether they finetune\nword2vec embeddings or not.\n\n## Strengths\n\n- Very simple yet powerful model formulation, which achieves really good\nperformance across datasets.\n\n- The different model formulations drive home the point that initializing\ninput vectors with word2vec embeddings is better than random initializations.\nFinetuning these embeddings for the task leads to further improvements over\nstatic embeddings.\n\n## Weaknesses / Notes\n\n- No intuition as to why the model with both static and non-static channels\ngives mixed results.\n\n- They briefly mention that they experimented with SENNA embeddings which lead\nto worse results although no quantitative results are provided. It would have been\ninteresting to have a comparative study with GloVe embeddings as well.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1408.5882"
    },
    "748": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SimonyanVZ13",
        "transcript": "This paper attempts to understand the representations learnt by deep\nconvolutional neural networks by introducing two interpretable visualization\ntechniques. Main contributions:\n\n- Class model visualizations\n    - These are obtained by making numerical optimizations in the input\n    space to maximize the class score. Gradients are calculated wrt input\n    and are used to update the input image (initialized with zero image),\n    while weights are kept fixed to those obtained from training.\n- Image-specific saliency map visualizations\n    - These are approximated by using the same gradient as before (gradient\n    of class score wrt input). The absolute pixel-wise max across channels produces\n    the saliency map.\n- Relation between DeconvNet and optimization-based visualizations\n    - Visualizations using DeconvNet are the same as gradient-based methods except\n    for ReLU. In regular backprop, gradients flow through ReLU to units with positive\n    input activations, whereas in case of a DeconvNet, it is computed on positive output\n    reconstructions.\n\n## Strengths\n\n- The visualization techniques are simple ideas and the results are interpretable. They show\nthat the method proposed by Erhan et al. in an unsupervised setting is useful to CNNs trained\nin a supervised manner as well.\n- The image-specific class saliency can be interpreted as those pixels which need to be changed\nthe least to have a maximum impact on the classification score.\n- The relation between DeconvNet visualizations and optimization-based visualizations is\ninsightful.\n\n## Weaknesses / Notes\n\n- The thinking behind initializing with zero image and L2 regularization in class model\nvisualizations was missing.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1312.6034"
    },
    "749": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SzegedyLJSRAEVR14",
        "transcript": "This paper introduces a neural network architecture\nthat is deeper and wider, yet optimizing for computational\nefficiency by approximating the expected sparse structure\n(following from Arora et al's work) using readily available\ndense blocks. An ensemble of 7 models (all with the same\narchitecture but different image sampling) achieved top spot\nin the classification task at ILSVRC2014.\n\n\"Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.\"\n\nMain contributions:\n\n- A more generalized exploration of the NIN architecture,\ncalled the Inception module.\n    - 1x1 convolutions to capture dense information clusters\n    - 3x3 and 5x5 to capture more spatially spread out\n    clusters\n    - Ratio of 3x3 and 5x5 to 1x1 convolutions increases as we go deeper\n    as features of higher abstraction are less spatially\n    concentrated.\n- To avoid the blow-up of output channels cause by merging outputs\nof convolutional layers and pooling layer, they use 1x1 convolutions\nfor dimensionality reduction. This has the added benefit of another\nlayer of non-linearity (and thus increasing discriminative capability).\n- Multiple intermediate layers are tied to the objective function. Since\nfeatures produced by intermediate layers of a deep network are\nsupposed to be very discriminative, and to strengthen the gradient signal\npassing through them during back-propagation, they attach auxiliary classifiers\nto intermediate layers.\n    - During training, they do a weighted sum of this loss with the total loss\n    of the network.\n    - At test time, these auxiliary networks are discarded.\n    - Architecture: average pooling, 1x1 convolution (for dimensionality reduction),\n    dropout, linear layer with softmax.\n\n## Strengths\n\n- Excellent results on ILSVRC2014.\n\n## Weaknesses / Notes\n\n- Even though the authors try to explain some of the intuition, most of\nthe design decisions seem arbitrary.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1409.4842"
    },
    "750": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/YosinskiCBL14",
        "transcript": "This paper studies the transferability of features learnt at different layers\nof a convolutional neural network. Typically, initial layers of a CNN learn\nfeatures that resemble Gabor filter or color blobs, and are fairly general, while\nthe later layers are more task-specific. Main contributions:\n\n- They create two splits of the ImageNet dataset (A/B) and explore how performance\nvaries for various network design choices such as\n    - Base: CNN trained on A or B.\n    - Selffer: first n layers are copied from a base network, and the rest of the\n    network is randomly initialized and trained on the same task.\n    - Transfer: first n layers are copied from a base network, and the rest of the\n    network is trained on a different task.\n    - Each of these 'copied' layers can either be fine-tuned or kept frozen.\n\n- Selffer networks without fine-tuning don't perform well when the split is somewhere\nin the middle of the network (n = 3-6). This is because neurons in these layers co-adapt\nto each other's activations in complex ways, which get broken up when split.\n    - As we approach final layers, there is lesser for the network to learn and so these\n    layers can be trained independently.\n    - Fine-tuning a selffer network gives it the chance to re-learn co-adaptations.\n\n- Transfer networks transferred at lower n perform better than larger n, indicating\nthat features get more task-specific as we move to higher layers.\n    - Fine-tuning transfer networks, however, results in better performance. They argue\n    that better generalization is due to the effect of having seen the base dataset,\n    even after considerable fine-tuning.\n\n- Fine-tuning works much better than using random features.\n\n- Features are more transferable across related tasks than unrelated tasks.\n    - They study transferability by taking two random data splits, and splits of\n    man-made v/s natural data.\n\n## Strengths\n\n- Experiments are thorough, and the results are intuitive and insightful.\n\n## Weaknesses / Notes\n\n- This paper only analyzes transferability across different splits of ImageNet\n(as similar/dissimilar tasks). They should have reported results on transferability\nfrom one task to another (classification/detection) or from one dataset to another\n(ImageNet/MSCOCO).\n\n- It would be interesting to study the role of dropout in preventing co-adaptations\nwhile transferring features.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks"
    },
    "751": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1312.6199",
        "transcript": "The paper introduces two key properties of deep neural networks:\n\n- Semantic meaning of individual units.\n    - Earlier works analyzed learnt semantics by finding images that maximally activate individual units.\n    - Authors observe that there is no difference between individual units and random linear combinations of units.\n    - It is the entire space of activations that contains the bulk of semantic information.\n\n- Stability of neural networks to small perturbations in input space.\n    - Networks that generalize well are expected to be robust to small perturbations in the input, i.e. imperceptible noise in the input shouldn't change the predicted class.\n    - Authors find that networks can be made to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error.\n    - These 'adversarial examples' generalize well to different architectures trained on different data subsets.\n\n## Strengths\n\n- The authors propose a way to make networks more robust to small perturbations by training them with adversarial examples in an adaptive manner, i.e. keep changing the pool of adversarial examples during training. In this regard, they draw a connection with hard-negative mining, and a network trained with adversarial examples performs better than others.\n\n- Formal description of how to generate adversarial examples and mathematical analysis of a network's stability to perturbations are useful studies.\n\n## Weaknesses / Notes\n\n- Two images that are visually indistinguishable to humans but classified differently by the network is indeed an intriguing observation.\n\n- The paper feels a little half-baked in parts, and some ideas could've been presented more clearly.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1312.6199"
    },
    "752": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhouLXTO14",
        "transcript": "This paper introduces the Places dataset, which is a scene-centric\ndataset at the scale of ImageNet (which is for object recognition)\nso as to enable training of deep CNNs like AlexNet, and achieves\nstate-of-the-art for scene benchmarks. Main contributions:\n\n- Collects a dataset at ImageNet scale for scene recognition.\n- Achieves state-of-the-art on scene benchmarks: SUN397, MIT Indoor67, Scene15, SUN Attribute.\n- Introduces measures for comparing datasets: density and diversity.\n- Makes a thorough comparison b/w ImageNet and Places, from dataset to classification results to learned representation visualizations.\n\n## Strengths\n\n- Relative density and diversity are neat ideas for comparing datasets, and are backed by AMT experiments.\n    - Relative density: The more visually similar a nearest neighbour is to a randomly sampled image from a dataset, the more dense it is.\n    - Relative diversity: The more visually similar two randomly sampled images from a dataset are, the less diverse it is.\n- Demonstrates via activation and mean image visualizations that different representations are learned by CNNs trained on ImageNet and Places\n    - Conv1 layer visualizations can be directly seen, and are similar for ImageNet-CNN and Places-CNN. They capture low-level information like oriented edges and colors.\n    - For higher layers, they visualize the average of top 100 images that maximize activations per unit. As we go deeper, ImageNet-CNN units have receptive fields that look more like object-blobs and Places-CNN have RFs that look more like landscapes with spatial structures.\n\n## Weaknesses / Notes\n\n- No explanation as to why the model trained on ImageNet and Places combined (minus overlapping images) performs better than ImageNet-CNN or Places-CNN on some benchmarks and worse on others.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database"
    },
    "753": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1312.4400",
        "transcript": "This paper studies a very natural generalization of convolutional layers\nby replacing a single filter that slides over the input feature map with\na \"micro network\" (multi-layer perceptron). The authors argue that good\nabstractions are highly non-linear functions of input data and instead of\ngenerating an overcomplete number of feature maps and shrinking them down\nin higher layers (as is the case in traditional CNNs), it would be beneficial\nto generate better representations on each local patch, before feeding into\nthe next layer. Main contributions:\n\n- Replaces the convolutional filter with a multi-layer perceptron.\n- Instead of fully connected layers, uses global average pooling.\n\n## Strengths\n\n- Natural generalization of convolutional layers and thorough analysis.\n- Global average pooling of feature layers is easier to interpret and less prone to overfitting.\n- Better or at par with state-of-the-art classification results on CIFAR-10, CIFAR-100, SVHN, MNIST.\n\n## Weaknesses / Notes\n\n- Should have explored NIN without dropout.\n- Results on ImageNet missing.\n- The global average pooling idea, although interpretable,\ndoesn't seem to give easily to fine-tuning the network to\nother datasets. In finetuning, we usually replace and learn\njust the last layer.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1312.4400"
    },
    "754": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GravesWD14",
        "transcript": "Neural Turing Machine (NTM) consists of a neural network controller interacting with a working memory bank in a learnable manner. This is analogous to computers \u2014 controllers = CPU (hidden activations as registers) and memory matrix = RAM. Key ideas:\n\n- Controller (modified RNN) interacts with external world via input and output vectors, and with memory via read and write \"heads\"\n\n- \"Read\" vector is a convex combination of row-vectors of $M_t$ (memory matrix at time $t$) \u2014 $r\\_t = \\sum w\\_t(i) M\\_t(i)$ where w_t is a vector of weightings over N memory locations\n\n- \"Writing\" is decomposed into 1) erasing and 2) adding\n    - The write head produces the erase vector e_t and the add vector a_t along with the vector of weightings over memory locations w_t\n    - $M\\_t(i) = M\\_{t-1}(i)[1 - w_t(i) e_t] + w\\_t(i) a\\_t$\n    - Erase and add vectors control which components of memory are updated, while weightings w_t control which locations are updated\n\n- Weight vectors are produced by an addressing mechanism\n    - Content-based addressing\n        - Each head produces length M key k_t that is compared to each vector M_t(i) by cosine similarity and a temperature parameter. The weightings are normalized (softmax).\n\n    - Location-based addressing\n        - Interpolation: Each head produces interpolation gate g_t that is used to blend between weighting at previous time step and the content weighting of current tilmestep $w^{g}\\_t = g\\_t w^{c}\\_t + (1-g\\_t)w\\_{t-1}$\n        - Shift: Circular convolution (modulo N) with a shift weighting distribution, for example softmax over integer shift positions (say 3 locations)\n        - Sharpening: Each head emits \\gamma_t to sharpen the final weighting\n\n- Experiments on copy, repeat-copy, associative memory, N-gram emulator and priority sort\n\n## Links\n\n- [Attention and Augmented RNNs](http://distill.pub/2016/augmented-rnns/)\n- [NTM-Lasagne](https://medium.com/snips-ai/ntm-lasagne-a-library-for-neural-turing-machines-in-lasagne-2cdce6837315)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1410.5401"
    },
    "755": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/GirshickDDM14",
        "transcript": "This paper presents R-CNN, an approach to do object detection using CNNs pre-trained for image classification. Object proposals are extracted from the image using Selective Search, dilated by few pixels, warped to CNN input size and fed into the CNN to extract features (they experiment with pool5, fc6, fc7). These extracted feature vectors are scored using SVMs, one per class. Bounding box regression, where they predict parameters to move the proposal closer to ground-truth, further boosts localization.\n\nThe authors use AlexNet, pre-trained on ImageNet and finetuned for detection. Object proposals with IOU overlap greater than 0.5 are treated as positive examples, and others as negative, and a 21-way classification (20 object categories + background) is set up to finetune the CNN. After finetuning, SVMs are trained per class, taking only the ground-truth boxes as positives, and IOU <= 0.3 as negatives.\n\nR-CNN achieves major performance improvements on PASCAL VOC 2007/2010 and ILSVRC2013 detection datasets. Finally, this method is extended to do semantic segmentation and achieves competitive results.\n\n## Strengths\n\n- The method is simple and effective.\n- Extensive ablation studies show why R-CNN works.\n    - FC7 is the best feature to use (against pool5, fc6).\n    - Fine-tuning provides a large boost in performance.\n    - VGG performs better than AlexNet.\n    - Bounding box regression further improves localization.\n\n## Weaknesses / Notes\n\n- Each region proposal is treated independently, which adds up to compute time.\n- There are lots of different parts; the network can't be trained end-to-end.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/CVPR.2014.81"
    },
    "756": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SutskeverVL14",
        "transcript": "This paper presents a simple approach to predicting\nsequences from sequential input. They use a multi-layer\nLSTM-based encoder-decoder architecture and show\npromising results on the task of neural machine translation.\nTheir approach beats a phrase-based statistical machine\ntranslation system by a BLEU score of > 1.0 and is close to\nstate-of-the-art if used to re-rank 1000-best predictions\nfrom the SMT system. Main contributions:\n\n- The first LSTM encodes an input sequence to a single\nvector, which is then decoded by a second LSTM. End of sequence\nis indicated by a special character.\n    - 4-layer deep LSTMs.\n    - 160k source vocabulary, 80k target vocabulary. Trained on\n    12M sentences. Words in output sequence are generated by a softmax\n    over fixed vocabulary.\n    - Beam search is used at test time to predict translations\n    (Beam size 2 does best).\n\n## Strengths\n\n- Qualitative results (PCA projections) show that learned representations are\nfairly insensitive to active/passive voice, as sentences similar in meaning\nare clustered together.\n\n- Another interesting observation was that reversing the source\nsequence gives a significant boost to translation of long sentences\nand results in performance gain, most likely due to the introduction of\nshort-term dependencies that are more easily captured by the gradients.\n\n## Weaknesses / Notes\n\n- The reversing source input idea needs better justification,\notherwise comes across as an 'ugly hack'.\n\n- To re-score the n-best list of predictions of the baseline,\nthey average confidences of LSTM and baseline model. They should\nhave reported re-ranking accuracies by using just the LSTM-model\nconfidences.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks"
    },
    "757": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=simonyan2014convolutional",
        "transcript": "This paper proposes a modified convolutional network architecture\nby increasing the depth, using smaller filters, data augmentation\nand a bunch of engineering tricks, an ensemble of which\nachieves second place in the classification task and first place\nin the localization task at ILSVRC2014.\n\nMain contributions:\n\n- Experiments with architectures with different depths from 11 to\n19 weight layers.\n- Changes in architecture\n    - Smaller convolution filters\n    - 1x1 convolutions: linear transformation of input channels\n    followed by a non-linearity, increases discriminative capability\n    of decision function.\n- Varying image scales\n    - During training, the image is rescaled to set the length of the shortest side\n    to S and then 224x224 crops are taken.\n    - Fixed S; S=256 and S=384\n    - Multi-scale; Randomly sampled S from [256,512]\n    - This can be interpreted as a kind of data augmentation by scale jittering,\n    where a single model is trained to recognize objects over a wide range of scales.\n    - Single scale evaluation: At test time, Q=S for fixed S and Q=0.5(S_min + S_max)\n    for jittered S.\n    - Multi-scale evaluation: At test time, Q={S-32,S,S+32} for fixed S and Q={S_min,\n    0.5(S_min + S_max), S_max} for jittered S. Resulting class posteriors are averaged.\n    This performs the best.\n- Dense v/s multi-crop evaluation\n    - In dense evaluation, the fully connected layers are converted to convolutional\n    layers at test time, and the uncropped image is passed through the fully convolutional net\n    to get dense class scores. Scores are averaged for the uncropped image and its\n    flip to obtain the final fixed-width class posteriors.\n    - This is compared against taking multiple crops of the test image and averaging scores\n    obtained by passing each of these through the CNN.\n    - Multi-crop evaluation works slightly better than dense evaluation, but the methods\n    are somewhat complementary as averaging scores from both did better than each of them\n    individually. The authors hypothesize that this is probably because of the different\n    boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured.\n\n## Strengths\n\n- Thoughtful design of network architectures and experiments to study the effect of\ndepth, LRN, 1x1 convolutions, pre-initialization of weights, image scales,\nand dense v/s multi-crop evaluations.\n\n\n## Weaknesses / Notes\n\n- No analysis of how much time these networks take to train.\n- It is interesting how the authors trained a deeper model (D,E)\nby initializing initial and final layer parameters with those from\na shallower model (A).\n- It would be interesting to visualize and see the representations\nlearnt by three stacked 3x3 conv layers and one 7x7 conv layer, and\nmaybe compare their receptive fields.\n- They mention that performance saturates with depth while going\nfrom D to E, but there should have been a more formal characterization\nof why that happens (deeper is usually better, yes? no?).\n- The ensemble consists of just 2 nets, yet performs really well.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1409.1556"
    },
    "758": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZeilerF13",
        "transcript": "This paper introduces a novel visualization technique to understand\nrepresentations learnt by intermediate layers of a deep convolutional\nneural network - DeconvNet. Using DeconvNet visualizations as a\ndiagnostic tool in different settings, the authors propose changes to the\nmodel proposed by Alex Krizhevsky, which performs slightly better and\ngeneralizes well to other datasets. Key contributions:\n\n- Deconvolutional network\n    - Feature activations are mapped back to input pixel space by setting\n    other activations in the layer to zero and successively unpooling,\n    rectifying and filtering (using the same parameters).\n    - Unpooling is approximated by using switch variables to remember\n    the location of highest input activation (and hence these visualizations\n    are image-specific).\n    - Rectification involves passing the signal through a ReLU\n    non-linearity.\n    - Filtering involves convolving the reconstructed signal with\n    the transpose of the convolutional layer filters.\n- Well-designed experiments to provide insights\n\n## Strengths\n\n- Observation of evolution of features\n    - Visualizations clearly demonstrate that lower layers\n    converge within a few epochs and upper layers\n    develop after a considerable number of epochs (40-50).\n- Feature invariance\n    - Visualizations show that small transformations have a\n    dramatic effect on lower layers and lesser impact on higher\n    layers. The model is fairly stable to translation and scaling,\n    not so much to rotation.\n- Occlusion sensitivity analysis\n    - Parts of the image are occluded, and posterior and activities\n    are visualized. Clearly show that activities drop when the object\n    is occluded.\n- Correspondence analysis\n    - The intuition is that CNNs implicitly learn the correspondence between different parts.\n    - To verify this, dog images with frontal pose are taken and the same part of the face\n    is occluded in each of them. Then the difference in feature maps for each of those and the\n    original image is calculated, and the consistency of this difference across all image pairs\n    is verified by Hamming distance. Lower scores as compared to random occlusions does show\n    that the model learns correspondences.\n- Proposed model performs better than Alex Krizhevsky's model, and generalizes\nwell to other datasets.\n\n## Weaknesses / Notes\n\n- The justification / intuition for choice of smaller filters wasn't convincing\nenough.\n- Why does removing layer 7 give better top-1 error rate on train and\nval?\n- Rotation invariance might be something worth looking into.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1311.2901"
    },
    "759": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=krizhevsky2012imagenet",
        "transcript": "This paper introduces a deep convolutional neural network (CNN) architecture\nthat achieved record-breaking performance in the 2012 ImageNet LSVRC. Notably,\nit brings together a bunch of neat ideas in an end-to-end, trainable model.\nMain contributions:\n\n- Achieves state-of-the-art performance in ILSVRC-2012.\n- Makes available an efficient, parallelized GPU implementation of their model.\n- Describes in detail the features of their model that help in improving performance\nand reducing training time, along with extensive ablative studies.\n- Uses data augmentation and dropout to prevent overfitting.\n\n## Strengths\n\n- Uses (and popularizes) ReLUs instead of tanh as the non-linear activation unit, which makes training six times faster.\n- Uses local response normalization and overlapped pooling.\n- Data augmentation\n    - Extracts random crops and performs image translations, horizontal reflections maintaining the label distribution.\n    - Alters RGB pixel values by performing PCA on training set, and adding multiples of eigenvalues times a random variable drawn from a Gaussian to image. Provides invariance to changes in intensity and color of illumination.\n- Dropout prevents overfitting. Randomly drops half of the neurons in the fully connected layers, and can be interpreted as averaging over exponentially-many dropout networks.\n\n## Weaknesses / Notes\n\n- Lacks theoretical insight. Design decisions are motivated solely by results.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "760": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/aim/Grosz12",
        "transcript": "- Turing, in his MIND paper in 1950, proposed an operational, behavioral alternative to the philosophical question \"Can machines think?\u201d by suggesting a simple \"Turing test\" where machines play the \"imitation game\u201d and humans are tasked with discerning machine from human given responses. He believed even partial success towards this goal given only 5 minutes of interaction would be hard and far-off.\n- The Turing test hasn\u2019t yet been met (except in restricted settings like Siri, Watson), but his prediction \"one will be able to speak of machines thinking without expecting to be contradicted\u201d has proved true \u2014 \"smart\u201d computers have become commonplace.\n- One of the reasons Turing test hasn\u2019t been met yet is because of the failures today\u2019s intelligent systems make. Their capabilities are limited as type of questions they can handle, domains and their ability to handle unexpected input. Failure cases when \"it doesn\u2019t know that it doesn\u2019t know\u201d making humans exclaim how stupid it is.\n- There is a realization that computers and humans have separate strengths, weaknesses and roles. Also, language is inherently social and connected to communicative purpose and human cooperation. It is intentional behavior, and not just stimulus-response. Language also assumes that participants have models of each other, models that influence what they say and how they say it. Retrospectively speaking, Turing\u2019s imitation game misses these aspects. \"Jeopardy\u201d was clever in avoiding dialogue context and modeling other people\u2019s behavior.\n- Another big change: instead of input-output interactions with computers by humans, today humans + computers exist in \"mixed\u201d networks.\n- Desirable properties in today's Turing test: interactive nature + use of language in real use (than success in game) + human-machine collaboration\n- Proposed Turing test: \"Is it imaginable that a computer (agent) team member could behave, over the long term and in uncertain, dynamic environments, in such a way that people on the team will not notice it is not human.\u201d\n- This doesn\u2019t ask machine to appear like human, act or be mistaken for one, but non-humanness shouldn\u2019t hit people in the face. Behavior shouldn\u2019t baffle teammates, leaving them wondering not about what it is thinking but whether it is. Such a system will also need a model of teammates\u2019 knowledge, abilities, preferences, etc.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aaai.org/ojs/index.php/aimagazine/article/view/2441"
    },
    "761": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/WangFG15",
        "transcript": "Originally posted [here](https://github.com/abhshkdz/papers/blob/master/reviews/actions-~-transformations.md).\n\nThis paper introduces a novel representation for actions in videos as transformations that change the state of the environment from what it was before the action (precondition) to what it will be after it (effect).\n\n- Model\n    - The model utilizes a Siamese architecture with each head having convolutional and fully-connected layers (similar to VGG16). Each head extracts features for a subset of video frames (precondition or effect) that are aggregated by average pooling and followed by a fully-connected layer.\n    - The precondition frames are indexed from 1 to z\\_p and the effect frames from z\\_e to t. Both z\\_p and z\\_e are latent variables, constrained to be from [1/3t, 1/2t] and [1/2t, 2/3t] respectively and estimated via brute force search during training.\n    - The action is represented as a linear transformation between the final fully-connected layers of the two heads. For n action categories, the transformation layer has n transformation matrices.\n    - The model is trained with a contrastive loss function to 1) maximize cosine similarity between the effect embedding and the transformed precondition embedding, and 2) maximize distance for incorrect transformations if greater than a chosen margin.\n- ACT Dataset\n    - 50 keywords, 43 classes, ~500 YouTube videos per keyword.\n    - The authors collect the ACT dataset primarily for the task of cross-category generalization (as it doesn't allow models to overfit to contextual information). For example, how would a model learned on \"opening a window\" generalize to recognize \"opening the trunk of the car\"? How about generalizing from a model trained on \"climbing a cliff\" to recognize \"climbing a tree\"?\n    - The ACT dataset has class and super-class annotations from human workers. Each super-class has different sub-categories which are the same action under different subjects, objects and scenes.\n- Experiments\n    - Action recognition on UCF101, HMDB51, ACT.\n    - Cross-category generalization on ACT.\n- Visualizations\n    - Nearest neighbor: modeling the actions as transformations gives semantically meaningful retrievals that don't just depend on motion and color.\n    - Gradient visualizations (Simonyan et al. 2014): model focuses on changes in scene (human + object) than context.\n    - Embedding retrievals based on transformed precondition embeddings.\n\n** Thoughts **\n\n- Modeling action as a transformation from precondition to effect is a very neat idea.\n- The exact formulation and supporting experiments and ablation studies are thorough.\n- During inference, the model first extracts features for all frames and then does a brute force search over (y,z\\_p,z\\_e) to estimate the action category and segmentation into precondition and effect. For longer sequences, this seems expensive. Although hard decisions aren't differentiable, a soft attention mechanism on z might be feasible and reduce computation to a single forward pass.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.00795"
    },
    "762": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1510.02777",
        "transcript": "# Very Short\n\nThe authors define a neural network as a nonlinear dynamical system whose fixed points correspond to the minima of some **energy function**.  They then show that if one were to start at a fixed-point and *perturb* the output units in the direction that minimizes a loss, the initial perturbation that would flow back through the network would be proportional to the gradient of the neural activations with respect to this loss.  Thus, the initial propagation of those propagations (i.e. **early inference**) **approximates** the **backpropagated** gradients of the loss.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1510.02777"
    },
    "763": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LotterKC16",
        "transcript": "# Very Short\n\nThe authors propose a deep, recurrent, convolutional architecture called PredNet, inspired by the idea of predictive coding from neuroscience.  In PredNet, first layer attempts to predict the input frame, based on past frames and input from higher layers.  The next layer then attempts to predict the *prediction error* of the first layer, and so on.  The authors show that such an architecture can predict future frames of video, and predict the parameters synthetically-generated video, better than a conventional recurrent autoencoder.\n\n# Short\n\n## The Model\nPredNet has the following architecture:\n\nhttps://i.imgur.com/7vOcGwI.png\n\nWhere the R blocks are Recurrent Neural Networks, and the A blocks are Convolutional Layers.   $E_l$ indictes the prediction error at layer $l$.  The network is trained from snippets of video, and the loss is given as:\n\n$L_{train} = \\sum_{t=1}^T \\sum_l \\frac{\\lambda_l}{n_l} \\sum_i^{2n_l} [E_l^t]_i$\n\nWhere $t$ indexes the time step, $l$ indexes the layer, $n_l$ is the number of units in the layer, $E_l^t = [ReLU(A_l^t-\\hat A_l^t) ; ReLU(\\hat A_l^t - A_l^t) ]$ is the concatenation of the negative and positive components of the error, $\\lambda_l$ is a hyperparameter determining the effect that layer $l$ error should have on the loss.  \n\nIn the experiments, they use two settings for the $\\lambda_l$ hyperparameters.  In the \"$L_0$\" setting, they set $\\lambda_0=1, \\lambda_{>0}=0$, which ends up being optimal when trying to optimize next-frame L1 error.  In the \"$L_{all}$\" setting, they use $\\lambda_0=1, \\lambda_{>0}=0.1$, which, in the synthetic-images experiment, seems to be better at predicting the parameters of the synthetic-image generator. \n\n## Results\n\nThey apply the model on two tasks: \n\n1) Predicting the future frames of a synthetic video generated by a graphics engine.  \n\nHere they predict both the next frame (in which their $L_0$ model does best), and the parameters (face characteristics, rotation, angle) of the program that generates the synthetic faces, (on which their $L_{all}$ model does best).  They predict face generating parameters by first training the model, and then freezing weights and regressing from the learned representations at a given layer to the parameters.  They show that both the $L_0$ and $L_{all}$ models outperform a more conventional recurrent autoencoder.\n\nhttps://i.imgur.com/S8PpJnf.png\n**Next-frame predictions on a sequence of faces (note: here, predictions are *not* fed back into the model to generate the next frame)**\n\n2) Predicting future frames of video from dashboard cameras.  \n\nhttps://i.imgur.com/Zus34Vm.png\n**Next-frame predictions of dashboard-camera images**\n\nThe authors conclude that allowing higher layers to model *prediction errors*, instead of *abstract representations* can lead to better modeling of video.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1605.08104"
    },
    "764": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HintonB99",
        "transcript": "# Very Short\n\nThe authors first describe how spiking neurons could be used to represent a multimodal probability distribution that evolves through time, and then use this idea to design a variant on a Restricted Boltzmann machine which can learn a temporal sequence.  \n\n# Short\n\n## 1. Population codes and energy landscapes\n\nImagine that a neural network perceives an object, which has some *instantiation parameters* (e.g. position, velocity, size, orientation), and we want to infer the values of these *instantiation parameters* through some kind of noisy observation process (for instance, the object being represented as an image).  Due to the noisiness of the observation process, we will never be able to exactly infer the parameters, but need to infer distributions over these parameters.\n\nOne way to interpret the state of a neural network as a probability distribution is to imagine that each neuron corresponds to a certain probability distribution in the space of instantiation parameters, and the activations of the neurons represent *unnormalized mixing proportions* i.e. the extent to which each neuron is right about the instantiation parameters.  The distribution represented by the network is then a weighted sum of the distributions represented by each neuron.   This is called a *disjunctive* representation.  The distribution of the network can never be as sharp as the distribution of each neuron.  \n\nAnother option is to instead to do a weighted addition of neurons' *Energy landscapes* (i.e. negative log probability distributions), where the neuron's activation represents this weight.   This is called a *conjunctive* representation.  When neuron distributions are combined like this, the distribution of the network can be *sharper* than the distributions for each neuron. \n\nhttps://i.imgur.com/u6W9kBU.png\n\n## 2 Representing the coe\u000efficients on the basis functions\n\nA biological spiking neuron outputs sparse binary signals.  However, these spikes are convolved by a temporal kernel when they cross synapses so that other neurons see them as smoothly-varying *postsynaptic potentials*.  If we consider these postsynaptic potentials to be the weights on the neuron's contribution the the energy landscape, we can see how a spiking neural network could define a probability distribution that varies smoothly in time.\n\nhttps://i.imgur.com/pTWOCPx.png\n**Left: Blue vertical lines are spikes.  Red lines are postsynaptic potentials.  Blue horizontal lines are contour lines of the distribution defined by adding both neurons contributions to the energy landscape.  Right: The effects of spikes on two neurons on the probability distribution.  The effects of a spike can be seen as an hourglass, with a \"neck\" at the point of maximum activation of the postsynaptic potential.**\n\n## 3. A learning algorithm for restricted Boltzmann machines\n\nRestricted Boltzmann Machines are an example of neural networks that use a *conjunctive* representation.  They consist of a layer of visible units connected to a layer of hidden units through symmetric connections.  They learn by *contrastive divergence*, which involves taking an input (a vector of visible activations $s_i^0$), projecting it to the hidden layer to get hidden layer activations $s_j^0$, where $\\Pr(s_j=1) = \\sigma\\left(\\sum_i s_i w_{ij} \\right)$, where $\\sigma = (1+e^{-x})^{-1}$ is the logistic sigmoid.  Hidden activations then similarily reconstruct visible activations $s_i^1$, and those are used to create new hidden activations $s_j^1$.   Their learning rule is:\n\n$\\Delta w_{ij} = \\epsilon ( < s_i s_j >^0 - < s_i s_j >^1)$  \nWhere :  \n$\\Delta w_{ij}$ is the change in the symmetric weight connecting visible unit $i$ to hidden unit $j$  \n$\\epsilon$ is a learning rate.  \n$< s_i s_j >^0$ and $< s_i s_j >^1$ are the averages (over samples) of the product of the visible activation $s_i$ and hidden activation $s_j$ on the first and second pass, respectively.\n\n## 4. Restricted Boltzmann Machines through time\n\nFinally, the authors propose a restricted Boltzmann machine through time, where an RBM is replicated across time.  The new weight matrix is defined as $w_{ij\\tau} = w_{ij} r(\\tau)$.\n\nWhere $r(\\tau)$ is a fixed, causal, temporal kernel: \n\nhttps://i.imgur.com/mLmfAhp.png\n\nThe authors also allow hidden-to-hidden and visible-to-visible connections.  These act as a predictive prior over activations.  To overcome the problem that introducing hidden-to-hidden connections makes sampling hidden states intractable, they use an approximation wherein the sampled values of past hidden activations are treated like data, and inference is not done over them.\n\nThe forward pass thus becomes:\n\n$\\Pr(s_j=1) = \\sigma \\left( \\sum_i w_{ij} \\sum_{\\tau=0}^\\infty s_i(t-\\tau) r(\\tau) +  \\sum_i w_{kj} \\sum_{\\tau=0}^\\infty s_k(t-\\tau) r(\\tau) \\right)$\n\nWhere $w_{ij}$'s index vidible to hidden connections and $w_{kj}$'s index hidden-to-hidden connections.  The hidden-to-visible pass is computed similarly. \n\nThe constrastive-divergence update rule then becomes:\n\n$\\Delta w_{ij} = \\epsilon \\sum_{t=1}^{\\infty} \\sum_{\\tau=0}^\\infty r(\\tau) \\left( < s_j(t) s_i(t-\\tau)>^0 - < s_j(t) s_i(t-\\tau)>^1  \\right)$\n\n## 5. Results:\n\nThe authors demonstrate that they can use this to train a network to learn a temporal model on a toy dataset consisting of an image of a ball that travels repeatedly along a circular path.  \n\n---\n\n## Notes:\n\nA description of Spiking Restricted Boltzmann Machines in the context of other variants on Boltzmann Machines can be found in [Boltzmann Machines for Time Series](https://arxiv.org/pdf/1708.06004.pdf)  (Sept 2017) by Takayuki Osogami",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://nips.djvuzone.org/djvu/nips12/0122.djvu"
    },
    "765": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.04474",
        "transcript": "# Very Short\n\nThe authors propose **learning** an optimizer **to** optimally **learn** a function (the *optimizee*) which is being trained **by gradient descent**.  This optimizer, a recurrent neural network, is trained to make optimal parameter updates to the optimizee **by gradient descent**.\n\n# Short\n\nLet's suppose we have a stochastic function $f: \\mathbb R^{\\text{dim}(\\theta)} \\rightarrow \\mathbb R^+$, (the *optimizee*) which we wish to minimize with respect to $\\theta$.  Note that this is the typical situation we encounter when training a neural network with Stochastic Gradient Descent - where the stochasticity comes from sampling random minibatches of the data (the data is omitted as an argument here).  \n\nThe \"vanilla\" gradient descent update is: $\\theta_{t+1} = \\theta_t - \\alpha_t \\nabla_{\\theta_t} f(\\theta_t)$, where $\\alpha_t$ is some learning rate.  Other optimizers (Adam, RMSProp, etc) replace the multiplication of the gradient by $-\\alpha_t$ with some sort of weighted sum of the history of gradients.\n\nThis paper proposes to apply an optimization step $\\theta_{t+1} = \\theta_t + g_t$, where the update $g_t \\in \\mathbb R^{\\text{dim}(\\theta)}$ is defined by a recurrent network $m_\\phi$: \n\n$$(g_t, h_{t+1}) := m_\\phi (\\nabla_{\\theta_t} f(\\theta_t), h_t)$$\n\nWhere in their implementation, $h_t \\in \\mathbb R^{\\text{dim}(\\theta)}$ is the hidden state of the recurrent network.  To make the number of parameters in the optimizer manageable, they implement their recurrent network $m$ as a *coordinatewise* LSTM (i.e. A set of $\\text{dim}(\\theta)$ small LSTMs that share parameters $\\phi$).   They train the optimizer networks's parameters $\\phi$ by \"unrolling\" T subsequent steps of optimization, and minimizing:\n\n$$\\mathcal L(\\phi) := \\mathbb E_f[f(\\theta^*(f, \\phi))]  \\approx \\frac1T \\sum_{t=1}^T f(\\theta_t)$$\n\nWhere $\\theta^*(f, \\phi)$ are the final optimizee parameters.  In order to avoid computing second derivatives while calculating $\\frac{\\partial \\mathcal L(\\phi)}{\\partial \\phi}$, they make the approximation $\\frac{\\partial}{\\partial \\phi}  \\nabla_{\\theta_t}f(\\theta_t) \\approx 0$ (corresponding to the dotted lines in the figure, along which gradients are not backpropagated).  \n\nhttps://i.imgur.com/HMaCeip.png\n**The computational graph of the optimization of the optimizer, unrolled across 3 time-steps.  Note that $\\nabla_t := \\nabla_{\\theta_t}f(\\theta_t)$.  The dotted line indicates that we do not backpropagate across this path.**\n\nThe authors demonstrate that their method usually outperforms traditional optimizers (ADAM, RMSProp, SGD, NAG), on a synthetic dataset, MNIST, CIFAR-10, and Neural Style Transfer.  They argue that their algorithm constitutes a form of transfer learning, since a pre-trained optimizer can be applied to accelerate training of a newly initialized network.  \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.04474"
    },
    "766": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/TallecO17",
        "transcript": "# Really Short\n\nA method for training a recurrent network that avoids doing backpropagation through time by instead approximately forward-propagating the derivatives of the recurrent-state variables with respect to the parameters.\n\n# Short\n\nThis paper deals with learning **Online** setting, where we have an infinite sequence of points $<(x_t, y_t): t \\in \\mathbb N>$, where at each time $t$ we would like to predict $y_t$ given $x_1, ... x_t$.  We'd like do this by training a **Recurrent** model: $o_t , s_t := f_\\theta(x_t, s_{t-1})$ to **Optimize** parameters $\\theta$ such that we minimize the error of the next prediction: $\\mathcal L_t := \\ell(o_t, y_t)$\n\nThe standard way to do this is is Truncated Backpropagation through Time (TBPTT).  We \"unroll\" the network for T steps (the truncation window), every T steps, and update $\\theta$ to minimize $\\sum_{\\tau=t-T+1}^t \\mathcal L_\\tau$ given the last T data points: $< (x_{\\tau}, y_\\tau) : \\tau \\in [t-T+1 .. t]>$ and the previous recurrent state $s_{t-T}$.  This has the disadvantage of having to store T intermediate hidden states and do 2T sequential operations in the forward/backward pass.  Moreover it gives a biased gradient estimate because it ignores the effect of $\\theta$ on $s_{t-T}$.\n\nAnother option which usually is even more expensive is Real-Time Recurrent Learning (RTRL).  RTRL is the application of [forward mode automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation#The_chain_rule,_forward_and_reverse_accumulation) to recurrent networks (Backpropagation Through Time is reverse-mode automatic differentiation).  Instead of first computing the loss and then backpropagating the gradient, we forward-propagate the jacobian defining the derivitive of the state with respect to the parameters: $\\frac{\\partial s_t}{\\partial \\theta} = \\frac{\\partial s_t}{\\partial s_{t-1}}  \\frac{\\partial s_{t-1}}{\\partial \\theta} +  \\frac{\\partial s_t}{\\partial \\theta}|_{s_{t-1}} \\in \\mathbb R^{|S| \\times |\\Theta|}$ (where the second term is \"the derivative with $s_{t-1}$ held constant\"), and updating $\\theta$ using the gradient  $\\frac{\\partial \\mathcal L_t}{\\partial \\theta} = \\frac{\\partial \\mathcal L_t}{\\partial s_{t-1}}  \\frac{\\partial s_{t-1}}{\\partial \\theta} +  \\frac{\\partial \\mathcal L_t}{\\partial \\theta}|_{s_{t-1}}$.  This is very expensive because it involves computing, storing and multiplying large Jacobian matrices.  \n\nThis paper proposes doing an approximate form of RTRL where the state-derivative is stochastically approximated as a Rank-1 matrix: $\\frac{\\partial s_t}{\\partial \\theta} \\approx \\tilde s_t \\otimes \\tilde \\theta_t: \\tilde s_t \\in \\mathbb R^{|S|}, \\tilde \\theta_t \\in \\mathbb R^{|\\Theta|}$ where $\\otimes$ denotes the outer-product.  The approximation uses the \"Rank-1 Trick\"*****.\n\nThey show that this approximation is **Unbiased**** (i.e. $\\mathbb E[\\tilde s_t \\otimes \\tilde \\theta_t] = \\frac{\\partial s_t}{\\partial \\theta}$), and that using this approximation we can do much more computationally efficient updates than RTRL, and without the biasedness and backtracking required in TBPTT.\n\nThey demonstrate this result on some toy datasets.  They demonstrate that it's possible to construct a situation where TBPTT fails (due to biasedness of the gradient) and UORO converges, and that for other tasks they achieve comparable performance to TBPTT.\n\n----\n\n\\* The \"Rank-1 Trick\", is that if matrix $A \\in \\mathbb R ^{M\\times N}$ can be decomposed as $A = \\sum_k^K v_k \\otimes w_k: v_k \\in \\mathbb R^M, w_k \\in \\mathbb R^N$, then we can define $\\tilde A :=\\left( \\sum_k^K \\nu_k v_k \\right) \\otimes \\left( \\sum_k^K \\nu_k w_k \\right) \\approx A$, with $\\nu_k = \\{1 \\text{ with } p=\\frac12 \\text{ otherwise } -1\\}$, and show that $\\mathbb E[\\tilde A] = A$.  This trick is applied twice to approximate the RTRL updates: First to approximate $s'\\otimes \\theta' :\\approx \\frac{\\partial s_t}{\\partial \\theta}|_{s_{t-1}}$, then $\\tilde s_t \\otimes \\tilde \\theta_t :\\approx \\frac{\\partial s_t}{\\partial s_{t-1}}  (\\tilde s_{t-1} \\otimes \\tilde \\theta_{t-1}) + s' \\otimes \\theta'$.  (Note that in the paper they add the additional terms $\\rho_k$ to this equation to reduce the variance of this estimator).\n\n\n** The \"unbiasedness\" is a bit of a lie, because it is only true if $\\theta$ is not changing over time, which it is during training.  This is the case for RTRL in general, and not just this work.  ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1702.05043"
    },
    "767": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/miccai/MoeskopsVLEP17",
        "transcript": "Problem\n=========\nBrain MRI segmentation using adversarial training approach\n\nDataset\n======\n55 T1 weighted brain MR images (35 adults and 20 elders) with respective label maps.\n\n\nContributions\n==========\n1. The authors suggest an adversarial loss in addition to the traditional loss.\n2. The authors compare 2 Generator (Segmentor) models - Fully convolutional and dilated networks.\n\nhttps://i.imgur.com/orhWhoM.png\n\nDilated network\n------------------\nUsing conv layers, allows for larger receptive field with fewer trainable weights (compared to the FCN option).\n\n\nHowever, the authors claim the adversarial loss contributes more when applying the FCN model",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-319-67558-9_7"
    },
    "768": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1711-11585",
        "transcript": "https://i.imgur.com/lM3EjK9.png\nProblem\n=============\nLabel map (semantic segmentation) to realistic image using GANs.\n\nContributions\n=========\n1. Coarse-to-fine generator\n2. Multi-scale discriminator\n3. Robust adversarial learning objective function\n\n\nCoarse-to-fine Generator\n=================\nhttps://i.imgur.com/osEyGOj.png\n\nG1 - Global generator\n\nG2 - Local enhancer\n\nGlobal Generator:\n1. convolutional front-end\n2. set of residual blocks\n3. transposed convolutional back-end\n\nA semantic label map is passed through the 3 components sequentially \n\n\nLocal Enhancer:\n1. convolutional front-end\n2. set of residual blocks\n3. transposed convolutional back-end\n\n\nTraining scheme:\n1. Train standalone global generator \n2. Freeze global generator weights, train local enhancer\n3. Fine-tune all weights together\n\nMulti scale Discriminator\n===================\nhttps://i.imgur.com/hNP1cni.png\n\nTo allow for global context but work at higher resolution as well, several discriminators are applied at different image scales.\n\n\nRobust adversarial learning objective function\n===============================\nhttps://i.imgur.com/j7CIbV3.png\n* Compare original and generated images in feature space at different scales. \n* This is done to ensure more abstract resemblance, not just pixel-space resemblance.\n* For feature extraction the discriminator is used.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.11585"
    },
    "769": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1710-10196",
        "transcript": "Contribution\n------------\n1. New GAN training methodology - progressively going from low-res to hi-res, adding additional layers to the model.\nhttps://i.imgur.com/2rQcnH1.png\n\n2. When introducing new layers during training, it is gradually faded-in using a coefficient.\nhttps://i.imgur.com/iuVaN1H.png\n\n3. increasing variation of generated images by counting the standard deviation in the discriminator.\n\n\n\n\nDatasets\n---------------------\n* CELEBA\n* LSUN\n* CIFAR10\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1710.10196"
    },
    "770": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ShrivastavaPTSW16",
        "transcript": "Problem\n--------------\nRefine synthetically simulated images to look real\n\nhttps://machinelearning.apple.com/images/journals/gan/real_synt_refined_gaze.png\n\nApproach\n--------------\n* Generative adversarial networks\n\nContributions\n----------\n1. **Refiner** FCN that improves simulated image to realistically looking image\n2. **Adversarial + Self regularization loss**\n* **Adversarial loss** term = CNN that Classifies whether the image is refined or real\n* **Self regularization** term = L1 distance of refiner produced image from simulated image. The distance can be either in pixel space or in feature space (to preserve gaze direction for example).\n\n\nhttps://i.imgur.com/I4KxCzT.png\n\nDatasets\n------------\n* grayscale eye images\n* depth sensor hand images \n\n\nTechnical Contributions\n-------------------------------\n1. **Local adversarial loss** - The discriminator is applied on image patches thus creating multiple \"realness\" metrices\nhttps://machinelearning.apple.com/images/journals/gan/local-d.png\n\n2. **Discriminator with history** - to avoid the refiner from going back to previously used refined images.\n\nhttps://machinelearning.apple.com/images/journals/gan/history.gif\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.07828"
    },
    "771": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1708.01155",
        "transcript": "https://i.imgur.com/vxBhb7B.png\n\nProblem\n------------\nConvert MR scans to CT scans.\n\n\nGeneral Approach\n----------\nCycleGAN\n\n\nDataseet\n-----------\nUnpaired brain CT:MR images.\nThe dataset contains both CT and MR scans of same patient taken on the same day. \nThe volumes are aligned using mutual information and contain some local minor misalignments.\n\nMethod\n--------\nTrain the following models:\n1. Syn_ct: CNN: MR -> CT\n2. Syn_mr: CNN: CT -> MR\n3. Dis_ct: classify real and synthetic CT images (result of Syn_ct)\n4. Dis_mr: classify real and synthetic MR images. Syn_mr(Syn_ct(MR Image))) or Syn_mr(CT image)\n\n\nhttps://i.imgur.com/GqVaskb.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1708.01155"
    },
    "772": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1707.05363",
        "transcript": "Problem\n----------\nMotion prediction\n\nDataset\n----------\nCMU\n\n\nApproach\n--------------\nauto-conditioned LSTM - an LSTM network that uses only fraction of the input timestamps, but all of the outputs (a little bit similar to keyframes).\n\nhttps://image.ibb.co/nimSs5/acLSTM.png\n\n\nVideo\n--------\nhttps://www.youtube.com/watch?v=AWlpNeOzMig",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1707.05363"
    },
    "773": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tog/Zitnick13",
        "transcript": "Problem\n----------\nMake stylus input prettier by making it closer to mean shape of input.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2461912.2461985"
    },
    "774": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/WalkerMGH17",
        "transcript": "Problem\n---------\nVideo prediction with human objects\n\n\nContribution\n--------------\nInstead of the common approach of predicting directly in pixel-space, use explicit knowledge of human motion space to predict the future of the video.\n\nApproach\n--------------\n1. VAE to model the possible future movements of humans in the pose space\n2. Conditional GAN - use pose information for to predict video in pixel space.\n\n\n\nhttps://image.ibb.co/b1omVF/The_pose_knows.png",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.00053"
    },
    "775": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ChaoYPCD17",
        "transcript": "Problem\n-------------\nPredict human motion from static image\n\nhttp://www-personal.umich.edu/~ywchao/pictures/cvpr2017.png\n\nApproach\n----------\n1. 2d pose sequence generator\n2. convert 2d pose to 3d skeleton\n\nhttps://image.ibb.co/eeBRxv/3D_PFNet.png\n\nhttps://image.ibb.co/kERaVQ/Forecasting_Human_Dynamics_from_Static_Images_architecture.png\n\n\n3 Step training strategy\n-------------------------\n1. Train human 2d pose extractor using annotated video with 2d joint positions\n2. 3d skeleton extractor: project mocap data to 2d and use as ground truth for training the 2d->3d skeleton converter\n3. Full network training\n\n\nDatasets\n-----------\n1.  Penn Action - Annotated human pose in sports image sequences: bench_press, jumping_jacks, pull_ups...\n2. MPII - human action videos with annotated single frame\n3. Human3.6M - video, depth and mocap. action include: sitting, purchasing, waiting\n\n\n\n\nEvaluation\n-------------\nOn the following tasks:\n1. 2D pose forecasting \n2. 3D pose recovery\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.03432"
    },
    "776": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1707.01058",
        "transcript": "Problem\n---------------\nVideo generation of human motion given:\n1. Single appearance reference image\n2. Skeleton motion sequence\n\n\nDatasets\n-----------\n* KTH - grayscale human actions\n* Human3.6M - color multiview human actions\n\n\nApproach\n---------------\nConditional GANs.\nThe authors try both Stack GAN and Siamese GAN.\nThe later provides better result.\n\nhttps://preview.ibb.co/ighxQQ/Skeleton_aided_Articulated_Motion_Generation.png\n\nQuestions\n----------------\nIsn't using a full sequence of human skeleton motion considered more then a \"hint\"?",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1707.01058"
    },
    "777": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eccv/BogoKLG0B16",
        "transcript": "Problem\n----------\nGiven an unconstrained image, estimate:\n1. 3d pose of human skeleton \n2. 3d body mesh\n\n\nContributions\n-----------\n1. full body mesh extraction from image\n2. improvement of state of the art\n\n\nDatasets\n-------------\n1. Leeds Sports\n2. HumanEva\n3. Human3.6M\n\n\nApproach\n----------------\nConsider the problem both bottom-up and top-down.\n1. Bottom-up: DeepCut cnn model to fit joints 2d positions onto the image.\n2. top-down: A skinned multi-person linear model (SMPL) is fitted and projected onto 2d joint positions and image.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-46454-1_34"
    },
    "778": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZhangXLZHWM16",
        "transcript": "Problem\n------------\nText to image\n\n\nContributions\n-----------------\n* Images are more photo realistic and higher resolution then previous methods\n* Stacked generative model\n\n\nApproach\n-------------\n2 stage process:\n1. Text-to-image: generates low resolution image with primitive shape and color.\n2. low-to-hi-res: using low res image and text, generates hi res image. adding details and sharpening the edges.\n\nhttps://pbs.twimg.com/media/Cziw6bfWgAAh3Yg.jpg\n\n\nDatasets\n--------------\n* CUB - Birds\n* Oxford-102 - Flowers\n\n\nResults\n--------\nhttps://cdn-images-1.medium.com/max/1012/1*sIphVx4tqaXJxtnZNt3JWA.png\n\n\nCriticism/ Questions\n-------------------\n* Is it possible the resulting images are replicas of images in the original dataset? To what extent does the model \"hallucinate\" new images?",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.03242"
    },
    "779": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/FinnGL16",
        "transcript": "Problem\n----------\nGiven a video of robot motion, predict future frames of the motion.\n\nDataset\n-----------\n1. The authors assembled a new dataset of 59,000 robot interactions involving pushing motions.\n2. Human3.6m - video, depth and mocap. action include: sitting, purchasing, waiting...\n\nApproach\n------------\n* Use LSTMs to \"remember\" previous frames.\n* Predict 10 transformations from previous frame (each approach represents the transformation differently).\n* Predict a mask to determine which transformation is applied to which pixel.\n\nThe authors suggest 3 models based on this approach:\n1. Dynamic Neural Advection\n2. Convolutional Dynamic Neural Advection\n3. Spatial Transformer Predictors",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1605.07157"
    },
    "780": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MathieuCL15",
        "transcript": "Predict frames of a video using 3 newly proposed and complementary methods:\n1. Multi scale cnn\n2. GAN\n3. Image gradient difference loss\n\n\nDatasets:\n-----------\n* UCF101\n* Sports1M\n\n\nGAN\n------\nGenerator:\n   * Input: several frames of video from dataset\n   * output: next frame of video\n\nDiscriminator:\n   * input: original and last frame\n   * output: is the last frame from dataset or generated\n\nProblem: Still blurry on edges on moving object.\nSolution: Image gradient difference loss",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05440"
    },
    "781": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1806.06621",
        "transcript": "The paper extends the [WGAN](http://www.shortscience.org/paper?bibtexKey=journals/corr/1701.07875) paper by replacing the L2 norm in the transportation cost by some other metric $d(x, y)$. By following the same reasoning as in the WGAN paper one arrives at a dual optimization problem similar to the WGAN's one except that the critic $f$ has to be 1-Lipschitz w.r.t. a given norm (rather than L2). This, in turn, means that critic's gradient (w.r.t. input $x$) has to be bounded in the dual norm (only in Banach spaces, hence the name). Authors build upon the [WGAN-GP](http://www.shortscience.org/paper?bibtexKey=journals/corr/1704.00028) to incorporate similar gradient penalty term to force critic's constraint.\n\nIn particular authors choose [Sobolev norm](https://en.wikipedia.org/wiki/Sobolev_space#Multidimensional_case):\n$$\n||f||_{W^{s,p}} = \\left( \\int \\sum_{k=0}^s ||\\nabla^k f(x)||_{L_p}^p dx \\right)^{1 / p}\n$$\n\nThis norm is chosen because it not only forces pixel values to be close, but also the gradients to be close as well. The gradients are small when you have smooth texture, and big on the edges -- so this loss can regulate how much you care about the edges. Alternatively, you could express the same norm by first transforming the $f$ using the Fourier Transform, then multiplying the result by $1 + ||x||_{L_2}^2$ pointwise, and then transforming it back and integrating over the whole space:\n$$\n||f||_{W^{s,p}} = \\left( \\int \\left( \\mathcal{F}^{-1} \\left[ (1 + ||x||_{L_2}^2)^{s/2} \\mathcal{F}[f] (x) \\right] (x) \\right)^p dx \\right)^{1 / p}\n$$\n\nHere $f(x)$ would be image pixels intensities, and $x$ would be image coordinates, so $\\nabla^k f(x)$ would be spatial gradient -- the one you don't have access to, and it's a bit hard to estimate one with finite differences, so the authors go for the second -- fourier -- option. Luckily, a DFT transform is just a linear operator, and fast implementations exists, so you can backpropagate through it (TensorFlow already includes tf.spectal)\n\nAuthors perform experiments on CIFAR and report state-of-the-art non-progressive results in terms of Inception Score (though not beating SNGANs by a statistically significant margin). The samples they present, however, are too small to tell if the network really cared about the edges.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1806.06621"
    },
    "782": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1807.01604",
        "transcript": "Variational Inference builds around the ELBO (Evidence Lower BOund) -- a lower bound on a marginal log-likelihood of the observed data $\\log p(x) = \\log \\int p(x, z) dz$ (which is typically intractable). The ELBO makes use of an approximate posterior to form a lower bound:\n\n$$\n\\log p(x) \\ge \\mathbb{E}_{q(z|x)} \\log \\frac{p(x, z)}{q(z|x)}\n$$\n\n# Introduction to Quasi Monte Carlo\n\nIt's assumed that both the join $p(x, z)$ (or, equivalently the likelihood $p(x|z)$ and the prior $p(z)$) and the approximate posterior $q(z|x)$ are tractable (have closed-form density and are easy to sample from). Then one can estimate the ELBO via Monte Carlo as\n\n$$\n\\text{ELBO} \\approx \\frac{1}{N} \\sum_{n=1}^N \\log \\frac{p(x, z_n)}{q(z_n|x)}, \\quad\\quad z_n \\sim q(z|x)\n$$\n\nThis estimate can be used in stochastic optimization, essentially stochastically maximizing the ELBO, which leads to either increasing marginal log-likelihood or decreasing the gap between the true posterior distribution $p(z|x)$ and the approximate one $q(z|x)$.\n\nEfficiency of stochastic optimization depends on the amount of stochasticity. The bigger the variance is -- the harder it's to locate the optimum. It's well-known that in typical Monte Carlo variance scales as 1/N for a sample of size N, and hence typical error of such \"approximation\" has an order of $1/\\sqrt{N}$\n\nHowever, there are more efficient schemes to evaluate the integrals of the form of the expectation. To give you some intuition, consider\n\n$$ \\mathbb{E}_{q(z)} f(z) = \\int_\\mathcal{Z} f(z) q(z) dz = \\int_{[0, 1]^d} f(z(u)) du $$\n\nHere I used the fact that any random variance can be expressed as a deterministic transformation of a uniform r.v. (by application of the inverse CDF of the former r.v.), so estimating the expectation using MC essentially means sampling a bunch of uniform r.v. $u_1, \\dots, u_N$ and transforming them into the corresponding $z$s. However, uniformly distributed random variables sometimes clump together and leave some areas uncovered:\n\nhttps://i.imgur.com/fejsl2t.png\n\nLow Discrepancy sequences are designed to cover the unit cube more uniformly in a sense that points are unlikely to clump and should not leave \"holes\" in the landscape, effectively facilitating a better exploration. The Quasi Monte Carlo then employs these sequences to evaluate the integral at, giving (a deterministic!) approximation with an error of an order $\\tfrac{(\\log N)^d}{N}$. If you want some randomness, there are clever randomization techniques, that give you Randomized Quasi Monte Carlo with roughly the same guarantees.\n\n# RQMC applied to VI\n\nAuthors estimate the ELBO using samples obtained from the Randomized QMC (scrambled Sobol sequence, in particular), and show experimentally that this leads to lower gradient variance and faster convergence.\n\n# Theoretical Properties\n\nAuthors also analyse Stochastic Gradient Descent with RQMC and prove several convergence theorems. To the best of my knowledge, this is the first work considering stochastic optimization using QMC (which is understandable given that one needs to be able to control the gradients to do so)\n\n# Critique\n\nThe paper was a great read, and spurred a great interest in me. I find the idea of using QMC very intriguing, however in my opinion there are several problems on the road to mass-adoption\n\n1. Authors use RQMC to get the stochastic nature of $z_n$, however that essentially changes the effective distribution of generated $z$, which should be accounted for in the ELBO, otherwise the objective they're maximizing is not an ELBO (if only asymptotically) and hence not necessary a lower bound on the marginal log-likelihood. However, finding the correct proposal density $q(z|x)$ (and successfully using it) does not seem easy as most randomization schemes give you degenerate support, and KL is not well-defined.\n2. Authors have an experiment on a Bayesian Neural Network, however a very small one, there are reasons to doubt their results will translate to real ones, as the positive effect of QMC vanishes as dimension grows (because it's harder for uniform samples to clump together)\n3. Standard control variates might no longer reduce the variance, further research is needed.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1807.01604"
    },
    "783": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.07535",
        "transcript": "If one is a Bayesian he or she best expresses beliefs about next observation $x_{n+1}$ after observing $x_1, \\dots, x_n$ using the **posterior predictive distribution**: $p(x_{n+1}\\vert x_1, \\dots, x_n)$. Typically one invokes the de Finetti theorem and assumes there exists an underlying model $p(x\\vert\\theta)$, hence $p(x_{n+1}\\vert x_1, \\dots, x_n) = \\int p(x_{n+1} \\vert \\theta) p(\\theta \\vert x_1, \\dots, x_n) d\\theta$, however this integral is far from tractable in most cases. Nevertheless, having tractable posterior predictive is useful in cases like few-shot generative learning where we only observe a few instances of a given class and are asked to produce more of it.\n\nIn this paper authors take a slightly different approach and build a neural model with tractable posterior predictive distribution $p(x_{n+1} | x_1, \\dots, x_n)$ suited for complex objects like images. In order to do so the authors take a simple model with tractable posterior predictive $p(z_{n+1} | z_1, \\dots, z_n)$ (like a Gaussian Process, but not quite) and use it as a latent code, which is obtained from observations using an analytically inversible encoder $f$. This setup lets you take a complex $x$ like an image, run it through $f$ to obtain $z = f(x)$ -- a simplified latent representation for which it's easier to build joint density of all possible representations and hence easier to model the posterior predictive. By feeding latent representations of $x_1, \\dots, x_n$ (namely, $z_1, \\dots, z_n$) to the posterior predictive $p(z_{n+1} | f(x_1), \\dots, f(x_n))$ we obtain obtain a distribution of latent representations that are coherent with those of already observed $x$s. By sampling $z$ from this distribution and running it through $f^{-1}$ we recover an object in the observation space, $x_\\text{pred} = f^{-1}(z)$ -- a sample most coherent with previous observations.\n\nImportant choices are:\n\n* Model for latent representations $z$: one could use Gaussian Process, however authors claim it lacks some helpful properties and go for a more general [Student-T Process](http://www.shortscience.org/paper?bibtexKey=journals/corr/1402.4306). Then authors assume that each component of $z$ is a univariate sample from this process (and hence is independent from other components)\n* Encoder $f$. It has to be easily inversible and have an easy-to-evaluate Jacobian (the determinant of the Jacobi matrix). The former is needed to perform decoding of predictions in latent representations space and the later is used to efficiently compute a density of observations $p(x_1, \\dots, x_n)$ using the standard change of variables formula $$p(x_1, \\dots, x_n) = p(z_1, \\dots, z_n) \\left\\vert\\text{det} \\frac{\\partial f(x)}{\\partial x} \\right\\vert$$The architecture of choice for this task is [RealNVP](http://www.shortscience.org/paper?bibtexKey=journals/corr/1605.08803)\n\nDone this way, it's possible to write out the marginal density $p(x_1, \\dots, x_n)$ on all the observed $x$s and maximize it (as in the Maximum Likelihood Estimation). Authors choose to factor the joint density in an auto-regressive fashion (via the chain rule) $$p(x_1, \\dots, x_n) = p(x_1) p(x_2 \\vert x_1) p(x_3 \\vert x_1, x_2) \\dots p(x_n \\vert x_1, \\dots, x_{n-1}) $$with all the conditional marginals $p(x_i \\vert x_1, \\dots, x_{i-1})$ having analytic (student t times the jacobian) density -- this allows one to form a fully differentiable recurrent computation graph whose parameters (parameters of Student Processes for each component of $z$ + parameters of the encoder $f$) to be learned using any stochastic gradient method.\n\nhttps://i.imgur.com/yRrRaMs.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.07535"
    },
    "784": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.13409",
        "transcript": "An attention mechanism and a separate encoder/decoder are two properties of almost every single neural translation model. The question asked in this paper is- how far can we go without attention and without a separate encoder and decoder? And the answer is- pretty far! The model presented preforms just as well as the attention model of Bahdanau on the four language directions that are studied in the paper.\n\nThe translation model presented in the paper is basically a simple recurrent language model. A recurrent language model receives at every timestep the current input word and has to predict the next word in the dataset. To translate with such a model, simply give it the current word from the source sentence and have it try to predict the next word from the target sentence.\n\nObviously, in many cases such a simple model wouldn't work. For example, if your sentence was \"The white dog\" and you wanted to translate to Spanish (\"El perro blanco\"), at the 2nd timestep, the input would be \"white\" and the expected output would be \"perro\" (dog). But how could the model predict \"perro\" when it hasn't seen \"dog\" yet?\n\nTo solve this issue, we preprocess the data before training and insert \"empty\" padding tokens into the target sentence. When the model outputs such a token, it means that the model would like to read more of the input sentence before emitting the next output word.\n\nSo in the example from above, we would change the target sentence to \"El PAD perro blanco\". Now, at timestep 2 the model emits the PAD symbol. At timestep 3, when the input is \"dog\", the model can emit the token \"perro\". These padding symbols are deleted in post-processing, before the output is returned to the user. You can see a visualization of the decoding process below:\n\nhttps://i.imgur.com/znI6xoN.png\n\nTo enable us to use beam search, our model actually receives the previous outputted target token in addition to receiving the current source token at every timestep.\n\nPyTorch code for the model is available at https://github.com/ofirpress/YouMayNotNeedAttention ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.13409"
    },
    "785": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.18653/v1/e17-2025",
        "transcript": "## __Background__\nRNN language models are composed of:\n1. Embedding layer\n2. Recurrent layer(s) (RNN/LSTM/GRU/...)\n3. Softmax layer (linear transformation + softmax operation)\n\nThe embedding matrix and the matrix of the linear transformation just before the softmax operation are of the same size (size_of_vocab * recurrent_state_size) . \nThey both contain one representation for each word in the vocabulary. \n\n## __Weight Tying__\nThis paper shows, that by using the same matrix as both the input embedding and the pre-softmax linear transformation (the output embedding), the performance of a wide variety of language models is improved while the number of parameters is massively reduced. \nIn weight tied models each word has just one representation that is used in both the input and output embedding.\n\n## __Why does weight tying work?__\n1. In the paper we show that in un-tied language models, the output embedding contains much better word representations that the input embedding. We show that when the embedding matrices are tied, the quality of the shared embeddings is comparable to that of the output embedding in the un-tied model. So in the tied model the quality of the input and output embeddings is superior to the quality of those embeddings in the un-tied model. \n2. In most language modeling tasks because of the small size of the datasets the models tend to overfit. When the number of parameters is reduced in a way that makes sense there is less overfitting because of the reduction in the capacity of the network.\n\n## __Can I tie the input and output embeddings of the decoder of an translation model?__\nYes, we show that this reduces the model's size while not hurting its performance. \nIn addition, we show that if you preprocess your data using BPE, because of the large overlap between the subword vocabularies of the source and target language, __Three-Way Weight Tying__ can be used. In Three-Way Weight Tying, we tie the input embedding in the encoder to the input and output embeddings of the decoder (so each word has one representation which is used across three matrices).\n\n[This](http://ofir.io/Neural-Language-Modeling-From-Scratch/) blog post contains more details about the weight tying method. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.18653/v1/e17-2025"
    },
    "786": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/PressBBBW17",
        "transcript": "This paper shows how to train a character level RNN to generate text using only the GAN objective (reinforcement learning and the maximum-likelihood objective are not used).\n\nThe baseline WGAN is made up of:\n* A recurrent **generator** that first embeds the previously omitted token, inputs this into a GRU, which outputs a state that is then transformed into a distribution over the character vocabulary (which represents the model's belief about the next output token). \n* A recurrent **discriminator** that embeds each input token and then feeds them into a GRU. A linear transformation is used on the final hidden state in order to give a \"score\" to the input (a correctly-trained discriminator should give a high score to real sequences of text and a low score to fake ones).\n\nThe paper shows that if you try to train this baseline model to generate sequences of length 32 it just wont work (only gibberish is generated).\n\nIn order to get the model to work, the baseline model is augmented in three different ways:\n1. **Curriculum Learning**: At first the generator has to generate sequences of length 1 and the discriminator only trains on real and generated sequences of length 1. After a while, the models moves on to sequences of length 2, and then 3, and so on, until we reach length 32. \n2. **Teacher Helping**: In GANs the problem is usually that the generator is too weak. In order to help it, this paper proposes a method in which at stage $i$ in the curriculum, when the generator should generate sequences of length $i$, we feed it a real sequence of length $i-1$ and ask it to just generate 1 character more. \n3. **Variable Lengths**: In each stage $i$ in the curriculum learning process, we generate and discriminate sequences of length $k$, for each $ 1 \\leq k \\leq i$ in each batch (instead of just generating and discriminating sequences of length exactly $i$). \n\n\n\n\n\n\n\n[[code]](https://github.com/amirbar/rnn.wgan)\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.01399"
    },
    "787": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.07792",
        "transcript": "TLDR; The authors jointly train a Logistic Regression Model with sparse features that is good at \"memorization\" and a deep feedforward net with embedded sparse features that is good at \"generalization\". The model is live in the Google Play store and has achieved a 3.9% gain in app acquisiton as measured by A/B testing.\n\n#### Key Points\n\n- Wide Model (Logistic Regression) gets cross product of binary features, e.g. \"AND(user_installed_app=netflix, impression_app=pandora\") as inputs. Good at memorization.\n- Deep Model alone has a hard time to learning embedding for cross-product features because no data for most combinations but still makes predictions.\n- Trained jointly on 500B examples.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.07792"
    },
    "788": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.05250",
        "transcript": "TLDR; A new dataset of ~100k questions and answers based on ~500 articles from Wikipedia. Both questions and answers were collected using crowdsourcing. Answers are of various types: 20% dates and numbers, 32% proper nouns, 31% noun phrase answers and 16% other phrases. Humans achieve an F1 score of 86%, and the proposed Logistic Regression model gets 51%. It does well on simple answers but struggles with more complex types of reasoning. Tge data set is publicly available at https://stanford-qa.com/.\n\n#### Key Points\n\n- System must select answers from all possible spans in a passage. $O(N^2)$ possibilities for N tokens in passage.\n- Answers are ambiguous. Humans achieve 77% on exact match and 86% on F1 (overlap based). Humans would probably achieve close to 100% if the answer phrases were unambiguous.\n- Lexicalized and dependency tree path features are most important for the LR model\n- Model performs best on dates and numbers, single tokens, and categories with few possible candidates",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.05250"
    },
    "789": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.08725",
        "transcript": "TLDR; The authors replace the standard attention mechanism (Bahdanau et al) with a RNN/GRU, hoping to model historical dependencies for translation and mitigating the \"coverage problem\". The authors evaluate their model on Chinese-English translation where they beat Moses (SMT) and GroundHog baselines. The authors also visualize the attention RNN and show that the activations make intuitive sense.\n\n#### Key Points\n\n- Training time: 2 weeks on Titan X, 300 batches per hour, 2.9M language pairs\n\n#### Notes\n\n- The authors argue that their attention mechanism works better b/c it can capture dependencies among the source states. I'm not convinced by this argument. These states already capture dependencies because they are generated by a bidirectional RNN.\n- Training seems *very* slow for only 2.9M pairs. I wonder if this model is prohibitively expensive for any production system.\n- I wonder if we can use RL to \"cover\" phrases in the source sentences out of order. At each step we pick a span to cover before generating the next token in the target sequence.\n- The authors don't evaluate Moses for long sentences, why?",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.08725"
    },
    "790": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.04671",
        "transcript": "TLDR; The authors propose Progressive Neural Networks (ProgNN), a new way to do transfer learning without forgetting prior knowledge (as is done in finetuning). ProgNNs train a neural neural on task 1, freeze the parameters, and then train a new network on task 2 while introducing lateral connections and adapter functions from network 1 to network 2. This process can be repeated with further columns (networks). The authors evaluate ProgNNs on 3 RL tasks and find that they outperform finetuning-based approaches.\n\n#### Key Points\n\n- Finetuning is a destructive process that forgets previous knowledge. We don't want that.\n- Layer h_k in network 3 gets additional lateral connections from layers h_(k-1) in network 2 and network 1. Parameters of those connections are learned, but network 2 and network 1 are frozen during training of network 3.\n- Downside: # of Parameters grows quadratically with the number of tasks. Paper discussed some approaches to address the problem, but not sure how well these work in practice.\n- Metric: AUC (Average score per episode during training) as opposed to final score. Transfer score = Relative performance compared with single net baseline. \n- Authors use Average Perturbation Sensitivity (APS) and Average Fisher Sensitivity (AFS) to analyze which features/layers from previous networks are actually used in the newly trained network.\n- Experiment 1: Variations of Pong game. Baseline that finetunes only final layer fails to learn. ProgNN beats other  baselines and APS shows re-use of knowledge.\n- Experiment 2: Different Atari games. ProgNets result in positive Transfer 8/12 times, negative transfer 2/12 times. Negative transfer may be a result of optimization problems. Finetuning final layers fails again. ProgNN beats other approaches.\n- Experiment 3: Labyrinth, 3D Maze. Pretty much same result as other experiments.\n\n\n#### Notes\n\n- It seems like the assumption is that layer k always wants to transfer knowledge from layer (k-1). But why is that true? Network are trained on different tasks, so the layer representations, or even numbers of layers, may be completely different. And Once you introduce lateral connections from all layers to all other layers the approach no longer scales.\n- Old tasks cannot learn from new tasks. Unlike humans.\n- Gating or residuals for lateral connection could make sense to allow to network to \"easily\" re-use previously learned knowledge.\n- Why use AUC metric? I also would've liked to see the final score. Maybe there's a good reason for this, but the paper doesn't explain.\n- Scary that finetuning the final layer only fails in most experiments. That's a very commonly used approach in non-RL domains.\n- Someone should try this on non-RL tasks.\n- What happens to training time and optimization difficult as you add more columns? Seems prohibitively expensive.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.04671"
    },
    "791": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.07843",
        "transcript": "TLDR; The authors combine a standard LSTM softmax with [Pointer Networks](https://arxiv.org/abs/1506.03134) in a mixture model called Pointer-Sentinel LSTM (PS-LSTM). The pointer networks helps with rare words and long-term dependencies but is unable to refer to words that are not in the input. The oppoosite is the case for the standard softmax. By combining the two approaches we get the best of both worlds. The probability of an output words is defined as a mixture of the pointer and softmax model and the mixture coefficient is calculated as part of the pointer attention. The authors evaluate their architecture on the PTB Language Modeling dataset where they achieve state of the art. They also present a novel WikiText dataset that is larger and more realistic then PTB.\n\n\n### Key Points:\n\n- Standard RNNs with softmax struggle with rare and unseen words, even when adding attention.\n- Use a window of the most recent`L` words to match against.\n- Probability of output with gating: `p(y|x) = g * p_vocab(y|x) + (1 - g) * p_ptr(y|x)`.\n- The gate `g` is calcualted as an extra element in the attention module. Probabilities for the pointer network are then normalized accordingly.\n- Integrating the gating funciton computation into the pointer network is crucial: It needs to have access to the pointer network state, not just the RNN state (which can't hold long-term info)\n- WikiText-2 dataset: 2M train tokens, 217k validation tokens, 245k test tokens. 33k vocab, 2.6% OOV. 2x larger than PTB.\n- WikiText-1-3 dataset: 103M train tokens, 217k validation tokens, 245k test tokens. 267k vocab, 2.4% OOV. 100x larger than PTB.\n- Pointer Sentiment Model leads to stronger improvements for rare words - that makes intuitive sense.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1609.07843"
    },
    "792": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1608.01281",
        "transcript": "TLDR; The authors use policy gradients on an RNN to train a \"hard\" attention mechanism that decides whether to output something at the current timestep or not. Their algorithm is online, which means it does not need to see the complete sequence before making a prediction, as is the case with soft attention. The authors evaluate their model on small- and medium-scale speech recognition tasks, where they achieve performance comparable to standard sequential models.\n\n#### Notes:\n\n- Entropy regularization and baselines were critical to make the model learn\n- Neat trick: Increase dropout as training progresses\n- Grid LSTMs outperformed standard LSTMs",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1608.01281"
    },
    "793": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.01874",
        "transcript": "TLDR; The authors add a reconstruction objective to the standard seq2seq model by adding a \"Reconstructor\" RNN that is trained to re-generate the source sequence based on the hidden states of the decoder. A reconstruction cost is then added to the cost function and the architecture is trained end-to-end. The authors find that the technique improves upon the baseline both when 1. used during training only and 2. when used as a rankign objective during beam search decoding.\n\n#### Key Points\n\n- Problem to solve:\n  - Standard seq2seq models tend to under- and over-translate because they don't ensure that all of the source information is covered by the target side.\n  - The MLE objective only captures information from source -> target, which favors short translations. Thus, Increasing the beam size actually lowers translation quality\n- Basic Idea\n  - Reconstruct source sentences form the latent representations of the decoder\n  - Use attention over decoder hidden states\n  - Add MLE reconstruction probability to the training objective\n- Beam Decoding is now two-phase scheme\n  1. Generate candidates using the encoder-decoder\n  2. For each candidate, compute a reconstruction score and use it to re-rank  together with the likelihood\n- Training Procedure\n  - Params Chinese-English: `vocab=30k, maxlen=80, embedding_dim=620, hidden_dim=1000, batch=80`.\n  - 1.25M pairs trained for 15 epochs using Adadelta, the train with reconstructor for 10 epochs.\n- Results:\n  - Model increases BLEU from 30.65 -> 31.17 (beam size 10) when used for training only and decoding stays unchaged\n  - BLEU increase from 31.17 -> 31.73 (beam size 10) when also used for decoding\n  - Model successfully deals with large decoding spaces, i.e. BLEU now increases together with beam size\n\n\n#### Notes\n\n- [See this issue for author's comments](https://github.com/dennybritz/deeplearning-papernotes/issues/3)\n- I feel like \"adequacy\" is a somewhat strange description of what the authors try to optimize. Wouldn't \"coverage\" be more appropriate?\n- In Table 1, why does BLEU score still decrease when length normalization is applied? The authors don't go into detail on this.\n- The training curves are a bit confusing/missing. I would've liked to see a standard training curve that shows the MLE objective loss and the finetuning with reconstruction objective side-by-side.\n- The training procedure somewhat confusing. The say \"We further train the model for 10 epochs\" with reconstruction objective, byt then \"we use a trained model at iteration 110k\". I'm assuming they do early-stopping at 110k * 80 = 8.8M steps. Again, would've liked to see the loss curves for this, not just BLEU curves.\n- I would've liked to see model performance on more \"standard\" NMT datasets like EN-FR and EN-DE, etc.\n- Is there perhaps a smarter way to do reconstruction iteratively by looking at what's missing from the reconstructed output? Trainig with reconstructor with MLE has some of the same drawbacks as training standard enc-dec with MLE and teacher forcing.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.01874"
    },
    "794": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.05108",
        "transcript": "TLDR; The standard attention model does not take into account the \"history\" of attention activations, even though this should be a good predictor of what to attend to next. The authors augment a seq2seq network with a dynamic memory that, for each input, keep track of an attention matrix over time. The model is evaluated on English-German and Englih-Chinese NMT tasks and beats competing models.\n\n#### Notes\n\n- How expensive is this, and how much more difficult are these networks to train?\n- Sequentiallly attending to neighboring words makes sense for some language pairs, but for others it doesn't. This method seems rather restricted because it only takes into account a window of k time steps.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.05108"
    },
    "795": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1610.00388",
        "transcript": "The authors propose a framework where a Reinforcement Learning agents makes decisions of reading the next input words or producing the next output word to trade off translation quality and time delay (caused by read operations). The reward function is based on both quality (BLEU score) and delay (various metrics and hyperparameters). The authors use Policy Gradient to optimize the model, which is initialized from a pre-trained translation model. They apply to approach to WMT'15 EN-DE and EN-RU translation and show that the model increases translation quality in all settings and is able to trade off effectively between quality and delay.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1610.00388"
    },
    "796": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.06450",
        "transcript": "TLDR; The authors propose a new normalization scheme called \"Layer Normalization\" that works especially well for recurrent networks. Layer Normalization is similar to Batch Normalization, but only depends on a single training case. As such, it's well suited for variable length sequences or small batches. In Layer Normalization each hidden unit shares the same normalization term. The authors show through experiments that Layer Normalization converges faster, and sometimes to better solutions, than batch- or unnormalized RNNs. Batch normalization still performs better for CNNs.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.06450"
    },
    "797": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.01704",
        "transcript": "TLDR; The authors propose a new \"Hierarchical Multiscale RNN\" (HM-RNN) architecture. This models explicitly learns both temporal and hierarchical (character -> word -> phrase -> ...) representations without needing to be told what the structure or timescale of the hierarchy is. This is done by adding binary boundary detectors at each layer. These detectors activate based on whether the segment in a certain layer is finished or not. Based on the activation of these boundary detectors information is then propagated to neighboring layers. Because this model involves discrete decision making based on binary outputs it is trained using a straight-through estimator. The authors evaluate the model on Language Modeling and Handwriting Sequence Generation tasks, where it outperforms competing models. Qualitatively the authors show that the network learns meaningful boundaries (e.g. spaces) without being needing to be told about them.\n\n### Key Points\n\n- Learning both hierarchical and temporal representations at the same time is a challenge for RNNs\n- Observation: High-level abstractions (e.g. paragraphs) change slowly, but low-level abstractions (e.g. words) change quickly. These should be updated at different timescales.\n- Benefits of HN-RNN: (1) Computational Efficiency (2) Efficient long-term dependency propagation (vanishing gradients) (3) Efficient resource allocation, e.g. higher layers can have more neurons\n- Binary boundary detector at each layer is turned on if the segment of the corresponding layer abstraction (char, word, sentence, etc) is finished.\n- Three operations based on boundary detector state: UPDATE, COPY, FLUSH\n- UPDATE Op: Standard LSTM update. This happens when the current segment is not finished, but the segment one layer below is finished.\n- COPY Op: Copies previous memory cell. Happens when neither the current segment nor the segment one layer below is finished. Basically, this waits for the lower-level representation to be \"done\".\n- FLUSH Op: Flushes state to layer above and resets the state to start a new segment. Happens when the segment of this layer is finished.\n- Boundary detector is binarized using a step function. This is non-differentiable and training is done with a straight-through estimator that estimates the gradient using a similar hard sigmoid function.\n- Slope annealing trick: Gradually increase the slop of the hard sigmoid function for the boundary estimation to make it closer to a discrete step function over time. Needed to be SOTA.\n- Language Modeling on PTB: Beats state of the art, but not by much.\n- Language Modeling on other data: Beats or matches state of the art.\n- Handwriting Sequence Generation: Beats Standard LSTM\n\n\n### My Notes\n\n- I think the ideas in this paper are very important, but I am somewhat disappointed by the results. The model is significantly more complex with more knobs to tune than competing models (e.g. a simple batch-normalized LSTM). However, it just barely beats those simpler models by adding new \"tricks\" like slope annealing. For example, the slope annealing schedule with a `0.04` constant looks very suspicious.\n- I don't know much about Handwriting Sequence Generation, but I don't see any comparisons to state of the art models. Why only compare to a standard LSTM?\n- The main argument is that the network can dynamically learn hierarchical representations and timescales. However, the number of layers implicitly restricts how many hierarchical representations the network can and cannot learn. So, there still is a hyperparameter involved here that needs to be set by hand.\n- One claim is that the model learns boundary information (spaces) without being told about them. That's true, but I'm not convinced that's as novel as the authors make it out to be. I'm pretty sure that a standard LSTM (perhaps with extra skip connections) will learn the same and that it's possible to tease these out of the LSTM parameter matrices.\n- Could be interesting to apply this to CJK languages where boundaries and hierarchical representations are more apparent.\n- The authors claim that \"computational efficiency\" is one of the main benefits of this model because higher level representations need to be updated less frequency. However, there are no experiments to verify this claim. Obviously this is true in theory, but I can imagine that in practice this model is actually slower to train. Also, what about convergence time?\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1609.01704"
    },
    "798": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.04558",
        "transcript": "TLDR; The authors train a multilingual Neural Machine Translation (NMT) system based on the Google NMT architecture by prepend a special `2[lang]` (e.g. `2fr`) token to the input sequence to specify the target language. They empirically evaluate model performance on many-to-one, one-to-many and many-to-many translation tasks and demonstrate evidence for shared representations (interlingua).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.04558"
    },
    "799": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.06349",
        "transcript": "TLDR; The authors present an RNN-based variational autoencoder that can learn a  latent sentence representation while learning to decode. A linear layer that predicts the parameter of a Gaussian distribution is inserted between encoder and decoder. The loss is a combination of the reconstruction objective and the KL divergence with the prior (Gaussian) - similar to the \"standard\" VAE does. The authors evaluate the model on Language Modeling and Impution (Inserting Missing Words) tasks and also present a qualitative analysis of the latent space.\n\n#### Key Points\n\n- Training is tricky. Vanilla training results in the decoder ignoring the encoder and the KL error term becoming zero.\n- Training Trick 1: KL Cost Annealing. During training, increase weight on the KL term of the cost to anneal from vanilla to VAE.\n- Training Trick 2: Word dropout using a word keep rate hyperparameter. This forces the decoder to rely more on the global representation.\n- Results on Language Modeling: Standard model (without cost annealing and word dropout) trails Vanilla RNNLM model, but not by much. KL cost term goes to zero in this setting. In an inputless decoder setting (word keep prob = 0) the VAE outperforms the RNNLM (obviously)\n- Results on Imputing Missing Words: Benchmarked using an adversarial error classifier. VAE significantly outperforms RNNLM. However, the comparison is somewhat unfair since the RNNML has nothing to condition on and relies on unigram distribution for the first token.\n- Qualitative: Can use higher word dropout to get more diverse sentences\n- Qualitative: Can walk the latent space and get grammatical and meaningful sentences.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.06349"
    },
    "800": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1610.06258",
        "transcript": "TLDR; The authors propose \"fast weights\", a type of attention mechanism to the recent past that performs multiple steps of computation between each hidden state computation step in an RNN. The authors evaluate their architecture on various tasks that require short-term memory, arguing that the fast weights mechanism frees up the RNN from memorizing sthings in the hidden state which is freed up for other types of computation.\n\n### Key Points\n\n- Currently, RNNs have slow-changing long-term memory (Permanent Weights) and fast changing short-term memory (hidden state). We want something in the middle: Fast weights with higher storage capacity.\n- For each transition in the RNN, multiple transitions can be made by the fast weights. They are a kind of attention mechanism to the recent past that is not parameterized separately but depends on the past states.\n- Fast weights are decayed over time and based on the outer product of previous hidden states: `A(t+1) = lambdaA(t) + eta*h(t)h(t)^T`.\n- The next hidden state of the RNN is computed by a regular transition based on input adn previous state combined by an \"inner loop\" of S steps of the fast weights.\n- \"At each iteration of the inner loop the fast weight matrix A is eqivalent to attending to past hidden vectors in proportion to their scalar product with the current hidden state, weighted by a decay factor\" - And this is efficient to compute.\n- Added Layer Normalization to fast weights to prevent exploding/vanishign gradients.\n- Associative Retrieval Toy Task: Memorize recent key-value pairs. Fast weights siginifcantly outperform RNN, LSTM and Associative LSTM.\n- Visual Attention on MNIST: Beats RNN/LSTM and is comparable to CovnNet for large number of features.\n- Agents with Memory: Fast Weight net learns faster in a partially obseverable environment where the networks must remember the previous states.\n\n### Thoughts\n\n-Overall I think this is very exciting work. It kind of reminds me of Adaptive Computation Time where you dynamically decide how many steps to \"ponder\" before making another outputs. However, it is also quite different in that this work explicitly \"attends\" over past states and isn't really about computation time.\n- In the experiments the authors say they set S=1 (i.e. just one inner loop step). Why is that? I thought one of the more important points of fast weights would be to have additional computation betwene each slow step. This also raises the question of how to pick this hyperparameter.\n- A lot of references to Machine Translation models with attention but not NLP experiments.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1610.06258"
    },
    "801": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.02270",
        "transcript": "TLDR; The authors prorpose the \"EpiReader\" model for Question Answering / Machine Comprehension. The model consists of two modules: An Extractor that selects answer candidates (single words) using a Pointer network, and a Reasoner that rank these candidates by estimating textual entailment. The model is trained end-to-end and works on cloze-style questions. The authors evaluate the model on CBT and CNN datasets where they beat Attention Sum Reader and MemNN architectures.\n\n\n#### Notes\n\n- In most architectures, the correct answer is among the top5 candidates 95% of the time.\n- Soft Attention is a problem in many architectures. Need a way to do hard attention.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.02270"
    },
    "802": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.01269",
        "transcript": "TLDR; The author present and end-2-end dialog system that consists of an LSTM, action templates, an entity extraction system, and custom code for declaring business rules. They test the systme on a toy task where the goal is to call a person from an address book. They train the system on 21 dialogs using Supervised Learning, and then optimize it using Reinforcement Learning, achieving 70% task completion rates.\n\n#### Key Points\n\n- Task: User asks to call person. Action: Find in address book and place call\n- 21 example dialogs\n- Several hundred lines of Python code to block certain actions\n- External entity recognition API\n- Hand-crafted features as input to the LSTM. Hand-crafted action template.\n- RNN maps from sequence to action template, First pre-train LSTM to reproduce dialogs using Supervised Learning, then train using RL / policy gradient\n- The system doesn't generate text, it picks a template\n\n\n#### Notes\n\n- I wonder how well the system would generalize to a task that has a larger action space and more varied conversations. The 21 provided dialogs cover a lot of the taks space already. Much harder to do that in larger spaces.\n- I wouldn't call this approach end-to-end ;)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.01269"
    },
    "803": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.00179",
        "transcript": "TLDR; The authors finetune an FR -> EN NMT model using a RL-based dual game. 1. Pick a French sentence from a monolingual corpus and translate it to EN. 2. Use an EN language model to get a reward for the translation 3. Translate the translation back into FR using an EN -> FR system. 4. Get a reward based on the consistency between original and reconstructed sentence. Training this architecture using Policy Gradient authors can make efficient use of monolingual data and show that a system trained on only 10% of parallel data and finetuned with monolingual data achieves comparable BLUE scores as a system trained on the full set of parallel data.\n\n\n### Key Points\n\n- Making efficient use of monolingual data to improve NMT systems is a challenge\n- Two Agent communication game: Agent A only knows language A and agent B only knows language B. A send message through a noisy translation channel, B receives message, checks its correctness, and sends it back through another noisy translation channel. A checks if it is consistent with the original message. Translation channels are then improves based on the feedback.\n- Pieces required: LanguageModel(A), LanguageModel(B), TranslationModel(A->B), TranslationModel(B->A). Monolingual Data.\n- Total reward is linear combination of: `r1 = LM(translated_message)`, `r2 = log(P(original_message | translated_message)`\n- Samples are based on beam search using the average value as the gradient approximation\n- EN -> FR pretrained on 100% of parallel data: 29.92 to 32.06 BLEU\n- EN -> FR pretrained on 10% of parallel data: 25.73 to 28.73 BLEU\n- FR -> EN pretrained on 100% of parallel data: 27.49 to 29.78 BLEU\n- FR -> EN pretrained on 10% of parallel data: 22.27 to 27.50 BLEU\n\n\n### Some Notes\n\n- I think the idea is very interesting and we'll see a lot related work coming out of this. It would be even more amazing if the architecture was trained from scratch using monolingual data only. Due the the high variance of RL methods this is probably quite hard to do though.\n- I think the key issue is that the rewards are  quite noisy, as is the case with MT in general. Neither the language model nor the BLEU scores gives good feedback for the \"correctness\" of a translation.\n- I wonder why there is such a huge jump in BLEU scores for FR->EN on 10% of data, but not for EN->FR on the same amount of data.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.00179"
    },
    "804": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.04636",
        "transcript": "TLDR; The authors train a DQN on text-based games. The main difference is that their Q-Value functions embeds the state (textual context) and action (text-based choice) separately and then takes the dot product between them. The authors call this a Deep Reinforcement Learning Relevance network. Basically, just a different Q function implementation. Empirically, the authors show that their network can learn to solve \"Saving John\" and \"Machine of Death\" text games.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.04636v5"
    },
    "805": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1610.02424",
        "transcript": "TLDR; The authors propose a new Diverse Beam Search (DBS) decoding procedure that produces more diverse responses than standard Beam Search (BS). The authors divide the beam of size B into G groups of size B/G. At each step they perform beam search for each group with an added similarity penalty (with scaling factor lambda) that encourages groups to be pick different outputs. This procedure is done greedily, i.e. group 1 does regular BS, group 2 is conditioned on group 1, group 3 is conditioned on group 1 and 2, and so on. Similarity functions include Hamming distance, Cumulative Diversity, n-gram diversity and neural embedding diversity. Hamming Distance tends to perform best. The authors evaluate their model on Image Captioning (COCO, PASCAL-50S), Machine Translation (europarl) and Visual Question Generation. For Image Captioning the authors perform a human evaluation (1000 examples on Mechanical Turk) and find that DBS is preferred over BS 60% of the time.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1610.02424v1"
    },
    "806": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.04717",
        "transcript": "TLDR; The authors encourage exploration by adding a pseudo-reward of the form $\\frac{\\beta}{\\sqrt{count(state)}}$ for infrequently visited states. State visits are counted using Locality Sensitive Hashing (LSH) based on an environment-specific feature representation like raw pixels or autoencoder representations. The authors show that this simple technique achieves gains in various classic RL control tasks and several games in the ATARI domain.\n\nWhile the algorithm itself is simple there are now several more hyperaprameters to tune: The bonus coefficient `beta`, the LSH hashing granularity (how many bits to use for hashing) as well as the type of feature representation based on which the hash is computed, which itself may have more parameters. The experiments don't paint a consistent picture and different environments seem to need vastly different hyperparameter settings, which in my opinion will make this technique difficult to use in practice.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1611.04717v1"
    },
    "807": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1603.06393",
        "transcript": "TLDR; The authors introduce CopyNet, a variation on the seq2seq that incorporates a \"copying mechanism\". With this mechanism, the effective vocabulary is the union of the standard vocab and the words in the current source sentence. CopyNet predicts words based on a mixed probability of the standard attention mechanism and a new copy mechanism. The authors show empirically that on toy and summarization tasks CopNet behaves as expected: The decoder is dominated by copy mode when it tries to replicate something from the source.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1603.06393"
    },
    "808": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1610.03017",
        "transcript": "\nTLDR; The authors propose a character-level Neural Machine Translation (NMT) architecture. The encoder is a convolutional network with max-pooling and highway layers that reduces size of the source representation. It does not use explicit segmentation. The decoder is a standard RNN. The authors apply their model to WMT'15 DE-EN, CS-EN, FI-EN and RU-EN data in bilingual and multilingual settings. They find that their model is competitive in bilingual settings and significantly outperforms competing models in the multilingual setting with a shared encoder.\n\n#### Key Points\n\n- Challenge: Apply standard seq2seq models to characters is hard because representation is too long. Attention network complexity grows quadratically with sequence length.\n- Word-Level models are unable to model rare and out-of-vocab tokens and softmax complexity grows with vocabulary size.\n- Character level models are more flexible: No need for explicit segmentation, can model morphological variants, multilingual without increasing model size.\n- Reducing the length of the source sentence is key to fast training in char models.\n- Encoder Network: Embedding -> Conv -> Maxpool -> Highway -> Bidirectional GRU\n- Attenton Network: Single Layer\n- Decoder: Two Layer GRU\n- Multilingual setting: Language examples are balanced within each batch. No language identifier is provided to the encoder\n- Bilingual Results: char2char performs as well as or better than bpe2char or bpe2bpe\n- Multilingual Results: char2char outperforms bpe2char\n- Trained model is robust to spelling mistakes and unseen morphologies\n- Training time: Single Titan X training time for bilingual model is ~2 weeks. ~2.5 updates per second with batch size 64.\n\n\n#### Notes\n\n- I wonder if you can extract segmentation info from the network post training.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1610.03017"
    },
    "809": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.04423",
        "transcript": "TLDR; The authors present a novel Attention-over-Attention (AoA) model for Machine Comprehension. Given a document and cloze-style question, the model predicts a single-word answer. The model,\n\n1. Embeds both context and query using a bidirectional GRU\n2. Computes a pairwise matching matrix between document and query words\n3. Computes query-to-document attention values\n4. Computes document-to-que attention averages for each query word\n5. Multiplies the two attention vectors to get final attention scores for words in the document\n6. Maps attention results back into the vocabulary space\n\nThe authors evaluate the model on the CNN News and CBTest Question Answering datasets, obtaining state-of-the-art results and beating other models including EpiReader, ASReader, etc.\n\n\n#### Notes:\n\n- Very good model visualization in the paper\n- I like that this model is much simpler than EpiReader while also performing better",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.04423"
    },
    "810": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/DanihelkaWUKG16",
        "transcript": "TLDR; The authors propose Associate LSTMs, a combination of external memory based on Holographic Reduced Representations and LSTMs. The memory provides noisy key-value lookup based on matrix multiplications without introducing additional parameters. The authors evaluate their model on various sequence copying and memorization tasks, where it outperforms vanilla LSTMs and competing models with a similar number of parameters.\n\n#### Key Points\n\n- Two limitations of LSTMs: 1. N cells require NxN weight matrices. 2. Lacks mechanism to index memory\n- Idea of memory comes from \"Holographic Reduced Representations\" (Plate, 2003), but authors add multiple redundant memory copies to reduce noise.\n- More copies of memory => Less noise during retrieval\n- In the LSTM update equations input and output keys to the memory are computed\n- Compared to: LSTM, Permutation LSTM, Unitary LSTM, Multiplicative Unitary LSTM\n- Tasks: Episodic Copy, XML modeling, variable assignment, arithmetic, sequence prediction\n\n#### Notes\n\n- Only brief comparison with Neural Turing Machines in appendix. Probably NTMs outperform this and are simpler. No comparison with attention mechanisms, memory networks, etc. Why?\n- It's surprising to me that deep LSTM without any bells and whistles actually perform pretty well on many of the tasks. Is the additional complexity really worth it?",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.03032"
    },
    "811": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.07725",
        "transcript": "TLDR; The authors apply adversarial training on labeld data and virtual adversarial training on unlabeled data to the embeddings in text classification tasks. Their models, which are straightforward LSTM architectures, either match or surpass the current state of the art on several text classification tasks. The authors also show that the embeddings learned using adversarial training tend to be tuned better to the corresponding classification task.\n\n#### Key Points\n\n- In Image classification we can apply adversarial training directly to the inputs. In Text classification the inputs are discrete and we cannot make small perturbations, but we can instead apply adversarial training to embeddings.\n- Trick: To prevent the model from making perturbations irrelevant by learning embeddings with large norms: Use normalized embeddings.\n- Adversarial Training (on labeled examples)\n  - At each step of training, identify the \"worst\" (in terms of cost) perturbation `r_adv` to the embeddings within a given constant epsilon, which a hyperparameter. Train on that. In practice `r_adv` is estimated using a linear approximation.\n  - Add a `L_adv` adversarial loss term to the cost function.\n- Virtual Adversarial Training (on unlabeled examples)\n  - Minimize the KL divergence between the outputs of the model given the regular and perturbed example as inputs.\n  - Add `L_vad` loss to the cost function.\n- Common misconception: Adversarial training is equivalent to training on noisy examples, but it actually is a stronger regularizer because it explicitly increases cost.\n- Model Architectures:\n  - (1) Unidirectional LSTM with prediction made at the last step\n  - (2) Bidirectional LSTM with predictions based on concatenated last outputs\n- Experiments/Results\n  - Pre-Training: For all experiments a 1-layer LSTM language model is pre-trained on all labeled and unlabeled examples and used to initialize the classification LSTM.\n  - Baseline Model: Only embedding dropout and pretraining\n  - IMDB: raining curves show that adversarial training acts as a good regularizer and prevents overfitting. VAT matches state of the art using a unidirectional LSTM only.\n  - IMDB embeddings: Baseline model places \"good\" close to \"bad\" in embedding space. Adv. training ensures that small perturbations in embeddings don't change the sentiment classification result so these two words become properly separated.\n  - Amazon Reviews and RCV1: Adv. + Vadv. achieve state of the art.\n  - Rotten Tomatoes: Adv. + Vadv. achieve state of the art. Because unlabeled data overwhelms labeled data vadv. training results in decrease of performance.\n  - DBPedia: Even the baseline outperforms state of the art (better optimizer?), adversarial training improves on that.\n\n### Thoughts\n\n- I think this is a very well-written paper with impressive results. The only thing that's lacking is a bit of consistency. Sometimes pure virtual adversarial training wins, and sometimes adversarial + virtual adversarial wins. Sometimes bi-LSTMs make things worse, sometimes better. What is the story behind that? Do we really need to try all combinations to figure out what works for a given dataset?\n- Not a big deal, but a few bi-LSTM experiments seem to be missing. This just always makes me if they are \"missing for a reason\" or not ;)\n- There are quite a few differences in hyperparameters and batch sizes between datasets. I wonder why. Is this to stay consistent with the models they compare to? Were these parameters optimized on a validation set (the authors say only dropout and epsilon were optimized)?\n- If Adversarial Training is a stronger regularizer than random permutations I wonder if we still need dropout in the embeddings. Shouldn't adversarial training take care of that?",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.07725"
    },
    "812": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.07086",
        "transcript": "TLDR; The authors propose to use the Actor Critic framework from Reinforcement Learning for Sequence prediction. They train an actor (policy) network to generate a sequence together with a critic (value) network that estimates the q-value function. Crucially, the actor network does not see the ground-truth output, but the critic does. This is different from LL (log likelihood) where errors are likely to cascade. The authors evaluate their framework on an artificial spelling correction and a real-world German-English Machine Translation tasks, beating baselines and competing approaches in both cases.\n\n#### Key Points\n\n- In LL training, the model is conditioned on its own guesses during search, leading to error compounding.\n- The critic is allowed to see the ground truth, but the actor isn't\n- The reward is a task-specific score, e.g. BLEU\n- Use bidirectional RNN for both actor and critic. Actor uses a soft attention mechanism.\n- The reward is partially receives at each intermediate step, not just at the end\n- Framework is analogous to TD-Learning in RL\n- Trick: Use additional target network to compute q_t (see Deep-Q paper) for stability\n- Trick: Use delayed actor (as in Deep Q paper) for stability\n- Trick: Put constraint on critic to deal with large action spaces (is this analogous to advantage functions?)\n- Pre-train actor and critic to encourage exploration of the right space\n- Task 1: Correct corrupt character sequence. AC outperforms LL training. Longer sequences lead to stronger lift.\n- Task 2: GER-ENG Machine Translation: Beats LL and Reinforce models\n- Qualitatively, critic assigns high values to words that make sense\n- BLUE scores during training are lower than those of LL model - Why? Strong regularization? Can't overfit the training data.\n\n#### Notes\n\n- Why does the sequence length for spelling prediction only go up to 30? This seems very short to me and something that an LSTM should be able to handle quite easily. Would've like to see much longer sequences.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.07086"
    },
    "813": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Graves16",
        "transcript": "TLDR; The paper presents Adaptive Computation Time (ACT), an algorithm that allows RNNs to adaptively decide how much computation to expend per time step, also called \"pondering\". To prevent the network from computng indefinitely an extra term that encourages shorter computation is added to the cost. The architecture is fully differentiable and applicable to any type of RNN (e.g. LSTMs). The authors evaluate ACT on tasks of Parity, Logic, Addition, Sorting and character prediction. An interesting observation is that the numbet of pondering steps seems to predict \"boundaries\" in the data.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.08983"
    },
    "814": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.05397",
        "transcript": "TLDR; The authors augment the A3C (Asynchronous Actor Critic) algorithm with auxiliary tasks. These tasks share some of the network parameters but value functions for them are learned off-policy using n-step Q-Learning. The auxiliary tasks only used to learn a better representation and don't directly influence the main policy control. The technique, called UNREAL (Unsupervised Reinforcement and Auxiliary Learning), outperforms A3C on both the Atari and Labyrinth domains in terms of performance and training efficiency.\n\n\n#### Key Points\n\n- Environments contain a wide variety of possible training signals, not just cumulative reward\n- Base A3C agent uses CNN + RNN\n- Auxiliary Control and Prediction tasks share the convolutional and LSTM network for the \"base agent\". This forces the agent to balance improvement and base and aux. tasks.\n- Auxiliary Tasks\n  - Use off-policy RL algorithms (e.g. n-step Q-Learning) so that the same stream of experience from the base agent can be used for maximizing all tasks. Experience is sampled from a replay buffer.\n  - Pixel Changes (Auxiliary Control): Learn a policy for maximally changing the pixels in a grid of cells overlaid over the images\n  - Network Features (Auxiliary Control): Learn a policy for maximally activating units in a specific hidden layer\n  - Reward Prediction (Auxiliary Reward): Predict the next reward given some historical context. Crucially, because rewards tend to be sparse, histories are sampled in a skewed manner from the replay buffer so that P(r!=0) = 0.5. Convolutional features are shared with the base agent.\n  - Value Function Replay: Value function regression for the base agent with varying window for n-step returns.\n- UNREAL\n  - Base agent is optimized on-policy (A3C) and aux. tasks are optimized off-policy.\n- Experiments\n  - Agent is trained with 20-step returns and aux. tasks are performed every 20 steps.\n  - Replay buffer stores the most recent 2k observations, actions and rewards\n  - UNREAL tends to be more robust to hyperparameter settings than A3C\n  - Labyrinth\n    - 38% -> 83% human-normalized score. Each aux. tasks independently adds to the performance.\n    - Significantly faster learning, 11x across all levels\n    - Compared to input reconstruction technique: Input reconstruction hurts final performance b/c it puts too much focus on reconstructing relevant parts.\n  - Atari\n    - Not all experiments are completed yet, but UNREAL already surpasses state of the art agents and is more robust.\n\n#### Thoughts\n\n- I want an algorithm box please :)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.05397"
    },
    "815": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1610.09038",
        "transcript": "TLDR; The authors adopt Generative Adversarial Networks (GANs) to RNNs and train a discriminator to distinguish between sequences generated using teacher forcing (feeding ground truth inputs to the RNN) and scheduled sampling (feeding generated outputs as the next inputs). The inputs to the discriminator are both the predictions and the hidden states of the generative RNN. The generator is trained to fool the discriminator, forcing the dynamics of teacher forcing and scheduled sampling to become more similar. This procedure acts as regularizer, and results in better sample quality and generalization, particularly for long sequences. The authors evaluate their framework on Language Model (PTB), Pixel Generation (Sequential MNIST), Handwriting Generation, and Musisc Synthesis.\n\n### Key Points\n\n- Problem: During inference, errors in an RNN easily compound because the conditioning context may diverge from what is seen during training when the ground-truth labels are fed as inputs (teacher forcing).\n- Goal of professor forcing: Make the generative (free-run) behavior and the teacher-forced behavior match as closely as possible.\n- Discriminator Details\n  - Input is a behavior sequence `B(x, y, theta)` from the generative RNN that contains the hidden states and outputs.\n  - The training objective is to correctly classify whether or not a behavior sequence is generated using teacher forcing vs. scheduled sampling.\n- Generator\n  - Standard RNN with MLE training objective and an additional term to fool the discrimator: Change the free-running behavior as to match the teacher-forced behavior while keeping the latter constant.\n  - Second optional another term: Change the teacher-forced behavior to match the free-running behavior.\n  - Like GAN, backprop from discriminator into generator.\n- Architectures\n  - Generator is a standard GRU Recurrent Neural Network with softmax\n  - Behavior function `B(x, y, theta)` outputs pre-tanh activation of GRU states and tje softmax output\n  - Discriminator: Bidirectional GRU with 3-layer MLP on top\n  - Training trick: To prevent \"bad gradients\" the authors backprop from the discriminator into the generator only if the classification accuracy is between 75% and 99%.\n  - Trained used Adam optimizer\n- Experiments\n  - PTB Chracter-Level Modeling: Reduction in test NLL, profesor forcing seem to act as a regularizier. 1.48 BPC\n  - Sequential MNIST: Second-best NLL (79.58) after PixelCNN\n  - Handwriting generation: Professor forcing is better at generating longer sequences than seen during training as per human eval.\n  - Music Synthesis: Human eval significantly better for Professor forcing\n  - Negative Results on word-level modeling: Professor forcing doesn't have any effect. Perhaps because long-term dependencies are more pronounced in character-level modeling.\n  - The authors show using t-SNE that the hidden state distributions actually become more similar when using professor forcing\n\n### Thoughts\n\n- Props to the authors for a very clear and well-written paper. This is rarer than it should be :)\n- It's an intersting idea to also match the states of the RNN instead of just the outputs. Intuitively, matching the outputs should implicitly match the state distribution. I wonder if the authors tried this and it didn't work as expected.\n- Note from [Ethan Caballero](https://github.com/ethancaballero) about why they chose to match hidden states: It's significantly harder to use GANs on sampled (argmax) output tokens because they are discrete as (as opposed to continuous like the hidden states and their respective softmaxes). They would have had to estimate discrete outputs with policy gradients like in [seqGAN](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/seq-gan.md) which is [harder to get to converge](https://www.quora.com/Do-you-have-any-ideas-on-how-to-get-GANs-to-work-with-text), which is why they probably just stuck with the hidden states which already contain info about the discrete sampled outputs (the index of the highest probability in the the distribution) anyway. Professor Forcing method is unique in that one has access to the continuous probability distribution of each token at each timestep of the two sequence generation modes trying to be pushed closer together. Conversely, when applying GANs to pushing real samples and generated samples closer together as is traditionally done in models like seqGAN, one only has access to the next dicrete token (not continuous probability distributions of next token) at each timestep, which prevents straight-forward differentiation (used in professor forcing) from being applied and forces one to use policy gradient estimation. However, there's a chance one might be able to use straight-forward differentiation to train seqGANs in the traditional sampling case if one swaps out each discrete sampled token with its continuous distributional word embedding (from pretrained word2vec, GloVe, etc.), but no one has tried it yet TTBOMK.\n- I would've liked to see a comparison of  the two regularization terms in the generator. The experiments don't make it clear if both or only of them them is used.\n- I'm guessing that this architecture is quite challenging to train. Woul've liked to see a bit more detail about when/how they trade off the training of discriminator and generator.\n- Translation is another obvious task to apply this too. I'm interested whether or not this works for seq2seq.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1610.09038"
    },
    "816": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.05473",
        "transcript": "TLDR; The authors train an Generative Adversarial Network where the generator is an RNN producing discrete tokens. The discriminator is used to provide a reward for each generated sequence (episode) and to train the generator network via via Policy Gradients. The discriminator network is a CNN in the experiments. The authors evaluate their model on a synthetic language modeling task and 3 real world tasks: Chinese poem generation, speech generation and music generation. Seq-GAN outperforms competing approaches (MLE, Schedule Sampling, PG-BLEU) on the synthetic task and outperforms MLE on the real world task based on a BLEU evaluation metric.\n\n#### Key Points\n\n- Code: https://github.com/LantaoYu/SeqGAN\n- RL Problem setup: State is already generated partial sequence. Action space is the space of possible tokens to output at the current step. Each episode is a fully generated sequence of fixed length T.\n- Exposure Bias in the Maximum Likelihood approach: During decoding the model generates the next token based on a series previously generated tokens that it may never have seen during training leading to compounding errors.\n- A discriminator can provide a reward when no task-specific reward (e.g. BLEU score) is available or when it is expensive to obtain such a reward (e.g. human eval).\n- The reward is provided by the discriminator at the end of each episode, i.e. when the full sequence is generated. To provide feedback at intermediate steps the rest of the sequence is sampled via Monte Carlo search.\n- Generator and discriminator are trained alternatively and strategy is defined by hyperparameters g-steps (# of Steps to train generator), d-steps (number of steps to train discriminator with newly generated data) and k (number of epochs to train discriminator with same set of generated data).\n- Synthetic task: Randomly initialized LSTM as oracle for a language modeling task. 10,000 sequences of length 20.\n- Hyperparameters g-steps, d-steps and k have a huge impact on training stability and final model performance. Bad settings lead to a model that is barely better than the MLE baseline.\n\n#### My notes:\n\n- Great paper overall. I also really like the synethtic task idea, I think it's a neat way to compare models.\n- For the real-world tasks I would've liked to see a comparison to PG-BLEU as they do in the synthetic task. The authors evaluate on BLEU score so I wonder how much difference a direct optimization of the evaluation metric makes.\n- It seems like SeqGAN outperforms MLE significantly only on the poem generation task, not the other tasks. What about the other baselines on the other tasks? What is it about the poem generation that makes SeqGAN perform so well?\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1609.05473"
    },
    "817": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/KimR16a",
        "transcript": "TLDR; The authors train a standard Neural Machine Translation (NMT) model (the teacher model) and distill it by having a smaller student model learn the distribution of the teacher model. They investigate three types of knowledge distillation for sequence models: 1. Word Level Distillation 2. Sequence Level Distillation and 3. Sequence Level Interpolation. Experiments on WMT'14 and IWSLT 2015 show that it is possible to significantly reduce the parameters of the model with only a minor loss in BLEU score. The experiments also demonstrates that the distillation techniques are largely complementary. Interestingly, the perplexity of distilled models is significantly higher than that of the baselines without leading to a loss in BLEU score.\n\n### Key Points\n\n- Knowledge Distillation: Learn a smaller student network from a larger teacher network.\n- Approach 1 - Word Level KD: This is standard Knowledge Distillation applied to sequences where we match the student output distribution of each word to the teacher's using the cross-entropy loss.\n- Approach 2 - Sequence Level KD: We want to mimic the distribution of a full sequence, not just per word. To do that we sample outputs from the teacher using beam search and then train the student on these \"examples\" using Cross Entropy. This is a very sparse approximation of the true objective.\n- Approach 3: Sequence-Level Interpolation: We train the student on a mixture of training data and teacher-generated data. We could use the approximation from #2 here, but that's not ideal because it doubles size of training data and leads to different targets conditioned on the same source. The solution is to use generate a response that has high probability under the teacher model and is similar to the ground truth and then have both mixture terms use it.\n- Greedy Decoding with seq-level fine-tuned model behaves similarly to beam search on original model.\n- Hypothesis: KD allows student to only model the mode of the teacher distribution, not wasting other parameters. Experiments show good evidence of this. Thus, greedy decoding has an easier time finding the true max whereas beam search was necessary to do that previously.\n- Lower perplexity does not lead to better BLEU. Distilled models have significantly higher perplexity (22.7 vs 8.2) but have better BLEU (+4.2).\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.07947"
    },
    "818": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/KalchbrennerESO16",
        "transcript": "TLDR; The authors apply a [WaveNet](https://arxiv.org/abs/1609.03499)-like architecture to the task of Machine Translation. Encoder (\"Source Network\") and Decoder (\"Target Network\") are CNNs that use Dilated Convolutions and they are stacked on top of each other. The Target Network uses [Masked Convolutions](https://arxiv.org/abs/1606.05328) to ensure that it only relies on information from the past. Crucially, the time complexity of the network is `c(|S| + |T|)`, which is cheaper than that of the common seq2seq attention architecture (`|S|*|T|`). Through dilated convolutions the network has constant path lengths between [source input -> target output] and [target inputs -> target output] nodes. This allows for efficient propagation of gradients. The authors evlauate their model on Character-Level Language Modeling and Character-Level Machine Translation (WMT EN-DE) and achieve state-of-the-art on the former and a competitive BLEU score on the latter.\n\n\n### Key Points\n\n- Problems with current approaches\n  - Runtime is not linear in the length of source/target sequence. E.g. seq2seq with attention is `O(|S|*|T|)`.\n  - Some architectures compress the source into a fixed-length \"though-vector\", putting a memorization burden on the model.\n  - RNNs are hard to parallelize\n- ByteNet: Stacked network of encoder/decoder. In this work the authors use CNNs, but the network could be RNNs.\n- ByteNet properties:\n  - Resolution preserving: The representation of the source sequence is linear in the length of the source. Thus, a longer source sentence will have a bigger representation.\n  - Runtime is linear in the length of source and target sequences: `O(|S| + |T|)`\n  - Source network can be run in parallel, it's a CNN.\n  - Distance (number of hops) between nodes in the network is short, allowing for efficient backprop.\n- Architecture Details\n  - Dynamic Unfolding: `representation_t(source)` is fed into time step `t` of the target network. Anything past the source sequence length is zero-padded. This is possible due to the resolution preserving property which ensures that the source representation is the same width as the source input.\n  - Masked 1D Convolutions: The target network uses masked convolutions to prevent it from looking at the future during training.\n  - Dilation: Dilated Convoltuions increase the receptive field exponentially in higher layers. This leads to short connection paths for efficient backprop.\n  - Each layer is wrapped in a residual block, either with ReLUs or multiplicative units (depending on the task).\n  - Sub-Batch Normalization: To preven the target network from conditioning on future tokens (similar to masked convolutions) a new variant of Batch Normalization is used.\n- Recurrent ByteNets, i.e. ByteNets with RNNs instead of CNNs, are possible but are not evaluated.\n- Architecture Comparison: Table 1 is great. It compares various enc-dec architectures across runtime, resolution preserving and path lengths properties.\n- Character Prediction Experiments:\n  - [Hutter Prize Version of Wikipedia](http://prize.hutter1.net/): ~90M characters\n  - Sample a batch of 515 characters predict latter 200 from the first 315\n  - New SOTA: 1.33 NLL (bits/chracter)\n-  Character-Level Machine Translation\n  - [WMT](http://www.statmt.org/wmt16/translation-task.html) EN-DE. Vocab size ~140\n  - Bags of character n-grams as additional embeddings\n  - Examples are bucketed according to length\n  - BLEU: 18.9. Current state of the art is ~22.8 and standard attention enc-dec is 20.6\n\n\n### Thoughts\n\n- Overall I think this a very interesting contribution. The ideas here are pretty much identical to the [WaveNet](https://arxiv.org/abs/1609.03499) + [PixelCNN](https://arxiv.org/abs/1606.05328) papers. This paper doesn't have much detail on any of the techniques, no equations whatsoever. Implementing the ByteNet architecture based on the paper alone would be very challenging. The fact that there's no code release makes this worse.\n- One of the main arguments is the linear runtime of the ByteNet model. I would've liked to see experiments that compare implementations in frameworks like Tensorflow to standard seq2seq implementations. What is the speedup in *practice*, and how does it scale with increased paralllelism? Theory is good and all, but I want to know how fast I can train with this.\n- Through dynamic unfolding  target inputs as time t depend directly on the source representation at time t. This makes sense for language pairs that are well aligned (i.e. English/German), but it may hurt performance for pairs that are not aligned since the the path length would be longer. Standard attention seq2seq on the other hand always has a fixed path length of 1. Experiments on this would've been nice.\n- I wonder how much difference the \"bag of character n-grams\" made in the MT experiments. Is this used by the other baselines?\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1610.10099"
    },
    "819": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/IoffeS15",
        "transcript": "TLDR; The authors introduce Batch Normalization, a technique to normalize unit activations to zero mean and unit variance within the network. The authors show that in Feedforward and Convolutional Networks, Batch Normalization leads to faster training and better accuracies. BN also acts as a regularizer, reducing the need for Dropout, etc. Using an ensemble of batch normalized networks the authors achieve state of the art on ILSVRC.\n\n\n#### Key Points\n\n- Network training is complicated because the input distributions to higher level change as the parameter in lower layers are changing: Internal Covariate Shift. Solution: Normalize within the network.\n- BN: Normalize input to nonlinearity to have zero mean and unit variance. Then add two additional parameters (scaling and bias) per unit to preserve expressability of the network. Statistics are calculated per minibatch.\n- Network parameters increase, but not by much: 2 parameter per unit that has batch normalization applied to it.\n- Works well for fully connected and convolutional layers. Authors didn't try RNNs.\n- Change to make when adding BN: Increase learning rate, remove/decrease dropout and l2 regularization, accelerate learning rate decay, shuffle training examples more thoroughly.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/ioffe15.html"
    },
    "820": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BojanowskiJM15",
        "transcript": "TLDR; The authors propose two different architectures to improve the performance of character-level RNNs. In the first architecture (\"mixed\") the authors condition the model on the state of a word-level RNN. In the second architecture (\"cond\") they condition the output classifier on character n-grams. The authors show that the proposed architecture outperform plain character-level RNNs in terms of entropy in bits per character.\n\n#### Key Points\n\n- Plain character-level RNNs need a huge hidden representation in order to model long-term dependencies. But Word-level RNNs can't generalize to new vocabulary and may require a huge output vocab.\n- Model 1: Jointly train word-level and char-level CNN. Interpolate the losses of the two models.\n- Model 2: Condition softmax on n-grams before character, \"relieving\" the network of memorizing some of the sequence.\n- Training: Constant learning rate, reduce every epoch when validation accuracy decreases\n- N-gram model can be applied to arbitrary data, not just characters. Authors evaluate on binary data.\n\n#### Notes / Questions\n\n- In the comparison table the authors don't show the number of parameters for the models. They compare models with the same number of hidden units, but their proposed architecture need extra parameters and computation. Unfair comparison?\n- People typically use LSTMs/GRUs for language modeling. Of course the proposed techniques can be applied to LSTM/GRU networks, but the experimental result may look very different. Do these architectures result in any benefit when using LSTM/GRU char data?\n- Entropy in bits per character seems like somewhat of a strange evaluation metric. I don't really know what to make of it, and no intuitive explanations are given.\n- One argument the authors make in the paper is that character-level models can be applied to arbitrary input data (different languages, binary data, code, etc). But their mixed is clearly very language-specific. It can't be applied to arbitrary data, and many languages don't have clear word boundaries. Similarly, n-grams may be prohibituvely expensive depending on what kind of data we're working with. \n- The n-gram conditioned models isn't clearly explained, I *think* I understand what it does, but I'm not quite sure. No intuitive explanations what any of the models are learning are given.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.06303"
    },
    "821": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/YaoZP15",
        "transcript": "TLDR; The authors propose an Attention with Intention (AWI) model for Conversation Modeling. AWI consists of three recurrent networks: An encoder that embeds the source sentence from the user, an intention network that models the intention of the conversation over time, and a decoder that generates responses. The authors show that the network can general natural responses.\n\n#### Key Points\n\n- Intuition: Intention changes over the course of a conversation, e.g. communicate problem -> resolve issue -> acknowledge.\n- Encoder RNN: Depends on last state of the decoder. Reads the input sequence and converts it into a fixed-length vector.\n- Intention RNN: Gets encoder representation, previous intention state, and previous decoder state as input and generates new representation of the intention.\n- Decoder RNN: Gets current intention state and attention vector over the encoder as an input. Generates a new output.\n- Architecture is evaluated on an internal helpdesk chat dataset with 10k dialogs, 100k turns and 2M tokens. Perplexity scores and a sample conversation are reported.\n\n#### Notes/Questions\n\n- It's a pretty short paper and not sure what to make of the results. The PPL scores were not compared to alternative implementations and no other evaluations (e.g. crowdsourced as in Neural Conversational Model) are done.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1510.08565"
    },
    "822": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LiaoP16",
        "transcript": "TLDR; The authors argue that the human visual cortex doesn't contain ultra-deep networks like ResNet's (100s or 1000s of layers), but that it does contain recurrent connections. The authors then explore ResNets with weight sharing and show how they are equivalent to unrolled standard RNNs with skip connections. The authors find that ResNets with weight sharing perform almost as well as ResNets without weight sharing, while needing drastically fewer parameters. Thus, they argue that the success of ultra-deep networks may actually stem from the fact that they can approximate recurrent computations. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.03640"
    },
    "823": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LakeUTG16",
        "transcript": "TLDR; The author explore the gap between Deep Learning methods and human learning. The argue that natural intelligence is still the best example of intelligence, so it's worth exploring. To demonstrate their points they explore two challenges: 1. Recognizing new characters and objects 2. Learning to play the game Frostbite. The authors make several arguments:\n\n-  Humans have an intuitive understanding of physics and psychology (understanding goals and agents) very early on. These two types of \"software\" help them to learn new tasks quickly.\n-  Humans build causal models of the world instead of just performing pattern recognition. These models allow humans to learn from far fewer examples than current Deep Learning methods. For example, AlphaGo played a billion games or so, Lee Sedol perhaps 50,000. Incorporating compositionality, learning-to-learn (transfer learning) and causality helps humans to build these models.\n- Humans use both model-free and model-based learning algorithms.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.00289"
    },
    "824": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/aaai/KimJSR16",
        "transcript": "TLDR; The authors build an LSTM Neural Language model, but instead of using word embeddings as inputs, they use the per-word outputs of a character-level CNN, plus a highway layer. This architecture results in state of the art performance and significantly fewer parameters. It also seems to work well on languages with rich morphology.\n\n\n#### Key Points \n\n- Small Model: 15-dimensional char embeddings, filter sizes 1-6, tanh, 1-layer highway with ReLU, 2-layer LSTM with 300-dimensional cells. 5M Parameters. Hiearchical Softmax.\n- Large Model: 15-dimensional char embeddings, filter sizes 1-7, tanh, 2-layer highway with ReLU, 2-layer LSTM with 670-dimensional cells. 19M Parameters. Hiearchical Softmax.\n- Can generalize to out of vocabulary words due to character-level representations. Some datasets already had OOV words replaced with a special token, so the results don't reflect this.\n- Highway Layers are key to performance. Susbtituting HW with MLP does not work well. Intuition is that HW layer adaptively combines different local features for higher-level representation.\n- Nearest neighbors after Highway layer are more smenatic than before highway layer. Suggests compositional nature.\n- Surprisingly combinbing word and char embeddings as LSTM input results in worse performance - Characters alone are sufficient?\n- Can apply same architecture to NML or Classification tasks. Highway Layers at the output may also help these tasks.\n\n\n#### Notes / Questions\n\n- Essentially this is a new way to learn word embeddings comprised of lower-level character embeddings. Given this, what about stacking this architecture and learn sentence representations based on these embeddings?\n- It is not 100% clear to me why the MLP at the output layer does so much worse. I understand that the highway layer can adaptively combine feature, but what if you combined MLP and plain representations and add dropout? Shouldn't that result in similar perfomance?\n- I wonder if the authors experimented with higher-dimensional character embeddings. What is the intuition behind the very low-dimensional (15) embeddings?",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12489"
    },
    "825": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhangZL15",
        "transcript": "TLDR; The authors evaluate the use for 9-layer deep CNNs on large-scale data sets for text classification, operating directly on one-hot encodings of characters. The architecture achieves competitive performance across datasets.\n\n#### Key Points\n\n- 9 Layers, 6 conv/ppol layers, 3 affine layers. 1024-dimensional input features for large model, 256-dimensional input features for small model.\n- Authors optionally use English thesaurus for training data augmentation\n- Fixed input length l: 1014 characters\n- Simple n-gram models performs very well on these data sets and beats other models and the smaller data sets (<= 500k examples). CNN wins on the larger data sets (>1M examples)\n\n#### Notes / Questions\n\n- Comparing the CNN with input restricted to 1014 characters to models that operate on words seems unfair. Also, how long is the average document? Would've liked to see some dataset statistics. The fixed input length doesn't make a lot of sense to me.\n- Contribution of this paper is that the architecture works without word knowledge and for any language, but at the same time the authors use a word-level English thesaurus to improve their performance? To be fair, the thesaurus doesn't seem to make a huge difference.\n- The reason this architecture requires so much data is probably because it's very deep (How many parameters?). Did the authors experiment with fewer layers? Did they perform much worse?\n- What about unsupervised pre-training? Can that reduce the amount of data required to achieve good performance. Currently this model doesn't seem very useful in practice as there are very few datasets of such size out there.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification"
    },
    "826": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GhoshVSRDH16",
        "transcript": "TLDR; The authors propose a Contextual LSTM (CLSTM) model that appends a context vector to input words when making predictions. The authors evaluate the model and Language Modeling, next sentence selection and next topic prediction tasks, beating standard LSTM baselines.\n\n\n#### Key Points\n\n- The topic vector comes from an internal classifier system and is supervised data. Topics can also be estimated using unsupervised techniques.\n- Topic can be calculated either based on the previous words of the current sentence (SentSegTopic), all words of the previous sentence (PrevSegTopic), and current paragraph (ParaSegTopic). Best CLSTM uses all of them. \n- English Wikipedia Dataset: 1400M words train, 177M validation, 178M words test. 129k vocab.\n- When current segment topic is present, the topic of the previous sentence doesn't matter.\n- Authors couldn't compare to other models that incorporate topics because they don't scale to large-scale datasets.\n- LSTMs are a long chain and authors don't reset the hidden state between sentence boundaries. So, a sentence has implicit access to the prev. sentence information, but explicitly modeling the topic still makes a difference.\n\n#### Notes/Thoughts\n\n- Increasing number of hidden units seems to have a *much* larger impact on performance than increasing model perplexity. The simple word-based LSTM model with more hidden units significantly outperforms the complex CLSTM model. This makes me question the practical usefulness of this model.\n- IMO the comparisons are somewhat unfair because by using an external classifier to obtain topic labels you are bringing in external data that the baseline models didn't have access to.\n- What about using other unsupervised sentence embeddings as context vectors, e.g. seq2seq autoencoders or PV?\n- If the LSTM was perfect in modeling long-range dependencies then we wouldn't need to feed extra topic vectors. What about residual connections?",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.06291"
    },
    "827": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/PiechBHGSGS15",
        "transcript": "TLDR; The authors apply an RNN to modeling the students knowledge. The input is an exercise question and answer (correct/incorrect), either as one-hot vectors or embedded. The network then predicts whether or not the student can answer a future question correctly. The authors show that the RNN approach results in significant improvement over previous models, can be used for curriculum optimization, and also discovers the latent structure in exercise concepts.\n\n#### Key Points\n\n- Two encodings tried: One hot, embedded\n- RNN/LSTM, 200-dimensional hidden layer, output dropout, NLL. \n- No expert annotation for concepts or question/answers are needed\n- Blocking (series of exercises of same type) vs Mixing for curriculum optimization: Blocking seems to perform better\n- Lots of cool future direction ideas\n\n#### Question / Notes\n\n- Can we not only predict whether an exercise is answered correctly, but also what the most likely student answer would be? My give insight into confusing concepts.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5654-deep-knowledge-tracing"
    },
    "828": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeZRS15",
        "transcript": "\nTLDR; The authors present Residual Nets, which achieve 3.57% error on the ImageNet test set and won the 1st place on the ILSVRC 2015 challenge. ResNets work by introducing \"shortcut\" connections across stacks of layers, allowing the optimizer to learn an easier residual function instead of the original mapping. This allows for efficient training of very deep nets without the introduction of additional parameters or training complexity. The authors present results on ImageNet and CIFAR-100 with nets as deep as 152 layers (and one ~1000 layer deep net).\n\n\n#### Key Points\n\n- Problem: Deeper networks experience a *degradation* problem. They don't overfit but nonetheless perform worse than shallower networks on both training and test data due to being more difficult to optimize. \n- Because Deep Nets can in theory learn an identity mapping for their additional layers they should strict outperform shallower nets. In practice however, optimizers have problems learning identity (or near-identity) mappings. Learning residual mappings is easier, mitigating this problem.\n- Residual Mapping: If the desired mapping is H(x), let the layers learn F(x) = H(x) - x and add x back through a shortcut connection H(x) = F(x) + x. An identity mapping can then be learned easily by driving the learned mapping F(x) to 0.\n- No additional parameters or computational complexity are introduced by residuals nets.\n- Similar to Highway Networks, but gates are not data-dependent (no extra parameters) and are always open.\n- Due the the nature of the residual formula, input and output must be of same size (just like Highway Networks). We can do size transformation by zero-padding or projections. Projections introduce additional parameters. Authors found that projections perform slightly better, but are \"not worth\" the large number of extra parameters.\n- 18 and 34-layer VGG-like plain net gets 27.94 and 28.54 error respectively, not that higher error for deeper net. ResNet gets 27.88 and 25.03 respectively. Error greatly reduces for deeper net.\n- Use Bottleneck architecture with 1x1 convolutions to change dimensions.\n- Single ResNet outperforms previous start of the art ensembles. ResNet ensemble even better.\n\n\n#### Notes/Questions\n\n- Love the simplicity of this.\n- I wonder how performance depends on the number of layers skipped by the shortcut connections. The authors only present results with 2 or 3 layers.\n- \"Stacked\" or recursive residuals?\n- In principle Highway Networks should be able to learn the same mappings quite easily. Is this an optimization problem? Do we just not have enough data. What if we made the gates less fine-grained and substituted sigmoid with something else?\n- Can we apply this to RNNs, similar to LSTM/GRU? Seems good for learning long-range dependencies.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.03385"
    },
    "829": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HintonVD15",
        "transcript": "TLDR; The authors show that we can distill the knowledge of a complex ensemble of models into a smaller model by letting the smaller model learn directly from the \"soft targets\" (softmax output with high temperature) of the ensemble. Intuitively, this works because the errors in probability assignment (e.g. assigning 0.1% to the wrong class) carry a lot of information about what the network learns. Learning directly from logits (unnormalized scores) as was done in a previous paper, is a special case of the distillation approach. The authors show how distillation works on the MNIST and an ASR data set.\n\n\n#### Key Points\n\n- Can use unlabeled data to transfer knowledge, but using the same training data seems to work well in practice.\n- Use softmax with temperature, values from 1-10 seem to work well, depending on the problem.\n- The MNIST networks learn to recognize digits without ever having seen base, solely based on the \"errors\" that the teacher network makes. (Bias needs to be adjusted)\n- Training on soft targets with less data performs much better than training on hard targets with same amount of data.\n\n\n#### Notes/Question\n\n- Breaking up the complex models into specialists didn't really fit into this paper without distilling those experts into one model. Also would've liked to see training of only specialists (without general network) and then distill their knowledge.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1503.02531"
    },
    "830": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/LeM14",
        "transcript": "TLDR; The authors present Paragraph Vector, which learns fixed-length, semantically meaningful vector representations for text of any length (sentences, paragraphs, documents, etc). The algorithm works by training a word vector model with an additional paragraph embedding vector as an input. This paragraph embedding is fixed for each paragraph, but varies across paragraphs. Similar to word2vec, PV comes in 2 flavors:\n\n- A Distributed Memory Model (PV-DM) that predicts the next word based on the paragraph and preceding words\n- A BoW model (PW-BoW) that predicts context words for a given paragraph\n\nA notable property of PV is that during inference (when you see a new paragraph) it requires training of a new vector, which can be slow. The learned embeddings can used as the input to other models. In their experiments the authors train both variants and concatenate the results. The authors evaluate PV on Classification and Information Retrieval Tasks and achieve new state-of-the-art.\n\n\n#### Data Sets / Results\n\nStanford Sentiment Treebank Polar error: 12.2%\nStanford Sentiment Treebank Fine-Grained error: 51.3%\nIMDB Polar error: 7.42%\nQuery-based search result retrieval (internal) error: 3.82%\n\n\n#### Key Points\n\n- Authors use 400-dimensional PV and word embeddings. The window size is a hyperparameter chosen on the validation set, values from 5-12 seem to work well. In IMDB, window size resulted in error fluctuation of ~0.7%.\n- PV-DM performs well on its own, but concatenating PV-DM and PV-BoW consistently leads to (small) improvements.\n- When training the PV-DM model, use concatenation instead of averaging to combine words and paragraph vectors (this preserves ordering information)\n- Hierarchical Softmax is used to deal with large vocabularies.\n- For final classification, authors use LR or MLP, depending on the task (see below)\n- IMDB Training (25k documents, 230 average length) takes 30min on 16 core machine, CPU I assume.\n\n\n#### Notes / Question\n\n- How did the authors choose the final classification model? Did they cross-validate this? The authors mention that NN performs better than LR for the IMDB data, but they don't show how large the gap is. Does PV maybe perform significantly worse with a simpler model?\n- I wonder if we can train hierarchical representations of words, sentences, paragraphs, documents, keep the vectors of each one fixed at each layer, and predicting sequences using RNNs.\n- I wonder how PV compares to an attention-based RNN autoencoder approach. When training PV you are in a way attending to specific parts of the paragraph to predict the missing parts.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v32/le14.html"
    },
    "831": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LiGBGD15",
        "transcript": "TLDR; The authors use a Maximum Mutual Information (MMI) objective function to generate conversational responses. They still train their models with maximum likelihood, but use MMI to generate responses during decoding. The idea behind MMI is that it promotes more diversity and penalizes trivial responses. The authors evaluate their method using BLEU scores, human evaluators, and qualitative analysis and find that the proposed metric indeed leads to more diverse responses.\n\n#### Key Points\n\n- In practice, NCM (Neural Conversation Models) often generate trivial responses using high-frequency terms partly due to the likelihood objective function.\n- Two models: MMI-antiLM and MMI-bidi depending on the formulation of the MMI objective. These objectives are used during response generation, not during training.\n- Use Deep 4-layer LSTM with 1000-dimensional hidden state, 1000-dimensional word embeddings.\n- Datasets: Twitter triples with 129M context-message-response triples. OpenSubtitles with 70M spoken lines that are noisy and don't include turn information.\n- Authors state that perplexity is not a good metric because their objective is to explicitly steer away from the high probability responses.\n\n\n#### Notes\n\n- BLEU score seems like a bad metric for this. Shouldn't more diverse responses result in a lower BLEU score?\n- Not sure if I like the direction of this. To me it seems wrong to \"artificially\" promote diversity. Shouldn't diversity come naturally as a function of context and intention?",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1510.03055"
    },
    "832": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/DaiOL15",
        "transcript": "TLDR; The authors evaluate Paragraph Vectors on large Wikipedia and arXiv document retrieval tasks and compare the results to LDA, BoW and word vector averaging models. Paragraph Vectors either outperform or match the performance of other models. The authors show how the embedding dimensionality affects the results. Furthermore, the authors find that one can perform arithemetic operations on paragraph vectors and obtain meaningful results and present qualitative analyses in the form of visualizations and document examples.\n\n\n#### Data Sets\n\nAccuracy is evaluated by constructing triples, where a pair of items are close to each other and the third one is unrelated (or less related). Cosine similarity is used to evaluate semantic closeness.\n\nWikipedia (hand-built) PV: 93%\nWikipedia (hand-built) LDA: 82%\nWikipedia (distantly supervised) PV: 78.8%\nWikipedia (distantly supervised) LDA: 67.7%\narXiv PV: 85%\narXiv LDA: 85%\n\n\n#### Key Points\n\n- Jointly training PV and word vectors seems to improve performance.\n- Used Hierarchical Softmax as Huffman tree for large vocabulary\n- The use only the PV-BoW model, because it's more efficient.\n\n#### Questions/Notes\n\n- Why the performance discrepancy between the arXiv and Wikipedia tasks? BoW performs surprisingly well on Wikipedia, but not arXiv. LDA is the opposite. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1507.07998"
    },
    "833": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/aaai/SerbanSBCP16",
        "transcript": "TLDR; The authors train a Hierarchical Recurrent Encoder-Decoder (HRED) network for dialog generation. The \"lower\" level encodes a sequence of words into a though vector, and the higher-level encoder uses these thought vectors to build a representation of the context. The authors evaluate their model on the *MoviesTriples* dataset using perplexity measures and achieve results better than plain RNNs and the DCGM model. Pre-training with a large Question-Answer corpus significantly reduces perplexity.\n\n#### Key Points\n\n- Three RNNs: Utterance encoder, context encoder, and decoder. GRU hidden units, ~300d hidden state spaces.\n- 10k vocabulary. Preprocessing: Remove entities and numbers using NLTK\n- The context in the experiments is only a single utterance\n- MovieTriples is a small dataset, about 200k training triples. Pretraining corpus has 5M Q-A pairs, 90M tokens.\n- Perplexity is used as an evaluation metric. Not perfect, but reasonable.\n- Pre-training has a much more significant impact than the choice of the model architecture. It reduces perplexity ~10 points, while model architecture makes a tiny difference (~1 point).\n- Authors suggest exploring architectures that separate semantic from syntactic structure\n- Realization: Most good predictions are generic. Evaluation metrics like BLEU will favor pronouns and punctuation marks that dominate during training and are therefore bad metrics.\n\n\n#### Notes/Questions\n\n- Does using a larger dataset eliminate the need for pre-training?\n- What about the more challenging task for longer contexts?",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11957"
    },
    "834": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/XiaoC16",
        "transcript": "TLDR; The authors use a CNN to extract features from character-based document representations. These features are then fed into a RNN to make a final prediction. This model, called ConvRec, has significantly fewer parameters (10-50x) then comparable convolutional models with more layers, but achieves similar to better performance on large-scale document classification tasks.\n\n#### Key Points\n\n- Shortcomings of word-level approach: Each word is distinct despite common roots, cannot handle OOV words, many parameters.\n- Character-level Convnets need many layers to capture long-term dependencies due to the small sizes of the receptive fields.\n- Network architecture: 1. Embedding 8-dim 2. Convnet: 2-5 layers, 5 and 3-dim convolutions, 2-dim pooling, ReLU activation, 3. RNN LSTM with 128d hidden state. Dropout after conv and recurrent layer.\n- Training: 96 characters, Adadelta, batch size of 128, Examples are padded and masked to longest sequence in batch, gradient norm clipping of 5, early stopping\n- Models tends to outperform large CNN for smaller datasets. Maybe because of overfitting?\n- More convolutional layers or more filters doesn't impact model performance much\n\n#### Notes/Questions\n\n- Would've been nice to graph the effect of #params on the model performance. How much do additional filters and conv layers help?\n- hat about training time? How does it compare? ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.00367"
    },
    "835": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SukhbaatarSWF15",
        "transcript": "TLDR; The authors propose a recurrent memory-based model that can reason over multiple hops and be trained end to end with standard gradient descent. The authors evaluate the model on QA and Language Modeling Tasks. In the case of QA, the network inputs are a list of sentences, a query and (during training) an answer. The network then attends to the sentences at each time step, considering the next piece information relevant to the question. The network outperforms baseline approaches, but does not come close to a strongly supervised (relevant sentences are pre-selected) approach.\n\n\n#### Key Takeaways\n\n- Sentence Representation: 1. Word embeddings are averaged (BoW) 2. Positional Encoding (PE)\n- Synthetic dataset with vocabulary size of ~180. Version one has 1k training example, version 2 has 10k training examples.\n- The model is similar to Bahdanau seq2seq attention model, only that it operates on sentences and does not output at every step and used a simpler scoring function.\n\n\n#### Questions / Notes\n\n- The positional encoding formula is not explained neither is it intutiive.\n- There are so many hyperparameters and model variations (jittering, linear start) that it's easy to lose track of the essential.\n- No intuitive explanation of what the model does. The easiest way for me to understand this model was to look at it as a variation of Bahdanau's attention model, which is very intuitive. I don't understand the intuition behind the proposed weight constraints.\n- The LM results are not convincing. The model beats the baselines by a little bit, but probably only due to very time-intensive hyperparameter optimization.\n- What is the training complexity and training time?\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5846-end-to-end-memory-networks"
    },
    "836": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/JozefowiczVSSW16",
        "transcript": "TLDR; The authors train large-scale language modeling LSTMs on the 1B word dataset to achieve new state of the art results for single models (51.3 -> 30 Perplexity) and ensemble models (41 -> 24.2 Perplexity). The authors evaluate how various architecture choices impact the model performance: Importance Sampling Loss, NCE Loss, Character-Level CNN inputs, Dropout, character-level CNN output, character-level LSTM Output.\n\n#### Key Points\n\n- 800k vocab, 1B words training data\n- Using a CNN on characters instead of a traditional softmax significantly reduces number of parameters, but lacks the ability to differentiate between similar-looking words with very different meanings. Solution: Add correction factor\n- Dropout on non-recurrent connections significantly improves results\n- Character-level LSTM for prediction performs significantly worse than softmax or CNN softmax\n- Sentences are not pre-processed, fed in 128-sized batches without resetting any LSTM state in between examples. Max word length for character-level input: 50\n- Training: Adagrad and learning rate of 0.2. Gradient norm clipping 1.0. RNN unrolled for 20 steps. Small LSTM beats state of the art after just 2 hours training, largest and best model trained for 3 weeks on 32 K40 GPUs.\n- NC vs. Importance Sampling: IC is sufficient\n- Using character-level CNN word embeddings instead of a traditional matrix is sufficient and performs better\n\n#### Notes/Questions\n\n- Exact hyperparameters in table 1 are not clear to me.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.02410"
    },
    "837": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/VinyalsKKPSH14",
        "transcript": "TLDR; Authors apply 3-layer seq2seq LSTM with 256 units and attention mechanism to consituency parsing task and achieve new state of the art. Attention made a huge difference for a small dataset (40k examples), but less so for a noisy large dataset (~11M examples).\n\n#### Data Sets and model performance\n\n- WSJ (40k examples): 90.5\n- Large distantly supervised corpus (90k gold examples, 11M noisy examples): 92.8\n\n#### Key Takeaways\n\n- The authors use existing parsers to label a large dataset to be used for training. The trained model then outperforms the \"teacher\" parsers. A possible explanation is that errors of supervising parsers look like noise to the more powerful LSTM model. These results are extremely valuable, as data is typically the limiting factor, but existing models almost always exist.\n- Attention mechanism can lead to huge improvements on small data sets.\n- All of the learned LSTM models were able to deal with long (~70 tokens) sentences without a significant impact of performance.\n- Reversing the input in seq2seq tasks is common. However, reversing resulted in only a 0.2 point bump in accuracy.\n- Pre-trained word vectors bumped scores by 0.4 (92.9 -> 94.3) only.\n\n#### Notes/Questions\n\n- How much does the ouput data representation matter? The authors linearized the parse tree using depth-first traversal and parentheses. Are there more efficient representations that may lead to better results?\n- How much does the noise in the auto-labeled training data matter when compared to the data size? Are there systematic errors in the auto-labeled data that put a ceiling on model performance?\n- Bidirectional LSTM?",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1412.7449"
    },
    "838": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/WangC15g",
        "transcript": "TLDR; The authors propose new ways to incorporate context (previous sentences) into a Recurrent Language Model (RLM). They propose 3 ways to model the context, and 2 ways to incorporate the context into the predictions for the current sentence. Context can be modeled with BoW, Sequence BoW (BoW for each sentence), and Sequence BoW with attention. Context can be incorporated using \"early fusion\", which gives the context as an input to the RNN, or \"late fusion\", which modifies the LSTM to directly incorporate the context. The authors evaluate their architecture on IMDB, BBC and Penn TreeBank corpora, and show that most approaches perform well (reducing perplexity), with Sequence BoW with attention + late fusion outperforming all others.\n\n#### Key Points:\n\n- Context as BoW: Compress N previous sentences into a single BoW vector\n- Context as Sequential Bow: Compress each of the N previous sentences into a BoW vector and use an LSTM to \"embed\" them. Alternatively, use an attention mechanism.\n- Early Fusion: Give the context vector as an input to the LSTM, together with the current word.\n- Late Fusion: Add another gate to the LSTM that incorporates the context vector. Helps to combat vanishing gradients.\n- Interestingly the Sequence BoW without attention performs very poorly. The reason here seems to be the same as for seq2seq, it's hard to compress the sentence vectors into a single fixed-length representation using an LSTM.\n- LSTM models trained with 1000 units, Adadelta. Only sentences up to 50 words are considered.\n- Noun phrases seem to benefit the most from the context, which makes intuitive sense.\n\n\n#### Notes/Questions:\n\n- A problem with current Language Models is that they are corpus-specific. A model trained on one corpus doesn't do well on another corpus because all sentences are treated as being independent. However, if we can correctly incorporate context we may be able to train a general-purpose LM that does well across various corpora. So I think this is important work.\n- I am surprised that the authors did not try using a sentence embedding (skip-thought, paragraph-vector) to construct their context vectors. That seems like an obvious choice over using BoW.\n- The argument for why the Sequence BoW without attention model performs poorly isn't convincing. In the seq2seq work the argument for attention was based on the length of the sequence. However, here the sequence is very short, so the LSTM should be able to capture all the dependencies. The performance may be poor due to the BoW representation, or due too little training data.\n- Would've been nice to visualize what the attention mechanism is modeling.\n- I'm not sure if I agree with the authors that relying explicit sentence boundaries is an advantage, I see it as a limiting factor.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.03729"
    },
    "839": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LingGHKSWB16",
        "transcript": "TLDR; The authors demonstrate how to condition on several predictors when generating text/code. For example, one may need to copy inputs or perform database lookups to produce good results, but training multiple predictors end-to-end is challenging. The authors propose Latent Predictor Networks that combine attention-based character generation with pointer networks to copy tokens from the input. The authors evaluate their model on the task of producing code for Trading Card Games like Magic and Hearthstone, where the card image is the input, and the code implementation of a card is the output. Latent Predictor Networks clearly beat seq2seq and attention-based baselines.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.06744"
    },
    "840": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LiLDZZ15",
        "transcript": "TLDR; The authors present DV-ngram, a new method to learn document embeddings. DV-ngrams is a variation on Paragraph Vectors with a training objective of predicting words and n-grams solely based on the document vector, forcing the embedding to capture the semantics of the text. The authors evaluate their model on the IMDB data sets, beating both n-gram based and Deep Learning models.\n\n#### Key Points\n\n- When the word vectors are already sufficiently predictive of the next words, the standard PV embedding cannot learn anything useful.\n- Training objective: Predict words and n-grams solely based on document vector. Negative Sampling to deal with large vocabulary. In practice, each n-gram is treated as a special token and appended to the document.\n- Code will be at https://github.com/libofang/DV-ngram\n\n\n#### Question/Notes\n\n- The argument that PV may not work when the word vectors themselves are predictive enough makes intuitive sense. But what about applying word-level dropout? Wouldn't that also force the PV to learn the document semantics?\n- It seems to be that predicting n-grams leads to a huge sparse vocabulary space. I wonder how this method scales, even with negative sampling. I am actually surprised this works well at all.\n- The authors mention that they beat \"other Deep Learning models, including PV, but neither their model nor PV are \"deep learning\". The networks are not deep ;)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.08183"
    },
    "841": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/ChoMGBBSB14",
        "transcript": "\nTLDR; The authors propose a novel encoder-decoder neural network architecture. The encoder RNN encodes a sequence into a fixed length vector representation and the decoder generates a new variable-length sequence based on this representation. The authors also introduce a new cell type (now called GRU) to be used with this network architecture. The model is evaluated on a statistical machine translation task where it is fed as an additional feature to a log-linear model. It leads to improved BLEU scores. The authors also find that the model learns syntactically and semantically meaningful representations of both words and phrases.\n\n#### Key Points:\n\n- New encoder-decoder architecture, seq2seq. Decoder conditioned on thought vector.\n- Architecture can be used for both scoring or generation\n- New hidden unit type, now called GRU. Simplified LSTM.\n- Could replace whole pipeline with this architecture, but this paper doesn't\n- 15k vocabulary (93% of dataset cover). 100d embeddings, 500 maxout units in final affine layer, batch size of 64, adagrad, 384M words, 3 days training time.\n- Architecture is trained without frequency information so we expect it to capture linguistic information rather than statistical information.\n- Visualizations of both words embeddings and thought vectors.\n\n\n#### Questions/Notes\n\n- Why not just use LSTM units?",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D14/D14-1179.pdf"
    },
    "842": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZarembaS14",
        "transcript": "TLDR; The authors show that seq2seq LSTM networks (2 layers, 400-dims) can learn to evaluate short Python programs (loops, conditionals, addition, subtraction, multiplication). The program code is fed one character at a time, and the LSTM is tasked with generating an output number (12 character vocab). The authors also present a new curriculum learning strategy, where the network is fed with a sensible mixture of easy and increasingly difficult examples, allowing it to gradually build up the concepts required to evaluate these programs.\n\n#### Key Points\n\n- LSTM unrolled for 50 steps, 2 layer, 400 cells per layer, ~2.5M parameters. Gradient norm constrained to 5.\n- 3 Curriculum Learning strategies: 1. Naive (increase example difficulty) 2. Mixed: Randomly sample easy and hard problems, 3. Combined: Sample from Naive and Mixed strategy. Mixed or Combined almost always performs better.\n- Output Vocabulary: 10 digits, minus, dot\n- For evaluation teacher forcing is used: Feed correct output when generating target sequence\n- Evaluation Tasks: Program Evaluation, Addition, Memorization\n- Tricks: Reverse Input sequence, Double input sequence. Seem to make big difference.\n- Nesting loops makes the tasks difficult since LSTMs can't deal with compositionality.\n- Feeding easy examples and before hard examples may require the LSTM to restructure its memory.\n\n#### Notes / Questions\n\n- I wonder if there's a relation between regularization/dropout and curriculum learning. The authors propose that mixing example difficulty forces a more general representation. Shouldn't dropout be doing a similar thing?",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1410.4615"
    },
    "843": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/FiratCB16",
        "transcript": "TLDR; The authors train a *single* Neural Machine Translation model that can translate between N*M language pairs, with a parameter spaces that grows linearly with the number of languages. The model uses a single attention mechanism shared across encoders/decoders. The authors demonstrate the the model performs particularly well for resource-constrained languages, outperforming single-pair models trained on the same data.\n\n#### Key Points\n\n- Attention mechanism: Both encoder and decoder output attention-specific vectors, which are then combined. Thus, adding a new source/target language does not result in a quadratic explosion of parameters.\n- Bidirectional RNN, 620-dimensional embeddings, GRU with 1k units, 1k affine layer tanh. Adam, minibatch 60 examples. Only use sentence up to length 50.\n- Model clearly outperforms single-pair models when parallel corpora are constrained to small size. Not so much for large corpora.\n- The single model doesn't fit on a GPU.\n- Can in theory be used to translate between pairs that didn't have a bilingual training corpus, but the authors don't evaluate this in the paper. \n- Main difference to \"Multi-task Sequence to Sequence Learning\": Uses attention mechanism\n\n\n#### Notes / Questions\n\n- I don't see anything that would force the encoders to map sequences of different languages into the same representation (as the authors briefly mentioned). Perhaps it just encodes language-specific information that the decoders can use to decide which source language it was? ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1601.01073"
    },
    "844": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GillickBVS15",
        "transcript": "TLDR; The authors train a deep seq-2-seq LSTM directly on byte-level input of several langauges (shuffling the examples of all languages) and apply it to NER and POS tasks, achieving state-of-the-art or close to that. The model outputs spans of the form `[START_POSITION, LENGTH, LABEL]`, where each span element is a separate token prediction. A single model works well for all languages and learns shared high-level representations. The authors also present a novel way to dropout input tokens (bytes in their case), by randomly replacing them with a `DROP` symbol.\n\n#### Data and model performance\n\nData:\n\n- POS Tagging: 13 languages, 2.87M tokens, 25.3M training segments\n- NER: 4 languags, 0.88M tokens, 6M training segments\n\nResults:\n\n- POS CRF Accuracy (average across languages): 95.41\n- POS BTS Accuracy (average across languages): 95.85\n- NER BTS en/de/es/nl F1: 86.50/76.22/82.95/82.84\n- (See paper for NER comparsion models)\n\n#### Key Takeaways\n\n- Surprising to me that the span generations works so well without imposing independence assumptions on it. It's state the LSTM has to keep in memory.\n- 0.2-0.3 Dropout, 320-dimensional embeddings, 320 units LSTM, 4 layers seems to perform well. The resulting model is surprisingly compact (~1M parameters) due to the small vocabulary size of 256 bytes. Changing input sequence order didn't have much of an effect. Dropout and Byte Dropout significantly (74 -> 78 -> 82) improved F1 for NER.\n- To limit sequence length the authors split the text into k=60 sized segment, with 50% overlap to avoid splitting mid-span.\n- Byte Dropout can be seen as \"blurring text\". I believe I've seen the same technique applied to words before and labeled word dropout. \n- Training examples for all languages are shuffled together. The biggest improvements in scores are seen observed for low-resource languages.\n- Not clear how to tune recall of the model since non-spans are simply not annotated.\n\n#### Notes / Questions\n\n- I wonder if the fixed-vector embedding of the input sequence is a bottleneck since the decoder LSTM has to carry information not only about the input sequence, but also about the structure that has been produced so far. I wonder if the authors have experimented with varying `k`, or using attention mechanisms to deal with long sequences (I've seen papers dealing with sequences of 2000 tokens?). 60 seems quite short to me. Of course, output vocabulary size is also a concern with longer sequences.\n- What about LSTM initialization? When feeding spans coming from the same document, is the state kept around or re-initialized? I strongly suspect it's kept since 60 bytes probably don't contain enough information for proper labeling, but didn't see an explicit reference.\n- Why not a bidirectional LSTM? Seems to be the standard in most other papers.\n- How exactly are multiple languages encoded in the LSTM memories? I *kind of* understand the reasoning behind this, but it's unclear what these \"high-level\" representations are. Experiments that demonstrate what the LSTM cells represent would be valuable.\n- Is there a way to easily re-train the model for a new language?\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.00103"
    },
    "845": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LuongLSVK15",
        "transcript": "TLDR; The authors show that we can improve the performance of a reference task (like translation) by simultaneously training other tasks, like image caption generation or parsing, and vice versa. The authors evaluate 3 MLT (Multi-Task Learning) scenarios: One-to-many, many-to-one and many-to-many. The authors also find that using skip-thought unsupervised training works well for improving translation performance, but sequence autoencoders don't.\n\n#### Key Points\n\n- 4-Layer seq2seq LSTM, 1000-dimensional cells each layer and embedding, batch size 128, dropout 0.2, SGD wit LR 0.7 and decay.\n- The authors define a mixing ratio for parameter updates that is defined with respect to a reference tasks. Picking the right mixing ratio is a hyperparameter.\n- One-To-Many experiments: Translation (EN -> GER) + Parsing (EN). Improves result for both tasks. Surprising that even a very small amount of parsing updates significantly improves MT result.\n- Many-to-One experiments: Captioning + Translation (GER -> EN). Improves result for both tasks (wrt. to reference task)\n- Many-to-Many experiments: Translation (EN <-> GER) + Autoencoders or Skip-Thought. Skip-Thought vectors improve the result, but autoencoders make it worse.\n- No attention mechanism\n\n\n#### Questions / Notes\n\n- I think this is very promising work. it may allow us to build general-purpose systems for many tasks, even those that are not strictly seq2seq. We can easily substitute classification.\n- How do the authors pick the mixing ratios for the parameter updates, and how sensitive are the results to these ratios? It's a new hyperparameter and I would've liked to see graphs for these. Makes me wonder if they picked \"just the right\" ratio to make their results look good, or if these architectures are robust.\n- The authors found that seq2seq autoencoders don't improve translation, but skip-thought does. In fact, autoencoders made translation performance significantly worse. That's very surprising to me. Is there any intuition behind that? ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.06114"
    },
    "846": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/RushCW15",
        "transcript": "TLDR; The authors apply a neural seq2seq model to sentence summarization. The model uses an attention mechanism (soft alignment).\n\n\n#### Key Points\n\n- Summaries generated on the sentence level, not paragraph level\n- Summaries have fixed length output\n- Beam search decoder\n- Extractive tuning for scoring function to encourage the model to take words from the input sequence\n- Training data: Headline + first sentence pair.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1509.00685"
    },
    "847": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/VinyalsL15",
        "transcript": "TLDR; The authors train a seq2seq model on conversations, building a chat bot. The first data set is an IT Helpdesk dataset with 33M tokens. The trained model can help solve simple IT problems. The second data set is the OpenSubtitles data with ~1.3B tokens (62M sentences). The resulting model learns simple world knowledge, can generalize to new questions, but lacks a coherent personality.\n\n#### Key Points\n\n- IT Helpdesk: 1-layer LSTM, 1024-dimensional cells, 20k vocabulary. Perplexity of 8.\n- OpenSubtitles: 2-layer LSTM, 4096-dimensional cells, 100k vocabulary, 2048 affine layer. Attention did not help.\n- OpenSubtitles: Treat two consecutive sentences as coming from different speakers. Noisy dataset.\n- Model lacks personality, gives different answers to similar questions (What do you do? What's your job?)\n- Feed previous context (whole conversation) into encoder, for IT data only.\n- In both data sets, the neural models achieve better perplexity than n-gram models.\n\n#### Notes / Questions\n\n- Authors mention that Attention didn't help in OpenSubtitles. It seems like the encoder/decoder context is very short (just two sentences, not a whole conversation). So perhaps attention doesn't help much here, as it's meant for long-range dependencies (or dealing with little data?)\n- Can we somehow encode conversation context in a separate vector, similar to paragraph vectors?\n- It seems like we need a principled way to deal with long sequences and context. It doesn't really make sense to treat each sentence tuple in OpenSubtitles as a separate conversation. Distant Supervision based on subtitles timestamps could also be interesting, or combine with multimodal learning.\n- How we can learn a \"personality vector\"? Do we need world knowledge or is it learnable from examples?",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.05869"
    },
    "848": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/ShangLL15",
        "transcript": "TLDR; The author train a three variants of a seq2seq model to generate a response to social media posts taken from Weibo. The first variant, NRM-glo is the standard model without attention mechanism using the last state as the decoder input. The second variant, NRM-loc, uses an attention mechanism. The third variant, NRM-hyb combines both by concatenating local and global state vectors. The authors use human users to evaluate their responses and compare them to retrievel-based and SMT-based systems. The authors find that SRM models generate reasonable responses ~75% of the time.\n\n#### Key Points\n\n- STC: Short-text conversation. Generate only a response to a post. Don't need to keep track of a whole conversation.\n- Training data: 200k posts, 4M responses.\n- Authors use GRU with 1000 hidden units. \n- Vocabulary: Most frequent 40k words for both input and response.\n- Retrieval is done using beam search with beam size 10.\n- Hybrid model is difficult to train jointly. The authors train the model individually and then fine-tune the hybrid model.\n- Tradeoff with retrieval based methods: Responses are written by a human and don't have grammatical errors, but cannot easily generalize to unseen inputs.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P15/P15-1152.pdf"
    },
    "849": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GravesWD14",
        "transcript": "TLDR; The authors propose Neural Turing Machines (NTMs). A NTM consists of a memory bank and a controller network. The controller network (LSTM or MLP in this paper) controls read/write heads by focusing their attention softly, using a distribution over all memory addresses. It can learn the parameters for two addressing mechanisms: Content-based addressing (\"find similar items\") and location-based addressing. NTMs can be trained end-to-end using gradient descent. The authors evaluate NTMs on program generations tasks and compare their performance against that of LSTMs. Tasks include copying, recall, prediction, and sorting binary vectors. While both LSTMs and NTMs seems to perform well on training data, only NTMs are able to generalize to longer sequences.\n\n\n#### Key Observations\n\n- Controller network tried with LSTM or MLP. Which one works better is task-dependent, but LSTM \"cache\" can be a bottleneck.\n- Controller size, number  of read/write heads, and memory size are hyperparameters. \n- Monitoring the memory addressing shows that the NTM actually learns meaningful programs.\n- Number LSTM parameters grow quadratically with hidden unit size due to recurrent connection, not so for NTMs, leading to models with fewer parameters.\n- Example problems are very small, typically using sequences 8 bit vectors.\n\n\n#### Notes/Questions\n\n- At what length to NTMs stop to work? Would've liked to see where results get significantly worse.\n- Can we automatically transform fuzzy NTM programs into deterministic ones?",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1410.5401"
    },
    "850": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BahdanauCB14",
        "transcript": "TLDR; The authors propose a novel \"attention\" mechanism that they evaluate on a Machine Translation task, achieving new state of the art (and large improvements in dealing with long sentences). Standard seq2seq models typically try to encode the input sequence into a fixed length vector (the last hidden state) based on which the decoder generates the output sequence. However, it is unreasonable to assume the all necessary information can be encoded in this one vector. Thus, the authors let the decoder depend on a attention vector, which based on the weighted sum (expectation) of the input hidden states. The attention weights are learned jointly, as part of the network architecture.\n\n\n#### Data Sets and model performance\n\nBidirectional GRU, 1000 hidden units. Multilayer maxout to compute output probabilities in decoder.\n\nWMT '14 BLEU: 36.15\n\n\n#### Key Takeaways\n\n- Attention mechanism is a weighted sum of the hidden states computed by the encoder. The weights come from a softmax-normalized attention function (a perceptron in this paper), which are learned during training.\n- Attention can be expensive, because it must be evaluated for each encoder-decoder output pair, resulting in a len(x) * len(y) matrix.\n- The attention mechanism improves performance across the board, but has a particularly large affect on long sentences, confirming the hyptohesis that the fixed vector encoding is a bottleneck.\n- The authors use a bidirectional-GRU, concatenating both hidden states into a final state at each time step.\n- It is easy to visualize the attention matrix (for a single input-ouput sequence pair). The authors show that in the case of English to French translations the matrix has large values on the diagonal, showing the these two languages are well aligned in terms of word order.\n\n\n#### Question/Notes\n\n- The attention mechanism seems limited in that it computes a simple weighted average. What about more complex attention functions that allow input states to interact?",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1409.0473"
    },
    "851": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SordoniGABJMNGD15",
        "transcript": "TLDR; The authors propose three neural models to generate a response (r) based on a context and message pair (c,m). The context is defined as a single message. The first model, RLMT, is a basic Recurrent Language Model that is fed the whole (c,m,r) triple. The second model, DCGM-1, encodes context and message into a BoW representation, put it through a feedforward neural network encoder, and then generates the response using an RNN decoder. The last model, DCGM-2, is similar but keeps the representations of context and message separate instead of encoding them into a single BoW vector. The authors train their models on 29M triple data set from Twitter and evaluate using BLEU, METEOR and human evaluator scores.\n\n#### Key Points:\n\n- 3 Models: RLMT, DCGM-1, DCGM-2\n- Data: 29M triples from Twitter\n- Because (c,m) is very long on average the authors expect RLMT to perform poorly.\n- Vocabulary: 50k words, trained with NCE loss\n- Generates responses degrade with length after ~8 tokens\n\n\n#### Notes/Questions:\n\n- Limiting the context to a single message kind of defeats the purpose of this. No real conversations have only a single message as context, and who knows how well the approach works with a larger context?\n- Authors complain that dealing with long sequences is hard, but they don't even use an LSTM/GRU. Why?\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.06714"
    },
    "852": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/JeanCMB15",
        "transcript": "TLDR; The authors propose an importance-sampling approach to deal with large vocabularies in NMT models. During training, the corpus is partitioned, and for each partition only target words occurring in that partition are chosen. To improve decoding speed over the full vocabulary, the authors build a dictionary mapping from source sentence to potential target vocabulary. The authors evaluate their approach on standard MT tasks and perform better than the baseline models with smaller vocabulary.\n\n#### Key Points:\n\n- Computing partition function is the bottleneck. Use sampling-based approach.\n- Dealing with large vocabulary during training is separate from dealing with large vocab during decoding. Training is handled with importance sampling. Decoding is handled with source-based candidate list.\n- Decoding with candidate list takes around 0.12s (0.05) per token on CPU (GPU). Without target list 0.8s (0.25s).\n- Issue: Candidate list is depended on source sentence, so it must be re-computed for each sentence. \n- Reshuffling the data set is expensive as new partitions need to be calculated (not necessary, but improved scores).\n\n#### Notes:\n\n- How is the corpus partitioned? What's the effect of the partitioning strategy?\n- The authors say that they replace UNK tokens using \"another word alignment model\" but don't go into detail what this is. The results show that doing this results in much larger score bump than increasing the vocab does. (The authors do this for all comparison models though).\n- Reshuffling the dataset also results in a significant performance bump, but this operation is expensive. IMO the authors should take all these into account when reporting performance numbers. A single training update may be a lot faster, but the setup time increases. I'd would've like to see the authors assign a global time budget to train/test and then compare the models based on that.\n- The authors only briefly mentioned that re-building the target vocab for each source sentence is an issue and how they solve it, no details given.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P15/P15-1001.pdf"
    },
    "853": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LuongM16",
        "transcript": "TLDR; The authors train a word-level NMT where UNK tokens in both source and target sentence are replaced by character-level RNNs that produce word representations. The authors can thus train a fast word-based system that still generalized that doesn't produce unknown words. The best system achieves a new state of the art BLEU score of 19.9 in WMT'15 English to Czech translation.\n\n\n#### Key Points\n\n- Source Sentence: Final hidden state of character-RNN is used as word representation.\n- Source Sentence: Character RNNs always initialized with 0 state to allow efficient pre-training \n- Target: Produce word-level sentence including UNK first and then run the char-RNNs\n- Target: Two ways to initialize char-RNN: With same hidden state as word-RNN (same-path), or with its own representation (separate-path)\n- Authors find that attention mechanism is critical for pure character-based NMT models\n\n\n#### Notes\n\n- Given that the authors demonstrate the potential of character-based models, is the hybrid approach the right direction? If we had more compute power, would pure character-based models win?",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.00788"
    },
    "854": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/VinyalsFJ15",
        "transcript": "TLDR; The authors propose a new architecture called \"Pointer Network\". A Pointer Network is a seq2seq architecture with attention mechanism where the output vocabulary is the set of input indices. Since the output vocabulary varies based on input sequence length, a Pointer Network can generalize to variable-length inputs. The attention method trough which this is achieved is O(n^2), and only a sight variation of the standard seq2seq attention mechanism. The authors evaluate the architecture on tasks where the outputs correspond to positions of the inputs: Convex Hull, Delaunay Triangulation and Traveling Salesman problems. The architecture performs well these, and generalizes to sequences longer than those found in the training data.\n\n\n#### Key Points\n\n- Similar to standard attention, but don't blend the encoder states, use the attention vector directory.\n- Softmax probabilities of outputs can be interpreted as a fuzzy pointer.\n- We can solve the same problem artificially using seq2seq and outputting \"coordinates\", but that ignores the output constraints and would be less efficient.\n- 512 unit LSTM, SGD with LR 1.0, batch size of 128, L2 gradient clipping of 2.0.\n- In the case of TSP, the \"student\" networks outperforms the \"teacher\" algorithm.\n\n\n#### Notes/  Questions\n\n- Seems like this architecture could be applied to generating spans (as in the newer \"Text Processing From Bytes\" paper), for POS tagging for example. That would require outputting classes in addition to input pointers. How?",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5866-pointer-networks"
    },
    "855": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ChoMBB14",
        "transcript": "TLDR; The authors empirically evaluate seq2seq Neural Machine Translation systems. They find that performance degrades significantly as sentences get longer, and as the number of unknown words in the source sentence increases. Thus, they propose that more investigation into how to deal with large vocabularies and long-range dependencies is needed. The authors also present a new gated recursive convolutional network (grConv) architecture, which consists of a binary tree using GRU units. While this network architecture does not perform as well as the RNN encoder, it seems to be learning grammatical properties represented in the gate activations in an unsupervised fashion.\n\n#### Key Points\n\n- GrConv: Neuron computed as combination between left and right neuron in previous layer, gated with the activations of those neurons. 3 gates: Left, right, reset.\n- In experiments, encoder varies between RNN and grConv. Decoder is always RNN.\n- Model size is only 500MB. 30k vocabulary. Only trained on sentences <= 30 tokens. Networks not trained to convergence. \n- Beam search with scores normalized by sequence length to choose translations.\n- Hypothesis is that fixed vector representation is a bottleneck, or that decoder is not powerful enough.\n\n\n#### Notes/Questions\n\n- THe network is only trained on sequences <= 30 tokens. Can we really expect it to perform well on long sequences? Long sequences may inherently have grammatical structures that cannot be observed in short sequences.\n- There's a mistake in the new activation formula, wrong time superscript, should be (t-1).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1409.1259"
    },
    "856": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MnihHGK14",
        "transcript": "TLDR; The authors train a RNN that takes as input a glimpse (part of the image subsamples to same size) and outputs a new glimpse and action (prediction, agent move) at each step. Thus, the model adaptively selects which part of an image to \"attend\" to. By defining the number of glimpses and their reoslutions we can control the complexity of the model independently of image size, which is not true for CNNs. The model is not differentiable, but can be trained using Reinforcement Learning techniques. The authors evaluate the model on the MNIST dataset, a cluttered version of MNIST, and a dynamic video game environment.\n\n#### Questions / Notes\n\n- I think the the author's claim taht the model works independently of image size is only partly true, as larger images are likely to require more glimpses or bigger regions.\n- Would be nice to see some large-scale benchmarks as MNIST is very simple tasks. However, the authors clearly identify this as future work.\n- No mentions about training time. Is it even feasible to train this for large images (which probably require more glimpses)?\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention"
    },
    "857": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/VisinKCMCB15",
        "transcript": "TLDR; The authors propose a novel architecture called ReNet, which replaces convolutional and max-pooling layers with RNNs that sweep over the image vertically and horizontally. These RNN layers are then stacked. The authors demonstrate that ReNet architecture is a viable alternative to CNNs. ReNet doesn't outperform CNNs in this paper, but further optimizations and hyperparameter tuning are likely going to lead to improved results in the future.\n\n#### Key Points:\n\n- Split images into patches, feed one patch per time step into RNN, vertically then horizontally. 4 RNNs per layer, 2 vertical and 2 horizontal, one per diretion.\n- Because the RNNs sweep over the whole image they can see the context of the full image, as opposed to just a local context in the case of conv/pool layers.\n- Smooth from end-end to end.\n- In experiments, 2 256-dimensional ReNet layers, 2x2 patches, 4096-dimensional affine layers.\n- Flipping and shifting for data augmentation.\n\n#### Notes/Questions:\n\n- What is the training time/complexity compared to a CNN? \n- Why split the image into patches at all? I wonder if the authors have experimented with various patch sizes, like defining patches that go over the full vertical height. 2x2 patches as used in the experiment seem quite small and like a waste of computational resources.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1505.00393"
    },
    "858": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/TranBM16",
        "transcript": "TLDR; the authors present Recurrent Memory Network. These networks use an attention mechanism (memory bank, MB) to explicitly incorporate information about preceding into the predictions at each time step. The MB is a layer that can be incorporated into any RNN, and the authors evaluate a total of 8 model variants: Optionally stacking another LSTM layer on top of the MB, optionally including a temporal matrix in the attention calcuation, and using a gating vs. linear function for the MB  output. The authors apply the model to Language Modeling tasks, achieving state of the art performance, and demonstrating that inspecting the attention weights yields intuitive insights into what the network learns: Co-occurence statistics and dependency type information. The authors also evaluate the models on a sentence completion task, achieving new state of the art.\n\n\n#### Key Points\n\n- RM: LSTM with MB as the top layer. No \"horizontal\" connections from MB to MB.\n- RMR:  LSTM with MB and another LSTM stacked on top.\n- RM with gating typically outperforms RMR.\n- Memory Bank (MB): Input is current hidden state, and n preceding inputs including the current one. Attention is then calculated over the inputs based on the hidden state. The Output is a new hidden state, which can be calculated with or without gating. Optionally apply temporal bias matrix to attention calculation.\n- Experiments: Hidden states and embeddings all of size 128. Memory size 15. SGD 15 epochs, halved each epoch after the forth.\n- Attention Analysis (Language Model): Obviously, most attention is given to current and recent words. But long-distance dependencies are also captured, e.g. separable verbs in German. Networks also discovers dependency types.\n\n\n#### Notes/Questions\n\n- This works seems related to \"Alternative structures for character-level RNNs\" where the authors feed n-grams from previous words into the classification layer. The idea is to relieve the network from having to memorize these. I wonder how the approaches compare. \n- No related work section? I don't know if I like the name memory bank and the reference to Memory Networks here. I think the main idea behind Memory Networks was to reason over multiple hops. The authors here only make one hop, which is essentially just a plain attention mechanism.\n- I wonder why exactly the RMR performs worse than the RM. I can't easily find an intuitive explanation for why that would be. Maybe just not enough training data?\n- How did the authors arrive at their hyperparameters (128 dimensions)? 128 seems small compared to other models.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1601.01272"
    },
    "859": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZarembaSV14",
        "transcript": "TLDR; The authors show that applying dropout to only the **non-recurrent** connections (between layers of the same timestep) in an LSTM works well, improving the scores on various sequence tasks.\n\n#### Data Sets and model performance\n\n- PTB Language Modeling Perplexity: 78.4\n- Google Icelandic Speech Dataset WER Accuracy: 70.5\n- WMT'14 English to French Machine Translation BLEU: 29.03\n- MS COCO Image Caption Generation BLEU: 24.3",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1409.2329"
    },
    "860": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DaiL15",
        "transcript": "TLDR; The authors show that we can pre-train RNNs using unlabeled data by either reconstructing the original sequence (SA-LSTM), or predicting the next token as in a language model (LM-LSTM). We can then fine-tune the weights on a supervised task. Pre-trained RNNs are more stable, generalize better, and achieve state-of-the-art results on various text classification tasks. The authors show that unlabeled data can compensate for a lack of labeled data.\n\n#### Data Sets\n\nError Rates for SA-LSTM, previous best results in parens.\n\n- IMDB: 7.24% (7.42%)\n- Rotten Tomatoes 16.7% (18.5%) (using additional unlabeled data)\n- 20 Newsgroups: 15.6% (17.1%)\n- DBPedia character-level: 1.19% (1.74%)\n\n#### Key Takeaways\n\n- SA-LSTM: Predict sequence based on final hidden state\n- LM-LSTM: Language-Model pretraining\n- LSTM, 1024-dimensional cell, 512-dimensional embedding, 512-dimensional hidden affine layer + 50% dropout, Truncated backprop 400 steps. Clipped cell outputs and gradients. Word and input embedding dropout tuned on dev set.\n- Linear Gain: Inject gradient at each step and linearly increase weights of prediction objectives\n\n#### Notes / Questions\n\n- Not clear when/how linear gain yields improvements. On some data sets it significantly reduces performance, on other it significantly improves performance. Any explanations?\n- Word dropout is used in the paper but not explained. I'm assuming it's replacing random words with `DROP` tokens?\n- The authors mention a joint training model, but it's only evaluated on the IMDB data set. I'm assuming the authors didn't evaluate it further because it performed badly, but it would be nice to get an intuition for why it doesn't work, and show results for other data sets.\n- All tasks are classification tasks. Does SA-LSTM also improve performance on seq2seq tasks?\n- What is the training time? :) (I also wonder how the batching is done, are texts padded to the same length with mask?)",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning"
    },
    "861": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZhangW15b",
        "transcript": "TLDR; The authors evaluate the impact of hyperparameters (embeddings, filter region size, number of feature maps, activation function, pooling, dropout and l2 norm constraint) on Kim's (2014) CNN for sentence classification. The authors present empirical findings with variance nunbers based on a large number of experiments on 7 classification data sets, and give practical recommendation for architecture decisions.\n\n\n#### Key Points\n\n- Recommended Baseline configuration: word2vec, (3,4,5) filter regions, 100 feature maps per region size, ReLU activation, 1-max-pooling, 0.5 dropout, l2 norm constraint on weight vector of 3.\n- One-hot vectors perform worse than pre-trained embeddings. word2vec outperforms GloVe most of the time.\n- Filter region size is dependent on data set in the range of 2-25. Recommended to do a line search over single region size and then combine multiple sizes.\n- Increasing the number of feature maps per filter region to more than 600 doesn't seem to help much.\n- ReLU almost always best activation function\n- Max-pooling almost always best pooling strategy\n- Dropout from 0.1 to 0.5 helps, l2 norm constraint not much\n\n\n#### Notes/Questions\n\n- All datasets analyzed in this paper are rather similar. They have similar average and max sentence length, and even the number of examples is of roughly the same magnitude. It would be interesting to see how the result change with very different datasets, such as long documents, or very large numbers of training examples.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1510.03820"
    },
    "862": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SutskeverVL14",
        "transcript": "TLDR; The authors show that a multilayer LSTM RNN (4 layers, 1000 cells per layer, 1000d embeddings, 160k source vocab, 80k target vocab) can achieve competitive results on Machine Translation tasks. The authors find that reversing the input sequence leads to significant improvements, most likely due to the introduction of short-term dependencies that are more easily captured by the gradients. Somewhat surprisingly, the LSTM did not have difficulties on long sentences. The model is evaluated on MT tasks and achieves competitive results (34.8 BLEU) by itself, and close to state of the art if coupled with existing baseline systems (36.5 BLEU).\n\n#### Key Points\n\n- Invert input sequence leads to significant improvement\n- Deep LSTM performs much better than shallow LSTM.\n- User different parameters for encoder/decoder. This allows parallel training for multiple languages decoders.\n- 4 Layers, 1000 cells per layer. 1000-dimensional words embeddings. 160k source vocabulary. 80k target vocabulary.Trained on 12M sentences (652M words). SGD with fixed learning rate of 0.7, decreasing by 1/2 every epoch after 5 initial epochs. Gradient clipping. Parallelization on GPU leads to 6.3k words/sec.\n- Batching sentences of approximately the same length leads to 2x speedup.\n- PCA projection shows meaningful clusters of sentences robust to passive/active voice, suggesting that the fixed vector representation captures meaning. \n- \"No complete explanation\" for why the LSTM does so much better with the introduced short-range dependencies.\n- Beam size 1 already performs well, beam size 2 is best in deep model.\n\n#### Notes/Questions\n\n- Seems like the performance here is mostly due to the computational resources available and optimized implementation. These models are pretty big by most standards, and other approaches (e.g. attention) may lead to better results if they had more computational resources.\n- Reversing the input still feels like a hack to me, there should be a more principled solution to deal with long-range dependencies.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks"
    },
    "863": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/XuBKCCSZB15",
        "transcript": "TLDR; The authors use an attention mechanism in image caption generation, allowing the decoder RNN focus on specific parts of the image. In order find the correspondence between words and image patches, the RNN uses a lower convolutional layer as its input (before pooling). The authors propose both a \"hard\" attention (trained using sampling methods) and \"soft\" attention (trained end-to-end) mechanism, and show qualitatively that the decoder focuses on sensible regions while generating text, adding an additional layer of interpretability to the model. The attention-based models achieve state-of-the art on Flickr8k, Flickr30 and MS Coco.\n\n#### Key Points\n\n- To find image correspondence use lower convolutional layers to attend to.\n- Two attention mechanisms: Soft and hard. Depending on evaluation metric (BLEU vs. METERO) one or the other performs better.\n- Largest data set (MS COCO) takes 3 days to train on Titan Black GPU. Oxford VGG.\n- Soft attention is same as for seq2seq models.\n- Attention weights are visualized by upsampling and applying a Gaussian\n\n#### Notes/Questions\n\n- Would've liked to see an explanation of when/how soft vs. hard attention does better.\n- What is the computational overhead of using the attention mechanism? Is it significant?",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/xuc15.html"
    },
    "864": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/KirosZSZUTF15",
        "transcript": "TLDR; The authors apply the skip-thoguth word2vec model to the sentence level, training auto-encoders that predict the previous and next sentences. The resulting general-purpose vector representations are called skip-thought vectors. The authors evaluate the performance of these vectors as features on semantic relatedness and classification tasks, achieving competitive results, but not beating fine-tuned models.\n\n#### Key Points\n\n- Code at https://github.com/ryankiros/skip-thoughts\n- Training is done on large book corpus (74M sentences, 1B tokens), takes 2 weeks. \n- Two variations: Bidirectional encoder and unidirectional encoder with 1200 and 2400 units per encoder respectively. GRU cell, Adam optimizer, gradient clipping norm 10.\n- Vocabulary can be expanded by learning a mapping from a large word2vec voab to the smaller skip-thought vocab. Could also used sampling/hierarchical softmax during training for larger vocab, or train on characters.\n\n#### Questions/Notes\n\n- Authors clearly state that this is not the goal of the paper, though I'd be curious how more sophisticated (non-linear) classifiers perform with skip-thought vectors. Authors probably tried this but it didn't do well ;)\n- The fact that the story generation doesn't seem work well shows that the model has problems learning or understanding long-term dependencies. I wonder if this can be solved by deeper encoders or attention.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5950-skip-thought-vectors"
    },
    "865": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/TianGHL16",
        "transcript": "TLDR; The authors train an RNN-based topic model that takes into consideration word order and assumes words in the same sentence sahre the same topic. The authors sample topic mixtures from a Dirichlet distribution and then train a \"topic embedding\" together with the rest of the generative LSTM. The model is evaluated quantitatively using perplexity on generated sentences and on classification tasks and clearly beats competing models. Qualitative evaluation shows that the model can generate sensible sentences conditioned on the topic.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.02038"
    },
    "866": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/JaderbergSZK15",
        "transcript": "TLDR; The authors introduce a new spatial transformation module that can be inserted into any Neural Network. The module consists of a spatial transformation network that predicts transformation parameters, a grid generator that chooses a sampling grid from the input, and a sampler that produces the output. Possible learned transformations include things cropping, translation, rotation, scaling or attention. The module can be trained end-to-end using backpropagation. The authors evaluate evaluate the module on both CNNs and MLPs, achieving state on distorted MNIST data, street view numbers, and fine-grained bird classification.\n\n#### Key Points:\n\n- STMs can be inserted between any layers, typically after the input or extracted features. The transform is dynamic and happens based on the input data.\n- The module is fast and doesn't adversely impact training speed.\n- The actual transformation parameters (output of localization network) can be fed into higher layers.\n- Attention can be seen as a special transformation that increases computational efficiency.\n- Can also be applied to RNNs, but more investigation is needed.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5854-spatial-transformer-networks"
    },
    "867": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=huang2016networks",
        "transcript": "TLDR; The authors randomly drop out complete layers during training using a modified ResNet architecture. The dropout probability hyperparameter decreases linearly (higher layers have a higher chance to be dropped) ending at 0.5 at the final layer in the experiments. This mechanisms helps vanishing gradients, diminishing feature reuse, and long training time. The model achieves new records on the CIFAR-10, CIFAR-100 and SVHN dataset.\n\n\n#### Key Points:\n\n- Can easily modify ResNet architecture to dropout out whole layer by only keeping the identity skip connection\n- Lower layers get lower probability of being dropped since they intuitively contain more \"stable\" features. Authors use linear decay with final value 0.5.\n- Training time reduces by 25% - 50% depending on dropout probability hyperparameter\n- Authors find that vanishing gradients are indeed reduces by plotting the gradient magnitudes vs. number of epochs\n- Can be interpreted as an ensemble of networks with varying depth\n- All layers are used during test time and need to scale activations appropriately\n- Authors successfully train network with 1000+ layers and achieve further error reduction\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.09382"
    },
    "868": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ChenGA15",
        "transcript": "TLDR; The authors evaluate softmax, hierarchical softmax, target sampling, NCE, self-normalization and differentiated softmax (novel technique presented in the paper) on data sets with varying vocabulary size (10k, 100k, 800k) with a fixed-time training budget. The authors find that techniques that work best for small vocabluaries are not necessarily the ones that work best for large vocabularies.\n\n#### Data and Models\n\nModels:\n\n- Sotmax\n- Hierarchical Softmax (cross-validation of clustering techniques)\n- Differentiated softmax, adjusting capacity based on token frequency (cross-validation of number of frequency bands and size)\n- Target Sampling (cross-validation of number of distractors)\n- NCE (cross-validation of noise ratio)\n- Self-normalization (cross-validation of regularization strenth)\n\nData:\n\n- PTB (1M tokens, 10k vocab)\n- Gigaword  (5B tokens, 100k vocab)\n- billionW (800M tokens, 800k vocab)\n\n\n#### Key Takeaways\n\n- Techniques that work best for small vocabluaries are not necessarily the ones that work best for large vocabularies.\n- Differentiated softmax varies the capacity (size of matrix slice in the last layer) based on token frequency. In practice, it's implemented as separate matrices with different sizes.\n- Perplexity doesn't seem to improve much after ~500M tokens\n- Models are trained for 1 week each\n- The competitiveness of softmax diminishes with vocabulary sizes. It seems to perform relatively well on 10k and 100k, but poorly on 800k since it need more processing time per example.\n- Traning time, not training data, is the main factor of limiting performance. The authors found that very large models are still making progress after one week and may eventually beat if the other models if allowed to run longer.\n\n\n#### Questions / Notes\n\n- What about the hyperparameters for Differentiated Softmax? The paper doesn't show an analysis. Also, the fact that this method introduces two additional hyperparameters makes it harder to apply in practice.\n- Would've liked to see more comparisons for Softmax, which is the simplest technique of all and doesn't need hyperparameter tuning. It doesn't work well on 800k vocab, but it does for 100k. So, the authors only show how it breaks down for one dataset.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.04906"
    },
    "869": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/TangQFL15",
        "transcript": "TLDR; The authors propose two LSTM-based models for target-dependent sentiment classification. TD-LSTM uses two LSTM networks running towards to target word from left and right respectively, making a prediction at the target time step. TC-LSTM is the same, but additionally incorporates the an averaged target word vector as an input at each time step. The authors evaluate their models with pre-trained word embeddings on a Twitter sentiment classification dataset, achieving state of the art.\n\n#### Key Points\n\n- TD-LSTM: Two LSTM networks, running from left to right towards the target. The final states of both networks are concatenated and the prediction is made at the target word.\n- TC-LSTM: Same architecture as TD-LSTM, but also incorporates the word vector as an input at each time step. The word vector is the average of the word vectors for the target phrase.\n- Embeddings seem to make a huge difference, state of the art is only obtained with 200-dimensional GloVe embeddings.\n\n\n#### Notes/Questions\n\n- A *huge* fraction of the performance improvement comes from pre-trained word embeddings. Without these, the proposed models clearly underperforms simpler models. This raises the question of whether incorporating the same embeddings into the simpler models would do.\n- Would've liked to see performance without *any* pre-trained embeddings.\n- The authors also experimented with attention mechanisms, but weren't able to achieve good results. Small size of training corpus may be the reason for this.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.01100"
    },
    "870": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HermannKGEKSB15",
        "transcript": "TLDR; The authors generate a large dataset (~1M examples) for question answering by using cloze deletion on summaries of crawled CNN and Daily Mail articles. They evaluate 2 baselines, 2 symbolic models (frame semantic, word distance), and 4 neural models (Deep LSTM, Uniform Reader, Attentive Reader, Impatient Reader) on the dataset. The neural models, particularly those with attenton, beat the syntactic models.\n\n- Deep LSTM: 2-layer bidirectional LSTM without attention mechanism\n- Attentive reader: 1-layer bidirectional LSTM with attention mechanism for the whole query\n- Impatient Reader: 1-layer bidirectional LSTM with attention mechanism for each token in the query (can be interpreted as being able to re-read the document at each token)\n- Uniform Reader: Uniform attention to all document tokens\n\nIn their experiments, the authors randomize document entities to avoid letting the models rely on world knowledge or co-occurence statistics, and intead purely testing document comprehension. This is done by replacing entities with consistent ids *within* a document, but using different ids across documents.\n\n#### Data and model performance\n\nAll numbers are accuracies on two datasets (CNN, Daily Mail)\n\n- Maximum Frequency Entity Baseline: 33.2 / 25.5\n- Exclusive Frequence Entity Baseline: 39.3 / 32.8\n- Frame-semantic model: 40.2 / 35.5\n- Word distance model: 50.9 / 55.5\n- Deep LSTM Reader: 57.0 / 62.2\n- Uniform Reader: 39.4 / 34.4\n- Attentive Reader: 63.0 / 69.0\n- Impatient Reader: 63.8 / 68.0\n\n\n#### Key Takeaways\n\n- The input to the RNN is defined as QUERY <DELIMITER> DOCUMENT, which is then embedded with or without attention and run through `softmax(W*x)` .\n- Some sequences are very long, up to 2000 tokens, and the average length was 763 tokens. All LSTM models seem to be able to deal with this, but the attention models show significantly higher accuracy.\n- Very nice attention visualizations and negative examples analysis that show the attention-based models focusing on the relevant parts of the document to answer the questions.\n\n\n#### Notes / Questions\n\n- How does document length affect the Deep LSTM reader? The appendix shows an analysis for attention models, but not for the Deep LSTM. A goal of the paper was to show that attention mechanisms are well suited for long documents because the fixed vector encoding is a bottleneck. The reuslts here aren't clear.\n- Are the gradient truncated? I can't imagine the network is unrolled for 2000 steps. The training parameters details don't mention this.\n- The mathematical notation in this paper needs some love. The concepts are relatively simple, but the formulas are hard to parse. \n- What if you limited the output vocabulary to words appearing in the query document?\n- Can you apply the same \"attention-based embedding\" mechanism to text classification?",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend"
    },
    "871": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=DBLP:journals/corr/ZhangL15",
        "transcript": "TLDR; Authors apply 6-layer and 9-layer (+3 affine) convolutional nets to character-level input and evaluate their models on Sentiment Analysis and Categorization tasks using (new) large-scale data sets. The authors don't use pre-trained word-embeddings, or any notion of words, and instead learn directly from character-level input with characters being encoded as one-hot vetors. This means the same model can be applied to any language (provided the vocabulary is small enough). The models presented in this paper beat BoW and word2vec baseline models.\n\n### Data and model performance\n\nBecause existing ones were too small the authors collected several new datasets that don't have standard benchmarks. \n\n- DBpedia Ontology Classification: 560k training, 70k test.\n- Amazon Reviews 5-class: 3M train, 650k test\n- Amazon Reviews polar: 3.6M train, 400k test\n- Yahoo! Answer topics 10-class: 1.4M train, 60k test\n- AG news classification 4-class: 120k train, 1.9k test\n- Sogou Chinese News 5-class: 450k train, 60k test\n\nModel accuracy for small and large models:\n\n- DBpedia: 98.02 / 98.27\n- Amazon 5-class: 59.47 / 58.69\n- Amazon 2-class: 94.50 / 94.49\n- Yahoo 10-class: 70.16 / 70.45\n- AG 4-class: 84.35 / 87.18\n- Chinese 5-class: 91.35 / 95.12\n\n#### Key Takeaways\n\n- Pretty Standard CNN architecture applied to characters. Conv, ReLU, Maxppol, fully-connected. Filter sizes of 7 and 3. See paper for parameter details.\n- Training takes a long time, presumably due to the size of the data. The authors quote 5 days per epoch on the large Amazon data set and large model.\n- Authors can't handle large vocabularies, they romanize Chinese. \n- Authors experiment with randomly replacing words with synonyms, seems to give a small improvements:\n\n\n#### Notes / Questions\n\n- The authors claim to do \"text understanding\" and learn representations, but all experiments are on simple classification tasks. There is no evidence that the network actually learns meaningful high-level representations, and doesn't just memorize n-grams for example.\n- These data sets are large, and the authors claim that they need large data sets, but there are no experiments in the paper that show this. How does performance vary with data size?\n- The comparision with other models is lacking. I would have liked to see some of the other state-of-the-art model being compared, e.g. Kim's CNN. Comparing with BoW doesn't show much. As these models are openly available the comparison should have been easy.\n- The romanization of Chinese is an ugly \"hack\" that goes against what the authors claim: Being language-independent and learning \"from scratch\".\n- It's strange that the authors use a thesaurus as a means for training example augmentation, as a theraus is word-level and language-specific, something that the authors explicitly argue against in this paper. Perhaps could have used word (character-level) dropout instead.\n- Are there any hyperparameters that were optimized? Authors don't mention any dev sets.\n- Have the datasets been made publicly available? The authors complain that \"the unfortunate fact in literature is that there are no large openly accessible datasets\", but fail to publish their own.\n- I'd expect the confustion matrix for the 5-star Amazon reviews to show mistakes coming from negations, but it doesn't, which suggests that the model really learns meaningful representations (such as negation).\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dblp.org"
    },
    "872": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SrivastavaGS15",
        "transcript": "TLDR; The authors propose \"Highway Networks\", which uses gates (inspired by LSTMs) to determine how much of a layer's activations to transform or just pass through. Highway Networks can be used with any kind of activation function, including recurrent and convnolutional units, and trained using plain SGD. The gating mechanism allows highway networks with tens or hundreds of layers to be trained efficiently. The authors show that highway networks with fewer parameters achieve results competitive with state-of-the art for the MNIST and CIFAR tasks. Gates outputs vary significantly with the input examples, demonstrating that the network not just learns a \"fixed structure\", but dynamically routes data based for specific examples examples.\n\nDatasets used: MNIST, CIFAR-10, CIFAR-100\n\n\n#### Key Takeaways\n\n- Apply LSTM-like gating to networks layers. Transform gate T and carry gate C.\n- The gating forces the layer inputs/outputs to be of the same size. We can use additional plain layers for dimensionality transformations.\n- Bias weights of the transform gates should be initialized to negative values (-1, -2, -3, etc) to initially force the networks to pass through information and learn long-term dependencies.\n- HWN does not learn a fixed structure (same gate outputs), but dynamic routing based on current input.\n- In complex data sets each layer makes an important contritbution, which is shown by lesioning (setting to pass-through) individual layers.\n\n\n#### Notes / Questions\n\n- Seems like the authors did not use dropout in their experiments. I wonder how these play together. Is dropout less effective for highway networks because the gates already learn efficients paths?\n- If we see that certain gates outputs have low variance across examples, can we \"prune\" the network into a fixed strucure to make it more efficient (for production deployments)?",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5850-training-very-deep-networks"
    },
    "873": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/WangQSHZ15a",
        "transcript": "TLDR; The authors evaluate the use of a Bidirectional LSTM RNN on POS tagging, chunking and NER tasks. The inputs are task-independent input features: The word and its capitalization. The authors incorporate prior knowledge about the taging tasks by restricting the decoder to output valid sequences of tags, and also propose a novel way of learning word embeddings: Randomly replacing words in a sequence and using an RNN to predict which words are correct vs.  incorrect. The authors show that their model combined with pre-trained word embeddings performs on par state of the art models.\n\n\n#### Key Points\n\n- Bidirectional LSTM with 100-dimensional embeddings, and 100-dimensional cells. Both 1 and 2 layers are evaluated. Predict tags at each step. Higher dimensionality of cells resultes in little improvement.\n- Word vector pretraining: Randomly replace words and use LSTM to predict correct/incorrect words.\n\n\n#### Notes/Questions\n\n- The fact that we need a task-specific decoder kind of defeats the purpose of this paper. The goal was to create a \"task-independent\" system. To be fair, the need for this decoder is probably only due to the small size of the training data. Not all tag combination appear in the training data.\n- The comparisons with other state of the art systems are somewhat unfair since the proposed model heavily relies on pre-trained word embeddings from external data (trained on more than 600M words) to achieve good performance. It also relies on external embeddings trained in yet another way.\n- I'm surprised that the authors didn't try combining all of the tagging tasks into one model, which seem like an obvious extension.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.00215"
    },
    "874": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/NogueiraC16",
        "transcript": "TLDR; The authors propose a web navigation task where an agent must find a target page containing a search query (typically a few sentences) by navigating a web graph with restrictions on memory, path length and number of exlorable nodes. Tey train Feedforward and Recurrent Neural Networks and evaluate their performance against that of human volunteers.\n\n\n#### Key Points\n\n\n- Datasets: Wiki-[NUM_ALLOWED_HOPS]: WikiNav-4 (6k train), WikiNav-8 (890k train), WikiNav-16 (12M train). Authors evaluate variosu query lengths for all data sets.\n- Vector representation of pages: BoW of pre-trained word2vec embeddings. \n- State-dependent action space: All possible outgoing links on the current page. At each step, the agent can peek at the neighboring nodes and see their full content.\n- Training, a single correct path is fed to the agent. Beam search to make predictions.\n- NeuAgent-FF uses a single tanh layer. NeuAgent-Rec uses LSTM.\n- Human performance typically worse than that of Neural agents\n\n\n#### Notes/Questions\n\n- Is it reasonable to allow the agents to \"peek\" at neighboring pages? Humans can make decisions based on the hyperlink context. In practice, peaking at each page may not be feasible if there are many links on the page.\n- I'm not sure if I buy the fact that this task requires Natural Language Understanding. Agents are just matching query word vectors against pages, which is no indication of NLU. An indication of NLU would be if the query was posed in a question format, which is typically short. But here, the authors use several sentences as queries and longer queries lead to better results, suggesting that the agents don't actually have any understanding of language. They just match text.\n- Authors say that NeuAgent-Rec performed consistently better for high hop length, but I don't see that in the data.\n- The training method seems a bit strange to me because the agent is fed only one correct path, but in reality there are a large number of correct paths and target pages. It may be more sensible to train the agent with all possible target pages and paths to answer a query.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.02261"
    },
    "875": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1703.10717",
        "transcript": "* [Detailed Summary](https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/)\n* [Tensorflow implementation](https://github.com/carpedm20/BEGAN-tensorflow)\n\n### Summary\n\n  * They suggest a GAN algorithm that is based on an autoencoder with Wasserstein distance.\n  * Their method generates highly realistic human faces.\n  * Their method has a convergence measure, which reflects the quality of the generates images.\n  * Their method has a diversity hyperparameter, which can be used to set the tradeoff between image diversity and image quality.\n\n### How\n  * Like other GANs, their method uses a generator G and a discriminator D.\n  * Generator\n    * The generator is fairly standard.\n    * It gets a noise vector `z` as input and uses upsampling+convolutions to generate images.\n    * It uses ELUs and no BN.\n  * Discriminator\n    * The discriminator is a full autoencoder (i.e. it converts input images to `8x8x3` tensors, then reconstructs them back to images).\n    * It has skip-connections from the `8x8x3` layer to each upsampling layer.\n    * It also uses ELUs and no BN.\n  * Their method now has the following steps:\n    1. Collect real images `x_real`.\n    2. Generate fake images `x_fake = G(z)`.\n    3. Reconstruct the real images `r_real = D(x_real)`.\n    4. Reconstruct the fake images `r_fake = D(x_fake)`.\n    5. Using an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of real images `d_real = Lp(x_real, r_real)`.\n    6. Using an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of fake images `d_fake = Lp(x_fake, r_fake)`.\n    7. The loss of D is now `L_D = d_real - d_fake`.\n    8. The loss of G is now `L_G = -L_D`.\n  * About the loss\n    * `r_real` and `r_fake` are really losses (e.g. L1-loss or L2-loss). In the paper they use `L(...)` for that. Here they are referenced as `d_*` in order to avoid confusion.\n    * The loss `L_D` is based on the Wasserstein distance, as in WGAN.\n    * `L_D` assumes, that the losses `d_real` and `d_fake` are normally distributed and tries to move their mean values. Ideally, the discriminator produces very different means for real/fake images, while the generator leads to very similar means.\n    * Their formulation of the Wasserstein distance does not require K-Lipschitz functions, which is why they don't have the weight clipping from WGAN.\n  * Equilibrium\n    * The generator and discriminator are at equilibrium, if `E[r_fake] = E[r_real]`. (That's undesirable, because it means that D can't differentiate between fake and real images, i.e. G doesn't get a proper gradient any more.)\n    * Let `g = E[r_fake] / E[r_real]`, then:\n      * Low `g` means that `E[r_fake]` is low and/or `E[r_real]` is high, which means that real images are not as well reconstructed as fake images. This means, that the discriminator will be more heavily trained towards reconstructing real images correctly (as that is the main source of error).\n      * High `g` conversely means that real images are well reconstructed (compared to fake ones) and that the discriminator will be trained more towards fake ones.\n      * `g` gives information about how much G and D should be trained each (so that none of the two overwhelms the other).\n    * They introduce a hyperparameter `gamma` (from interval `[0,1]`), which reflects the target value of the balance `g`.\n    * Using `gamma`, they change their losses `L_D` and `L_G` slightly:\n      * `L_D = d_real - k_t d_fake`\n      * `L_G = r_fake`\n      * `k_t+1 = k_t + lambda_k (gamma d_real - d_fake)`.\n    * `k_t` is a control term that controls how much D is supposed to focus on the fake images. It changes with every batch.\n    * `k_t` is clipped to `[0,1]` and initialized at `0` (max focus on reconstructing real images).\n    * `lambda_k` is like the learning rate of the control term, set to `0.001`.\n    * Note that `gamma d_real - d_fake = 0 <=> gamma d_real = d_fake <=> gamma = d_fake / d_real`.\n  * Convergence measure\n    * They measure the convergence of their model using `M`:\n    * `M = d_real + |gamma d_real - d_fake|`\n    * `M` goes down, if `d_real` goes down (D becomes better at autoencoding real images).\n    * `M` goes down, if the difference in reconstruction error between real and fake images goes down, i.e. if G becomes better at generating fake images.\n  * Other\n    * They use Adam with learning rate 0.0001. They decrease it by a factor of 2 whenever M stalls.\n    * Higher initial learning rate could lead to model collapse or visual artifacs.\n    * They generate images of max size 128x128.\n    * They don't use more than 128 filters per conv layer.\n\n### Results\n  * NOTES:\n    * Below example images are NOT from generators trained on CelebA. They used a custom dataset of celebrity images. They don't show any example images from the dataset. The generated images look like there is less background around the faces, making the task easier.\n    * Few example images. Unclear how much cherry picking was involved. Though the results from the tensorflow example (see like at top) make it look like the examples are representative (aside from speckle-artifacts).\n    * No LSUN Bedrooms examples. Human faces are comparatively easy to generate.\n  * Example images at 128x128:\n    * ![Examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/BEGAN__examples.jpg?raw=true \"Examples\")\n  * Effect of changing the target balance `gamma`:\n    * ![Examples gamma](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/BEGAN__examples_gamma.jpg?raw=true \"Examples gamma\")\n    * High gamma leads to more diversity at lower quality.\n  * Interpolations:\n    * ![Interpolations](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/BEGAN__interpolations.jpg?raw=true \"Interpolations\")\n  * Convergence measure `M` and associated image quality during the training:\n    * ![M](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/BEGAN__convergence.jpg?raw=true \"M\")",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1703.10717"
    },
    "876": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZhangXLZHWM16",
        "transcript": "  * They propose a two-stage GAN architecture that generates 256x256 images of (relatively) high quality.\n  * The model gets text as an additional input and the images match the text.\n\n### How\n  * Most of the architecture is the same as in any GAN:\n    * Generator G generates images.\n    * Discriminator D discriminates betweens fake and real images.\n    * G gets a noise variable `z`, so that it doesn't always do the same thing.\n  * Two-staged image generation:\n    * Instead of one step, as in most GANs, they use two steps, each consisting of a G and D.\n    * The first generator creates 64x64 images via upsampling.\n    * The first discriminator judges these images via downsampling convolutions.\n    * The second generator takes the image from the first generator, downsamples it via convolutions, then applies some residual convolutions and then re-upsamples it to 256x256.\n    * The second discriminator is comparable to the first one (downsampling convolutions).\n    * Note that the second generator does not get an additional noise term `z`, only the first one gets it.\n    * For upsampling, they use 3x3 convolutions with ReLUs, BN and nearest neighbour upsampling.\n    * For downsampling, they use 4x4 convolutions with stride 2, Leaky ReLUs and BN (the first convolution doesn't seem to use BN).\n  * Text embedding:\n    * The generated images are supposed to match input texts.\n    * These input texts are embedded to vectors.\n    * These vectors are added as:\n      1. An additional input to the first generator.\n      2. An additional input to the second generator (concatenated after the downsampling and before the residual convolutions).\n      3. An additional input to the first discriminator (concatenated after the downsampling).\n      4. An additional input to the second discriminator (concatenated after the downsampling).\n    * In case the text embeddings need to be matrices, the values are simply reshaped to `(N, 1, 1)` and then repeated to `(N, H, W)`.\n    * The texts are converted to embeddings via a network at the start of the model.\n      * Input to that vector: Unclear. (Concatenated word vectors? Seems to not be described in the text.)\n      * The input is transformed to a vector via a fully connected layer (the text model is apparently not recurrent).\n      * The vector is transformed via fully connected layers to a mean vector and a sigma vector.\n      * These are then interpreted as normal distributions, from which the final output vector is sampled. This uses the reparameterization trick, similar to the method in VAEs.\n      * Just like in VAEs, a KL-divergence term is added to the loss, which prevents each single normal distribution from deviating too far from the unit normal distribution `N(0,1)`.\n      * The authors argue, that using the VAE-like formulation -- instead of directly predicting an output vector (via FC layers) -- compensated for the lack of labels (smoother manifold).\n      * Note: This way of generating text embeddings seems very simple. (No recurrence, only about two layers.) It probably won't do much more than just roughly checking for the existence of specific words and word combinations (e.g. \"red head\").\n  * Visualization of the architecture:\n    * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/StackGAN__architecture.jpg?raw=true \"Architecture\")\n\n### Results\n  * Note: No example images of the two-stage architecture for LSUN bedrooms.\n  * Using only the first stage of the architecture (first G and D) reduces the Inception score significantly.\n  * Adding the text to both the first and second generator improves the Inception score slightly.\n  * Adding the VAE-like text embedding generation (as opposed to only FC layers) improves the Inception score slightly.\n  * Generating images at higher resolution (256x256 instead of 128x128) improves the Inception score significantly\n    * Note: The 256x256 architecture has more residual convolutions than the 128x128 one.\n    * Note: The 128x128 and the 256x256 are both upscaled to 299x299 images before computing the Inception score. That should make the 128x128 images quite blurry and hence of low quality.\n  * Example images, with text and stage 1/2 results:\n    * ![Examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/StackGAN__examples.jpg?raw=true \"Examples\")\n  * More examples of birds:\n    * ![Examples birds](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/StackGAN__examples_birds.jpg?raw=true \"Examples birds\")\n  * Examples of failures:\n    * ![Failure Cases](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/StackGAN__failures.jpg?raw=true \"Failure Cases\")\n    * The authors argue, that most failure cases happen when stage 1 messes up.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.03242"
    },
    "877": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1706.02515",
        "transcript": "https://github.com/bioinf-jku/SNNs\n\n  * They suggest a variation of ELUs, which leads to networks being automatically normalized.\n  * The effects are comparable to Batch Normalization, while requiring significantly less computation (barely more than a normal ReLU).\n\n### How\n  * They define Self-Normalizing Neural Networks (SNNs) as neural networks, which automatically keep their activations at zero-mean and unit-variance (per neuron).\n  * SELUs\n    * They use SELUs to turn their networks into SNNs.\n    * Formula:\n      * ![SELU](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Self-Normalizing_Neural_Networks__SELU.jpg?raw=true \"SELU\")\n      * with `alpha = 1.6733` and `lambda = 1.0507`.\n    * They proof that with properly normalized weights the activations approach a fixed point of zero-mean and unit-variance. (Different settings for alpha and lambda can lead to other fixed points.)\n    * They proof that this is still the case when previous layer activations and weights do not have optimal values.\n    * They proof that this is still the case when the variance of previous layer activations is very high or very low and argue that the mean of those activations is not so important.\n    * Hence, SELUs with these hyperparameters should have self-normalizing properties.\n    * SELUs are here used as a basis because:\n      1. They can have negative and positive values, which allows to control the mean.\n      2. They have saturating regions, which allows to dampen high variances from previous layers.\n      3. They have a slope larger than one, which allows to increase low variances from previous layers.\n      4. They generate a continuous curve, which ensures that there is a fixed point between variance damping and increasing.\n    * ReLUs, Leaky ReLUs, Sigmoids and Tanhs do not offer the above properties.\n  * Initialization\n    * SELUs for SNNs work best with normalized weights.\n    * They suggest to make sure per layer that:\n      1. The first moment (sum of weights) is zero.\n      2. The second moment (sum of squared weights) is one.\n    * This can be done by drawing weights from a normal distribution `N(0, 1/n)`, where `n` is the number of neurons in the layer.\n  * Alpha-dropout\n    * SELUs don't perform as well with normal Dropout, because their point of low variance is not 0.\n    * They suggest a modification of Dropout called Alpha-dropout.\n    * In this technique, values are not dropped to 0 but to `alpha' = -lambda * alpha = -1.0507 * 1.6733 = -1.7581`.\n    * Similar to dropout, activations are changed during training to compensate for the dropped units.\n    * Each activation `x` is changed to `a(xd+alpha'(1-d))+b`.\n      * `d = B(1, q)` is the dropout variable consisting of 1s and 0s.\n      * `a = (q + alpha'^2 q(1-q))^(-1/2)`\n      * `b = -(q + alpha'^2 q(1-q))^(-1/2) ((1-q)alpha')`\n    * They made good experiences with dropout rates around 0.05 to 0.1.\n\n### Results\n  * Note: All of their tests are with fully connected networks. No convolutions.\n  * Example training results:\n    * ![MINST CIFAR10](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Self-Normalizing_Neural_Networks__MNIST_CIFAR10.jpg?raw=true \"MNIST CIFAR10\")\n    * Left: MNIST, Right: CIFAR10\n    * Networks have N layers each, see legend. No convolutions.\n  * 121 UCI Tasks\n    * They manage to beat SVMs and RandomForests, while other networks (Layer Normalization, BN, Weight Normalization, Highway Networks, ResNet) perform significantly worse than their network (and usually don't beat SVMs/RFs).\n  * Tox21\n    * They achieve better results than other networks (again, Layer Normalization, BN, etc.).\n    * They achive almost the same result as the so far best model on the dataset, which consists of a mixture of neural networks, SVMs and Random Forests.\n  * HTRU2\n    * They achieve better results than other networks.\n    * They beat the best non-neural method (Naive Bayes).\n  * Among all tested other networks, MSRAinit performs best, which references a network withput any normalization, only ReLUs and Microsoft Weight Initialization (see paper: `Delving deep into rectifiers: Surpassing human-level performance on imagenet classification`).\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1706.02515"
    },
    "878": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1701.07875",
        "transcript": "  * They suggest a slightly altered algorithm for GANs.\n  * The new algorithm is more stable than previous ones.\n\n### How\n  * Each GAN contains a Generator that generates (fake-)examples and a Discriminator that discriminates between fake and real examples.\n  * Both fake and real examples can be interpreted as coming from a probability distribution.\n  * The basis of each GAN algorithm is to somehow measure the difference between these probability distributions\n    and change the network parameters of G so that the fake-distribution becomes more and more similar to the real distribution.\n  * There are multiple distance measures to do that:\n    * Total Variation (TV)\n    * KL-Divergence (KL)\n    * Jensen-Shannon divergence (JS)\n      * This one is based on the KL-Divergence and is the basis of the original GAN, as well as LAPGAN and DCGAN.\n    * Earth-Mover distance (EM), aka Wasserstein-1\n      * Intuitively, one can imagine both probability distributions as hilly surfaces. EM then reflects, how much mass has to be moved to convert the fake distribution to the real one.\n  * Ideally, a distance measure has everywhere nice values and gradients\n    (e.g. no +/- infinity values; no binary 0 or 1 gradients; gradients that get continously smaller when the generator produces good outputs).\n  * In that regard, EM beats JS and JS beats TV and KL (roughly speaking). So they use EM.\n  * EM\n    * EM is defined as\n      * ![EM](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/WGAN__EM.jpg?raw=true \"EM\")\n      * (inf = infinum, more or less a minimum)\n    * which is intractable, but following the Kantorovich-Rubinstein duality it can also be calculated via\n      * ![EM tractable](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/WGAN__EM_tractable.jpg?raw=true \"EM tractable\")\n      * (sup = supremum, more or less a maximum)\n    * However, the second formula is here only valid if the network is a K-Lipschitz function (under every set of parameters).\n    * This can be guaranteed by simply clipping the discriminator's weights to the range `[-0.01, 0.01]`.\n    * Then in practice the following version of the tractable EM is used, where `w` are the parameters of the discriminator:\n      * ![EM tractable in practice](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/WGAN__EM_tractable_practice.jpg?raw=true \"EM tractable in practice\")\n  * The full algorithm is mostly the same as for DCGAN:\n    * ![Algorithm](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/WGAN__algorithm.jpg?raw=true \"Algorithm\")\n    * Line 2 leads to training the discriminator multiple times per batch (i.e. more often than the generator).\n      * This is similar to the `max w in W` in the third formula (above).\n      * This was already part of the original GAN algorithm, but is here more actively used.\n      * Because of the EM distance, even a \"perfect\" discriminator still gives good gradient (in contrast to e.g. JS, where the discriminator should not be too far ahead). So the discriminator can be safely trained more often than the generator.\n    * Line 5 and 10 are derived from EM. Note that there is no more Sigmoid at the end of the discriminator!\n    * Line 7 is derived from the K-Lipschitz requirement (clipping of weights).\n    * High learning rates or using momentum-based optimizers (e.g. Adam) made the training unstable, which is why they use a small learning rate with RMSprop.\n\n### Results\n  * Improved stability. The method converges to decent images with models which failed completely when using JS-divergence (like in DCGAN).\n    * For example, WGAN worked with generators that did not have batch normalization or only consisted of fully connected layers.\n  * Apparently no more mode collapse. (Mode collapse in GANs = the generator starts to generate often/always the practically same image, independent of the noise input.)\n  * There is a relationship between loss and image quality. Lower loss (at the generator) indicates higher image quality. Such a relationship did not exist for JS divergence.\n  * Example images:\n    * ![Example images](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/WGAN__examples.jpg?raw=true \"Example images\")",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1701.07875"
    },
    "879": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1612.08242",
        "transcript": "  * They suggest a new version of YOLO, a model to detect bounding boxes in images.\n  * Their new version is more accurate, faster and is trained to recognize up to 9000 classes.\n\n### How\n  * Their base model is the previous YOLOv1, which they improve here.\n  * Accuracy improvements\n    * They add batch normalization to the network.\n    * Pretraining usually happens on ImageNet at 224x224, fine tuning for bounding box detection then on another dataset, say Pascal VOC 2012, at higher resolutions, e.g. 448x448 in the case of YOLOv1.\n      This is problematic, because the pretrained network has to learn to deal with higher resolutions and a new task at the same time.\n      They instead first pretrain on low resolution ImageNet examples, then on higher resolution ImegeNet examples and only then switch to bounding box detection.\n      That improves their accuracy by about 4 percentage points mAP.\n    * They switch to anchor boxes, similar to Faster R-CNN. That's largely the same as in YOLOv1. Classification is now done per tested anchor box shape, instead of per grid cell.\n      The regression of x/y-coordinates is now a bit smarter and uses sigmoids to only translate a box within a grid cell.\n    * In Faster R-CNN the anchor box shapes are manually chosen (e.g. small squared boxes, large squared boxes, thin but high boxes, ...).\n      Here instead they learn these shapes from data.\n      That is done by applying k-Means to the bounding boxes in a dataset.\n      They cluster them into k=5 clusters and then use the centroids as anchor box shapes.\n      Their accuracy this way is the same as with 9 manually chosen anchor boxes.\n      (Using k=9 further increases their accuracy significantly, but also increases model complexity. As they want to predict 9000 classes they stay with k=5.)\n    * To better predict small bounding boxes, they add a pass-through connection from a higher resolution layer to the end of the network.\n    * They train their network now at multiple scales. (As the network is now fully convolutional, they can easily do that.)\n  * Speed improvements\n    * They get rid of their fully connected layers. Instead the network is now fully convolutional.\n    * They have also removed a handful or so of their convolutional layers.\n  * Capability improvement (weakly supervised learning)\n    * They suggest a method to predict bounding boxes of the 9000 most common classes in ImageNet.\n      They add a few more abstract classes to that (e.g. dog for all breeds of dogs) and arrive at over 9000 classes (9418 to be precise).\n    * They train on ImageNet and MSCOCO.\n    * ImageNet only contains class labels, no bounding boxes. MSCOCO only contains general classes (e.g. \"dog\" instead of the specific breed).\n    * They train iteratively on both datasets. MSCOCO is used for detection and classification, while ImageNet is only used for classification.\n      For an ImageNet example of class `c`, they search among the predicted bounding boxes for the one that has highest predicted probability of being `c`\n      and backpropagate only the classification loss for that box.\n    * In order to compensate the problem of different abstraction levels on the classes (e.g. \"dog\" vs a specific breed), they make use of WordNet.\n      Based on that data they generate a hierarchy/tree of classes, e.g. one path through that tree could be: object -> animal -> canine -> dog -> hunting dog -> terrier -> yorkshire terrier.\n      They let the network predict paths in that hierarchy, so that the prediction \"dog\" for a specific dog breed is not completely wrong.\n    * Visualization of the hierarchy:\n      * ![YOLO9000 hierarchy](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/YOLO9000__hierarchy.jpg?raw=true \"YOLO9000 hierarchy\")\n    * They predict many small softmaxes for the paths in the hierarchy, one per node:\n      * ![YOLO9000 softmaxes](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/YOLO9000__softmaxes.jpg?raw=true \"YOLO9000 softmaxes\")\n\n### Results\n  * Accuracy\n    * They reach about 73.4 mAP when training on Pascal VOC 2007 and 2012. That's slightly behind Faster R-CNN with VGG16 with 75.9 mAP, trained on MSCOCO+2007+2012.\n  * Speed\n    * They reach 91 fps (10ms/image) at image resolution 288x288 and 40 fps (25ms/image) at 544x544.\n  * Weakly supervised learning\n    * They test their 9000-class-detection on ImageNet's detection task, which contains bounding boxes for 200 object classes.\n    * They achieve 19.7 mAP for all classes and 16.0% mAP for the 156 classes which are not part of MSCOCO.\n    * For some classes they get 0 mAP accuracy.\n    * The system performs well for all kinds of animals, but struggles with not-living objects, like sunglasses.\n    * Example images (notice the class labels):\n      * ![YOLO9000 examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/YOLO9000__examples.jpg?raw=true \"YOLO9000 examples\")",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1612.08242"
    },
    "880": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/RedmonDGF16",
        "transcript": "  * They suggest a model (\"YOLO\") to detect bounding boxes in images.\n  * In comparison to Faster R-CNN, this model is faster but less accurate.\n\n### How\n  * Architecture\n    * Input are images with a resolution of 448x448.\n    * Output are `S*S*(B*5 + C)` values (per image).\n      * `S` is the grid size (default value: 7). Each image is split up into `S*S` cells.\n      * `B` is the number of \"tested\" bounding box shapes at each cell (default value: 2).\n        So at each cell, the network might try one large and one small bounding box.\n        The network predicts additionally for each such tested bounding box `5` values.\n        These cover the exact position (x, y) and scale (height, width) of the bounding box as well as a confidence value.\n        They allow the network to fine tune the bounding box shape and reject it, e.g. if there is no object in the grid cell.\n        The confidence value is zero if there is no object in the grid cell and otherwise matches the IoU between predicted and true bounding box.\n      * `C` is the number of classes in the dataset (e.g. 20 in Pascal VOC). For each grid cell, the model decides once to which of the `C` objects the cell belongs.\n    * Rough overview of their outputs:\n      * ![Method](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/YOLO__method.jpg?raw=true \"Method\")\n    * In contrast to Faster R-CNN, their model does *not* use a separate region proposal network (RPN).\n    * Per bounding box they actually predict the *square root* of height and width instead of the raw values.\n      That is supposed to result in similar errors/losses for small and big bounding boxes.\n    * They use a total of 24 convolutional layers and 2 fully connected layers.\n      * Some of these convolutional layers are 1x1-convs that halve the number of channels (followed by 3x3s that double them again).\n      * Overview of the architecture:\n        * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/YOLO__architecture.jpg?raw=true \"Architecture\")\n    * They use Leaky ReLUs (alpha=0.1) throughout the network. The last layer uses linear activations (apparently even for the class prediction...!?).\n    * Similarly to Faster R-CNN, they use a non maximum suppression that drops predicted bounding boxes if they are too similar to other predictions.\n  * Training\n    * They pretrain their network on ImageNet, then finetune on Pascal VOC.\n    * Loss\n      * They use sum-squared losses (apparently even for the classification, i.e. the `C` values).\n      * They dont propagate classification loss (for `C`) for grid cells that don't contain an object.\n      * For each grid grid cell they \"test\" `B` example shapes of bounding boxes (see above).\n        Among these `B` shapes, they only propagate the bounding box losses (regarding x, y, width, height, confidence) for the shape that has highest IoU with a ground truth bounding box.\n      * Most grid cells don't contain a bounding box. Their confidence values will all be zero, potentialle dominating the total loss.\n        To prevent that, the weighting of the confidence values in the loss function is reduced relative to the regression components (x, y, height, width).\n\n### Results\n  * The coarse grid and B=2 setting lead to some problems. Namely, small objects are missed and bounding boxes can end up being dropped if they are too close to other bounding boxes.\n  * The model also has problems with unusual bounding box shapes.\n  * Overall their accuracy is about 10 percentage points lower than Faster R-CNN with VGG16 (63.4% vs 73.2%, measured in mAP on Pascal VOC 2007).\n  * They achieve 45fps (22ms/image), compared to 7fps (142ms/image) with Faster R-CNN + VGG16.\n  * Overview of results on Pascal VOC 2012:\n    * ![Results on VOC2012](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/YOLO__results.jpg?raw=true \"Results on VOC2012\")\n  * They also suggest a faster variation of their model which reached 145fps (7ms/image) at a further drop of 10 percentage points mAP (to 52.7%).\n  * A significant part of their error seems to come from badly placed or sized bounding boxes (e.g. too wide or too much to the right).\n  * They mistake background less often for objects than Fast R-CNN. They test combining both models with each other and can improve Fast R-CNN's accuracy by about 2.5 percentage points mAP.\n  * They test their model on paintings/artwork (Picasso and People-Art datasets) and notice that it generalizes fairly well to that domain.\n  * Example results (notice the paintings at the top):\n    * ![Examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/YOLO__examples.jpg?raw=true \"Examples\")",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2016.91"
    },
    "881": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.08588",
        "transcript": "  * They present a variation of Faster R-CNN.\n    * Faster R-CNN is a model that detects bounding boxes in images.\n  * Their variation is about as accurate as the best performing versions of Faster R-CNN.\n  * Their variation is significantly faster than these variations (roughly 50ms per image).\n\n### How\n  * PVANET reuses the standard Faster R-CNN architecture:\n    * A base network that transforms an image into a feature map.\n    * A region proposal network (RPN) that uses the feature map to predict bounding box candidates.\n    * A classifier that uses the feature map and the bounding box candidates to predict the final bounding boxes.\n  * PVANET modifies the base network and keeps the RPN and classifier the same.\n  * Inception\n    * Their base network uses eight Inception modules.\n    * They argue that these are good choices here, because they are able to represent an image at different scales (aka at different receptive field sizes)\n      due to their mixture of 3x3 and 1x1 convolutions.\n      * ![Receptive field sizes in inception modules](images/PVANET__inception_fieldsize.jpg?raw=true \"Receptive field sizes in inception modules\")\n    * Representing an image at different scales is useful here in order to detect both large and small bounding boxes.\n    * Inception modules are also reasonably fast.\n    * Visualization of their Inception modules:\n      * ![Inception modules architecture](images/PVANET__inception_modules.jpg?raw=true \"Inception modules architecture\")\n  * Concatenated ReLUs\n    * Before the eight Inception modules, they start the network with eight convolutions using concatenated ReLUs.\n    * These CReLUs compute both the classic ReLU result (`max(0, x)`) and concatenate to that the negated result, i.e. something like `f(x) = max(0, x <concat> (-1)*x)`.\n    * That is done, because among the early one can often find pairs of convolution filters that are the negated variations of each other.\n      So by adding CReLUs, the network does not have to compute these any more, instead they are created (almost) for free, reducing the computation time by up to 50%.\n    * Visualization of their final CReLU block:\n      * TODO\n      * ![CReLU modules](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/PVANET__crelu.jpg?raw=true \"CReLU modules\")\n  * Multi-Scale output\n    * Usually one would generate the final feature map simply from the output of the last convolution.\n    * They instead combine the outputs of three different convolutions, each resembling a different scale (or level of abstraction).\n    * They take one from an early point of the network (downscaled), one from the middle part (kept the same) and one from the end (upscaled).\n    * They concatenate these and apply a 1x1 convolution to generate the final output.\n  * Other stuff\n    * Most of their network uses residual connections (including the Inception modules) to facilitate learning.\n    * They pretrain on ILSVRC2012 and then perform fine-tuning on MSCOCO, VOC 2007 and VOC 2012.\n    * They use plateau detection for their learning rate, i.e. if a moving average of the loss does not improve any more, they decrease the learning rate. They say that this increases accuracy significantly.\n    * The classifier in Faster R-CNN consists of fully connected layers. They compress these via Truncated SVD to speed things up. (That was already part of Fast R-CNN, I think.)\n\n### Results\n  * On Pascal VOC 2012 they achieve 82.5% mAP at 46ms/image (Titan X GPU).\n    * Faster R-CNN + ResNet-101: 83.8% at 2.2s/image.\n    * Faster R-CNN + VGG16: 75.9% at 110ms/image.\n    * R-FCN + ResNet-101: 82.0% at 133ms/image.\n  * Decreasing the number of region proposals from 300 per image to 50 almost doubles the speed (to 27ms/image) at a small loss of 1.5 percentage points mAP.\n  * Using Truncated SVD for the classifier reduces the required timer per image by about 30% at roughly 1 percentage point of mAP loss.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.08588"
    },
    "882": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DaiLHS16",
        "transcript": "  * They present a variation of Faster R-CNN, i.e. a model that predicts bounding boxes in images and classifies them.\n  * In contrast to Faster R-CNN, their model is fully convolutional.\n  * In contrast to Faster R-CNN, the computation per bounding box candidate (region proposal) is very low.\n\n### How\n  * The basic architecture is the same as in Faster R-CNN:\n    * A base network transforms an image to a feature map. Here they use ResNet-101 to do that.\n    * A region proposal network (RPN) uses the feature map to locate bounding box candidates (\"region proposals\") in the image.\n    * A classifier uses the feature map and the bounding box candidates and classifies each one of them into `C+1` classes,\n      where `C` is the number of object classes to spot (e.g. \"person\", \"chair\", \"bottle\", ...) and `1` is added for the background.\n      * During that process, small subregions of the feature maps (those that match the bounding box candidates) must be extracted and converted to fixed-sizes matrices.\n        The method to do that is called \"Region of Interest Pooling\" (RoI-Pooling) and is based on max pooling.\n        It is mostly the same as in Faster R-CNN.\n    * Visualization of the basic architecture:\n      * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/R-FCN__architecture.jpg?raw=true \"Architecture\")\n  * Position-sensitive classification\n    * Fully convolutional bounding box detectors tend to not work well.\n    * The authors argue, that the problems come from the translation-invariance of convolutions, which is a desirable property in the case of classification but not when precise localization of objects is required.\n    * They tackle that problem by generating multiple heatmaps per object class, each one being slightly shifted (\"position-sensitive score maps\").\n    * More precisely:\n      * The classifier generates per object class `c` a total of `k*k` heatmaps.\n      * In the simplest form `k` is equal to `1`. Then only one heatmap is generated, which signals whether a pixel is part of an object of class `c`.\n      * They use `k=3*3`. The first of those heatmaps signals, whether a pixel is part of the *top left* corner of a bounding box of class `c`. The second heatmap signals, whether a pixel is part of the *top center* of a bounding box of class `c` (and so on).\n      * The RoI-Pooling is applied to these heatmaps.\n      * For `k=3*3`, each bounding box candidate is converted to `3*3` values. The first one resembles the top left corner of the bounding box candidate. Its value is generated by taking the average of the values in that area in the first heatmap.\n      * Once the `3*3` values are generated, the final score of class `c` for that bounding box candidate is computed by averaging the values.\n      * That process is repeated for all classes and a softmax is used to determine the final class.\n      * The graphic below shows examples for that:\n        * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/R-FCN__examples.jpg?raw=true \"Examples\")\n    * The above described RoI-Pooling uses only averages and hence is almost (computationally) free.\n      * They make use of that during the training by sampling many candidates and only backpropagating on those with high losses (online hard example mining, OHEM).\n  * \u00c0 trous trick\n    * In order to increase accuracy for small bounding boxes they use the \u00e0 trous trick.\n    * That means that they use a pretrained base network (here ResNet-101), then remove a pooling layer and set the \u00e0 trous rate (aka dilation) of all convolutions after the removed pooling layer to `2`.\n      * The \u00e1 trous rate describes the distance of sampling locations of a convolution. Usually that is `1` (sampled locations are right next to each other). If it is set to `2`, there is one value \"skipped\" between each pair of neighbouring sampling location.\n    * By doing that, the convolutions still behave as if the pooling layer existed (and therefore their weights can be reused). At the same time, they work at an increased resolution, making them more capable of classifying small objects. (Runtime increases though.)\n  * Training of R-FCN happens similarly to Faster R-CNN.\n\n### Results\n  * Similar accuracy as the most accurate Faster R-CNN configurations at a lower runtime of roughly 170ms per image.\n  * Switching to ResNet-50 decreases accuracy by about 2 percentage points mAP (at faster runtime). Switching to ResNet-152 seems to provide no measureable benefit.\n  * OHEM improves mAP by roughly 2 percentage points.\n  * \u00c0 trous trick improves mAP by roughly 2 percentage points.\n  * Training on `k=1` (one heatmap per class) results in a failure, i.e. a model that fails to predict bounding boxes. `k=7` is slightly more accurate than `k=3`.\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6465-r-fcn-object-detection-via-region-based-fully-convolutional-networks"
    },
    "883": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/RenHGS15",
        "transcript": "* R-CNN and its successor Fast R-CNN both rely on a \"classical\" method to find region proposals in images (i.e. \"Which regions of the image look like they *might* be objects?\").\n  * That classical method is selective search.\n  * Selective search is quite slow (about two seconds per image) and hence the bottleneck in Fast R-CNN.\n  * They replace it with a neural network (region proposal network, aka RPN).\n  * The RPN reuses the same features used for the remainder of the Fast R-CNN network, making the region proposal step almost free (about 10ms).\n\n### How\n  * They now have three components in their network:\n    * A model for feature extraction, called the \"feature extraction network\" (**FEN**). Initialized with the weights of a pretrained network (e.g. VGG16).\n    * A model to use these features and generate region proposals, called the \"Region Proposal Network\" (**RPN**).\n    * A model to use these features and region proposals to classify each regions proposal's object and readjust the bounding box, called the \"classification network\" (**CN**). Initialized with the weights of a pretrained network (e.g. VGG16).\n    * Usually, FEN will contain the convolutional layers of the pretrained model (e.g. VGG16), while CN will contain the fully connected layers.\n    * (Note: Only \"RPN\" really pops up in the paper, the other two remain more or less unnamed. I added the two names to simplify the description.)\n    * Rough architecture outline:\n      * ![Architecture](images/Faster_R-CNN__architecture.jpg?raw=true \"Architecture\")\n  * The basic method at test is as follows:\n    1. Use FEN to convert the image to features.\n    2. Apply RPN to the features to generate region proposals.\n    3. Use Region of Interest Pooling (RoI-Pooling) to convert the features of each region proposal to a fixed sized vector.\n    4. Apply CN to the RoI-vectors to a) predict the class of each object (out of `K` object classes and `1` background class) and b) readjust the bounding box dimensions (top left coordinate, height, width).\n  * RPN\n    * Basic idea:\n      * Place anchor points on the image, all with the same distance to each other (regular grid).\n      * Around each anchor point, extract rectangular image areas in various shapes and sizes (\"anchor boxes\"), e.g. thin/square/wide and small/medium/large rectangles. (More precisely: The features of these areas are extracted.)\n      * Visualization:\n        * ![Anchor Boxes](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Faster_R-CNN__anchor_boxes.jpg?raw=true \"Anchor Boxes\")\n      * Feed the features of these areas through a classifier and let it rate/predict the \"regionness\" of the rectangle in a range between 0 and 1. Values greater than 0.5 mean that the classifier thinks the rectangle might be a bounding box. (CN has to analyze that further.)\n      * Feed the features of these areas through a regressor and let it optimize the region size (top left coordinate, height, width). That way you get all kinds of possible bounding box shapes, even though you only use a few base shapes.\n    * Implementation:\n      * The regular grid of anchor points naturally arises due to the downscaling of the FEN, it doesn't have to be implemented explicitly.\n      * The extraction of anchor boxes and classification + regression can be efficiently implemented using convolutions.\n        * They first apply a 3x3 convolution on the feature maps. Note that the convolution covers a large image area due to the downscaling.\n          * Not so clear, but sounds like they use 256 filters/kernels for that convolution.\n        * Then they apply some 1x1 convolutions for the classification and regression.\n          * They use `2*k` 1x1 convolutions for classification and `4*k` 1x1 convolutions for regression, where `k` is the number of different shapes of anchor boxes.\n      * They use `k=9` anchor box types: Three sizes (small, medium, large), each in three shapes (thin, square, wide).\n      * The way they build training examples (below) forces some 1x1 convolutions to react only to some anchor box types.\n    * Training:\n      * Positive examples are anchor boxes that have an IoU with a ground truth bounding box of 0.7 or more. If no anchor point has such an IoU with a specific box, the one with the highest IoU is used instead.\n      * Negative examples are all anchor boxes that have IoU that do not exceed 0.3 for any bounding box.\n      * Any anchor point that falls in neither of these groups does not contribute to the loss.\n      * Anchor boxes that would violate image boundaries are not used as examples.\n      * The loss is similar to the one in Fast R-CNN: A sum consisting of log loss for the classifier and smooth L1 loss (=smoother absolute distance) for regression.\n      * Per batch they only sample examples from one image (for efficiency).\n      * They use 128 positive examples and 128 negative ones. If they can't come up with 128 positive examples, they add more negative ones.\n    * Test:\n      * They use non-maximum suppression (NMS) to remove too identical region proposals, i.e. among all region proposals that have an IoU overlap of 0.7 or more, they pick the one that has highest score.\n      * They use the 300 proposals with highest score after NMS (or less if there aren't that many).\n  * Feature sharing\n    * They want to share the features of the FEN between the RPN and the CN.\n    * So they need a special training method that fine-tunes all three components while keeping the features extracted by FEN useful for both RPN and CN at the same time (not only for one of them).\n    * Their training methods are:\n      * Alternating traing: One batch for FEN+RPN, one batch for FEN+CN, then again one batch for FEN+RPN and so on.\n      * Approximate joint training: Train one network of FEN+RPN+CN. Merge the gradients of RPN and CN that arrive at FEN via simple summation. This method does not compute a gradient from CN through the RPN's regression task, as that is non-trivial. (This runs 25-50% faster than alternating training, accuracy is mostly the same.)\n      * Non-approximate joint training: This would compute the above mentioned missing gradient, but isn't implemented.\n      * 4-step alternating training:\n        1. Clone FEN to FEN1 and FEN2.\n        2. Train the pair FEN1 + RPN.\n        3. Train the pair FEN2 + CN using the region proposals from the trained RPN.\n        4. Fine-tune the pair FEN2 + RPN. FEN2 is fixed, RPN takes the weights from step 2.\n        5. Fine-tune the pair FEN2 + CN. FEN2 is fixed, CN takes the weights from step 3, region proposals come from RPN from step 4.\n\n* Results\n  * Example images:\n    * ![Example images](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Faster_R-CNN__examples.jpg?raw=true \"Example images\")\n  * Pascal VOC (with VGG16 as FEN)\n    * Using an RPN instead of SS (selective search) slightly improved mAP from 66.9% to 69.9%.\n    * Training RPN and CN on the same FEN (sharing FEN's weights) does not worsen the mAP, but instead improves it slightly from 68.5% to 69.9%.\n  * Using the RPN instead of SS significantly speeds up the network, from 1830ms/image (less than 0.5fps) to 198ms/image (5fps). (Both stats with VGG16. They also use ZF as the FEN, which puts them at 17fps, but mAP is lower.)\n  * Using per anchor point more scales and shapes (ratios) for the anchor boxes improves results.\n    * 1 scale, 1 ratio: 65.8% mAP (scale `128*128`, ratio 1:1) or 66.7% mAP (scale `256*256`, same ratio).\n    * 3 scales, 3 ratios: 69.9% mAP (scales `128*128`, `256*256`, `512*512`; ratios 1:1, 1:2, 2:1).\n  * Two-staged vs one-staged\n    * Instead of the two-stage system (first, generate proposals via RPN, then classify them via CN), they try a one-staged system.\n    * In the one-staged system they move a sliding window over the computed feature maps and regress at every location the bounding box sizes and classify the box.\n    * When doing this, their performance drops from 58.7% to about 54%.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks"
    },
    "884": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/Girshick15",
        "transcript": "  * The original R-CNN had three major disadvantages:\n    1. Two-staged training pipeline: Instead of only training a CNN, one had to train first a CNN and then multiple SVMs.\n    2. Expensive training: Training was slow and required lots of disk space (feature vectors needed to be written to disk for all region proposals (2000 per image) before training the SVMs).\n    3. Slow test: Each region proposal had to be handled independently.\n  * Fast R-CNN ist an improved version of R-CNN and tackles the mentioned problems.\n    * It no longer uses SVMs, only CNNs (single-stage).\n    * It does one single feature extraction per image instead of per region, making it much faster (9x faster at training, 213x faster at test).\n    * It is more accurate than R-CNN.\n\n### How\n  * The basic architecture, training and testing methods are mostly copied from R-CNN.\n  * For each image at test time they do:\n    * They generate region proposals via selective search.\n    * They feed the image once through the convolutional layers of a pre-trained network, usually VGG16.\n    * For each region proposal they extract the respective region from the features generated by the network.\n    * The regions can have different sizes, but the following steps need fixed size vectors. So each region is downscaled via max-pooling so that it has a size of 7x7 (so apparently they ignore regions of sizes below 7x7...?).\n      * This is called Region of Interest Pooling (RoI-Pooling).\n      * During the backwards pass, partial derivatives can be transferred to the maximum value (as usually in max pooling). That derivative values are summed up over different regions (in the same image).\n    * They reshape the 7x7 regions to vectors of length `F*7*7`, where `F` was the number of filters in the last convolutional layer.\n    * They feed these vectors through another network which predicts:\n      1. The class of the region (including background class).\n      2. Top left x-coordinate, top left y-coordinate, log height and log width of the bounding box (i.e. it fine-tunes the region proposal's bounding box). These values are predicted once for every class (so `K*4` values).\n  * Architecture as image:\n    * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Fast_R-CNN__architecture.jpg?raw=true \"Architecture\")\n  * Sampling for training\n    * Efficiency\n      * If batch size is `B` it is inefficient to sample regions proposals from `B` images as each image will require a full forward pass through the base network (e.g. VGG16).\n      * It is much more efficient to use few images to share most of the computation between region proposals.\n      * They use two images per batch (each 64 region proposals) during training.\n      * This technique introduces correlations between examples in batches, but they did not observe any problems from that.\n      * They call this technique \"hierarchical sampling\" (first images, then region proposals).\n    * IoUs\n      * Positive examples for specific classes during training are region proposals that have an IoU with ground truth bounding boxes of `>=0.5`.\n      * Examples for background region proposals during training have IoUs with any ground truth box in the interval `(0.1, 0.5]`.\n        * Not picking IoUs below 0.1 is similar to hard negative mining.\n    * They use 25% positive examples, 75% negative/background examples per batch.\n    * They apply horizontal flipping as data augmentation, nothing else.\n  * Outputs\n    * For their class predictions the use a simple softmax with negative log likelihood.\n    * For their bounding box regression they use a smooth L1 loss (similar to mean absolute error, but switches to mean squared error for very low values).\n      * Smooth L1 loss is less sensitive to outliers and less likely to suffer from exploding gradients.\n    * The smooth L1 loss is only active for positive examples (not background examples). (Not active means that it is zero.)\n  * Training schedule\n    * The use SGD.\n    * They train 30k batches with learning rate 0.001, then 0.0001 for another 10k batches. (On Pascal VOC, they use more batches on larger datasets.)\n    * They use twice the learning rate for the biases.\n    * They use momentum of 0.9.\n    * They use parameter decay of 0.0005.\n  * Truncated SVD\n    * The final network for class prediction and bounding box regression has to be applied to every region proposal.\n    * It contains one large fully connected hidden layer and one fully connected output layer (`K+1` classes plus `K*4` regression values).\n    * For 2000 proposals that becomes slow.\n    * So they compress the layers after training to less weights via truncated SVD.\n      * A weights matrix is approximated via ![T-SVD equation](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Fast_R-CNN__tsvd.jpg?raw=true \"T-SVD equation\")\n        * U (`u x t`) are the first `t` left-singular vectors of W.\n        * Sigma is a `t x t` diagonal matrix of the top `t` singular values.\n        * V (`v x t`) are the first `t` right-singular vectors of W.\n      * W is then replaced by two layers: One contains `Sigma V^T` as weights (no biases), the other contains `U` as weights (with original biases).\n      * Parameter count goes down to `t(u+v)` from `uv`.\n\n### Results\n  * They try three base models:\n    * AlexNet (Small, S)\n    * VGG-CNN-M-1024 (Medium, M)\n    * VGG16 (Large, L)\n  * On VGG16 and Pascal VOC 2007, compared to original R-CNN:\n    * Training time down to 9.5h from 84h (8.8x faster).\n    * Test rate *with SVD* (1024 singular values) improves from 47 seconds per image to 0.22 seconds per image (213x faster).\n    * Test rate *without SVD* improves similarly to 0.32 seconds per image.\n    * mAP improves from 66.0% to 66.6% (66.9% without SVD).\n    * Per class accuracy results:\n      * Fast_R-CNN__pvoc2012.jpg\n      * ![VOC2012 results](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Fast_R-CNN__pvoc2012.jpg?raw=true \"VOC2012 results\")\n  * Fixing the weights of VGG16's convolutional layers and only fine-tuning the fully connected layers (those are applied to each region proposal), decreases the accuracy to 61.4%.\n    * This decrease in accuracy is most significant for the later convolutional layers, but marginal for the first layers.\n    * Therefor they only train the convolutional layers starting with `conv3_1` (9 out of 13 layers), which speeds up training.\n  * Multi-task training\n    * Training models on classification and bounding box regression instead of only on classification improves the mAP (from 62.6% to 66.9%).\n    * Doing this in one hierarchy instead of two seperate models (one for classification, one for bounding box regression) increases mAP by roughly 2-3 percentage points.\n  * They did not find a significant benefit of training the model on multiple scales (e.g. same image sometimes at 400x400, sometimes at 600x600, sometimes at 800x800 etc.).\n    * Note that their raw CNN (everything before RoI-Pooling) is fully convolutional, so they can feed the images at any scale through the network.\n  * Increasing the amount of training data seemed to improve mAP a bit, but not as much as one might hope for.\n  * Using a softmax loss instead of an SVM seemed to marginally increase mAP (0-1 percentage points).\n  * Using more region proposals from selective search does not simply increase mAP. Instead it can lead to higher recall, but lower precision.\n    * ![Proposal schemes](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Fast_R-CNN__proposal_schemes.jpg?raw=true \"Proposal schemes\")\n  * Using densely sampled region proposals (as in sliding window) significantly reduces mAP (from 59.2% to 52.9%). If SVMs instead of softmaxes are used, the results are even worse (49.3%).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ICCV.2015.169"
    },
    "885": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/GirshickDDM14",
        "transcript": "  * Previously, methods to detect bounding boxes in images were often based on the combination of manual feature extraction with SVMs.\n  * They replace the manual feature extraction with a CNN, leading to significantly higher accuracy.\n  * They use supervised pre-training on auxiliary datasets to deal with the small amount of labeled data (instead of the sometimes used unsupervised pre-training).\n  * They call their method R-CNN (\"Regions with CNN features\").\n\n### How\n  * Their system has three modules: 1) Region proposal generation, 2) CNN-based feature extraction per region proposal, 3) classification.\n    * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation__architecture.jpg?raw=true \"Architecture\")\n  * Region proposals generation\n    * A region proposal is a bounding box candidate that *might* contain an object.\n    * By default they generate 2000 region proposals per image.\n    * They suggest \"simple\" (i.e. not learned) algorithms for this step (e.g. objectneess, selective search, CPMC).\n    * They use selective search (makes it comparable to previous systems).\n  * CNN features\n    * Uses a CNN to extract features, applied to each region proposal (replaces the previously used manual feature extraction).\n    * So each region proposal ist turned into a fixed length vector.\n    * They use AlexNet by Krizhevsky et al. as their base CNN (takes 227x227 RGB images, converts them into 4096-dimensional vectors).\n    * They add `p=16` pixels to each side of every region proposal, extract the pixels and then simply resize them to 227x227 (ignoring aspect ratio, so images might end up distorted).\n    * They generate one 4096d vector per image, which is less than what some previous manual feature extraction methods used. That enables faster classification, less memory usage and thus more possible classes.\n  * Classification\n    * A classifier that receives the extracted feature vectors (one per region proposal) and classifies them into a predefined set of available classes (e.g. \"person\", \"car\", \"bike\", \"background / no object\").\n    * They use one SVM per available class.\n    * The regions that were not classified as background might overlap (multiple bounding boxes on the same object).\n      * They use greedy non-maximum suppresion to fix that problem (for each class individually).\n      * That method simply rejects regions if they overlap strongly with another region that has higher score.\n      * Overlap is determined via Intersection of Union (IoU).\n  * Training method\n    * Pre-Training of CNN\n      * They use AlexNet pretrained on Imagenet (1000 classes).\n      * They replace the last fully connected layer with a randomly initialized one that leads to `C+1` classes (`C` object classes, `+1` for background).\n    * Fine-Tuning of CNN\n      * The use SGD with learning rate `0.001`.\n      * Batch size is 128 (32 positive windows, 96 background windows).\n      * A region proposal is considered positive, if its IoU with any ground-truth bounding box is `>=0.5`.\n    * SVM\n      * They train one SVM per class via hard negative mining.\n      * For positive examples they use here an IoU threshold of `>=0.3`, which performed better than 0.5.\n\n### Results\n  * Pascal VOC 2010\n    * They: 53.7% mAP\n    * Closest competitor (SegDPM): 40.4% mAP\n    * Closest competitor that uses the same region proposal method (UVA): 35.1% mAP\n    * ![Scores on Pascal VOC 2010](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation__scores.jpg?raw=true \"Scores on Pascal VOC 2010\")\n  * ILSVRC2013 detection\n    * They: 31.4% mAP\n    * Closest competitor (OverFeat): 24.3% mAP\n  * The feed a large number of region proposals through the network and log for each filter in the last conv-layer which images activated it the most:\n    * ![Activations](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation__activations.jpg?raw=true \"Activations\")\n  * Usefulness of layers:\n    * They remove later layers of the network and retrain in order to find out which layers are the most useful ones.\n    * Their result is that both fully connected layers of AlexNet seemed to be very domain-specific and profit most from fine-tuning.\n  * Using VGG16:\n    * Using VGG16 instead of AlexNet increased mAP from 58.5% to 66.0% on Pascal VOC 2007.\n    * Computation time was 7 times higher.\n  * They train a linear regression model that improves the bounding box dimensions based on the extracted features of the last pooling layer. That improved their mAP by 3-4 percentage points.\n  * The region proposals generated by selective search have a recall of 98% on Pascal VOC and 91.6% on ILSVRC2013 (measured by IoU of `>=0.5`).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/CVPR.2014.81"
    },
    "886": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eccv/BenensonOHS14",
        "transcript": "  * They compare the results of various models for pedestrian detection.\n  * The various models were developed over the course of ~10 years (2003-2014).\n  * They analyze which factors seemed to improve the results.\n  * They derive new models for pedestrian detection from that.\n\n### Comparison: Datasets\n  * Available datasets\n    * INRIA: Small dataset. Diverse images.\n    * ETH: Video dataset. Stereo images.\n    * TUD-Brussels: Video dataset.\n    * Daimler: No color channel.\n    * Daimler stereo: Stereo images.\n    * Caltech-USA: Most often used. Large dataset.\n    * KITTI: Often used. Large dataset. Stereo images.\n  * All datasets except KITTI are part of the \"unified evaluation toolbox\" that allows authors to easily test on all of these datasets.\n  * The evaluation started initially with per-window (FPPW) and later changed to per-image (FPPI), because per-window skewed the results.\n  * Common evaluation metrics:\n    * MR: Log-average miss-rate (lower is better)\n    * AUC: Area under the precision-recall curve (higher is better)\n\n### Comparison: Methods\n  * Families\n    * They identified three families of methods: Deformable Parts Models, Deep Neural Networks, Decision Forests.\n    * Decision Forests was the most popular family.\n    * No specific family seemed to perform better than other families.\n    * There was no evidence that non-linearity in kernels was needed (given sophisticated features).\n  * Additional data\n    * Adding (coarse) optical flow data to each image seemed to consistently improve results.\n    * There was some indication that adding stereo data to each image improves the results.\n  * Context\n    * For sliding window detectors, adding context from around the window seemed to improve the results.\n    * E.g. context can indicate whether there were detections next to the window as people tend to walk in groups.\n  * Deformable parts\n    * They saw no evidence that deformable part models outperformed other models.\n  * Multi-Scale models\n    * Training separate models for each sliding window scale seemed to improve results slightly.\n  * Deep architectures\n    * They saw no evidence that deep neural networks outperformed other models. (Note: Paper is from 2014, might have changed already?)\n  * Features\n    * Best performance was usually achieved with simple HOG+LUV features, i.e. by converting each window into:\n      * 6 channels of gradient orientations\n      * 1 channel of gradient magnitude\n      * 3 channels of LUV color space\n    * Some models use significantly more channels for gradient orientations, but there was no evidence that this was necessary to achieve good accuracy.\n    * However, using more different features (and more sophisticated ones) seemed to improve results.\n\n### Their new model:\n  * They choose Decisions Forests as their model framework (2048 level-2 trees, i.e. 3 thresholds per tree).\n  * They use features from the [Integral Channels Features framework](http://pages.ucsd.edu/~ztu/publication/dollarBMVC09ChnFtrs_0.pdf). (Basically just a mixture of common/simple features per window.)\n  * They add optical flow as a feature.\n  * They add context around the window as a feature. (A second detector that detects windows containing two persons.)\n  * Their model significantly improves upon the state of the art (from 34 to 22% MR on Caltech dataset).\n\n\n![Table](https://raw.githubusercontent.com/aleju/papers/master/mixed/images/Ten_Years_of_Pedestrian_Detection_What_Have_We_Learned__table.png?raw=true \"Table\")\n\n*Overview of models developed over the years, starting with Viola Jones (VJ) and ending with their suggested model (Katamari-v1). (DF = Decision Forest, DPM = Deformable Parts Model, DN = Deep Neural Network; I = Inria Dataset, C = Caltech Dataset)*\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-16181-5_47"
    },
    "887": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.08022",
        "transcript": "  * Style transfer between images works - in its original form - by iteratively making changes to a content image, so that its style matches more and more the style of a chosen style image.\n  * That iterative process is very slow.\n  * Alternatively, one can train a single feed-forward generator network to apply a style in one forward pass. The network is trained on a dataset of input images and their stylized versions (stylized versions can be generated using the iterative approach).\n  * So far, these generator networks were much faster than the iterative approach, but their quality was lower.\n  * They describe a simple change to these generator networks to increase the image quality (up to the same level as the iterative approach).\n\n### How\n  * In the generator networks, they simply replace all batch normalization layers with instance normalization layers.\n  * Batch normalization normalizes using the information from the whole batch, while instance normalization normalizes each feature map on its own.\n  * Equations\n    * Let `H` = Height, `W` = Width, `T` = Batch size\n    * Batch Normalization:\n      * ![Batch Normalization Equations](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Instance_Normalization_The_Missing_Ingredient_for_Fast_Stylization__batch_normalization.jpg?raw=true \"Batch Normalization Equations\")\n    * Instance Normalization\n      * ![Instance Normalization Equations](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Instance_Normalization_The_Missing_Ingredient_for_Fast_Stylization__instance_normalization.jpg?raw=true \"Instance Normalization Equations\")\n  * They apply instance normalization at test time too (identically).\n\n### Results\n  * Same image quality as iterative approach (at a fraction of the runtime).\n  * One content image with two different styles using their approach:\n    * ![Example](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Instance_Normalization_The_Missing_Ingredient_for_Fast_Stylization__example.jpg?raw=true \"Example\")",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.08022"
    },
    "888": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1603.06937",
        "transcript": "Official code: https://github.com/anewell/pose-hg-train\n\n  * They suggest a new model architecture for human pose estimation (i.e. \"lay a skeleton over a person\").\n  * Their architecture is based progressive pooling followed by progressive upsampling, creating an hourglass form.\n  * Input are images showing a person's body.\n  * Outputs are K heatmaps (for K body joints), with each heatmap showing the likely position of a single joint on the person (e.g. \"akle\", \"wrist\", \"left hand\", ...).\n\n### How\n  * *Basic building block*\n    * They use residuals as their basic building block.\n    * Each residual has three layers: One 1x1 convolution for dimensionality reduction (from 256 to 128 channels), a 3x3 convolution, a 1x1 convolution for dimensionality increase (back to 256).\n    * Visualized:\n      * ![Building Block](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Stacked_Hourglass_Networks_for_Human_Pose_Estimation__building_block.jpg?raw=true \"Building Block\")\n  * *Architecture*\n    * Their architecture starts with one standard 7x7 convolutions that has strides of (2, 2).\n    * They use MaxPooling (2x2, strides of (2, 2)) to downsample the images/feature maps.\n    * They use Nearest Neighbour upsampling (factor 2) to upsample the images/feature maps.\n    * After every pooling step they add three of their basic building blocks.\n    * Before each pooling step they branch off the current feature map as a minor branch and apply three basic building blocks to it. Then they add it back to the main branch after that one has been upsampeled again to the original size.\n    * The feature maps between each basic building block have (usually) 256 channels.\n    * Their HourGlass ends in two 1x1 convolutions that create the heatmaps.\n    * They stack two of their HourGlass networks after each other. Between them they place an intermediate loss. That way, the second network can learn to improve the predictions of the first network.\n    * Architecture visualized:\n      * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Stacked_Hourglass_Networks_for_Human_Pose_Estimation__architecture.jpg?raw=true \"Architecture\")\n  * *Heatmaps*\n    * The output generated by the network are heatmaps, one per joint.\n    * Each ground truth heatmap has a small gaussian peak at the correct position of a joint, everything else has value 0.\n    * If a joint isn't visible, the ground truth heatmap for that joint is all zeros.\n  * *Other stuff*\n    * They use batch normalization.\n    * Activation functions are ReLUs.\n    * They use RMSprob as their optimizer.\n    * Implemented in Torch.\n\n### Results\n  * They train and test on FLIC (only one HourGlass) and MPII (two stacked HourGlass networks).\n  * Training is done with augmentations (horizontal flip, up to 30 degress rotation, scaling, no translation to keep the body of interest in the center of the image).\n  * Evaluation is done via PCK@0.2 (i.e. percentage of predicted keypoints that are within 0.2 head sizes of their ground truth annotation (head size of the specific body)).\n  * Results on FLIC are at >95%.\n  * Results on MPII are between 80.6% (ankle) and 97.6% (head). Average is 89.4%.\n  * Using two stacked HourGlass networks performs around 3% better than one HourGlass network (even when adjusting for parameters).\n  * Training time was 5 days on a Titan X (9xx generation).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1603.06937"
    },
    "889": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/TaigmanYRW14",
        "transcript": "They describe a CNN architecture that can be used to identify a person given an image of their face.\n\n### How\n  * The expected input is the image of a face (i.e. it does not search for faces in images, the faces already have to be extracted by a different method).\n  * *Face alignment / Frontalization*\n    * Target of this step: Get rid of variations within the face images, so that every face seems to look straight into the camera (\"frontalized\").\n    * 2D alignment\n      * They search for landmarks (fiducial points) on the face.\n        * They use SVRs (features: LBPs) for that.\n        * After every application of the SVR, the localized landmarks are used to transform/normalize the face. Then the SVR is applied again. By doing this, the locations of the landmarks are gradually refined.\n      * They use the detected landmarks to normalize the face images (via scaling, rotation and translation).\n    * 3D alignment\n      * The 2D alignment allows to normalize variations within the 2D-plane, not out-of-plane variations (e.g. seeing that face from its left/right side). To normalize out-of-plane variations they need a 3D transformation.\n      * They detect an additional 67 landmarks on the faces (again via SVRs).\n      * They construct a human face mesh from a dataset (USF Human-ID).\n      * They map the 67 landmarks to that mesh.\n      * They then use some more complicated steps to recover the frontalized face image.\n  * *CNN architecture*\n    * The CNN receives the frontalized face images (152x152, RGB).\n    * It then applies the following steps:\n      * Convolution, 32 filters, 11x11, ReLU (-> 32x142x142, CxHxW)\n      * Max pooling over 3x3, stride 2 (-> 32x71x71)\n      * Convolution, 16 filters, 9x9, ReLU (-> 16x63x63)\n      * Local Convolution, 16 filters, 9x9, ReLU (-> 16x55x55)\n      * Local Convolution, 16 filters, 7x7, ReLU (-> 16x25x25)\n      * Local Convolution, 16 filters, 5x5, ReLU (-> 16x21x21)\n      * Fully Connected, 4096, ReLU\n      * Fully Connected, 4030, Softmax\n    * Local Convolutions use a different set of learned weights at every \"pixel\" (while a normal convolution uses the same set of weights at all locations).\n    * They can afford to use local convolutions because of their frontalization, which roughly forces specific landmarks to be at specific locations.\n    * They use dropout (apparently only after the first fully connected layer).\n    * They normalize \"the features\" (probably the 4096 fully connected layer). Each component is divided by its maximum value across a training set. Additionally, the whole vector is L2-normalized. The goal of this step is to make the network less sensitive to illumination changes.\n    * The whole network has about 120 million parameters.\n    * Visualization of the architecture:\n      * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/DeepFace__architecture.jpg?raw=true \"Architecture\")\n  * *Training*\n    * The network receives images, each showing a face, and is trained to classify the identity of the face (e.g. gets image of Obama, has to return \"that's Obama\").\n    * They use cross-entropy as their loss.\n  * *Face verification*\n    * In order to tell whether two images of faces show the same person they try three different methods.\n    * Each of these relies on the vector extracted by the first fully connected layer in the network (4096d).\n    * Let these vectors be `f1` (image 1) and `f2` (image 2). The methods are then:\n      1. Inner product between `f1` and `f2`. The classification (same person/not same person) is then done by a simple threshold.\n      2. Weighted X^2 (chi-squared) distance. Equation, per vector component i: `weight_i (f1[i] - f2[i])^2 / (f1[i] + f2[i])`. The vector is then fed into an SVM.\n      3. Siamese network. Means here simply that the absolute distance between `f1` and `f2` is calculated (`|f1-f2|`), each component is weighted by a learned weight and then the sum of the components is calculated. If the result is above a threshold, the faces are considered to show the same person.\n\n### Results\n  * They train their network on the Social Face Classification (SFC) dataset. That seems to be a Facebook-internal dataset (i.e. not public) with 4.4 million faces of 4k people.\n  * When applied to the LFW dataset:\n    * Face recognition (\"which person is shown in the image\") (apparently they retrained the whole model on LFW for this task?):\n      * Simple SVM with LBP (i.e. not their network): 91.4% mean accuracy.\n      * Their model, with frontalization, with 2d alignment: ??? no value.\n      * Their model, no frontalization (only 2d alignment): 94.3% mean accuracy.\n      * Their model, no frontalization, no 2d alignment: 87.9% mean accuracy.\n    * Face verification (two images -> same/not same person) (apparently also trained on LFW? unclear):\n      * Method 1 (inner product + threshold): 95.92% mean accuracy.\n      * Method 2 (X^2 vector + SVM): 97.00% mean accurracy.\n      * Method 3 (siamese): Apparently 96.17% accuracy alone, and 97.25% when used in an ensemble with other methods (under special training schedule using SFC dataset).\n  * When applied to the YTF dataset (YouTube video frames):\n    * 92.5% accuracy via X^2-method.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2014.220"
    },
    "890": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/Costa-JussaF16",
        "transcript": "  * Most neural machine translation models currently operate on word vectors or one hot vectors of words.\n  * They instead generate the vector of each word on a character-level.\n    * Thereby, the model can spot character-similarities between words and treat them in a similar way.\n    * They do that only for the source language, not for the target language.\n\n### How\n  * They treat each word of the source text on its own.\n  * To each word they then apply the model from [Character-aware neural language models](https://arxiv.org/abs/1508.06615), i.e. they do per word:\n    * Embed each character into a 620-dimensional space.\n    * Stack these vectors next to each other, resulting in a 2d-tensor in which each column is one of the vectors (i.e. shape `620xN` for `N` characters).\n    * Apply convolutions of size `620xW` to that tensor, where a few different values are used for `W` (i.e. some convolutions cover few characters, some cover many characters).\n    * Apply a tanh after these convolutions.\n    * Apply a max-over-time to the results of the convolutions, i.e. for each convolution use only the maximum value.\n    * Reshape to 1d-vector.\n    * Apply two highway-layers.\n    * They get 1024-dimensional vectors (one per word).\n    * Visualization of their steps:\n      * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Character-based_Neural_Machine_Translation__architecture.jpg?raw=true \"Architecture\")\n  * Afterwards they apply the model from [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) to these vectors, yielding a translation to a target language.\n  * Whenever that translation yields an unknown target-language-word (\"UNK\"), they replace it with the respective (untranslated) word from the source text.\n\n### Results\n  * They the German-English [WMT](http://www.statmt.org/wmt15/translation-task.html) dataset.\n  * BLEU improvemements (compared to neural translation without character-level words):\n    * German-English improves by about 1.5 points.\n    * English-German improves by about 3 points.\n  * Reduction in the number of unknown target-language-words (same baseline again):\n    * German-English goes down from about 1500 to about 1250.\n    * English-German goes down from about 3150 to about 2650.\n  * Translation examples (Phrase = phrase-based/non-neural translation, NN = non-character-based neural translation, CHAR = theirs):\n    * ![Examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Character-based_Neural_Machine_Translation__examples.jpg?raw=true \"Examples\")",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P16/P16-2058.pdf"
    },
    "891": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/WeiRKS16",
        "transcript": "  * They suggest a new model for human pose estimation (i.e. to lay a \"skeleton\" over the image of a person).\n  * Their model has a (more or less) recurrent architecture.\n    * Initial estimates of keypoint locations are refined in several steps.\n    * The idea of the recurrent architecture is derived from message passing, unrolled into one feed-forward model.\n\n### How\n  * Architecture\n    * They generate the end result in multiple steps, similar to a recurrent network.\n    * Step 1:\n      * Receives the image (368x368 resolution).\n      * Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).\n    * Step 2 and later:\n      * (Modified) Receives the image (368x368 resolution) and the previous likelihood scores.\n      * (Same) Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).\n      * (New) Concatenates the likelihoods with the likelihoods of the previous step.\n      * (New) Applies a few more convolutions to the concatenation to compute the final likelihood scores.\n    * Visualization of the architecture:\n      * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Convolutional_Pose_Machines__architecture.jpg?raw=true \"Architecture\")\n  * Loss function\n    * The basic loss function is a simple mean squared error between the expected output maps per keypoint and the predicted ones.\n    * In the expected output maps they mark the correct positions of the keypoints using a small gaussian function.\n    * They apply losses after each step in the architecture, argueing that this helps against vanishing gradients (they don't seem to be using BN).\n    * The expected output maps of the first step actually have the positions of all keypoints of a certain type (e.g. neck) marked, i.e. if there are multiple people in the extracted image patch there might be multiple correct keypoint positions. Only at step 2 and later they reduce that to the expected person (i.e. one keypoint position per map).\n\n### Results\n  * Example results:\n    * ![Example results](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Convolutional_Pose_Machines__results.jpg?raw=true \"Example results\")\n  * Self-correction of predictions over several timesteps:\n    * ![Effect of timesteps](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Convolutional_Pose_Machines__timesteps.jpg?raw=true \"Effect of timesteps\")\n  * They beat existing methods on the datasets MPII, LSP and FLIC.\n  * Applying a loss function after each step (instead of only once after the last step) improved their results and reduced problems related to vanishing gradients.\n  * The effective receptive field size of each step had a significant influence on the results. They increased it to up to 300px (about 80% of the image size) and saw continuous improvements in accuracy.\n    * ![Receptive field size effect](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Convolutional_Pose_Machines__rf_size.jpg?raw=true \"Receptive field size effect\")",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2016.511"
    },
    "892": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/RanjanPC16",
        "transcript": "  * They suggest a single architecture that tries to solve the following tasks:\n    * Face localization (\"Where are faces in the image?\")\n    * Face landmark localization (\"For a given face, where are its landmarks, e.g. eyes, nose and mouth?\")\n    * Face landmark visibility estimation (\"For a given face, which of its landmarks are actually visible and which of them are occluded by other objects/people?\")\n    * Face roll, pitch and yaw estimation (\"For a given face, what is its rotation on the x/y/z-axis?\")\n    * Face gender estimation (\"For a given face, which gender does the person have?\")\n\n### How\n  * *Pretraining the base model*\n    * They start with a basic model following the architecture of AlexNet.\n    * They train that model to classify whether the input images are faces or not faces.\n    * They then remove the fully connected layers, leaving only the convolutional layers.\n  * *Locating bounding boxes of face candidates*\n    * They then use a [selective search and segmentation algorithm](https://www.robots.ox.ac.uk/~vgg/rg/papers/sande_iccv11.pdf) on images to extract bounding boxes of objects.\n    * Each bounding box is considered a possible face.\n    * Each bounding box is rescaled to 227x227.\n  * *Feature extraction per face candidate*\n    * They feed each bounding box through the above mentioned pretrained network.\n    * They extract the activations of the network from the layers `max1` (27x27x96), `conv3` (13x13x384) and `pool5` (6x6x256).\n    * They apply to the first two extracted tensors (from max1, conv3) convolutions so that their tensor shapes are reduced to 6x6xC.\n    * They concatenate the three tensors to a 6x6x768 tensor.\n    * They apply a 1x1 convolution to that tensor to reduce it to 6x6x192.\n    * They feed the result through a fully connected layer resulting in 3072-dimensional vectors (per face candidate).\n  * *Classification and regression*\n    * They feed each 3072-dimensional vector through 5 separate networks:\n      1. Detection: Does the bounding box contain a face or no face. (2 outputs, i.e. yes/no)\n      2. Landmark Localization: What are the coordinates of landmark features (e.g. mouth, nose, ...). (21 landmarks, each 2 values for x/y = 42 outputs total)\n      3. Landmark Visibility: Which landmarks are visible. (21 yes/no outputs)\n      4. Pose estimation: Roll, pitch, yaw of the face. (3 outputs)\n      5. Gender estimation: Male/female face. (2 outputs)\n    * Each of these network contains a single fully connected layer with 512 nodes, followed by the output layer with the above mentioned number of nodes.\n  * *Architecture Visualization*:\n    * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/HyperFace__architecture.jpg?raw=true \"Architecture\")\n  * *Training*\n    * The base model is trained once (see above).\n    * The feature extraction layers and the five classification/regression networks are trained afterwards (jointly).\n    * The loss functions for the five networks are:\n      1. Detection: BCE (binary cross-entropy). Detected bounding boxes that have an overlap `>=0.5` with an annotated face are considered positive samples, bounding boxes with overlap `<0.35` are considered negative samples, everything in between is ignored.\n      2. Landmark localization: Roughly MSE (mean squared error), with some weighting for visibility. Only bounding boxes with overlap `>0.35` are considered. Coordinates are normalized with respect to the bounding boxes center, width and height.\n      3. Landmark visibility: MSE (predicted visibility factor vs. expected visibility factor). Only for bounding boxes with overlap `>0.35`.\n      4. Pose estimation: MSE.\n      5. Gender estimation: BCE.\n  * *Testing*\n    * They use two postprocessing methods for detected faces:\n      * Iterative Region Proposals:\n        * They localize landmarks per face region.\n        * Then they compute a more appropriate face bounding box based on the localized landmarks.\n        * They feed that new bounding box through the network.\n        * They compute the face score (face / not face, i.e. number between 0 and 1) for both bounding boxes and choose the one with the higher score.\n        * This shrinks down bounding boxes that turned out to be too big.\n        * The method visualized:\n          * ![IRP](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/HyperFace__irp.jpg?raw=true \"IRP\")\n      * Landmarks-based Non-Maximum Suppression:\n        * When multiple detected face bounding boxes overlap, one has to choose which of them to keep.\n        * A method to do that is to only keep the bounding box with the highest face-score.\n        * They instead use a median-of-k method.\n        * Their steps are:\n          1. Reduce every box in size so that it is a bounding box around the localized landmarks.\n          2. For every box, find all bounding boxes with a certain amount of overlap.\n          3. Among these bounding boxes, select the `k` ones with highest face score.\n          4. Based on these boxes, create a new box which's size is derived from the median coordinates of the landmarks.\n          5. Compute the median values for landmark coordinates, landmark visibility, gender, pose and use it as the respective values for the new box.\n\n### Results\n  * Example results:\n    * ![Example results](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/HyperFace__example_results.jpg?raw=true \"Example results\")\n  * They test on AFW, AFWL, PASCAL, FDDB, CelebA.\n  * They achieve the best mean average precision values on PASCAL and AFW (compared to selected competitors).\n  * AFW results visualized:\n    * ![AFW](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/HyperFace__afw.jpg?raw=true \"AFW\")\n  * Their approach achieve good performance on FDDB. It has some problems with small and/or blurry faces.\n  * If the feature fusion is removed from their approach (i.e. extracting features only from one fully connected layer at the end of the base network instead of merging feature maps from different convolutional layers), the accuracy of the predictions goes down.\n  * Their architecture ends in 5 shallow networks and shares many layers before them. If instead these networks share no or few layers, the accuracy of the predictions goes down.\n  * The postprocessing of bounding boxes (via Iterative Region Proposals and Landmarks-based Non-Maximum Suppression) has a quite significant influence on the performance.\n  * Processing time per image is 3s, of which 2s is the selective search algorithm (for the bounding boxes).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.01249"
    },
    "893": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icb/ZhongSL16",
        "transcript": "  * When using pretrained networks (like VGG) to solve tasks, one has to use features generated by these networks.\n  * These features come from specific layers, e.g. from the fully connected layers at the end of the network.\n  * They test whether the features from fully connected layers or from the last convolutional layer are better suited for face attribute prediction.\n\n### How\n  * Base networks\n    * They use standard architectures for their test networks, specifically the architectures of FaceNet and VGG (very deep version).\n    * They modify these architectures to both use PReLUs.\n    * They do not use the pretrained weights, instead they train the networks on their own.\n    * They train them on the WebFace dataset (350k images, 10k different identities) to classify the identity of the shown person.\n  * Attribute prediction\n    * After training of the base networks, they train a separate SVM to predict attributes of faces.\n    * The datasets used for this step are CelebA (100k images, 10k identities) and LFWA (13k images, 6k identities).\n    * Each image in these datasets is annotated with 40 binary face attributes.\n    * Examples for attributes: Eyeglasses, bushy eyebrows, big lips, ...\n    * The features for the SVM are extracted from the base networks (i.e. feed forward a face through the network, then take the activations of a specific layer).\n    * The following features are tested:\n      * FC2: Activations of the second fully connected layer of the base network.\n      * FC1: As FC2, but the first fully connected layer.\n      * Spat 3x3: Activations of the last convolutional layer, max-pooled so that their widths and heights are both 3 (i.e. shape Cx3x3).\n      * Spat 1x1: Same as \"Spat 3x3\", but max-pooled to Cx1x1.\n\n### Results\n  * The SVMs trained on \"Spat 1x1\" performed overall worst, the ones trained on \"Spat 3x3\" performed best.\n  * The accuracy order was roughly: `Spat 3x3 > FC1 > FC2 > Spat 1x1`.\n  * This effect was consistent for both networks (VGG, FaceNet) and for other training datasets as well.\n  * FC2 performed particularly bad for the \"blurry\" attribute (most likely because that was unimportant to the classification task).\n  * Accuracy comparison per attribute:\n    * ![Comparison](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Face_Attribute_Prediction_Using_Off-the-Shelf_CNN_Features__comparison.png?raw=true \"Comparison\")\n  * The conclusion is, that when using pretrained networks one should not only try the last fully connected layer. Many characteristics of the input image might not appear any more in that layer (and later ones in general) as they were unimportant to the classification task.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ICB.2016.7550092"
    },
    "894": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZhuZLS16",
        "transcript": "  * They describe a model to locate faces in images.\n  * Their model uses information from suspected face regions *and* from the corresponding suspected body regions to classify whether a region contains a face.\n  * The intuition is, that seeing the region around the face (specifically where the body should be) can help in estimating whether a suspected face is really a face (e.g. it might also be part of a painting, statue or doll).\n\n### How\n  * Their whole model is called \"CMS-RCNN\" (Contextual Multi-Scale Region-CNN).\n  * It is based on the \"Faster R-CNN\" architecture.\n  * It uses the VGG network.\n  * Subparts of their model are: MS-RPN, CMS-CNN.\n  * MS-RPN finds candidate face regions. CMS-CNN refines their bounding boxes and classifies them (face / not face).\n  * **MS-RPN** (Multi-Scale Region Proposal Network)\n    * \"Looks\" at the feature maps of the network (VGG) at multiple scales (i.e. before/after pooling layers) and suggests regions for possible faces.\n    * Steps:\n      * Feed an image through the VGG network.\n      * Extract the feature maps of the three last convolutions that are before a pooling layer.\n      * Pool these feature maps so that they have the same heights and widths.\n      * Apply L2 normalization to each feature map so that they all have the same scale.\n      * Apply a 1x1 convolution to merge them to one feature map.\n      * Regress face bounding boxes from that feature map according to the Faster R-CNN technique.\n  * **CMS-CNN** (Contextual Multi-Scale CNN):\n    * \"Looks\" at feature maps of face candidates found by MS-RPN and classifies whether these regions contains faces.\n    * It also uses the same multi-scale technique (i.e. take feature maps from convs before pooling layers).\n    * It uses some area around these face regions as additional information (suspected regions of bodies).\n    * Steps:\n      * Receive face candidate regions from MS-RPN.\n      * Do per candidate region:\n        * Calculate the suspected coordinates of the body (only based on the x/y-position and size of the face region, i.e. not learned).\n        * Extract the feature maps of the *face* region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).\n        * Extract the feature maps of the *body* region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).\n        * L2-normalize each feature map.\n        * Concatenate the (RoI-pooled and normalized) feature maps of the face (at multiple scales) with each other (creates one tensor).\n        * Concatenate the (RoI-pooled and normalized) feature maps of the body (at multiple scales) with each other (creates another tensor).\n        * Apply a 1x1 convolution to the face tensor.\n        * Apply a 1x1 convolution to the body tensor.\n        * Apply two fully connected layers to the face tensor, creating a vector.\n        * Apply two fully connected layers to the body tensor, creating a vector.\n        * Concatenate both vectors.\n        * Based on that vector, make a classification of whether it is really a face.\n        * Based on that vector, make a regression of the face's final bounding box coordinates and dimensions.\n  * Note: They use in both networks the multi-scale approach in order to be able to find small or tiny faces. Otherwise, after pooling these small faces would be hard or impossible to detect.\n\n### Results\n  * Adding context to the classification (i.e. the body regions) empirically improves the results.\n  * Their model achieves the highest recall rate on FDDB compared to other models. However, it has lower recall if only very few false positives are accepted.\n  * FDDB ROC curves (theirs is bold red):\n    * ![FDDB results](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/CMS-RCNN__fddb.jpg?raw=true \"FDDB results\")\n  * Example results on FDDB:\n    * ![FDDB examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/CMS-RCNN__examples.jpg?raw=true \"FDDB examples\")\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.05413"
    },
    "895": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.05328",
        "transcript": "  * PixelRNN\n    * PixelRNNs generate new images pixel by pixel (and row by row) via LSTMs (or other RNNs).\n    * Each pixel is therefore conditioned on the previously generated pixels.\n    * Training of PixelRNNs is slow due to the RNN-architecture (hard to parallelize).\n    * Previously PixelCNNs have been suggested, which use masked convolutions during training (instead of RNNs), but their image quality was worse.\n  * They suggest changes to PixelCNNs that improve the quality of the generated images (while still keeping them faster than RNNs).\n\n### How\n  * PixelRNNs split up the distribution `p(image)` into many conditional probabilities, one per pixel, each conditioned on all previous pixels: `p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1)`.\n  * PixelCNNs implement that using convolutions, which are faster to train than RNNs.\n    * These convolutions uses masked filters, i.e. the center weight and also all weights right and/or below the center pixel are `0` (because they are current/future values and we only want to condition on the past).\n    * In most generative models, several layers are stacked, ultimately ending in three float values per pixel (RGB images, one value for grayscale images). PixelRNNs (including this implementation) traditionally end in a softmax over 255 values per pixel and channel (so `3*255` per RGB pixel).\n    * The following image shows the application of such a convolution with the softmax output (left) and the mask for a filter (right):\n      * ![Masked convolution](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Conditional_Image_Generation_with_PixelCNN_Decoders__masked_convolution.png?raw=true \"Masked convolution\")\n  * Blind spot\n    * Using the mask on each convolutional filter effectively converts them into non-squared shapes (the green values in the image).\n    * Advantage: Using such non-squared convolutions prevents future values from leaking into present values.\n    * Disadvantage: Using such non-squared convolutions creates blind spots, i.e. for each pixel, some past values (diagonally top-right from it) cannot influence the value of that pixel.\n      * ![Blind spot](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Conditional_Image_Generation_with_PixelCNN_Decoders__blind_spot.png?raw=true \"Blind Spot\")\n    * They combine horizontal (1xN) and vertical (Nx1) convolutions to prevent that.\n  * Gated convolutions\n    * PixelRNNs via LSTMs so far created visually better images than PixelCNNs.\n    * They assume that one advantage of LSTMs is, that they (also) have multiplicative gates, while stacked convolutional layers only operate with summations.\n    * They alleviate that problem by adding gates to their convolutions:\n      * Equation: `output image = tanh(weights_1 * image) <element-wise product> sigmoid(weights_2 * image)`\n        * `*` is the convolutional operator.\n        * `tanh(weights_1 * image)` is a classical convolution with tanh activation function.\n        * `sigmoid(weights_2 * image)` are the gate values (0 = gate closed, 1 = gate open).\n        * `weights_1` and `weights_2` are learned.\n  * Conditional PixelCNNs\n    * When generating images, they do not only want to condition the previous values, but also on a laten vector `h` that describes the image to generate.\n    * The new image distribution becomes: `p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1, h)`.\n    * To implement that, they simply modify the previously mentioned gated convolution, adding `h` to it:\n      * Equation: `output image = tanh(weights_1 * image + weights_2 . h) <element-wise product> sigmoid(weights_3 * image + weights_4 . h)`\n        * `.` denotes here the matrix-vector multiplication.\n  * PixelCNN Autoencoder\n    * The decoder in a standard autoencoder can be replaced by a PixelCNN, creating a PixelCNN-Autoencoder.\n\n### Results\n  * They achieve similar NLL-results as PixelRNN on CIFAR-10 and ImageNet, while training about twice as fast.\n    * Here, \"fast\" means that they used 32 GPUs for 60 hours.\n  * Using Conditional PixelCNNs on ImageNet (i.e. adding class information to each convolution) did not improve the NLL-score, but it did improve the image quality.\n    * ![ImageNet](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Conditional_Image_Generation_with_PixelCNN_Decoders__imagenet.png?raw=true \"ImageNet\")\n  * They use a different neural network to create embeddings of human faces. Then they generate new faces based on these embeddings via PixelCNN.\n    * ![Portraits](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Conditional_Image_Generation_with_PixelCNN_Decoders__portraits.png?raw=true \"Portraits\")\n  * Their PixelCNN-Autoencoder generates significantly sharper (i.e. less blurry) images than a \"normal\" autoencoder.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.05328"
    },
    "896": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ChenCDHSSA16",
        "transcript": "  * Usually GANs transform a noise vector `z` into images. `z` might be sampled from a normal or uniform distribution.\n  * The effect of this is, that the components in `z` are deeply entangled.\n    * Changing single components has hardly any influence on the generated images. One has to change multiple components to affect the image.\n    * The components end up not being interpretable. Ideally one would like to have meaningful components, e.g. for human faces one that controls the hair length and a categorical one that controls the eye color.\n  * They suggest a change to GANs based on Mutual Information, which leads to interpretable components.\n    * E.g. for MNIST a component that controls the stroke thickness and a categorical component that controls the digit identity (1, 2, 3, ...).\n    * These components are learned in a (mostly) unsupervised fashion.\n\n### How\n  * The latent code `c`\n    * \"Normal\" GANs parameterize the generator as `G(z)`, i.e. G receives a noise vector and transforms it into an image.\n    * This is changed to `G(z, c)`, i.e. G now receives a noise vector `z` and a latent code `c` and transforms both into an image.\n    * `c` can contain multiple variables following different distributions, e.g. in MNIST a categorical variable for the digit identity and a gaussian one for the stroke thickness.\n  * Mutual Information\n    * If using a latent code via `G(z, c)`, nothing forces the generator to actually use `c`. It can easily ignore it and just deteriorate to `G(z)`.\n    * To prevent that, they force G to generate images `x` in a way that `c` must be recoverable. So, if you have an image `x` you must be able to reliable tell which latent code `c` it has, which means that G must use `c` in a meaningful way.\n    * This relationship can be expressed with mutual information, i.e. the mutual information between `x` and `c` must be high.\n      * The mutual information between two variables X and Y is defined as `I(X; Y) = entropy(X) - entropy(X|Y) = entropy(Y) - entropy(Y|X)`.\n      * If the mutual information between X and Y is high, then knowing Y helps you to decently predict the value of X (and the other way round).\n      * If the mutual information between X and Y is low, then knowing Y doesn't tell you much about the value of X (and the other way round).\n    * The new GAN loss becomes `old loss - lambda * I(G(z, c); c)`, i.e. the higher the mutual information, the lower the result of the loss function.\n  * Variational Mutual Information Maximization\n    * In order to minimize `I(G(z, c); c)`, one has to know the distribution `P(c|x)` (from image to latent code), which however is unknown.\n    * So instead they create `Q(c|x)`, which is an approximation of `P(c|x)`.\n    * `I(G(z, c); c)` is then computed using a lower bound maximization, similar to the one in variational autoencoders (called \"Variational Information Maximization\", hence the name \"InfoGAN\").\n    * Basic equation: `LowerBoundOfMutualInformation(G, Q) = E[log Q(c|x)] + H(c) <= I(G(z, c); c)`\n      * `c` is the latent code.\n      * `x` is the generated image.\n      * `H(c)` is the entropy of the latent codes (constant throughout the optimization).\n      * Optimization w.r.t. Q is done directly.\n      * Optimization w.r.t. G is done via the reparameterization trick.\n    * If `Q(c|x)` approximates `P(c|x)` *perfectly*, the lower bound becomes the mutual information (\"the lower bound becomes tight\").\n    * In practice, `Q(c|x)` is implemented as a neural network. Both Q and D have to process the generated images, which means that they can share many convolutional layers, significantly reducing the extra cost of training Q.\n\n### Results\n  * MNIST\n    * They use for `c` one categorical variable (10 values) and two continuous ones (uniform between -1 and +1).\n    * InfoGAN learns to associate the categorical one with the digit identity and the continuous ones with rotation and width.\n    * Applying Q(c|x) to an image and then classifying only on the categorical variable (i.e. fully unsupervised) yields 95% accuracy.\n    * Sampling new images with exaggerated continuous variables in the range `[-2,+2]` yields sound images (i.e. the network generalizes well).\n    * ![MNIST examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/InfoGAN__mnist.png?raw=true \"MNIST examples\")\n  * 3D face images\n    * InfoGAN learns to represent the faces via pose, elevation, lighting.\n    * They used five uniform variables for `c`. (So two of them apparently weren't associated with anything sensible? They are not mentioned.)\n  * 3D chair images\n    * InfoGAN learns to represent the chairs via identity (categorical) and rotation or width (apparently they did two experiments).\n    * They used one categorical variable (four values) and one continuous variable (uniform `[-1, +1]`).\n  * SVHN\n    * InfoGAN learns to represent lighting and to spot the center digit.\n    * They used four categorical variables (10 values each) and two continuous variables (uniform `[-1, +1]`). (Again, a few variables were apparently not associated with anything sensible?)\n  * CelebA\n    * InfoGAN learns to represent pose, presence of sunglasses (not perfectly), hair style and emotion (in the sense of \"smiling or not smiling\").\n    * They used 10 categorical variables (10 values each). (Again, a few variables were apparently not associated with anything sensible?)\n    * ![CelebA examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/InfoGAN__celeba.png?raw=true \"CelebA examples\")",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets"
    },
    "897": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SalimansGZCRC16",
        "transcript": "  * They suggest some small changes to the GAN training scheme that lead to visually improved results.\n  * They suggest a new scoring method to compare the results of different GAN models with each other.\n\n### How\n  * Feature Matching\n    * Usually G would be trained to mislead D as often as possible, i.e. to maximize D's output.\n    * Now they train G to minimize the feature distance between real and fake images. I.e. they do:\n      1. Pick a layer $l$ from D.\n      2. Forward real images through D and extract the features from layer $l$.\n      3. Forward fake images through D and extract the features from layer $l$.\n      4. Compute the squared euclidean distance between the layers and backpropagate.\n  * Minibatch discrimination\n    * They allow D to look at multiple images in the same minibatch.\n    * That is, they feed the features (of each image) extracted by an intermediate layer of D through a linear operation, resulting in a matrix per image.\n    * They then compute the L1-distances between these matrices.\n    * They then let D make its judgement (fake/real image) based on the features extracted from the image and these distances.\n    * They add this mechanism so that the diversity of images generated by G increases (which should also prevent collapses).\n  * Historical averaging\n    * They add a penalty term that punishes weights which are rather far away from their historical average values.\n    * I.e. the cost is `distance(current parameters, average of parameters over the last t batches)`.\n    * They argue that this can help the network to find equilibria that normal gradient descent would not find.\n  * One-sided label smoothing\n    * Usually one would use the labels 0 (image is fake) and 1 (image is real).\n    * Using smoother labels (0.1 and 0.9) seems to make networks more resistent to adversarial examples.\n    * So they smooth the labels of real images (apparently to 0.9?).\n    * Smoothing the labels of fake images would lead to (mathematical) problems in some cases, so they keep these at 0.\n  * Virtual Batch Normalization (VBN)\n    * Usually BN normalizes each example with respect to the other examples in the same batch.\n    * They instead normalize each example with respect to the examples in a reference batch, which was picked once at the start of the training.\n    * VBN is intended to reduce the dependence of each example on the other examples in the batch.\n    * VBN is computationally expensive, because it requires forwarding of two minibatches.\n    * They use VBN for their G.\n  * Inception Scoring\n    * They introduce a new scoring method for GAN results.\n    * Their method is based on feeding the generated images through another network, here they use Inception.\n    * For an image `x` and predicted classes `y` (softmax-output of Inception):\n      * They argue that they want `p(y|x)` to have low entropy, i.e. the model should be rather certain of seeing a class (or few classes) in the image.\n      * They argue that they want `p(y)` to have high entropy, i.e. the predicted classes (and therefore image contents) should have high diversity. (This seems like something that is quite a bit dependend on the used dataset?)\n      * They combine both measurements to the final score of `exp(KL(p(y|x) || p(y))) = exp( <sum over images> p(y|xi) * (log(p(y|xi)) - log(p(y))) )`.\n        * `p(y)` can be approximated as the mean of the softmax-outputs over many examples.\n      * Relevant python code that they use (where `part` seems to be of shape `(batch size, number of classes)`, i.e. the softmax outputs): `kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0))); kl = np.mean(np.sum(kl, 1)); scores.append(np.exp(kl));`\n    * They average this score over 50,000 generated images.\n  * Semi-supervised Learning\n    * For a dataset with K classes they extend D by K outputs (leading to K+1 outputs total).\n    * They then optimize two loss functions jointly:\n      * Unsupervised loss: The classic GAN loss, i.e. D has to predict the fake/real output correctly. (The other outputs seem to not influence this loss.)\n      * Supervised loss: D must correctly predict the image's class label, if it happens to be a real image and if it was annotated with a class.\n    * They note that training G with feature matching produces the best results for semi-supervised classification.\n    * They note that training G with minibatch discrimination produces significantly worse results for semi-supervised classification. (But visually the samples look better.)\n    * They note that using semi-supervised learning overall results in higher image quality than not using it. They speculate that this has to do with the class labels containing information about image statistics that are important to humans.\n\n### Results\n  * MNIST\n    * They use weight normalization and white noise in D.\n    * Samples of high visual quality when using minibatch discrimination with semi-supervised learning.\n    * Very good results in semi-supervised learning when using feature matching.\n    * Using feature matching decreases visual quality of generated images, but improves results of semi-supervised learning.\n  * CIFAR-10\n    * D: 9-layer CNN with dropout, weight normalization.\n    * G: 4-layer CNN with batch normalization (so no VBN?).\n    * Visually very good generated samples when using minibatch discrimination with semi-supervised learning. (Probably new record quality.)\n      * Note: No comparison with nearest neighbours from the dataset.\n    * When using feature matching the results are visually not as good.\n    * Again, very good results in semi-supervised learning when using feature matching.\n  * SVHN\n    * Same setup as in CIFAR-10 and similar results.\n  * ImageNet\n    * They tried to generate 128x128 images and compared to DCGAN.\n    * They improved from \"total garbage\" to \"garbage\" (they now hit some textures, but structure is still wildly off).\n\n\n![CIFAR-10 Examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Improved_Techniques_for_Training_GANs__cifar.jpg?raw=true \"CIFAR-10 Examples\")\n\n*Generated CIFAR-10-like images (with minibatch discrimination and semi-supervised learning).*",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.03498"
    },
    "898": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/NguyenDYBC16",
        "transcript": "  * They suggest a new method to generate images which maximize the activation of a specific neuron in a (trained) target network (abbreviated with \"**DNN**\").\n    * E.g. if your DNN contains a neuron that is active whenever there is a car in an image, the method should generate images containing cars.\n    * Such methods can be used to investigate what exactly a network has learned.\n  * There are plenty of methods like this one. They usually differ from each other by using different *natural image priors*.\n    * A natural image prior is a restriction on the generated images.\n    * Such a prior pushes the generated images towards realistic looking ones.\n    * Without such a prior it is easy to generate images that lead to high activations of specific neurons, but don't look realistic at all (e.g. they might look psychodelic or like white noise).\n      * That's because the space of possible images is extremely high-dimensional and can therefore hardly be covered reliably by a single network. Note also that training datasets usually only show a very limited subset of all possible images.\n    * Their work introduces a new natural image prior.\n\n### How\n  * Usually, if one wants to generate images that lead to high activations, the basic/naive method is to:\n    1. Start with a noise image,\n    2. Feed that image through DNN,\n    3. Compute an error that is high if the activation of the specified neuron is low (analogous for high activation),\n    4. Backpropagate the error through DNN,\n    5. Change the noise image according to the gradient,\n    6. Repeat.\n  * So, the noise image is basically treated like weights in the network.\n  * Their alternative method is based on a Generator network **G**.\n    * That G is trained according to the method described in [Generating Images with Perceptual Similarity Metrics based on Deep Networks].\n    * Very rough outline of that method:\n      * First, a pretrained network **E** is given (they picked CaffeNet, which is a variation of AlexNet).\n      * G then has to learn to inverse E, i.e. G receives per image the features extracted by a specific layer in E (e.g. the last fully connected layer before the output) and has to generate (recreate) the image from these features.\n  * Their modified steps are:\n    1. *(New step)* Start with a noise vector,\n    2. *(New step)* Feed that vector through G resulting in an image,\n    3. *(Same)* Feed that image through DNN,\n    4. *(Same)* Compute an error that is low if the activation of the specified neuron is high (analogous for low activations),\n    5. *(Same)* Backpropagate the error through DNN,\n    6. *(Modified)* Change the noise *vector* according to the gradient,\n    7. *(Same)* Repeat.\n  * Visualization of their architecture:\n    * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Synthesizing_the_preferred_inputs_for_neurons_in_neural_networks_via_deep_generator_networks__architecture.jpg?raw=true \"Architecture\")\n  * Additionally they do:\n    * Apply an L2 norm to the noise vector, which adds pressure to each component to take low values. They say that this improved the results.\n    * Clip each component of the noise vector to a range `[0, a]`, which improved the results significantly.\n      * The range starts at `0`, because the network (E) inverted by their Generator (G) is based on ReLUs.\n      * `a` is derived from test images fed through E and set to 3 standard diviations of the mean activation of that component (recall that the \"noise\" vector mirrors a specific layer in E).\n      * They argue that this clipping is similar to a prior on the noise vector components. That prior reflects likely values of the layer in E that is used for the noise vector.\n\n### Results\n  * Examples of generated images:\n    * ![Examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Synthesizing_the_preferred_inputs_for_neurons_in_neural_networks_via_deep_generator_networks__examples.jpg?raw=true \"Examples\")\n  * Early vs. late layers\n    * For G they have to pick a specific layer from E that G has to invert. They found that using \"later\" layers (e.g. the fully connected layers at the end) produced images with more reasonable overall structure than using \"early\" layers (e.g. first convolutional layers). Early layers led to repeating structures.\n  * Datasets and architectures\n    * Both G and DNN have to be trained on datasets.\n    * They found that these networks can actually be trained on different datasets, the results will still look good.\n    * However, they found that the architectures of DNN and E should be similar to create the best looking images (though this might also be down to depth of the tested networks).\n  * Verification that the prior can generate any image\n    * They tested whether the generated images really show what the DNN-neurons prefer and not what the Generator/prior prefers.\n    * To do that, they retrained DNNs on images that were both directly from the dataset as well as images that were somehow modified.\n    * Those modifications were:\n      * Treated RGB images as if they were BGR (creating images with weird colors).\n      * Copy-pasted areas in the images around (creating mosaics).\n      * Blurred the images (with gaussian blur).\n    * The DNNs were then trained to classify the \"normal\" images into 1000 classes and the modified images into 1000 other classes (2000 total).\n    * So at the end there were (in the same DNN) neurons reacting strongly to specific classes of unmodified images and other neurons that reacted strongly to specific classes of modified images.\n    * When generating images to maximize activations of specific neurons, the Generator was able to create both modified and unmodified images. Though it seemed to have some trouble with blurring.\n    * That shows that the generated images probably indeed show what the DNN has learned and not just what G has learned.\n  * Uncanonical images\n    * The method can sometimes generate uncanonical images (e.g. instead of a full dog just blobs of texture).\n    * They found that this seems to be mostly the case when the dataset images have uncanonical pose, i.e. are very diverse/multi-modal.\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6519-synthesizing-the-preferred-inputs-for-neurons-in-neural-networks-via-deep-generator-networks"
    },
    "899": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LarssonMS16a",
        "transcript": "  * They describe an architecture for deep CNNs that contains short and long paths. (Short = few convolutions between input and output, long = many convolutions between input and output)\n  * They achieve comparable accuracy to residual networks, without using residuals.\n\n### How\n  * Basic principle:\n    * They start with two branches. The left branch contains one convolutional layer, the right branch contains a subnetwork.\n    * That subnetwork again contains a left branch (one convolutional layer) and a right branch (a subnetwork).\n    * This creates a recursion.\n    * At the last step of the recursion they simply insert two convolutional layers as the subnetwork.\n    * Each pair of branches (left and right) is merged using a pair-wise mean. (Result: One of the branches can be skipped or removed and the result after the merge will still be sound.)\n    * Their recursive expansion rule (left) and architecture (middle and right) visualized:\n      ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/FractalNet_Ultra-Deep_Networks_without_Residuals__architecture.png?raw=true \"Architecture\")\n  * Blocks:\n    * Each of the recursively generated networks is one block.\n    * They chain five blocks in total to create the network that they use for their experiments.\n    * After each block they add a max pooling layer.\n    * Their first block uses 64 filters per convolutional layer, the second one 128, followed by 256, 512 and again 512.\n  * Drop-path:\n    * They randomly dropout whole convolutional layers between merge-layers.\n    * They define two methods for that:\n      * Local drop-path: Drops each input to each merge layer with a fixed probability, but at least one always survives. (See image, first three examples.)\n      * Global drop-path: Drops convolutional layers so that only a single columns (and thereby path) in the whole network survives. (See image, right.)\n    * Visualization:\n      ![Drop-path](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/FractalNet_Ultra-Deep_Networks_without_Residuals__drop_path.png?raw=true \"Drop-path\")\n\n### Results\n  * They test on CIFAR-10, CIFAR-100 and SVHN with no or mild (crops, flips) augmentation.\n  * They add dropout at the start of each block (probabilities: 0%, 10%, 20%, 30%, 40%).\n  * They use for 50% of the batches local drop-path at 15% and for the other 50% global drop-path.\n  * They achieve comparable accuracy to ResNets (a bit behind them actually).\n    * Note: The best ResNet that they compare to is \"ResNet with Identity Mappings\". They don't compare to Wide ResNets, even though they perform best.\n  * If they use image augmentations, dropout and drop-path don't seem to provide much benefit (only small improvement).\n  * If they extract the deepest column and test on that one alone, they achieve nearly the same performance as with the whole network.\n    * They derive from that, that their fractal architecture is actually only really used to help that deepest column to learn anything. (Without shorter paths it would just learn nothing due to vanishing gradients.)\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1605.07648"
    },
    "900": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eccv/WeyandKP16",
        "transcript": "  * They describe a convolutional network that takes in photos and returns where (on the planet) these photos were likely made.\n  * The output is a distribution over locations around the world (so not just one single location). This can be useful in the case of ambiguous images.\n\n### How\n  * Basic architecture\n    * They simply use the Inception architecture for their model.\n    * They have 97M parameters.\n  * Grid\n    * The network uses a grid of cells over the planet.\n    * For each photo and every grid cell it returns the likelihood that the photo was made within the region covered by the cell (simple softmax layer).\n    * The naive way would be to use a regular grid around the planet (i.e. a grid in which all cells have the same size).\n      * Possible disadvantages:\n        * In places where lots of photos are taken you still have the same grid cell size as in places where barely any photos are taken.\n        * Maps are often distorted towards the poles (countries are represented much larger than they really are). This will likely affect the grid cells too.\n    * They instead use an adaptive grid pattern based on S2 cells.\n      * S2 cells interpret the planet as a sphere and project a cube onto it.\n      * The 6 sides of the cube are then partitioned using quad trees, creating the grid cells.\n      * They don't use the same depth for all quad trees. Instead they subdivide them only if their leafs contain enough photos (based on their dataset of geolocated images).\n      * They remove some cells for which their dataset does not contain enough images, e.g. cells on oceans. (They also remove these images from the dataset. They don't say how many images are affected by this.)\n      * They end up with roughly 26k cells, some of them reaching the street level of major cities.\n      * Visualization of their cells:\n        ![S2 cells](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/PlaNet__S2.jpg?raw=true \"S2 cells\")\n  * Training\n    * For each example photo that they feed into the network, they set the correct grid cell to `1.0` and all other grid cells to `0.0`.\n    * They train on a dataset of 126M images with Exif geolocation information. The images were collected from all over the web.\n    * They used Adagrad.\n    * They trained on 200 CPUs for 2.5 months.\n  * Album network\n    * For photo albums they develop variations of their network.\n    * They do that because albums often contain images that are very hard to geolocate on their own, but much easier if the other images of the album are seen.\n    * They use LSTMs for their album network.\n    * The simplest one just iterates over every photo, applies their previously described model to it and extracts the last layer (before output) from that model. These vectors (one per image) are then fed into an LSTM, which is trained to predict (again) the grid cell location per image.\n    * More complicated versions use multiple passes or are bidirectional LSTMs (to use the information from the last images to classify the first ones in the album).\n\n### Results\n  * They beat previous models (based on hand-engineered features or nearest neighbour methods) by a significant margin.\n  * In a small experiment they can beat experienced humans in geoguessr.com.\n  * Based on a dataset of 2.3M photos from Flickr, their method correctly predicts the country where the photo was made in 30% of all cases (top-1; top-5: about 50%). City-level accuracy is about 10% (top-1; top-5: about 18%).\n  * Example predictions (using in coarser grid with 354 cells):\n    ![Examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/PlaNet__examples.png?raw=true \"Examples\")\n  * Using the LSTM-technique for albums significantly improves prediction accuracy for these images.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-46484-8_3"
    },
    "901": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/KingmaB14",
        "transcript": "  * They suggest a new stochastic optimization method, similar to the existing SGD, Adagrad or RMSProp.\n    * Stochastic optimization methods have to find parameters that minimize/maximize a stochastic function.\n    * A function is stochastic (non-deterministic), if the same set of parameters can generate different results. E.g. the loss of different mini-batches can differ, even when the parameters remain unchanged. Even for the same mini-batch the results can change due to e.g. dropout.\n  * Their method tends to converge faster to optimal parameters than the existing competitors.\n  * Their method can deal with non-stationary distributions (similar to e.g. SGD, Adadelta, RMSProp).\n  * Their method can deal with very sparse or noisy gradients (similar to e.g. Adagrad).\n\n### How\n  * Basic principle\n    * Standard SGD just updates the parameters based on `parameters = parameters - learningRate * gradient`.\n    * Adam operates similar to that, but adds more \"cleverness\" to the rule.\n    * It assumes that the gradient values have means and variances and tries to estimate these values.\n      * Recall here that the function to optimize is stochastic, so there is some randomness in the gradients.\n      * The mean is also called \"the first moment\".\n      * The variance is also called \"the second (raw) moment\".\n    * Then an update rule very similar to SGD would be `parameters = parameters - learningRate * means`.\n    * They instead use the update rule `parameters = parameters - learningRate * means/sqrt(variances)`.\n      * They call `means/sqrt(variances)` a 'Signal to Noise Ratio'.\n      * Basically, if the variance of a specific parameter's gradient is high, it is pretty unclear how it should be changend. So we choose a small step size in the update rule via `learningRate * mean/sqrt(highValue)`.\n      * If the variance is low, it is easier to predict how far to \"move\", so we choose a larger step size via `learningRate * mean/sqrt(lowValue)`.\n  * Exponential moving averages\n    * In order to approximate the mean and variance values you could simply save the last `T` gradients and then average the values.\n    * That however is a pretty bad idea, because it can lead to high memory demands (e.g. for millions of parameters in CNNs).\n    * A simple average also has the disadvantage, that it would completely ignore all gradients before `T` and weight all of the last `T` gradients identically. In reality, you might want to give more weight to the last couple of gradients.\n    * Instead, they use an exponential moving average, which fixes both problems and simply updates the average at every timestep via the formula `avg = alpha * avg + (1 - alpha) * avg`.\n    * Let the gradient at timestep (batch) `t` be `g`, then we can approximate the mean and variance values using:\n      * `mean = beta1 * mean + (1 - beta1) * g`\n      * `variance = beta2 * variance + (1 - beta2) * g^2`.\n      * `beta1` and `beta2` are hyperparameters of the algorithm. Good values for them seem to be `beta1=0.9` and `beta2=0.999`.\n      * At the start of the algorithm, `mean` and `variance` are initialized to zero-vectors.\n  * Bias correction\n    * Initializing the `mean` and `variance` vectors to zero is an easy and logical step, but has the disadvantage that bias is introduced.\n    * E.g. at the first timestep, the mean of the gradient would be `mean = beta1 * 0 + (1 - beta1) * g`, with `beta1=0.9` then: `mean = 0.9 * g`. So `0.9g`, not `g`. Both the mean and the variance are biased (towards 0).\n    * This seems pretty harmless, but it can be shown that it lowers the convergence speed of the algorithm by quite a bit.\n    * So to fix this pretty they perform bias-corrections of the mean and the variance:\n      * `correctedMean = mean / (1-beta1^t)` (where `t` is the timestep).\n      * `correctedVariance = variance / (1-beta2^t)`.\n      * Both formulas are applied at every timestep after the exponential moving averages (they do not influence the next timestep).\n\n![Algorithm](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Adam__algorithm.png?raw=true \"Algorithm\")",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1412.6980"
    },
    "902": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1602.05110",
        "transcript": "* What\n  * They describe a new architecture for GANs.\n  * The architecture is based on letting the Generator (G) create images in multiple steps, similar to DRAW.\n  * They also briefly suggest a method to compare the quality of the results of different generators with each other.\n\n* How\n  * In a classic GAN one samples a noise vector `z`, feeds that into a Generator (`G`), which then generates an image `x`, which is then fed through the Discriminator (`D`) to estimate its quality.\n  * Their method operates in basically the same way, but internally G is changed to generate images in multiple time steps.\n  * Outline of how their G operates:\n    * Time step 0:\n      * Input: Empty image `delta C-1`, randomly sampled `z`.\n      * Feed `delta C-1` through a number of downsampling convolutions to create a tensor. (Not very useful here, as the image is empty. More useful in later timesteps.)\n      * Feed `z` through a number of upsampling convolutions to create a tensor (similar to DCGAN).\n      * Concat the output of the previous two steps.\n      * Feed that concatenation through a few more convolutions.\n      * Output: `delta C0` (changes to apply to the empty starting canvas).\n    * Time step 1 (and later):\n      * Input: Previous change `delta C0`, randomly sampled `z` (can be the same as in step 0).\n      * Feed `delta C0` through a number of downsampling convolutions to create a tensor.\n      * Feed `z` through a number of upsampling convolutions to create a tensor (similar to DCGAN).\n      * Concat the output of the previous two steps.\n      * Feed that concatenation through a few more convolutions.\n      * Output: `delta C1` (changes to apply to the empty starting canvas).\n    * At the end, after all timesteps have been performed:\n      * Create final output image by summing all the changes, i.e. `delta C0 + delta C1 + ...`, which basically means `empty start canvas + changes from time step 0 + changes from time step 1 + ...`.\n  * Their architecture as an image:\n    * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Generating_Images_with_Recurrent_Adversarial_Networks__architecture.png?raw=true \"Architecture\")\n  * Comparison measure\n    * They suggest a new method to compare GAN results with each other.\n    * They suggest to train pairs of G and D, e.g. for two pairs (G1, D1), (G2, D2). Then they let the pairs compete with each other.\n    * To estimate the quality of D they suggest `r_test = errorRate(D1, testset) / errorRate(D2, testset)`. (\"Which D is better at spotting that the test set images are real images?\")\n    * To estimate the quality of the generated samples they suggest `r_sample = errorRate(D1, images by G2) / errorRate(D2, images by G1)`. (\"Which G is better at fooling an unknown D, i.e. possibly better at generating life-like images?\")\n    * They suggest to estimate which G is better using r_sample and then to estimate how valid that result is using r_test.\n\n* Results\n  * Generated images of churches, with timesteps 1 to 5:\n    * ![Churches](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Generating_Images_with_Recurrent_Adversarial_Networks__churches.jpg?raw=true \"Churches\")\n  * Overfitting\n    * They saw no indication of overfitting in the sense of memorizing images from the training dataset.\n    * They however saw some indication of G just interpolating between some good images and of G reusing small image patches in different images.\n  * Randomness of noise vector `z`:\n    * Sampling the noise vector once seems to be better than resampling it at every timestep.\n    * Resampling it at every time step often led to very similar looking output images.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1602.05110"
    },
    "903": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.00704",
        "transcript": "  * They suggest a new architecture for GANs.\n  * Their architecture adds another Generator for a reverse branch (from images to noise vector `z`).\n  * Their architecture takes some ideas from VAEs/variational neural nets.\n  * Overall they can improve on the previous state of the art (DCGAN).\n\n### How\n  * Architecture\n    * Usually, in GANs one feeds a noise vector `z` into a Generator (G), which then generates an image (`x`) from that noise.\n    * They add a reverse branch (G2), in which another Generator takes a real image (`x`) and generates a noise vector `z` from that.\n      * The noise vector can now be viewed as a latent space vector.\n    * Instead of letting G2 generate *discrete* values for `z` (as it is usually done), they instead take the approach commonly used VAEs and use *continuous* variables instead.\n      * That is, if `z` represents `N` latent variables, they let G2 generate `N` means and `N` variances of gaussian distributions, with each distribution representing one value of `z`.\n      * So the model could e.g. represent something along the lines of \"this face looks a lot like a female, but with very low probability could also be male\".\n  * Training\n    * The Discriminator (D) is now trained on pairs of either `(real image, generated latent space vector)` or `(generated image, randomly sampled latent space vector)` and has to tell them apart from each other.\n    * Both Generators are trained to maximally confuse D.\n      * G1 (from `z` to `x`) confuses D maximally, if it generates new images that (a) look real and (b) fit well to the latent variables in `z` (e.g. if `z` says \"image contains a cat\", then the image should contain a cat).\n      * G2 (from `x` to `z`) confuses D maximally, if it generates good latent variables `z` that fit to the image `x`.\n    * Continuous variables\n      * The variables in `z` follow gaussian distributions, which makes the training more complicated, as you can't trivially backpropagate through gaussians.\n      * When training G1 (from `z` to `x`) the situation is easy: You draw a random `z`-vector following a gaussian distribution (`N(0, I)`). (This is basically the same as in \"normal\" GANs. They just often use uniform distributions instead.)\n      * When training G2 (from `x` to `z`) the situation is a bit harder.\n        * Here we need to use the reparameterization trick here.\n        * That roughly means, that G2 predicts the means and variances of the gaussian variables in `z` and then we draw a sample of `z` according to exactly these means and variances.\n        * That sample gives us discrete values for our backpropagation.\n        * If we do that sampling often enough, we get a good approximation of the true gradient (of the continuous variables). (Monte Carlo approximation.)\n\n* Results\n  * Images generated based on Celeb-A dataset:\n    * ![Celeb-A samples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Adversarially_Learned_Inference__celeba-samples.png?raw=true \"Celeb-A samples\")\n  * Left column per pair: Real image, right column per pair: reconstruction (`x -> z` via G2, then `z -> x` via G1)\n    * ![Celeb-A reconstructions](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Adversarially_Learned_Inference__celeba-reconstructions.png?raw=true \"Celeb-A reconstructions\")\n  * Reconstructions of SVHN, notice how the digits often stay the same, while the font changes:\n    * ![SVHN reconstructions](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Adversarially_Learned_Inference__svhn-reconstructions.png?raw=true \"SVHN reconstructions\")\n  * CIFAR-10 samples, still lots of errors, but some quite correct:\n    * ![CIFAR10 samples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Adversarially_Learned_Inference__cifar10-samples.png?raw=true \"CIFAR10 samples\")\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.00704"
    },
    "904": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/TargAL16",
        "transcript": "  * They describe an architecture that merges classical convolutional networks and residual networks.\n  * The architecture can (theoretically) learn anything that a classical convolutional network or a residual network can learn, as it contains both of them.\n  * The architecture can (theoretically) learn how many convolutional layers it should use per residual block (up to the amount of convolutional layers in the whole network).\n\n### How\n  * Just like residual networks, they have \"blocks\". Each block contains convolutional layers.\n  * Each block contains residual units and non-residual units.\n  * They have two \"streams\" of data in their network (just matrices generated by each block):\n    * Residual stream: The residual blocks write to this stream (i.e. it's their output).\n    * Transient stream: The non-residual blocks write to this stream.\n  * Residual and non-residual layers receive *both* streams as input, but only write to *their* stream as output.\n  * Their architecture visualized:\n    ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Resnet_in_Resnet__architecture.png?raw=true \"Architecture\")\n  * Because of this architecture, their model can learn the number of layers per residual block (though BN and ReLU might cause problems here?):\n    ![Learning layercount](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Resnet_in_Resnet__learning_layercount.png?raw=true \"Learning layercount\")\n  * The easiest way to implement this should be along the lines of the following (some of the visualized convolutions can be merged):\n    * Input of size CxHxW (both streams, each C/2 planes)\n      * Concat\n        * Residual block: Apply C/2 convolutions to the C input planes, with shortcut addition afterwards.\n        * Transient block: Apply C/2 convolutions to the C input planes.\n      * Apply BN\n      * Apply ReLU\n    * Output of size CxHxW.\n  * The whole operation can also be implemented with just a single convolutional layer, but then one has to make sure that some weights stay at zero.\n\n### Results\n  * They test on CIFAR-10 and CIFAR-100.\n  * They search for optimal hyperparameters (learning rate, optimizer, L2 penalty, initialization method, type of shortcut connection in residual blocks) using a grid search.\n  * Their model improves upon a wide ResNet and an equivalent non-residual CNN by a good margin (CIFAR-10: 0.5-1%, CIFAR-100: 1-2%).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.08029"
    },
    "905": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.01749",
        "transcript": "  * Autoencoders typically have some additional criterion that pushes them towards learning meaningful representations.\n    * E.g. L1-Penalty on the code layer (z), Dropout on z, Noise on z.\n    * Often, representations with sparse activations are considered meaningful (so that each activation reflects are clear concept).\n  * This paper introduces another technique that leads to sparsity.\n  * They use a rank ordering on z.\n  * The first (according to the ranking) activations have to do most of the reconstruction work of the data (i.e. image).\n\n### How\n  * Basic architecture:\n    * They use an Autoencoder architecture: Input -> Encoder -> z -> Decoder -> Output.\n    * Their encoder and decoder seem to be empty, i.e. z is the only hidden layer in the network.\n    * Their output is not just one image (or whatever is encoded), instead they generate one for every unit in layer z.\n    * Then they order these outputs based on the activation of the units in z (rank ordering), i.e. the output of the unit with the highest activation is placed in the first position, the output of the unit with the 2nd highest activation gets the 2nd position and so on.\n    * They then generate the final output image based on a cumulative sum. So for three reconstructed output images `I1, I2, I3` (rank ordered that way) they would compute `final image = I1 + (I1+I2) + (I1+I2+I3)`.\n    * They then compute the error based on that reconstruction (`reconstruction - input image`) and backpropagate it.\n  * Cumulative sum:\n    * Using the cumulative sum puts most optimization pressure on units with high activation, as they have the largest influence on the reconstruction error.\n    * The cumulative sum is best optimized by letting few units have high activations and generate most of the output (correctly). All the other units have ideally low to zero activations and low or no influence on the output. (Though if the output generated by the first units is wrong, you should then end up with an extremely high cumulative error sum...)\n      * So their `z` coding should end up with few but high activations, i.e. it should become very sparse.\n    * The cumulative generates an individual error per output, while an ordinary sum generates the same error for every output. They argue that this \"blurs\" the error less.\n  * To avoid blow ups in their network they use TReLUs, which saturate below 0 and above 1, i.e. `min(1, max(0, input))`.\n  * They use a custom derivative function for the TReLUs, which is dependent on both the input value of the unit and its gradient. Basically, if the input is `>1` (saturated) and the error is high, then the derivative pushes the weight down, so that the input gets into the unsaturated regime. Similarly for input values `<0` (pushed up). If the input value is between 0 and 1 and/or the error is low, then nothing is changed.\n  * They argue that the algorithmic complexity of the rank ordering should be low, due to sorts being `O(n log(n))`, where `n` is the number of hidden units in `z`.\n\n### Results\n  * They autoencode 7x7 patches from CIFAR-10.\n  * They get very sparse activations.\n  * Training and test loss develop identically, i.e. no overfitting.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.01749"
    },
    "906": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.07146",
        "transcript": "  * The authors start with a standard ResNet architecture (i.e. residual network has suggested in \"Identity Mappings in Deep Residual Networks\").\n    * Their residual block:\n      ![Residual block](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Wide_Residual_Networks__residual_block.png?raw=true \"Residual block\")\n    * Several residual blocks of 16 filters per conv-layer, followed by 32 and then 64 filters per conv-layer.\n  * They empirically try to answer the following questions:\n    * How many residual blocks are optimal? (Depth)\n    * How many filters should be used per convolutional layer? (Width)\n    * How many convolutional layers should be used per residual block?\n    * Does Dropout between the convolutional layers help?\n\n### Results\n  * *Layers per block and kernel sizes*:\n    * Using 2 convolutional layers per residual block seems to perform best:\n      ![Convs per block](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Wide_Residual_Networks__convs_per_block.png?raw=true \"Convs per block\")\n    * Using 3x3 kernel sizes for both layers seems to perform best.\n    * However, using 3 layers with kernel sizes 3x3, 1x1, 3x3 and then using less residual blocks performs nearly as good and decreases the required time per batch.\n  * *Width and depth*:\n    * Increasing the width considerably improves the test error.\n    * They achieve the best results (on CIFAR-10) when decreasing the depth to 28 convolutional layers, with each having 10 times their normal width (i.e. 16\\*10 filters, 32\\*10 and 64\\*10):\n      ![Depth and width results](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Wide_Residual_Networks__depth_and_width.png?raw=true \"Depth and width results\")\n    * They argue that their results show no evidence that would support the common theory that thin and deep networks somehow regularized better than wide and shallow(er) networks.\n  * *Dropout*:\n    * They use dropout with p=0.3 (CIFAR) and p=0.4 (SVHN).\n    * On CIFAR-10 dropout doesn't seem to consistently improve test error.\n    * On CIFAR-100 and SVHN dropout seems to lead to improvements that are either small (wide and shallower net, i.e. depth=28, width multiplier=10) or significant (ResNet-50).\n      ![Dropout](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Wide_Residual_Networks__dropout.png?raw=true \"Dropout\")\n    * They also observed oscillations in error (both train and test) during the training. Adding dropout decreased these oscillations.\n  * *Computational efficiency*:\n    * Applying few big convolutions is much more efficient on GPUs than applying many small ones sequentially.\n    * Their network with the best test error is 1.6 times faster than ResNet-1001, despite having about 3 times more parameters.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.07146"
    },
    "907": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eccv/HeZRS16",
        "transcript": "  * The authors reevaluate the original residual design of neural networks.\n  * They compare various architectures of residual units and actually find one that works quite a bit better.\n\n### How\n  * The new variation starts the transformation branch of each residual unit with BN and a ReLU.\n  * It removes BN and ReLU after the last convolution.\n  * As a result, the information from previous layers can flow completely unaltered through the shortcut branch of each residual unit.\n  * The image below shows some variations (of the position of BN and ReLU) that they tested. The new and better design is on the right:\n    ![BN and ReLU positions](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Identity_Mappings_in_Deep_Residual_Networks__activations.png?raw=true \"BN and ReLU positions\")\n  * They also tried various alternative designs for the shortcut connections. However, all of these designs performed worse than the original one. Only one (d) came close under certain conditions. Therefore, the recommendation is to stick with the old/original design.\n    ![Shortcut designs](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Identity_Mappings_in_Deep_Residual_Networks__shortcuts.png?raw=true \"Shortcut designs\")\n\n### Results\n  * Significantly faster training for very deep residual networks (1001 layers).\n  * Better regularization due to the placement of BN.\n  * CIFAR-10 and CIFAR-100 results, old vs. new design:\n    ![Old vs new results](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Identity_Mappings_in_Deep_Residual_Networks__old_vs_new.png?raw=true \"Old vs new results\")",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-46493-0_38"
    },
    "908": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.06465",
        "transcript": "  * They describe a regularization method similar to dropout and stochastic depth.\n  * The method could be viewed as a merge of the two techniques (dropout, stochastic depth).\n  * The method seems to regularize better than any of the two alone.\n\n### How\n  * Let `x` be the input to a layer. That layer produces an output. The output can be:\n    * Feed forward (\"classic\") network: `F(x)`.\n    * Residual network: `x + F(x)`.\n  * The standard dropout-like methods do the following:\n    * Dropout in feed forward networks: Sometimes `0`, sometimes `F(x)`. Decided per unit.\n    * Dropout in residual networks (rarely used): Sometimes `0`, sometimes `x + F(x)`. Decided per unit.\n    * Stochastic depth (only in residual networks): Sometimes `x`, sometimes `x + F(x)`. Decided per *layer*.\n    * Skip forward (only in residual networks): Sometimes `x`, sometimes `x + F(x)`. Decided per unit.\n    * **Swapout** (any network): Sometimes `0`, sometimes `F(x)`, sometimes `x`, sometimes `x + F(x)`. Decided per unit.\n  * Swapout can be represented using the formula `y = theta_1 * x + theta_2 * F(x)`.\n    * `*` is the element-wise product.\n    * `theta_1` and `theta_2` are tensors following bernoulli distributions, i.e. their values are all exactly `0` or exactly `1`.\n    * Setting the values of `theta_1` and `theta_2` per unit in the right way leads to the values `0` (both 0), `x` (1, 0), `F(x)` (0, 1) or `x + F(x)` (1, 1).\n  * Deterministic and Stochastic Inference\n    * Ideally, when using a dropout-like technique you would like to get rid of its stochastic effects during prediction, so that you can predict values with exactly *one* forward pass through the network (instead of having to average over many passes).\n    * For Swapout it can be mathematically shown that you can't calculate a deterministic version of it that performs equally to the stochastic one (averaging over many forward passes).\n    * This is even more the case when using Batch Normalization in a network. (Actually also when not using Swapout, but instead Dropout + BN.)\n    * So for best results you should use the stochastic method (averaging over many forward passes).\n\n### Results\n  * They compare various dropout-like methods, including Swapout, applied to residual networks. (On CIFAR-10 and CIFAR-100.)\n  * General performance:\n    * Results with Swapout are better than with the other methods.\n    * According to their results, the ranking of methods is roughly: Swapout > Dropout > Stochastic Depth > Skip Forward > None.\n  * Stochastic vs deterministic method:\n    * The stochastic method of swapout (average over N forward passes) performs significantly better than the deterministic one.\n    * Using about 15-30 forward passes seems to yield good results.\n  * Optimal parameter choice:\n    * Previously the Swapout-formula `y = theta_1 * x + theta_2 * F(x)` was mentioned.\n    * `theta_1` and `theta_2` are generated via Bernoulli distributions which have parameters `p_1` and `p_2`.\n    * If using fixed values for `p_1` and `p_2` throughout the network, it seems to be best to either set both of them to `0.5` or to set `p_1` to `>0.5` and `p_2` to `<0.5` (preference towards `y = x`).\n    * It's best however to start both at `1.0` (always `y = x + F(x)`) and to then linearly decay them to both `0.5` towards the end of the network, i.e. to apply less noise to the early layers. (This is similar to the results in the Stochastic Depth paper.)\n  * Thin vs. wide residual networks:\n    * The standard residual networks that they compared to used a `(16, 32, 64)` pattern for their layers, i.e. they started with layers of each having 16 convolutional filters, followed by some layers with each having 32 filters, followed by some layers with 64 filters.\n    * They tried instead a `(32, 64, 128)` pattern, i.e. they doubled the amount of filters.\n    * Then they reduced the number of layers from 100 down to 20.\n    * Their wider residual network performed significantly better than the deep and thin counterpart. However, their parameter count also increased by about `4` times.\n    * Increasing the pattern again to `(64, 128, 256)` and increasing the number of layers from 20 to 32 leads to another performance improvement, beating a 1000-layer network of pattern `(16, 32, 64)`. (Parameter count is then `27` times the original value.)\n\n* Comments\n  * Stochastic depth works layer-wise, while Swapout works unit-wise. When a layer in Stochastic Depth is dropped, its whole forward- and backward-pass don't have to be calculated. That saves time. Swapout is not going to save time.\n  * They argue that dropout+BN would also profit from using stochastic inference instead of deterministic inference, just like Swapout does. However, they don't mention using it for dropout in their comparison, only for Swapout.\n  * They show that linear decay for their parameters (less dropping on early layers, more on later ones) significantly improves the results of Swapout. However, they don't mention testing the same thing for dropout. Maybe dropout would also profit from it?\n  * For the above two points: Dropout's test error is at 5.87, Swapout's test error is at 5.68. So the difference is already quite small, making any disadvantage for dropout significant.\n\n\n![Visualization](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Swapout__visualization.png?raw=true \"Visualization\")\n\n*Visualization of how Swapout works. From left to right: An input `x`; a standard layer is applied to the input `F(x)`; a residual layer is applied to the input `x + F(x)`; Skip Forward is applied to the layer; Swapout is applied to the layer. Stochastic Depth would be all units being orange (`x`) or blue (`x + F(x)`).*\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.06465"
    },
    "909": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/YuK15",
        "transcript": "  * They describe a variation of convolutions that have a differently structured receptive field.\n  * They argue that their variation works better for dense prediction, i.e. for predicting values for every pixel in an image (e.g. coloring, segmentation, upscaling).\n\n### How\n  * One can image the input into a convolutional layer as a 3d-grid. Each cell is a \"pixel\" generated by a filter.\n  * Normal convolutions compute their output per cell as a weighted sum of the input cells in a dense area. I.e. all input cells are right next to each other.\n  * In dilated convolutions, the cells are not right next to each other. E.g. 2-dilated convolutions skip 1 cell between each input cell, 3-dilated convolutions skip 2 cells etc. (Similar to striding.)\n  * Normal convolutions are simply 1-dilated convolutions (skipping 0 cells).\n  * One can use a 1-dilated convolution and then a 2-dilated convolution. The receptive field of the second convolution will then be 7x7 instead of the usual 5x5 due to the spacing.\n  * Increasing the dilation factor by 2 per layer (1, 2, 4, 8, ...) leads to an exponential increase in the receptive field size, while every cell in the receptive field will still be part in the computation of at least one convolution.\n  * They had problems with badly performing networks, which they fixed using an identity initialization for the weights. (Sounds like just using resdiual connections would have been easier.)\n\n![Receptive field](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Multi-Scale_Context_Aggregation_by_Dilated_Convolutions__receptive.png?raw=true \"Receptive field\")\n\n*Receptive fields of a 1-dilated convolution (1st image), followed by a 2-dilated conv. (2nd image), followed by a 4-dilated conv. (3rd image). The blue color indicates the receptive field size (notice the exponential increase in size). Stronger blue colors mean that the value has been used in more different convolutions.*\n\n\n### Results\n  * They took a VGG net, removed the pooling layers and replaced the convolutions with dilated ones (weights can be kept).\n  * They then used the network to segment images.\n  * Their results were significantly better than previous methods.\n  * They also added another network with more dilated convolutions in front of the VGG one, again improving the results.\n\n\n![Segmentation performance](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Multi-Scale_Context_Aggregation_by_Dilated_Convolutions__segmentation.png?raw=true \"Segmentation performance\")\n\n*Their performance on a segmentation task compared to two competing methods. They only used VGG16 without pooling layers and with convolutions replaced by dilated convolutions.*",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.07122"
    },
    "910": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LiuGX16",
        "transcript": "\n  * The well known method of Artistic Style Transfer can be used to generate new texture images (from an existing example) by skipping the content loss and only using the style loss.\n  * The method however can have problems with large scale structures and quasi-periodic patterns.\n  * They add a new loss based on the spectrum of the images (synthesized image and style image), which decreases these problems and handles especially periodic patterns well.\n\n### How\n  * Everything is handled in the same way as in the Artistic Style Transfer paper (without content loss).\n  * On top of that they add their spectrum loss:\n    * The loss is based on a squared distance, i.e. $1/2 d(I_s, I_t)^2$.\n      * $I_s$ is the last synthesized image.\n      * $I_t$ is the texture example.\n    * $d(I_s, I_t)$ then does the following:\n      * It assumes that $I_t$ is an example for a space of target images.\n      * Within that set it finds the image $I_p$ which is most similar to $I_s$. That is done using a projection via Fourier Transformations. (See formula 5 in the paper.)\n      * The returned distance is then $I_s - I_p$.\n\n### Results\n  * Equal quality for textures without quasi-periodic structures.\n  * Significantly better quality for textures with quasi-periodic structures.\n\n\n![Overview](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Texture_Synthesis_Through_CNNs_and_Spectrum_Constraints__overview.png?raw=true \"Overview\")\n\n*Overview over their method, i.e. generated textures using style and/or spectrum-based loss.*",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1605.01141"
    },
    "911": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eccv/LiW16",
        "transcript": "https://www.youtube.com/watch?v=PRD8LpPvdHI\n\n  * They describe a method that can be used for two problems:\n    * (1) Choose a style image and apply that style to other images.\n    * (2) Choose an example texture image and create new texture images that look similar.\n  * In contrast to previous methods their method can be applied very fast to images (style transfer) or noise (texture creation). However, per style/texture a single (expensive) initial training session is still necessary.\n  * Their method builds upon their previous paper \"Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis\".\n\n### How\n  * Rough overview of their previous method:\n    * Transfer styles using three losses:\n      * Content loss: MSE between VGG representations.\n      * Regularization loss: Sum of x-gradient and y-gradients (encouraging smooth areas).\n      * MRF-based style loss: Sample `k x k` patches from VGG representations of content image and style image. For each patch from content image find the nearest neighbor (based on normalized cross correlation) from style patches. Loss is then the sum of squared errors of euclidean distances between content patches and their nearest neighbors.\n    * Generation of new images is done by starting with noise and then iteratively applying changes that minimize the loss function.\n  * They introduce mostly two major changes:\n    * (a) Get rid of the costly nearest neighbor search for the MRF loss. Instead, use a discriminator-network that receives a patch and rates how real that patch looks.\n      * This discriminator-network is costly to train, but that only has to be done once (per style/texture).\n    * (b) Get rid of the slow, iterative generation of images. Instead, start with the content image (style transfer) or noise image (texture generation) and feed that through a single generator-network to create the output image (with transfered style or generated texture).\n      * This generator-network is costly to train, but that only has to be done once (per style/texture). \n  * MDANs\n    * They implement change (a) to the standard architecture and call that an \"MDAN\" (Markovian Deconvolutional Adversarial Networks).\n    * So the architecture of the MDAN is:\n      * Input: Image (RGB pixels)\n      * Branch 1: Markovian Patch Quality Rater (aka Discriminator)\n        * Starts by feeding the image through VGG19 until layer `relu3_1`. (Note: VGG weights are fixed/not trained.)\n        * Then extracts `k x k` patches from the generated representations.\n        * Feeds each patch through a shallow ConvNet (convolution with BN then fully connected layer).\n        * Training loss is a hinge loss, i.e. max margin between classes +1 (real looking patch) and -1 (fake looking patch). (Could also take a single sigmoid output, but they argue that hinge loss isn't as likely to saturate.)\n        * This branch will be trained continuously while synthesizing a new image.\n      * Branch 2: Content Estimation/Guidance\n        * Note: This branch is only used for style transfer, i.e if using an content image and not for texture generation.\n        * Starts by feeding the currently synthesized image through VGG19 until layer `relu5_1`. (Note: VGG weights are fixed/not trained.)\n        * Also feeds the content image through VGG19 until layer `relu5_1`.\n        * Then uses a MSE loss between both representations (so similar to a MSE on RGB pixels that is often used in autoencoders).\n        * Nothing in this branch needs to trained, the loss only affects the synthesizing of the image.\n  * MGANs\n    * The MGAN is like the MDAN, but additionally implements change (b), i.e. they add a generator that takes an image and stylizes it.\n    * The generator's architecture is:\n      * Input: Image (RGB pixels) or noise (for texture synthesis)\n      * Output: Image (RGB pixels) (stylized input image or generated texture)\n      * The generator takes the image (pixels) and feeds that through VGG19 until layer `relu4_1`.\n      * Similar to the DCGAN generator, they then apply a few fractionally strided convolutions (with BN and LeakyReLUs) to that, ending in a Tanh output. (Fractionally strided convolutions increase the height/width of the images, here to compensate the VGG pooling layers.)\n      * The output after the Tanh is the output image (RGB pixels).\n    * They train the generator with pairs of `(input image, stylized image or texture)`. These pairs can be gathered by first running the MDAN alone on several images. (With significant augmentation a few dozen pairs already seem to be enough.)\n    * One of two possible loss functions can then be used:\n      * Simple standard choice: MSE on the euclidean distance between expected output pixels and generated output pixels. Can cause blurriness.\n      * Better choice: MSE on a higher VGG representation. Simply feed the generated output pixels through VGG19 until `relu4_1` and the reuse the already generated (see above) VGG-representation of the input image. This is very similar to the pixel-wise comparison, but tends to cause less blurriness.\n    * Note: For some reason the authors call their generator a VAE, but don't mention any typical VAE technique, so it's not described like one here.\n  * They use Adam to train their networks.\n  * For texture generation they use Perlin Noise instead of simple white noise. In Perlin Noise, lower frequency components dominate more than higher frequency components. White noise didn't work well with the VGG representations in the generator (activations were close to zero).\n\n### Results\n  * Similar quality like previous methods, but much faster (compared to most methods).\n  * For the Markovian Patch Quality Rater (MDAN branch 1):\n    * They found that the weights of this branch can be used as initialization for other training sessions (e.g. other texture styles), leading to a decrease in required iterations/epochs.\n    * Using VGG for feature extraction seems to be crucial. Training from scratch generated in worse results.\n    * Using larger patch sizes preserves more structure of the structure of the style image/texture. Smaller patches leads to more flexibility in generated patterns.\n    * They found that using more than 3 convolutional layers or more than 64 filters per layer provided no visible benefit in quality.\n\n\n![Example](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Markovian_GANs__example.png?raw=true \"Example\")\n\n*Result of their method, compared to other methods.*\n\n\n![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Markovian_GANs__architecture.png?raw=true \"Architecture\")\n\n*Architecture of their model.*",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-46487-9_43"
    },
    "912": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1603.01768",
        "transcript": "\n  * They describe a method to transfer image styles based on semantic classes.\n  * This allows to:\n    * (1) Transfer styles between images more accurately than with previous models. E.g. so that the background of an image does not receive the style of skin/hair/clothes/... seen in the style image. Skin in the synthesized image should receive the style of skin from the style image. Same for hair, clothes, etc.\n    * (2) Turn simple doodles into artwork by treating the simplified areas in the doodle as semantic classes and annotating an artwork with these same semantic classes. (E.g. \"this blob should receive the style from these trees.\")\n\n### How\n  * Their method is based on [Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis](Combining_MRFs_and_CNNs_for_Image_Synthesis.md).\n  * They use the same content loss and mostly the same MRF-based style loss. (Apparently they don't use the regularization loss.)\n  * They change the input of the MRF-based style loss.\n    * Usually that input would only be the activations of a VGG-layer (for the synthesized image or the style source image).\n    * They add a semantic map with weighting `gamma` to the activation, i.e. `<representation of image> = <activation of specific layer for that image> || gamma * <semantic map>`.\n    * The semantic map has N channels with 1s in a channel where a specific class is located (e.g. skin).\n    * The semantic map has to be created by the user for both the content image and the style image.\n    * As usually for the MRF loss, patches are then sampled from the representations. The semantic maps then influence the distance measure. I.e. patches are more likely to be sampled from the same semantic class.\n    * Higher `gamma` values make it more likely to sample from the same semantic class (because the distance from patches from different classes gets larger).\n  * One can create a small doodle with few colors, then use the colors as the semantic map. Then add a semantic map to an artwork and run the algorithm to transform the doodle into an artwork. \n\n### Results\n  * More control over the transfered styles than previously.\n  * Less sensitive to the style weighting, because of the additional `gamma` hyperparameter.\n  * Easy transformation from doodle to artwork.\n\n![Example](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Neural_Doodle__example.png?raw=true \"Example\")\n\n*Turning a doodle into an artwork. Note that the doodle input image is also used as the semantic map of the input.*",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1603.01768"
    },
    "913": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/LiW16",
        "transcript": "\n  * They describe a method that applies the style of a source image to a target image.\n    * Example: Let a normal photo look like a van Gogh painting.\n    * Example: Let a normal car look more like a specific luxury car.\n  * Their method builds upon the well known artistic style paper and uses a new MRF prior.\n  * The prior leads to locally more plausible patterns (e.g. less artifacts).\n\n### How\n  * They reuse the content loss from the artistic style paper.\n    * The content loss was calculated by feed the source and target image through a network (here: VGG19) and then estimating the squared error of the euclidean distance between one or more hidden layer activations.\n    * They use layer `relu4_2` for the distance measurement.\n  * They replace the original style loss with a MRF based style loss.\n    * Step 1: Extract from the source image `k x k` sized overlapping patches.\n    * Step 2: Perform step (1) analogously for the target image.\n    * Step 3: Feed the source image patches through a pretrained network (here: VGG19) and select the representations `r_s` from specific hidden layers (here: `relu3_1`, `relu4_1`).\n    * Step 4: Perform step (3) analogously for the target image. (Result: `r_t`)\n    * Step 5: For each patch of `r_s` find the best matching patch in `r_t` (based on normalized cross correlation).\n    * Step 6: Calculate the sum of squared errors (based on euclidean distances) of each patch in `r_s` and its best match (according to step 5).\n  * They add a regularizer loss.\n    * The loss encourages smooth transitions in the synthesized image (i.e. few edges, corners).\n    * It is based on the raw pixel values of the last synthesized image.\n    * For each pixel in the synthesized image, they calculate the squared x-gradient and the squared y-gradient and then add both.\n    * They use the sum of all those values as their loss (i.e. `regularizer loss = <sum over all pixels> x-gradient^2 + y-gradient^2`).\n  * Their whole optimization problem is then roughly `image = argmin_image MRF-style-loss + alpha1 * content-loss + alpha2 * regularizer-loss`.\n  * In practice, they start their synthesis with a low resolution image and then progressively increase the resolution (each time performing some iterations of optimization).\n  * In practice, they sample patches from the style image under several different rotations and scalings.\n\n### Results\n  * In comparison to the original artistic style paper:\n    * Less artifacts.\n    * Their method tends to preserve style better, but content worse.\n    * Can handle photorealistic style transfer better, so long as the images are similar enough. If no good matches between patches can be found, their method performs worse.\n\n![Non-photorealistic example images](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Combining_MRFs_and_CNNs_for_Image_Synthesis__examples.png?raw=true \"Non-photorealistic example images\")\n\n*Non-photorealistic example images. Their method vs. the one from the original artistic style paper.*\n\n\n![Photorealistic example images](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Combining_MRFs_and_CNNs_for_Image_Synthesis__examples_real.png?raw=true \"Photorealistic example images\")\n\n*Photorealistic example images. Their method vs. the one from the original artistic style paper.*",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2016.272"
    },
    "914": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/KimLL16a",
        "transcript": "  * They describe a model that upscales low resolution images to their high resolution equivalents (\"Single Image Super Resolution\").\n  * Their model uses a deeper architecture than previous models and has a residual component.\n\n### How\n  * Their model is a fully convolutional neural network.\n  * Input of the model: The image to upscale, *already upscaled to the desired size* (but still blurry).\n  * Output of the model: The upscaled image (without the blurriness).\n  * They use 20 layers of padded 3x3 convolutions with size 64xHxW with ReLU activations. (No pooling.)\n  * They have a residual component, i.e. the model only learns and outputs the *change* that has to be applied/added to the blurry input image (instead of outputting the full image). That change is applied to the blurry input image before using the loss function on it. (Note that this is a bit different from the currently used \"residual learning\".)\n  * They use a MSE between the \"correct\" upscaling and the generated upscaled image (input image + residual).\n  * They use SGD starting with a learning rate of 0.1 and decay it 3 times by a factor of 10.\n  * They use weight decay of 0.0001.\n  * During training they use a special gradient clipping adapted to the learning rate. Usually gradient clipping restricts the gradient values to `[-t, t]` (`t` is a hyperparameter). Their gradient clipping restricts the values to `[-t/lr, t/lr]` (where `lr` is the learning rate).\n  * They argue that their special gradient clipping allows the use of significantly higher learning rates.\n  * They train their model on multiple scales, e.g. 2x, 3x, 4x upscaling. (Not really clear how. They probably feed their upscaled image again into the network or something like that?)\n\n### Results\n  * Higher accuracy upscaling than all previous methods.\n  * Can handle well upscaling factors above 2x.\n  * Residual network learns significantly faster than non-residual network.\n\n![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Accurate_Image_Super-Resolution__architecture.png?raw=true \"Architecture\")\n\n*Architecture of the model.*\n\n\n![Examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Accurate_Image_Super-Resolution__examples.png?raw=true \"Examples\")\n\n*Super-resolution quality of their model (top, bottom is a competing model).*\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2016.182"
    },
    "915": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/TompsonJLB14",
        "transcript": "\n  * They describe a model for human pose estimation, i.e. one that finds the joints (\"skeleton\") of a person in an image.\n  * They argue that part of their model resembles a Markov Random Field (but in reality its implemented as just one big neural network).\n\n### How\n  * They have two components in their network:\n    * Part-Detector:\n      * Finds candidate locations for human joints in an image.\n      * Pretty standard ConvNet. A few convolutional layers with pooling and ReLUs.\n      * They use two branches: A fine and a coarse one. Both branches have practically the same architecture (convolutions, pooling etc.). The coarse one however receives the image downscaled by a factor of 2 (half width/height) and upscales it by a factor of 2 at the end of the branch.\n      * At the end they merge the results of both branches with more convolutions.\n      * The output of this model are 4 heatmaps (one per joint? unclear), each having lower resolution than the original image.\n    * Spatial-Model:\n      * Takes the results of the part detector and tries to remove all detections that were false positives.\n      * They derive their architecture from a fully connected Markov Random Field which would be solved with one step of belief propagation.\n      * They use large convolutions (128x128) to resemble the \"fully connected\" part.\n      * They initialize the weights of the convolutions with joint positions gathered from the training set.\n      * The convolutions are followed by log(), element-wise additions and exp() to resemble an energy function.\n      * The end result are the input heatmaps, but cleaned up.\n\n### Results\n  * Beats all previous models (with and without spatial model).\n  * Accuracy seems to be around 90% (with enough (16px) tolerance in pixel distance from ground truth).\n  * Adding the spatial model adds a few percentage points of accuracy.\n  * Using two branches instead of one (in the part detector) adds a bit of accuracy. Adding a third branch adds a tiny bit more.\n\n![Results](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Joint_Training_of_a_ConvNet_and_a_PGM_for_HPE__results.png?raw=true \"Results\")\n\n*Example results.*\n\n![Part Detector](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Joint_Training_of_a_ConvNet_and_a_PGM_for_HPE__part_detector.png?raw=true \"Part Detector\")\n\n*Part Detector network.*\n\n![Spatial Model](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Joint_Training_of_a_ConvNet_and_a_PGM_for_HPE__spatial_model.png?raw=true \"Spatial Model\")\n\n*Spatial Model (apparently only for two input heatmaps).*\n\n-------------------------\n\n# Rough chapter-wise notes\n\n* (1) Introduction\n  * Human Pose Estimation (HPE) from RGB images is difficult due to the high dimensionality of the input.\n  * Approaches:\n    * Deformable-part models: Traditionally based on hand-crafted features.\n    * Deep-learning based disciminative models: Recently outperformed other models. However, it is hard to incorporate priors (e.g. possible joint- inter-connectivity) into the model.\n  * They combine:\n    * A part-detector (ConvNet, utilizes multi-resolution feature representation with overlapping receptive fields)\n    * Part-based Spatial-Model (approximates loopy belief propagation)\n  * They backpropagate through the spatial model and then the part-detector.\n\n* (3) Model\n  * (3.1) Convolutional Network Part-Detector\n    * This model locates possible positions of human key joints in the image (\"part detector\").\n    * Input: RGB image.\n    * Output: 4 heatmaps, one per key joint (per pixel: likelihood).\n    * They use a fully convolutional network.\n    * They argue that applying convolutions to every pixel is similar to moving a sliding window over the image.\n    * They use two receptive field sizes for their \"sliding window\": A large but coarse/blurry one, a small but fine one.\n    * To implement that, they use two branches. Both branches are mostly identical (convolutions, poolings, ReLU). They simply feed a downscaled (half width/height) version of the input image into the coarser branch. At the end they upscale the coarser branch once and then merge both branches. \n    * After the merge they apply 9x9 convolutions and then 1x1 convolutions to get it down to 4xHxW (H=60, W=90 where expected input was H=320, W=240).\n  * (3.2) Higher-level Spatial-Model\n    * This model takes the detected joint positions (heatmaps) and tries to remove those that are probably false positives.\n    * It is a ConvNet, which tries to emulate (1) a Markov Random Field and (2) solving that MRF approximately via one step of belief propagation.\n    * The raw MRF formula would be something like `<likelihood of joint A per px> = normalize( <product over joint v from joints V> <probability of joint A per px given a> * <probability of joint v at px?> + someBiasTerm)`.\n    * They treat the probabilities as energies and remove from the formula the partition function (`normalize`) for various reasons (e.g. because they are only interested in the maximum value anyways).\n    * They use exp() in combination with log() to replace the product with a sum.\n    * They apply SoftPlus and ReLU so that the energies are always positive (and therefore play well with log).\n    * Apparently `<probability of joint v at px?>` are the input heatmaps of the part detector.\n    * Apparently `<probability of joint A per px given a>` is implemented as the weights of a convolution.\n    * Apparently `someBiasTerm` is implemented as the bias of a convolution.\n    * The convolutions that they use are large (128x128) to emulate a fully connected graph.\n    * They initialize the convolution weights based on histograms gathered from the dataset (empirical distribution of joint displacements).\n  * (3.3) Unified Models\n    * They combine the part-based model and the spatial model to a single one.\n    * They first train only the part-based model, then only the spatial model, then both.\n\n* (4) Results\n  * Used datasets: FLIC (4k training images, 1k test, mostly front-facing and standing poses), FLIC-plus (17k, 1k ?), extended-LSP (10k, 1k).\n  * FLIC contains images showing multiple persons with only one being annotated. So for FLIC they add a heatmap of the annotated body torso to the input (i.e. the part-detector does not have to search for the person any more).\n  * The evaluation metric roughly measures, how often predicted joint positions are within a certain radius of the true joint positions.\n  * Their model performs significantly better than competing models (on both FLIC and LSP).\n  * Accuracy seems to be at around 80%-95% per joint (when choosing high enough evaluation tolerance, i.e. 10px+).\n  * Adding the spatial model to the part detector increases the accuracy by around 10-15 percentage points.\n  * Training the part detector and the spatial model jointly adds ~3 percentage points accuracy over training them separately.\n  * Adding the second filter bank (coarser branch in the part detector) adds around 5 percentage points accuracy. Adding a third filter bank adds a tiny bit more accuracy.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5573-joint-training-of-a-convolutional-network-and-a-graphical-model-for-human-pose-estimation"
    },
    "916": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1604.06057",
        "transcript": "\n  * They present a hierarchical method for reinforcement learning.\n  * The method combines \"long\"-term goals with short-term action choices.\n\n### How\n  * They have two components:\n    * Meta-Controller:\n      * Responsible for the \"long\"-term goals.\n      * Is trained to pick goals (based on the current state) that maximize (extrinsic) rewards, just like you would usually optimize to maximize rewards by picking good actions.\n      * The Meta-Controller only picks goals when the Controller terminates or achieved the goal.\n    * Controller:\n      * Receives the current state and the current goal.\n      * Has to pick a reward maximizing action based on those, just as the agent would usually do (only the goal is added here).\n      * The reward is intrinsic. It comes from the Critic. The Critic gives reward whenever the current goal is reached.\n  * For Montezuma's Revenge:\n    * A goal is to reach a specific object.\n    * The goal is encoded via a bitmask (as big as the game screen). The mask contains 1s wherever the object is.\n    * They hand-extract the location of a few specific objects.\n    * So basically:\n      * The Meta-Controller picks the next object to reach via a Q-value function.\n      * It receives extrinsic reward when objects have been reached in a specific sequence.\n      * The Controller picks actions that lead to reaching the object based on a Q-value function. It iterates action-choosing until it terminates or reached the goal-object.\n      * The Critic awards intrinsic reward to the Controller whenever the goal-object was reached.\n    * They use CNNs for the Meta-Controller and the Controller, similar in architecture to the Atari-DQN paper (shallow CNNs).\n    * They use two replay memories, one for the Meta-Controller (size 40k) and one for the Controller (size 1M).\n    * Both follow an epsilon-greedy policy (for picking goals/actions). Epsilon starts at 1.0 and is annealed down to 0.1.\n    * They use a discount factor / gamma of 0.9.\n    * They train with SGD. \n\n### Results\n  * Learns to play Montezuma's Revenge.\n  * Learns to act well in a more abstract MDP with delayed rewards and where simple Q-learning failed.\n\n--------------------\n\n# Rough chapter-wise notes\n\n\n* (1) Introduction\n  * Basic problem: Learn goal directed behaviour from sparse feedbacks.\n  * Challenges:\n    * Explore state space efficiently\n    * Create multiple levels of spatio-temporal abstractions\n  * Their method: Combines deep reinforcement learning with hierarchical value functions.\n  * Their agent is motivated to solve specific intrinsic goals.\n  * Goals are defined in the space of entities and relations, which constraints the search space.\n  * They define their value function as V(s, g) where s is the state and g is a goal.\n  * First, their agent learns to solve intrinsically generated goals. Then it learns to chain these goals together.\n  * Their model has two hiearchy levels:\n    * Meta-Controller: Selects the current goal based on the current state.\n    * Controller: Takes state s and goal g, then selects a good action based on s and g. The controller operates until g is achieved, then the meta-controller picks the next goal.\n  * Meta-Controller gets extrinsic rewards, controller gets intrinsic rewards.\n  * They use SGD to optimize the whole system (with respect to reward maximization).\n\n* (3) Model\n  * Basic setting: Action a out of all actions A, state s out of S, transition function T(s,a)->s', reward by state F(s)->R.\n  * epsilon-greedy is good for local exploration, but it's not good at exploring very different areas of the state space.\n  * They use intrinsically motivated goals to better explore the state space.\n  * Sequences of goals are arranged to maximize the received extrinsic reward.\n  * The agent learns one policy per goal.\n  * Meta-Controller: Receives current state, chooses goal.\n  * Controller: Receives current state and current goal, chooses action. Keeps choosing actions until goal is achieved or a terminal state is reached. Has the optimization target of maximizing cumulative reward.\n  * Critic: Checks if current goal is achieved and if so provides intrinsic reward.\n  * They use deep Q learning to train their model.\n  * There are two Q-value functions. One for the controller and one for the meta-controller.\n  * Both formulas are extended by the last chosen goal g.\n  * The Q-value function of the meta-controller does not depend on the chosen action.\n  * The Q-value function of the controller receives only intrinsic direct reward, not extrinsic direct reward.\n  * Both Q-value functions are reprsented with DQNs.\n  * Both are optimized to minimize MSE losses.\n  * They use separate replay memories for the controller and meta-controller.\n  * A memory is added for the meta-controller whenever the controller terminates.\n  * Each new goal is picked by the meta-controller epsilon-greedy (based on the current state).\n  * The controller picks actions epsilon-greedy (based on the current state and goal).\n  * Both epsilons are annealed down.\n\n* (4) Experiments\n  * (4.1) Discrete MDP with delayed rewards\n    * Basic MDP setting, following roughly: Several states (s1 to s6) organized in a chain. The agent can move left or right. It gets high reward if it moves to state s6 and then back to s1, otherwise it gets small reward per reached state.\n    * They use their hierarchical method, but without neural nets.\n    * Baseline is Q-learning without a hierarchy/intrinsic rewards.\n    * Their method performs significantly better than the baseline.\n  * (4.2) ATARI game with delayed rewards\n    * They play Montezuma's Revenge with their method, because that game has very delayed rewards.\n    * They use CNNs for the controller and meta-controller (architecture similar to the Atari-DQN paper).\n    * The critic reacts to (entity1, relation, entity2) relationships. The entities are just objects visible in the game. The relation is (apparently ?) always \"reached\", i.e. whether object1 arrived at object2.\n    * They extract the objects manually, i.e. assume the existance of a perfect unsupervised object detector.\n    * They encode the goals apparently not as vectors, but instead just use a bitmask (game screen heightand width), which has 1s at the pixels that show the object.\n    * Replay memory sizes: 1M for controller, 50k for meta-controller.\n    * gamma=0.99\n    * They first only train the controller (i.e. meta-controller completely random) and only then train both jointly.\n    * Their method successfully learns to perform actions which lead to rewards with long delays.\n    * It starts with easier goals and then learns harder goals.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1604.06057"
    },
    "917": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=IizukaSIGGRAPH2016",
        "transcript": "  * They present a model which adds color to grayscale images (e.g. to old black and white images).\n  * It works best with 224x224 images, but can handle other sizes too.\n\n### How\n  * Their model has three feature extraction components:\n    * Low level features:\n      * Receives 1xHxW images and outputs 512xH/8xW/8 matrices.\n      * Uses 6 convolutional layers (3x3, strided, ReLU) for that.\n    * Global features:\n      * Receives the low level features and converts them to 256 dimensional vectors.\n      * Uses 4 convolutional layers (3x3, strided, ReLU) and 3 fully connected layers (1024 -> 512 -> 256; ReLU) for that.\n    * Mid-level features:\n      * Receives the low level features and converts them to 256xH/8xW/8 matrices.\n      * Uses 2 convolutional layers (3x3, ReLU) for that.\n  * The global and mid-level features are then merged with a Fusion Layer.\n    * The Fusion Layer is basically an extended convolutional layer.\n    * It takes the mid-level features (256xH/8xW/8) and the global features (256) as input and outputs a matrix of shape 256xH/8xW/8.\n    * It mostly operates like a normal convolutional layer on the mid-level features. However, its weight matrix is extended to also include weights for the global features (which will be added at every pixel).\n    * So they use something like `fusion at pixel u,v = sigmoid(bias + weights * [global features, mid-level features at pixel u,v])` - and that with 256 different weight matrices and biases for 256 filters.\n  * After the Fusion Layer they use another network to create the coloring:\n    * This network receives 256xH/8xW/8 matrices (merge of global and mid-level features) and generates 2xHxW outputs (color in L\\*a\\*b\\* color space).\n    * It uses a few convolutional layers combined with layers that do nearest neighbour upsampling.\n  * The loss for the colorization network is a MSE based on the true coloring.\n  * They train the global feature extraction also on the true class labels of the used images.\n  * Their model can handle any sized image. If the image doesn't have a size of 224x224, it must be resized to 224x224 for the gobal feature extraction. The mid-level feature extraction only uses convolutions, therefore it can work with any image size. \n\n### Results\n  * The training set that they use is the \"Places scene dataset\".\n  * After cleanup the dataset contains 2.3M training images (205 different classes) and 19k validation images.\n  * Users rate images colored by their method in 92.6% of all cases as real-looking (ground truth: 97.2%).\n  * If they exclude global features from their method, they only achieve 70% real-looking images.\n  * They can also extract the global features from image A and then use them on image B. That transfers the style from A to B. But it only works well on semantically similar images.\n\n![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Let_there_be_Color__architecture.png?raw=true \"Architecture\")\n\n*Architecture of their model.*\n\n\n![Old images](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Let_there_be_Color__old_images.png?raw=true \"Old images\")\n\n*Their model applied to old images.*\n\n--------------------\n\n# Rough chapter-wise notes\n\n* (1) Introduction\n  * They use a CNN to color images.\n  * Their network extracts global priors and local features from grayscale images.\n  * Global priors:\n    * Extracted from the whole image (e.g. time of day, indoor or outdoors, ...).\n    * They use class labels of images to train those. (Not needed during test.)\n  * Local features: Extracted from small patches (e.g. texture).\n  * They don't generate a full RGB image, instead they generate the chrominance map using the CIE L\\*a\\*b\\* colorspace.\n  * Components of the model:\n    * Low level features network: Generated first.\n    * Mid level features network: Generated based on the low level features.\n    * Global features network: Generated based on the low level features.\n    * Colorization network: Receives mid level and global features, which were merged in a fusion layer.\n  * Their network can process images of arbitrary size.\n  * Global features can be generated based on another image to change the style of colorization, e.g. to change the seasonal colors from spring to summer.\n\n* (3) Joint Global and Local Model\n  * <repetition of parts of the introduction>\n  * They mostly use ReLUs.\n  * (3.1) Deep Networks\n    * <standard neural net introduction>\n  * (3.2) Fusing Global and Local Features for Colorization\n    * Global features are used as priors for local features.\n    * (3.2.1) Shared Low-Level Features\n      * The low level features are which's (low level) features are fed into the networks of both the global and the medium level features extractors.\n      * They generate them from the input image using a ConvNet with 6 layers (3x3, 1x1 padding, strided/no pooling, ends in 512xH/8xW/8).\n    * (3.2.2) Global Image Features\n      * They process the low level features via another network into global features.\n      * That network has 4 conv-layers (3x3, 2 strided layers, all 512 filters), followed by 3 fully connected layers (1024, 512, 256).\n      * Input size (of low level features) is expected to be 224x224.\n    * (3.2.3) Mid-Level Features\n      * Takes the low level features (512xH/8xW/8) and uses 2 conv layers (3x3) to transform them to 256xH/8xW/8.\n    * (3.2.4) Fusing Global and Local Features\n      * The Fusion Layer is basically an extended convolutional layer.\n      * It takes the mid-level features (256xH/8xW/8) and the global features (256) as input and outputs a matrix of shape 256xH/8xW/8.\n      * It mostly operates like a normal convolutional layer on the mid-level features. However, its weight matrix is extended to also include weights for the global features (which will be added at every pixel).\n      * So they use something like `fusion at pixel u,v = sigmoid(bias + weights * [global features, mid-level features at pixel u,v])` - and that with 256 different weight matrices and biases for 256 filters.\n    * (3.2.5) Colorization Network\n      * The colorization network receives the 256xH/8xW/8 matrix from the fusion layer and transforms it to the 2xHxW chrominance map.\n      * It basically uses two upsampling blocks, each starting with a nearest neighbour upsampling layer, followed by 2 3x3 convs.\n      * The last layer uses a sigmoid activation.\n      * The network ends in a MSE.\n  * (3.3) Colorization with Classification\n    * To make training more effective, they train parts of the global features network via image class labels.\n    * I.e. they take the output of the 2nd fully connected layer (at the end of the global network), add one small hidden layer after it, followed by a sigmoid output layer (size equals number of class labels).\n    * They train that with cross entropy. So their global loss becomes something like `L = MSE(color accuracy) + alpha*CrossEntropy(class labels accuracy)`.\n  * (3.4) Optimization and Learning\n    * Low level feature extraction uses only convs, so they can be extracted from any image size.\n    * Global feature extraction uses fc layers, so they can only be extracted from 224x224 images.\n    * If an image has a size unequal to 224x224, it must be (1) resized to 224x224, fed through low level feature extraction, then fed through the global feature extraction and (2) separately (without resize) fed through the low level feature extraction and then fed through the mid-level feature extraction.\n    * However, they only trained on 224x224 images (for efficiency).\n    * Augmentation: 224x224 crops from 256x256 images; random horizontal flips.\n    * They use Adadelta, because they don't want to set learning rates. (Why not adagrad/adam/...?)\n\n* (4) Experimental Results and Discussion\n  * They set the alpha in their loss to `1/300`.\n  * They use the \"Places scene dataset\". They filter images with low color variance (including grayscale images). They end up with 2.3M training images and 19k validation images. They have 205 classes.\n  * Batch size: 128.\n  * They train for about 11 epochs.\n  * (4.1) Colorization results\n    * Good looking colorization results on the Places scene dataset.\n  * (4.2) Comparison with State of the Art\n    * Their method succeeds where other methods fail.\n    * Their method can handle very different kinds of images.\n  * (4.3) User study\n    * When rated by users, 92.6% think that their coloring is real (ground truth: 97.2%).\n    * Note: Users were told to only look briefly at the images.\n  * (4.4) Importance of Global Features\n    * Their model *without* global features only achieves 70% user rating.\n    * There are too many ambiguities on the local level.\n  * (4.5) Style Transfer through Global Features\n    * They can perform style transfer by extracting the global features of image B and using them for image A.\n  * (4.6) Colorizing the past\n    * Their model performs well on old images despite the artifacts commonly found on those.\n  * (4.7) Classification Results\n    * Their method achieves nearly as high classification accuracy as VGG (see classification loss for global features).\n  * (4.8) Comparison of Color Spaces\n    * L\\*a\\*b\\* color space performs slightly better than RGB and YUV, so they picked that color space.\n  * (4.9) Computation Time\n    * One image is usually processed within seconds.\n    * CPU takes roughly 5x longer.\n  * (4.10) Limitations and Discussion\n    * Their approach is data driven, i.e. can only deal well with types of images that appeared in the dataset.\n    * Style transfer works only really well for semantically similar images.\n    * Style transfer cannot necessarily transfer specific colors, because the whole model only sees the grayscale version of the image.\n    * Their model tends to strongly prefer the most common color for objects (e.g. grass always green).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf"
    },
    "918": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1604.08610",
        "transcript": "https://www.youtube.com/watch?v=vQk_Sfl7kSc&feature=youtu.be\n\n  * The paper describes a method to transfer the style (e.g. choice of colors, structure of brush strokes) of an image to a whole video.\n  * The method is designed so that the transfered style is consistent over many frames.\n  * Examples for such consistency:\n    * No flickering of style between frames. So the next frame has always roughly the same style in the same locations.\n    * No artefacts at the boundaries of objects, even if they are moving.\n    * If an area gets occluded and then unoccluded a few frames later, the style of that area is still the same as before the occlusion.\n\n### How\n  * Assume that we have a frame to stylize $x$ and an image from which to extract the style $a$.\n  * The basic process is the same as in the original Artistic Style Transfer paper, they just add a bit on top of that.\n  * They start with a gaussian noise image $x'$ and change it gradually so that a loss function gets minimized.\n  * The loss function has the following components:\n    * Content loss *(old, same as in the Artistic Style Transfer paper)*\n      * This loss makes sure that the content in the generated/stylized image still matches the content of the original image.\n      * $x$ and $x'$ are fed forward through a pretrained network (VGG in their case).\n      * Then the generated representations of the intermediate layers of the network are extracted/read.\n      * One or more layers are picked and the difference between those layers for $x$ and $x'$ is measured via a MSE.\n      * E.g. if we used only the representations of the layer conv5 then we would get something like `(conv5(x) - conv5(x'))^2` per example. (Where conv5() also executes all previous layers.)\n    * Style loss *(old)*\n      * This loss makes sure that the style of the generated/stylized image matches the style source $a$.\n      * $x'$ and $a$ are fed forward through a pretrained network (VGG in their case).\n      * Then the generated representations of the intermediate layers of the network are extracted/read.\n      * One or more layers are picked and the Gram Matrices of those layers are calculated.\n      * Then the difference between those matrices is measured via a MSE.\n    * Temporal loss *(new)*\n      * This loss enforces consistency in style between a pair of frames.\n      * The main sources of inconsistency are boundaries of moving objects and areas that get unonccluded.\n      * They use the optical flow to detect motion.\n        * Applying an optical flow method to two frames $(i, i+1)$ returns per pixel the movement of that pixel, i.e. if the pixel at $(x=1, y=2)$ moved to $(x=2, y=4)$ the optical flow at that pixel would be $(u=1, v=2)$.\n        * The optical flow can be split into the forward flow (here `fw`) and the backward flow (here `bw`). The forward flow is the flow from frame i to i+1 (as described in the previous point). The backward flow is the flow from frame $i+1$ to $i$ (reverse direction in time).\n      * Boundaries\n        * At boundaries of objects the derivative of the flow is high, i.e. the flow \"suddenly\" changes significantly from one pixel to the other.\n        * So to detect boundaries they use (per pixel) roughly the equation `gradient(u)^2 + gradient(v)^2 > length((u,v))`.\n      * Occlusions and disocclusions\n        * If a pixel does not get occluded/disoccluded between frames, the optical flow method should be able to correctly estimate the motion of that pixel between the frames. The forward and backward flows then should be roughly equal, just in opposing directions.\n        * If a pixel does get occluded/disoccluded between frames, it will not be visible in one the two frames and therefore the optical flow method cannot reliably estimate the motion for that pixel. It is then expected that the forward and backward flow are unequal.\n        * To measure that effect they roughly use (per pixel) a formula matching `length(fw + bw)^2 > length(fw)^2 + length(bw)^2`.\n      * Mask $c$\n        * They create a mask $c$ with the size of the frame.\n        * For every pixel they estimate whether the boundary-equation *or* the disocclusion-equation is true.\n        * If either of them is true, they add a 0 to the mask, otherwise a 1. So the mask is 1 wherever there is *no* disocclusion or motion boundary.\n      * Combination\n        * The final temporal loss is the mean (over all pixels) of $c*(x-w)^2$.\n          * $x$ is the frame to stylize.\n          * $w$ is the previous *stylized* frame (frame i-1), warped according to the optical flow between frame i-1 and i.\n          * `c` is the mask value at the pixel.\n        * By using the difference `x-w` they ensure that the difference in styles between two frames is low.\n        * By adding `c` they ensure the style-consistency only at pixels that probably should have a consistent style.\n    * Long-term loss *(new)*\n      * This loss enforces consistency in style between pairs of frames that are longer apart from each other.\n      * It is a simple extension of the temporal (short-term) loss.\n      * The temporal loss was computed for frames (i-1, i). The long-term loss is the sum of the temporal losses for the frame pairs {(i-4,i), (i-2,i), (i-1,i)}.\n      * The $c$ mask is recomputed for every pair and 1 if there are no boundaries/disocclusions detected, but only if there is not a 1 for the same pixel in a later mask. The additional condition is intended to associate pixels with their closest neighbours in time to minimize possible errors.\n      * Note that the long-term loss can completely replace the temporal loss as the latter one is contained in the former one.\n  * Multi-pass approach *(new)*\n    * They had problems with contrast around the boundaries of the frames.\n    * To combat that, they use a multi-pass method in which they seem to calculate the optical flow in multiple forward and backward passes? (Not very clear here what they do and why it would help.)\n  * Initialization with previous frame *(new)*\n    * Instead of starting at a gaussian noise image every time, they instead use the previous stylized frame.\n    * That immediately leads to more similarity between the frames.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1604.08610"
    },
    "919": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1312.5602",
        "transcript": "  * They use an implementation of Q-learning (i.e. reinforcement learning) with CNNs to automatically play Atari games.\n  * The algorithm receives the raw pixels as its input and has to choose buttons to press as its output. No hand-engineered features are used. So the model \"sees\" the game and \"uses\" the controller, just like a human player would.\n  * The model achieves good results on various games, beating all previous techniques and sometimes even surpassing human players.\n\n### How\n  * Deep Q Learning\n    * *This is yet another explanation of deep Q learning, see also [this blog post](http://www.nervanasys.com/demystifying-deep-reinforcement-learning/) for longer explanation.*\n    * While playing, sequences of the form (`state1`, `action1`, `reward`, `state2`) are generated.\n      * `state1` is the current game state. The agent only sees the pixels of that state. (Example: Screen shows enemy.)\n      * `action1` is an action that the agent chooses. (Example: Shoot!)\n      * `reward` is the direct reward received for picking `action1` in `state1`. (Example: +1 for a kill.)\n      * `state2` is the next game state, after the action was chosen in `state1`. (Example: Screen shows dead enemy.)\n    * One can pick actions at random for some time to generate lots of such tuples. That leads to a replay memory.\n    * Direct reward\n      * After playing randomly for some time, one can train a model to predict the direct reward given a screen (we don't want to use the whole state, just the pixels) and an action, i.e. `Q(screen, action) -> direct reward`.\n      * That function would need a forward pass for each possible action that we could take. So for e.g. 8 buttons that would be 8 forward passes. To make things more efficient, we can let the model directly predict the direct reward for each available action, e.g. for 3 buttons `Q(screen) -> (direct reward of action1, direct reward of action2, direct reward of action3)`.\n      * We can then sample examples from our replay memory. The input per example is the screen. The output is the reward as a tuple. E.g. if we picked button 1 of 3 in one example and received a reward of +1 then our output/label for that example would be `(1, 0, 0)`.\n      * We can then train the model by playing completely randomly for some time, then sample some batches and train using a mean squared error. Then play a bit less randomly, i.e. start to use the action which the network thinks would generate the highest reward. Then train again, and so on.\n    * Indirect reward\n      * Doing the previous steps, the model will learn to anticipate the *direct* reward correctly. However, we also want it to predict indirect rewards. Otherwise, the model e.g. would never learn to shoot rockets at enemies, because the reward from killing an enemy would come many frames later.\n      * To learn the indirect reward, one simply adds the reward value of highest reward action according to `Q(state2)` to the direct reward.\n      * I.e. if we have a tuple (`state1`, `action1`, `reward`, `state2`), we would not add (`state1`, `action1`, `reward`) to the replay memory, but instead (`state1`, `action1`, `reward + highestReward(Q(screen2))`). (Where `highestReward()` returns the reward of the action with the highest reward according to Q().)\n      * By training to predict `reward + highestReward(Q(screen2))` the network learns to anticipate the direct reward *and* the indirect reward. It takes a leap of faith to accept that this will ever converge to a good solution, but it does.\n      * We then add `gamma` to the equation: `reward + gamma*highestReward(Q(screen2))`. `gamma` may be set to 0.9. It is a discount factor that devalues future states, e.g. because the world is not deterministic and therefore we can't exactly predict what's going to happen. Note that Q will automatically learn to stack it, e.g. `state3` will be discounted to `gamma^2` at `state1`.\n  * This paper\n    * They use the mentioned Deep Q Learning to train their model Q.\n    * They use a k-th frame technique, i.e. they let the model decide upon an action at (here) every 4th frame.\n    * Q is implemented via a neural net. It receives 84x84x4 grayscale pixels that show the game and projects that onto the rewards of 4 to 18 actions.\n    * The input is HxWx4 because they actually feed the last 4 frames into the network, instead of just 1 frame. So the network knows more about what things are moving how.\n    * The network architecture is:\n      * 84x84x4 (input)\n      * 16 convs, 8x8, stride 4, ReLU\n      * 32 convs, 4x4, stride 2, ReLU\n      * 256 fully connected neurons, ReLU\n      * <N_actions> fully connected neurons, linear\n    * They use a replay memory of 1 million frames.\n\n### Results\n  * They ran experiments on the Atari games Beam Rider, Breakout, Enduro, Pong, Qbert, Seaquest and Space Invaders.\n  * Same architecture and hyperparameters for all games.\n  * Rewards were based on score changes in the games, i.e. they used +1 (score increases) and -1 (score decreased).\n  * Optimizer: RMSProp, Batch Size: 32.\n  * Trained for 10 million examples/frames per game.\n  * They had no problems with instability and their average Q value per game increased smoothly.\n  * Their method beats all other state of the art methods.\n  * They managed to beat a human player in games that required not so much \"long\" term strategies (the less frames the better).\n  * Video: starts at 46:05. \n https://youtu.be/dV80NAlEins?t=46m05s \n\n\n![Algorithm](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Playing_Atari_with_Deep_Reinforcement_Learning__algorithm.png?raw=true \"Algorithm\")\n\n*The original full algorithm, as shown in the paper.*\n\n--------------------\n\n### Rough chapter-wise notes\n\n* (1) Introduction\n  * Problems when using neural nets in reinforcement learning (RL):\n    * Reward signal is often sparse, noise and delayed.\n    * Often assumption that data samples are independent, while they are correlated in RL.\n    * Data distribution can change when the algorithm learns new behaviours.\n  * They use Q-learning with a CNN and stochastic gradient descent.\n  * They use an experience replay mechanism (i.e. memory) from which they can sample previous transitions (for training).\n  * They apply their method to Atari 2600 games in the Arcade Learning Environment (ALE).\n  * They use only the visible pixels as input to the network, i.e. no manual feature extraction.\n\n* (2) Background\n  * blablabla, standard deep q learning explanation\n\n* (3) Related Work\n  * TD-Backgammon: \"Solved\" backgammon. Worked similarly to Q-learning and used a multi-layer perceptron.\n  * Attempts to copy TD-Backgammon to other games failed.\n  * Research was focused on linear function approximators as there were problems with non-linear ones diverging.\n  * Recently again interest in using neural nets for reinforcement learning. Some attempts to fix divergence problems with gradient temporal-difference methods.\n  * NFQ is a very similar method (to the one in this paper), but worked on the whole batch instead of minibatches, making it slow. It also first applied dimensionality reduction via autoencoders on the images instead of training on them end-to-end.\n  * HyperNEAT was applied to Atari games and evolved a neural net for each game. The networks learned to exploit design flaws.\n\n* (4) Deep Reinforcement Learning\n  * They want to connect a reinforcement learning algorithm with a deep neural network, e.g. to get rid of handcrafted features.\n  * The network is supposes to run on the raw RGB images.\n  * They use experience replay, i.e. store tuples of (pixels, chosen action, received reward) in a memory and use that during training.\n  * They use Q-learning.\n  * They use an epsilon-greedy policy.\n  * Advantages from using experience replay instead of learning \"live\" during game playing:\n    * Experiences can be reused many times (more efficient).\n    * Samples are less correlated.\n    * Learned parameters from one batch don't determine as much the distributions of the examples in the next batch.\n  * They save the last N experiences and sample uniformly from them during training.\n  * (4.1) Preprocessing and Model Architecture\n    * Raw Atari images are 210x160 pixels with 128 possible colors.\n    * They downsample them to 110x84 pixels and then crop the 84x84 playing area out of them.\n    * They also convert the images to grayscale.\n    * They use the last 4 frames as input and stack them.\n    * So their network input has shape 84x84x4.\n    * They use one output neuron per possible action. So they can compute the Q-value (expected reward) of each action with one forward pass.\n    * Architecture: 84x84x4 (input) => 16 8x8 convs, stride 4, ReLU => 32 4x4 convs stride 2 ReLU => fc 256, ReLU => fc N actions, linear\n    * 4 to 18 actions/outputs (depends on the game).\n    * Aside from the outputs, the architecture is the same for all games.\n\n* (5) Experiments\n  * Games that they played: Beam Rider, Breakout, Enduro, Pong, Qbert, Seaquest, Space Invaders\n  * They use the same architecture und hyperparameters for all games.\n  * They give a reward of +1 whenever the in-game score increases and -1 whenever it decreases.\n  * They use RMSProp.\n  * Mini batch size was 32.\n  * They train for 10 million frames/examples.\n  * They initialize epsilon (in their epsilon greedy strategy) to 1.0 and decrease it linearly to 0.1 at one million frames.\n  * They let the agent decide upon an action at every 4th in-game frame (3rd in space invaders).\n  * (5.1) Training and stability\n    * They plot the average reward und Q-value per N games to evaluate the agent's training progress,\n    * The average reward increases in a noisy way.\n    * The average Q value increases smoothly.\n    * They did not experience any divergence issues during their training.\n  * (5.2) Visualizating the Value Function\n    * The agent learns to predict the value function accurately, even for rather long sequences (here: ~25 frames).\n  * (5.3) Main Evaluation\n    * They compare to three other methods that use hand-engineered features and/or use the pixel data combined with significant prior knownledge.\n    * They mostly outperform the other methods.\n    * They managed to beat a human player in three games. The ones where the human won seemed to require strategies that stretched over longer time frames.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1312.5602"
    },
    "920": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/EslamiHWTKH16",
        "transcript": "  * AIR (attend, infer, repeat) is a recurrent autoencoder architecture to transform images into latent representations object by object.\n  * As an autoencoder it is unsupervised.\n  * The latent representation is generated in multiple time steps.\n  * Each time step is intended to encode information about exactly one object in the image.\n  * The information encoded for each object is (mostly) a what-where information, i.e. which class the object has and where (in 2D: translation, scaling) it is shown.\n  * AIR has a dynamic number of time step. After encoding one object the model can decide whether it has encoded all objects or whether there is another one to encode. As a result the latent layer size is not fixed.\n  * AIR uses an attention mechanism during the encoding to focus on each object.\n\n### How\n  * At its core, AIR is a variational autoencoder.\n  * It maximizes lower bounds on the error instead of using a \"classic\" reconstruction error (like MSE on the euclidean distance).\n  * It has an encoder and a decoder.\n  * The model uses a recurrent architecture via an LSTM.\n  * It (ideally) encodes/decodes one object per time step.\n  * Encoder\n    * The encoder receives the image and generates latent information for one object (what object, where it is).\n    * At the second timestep it receives the image, the previous timestep's latent information and the previous timestep's hidden layer. It then generates another latent information (for another object).\n    * And so on.\n  * Decoder\n    * The decoder receives latent information from the encoder (timestep by timestep) and treats it as a what-where information when reconstructing the images.\n      * It takes the what-part and uses a \"normal\" decoder to generate an image that shows the object.\n      * It takes the where-part and the generated image and feeds both into a spatial transformer, which then transforms the generated image by translating or rotating it.\n  * Dynamic size\n    * AIR makes use of a dynamically sized latent layer. It is not necessarily limited to a fixed number of time steps.\n    * Implementation: Instead of just letting the encoder generate what-where information, the encoder also generates a \"present\" information, which is 0 or 1. If it is 1, the reccurence will continue with encoding and decoding another object. Otherwise it will stop.\n  * Attention\n    * To add an attention mechanism, AIR first uses the LSTM's hidden layer to generate \"where\" and \"present\" information per object.\n    * It stops if the \"present\" information is 0.\n    * Otherwise it uses the \"where\" information to focus on the object using a spatial transformer. The object is then encoded to the \"what\" information.\n\n### Results\n  * On a dataset of images, each containing multiple MNIST digits, AIR learns to accurately count the digits and estimate their position and scale.\n  * When AIR is trained on images of 0 to 2 digits and tested on images containing 3 digits it performs poorly.\n  * When AIR is trained on images of 0, 1 or 3 digits and tested on images containing 2 digits it performs mediocre.\n  * DAIR performs well on both tasks. Likely because it learns to remove each digit from the image after it has investigated it.\n  * When AIR is trained on 0 to 2 digits and a second network is trained (separately) to work with the generated latent layer (trained to sum the shown digits and rate whether they are shown in ascending order), then that second network reaches high accuracy with relatively few examples. That indicates usefulness for unsupervised learning.\n  * When AIR is trained on a dataset of handwritten characters from different alphabets, it learns to represent distinct strokes in its latent layer.\n  * When AIR is trained in combination with a renderer (inverse graphics), it is able to accurately recover latent parameters of rendered objects - better than supervised networks. That indicates usefulness for robots which have to interact with objects.\n\n\n![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Attend_Infer_Repeat__architecture.png?raw=true \"Architecture.\")\n\n*AIR architecture for MNIST. Left: Decoder for two objects that are each first generated (y_att) and then fed into a Spatial Transformer (y) before being combined into an image (x). Middle: , Right: Encoder with multiple time steps that generates what-where information per object and stops when the \"present\" information (z_pres) is 0. Right: Combination of both for MNIST with Spatial Transformer for the attention mechanism (top left).*\n\n\n![DAIR Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Attend_Infer_Repeat__architecture_dair.png?raw=true \"DAIR Architecture.\")\n\n*Encoder with DAIR architecture. DAIR modifies the image after every timestep (e.g. to remove objects that were already encoded).*\n\n\n--------------------\n\n### Rough chapter-wise notes\n\n* (1) Introduction\n  * Assumption: Images are made up of distinct objects. These objects have visual and physical properties.\n  * They developed a framework for efficient inference in images (i.e. get from the image to a latent representation of the objects, i.e. inverse graphics).\n  * Parts of the framework: High dimensional representations (e.g. object images), interpretable latent variables (e.g. for rotation) and generative processes (to combine object images with latent variables).\n  * Contributions:\n    * A scheme for efficient variational inference in latent spaces of variable dimensionality.\n      * Idea: Treat inference as an iterative process, implemented via an RNN that looks at one object at a time and learns an appropriate number of inference steps. (Attend-Infer-Repeat, AIR)\n      * End-to-end training via amortized variational inference (continuous variables: gradient descent, discrete variables: black-box optimization).\n    * AIR allows to train generative models that automatically learn to decompose scenes.\n    * AIR allows to recover objects and their attributes from rendered 3D scenes (inverse rendering).\n\n* (2) Approach\n  * Just like in VAEs, the scene interpretation is treated with a bayesian approach.\n  * There are latent variables `z` and images `x`.\n  * Images are generated via a probability distribution `p(x|z)`.\n  * This can be reversed via bayes rule to `p(x|z) = p(x)p(z|x) / p(z)`, which means that `p(x|z)p(z) / p(x) = p(z|x)`.\n  * The prior `p(z)` must be chosen and captures assumptions about the distributions of the latent variables.\n  * `p(x|z)` is the likelihood and represents the model that generates images from latent variables.\n  * They assume that there can be multiple objects in an image.\n  * Every object gets its own latent variables.\n  * A probability distribution p(x|z) then converts each object (on its own) from the latent variables to an image.\n  * The number of objects follows a probability distribution `p(n)`.\n  * For the prior and likelihood they assume two scenarios:\n    * 2D: Three dimensions for X, Y and scale. Additionally n dimensions for its shape.\n    * 3D: Dimensions for X, Y, Z, rotation, object identity/category (multinomial variable). (No scale?)\n  * Both 2D and 3D can be separated into latent variables for \"where\" and \"what\".\n  * It is assumed that the prior latent variables are independent of each other.\n  * (2.1) Inference\n    * Inference for their model is intractable, therefore they use an approximation `q(z,n|x)`, which minizes `KL(q(z,n|x)||p(z,n|x))`, i.e. KL(approximation||real) using amortized variational approximation.\n    * Challenges for them:\n      * The dimensionality of their latent variable layer is a random variable p(n) (i.e. no static size.).\n      * Strong symmetries.\n    * They implement inference via an RNN which encodes the image object by object.\n    * The encoded latent variables can be gaussians.\n    * They encode the latent layer length `n` via a vector (instead of an integer). The vector has the form of `n` ones followed by one zero.\n    * If the length vector is `#z` then they want to approximate `q(z,#z|x)`.\n    * That can apparently be decomposed into `<product> q(latent variable value i, #z is still 1 at i|x, previous latent variable values) * q(has length n|z,x)`.\n    * So instead of computing `#z` once, they instead compute at every time step whether there is another object in the image, which indirectly creates a chain of ones followed by a zero (the `#z` vector).\n  * (2.2) Learning\n    * The parameters theta (`p`, latent variable -> image) and phi (`q`, image -> latent variables) are jointly optimized.\n    * Optimization happens by maximizing a lower bound `E[log(p(x,z,n) / q(z,n|x))]` called the negative free energy.\n    * (2.2.1) Parameters of the model theta\n      * Parameters theta of log(p(x,z,n)) can easily be obtained using differentiation, so long as z and n are well approximated.\n      * The differentiation of the lower bound with repsect to theta can be approximated using Monte Carlo methods.\n    * (2.2.2) Parameters of the inference network phi\n      * phi are the parameters of q, i.e. of the RNN that generates z and #z in i timesteps.\n      * At each timestep (i.e. per object) the RNN generates three kinds of information: What (object), where (it is), whether it is present (i <= n).\n      * Each of these information is represented via variables. These variables can be discrete or continuous.\n      * When differentiating w.r.t. a continuous variable they use the reparameterization trick.\n      * When differentiating w.r.t. a discrete variable they use the likelihood ratio estimator.\n\n* (3) Models and Experiments\n  * The RNN is implemented via an LSTM.\n  * DAIR\n    * The \"normal\" AIR model uses at every time step the image and the RNN's hidden layer to generate the next latent information (what object, where it is and whether it is present).\n    * DAIR uses that latent information to change the image at every time step and then use the difference (D) image for the next time step, i.e. DAIR can remove an object from the image after it has generated latent variables for it.\n  * (3.1) Multi-MNIST\n    * They generate a dataset of images containing multiple MNIST digits.\n    * Each image contains 0 to 2 digits.\n    * AIR is trained on the dataset.\n    * It learns without supervision a good attention scanning policy for the images (to \"hit\" all digits), to count the digits visible in the image and to use a matching number of time steps.\n    * During training, the model seems to first learn proper reconstruction of the digits and only then to do it with as few timesteps as possible.\n    * (3.1.1) Strong Generalization\n      * They test the generalization capabilities of AIR.\n      * *Extrapolation task*: They generate images with 0 to 2 digits for training, then test on images with 3 digits. The model is unable to correctly count the digits (~0% accuracy).\n      * *Interpolation task*: They generate images with 0, 1 or 3 digits for training, then test on images with 2 digits. The model performs OK-ish (~60% accuracy).\n      * DAIR performs in both cases well (~80% for extrapolation, ~95% accuracy for interpolation).\n    * (3.1.2) Representational Power\n      * They train AIR on images containing 0, 1 or 2 digits.\n      * Then they train a second network. That network takes the output of the first one and computes a) the sum of the digits and b) estimates whether they are shown in ascending order.\n      * Accuracy for both tasks is ~95%.\n      * The network reaches that accuracy significantly faster than a separately trained CNN (i.e. requires less labels / is more unsupervised).\n  * (3.2) Omniglot\n    * They train AIR on the Omniglot dataset (1.6k handwritten characters from 50 alphabets).\n    * They allow the model to use up to 4 timesteps.\n    * The model learns to reconstruct the images in timesteps that resemble strokes.\n  * (3.3) 3D Scenes\n    * Here, the generator p(x|z) is a 3D renderer, only q(z|x) must be approximated.\n    * The model has to learn to count the objects and to estimate per object its identity (class) and pose.\n    * They use \"finite-differencing\" to get gradients through the renderer and use \"score function estimators\" to get gradients with respect to discrete variables.\n    * They first test with a setup where the object count is always 1. The network learns to accurately recover the object parameters.\n    * A similar \"normal\" network has much more problems with recovering the parameters, especially rotation, because the conditional probabilities are multi-modal. The lower bound maximization strategy seems to work better in those cases.\n    * In a second experiment with multiple complex objects, AIR also achieves high reconstruction accuracy.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.08575"
    },
    "921": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/GulcehreMDB16",
        "transcript": "\n  * Certain activation functions, mainly sigmoid, tanh, hard-sigmoid and hard-tanh can saturate.\n  * That means that their gradient is either flat 0 after threshold values (e.g. -1 and +1) or that it approaches zero for high/low values.\n  * If there's no gradient, training becomes slow or stops completely.\n  * That's a problem, because sigmoid, tanh, hard-sigmoid and hard-tanh are still often used in some models, like LSTMs, GRUs or Neural Turing Machines.\n  * To fix the saturation problem, they add noise to the output of the activation functions.\n  * The noise increases as the unit saturates.\n  * Intuitively, once the unit is saturating, it will occasionally \"test\" an activation in the non-saturating regime to see if that output performs better.\n\n### How\n  * The basic formula is: `phi(x,z) = alpha*h(x) + (1-alpha)u(x) + d(x)std(x)epsilon`\n  * Variables in that formula:\n    * Non-linear part `alpha*h(x)`:\n      * `alpha`: A constant hyperparameter that determines the \"direction\" of the noise and the slope. Values below 1.0 let the noise point away from the unsaturated regime. Values <=1.0 let it point towards the unsaturated regime (higher alpha = stronger noise).\n      * `h(x)`: The original activation function.\n    * Linear part `(1-alpha)u(x)`:\n      * `u(x)`: First-order Taylor expansion of h(x).\n        * For sigmoid: `u(x) = 0.25x + 0.5`\n        * For tanh: `u(x) = x`\n        * For hard-sigmoid: `u(x) = max(min(0.25x+0.5, 1), 0)`\n        * For hard-tanh: `u(x) = max(min(x, 1), -1)`\n    * Noise/Stochastic part `d(x)std(x)epsilon`:\n      * `d(x) = -sgn(x)sgn(1-alpha)`: Changes the \"direction\" of the noise.\n      * `std(x) = c(sigmoid(p*v(x))-0.5)^2 = c(sigmoid(p*(h(x)-u(x)))-0.5)^2`\n        * `c` is a hyperparameter that controls the scale of the standard deviation of the noise.\n        * `p` controls the magnitude of the noise. Due to the `sigmoid(y)-0.5` this can influence the sign. `p` is learned.\n      * `epsilon`: A noise creating random variable. Usually either a Gaussian or the positive half of a Gaussian (i.e. `z` or `|z|`).\n  * The hyperparameter `c` can be initialized at a high value and then gradually decreased over time. That would be comparable to simulated annealing.\n  * Noise could also be applied to the input, i.e. `h(x)` becomes `h(x + noise)`.\n\n### Results\n  * They replaced sigmoid/tanh/hard-sigmoid/hard-tanh units in various experiments (without further optimizations).\n  * The experiments were:\n    * Learn to execute source code (LSTM?)\n    * Language model from Penntreebank (2-layer LSTM)\n    * Neural Machine Translation engine trained on Europarl (LSTM?)\n    * Image caption generation with soft attention trained on Flickr8k (LSTM)\n    * Counting unique integers in a sequence of integers (LSTM)\n    * Associative recall (Neural Turing Machine)\n  * Noisy activations practically always led to a small or moderate improvement in resulting accuracy/NLL/BLEU.\n  * In one experiment annealed noise significantly outperformed unannealed noise, even beating careful curriculum learning. (Somehow there are not more experiments about that.)\n  * The Neural Turing Machine learned far faster with noisy activations and also converged to a much better solution. \n\n\n![Influence of alphas](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Noisy_Activation_Functions__alphas.png?raw=true \"Influence of alphas.\")\n\n*Hard-tanh with noise for various alphas. Noise increases in different ways in the saturing regimes.*\n\n\n![Neural Turing Machine results](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Noisy_Activation_Functions__ntm.png?raw=true \"Neural Turing Machine results.\")\n\n*Performance during training of a Neural Turing Machine with and without noisy activation units.*\n\n--------------------\n\n# Rough chapter-wise notes\n\n* (1) Introduction\n  * ReLU and Maxout activation functions have improved the capabilities of training deep networks.\n  * Previously, tanh and sigmoid were used, which were only suited for shallow networks, because they saturate, which kills the gradient.\n  * They suggest a different avenue: Use saturating nonlinearities, but inject noise when they start to saturate (and let the network learn how much noise is \"good\").\n  * The noise allows to train deep networks with saturating activation functions.\n  * Many current architectures (LSTMs, GRUs, Neural Turing Machines, ...) require \"hard\" decisions (yes/no). But they use \"soft\" activation functions to implement those, because hard functions lack gradient.\n  * The soft activation functions can still saturate (no more gradient) and don't match the nature of the binary decision problem. So it would be good to replace them with something better.\n  * They instead use hard activation functions and compensate for the lack of gradient by using noise (during training).\n  * Networks with hard activation functions outperform those with soft ones.\n\n* (2) Saturating Activation Functions\n  * Activation Function = A function that maps a real value to a new real value and is differentiable almost everywhere.\n  * Right saturation = The gradient of an activation function becomes 0 if the input value goes towards infinity.\n  * Left saturation = The gradient of an activation function becomes 0 if the input value goes towards -infinity.\n  * Saturation = A activation function saturates if it right-saturates and left-saturates.\n  * Hard saturation = If there is a constant c for which for which the gradient becomes 0.\n  * Soft saturation = If there is no constant, i.e. the input value must become +/- infinity.\n  * Soft saturating activation functions can be converted to hard saturating ones by using a first-order Taylor expansion and then clipping the values to the required range (e.g. 0 to 1).\n  * A hard activating tanh is just `f(x) = x`. With clipping to [-1, 1]: `max(min(f(x), 1), -1)`.\n  * The gradient for hard activation functions is 0 above/below certain constants, which will make training significantly more challenging.\n  * hard-sigmoid, sigmoid and tanh are contractive mappings, hard-tanh for some reason only when it's greater than the threshold.\n  * The fixed-point for tanh is 0, for the others !=0. That can have influences on the training performance.\n\n* (3) Annealing with Noisy Activation Functions\n  * Suppose that there is an activation function like hard-sigmoid or hard-tanh with additional noise (iid, mean=0, variance=std^2).\n  * If the noise's `std` is 0 then the activation function is the original, deterministic one.\n  * If the noise's `std` is very high then the derivatives and gradient become high too. The noise then \"drowns\" signal and the optimizer just moves randomly through the parameter space.\n  * Let the signal to noise ratio be `SNR = std_signal / std_noise`. So if SNR is low then noise drowns the signal and exploration is random.\n  * By letting SNR grow (i.e. decreaseing `std_noise`) we switch the model to fine tuning mode (less coarse exploration).\n  * That is similar to simulated annealing, where noise is also gradually decreased to focus on better and better regions of the parameter space.\n\n* (4) Adding Noise when the Unit Saturate\n  * This approach does not always add the same noise. Instead, noise is added proportinally to the saturation magnitude. More saturation, more noise.\n  * That results in a clean signal in \"good\" regimes (non-saturation, strong gradients) and a noisy signal in \"bad\" regimes (saturation).\n  * Basic activation function with noise: `phi(x, z) = h(x) + (mu + std(x)*z)`, where `h(x)` is the saturating activation function, `mu` is the mean of the noise, `std` is the standard deviation of the noise and `z` is a random variable.\n  * Ideally the noise is unbiased so that the expectation values of `phi(x,z)` and `h(x)` are the same.\n  * `std(x)` should take higher values as h(x) enters the saturating regime.\n  * To calculate how \"saturating\" a activation function is, one can `v(x) = h(x) - u(x)`, where `u(x)` is the first-order Taylor expansion of `h(x)`.\n  * Empirically they found that a good choice is `std(x) = c(sigmoid(p*v(x)) - 0.5)^2` where `c` is a hyperparameter and `p` is learned.\n  * (4.1) Derivatives in the Saturated Regime\n    * For values below the threshold, the gradient of the noisy activation function is identical to the normal activation function.\n    * For values above the threshold, the gradient of the noisy activation function is `phi'(x,z) = std'(x)*z`. (Assuming that z is unbiased so that mu=0.)\n  * (4.2) Pushing Activations towards the Linear Regime\n    * In saturated regimes, one would like to have more of the noise point towards the unsaturated regimes than away from them (i.e. let the model try often whether the unsaturated regimes might be better).\n    * To achieve this they use the formula `phi(x,z) = alpha*h(x) + (1-alpha)u(x) + d(x)std(x)epsilon`\n      * `alpha`: A constant hyperparameter that determines the \"direction\" of the noise and the slope. Values below 1.0 let the noise point away from the unsaturated regime. Values <=1.0 let it point towards the unsaturated regime (higher alpha = stronger noise).\n      * `h(x)`: The original activation function.\n      * `u(x)`: First-order Taylor expansion of h(x).\n      * `d(x) = -sgn(x)sgn(1-alpha)`: Changes the \"direction\" of the noise.\n      * `std(x) = c(sigmoid(p*v(x))-0.5)^2 = c(sigmoid(p*(h(x)-u(x)))-0.5)^2` with `c` being a hyperparameter and `p` learned.\n      * `epsilon`: Either `z` or `|z|`. If `z` is a Gaussian, then `|z|` is called \"half-normal\" while just `z` is called \"normal\". Half-normal lets the noise only point towards one \"direction\" (towards the unsaturated regime or away from it), while normal noise lets it point in both directions (with the slope being influenced by `alpha`).\n    * The formula can be split into three parts:\n      * `alpha*h(x)`: Nonlinear part.\n      * `(1-alpha)u(x)`: Linear part.\n      * `d(x)std(x)epsilon`: Stochastic part.\n    * Each of these parts resembles a path along which gradient can flow through the network.\n    * During test time the activation function is made deterministic by using its expectation value: `E[phi(x,z)] = alpha*h(x) + (1-alpha)u(x) + d(x)std(x)E[epsilon]`.\n    * If `z` is half-normal then `E[epsilon] = sqrt(2/pi)`. If `z` is normal then `E[epsilon] = 0`.\n\n* (5) Adding Noise to Input of the Function\n  * Noise can also be added to the input of an activation function, i.e. `h(x)` becomes `h(x + noise)`.\n  * The noise can either always be applied or only once the input passes a threshold.\n\n* (6) Experimental Results\n  * They applied noise only during training.\n  * They used existing setups and just changed the activation functions to noisy ones. No further optimizations.\n  * `p` was initialized uniformly to [-1,1].\n  * Basic experiment settings:\n    * NAN: Normal noise applied to the outputs.\n    * NAH: Half-normal noise, i.e. `|z|`, i.e. noise is \"directed\" towards the unsaturated or satured regime.\n    * NANI: Normal noise applied to the *input*, i.e. `h(x+noise)`.\n    * NANIL: Normal noise applied to the input with learned variance.\n    * NANIS: Normal noise applied to the input, but only if the unit saturates (i.e. above/below thresholds).\n  * (6.1) Exploratory analysis\n    * A very simple MNIST network performed slightly better with noisy activations than without. But comparison was only to tanh and hard-tanh, not ReLU or similar.\n    * In an experiment with a simple GRU, NANI (noisy input) and NAN (noisy output) performed practically identical. NANIS (noisy input, only when saturated) performed significantly worse.\n  * (6.2) Learning to Execute\n    * Problem setting: Predict the output of some lines of code.\n    * They replaced sigmoids and tanhs with their noisy counterparts (NAH, i.e. half-normal noise on output). The model learned faster.\n  * (6.3) Penntreebank Experiments\n    * They trained a standard 2-layer LSTM language model on Penntreebank.\n    * Their model used noisy activations, as opposed to the usually non-noisy ones.\n    * They could improve upon the previously best value. Normal noise and half-normal noise performed roughly the same.\n  * (6.4) Neural Machine Translation Experiments\n    * They replaced all sigmoids and tanh units in the Neural Attention Model with noisy ones. Then they trained on the Europarl corpus.\n    * They improved upon the previously best score.\n  * (6.5) Image Caption Generation Experiments\n    * They train a network with soft attention to generate captions for the Flickr8k dataset.\n    * Using noisy activation units improved the result over normal sigmoids and tanhs.\n  * (6.6) Experiments with Continuation\n    * They build an LSTM and train it to predict how many unique integers there are in a sequence of random integers.\n    * Instead of using a constant value for hyperparameter `c` of the noisy activations (scale of the standard deviation of the noise), they start at `c=30` and anneal down to `c=0.5`.\n    * Annealed noise performed significantly better then unannealed noise.\n    * Noise applied to the output (NAN) significantly beat noise applied to the input (NANIL).\n    * In a second experiment they trained a Neural Turing Machine on the associative recall task.\n    * Again they used annealed noise.\n    * The NTM with annealed noise learned by far faster than the one without annealed noise and converged to a perfect solution.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v48/gulcehre16.html"
    },
    "922": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/IandolaMAHDK16",
        "transcript": "  * The authors train a variant of AlexNet that has significantly fewer parameters than the original network, while keeping the network's accuracy stable.\n  * Advantages of this:\n    * More efficient distributed training, because less parameters have to be transferred.\n    * More efficient transfer via the internet, because the model's file size is smaller.\n    * Possibly less memory demand in production, because fewer parameters have to be kept in memory.\n\n### How\n  * They define a Fire Module. A Fire Module contains of:\n    * Squeeze Module: A 1x1 convolution that reduces the number of channels (e.g. from 128x32x32 to 64x32x32).\n    * Expand Module: A 1x1 convolution and a 3x3 convolution, both applied to the output of the Squeeze Module. Their results are concatenated.\n  * Using many 1x1 convolutions is advantageous, because they need less parameters than 3x3s.\n  * They use ReLUs, only convolutions (no fully connected layers) and Dropout (50%, before the last convolution).\n  * They use late maxpooling. They argue that applying pooling late - rather than early - improves accuracy while not needing more parameters.\n  * They try residual connections:\n    * One network without any residual connections (performed the worst).\n    * One network with residual connections based on identity functions, but only between layers of same dimensionality (performed the best).\n    * One network with residual connections based on identity functions and other residual connections with 1x1 convs (where dimensionality changed) (performance between the other two).\n  * They use pruning from Deep Compression to reduce the parameters further. Pruning simply collects the 50% of all parameters of a layer that have the lowest values and sets them to zero. That creates a sparse matrix.\n\n### Results\n  * 50x parameter reduction of AlexNet (1.2M parameters before pruning, 0.4M after pruning).\n  * 510x file size reduction of AlexNet (from 250mb to 0.47mb) when combined with Deep Compression.\n  * Top-1 accuracy remained stable.\n  * Pruning apparently can be used safely, even after the network parameters have already been reduced significantly.\n  * While pruning was generally safe, they found that two of their later layers reacted quite sensitive to it. Adding parameters to these (instead of removing them) actually significantly improved accuracy.\n  * Generally they found 1x1 convs to react more sensitive to pruning than 3x3s. Therefore they focused pruning on 3x3 convs.\n  * First pruning a network, then re-adding the pruned weights (initialized with 0s) and then retraining for some time significantly improved accuracy.\n  * The network was rather resilient to significant channel reduction in the Squeeze Modules. Reducing to 25-50% of the original channels (e.g. 128x32x32 to 64x32x32) seemed to be a good choice.\n  * The network was rather resilient to removing 3x3 convs and replacing them with 1x1 convs. A ratio of 2:1 to 1:1 (1x1 to 3x3) seemed to produce good results while mostly keeping the accuracy.\n  * Adding some residual connections between the Fire Modules improved the accuracy.\n  * Adding residual connections with identity functions *and also* residual connections with 1x1 convs (where dimensionality changed) improved the accuracy, but not as much as using *only* residual connections with identity functions (i.e. it's better to keep some modules without identity functions).\n\n\n--------------------\n\n### Rough chapter-wise notes\n\n* (1) Introduction and Motivation\n  * Advantages from having less parameters:\n    * More efficient distributed training, because less data (parameters) have to be transfered.\n    * Less data to transfer to clients, e.g. when a model used by some app is updated.\n    * FPGAs often have hardly any memory, i.e. a model has to be small to be executed.\n  * Target here: Find a CNN architecture with less parameters than an existing one but comparable accuracy.\n\n* (2) Related Work\n  * (2.1) Model Compression\n    * SVD-method: Just apply SVD to the parameters of an existing model.\n    * Network Pruning: Replace parameters below threshold with zeros (-> sparse matrix), then retrain a bit.\n    * Add quantization and huffman encoding to network pruning = Deep Compression.\n  * (2.2) CNN Microarchitecture\n    * The term \"CNN Microarchitecture\" refers to the \"organization and dimensions of the individual modules\" (so an Inception module would have a complex CNN microarchitecture).\n  * (2.3) CNN Macroarchitecture\n    * CNN Macroarchitecture = \"big picture\" / organization of many modules in a network / general characteristics of the network, like depth\n    * Adding connections between modules can help (e.g. residual networks)\n  * (2.4) Neural Network Design Space Exploration\n    * Approaches for Design Space Exporation (DSE):\n      * Bayesian Optimization, Simulated Annealing, Randomized Search, Genetic Algorithms\n\n* (3) SqueezeNet: preserving accuracy with few parameters\n  * (3.1) Architectural Design Strategies\n    * A conv layer with N filters applied to CxHxW input (e.g. 3x128x128 for a possible first layer) with kernel size kHxkW (e.g. 3x3) has `N*C*kH*kW` parameters.\n    * So one way to reduce the parameters is to decrease kH and kW, e.g. from 3x3 to 1x1 (reduces parameters by a factor of 9).\n    * A second way is to reduce the number of channels (C), e.g. by using 1x1 convs before the 3x3 ones.\n    * They think that accuracy can be improved by performing downsampling later in the network (if parameter count is kept constant).\n  * (3.2) The Fire Module\n    * The Fire Module has two components:\n      * Squeeze Module:\n        * One layer of 1x1 convs\n      * Expand Module:\n        * Concat the results of:\n          * One layer of 1x1 convs\n          * One layer of 3x3 convs\n    * The Squeeze Module decreases the number of input channels significantly.\n    * The Expand Module then increases the number of input channels again.\n  * (3.3) The SqueezeNet architecture\n    * One standalone conv, then several fire modules, then a standalone conv, then global average pooling, then softmax.\n    * Three late max pooling laters.\n    * Gradual increase of filter numbers.\n    * (3.3.1) Other SqueezeNet details\n      * ReLU activations\n      * Dropout before the last conv layer.\n      * No linear layers.\n\n* (4) Evaluation of SqueezeNet\n  * Results of competing methods:\n    * SVD: 5x compression, 56% top-1 accuracy\n    * Pruning: 9x compression, 57.2% top-1 accuracy\n    * Deep Compression: 35x compression, ~57% top-1 accuracy\n  * SqueezeNet: 50x compression, ~57% top-1 accuracy\n  * SqueezeNet combines low parameter counts with Deep Compression.\n  * The accuracy does not go down because of that, i.e. apparently Deep Compression can even be applied to small models without giving up on performance.\n\n* (5) CNN Microarchitecture Design Space Exploration\n  * (5.1) CNN Microarchitecture metaparameters\n    * blabla we test various values for this and that parameter\n  * (5.2) Squeeze Ratio\n    * In a Fire Module there is first a Squeeze Module and then an Expand Module. The Squeeze Module decreases the number of input channels to which 1x1 and 3x3 both are applied (at the same time).\n    * They analyzed how far you can go down with the Sqeeze Module by training multiple networks and calculating the top-5 accuracy for each of them.\n    * The accuracy by Squeeze Ratio (percentage of input channels kept in 1x1 squeeze, i.e. 50% = reduced by half, e.g. from 128 to 64):\n      * 12%: ~80% top-5 accuracy\n      * 25%: ~82% top-5 accuracy\n      * 50%: ~85% top-5 accuracy\n      * 75%: ~86% top-5 accuracy\n      * 100%: ~86% top-5 accuracy\n  * (5.3) Trading off 1x1 and 3x3 filters\n    * Similar to the Squeeze Ratio, they analyze the optimal ratio of 1x1 filters to 3x3 filters.\n    * E.g. 50% would mean that half of all filters in each Fire Module are 1x1 filters.\n    * Results:\n      * 01%: ~76% top-5 accuracy\n      * 12%: ~80% top-5 accuracy\n      * 25%: ~82% top-5 accuracy\n      * 50%: ~85% top-5 accuracy\n      * 75%: ~85% top-5 accuracy\n      * 99%: ~85% top-5 accuracy\n\n* (6) CNN Macroarchitecture Design Space Exploration\n  * They compare the following networks:\n    * (1) Without residual connections\n    * (2) With residual connections between modules of same dimensionality\n    * (3) With residual connections between all modules (except pooling layers) using 1x1 convs (instead of identity functions) where needed\n  * Adding residual connections (2) improved top-1 accuracy from 57.5% to 60.4% without any new parameters.\n  * Adding complex residual connections (3) worsed top-1 accuracy again to 58.8%, while adding new parameters.\n\n* (7) Model Compression Design Space Exploration\n  * (7.1) Sensitivity Analysis: Where to Prune or Add parameters\n    * They went through all layers (including each one in the Fire Modules).\n    * In each layer they set the 50% smallest weights to zero (pruning) and measured the effect on the top-5 accuracy.\n    * It turns out that doing that has basically no influence on the top-5 accuracy in most layers.\n    * Two layers towards the end however had significant influence (accuracy went down by 5-10%).\n    * Adding parameters to these layers improved top-1 accuracy from 57.5% to 59.5%.\n    * Generally they found 1x1 layers to be more sensitive than 3x3 layers so they pruned them less aggressively.\n  * (7.2) Improving Accuracy by Densifying Sparse Models\n    * They found that first pruning a model and then retraining it again (initializing the pruned weights to 0) leads to higher accuracy.\n    * They could improve top-1 accuracy by 4.3% in this way.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.07360"
    },
    "923": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.07571",
        "transcript": "  * They define four subtasks of image understanding:\n    * *Classification*: Assign a single label to a whole image.\n    * *Captioning*: Assign a sequence of words (description) to a whole image*\n    * *Detection*: Find objects/regions in an image and assign a single label to each one.\n    * *Dense Captioning*: Find objects/regions in an image and assign a sequence of words (description) to each one.\n  * DenseCap accomplishes the fourth task, i.e. it is a model that finds objects/regions in images and describes them with natural language.\n\n### How\n  * Their model consists of four subcomponents, which run for each image in sequence:\n    * (1) **Convolutional Network**:\n      * Basically just VGG-16.\n    * (2) **Localization Layer**:\n      * This layer uses a convolutional network that has mostly the same architecture as in the \"Faster R-CNN\" paper.\n      * That ConvNet is applied to a grid of anchor points on the image.\n      * For each anchor point, it extracts the features generated by the VGG-Net (model 1) around that point.\n      * It then generates the attributes of `k` (default: 12) boxes using a shallow convolutional net. These attributes are (roughly): Height, width, center x, center y, confidence score.\n      * It then extracts the features of these boxes from the VGG-Net output (model 1) and uses bilinear sampling to project them onto a fixed size (height, width) for the next model. The result are the final region proposals.\n      * By default every image pixel is an anchor point, which results in a large number of regions. Hence, subsampling is used during training and testing.\n    * (3) **Recognition Network**:\n      * Takes a region (flattened to 1d vector) and projects it onto a vector of length 4096.\n      * It uses fully connected layers to do that (ReLU, dropout).\n      * Additionally, the network takes the 4096 vector and outputs new values for the region's position and confidence (for late fine tuning).\n      * The 4096 vectors of all regions are combined to a matrix that is fed into the next component (RNN).\n      * The intended sense of the this component seems to be to convert the \"visual\" features of each region to a more abstract, high-dimensional representation/description.\n    * (4) **RNN Language Model**:\n      * The take each 4096 vector and apply a fully connected layer + ReLU to it.\n      * Then they feed it into an LSTM, followed by a START token.\n      * The LSTM then generates word (as one hot vectors), which are fed back into the model for the next time step.\n      * This is continued until the LSTM generates an END token.\n  * Their full loss function has five components:\n    * Binary logistic loss for the confidence values generated by the localization layer.\n    * Binary logistic loss for the confidence values generated by the recognition layer.\n    * Smooth L1 loss for the region dimensions generated by the localization layer.\n    * Smooth L1 loss for the region dimensiosn generated by the recognition layer.\n    * Cross-entropy at every time-step of the language model.\n  * The whole model can be trained end-to-end.\n\n* Results\n  * They mostly use the Visual Genome dataset.\n  * Their model finds lots of good regions in images.\n  * Their model generates good captions for each region. (Only short captions with simple language however.)\n  * The model seems to love colors. Like 30-50% of all captions contain a color. (Probably caused by the dataset?)\n  * They compare to EdgeBoxes (other method to find regions in images). Their model seems to perform better.\n  * Their model requires about 240ms per image (test time).\n  * The generated regions and captions enable one to search for specific objects in images using text queries.\n\n\n![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/DenseCap__architecture.png?raw=true \"Architecture.\")\n\n*Architecture of the whole model. It starts with the VGG-Net (\"CNN\"), followed by the localization layer, which generates region proposals. Then the recognition network converts the regions to abstract high-dimensional representations. Then the language model (\"RNN\") generates the caption.*\n\n\n![Elephant image with dense captioning.](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/DenseCap__elephant.png?raw=true \"Elephant image with dense captioning.\")\n\n![Airplane image with dense captioning.](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/DenseCap__airplane.png?raw=true \"Airplane image with dense captioning.\")\n\n\n--------------------\n\n### Rough chapter-wise notes\n\n* (1) Introduction\n  * They define four subtasks of visual scene understanding:\n    * Classification: Assign a single label to a whole image\n    * Captioning: Assign a sequence of words (description) to a whole image\n    * Detection: Find objects in an image and assign a single label to each one\n    * Dense Captioning: Find objects in an image and assign a sequence of words (description) to each one\n  * They developed a model for dense captioning.\n  * It has two three important components:\n    * A convoltional network for scene understanding\n    * A localization layer for region level predictions. It predicts regions of interest and then uses bilinear sampling to extract the activations of these regions.\n    * A recurrent network as the language model\n  * They evaluate the model on the large-scale Visual Genome dataset (94k images, 4.1M region captions).\n\n* (3) Model\n  * Model architecture\n    * Convolutional Network\n      * They use VGG-16, but remove the last pooling layer.\n      * For an image of size W, H the output is 512xW/16xH/16.\n      * That output is the input into the localization layer.\n    * Fully Convolutional Localization Layer\n      * Input to this layer: Activations from the convolutional network.\n      * Output of this layer: Regions of interest, as fixed-sized representations.\n        * For B Regions:\n          * Coordinates of the bounding boxes (matrix of shape Bx4)\n          * Confidence scores (vector of length B)\n          * Features (matrix of shape BxCxXxY)\n      * Method: Faster R-CNN (pooling replaced by bilinear interpolation)\n      * This layer is fully differentiable.\n      * The localization layer predicts boxes at anchor points.\n      * At each anchor point it proposes `k` boxes using a small convolutional network. It assigns a confidence score and coordinates (center x, center y, height, width) to each proposal.\n      * For an image with size 720x540 and k=12 the model would have to predict 17,280 boxes, hence subsampling is used.\n      * During training they use minibatches with 256/2 positive and 256/2 negative region examples. A box counts as a positive example for a specific image if it has high overlap (intersection) with an annotated box for that image.\n      * During test time they use greedy non-maximum suppression (NMS) (?) to subsample the 300 most confident boxes.\n      * The region proposals have varying box sizes, but the output of the localization layer (which will be fed into the RNN) is ought to have fixed sizes.\n      * So they project each proposed region onto a fixed sized region. They use bilinear sampling for that projection, which is differentiable.\n    * Recognition network\n      * Each region is flattened to a one-dimensional vector.\n      * That vector is fed through 2 fully connected layers (unknown size, ReLU, dropout), ending with a 4096 neuron layer.\n      * The confidence score and box coordinates are also adjusted by the network during that process (fine tuning).\n    * RNN Language Model\n      * Each region is translated to a sentence.\n      * The region is fed into an LSTM (after a linear layer + ReLU), followed by a special START token.\n      * The LSTM outputs multiple words as one-hot-vectors, where each vector has the length `V+1` (i.e. vocabulary size + END token).\n      * Loss function is average crossentropy between output words and target words.\n      * During test time, words are sampled until an END tag is generated.\n  * Loss function\n    * Their full loss function has five components:\n      * Binary logistic loss for the confidence values generated by the localization layer.\n      * Binary logistic loss for the confidence values generated by the recognition layer.\n      * Smooth L1 loss for the region dimensions generated by the localization layer.\n      * Smooth L1 loss for the region dimensiosn generated by the recognition layer.\n      * Cross-entropy at every time-step of the language model.\n    * The language model term has a weight of 1.0, all other components have a weight of 0.1.\n  * Training an optimization\n    * Initialization: CNN pretrained on ImageNet, all other weights from `N(0, 0.01)`.\n    * SGD for the CNN (lr=?, momentum=0.9)\n    * Adam everywhere else (lr=1e-6, beta1=0.9, beta2=0.99)\n    * CNN is trained after epoch 1. CNN's first four layers are not trained.\n    * Batch size is 1.\n    * Image size is 720 on the longest side.\n    * They use Torch.\n    * 3 days of training time.\n\n* (4) Experiments\n  * They use the Visual Genome Dataset (94k images, 4.1M regions with captions)\n  * Their total vocabulary size is 10,497 words. (Rare words in captions were replaced with `<UNK>`.)\n  * They throw away annotations with too many words as well as images with too few/too many regions.\n  * They merge heavily overlapping regions to single regions with multiple captions.\n  * Dense Captioning\n    * Dense captioning task: The model receives one image and produces a set of regions, each having a caption and a confidence score.\n    * Evaluation metrics\n      * Evaluation of the output is non-trivial.\n      * They compare predicted regions with regions from the annotation that have high overlap (above a threshold).\n      * They then compare the predicted caption with the captions having similar METEOR score (above a threshold).\n      * Instead of setting one threshold for each comparison they use multiple thresholds. Then they calculate the Mean Average Precision using the various pairs of thresholds.\n    * Baseline models\n      * Sources of region proposals during test time:\n        * GT: Ground truth boxes (i.e. found by humans).\n        * EB: EdgeBox (completely separate and pretrained system).\n        * RPN: Their localization and recognition networks trained separately on VG regions dataset (i.e. trained without the RNN language model).\n      * Models:\n        * Region RNN model: Apparently the recognition layer and the RNN language model, trained on predefined regions. (Where do these regions come from? VG training dataset?)\n        * Full Image RNN model: Apparently the recognition layer and the RNN language model, trained on full images from MSCOCO instead of small regions.\n        * FCLN on EB: Apparently the recognition layer and the RNN language model, trained on regions generated by EdgeBox (EB) (on VG dataset?).\n        * FCLN: Apparently their full model (trained on VG dataset?).\n    * Discrepancy between region and image level statistics\n      * When evaluating the models only on METEOR (language \"quality\"), the *Region RNN model* consistently outperforms the *Full Image RNN model*.\n      * That's probably because the *Full Image RNN model* was trained on captions of whole images, while the *Region RNN model* was trained on captions of small regions, which tend to be a bit different from full image captions.\n    * RPN outperforms external region proposals\n      * Generating region proposals via RPN basically always beats EB.\n    * Our model outperforms individual region description\n      * Their full jointly trained model (FCLN) achieves the best results.\n      * The full jointly trained model performs significantly better than `RPN + Region RNN model` (i.e. separately trained region proposal and region captioning networks).\n    * Qualitative results\n      * Finds plenty of good regions and generates reasonable captions for them.\n      * Sometimes finds the same region twice.\n    * Runtime evaluation\n      * 240ms on 720x600 image with 300 region proposals.\n      * 166ms on 720x600 image with 100 region proposals.\n      * Recognition of region proposals takes up most time.\n      * Generating region proposals takes up the 2nd most time.\n      * Generating captions for regions (RNN) takes almost no time.\n  * Image Retrieval using Regions and Captions\n    * They try to search for regions based on search queries.\n    * They search by letting their FCLN network or EB generate 100 region proposals per network. Then they calculate per region the probability of generating the search query as the caption. They use that probability to rank the results.\n    * They pick images from the VG dataset, then pick captions within those images as search query. Then they evaluate the ranking of those images for the respective search query.\n    * The results show that the model can learn to rank objects, object parts, people and actions as expected/desired.\n    * The method described can also be used to detect an arbitrary number of distinct classes in images (as opposed to the usual 10 to 1000 classes), because the classes are contained in the generated captions.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.07571"
    },
    "924": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=huang2016networks",
        "transcript": "  * Stochastic Depth (SD) is a method for residual networks, which randomly removes/deactivates residual blocks during training.\n  * As such, it is similar to dropout.\n  * While dropout removes neurons, SD removes blocks (roughly the layers of a residual network).\n  * One can argue that dropout randomly changes the width of layers, while SD randomly changes the depth of the network.\n  * One can argue that using dropout is similar to training an ensemble of networks with different layer widths, while using SD is similar to training an ensemble of networks with different depths.\n  * Using SD has the following advantages:\n    * It decreases the effects of vanishing gradients, because on average the network is shallower during training (per batch), thereby increasing the gradient that reaches the early blocks.\n    * It increases training speed, because on average less convolutions have to be applied (due to blocks being removed).\n    * It has a regularizing effect, because blocks cannot easily co-adapt any more. (Similar to dropout avoiding co-adaption of neurons.)\n    * If using an increasing removal probability for later blocks: It spends more training time on the early (and thus most important) blocks than on the later blocks.\n\n### How\n  * Normal formula for a residual block (test and train):\n    * `output = ReLU(f(input) + identity(input))`\n    * `f(x)` are usually one or two convolutions.\n  * Formula with SD (during training):\n    * `output = ReLU(b * f(input) + identity(input))`\n    * `b` is either exactly `1` (block survived, i.e. is not removed) or exactly `0` (block was removed).\n    * `b` is sampled from a bernoulli random variable that has the hyperparameter `p`.\n    * `p` is the survival probability of a block (i.e. chance to *not* be removed). (Note that this is the opposite of dropout, where higher values lead to more removal.)\n  * Formula with SD (during test):\n    * `output = ReLU(p * f(input) + input)`\n    * `p` is the average probability with which this residual block survives during training, i.e. the hyperparameter for the bernoulli variable.\n    * The test formula has to be changed, because the network will adapt during training to blocks being missing. Activating them all at the same time can lead to overly strong signals. This is similar to dropout, where weights also have to be changed during test.\n  * There are two simple schemas to set `p` per layer:\n    * Uniform schema: Every block gets the same `p` hyperparameter, i.e. the last block has the same chance of survival as the first block.\n    * Linear decay schema: Survival probability is higher for early layers and decreases towards the end.\n      * The formula is `p = 1 - (l/L)(1-q)`.\n      * `l`: Number of the block for which to set `p`.\n      * `L`: Total number of blocks.\n      * `q`: Desired survival probability of the last block (0.5 is a good value).\n  * For linear decay with `q=0.5` and `L` blocks, on average `(3/4)L` blocks will be trained per minibatch.\n  * For linear decay with `q=0.5` the average speedup will be about `1/4` (25%). If using `q=0.2` the speedup will be ~40%.\n\n### Results\n  * 152 layer networks with SD outperform identical networks without SD on CIFAR-10, CIFAR-100 and SVHN.\n  * The improvement in test error is quite significant.\n  * SD seems to have a regularizing effect. Networks with SD are not overfitting where networks without SD already are.\n  * Even networks with >1000 layers are well trainable with SD.\n  * The gradients that reach the early blocks of the networks are consistently significantly higher with SD than without SD (i.e. less vanishing gradient).\n  * The linear decay schema consistently outperforms the uniform schema (in test error). The best value seems to be `q=0.5`, though values between 0.4 and 0.8 all seem to be good. For the uniform schema only 0.8 seems to be good.\n\n![SVHN 152 layers](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Deep_Networks_with_Stochastic_Depth__svhn.png?raw=true \"SVHN 152 layers\")\n\n*Performance on SVHN with 152 layer networks with SD (blue, bottom) and without SD (red, top).*\n\n\n![CIFAR-10 1202 layers](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Deep_Networks_with_Stochastic_Depth__svhn1202.png?raw=true \"CIFAR-10 1202 layers\")\n\n*Performance on CIFAR-10 with 1202 layer networks with SD (blue, bottom) and without SD (red, top).*\n\n\n![Optimal choice of p_L](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Deep_Networks_with_Stochastic_Depth__optimal_p.png?raw=true \"Optimal choice of p_l\")\n\n*Optimal choice of the survival probability `p_L` (in this summary `q`) for the last layer, for the uniform schema (same for all other layers) and the linear decay schema (decreasing towards `p_L`). Linear decay performs consistently better and allows for lower `p_L` values, leading to more speedup.*\n\n\n--------------------\n\n### Rough chapter-wise notes\n\n* (1) Introduction\n  * Problems of deep networks:\n    * Vanishing Gradients: During backpropagation, gradients approach zero due to being repeatedly multiplied with small weights. Possible counter-measures: Careful initialization of weights, \"hidden layer supervision\" (?), batch normalization.\n    * Diminishing feature reuse: Aequivalent problem to vanishing gradients during forward propagation. Results of early layers are repeatedly multiplied with later layer's (randomly initialized) weights. The total result then becomes meaningless noise and doesn't have a clear/strong gradient to fix it.\n    * Long training time: The time of each forward-backward increases linearly with layer depth. Current 152-layer networks can take weeks to train on ImageNet.\n  * I.e.: Shallow networks can be trained effectively and fast, but deep networks would be much more expressive.\n  * During testing we want deep networks, during training we want shallow networks.\n  * They randomly \"drop out\" (i.e. remove) complete layers during training (per minibatch), resulting in shallow networks.\n  * Result: Lower training time *and* lower test error.\n  * While dropout randomly removes width from the network, stochastic depth randomly removes depth from the networks.\n  * While dropout can be thought of as training an ensemble of networks with different depth, stochastic depth can be thought of as training an ensemble of networks with different depth.\n  * Stochastic depth acts as a regularizer, similar to dropout and batch normalization. It allows deeper networks without overfitting (because 1000 layers clearly wasn't enough!).\n\n* (2) Background\n  * Some previous methods to train deep networks: Greedy layer-wise training, careful initializations, batch normalization, highway connections, residual connections.\n  * <Standard explanation of residual networks>\n  * <Standard explanation of dropout>\n  * Dropout loses effectiveness when combined with batch normalization. Seems to have basically no benefit any more for deep residual networks with batch normalization.\n\n* (3) Deep Networks with Stochastic Depth\n  * They randomly skip entire layers during training.\n  * To do that, they use residual connections. They select random layers and use only the identity function for these layers (instead of the full residual block of identity + convolutions + add).\n  * ResNet architecture: They use standard residual connections. ReLU activations, 2 convolutional layers (conv->BN->ReLU->conv->BN->add->ReLU). They use <= 64 filters per conv layer.\n  * While the standard formula for residual connections is `output = ReLU(f(input) + identity(input))`, their formula is `output = ReLU(b * f(input) + identity(input))` with `b` being either 0 (inactive/removed layer) or 1 (active layer), i.e. is a sample of a bernoulli random variable.\n  * The probabilities of the bernoulli random variables are now hyperparameters, similar to dropout.\n  * Note that the probability here means the probability of *survival*, i.e. high value = more survivors.\n  * The probabilities could be set uniformly, e.g. to 0.5 for each variable/layer.\n  * They can also be set with a linear decay, so that the first layer has a very high probability of survival, while the last layer has a very low probability of survival.\n  * Linear decay formula: `p = 1 - (l/L)(1-q)` where `l` is the current layer's number, `L` is the total number of layers, `p` is the survival probability of layer `l` and `q` is the desired survival probability of the last layer (e.g. 0.5).\n  * They argue that linear decay is better, as the early layer extract low level features and are therefor more important.\n  * The expected number of surviving layers is simply the sum of the probabilities.\n  * For linear decay with `q=0.5` and `L=54` (i.e. 54 residual blocks = 110 total layers) the expected number of surviving blocks is roughly `(3/4)L = (3/4)54 = 40`, i.e. on average 14 residual blocks will be removed per training batch.\n  * With linear decay and `q=0.5` the expected speedup of training is about 25%. `q=0.2` leads to about 40% speedup (while in one test still achieving the test error of the same network without stochastic depth).\n  * Depending on the `q` setting, they observe significantly lower test errors. They argue that stochastic depth has a regularizing effect (training an ensemble of many networks with different depths).\n  * Similar to dropout, the forward pass rule during testing must be slightly changed, because the network was trained on missing values. The residual formular during test time becomes `output = ReLU(p * f(input) + input)` where `p` is the average probability with which this residual block survives during training.\n\n* (4) Results\n  * Their model architecture:\n    * Three chains of 18 residual blocks each, so 3*18 blocks per model.\n    * Number of filters per conv. layer: 16 (first chain), 32 (second chain), 64 (third chain)\n    * Between each block they use average pooling. Then they zero-pad the new dimensions (e.g. from 16 to 32 at the end of the first chain).\n  * CIFAR-10:\n    * Trained with SGD (momentum=0.9, dampening=0, lr=0.1 after 1st epoch, 0.01 after epoch 250, 0.001 after epoch 375).\n    * Weight decay/L2 of 1e-4.\n    * Batch size 128.\n    * Augmentation: Horizontal flipping, crops (4px offset).\n    * They achieve 5.23% error (compared to 6.41% in the original paper about residual networks).\n  * CIFAR-100:\n    * Same settings as before.\n    * 24.58% error with stochastic depth, 27.22% without.\n  * SVHN:\n    * The use both the hard and easy sub-datasets of images.\n    * They preprocess to zero-mean, unit-variance.\n    * Batch size 128.\n    * Learning rate is 0.1 (start), 0.01 (after epoch 30), 0.001 (after epoch 35).\n    * 1.75% error with stochastic depth, 2.01% error without.\n    * Network without stochastic depth starts to overfit towards the end.\n  * Stochastic depth with linear decay and `q=0.5` gives ~25% speedup.\n  * 1202-layer CIFAR-10:\n    * They trained a 1202-layer deep network on CIFAR-10 (previous tests: 152 layers).\n    * Without stochastic depth: 6.72% test error.\n    * With stochastic depth: 4.91% test error.\n\n* (5) Analytic experiments\n  * Vanishing Gradient:\n    * They analyzed the gradient that reaches the first layer.\n    * The gradient with stochastic depth is consistently higher (throughout the epochs) than without stochastic depth.\n    * The difference is very significant after decreasing the learning rate.\n  * Hyper-parameter sensitivity:\n    * They evaluated with test error for different choices of the survival probability `q`.\n    * Linear decay schema: Values between 0.4 and 0.8 perform best. 0.5 is suggested (nearly best value, good spedup). Even 0.2 improves the test error (compared to no stochastic depth).\n    * Uniform schema: 0.8 performs best, other values mostly significantly worse.\n    * Linear decay performs consistently better than the uniform schema.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.09382"
    },
    "925": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/mir/FarfadeSL15",
        "transcript": "* They propose a CNN-based approach to detect faces in a wide range of orientations using a single model. However, since the training set is skewed, the network is more confident about up-right faces.\n* The model does not require additional components such as segmentation, bounding-box regression, segmentation, or SVM classifiers\n\n### How\n* __Data augmentation__: to increase the number of positive samples (24K face annotations), the authors used randomly sampled sub-windows of the images with IOU > 50% and also randomly flipped these images. In total, there were 20K positive and 20M negative training samples.  \n* __CNN Architecture__: 5 convolutional layers followed by 3 fully-connected. The fully-connected layers were converted to convolutional layers. Non-Maximal Suppression is applied to merge predicted bounding boxes.\n* __Training__: the CNN was trained using Caffe Library in the AFLW dataset with the following parameters:\n * Fine-tuning with AlexNet model\n  * Input image size = 227x227\n  * Batch size = 128 (32+, 96-)\n  * Stride = 32\n* __Test__: the model was evaluated on PASCAL FACE, AFW, and FDDB dataset.\n* __Running time__: since the fully-connected layers were converted to convolutional layers, the input image in running time may be of any size, obtaining a heat map as output. To detect faces of different sizes though, the image is scaled up/down and new heatmaps are obtained. The authors found that rescaling image 3 times per octave gives reasonable good performance.\n![DDFD heatmap](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/DDFD__heatmap.png?raw=true \"DDFD heatmap\")\n* The authors realized that the model is more confident about up-right faces than rotated/occluded ones. This trend is because the lack of good training examples to represent such faces in the training process. Better results can be achieved by using better sampling strategies and more sophisticated data augmentation techniques.\n![DDFD example](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/DDFD__example.png?raw=true \"DDFD example\")\n* The authors tested different strategies for NMS and the effect of bounding-box regression for improving face detection. They NMS-avg had better performance compared to NMS-max in terms of average precision. On the other hand, adding a bounding-box regressor degraded the performance for both NMS strategies due to the mismatch between annotations of the training set and the test set. This mismatch is mostly for side-view faces.\n\n### Results:\n* In comparison to R-CNN, the proposed face detector had significantly better performance independent of the NMS strategy. The authors believe the inferior performance of R-CNN due to the loss of recall since selective search may miss some of the face regions; and loss in localization since bounding-box regression is not perfect and may not be able to fully align the segmentation bounding-boxes, provided by selective search, with the ground truth.\n* In comparison to other state-of-art methods like structural model, TSM and cascade-based methods the DDFD achieve similar or better results. However, this comparison is not completely fair since the most of methods use extra information of pose annotation or information about facial landmarks during the training.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2671188.2749408"
    },
    "926": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1704.03971",
        "transcript": "  * They analyze the effects of using Batch Normalization (BN) and Weight Normalization (WN) in GANs (classical algorithm, like DCGAN).\n  * They introduce a new measure to rate the quality of the generated images over time.\n\n### How\n  * They use BN as it is usually defined.\n  * They use WN with the following formulas:\n    * Strict weight-normalized layer:\n      * ![Strict WN layer](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/On_the_Effects_of_BN_and_WN_in_GANs__strict_wn.jpg?raw=true \"Strict WN layer\")\n    * Affine weight-normalized layer:\n      * ![Affine WN layer](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/On_the_Effects_of_BN_and_WN_in_GANs__affine_wn.jpg?raw=true \"Affine WN layer\")\n    * As activation units they use Translated ReLUs (aka \"threshold functions\"):\n      * ![TReLU](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/On_the_Effects_of_BN_and_WN_in_GANs__trelu.jpg?raw=true \"TReLU\")\n      * `alpha` is a learned parameter.\n      * TReLUs play better with their WN layers than normal ReLUs.\n  * Reconstruction measure\n    * To evaluate the quality of the generated images during training, they introduce a new measure.\n    * The measure is based on a L2-Norm (MSE) between (1) a real image and (2) an image created by the generator that is as similar as possible to the real image.\n    * They generate (2) by starting `G(z)` with a noise vector `z` that is filled with zeros. The desired output is the real image. They compute a MSE between the generated and real image and backpropagate the result. Then they use the generated gradient to update `z`, while leaving the parameters of `G` unaltered. They repeat this for a defined number of steps.\n    * Note that the above described method is fairly time-consuming, so they don't do it often.\n  * Networks\n    * Their networks are fairly standard.\n    * Generator: Starts at 1024 filters, goes down to 64 (then 3 for the output). Upsampling via fractionally strided convs.\n    * Discriminator: Starts at 64 filters, goes to 1024 (then 1 for the output). Downsampling via strided convolutions.\n    * They test three variations of these networks:\n      * Vanilla: No normalization. PReLUs in both G and D.\n      * BN: BN in G and D, but not in the last layers and not in the first layer of D. PReLUs in both G and D.\n      * WN: Strict weight-normalized layers in G and D, except for the last layers, which are affine weight-normalized layers. TPReLUs (Translated PReLUs) in both G and D.\n  * Other\n    * They train with RMSProp and batch size 32.\n\n### Results\n  * Their WN formulation trains stable, provided the learning rate is set to 0.0002 or lower.\n  * They argue, that their achieved stability is similar to the one in WGAN.\n  * BN had significant swings in quality.\n  * Vanilla collapsed sooner or later.\n  * Both BN and Vanilla reached an optimal point shortly after the start of the training. After that, the quality of the generated images only worsened.\n  * Plot of their quality measure:\n    * ![Losses over time](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/On_the_Effects_of_BN_and_WN_in_GANs__losses_over_time.jpg?raw=true \"Losses over time\")\n  * Their quality measure is based on reconstruction of input images. The below image shows examples for that reconstruction (each person: original image, vanilla reconstruction, BN rec., WN rec.).\n    * ![Reconstructions](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/On_the_Effects_of_BN_and_WN_in_GANs__reconstructions.jpg?raw=true \"Reconstructions\")\n  * Examples generated by their WN network:\n    * ![WN Examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/On_the_Effects_of_BN_and_WN_in_GANs__wn_examples.jpg?raw=true \"WN Examples\")\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1704.03971"
    },
    "927": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SalimansK16",
        "transcript": "  * Weight Normalization (WN) is a normalization technique, similar to Batch Normalization (BN).\n  * It normalizes each layer's weights.\n\n### Differences to BN\n  * WN normalizes based on each weight vector's orientation and magnitude. BN normalizes based on each weight's mean and variance in a batch.\n  * WN works on each example on its own. BN works on whole batches.\n  * WN is more deterministic than BN (due to not working an batches).\n  * WN is better suited for noisy environment (RNNs, LSTMs, reinforcement learning, generative models). (Due to being more deterministic.)\n  * WN is computationally simpler than BN.\n\n### How its done\n  * WN is a module added on top of a linear or convolutional layer.\n  * If that layer's weights are `w` then WN learns two parameters `g` (scalar) and `v` (vector, identical dimension to `w`) so that `w = gv / ||v||` is fullfilled (`||v||` = euclidean norm of v).\n  * `g` is the magnitude of the weights, `v` are their orientation.\n  * `v` is initialized to zero mean and a standard deviation of 0.05.\n  * For networks without recursions (i.e. not RNN/LSTM/GRU):\n    * Right after initialization, they feed a single batch through the network.\n    * For each neuron/weight, they calculate the mean and standard deviation after the WN layer.\n    * They then adjust the bias to `-mean/stdDev` and `g` to `1/stdDev`.\n    * That makes the network start with each feature being roughly zero-mean and unit-variance.\n    * The same method can also be applied to networks without WN.\n\n### Results:\n  * They define BN-MEAN as a variant of BN which only normalizes to zero-mean (not unit-variance).\n  * CIFAR-10 image classification (no data augmentation, some dropout, some white noise):\n    * WN, BN, BN-MEAN all learn similarly fast. Network without normalization learns slower, but catches up towards the end.\n    * BN learns \"more\" per example, but is about 16% slower (time-wise) than WN.\n    * WN reaches about same test error as no normalization (both ~8.4%), BN achieves better results (~8.0%).\n    * WN + BN-MEAN achieves best results with 7.31%.\n    * Optimizer: Adam\n  * Convolutional VAE on MNIST and CIFAR-10:\n    * WN learns more per example und plateaus at better values than network without normalization. (BN was not tested.)\n    * Optimizer: Adamax\n  * DRAW on MNIST (heavy on LSTMs):\n    * WN learns significantly more example than network without normalization.\n    * Also ends up with better results. (Normal network might catch up though if run longer.)\n  * Deep Reinforcement Learning (Space Invaders):\n    * WN seemed to overall acquire a bit more reward per epoch than network without normalization. Variance (in acquired reward) however also grew.\n    * Results not as clear as in DRAW.\n    * Optimizer: Adamax\n\n### Extensions\n  * They argue that initializing `g` to `exp(cs)` (`c` constant, `s` learned) might be better, but they didn't get better test results with that.\n  * Due to some gradient effects, `||v||` currently grows monotonically with every weight update. (Not necessarily when using optimizers that use separate learning rates per parameters.)\n  * That grow effect leads the network to be more robust to different learning rates.\n  * Setting a small hard limit/constraint for `||v||` can lead to better test set performance (parameter updates are larger, introducing more noise).\n\n\n![CIFAR-10 results](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Weight_Normalization__cifar10.png?raw=true \"CIFAR-10 results\")\n\n*Performance of WN on CIFAR-10 compared to BN, BN-MEAN and no normalization.*\n\n![DRAW, DQN results](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Weight_Normalization__draw_dqn.png?raw=true \"DRAW, DQN results\")\n\n*Performance of WN for DRAW (left) and deep reinforcement learning (right).*",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks"
    },
    "928": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SzegedyIV16",
        "transcript": "  * Inception v4 is like Inception v3, but\n    * Slimmed down, i.e. some parts were simplified\n    * One new version with residual connections (Inception-ResNet-v2), one without (Inception-v4)\n  * They didn't observe an improved error rate when using residual connections.\n  * They did however oberserve that using residual connections decreased their training times.\n  * They had to scale down the results of their residual modules (multiply them by a constant ~0.1). Otherwise their networks would die (only produce 0s).\n  * Results on ILSVRC 2012 (val set, 144 crops/image):\n    * Top-1 Error:\n      * Inception-v4: 17.7%\n      * Inception-ResNet-v2: 17.8%\n    * Top-5 Error (ILSVRC 2012 val set, 144 crops/image):\n      * Inception-v4: 3.8%\n      * Inception-ResNet-v2: 3.7% \n\n### Architecture\n  * Basic structure of Inception-ResNet-v2 (layers, dimensions):\n    * `Image -> Stem -> 5x Module A -> Reduction-A -> 10x Module B -> Reduction B -> 5x Module C -> AveragePooling -> Droput 20% -> Linear, Softmax`\n    * `299x299x3 -> 35x35x256 -> 35x35x256 -> 17x17x896 -> 17x17x896 -> 8x8x1792 -> 8x8x1792 -> 1792 -> 1792 -> 1000`\n  * Modules A, B, C are very similar.\n  * They contain 2 (B, C) or 3 (A) branches.\n  * Each branch starts with a 1x1 convolution on the input.\n  * All branches merge into one 1x1 convolution (which is then added to the original input, as usually in residual architectures).\n  * Module A uses 3x3 convolutions, B 7x1 and 1x7, C 3x1 and 1x3.\n  * The reduction modules also contain multiple branches. One has max pooling (3x3 stride 2), the other branches end in convolutions with stride 2.\n\n![Module A](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Inception_v4__module_a.png?raw=true \"Module A\")\n![Module B](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Inception_v4__module_b.png?raw=true \"Module B\")\n![Module C](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Inception_v4__module_c.png?raw=true \"Module C\")\n![Reduction Module A](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Inception_v4__reduction_a.png?raw=true \"Reduction Module A\")\n\n*From top to bottom: Module A, Module B, Module C, Reduction Module A.*\n\n![Top 5 error](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Inception_v4__top5_error.png?raw=true \"Top 5 error\")\n\n*Top 5 eror by epoch, models with (red, solid, bottom) and without (green, dashed) residual connections.*\n\n-------------------------\n\n### Rough chapter-wise notes\n\n### Introduction, Related Work\n  * Inception v3 was adapted to run on DistBelief. Inception v4 is designed for TensorFlow, which gets rid of some constraints and allows a simplified architecture.\n  * Authors don't think that residual connections are inherently needed to train deep nets, but they do speed up the training.\n  * History:\n    * Inception v1 - Introduced inception blocks\n    * Inception v2 - Added Batch Normalization\n    * Inception v3 - Factorized the inception blocks further (more submodules)\n    * Inception v4 - Adds residual connections\n\n### Architectural Choices\n  * Previous architectures were constrained due to memory problems. TensorFlow got rid of that problem.\n  * Previous architectures were carefully/conservatively extended. Architectures ended up being quite complicated. This version slims down everything.\n  * They had problems with residual networks dieing when they contained more than 1000 filters (per inception module apparently?). They could fix that by multiplying the results of the residual subnetwork (before the element-wise addition) with a constant factor of ~0.1.\n\n### Training methodology\n  * Kepler GPUs, TensorFlow, RMSProb (SGD+Momentum apprently performed worse)\n\n### Experimental Results\n  * Their residual version of Inception v4 (\"Inception-ResNet-v2\") seemed to learn faster than the non-residual version.\n  * They both peaked out at almost the same value.\n  * Top-1 Error (ILSVRC 2012 val set, 144 crops/image):\n    * Inception-v4: 17.7%\n    * Inception-ResNet-v2: 17.8%\n  * Top-5 Error (ILSVRC 2012 val set, 144 crops/image):\n    * Inception-v4: 3.8%\n    * Inception-ResNet-v2: 3.7%",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.07261"
    },
    "929": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/GoodfellowPMXWOCB14",
        "transcript": "  * GANs are based on adversarial training.\n  * Adversarial training is a basic technique to train generative models (so here primarily models that create new images).\n  * In an adversarial training one model (G, Generator) generates things (e.g. images). Another model (D, discriminator) sees real things (e.g. real images) as well as fake things (e.g. images from G) and has to learn how to differentiate the two.\n  * Neural Networks are models that can be trained in an adversarial way (and are the only models discussed here).\n\n### How\n  * G is a simple neural net (e.g. just one fully connected hidden layer). It takes a vector as input (e.g. 100 dimensions) and produces an image as output.\n  * D is a simple neural net (e.g. just one fully connected hidden layer). It takes an image as input and produces a quality rating as output (0-1, so sigmoid).\n  * You need a training set of things to be generated, e.g. images of human faces.\n  * Let the batch size be B.\n  * G is trained the following way:\n    * Create B vectors of 100 random values each, e.g. sampled uniformly from [-1, +1]. (Number of values per components depends on the chosen input size of G.)\n    * Feed forward the vectors through G to create new images.\n    * Feed forward the images through D to create ratings.\n    * Use a cross entropy loss on these ratings. All of these (fake) images should be viewed as label=0 by D. If D gives them label=1, the error will be low (G did a good job).\n    * Perform a backward pass of the errors through D (without training D). That generates gradients/errors per image and pixel.\n    * Perform a backward pass of these errors through G to train G.\n  * D is trained the following way:\n    * Create B/2 images using G (again, B/2 random vectors, feed forward through G).\n    * Chose B/2 images from the training set. Real images get label=1.\n    * Merge the fake and real images to one batch. Fake images get label=0.\n    * Feed forward the batch through D.\n    * Measure the error using cross entropy.\n    * Perform a backward pass with the error through D.\n  * Train G for one batch, then D for one (or more) batches. Sometimes D can be too slow to catch up with D, then you need more iterations of D per batch of G.\n\n### Results\n  * Good looking images MNIST-numbers and human faces. (Grayscale, rather homogeneous datasets.)\n  * Not so good looking images of CIFAR-10. (Color, rather heterogeneous datasets.)\n\n\n![Generated Faces](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Generative_Adversarial_Networks__faces.jpg?raw=true \"Generated Faces\")\n\n*Faces generated by MLP GANs. (Rightmost column shows examples from the training set.)*\n\n-------------------------\n\n### Rough chapter-wise notes\n\n* Introduction\n  * Discriminative models performed well so far, generative models not so much.\n  * Their suggested new architecture involves a generator and a discriminator.\n  * The generator learns to create content (e.g. images), the discriminator learns to differentiate between real content and generated content.\n  * Analogy: Generator produces counterfeit art, discriminator's job is to judge whether a piece of art is a counterfeit.\n  * This principle could be used with many techniques, but they use neural nets (MLPs) for both the generator as well as the discriminator.\n \n* Adversarial Nets\n  * They have a Generator G (simple neural net)\n    * G takes a random vector as input (e.g. vector of 100 random values between -1 and +1).\n    * G creates an image as output.\n  * They have a Discriminator D (simple neural net)\n    * D takes an image as input (can be real or generated by G).\n    * D creates a rating as output (quality, i.e. a value between 0 and 1, where 0 means \"probably fake\").\n  * Outputs from G are fed into D. The result can then be backpropagated through D and then G. G is trained to maximize log(D(image)), so to create a high value of D(image).\n  * D is trained to produce only 1s for images from G.\n  * Both are trained simultaneously, i.e. one batch for G, then one batch for D, then one batch for G...\n  * D can also be trained multiple times in a row. That allows it to catch up with G.\n\n* Theoretical Results\n  * Let\n    * pd(x): Probability that image `x` appears in the training set.\n    * pg(x): Probability that image `x` appears in the images generated by G.\n  * If G is now fixed then the best possible D classifies according to: `D(x) = pd(x) / (pd(x) + pg(x))`\n  * It is proofable that there is only one global optimum for GANs, which is reached when G perfectly replicates the training set probability distribution. (Assuming unlimited capacity of the models and unlimited training time.)\n  * It is proofable that G and D will converge to the global optimum, so long as D gets enough steps per training iteration to model the distribution generated by G. (Again, assuming unlimited capacity/time.)\n  * Note that these things are proofed for the general principle for GANs. Implementing GANs with neural nets can then introduce problems typical for neural nets (e.g. getting stuck in saddle points).\n\n* Experiments\n  * They tested on MNIST, Toronto Face Database (TFD) and CIFAR-10.\n  * They used MLPs for G and D.\n  * G contained ReLUs and Sigmoids.\n  * D contained Maxouts.\n  * D had Dropout, G didn't.\n  * They use a Parzen Window Estimate aka KDE (sigma obtained via cross validation) to estimate the quality of their images.\n  * They note that KDE is not really a great technique for such high dimensional spaces, but its the only one known.\n  * Results on MNIST and TDF are great. (Note: both grayscale)\n  * CIFAR-10 seems to match more the texture but not really the structure.\n  * Noise is noticeable in CIFAR-10 (a bit in TFD too). Comes from MLPs (no convolutions).\n  * Their KDE score for MNIST and TFD is competitive or better than other approaches.\n\n* Advantages and Disadvantages\n  * Advantages\n    * No Markov Chains, only backprob\n    * Inference-free training\n    * Wide variety of functions can be incorporated into the model (?)\n    * Generator never sees any real example. It only gets gradients. (Prevents overfitting?)\n    * Can represent a wide variety of distributions, including sharp ones (Markov chains only work with blurry images).\n   * Disadvantages\n    * No explicit representation of the distribution modeled by G (?)\n    * D and G must be well synchronized during training\n      * If G is trained to much (i.e. D can't catch up), it can collapse many components of the random input vectors to the same output (\"Helvetica\")\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5423-generative-adversarial-nets"
    },
    "930": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1412.6071",
        "transcript": "  * Traditionally neural nets use max pooling with 2x2 grids (2MP).\n  * 2MP reduces the image dimensions by a factor of 2.\n  * An alternative would be to use pooling schemes that reduce by factors other than two, e.g. `1 < factor < 2`.\n  * Pooling by a factor of `sqrt(2)` would allow twice as many pooling layers as 2MP, resulting in \"softer\" image size reduction throughout the network.\n  * Fractional Max Pooling (FMP) is such a method to perform max pooling by factors other than 2.\n\n### How\n  * In 2MP you move a 2x2 grid always by 2 pixels.\n  * Imagine that these step sizes follow a sequence, i.e. for 2MP: `2222222...`\n  * If you mix in just a single `1` you get a pooling factor of `<2`.\n  * By chosing the right amount of `1s` vs. `2s` you can pool by any factor between 1 and 2.\n  * The sequences of `1s` and `2s` can be generated in fully *random* order or in *pseudorandom* order, where pseudorandom basically means \"predictable sub patterns\" (e.g. 211211211211211...).\n  * FMP can happen *disjoint* or *overlapping*. Disjoint means 2x2 grids, overlapping means 3x3.\n\n### Results\n  * FMP seems to perform generally better than 2MP.\n  * Better results on various tests, including CIFAR-10 and CIFAR-100 (often quite significant improvement).\n  * Best configuration seems to be *random* sequences with *overlapping* regions.\n  * Results are especially better if each test is repeated multiple times per image (as the random sequence generation creates randomness, similar to dropout). First 5-10 repetitions seem to be most valuable, but even 100+ give some improvement.\n  * An FMP-factor of `sqrt(2)` was usually used.\n\n\n![Examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Fractional_Max_Pooling__examples.jpg?raw=true \"Examples\")\n\n*Random FMP with a factor of sqrt(2) applied five times to the same input image (results upscaled back to original size).*\n\n-------------------------\n\n### Rough chapter-wise notes\n\n* (1) Convolutional neural networks\n  * Advantages of 2x2 max pooling (2MP): fast; a bit invariant to translations and distortions; quick reduction of image sizes\n  * Disadvantages: \"disjoint nature of pooling regions\" can limit generalization (i.e. that they don't overlap?); reduction of image sizes can be too quick\n  * Alternatives to 2MP: 3x3 pooling with stride 2, stochastic 2x2 pooling\n  * All suggested alternatives to 2MP also reduce sizes by a factor of 2\n  * Author wants to have reduction by sqrt(2) as that would enable to use twice as many pooling layers\n  * Fractional Max Pooling = Pooling that reduces image sizes by a factor of `1 < alpha < 2`\n  * FMP introduces randomness into pooling (by the choice of pooling regions)\n  * Settings of FMP:\n    * Pooling Factor `alpha` in range [1, 2] (1 = no change in image sizes, 2 = image sizes get halfed)\n    * Choice of Pooling-Regions: Random or pseudorandom. Random is stronger (?). Random+Dropout can result in underfitting.\n    * Disjoint or overlapping pooling regions. Results for overlapping are better.\n\n* (2) Fractional max-pooling\n  * For traditional 2MP, every grid's top left coordinate is at `(2i-1, 2j-1)` and it's bottom right coordinate at `(2i, 2j)` (i=col, j=row).\n  * It will reduce the original size N to 1/2N, i.e. `2N_in = N_out`.\n  * Paper analyzes `1 < alpha < 2`, but `alpha > 2` is also possible.\n  * Grid top left positions can be described by sequences of integers, e.g. (only column): 1, 3, 5, ...\n  * Disjoint 2x2 pooling might be 1, 3, 5, ... while overlapping would have the same sequence with a larger 3x3 grid.\n  * The increment of the sequences can be random or pseudorandom for alphas < 2.\n  * For 2x2 FMP you can represent any alpha with a \"good\" sequence of increments that all have values `1` or `2`, e.g. 2111121122111121...\n  * In the case of random FMP, the optimal fraction of 1s and 2s is calculated. Then a random permutation of a sequence of 1s and 2s is generated.\n  * In the case of pseudorandom FMP, the 1s and 2s follow a pattern that leads to the correct alpha, e.g. 112112121121211212...\n  * Random FMP creates varying distortions of the input image. Pseudorandom FMP is a faithful downscaling.\n \n* (3) Implementation\n  * In their tests they use a convnet starting with 10 convolutions, then 20, then 30, ...\n  * They add FMP with an alpha of sqrt(2) after every conv layer.\n  * They calculate the desired output size, then go backwards through their network to the input. They multiply the size of the image by sqrt(2) with every FMP layer and add a flat 1 for every conv layer. The result is the required image size. They pad the images to that size.\n  * They use dropout, with increasing strength from 0% to 50% towards the output.\n  * They use LeakyReLUs.\n  * Every time they apply an FMP layer, they generate a new sequence of 1s and 2s. That indirectly makes the network an ensemble of similar networks.\n  * The output of the network can be averaged over several forward passes (for the same image). The result then becomes more accurate (especially up to >=6 forward passes).\n\n* (4) Results\n  * Tested on MNIST and CIFAR-100\n  * Architectures (somehow different from (3)?):\n    * MNIST: 36x36 img -> 6 times (32 conv (3x3?) -> FMP alpha=sqrt(2)) -> ? -> ? -> output\n    * CIFAR-100: 94x94 img -> 12 times (64 conv (3x3?) -> FMP alpha=2^(1/3)) -> ? -> ? -> output\n  * Overlapping pooling regions seemed to perform better than disjoint regions.\n  * Random FMP seemed to perform better than pseudorandom FMP.\n  * Other tests:\n    * \"The Online Handwritten Assamese Characters Dataset\": FMP performed better than 2MP (though their network architecture seemed to have significantly more parameters\n    * \"CASIA-OLHWDB1.1 database\": FMP performed better than 2MP (again, seemed to have more parameters)\n    * CIFAR-10: FMP performed better than current best network (especially with many tests per image)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1412.6071"
    },
    "931": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.07289",
        "transcript": "  * ELUs are an activation function\n  * The are most similar to LeakyReLUs and PReLUs\n\n### How (formula)\n  * f(x):\n    * `if x >= 0: x`\n    * `else: alpha(exp(x)-1)`\n  * f'(x) / Derivative:\n    * `if x >= 0: 1`\n    * `else: f(x) + alpha`\n  * `alpha` defines at which negative value the ELU saturates.\n  * E. g. `alpha=1.0` means that the minimum value that the ELU can reach is `-1.0`\n  * LeakyReLUs however can go to `-Infinity`, ReLUs can't go below 0.\n\n![ELUs vs LeakyReLUs vs ReLUs](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/ELUs__slopes.png?raw=true \"ELUs vs LeakyReLUs vs ReLUs\")\n\n*Form of ELUs(alpha=1.0) vs LeakyReLUs vs ReLUs.*\n\n\n### Why\n  * They derive from the unit natural gradient that a network learns faster, if the mean activation of each neuron is close to zero.\n  * ReLUs can go above 0, but never below. So their mean activation will usually be quite a bit above 0, which should slow down learning.\n  * ELUs, LeakyReLUs and PReLUs all have negative slopes, so their mean activations should be closer to 0.\n  * In contrast to LeakyReLUs and PReLUs, ELUs saturate at a negative value (usually -1.0).\n  * The authors think that is good, because it lets ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence.\n  * So ELUs can measure the presence of concepts quantitatively, but the absence only qualitatively.\n  * They think that this makes ELUs more robust to noise.\n\n### Results\n  * In their tests on MNIST, CIFAR-10, CIFAR-100 and ImageNet, ELUs perform (nearly always) better than ReLUs and LeakyReLUs.\n  * However, they don't test PReLUs at all and use an alpha of 0.1 for LeakyReLUs (even though 0.33 is afaik standard) and don't test LeakyReLUs on ImageNet (only ReLUs).\n\n![CIFAR-100](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/ELUs__cifar100.png?raw=true \"CIFAR-100\")\n\n*Comparison of ELUs, LeakyReLUs, ReLUs on CIFAR-100. ELUs ends up with best values, beaten during the early epochs by LeakyReLUs. (Learning rates were optimized for ReLUs.)*\n\n-------------------------\n\n### Rough chapter-wise notes\n\n* Introduction\n  * Currently popular choice: ReLUs\n  * ReLU: max(0, x)\n  * ReLUs are sparse and avoid the vanishing gradient problem, because their derivate is 1 when they are active.\n  * ReLUs have a mean activation larger than zero.\n  * Non-zero mean activation causes a bias shift in the next layer, especially if multiple of them are correlated.\n  * The natural gradient (?) corrects for the bias shift by adjusting the weight update.\n  * Having less bias shift would bring the standard gradient closer to the natural gradient, which would lead to faster learning.\n  * Suggested solutions:\n    * Centering activation functions at zero, which would keep the off-diagonal entries of the Fisher information matrix small.\n    * Batch Normalization\n    * Projected Natural Gradient Descent (implicitly whitens the activations)\n  * These solutions have the problem, that they might end up taking away previous learning steps, which would slow down learning unnecessarily.\n  * Chosing a good activation function would be a better solution.\n  * Previously, tanh was prefered over sigmoid for that reason (pushed mean towards zero).\n  * Recent new activation functions:\n    * LeakyReLUs: x if x > 0, else alpha*x\n    * PReLUs: Like LeakyReLUs, but alpha is learned\n    * RReLUs: Slope of part < 0 is sampled randomly\n  * Such activation functions with non-zero slopes for negative values seemed to improve results.\n  * The deactivation state of such units is not very robust to noise, can get very negative.\n  * They suggest an activation function that can return negative values, but quickly saturates (for negative values, not for positive ones).\n  * So the model can make a quantitative assessment for positive statements (there is an amount X of A in the image), but only a qualitative negative one (something indicates that B is not in the image).\n  * They argue that this makes their activation function more robust to noise.\n  * Their activation function still has activations with a mean close to zero.\n\n* Zero Mean Activations Speed Up Learning\n  * Natural Gradient = Update direction which corrects the gradient direction with the Fisher Information Matrix\n  * Hessian-Free Optimization techniques use an extended Gauss-Newton approximation of Hessians and therefore can be interpreted as versions of natural gradient descent.\n  * Computing the Fisher matrix is too expensive for neural networks.\n  * Methods to approximate the Fisher matrix or to perform natural gradient descent have been developed.\n  * Natural gradient = inverse(FisherMatrix) * gradientOfWeights\n  * Lots of formulas. Apparently first explaining how the natural gradient descent works, then proofing that natural gradient descent can deal well with non-zero-mean activations.\n  * Natural gradient descent auto-corrects bias shift (i.e. non-zero-mean activations).\n  * If that auto-correction does not exist, oscillations (?) can occur, which slow down learning.\n  * Two ways to push means towards zero:\n    * Unit zero mean normalization (e.g. Batch Normalization)\n    * Activation functions with negative parts\n\n* Exponential Linear Units (ELUs)\n  * *Formula*\n    * f(x):\n      * if x >= 0: x\n      * else: alpha(exp(x)-1)\n    * f'(x) / Derivative:\n      * if x >= 0: 1\n      * else: f(x) + alpha\n    * `alpha` defines at which negative value the ELU saturates.\n    * `alpha=0.5` => minimum value is -0.5 (?)\n  * ELUs avoid the vanishing gradient problem, because their positive part is the identity function (like e.g. ReLUs)\n  * The negative values of ELUs push the mean activation towards zero.\n  * Mean activations closer to zero resemble more the natural gradient, therefore they should speed up learning.\n  * ELUs are more noise robust than PReLUs and LeakyReLUs, because their negative values saturate and thus should create a small gradient.\n  * \"ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence\"\n\n* Experiments Using ELUs\n  * They compare ELUs to ReLUs and LeakyReLUs, but not to PReLUs (no explanation why).\n  * They seem to use a negative slope of 0.1 for LeakyReLUs, even though 0.33 is standard afaik.\n  * They use an alpha of 1.0 for their ELUs (i.e. minimum value is -1.0).\n  * MNIST classification:\n    * ELUs achieved lower mean activations than ReLU/LeakyReLU\n    * ELUs achieved lower cross entropy loss than ReLU/LeakyReLU (and also seemed to learn faster)\n    * They used 5 hidden layers of 256 units each (no explanation why so many)\n    * (No convolutions)\n  * MNIST Autoencoder:\n    * ELUs performed consistently best (at different learning rates)\n    * Usually ELU > LeakyReLU > ReLU\n    * LeakyReLUs not far off, so if they had used a 0.33 value maybe these would have won\n  * CIFAR-100 classification:\n    * Convolutional network, 11 conv layers\n    * LeakyReLUs performed better during the first ~50 epochs, ReLUs mostly on par with ELUs\n    * LeakyReLUs about on par for epochs 50-100\n    * ELUs win in the end (the learning rates used might not be optimal for ELUs, were designed for ReLUs)\n  * CIFER-100, CIFAR-10 (big convnet):\n    * 6.55% error on CIFAR-10, 24.28% on CIFAR-100\n    * No comparison with ReLUs and LeakyReLUs for same architecture\n  * ImageNet\n    * Big convnet with spatial pyramid pooling (?) before the fully connected layers\n    * Network with ELUs performed better than ReLU network (better score at end, faster learning)\n    * Networks were still learning at the end, they didn't run till convergence\n    * No comparison to LeakyReLUs",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.07289"
    },
    "932": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeZRS15",
        "transcript": "  * Deep plain/ordinary networks usually perform better than shallow networks.\n  * However, when they get too deep their performance on the *training* set decreases. That should never happen and is a shortcoming of current optimizers.\n  * If the \"good\" insights of the early layers could be transferred through the network unaltered, while changing/improving the \"bad\" insights, that effect might disappear.\n\n###  What residual architectures are\n  * Residual architectures use identity functions to transfer results from previous layers unaltered.\n  * They change these previous results based on results from convolutional layers.\n  * So while a plain network might do something like `output = convolution(image)`, a residual network will do `output = image + convolution(image)`.\n  * If the convolution resorts to just doing nothing, that will make the result a lot worse in the plain network, but not alter it at all in the residual network.\n  * So in the residual network, the convolution can focus fully on learning what positive changes it has to perform, while in the plain network it *first* has to learn the identity function and then what positive changes it can perform.\n\n### How it works\n  * Residual architectures can be implemented in most frameworks. You only need something like a split layer and an element-wise addition.\n  * Use one branch with an identity function and one with 2 or more convolutions (1 is also possible, but seems to perform poorly). Merge them with the element-wise addition.\n  * Rough example block (for a 64x32x32 input):\nhttps://i.imgur.com/NJVb9hj.png  \n  * An example block when you have to change the dimensionality (e.g. here from 64x32x32 to 128x32x32):\nhttps://i.imgur.com/9NXvTjI.png\n  * The authors seem to prefer using either two 3x3 convolutions or the chain of 1x1 then 3x3 then 1x1. They use the latter one for their very deep networks.\n  * The authors also tested:\n    * To use 1x1 convolutions instead of identity functions everywhere. Performed a bit better than using 1x1 only for dimensionality changes. However, also computation and memory demands.\n    * To use zero-padding for dimensionality changes (no 1x1 convs, just fill the additional dimensions with zeros). Performed only a bit worse than 1x1 convs and a lot better than plain network architectures.\n  * Pooling can be used as in plain networks. No special architectures are necessary.\n  * Batch normalization can be used as usually (before nonlinearities).\n\n### Results\n  * Residual networks seem to perform generally better than similarly sized plain networks.\n  * They seem to be able to achieve similar results with less computation.\n  * They enable well-trainable very deep architectures with up to 1000 layers and more.\n  * The activations of the residual layers are low compared to plain networks. That indicates that the residual networks indeed only learn to make \"good\" changes and default to \"if in doubt, change nothing\".\n\n\n![Building blocks](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Deep_Residual_Learning_for_Image_Recognition__building_blocks.png?raw=true \"Building blocks\")\n\n*Examples of basic building blocks (other architectures are possible). The paper doesn't discuss the placement of the ReLU (after add instead of after the layer).*\n\n\n![Activations](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Deep_Residual_Learning_for_Image_Recognition__activations.png?raw=true \"Activations\")\n\n*Activations of layers (after batch normalization, before nonlinearity) throughout the network for plain and residual nets. Residual networks have on average lower activations.*\n\n-------------------------\n\n### Rough chapter-wise notes\n\n* (1) Introduction\n  * In classical architectures, adding more layers can cause the network to perform worse on the training set.\n  * That shouldn't be the case. (E.g. a shallower could be trained and then get a few layers of identity functions on top of it to create a deep network.)\n  * To combat that problem, they stack residual layers.\n  * A residual layer is an identity function and can learn to add something on top of that.\n  * So if `x` is an input image and `f(x)` is a convolution, they do something like `x + f(x)` or even `x + f(f(x))`.\n  * The classical architecture would be more like `f(f(f(f(x))))`.\n  * Residual architectures can be easily implemented in existing frameworks using skip connections with identity functions (split + merge).\n  * Residual architecture outperformed other in ILSVRC 2015 and COCO 2015.\n\n* (3) Deep Residual Learning\n  * If some layers have to fit a function `H(x)` then they should also be able to fit `H(x) - x` (change between `x` and `H(x)`).\n  * The latter case might be easier to learn than the former one.\n  * The basic structure of a residual block is `y = x + F(x, W)`, where `x` is the input image, `y` is the output image (`x + change`) and `F(x, W)` is the residual subnetwork that estimates a good change of `x` (W are the subnetwork's weights).\n  * `x` and `F(x, W)` are added using element-wise addition.\n  * `x` and the output of `F(x, W)` must be have equal dimensions (channels, height, width).\n  * If different dimensions are required (mainly change in number of channels) a linear projection `V` is applied to `x`: `y = F(x, W) + Vx`. They use a 1x1 convolution for `V` (without nonlinearity?).\n  * `F(x, W)` subnetworks can contain any number of layer. They suggest 2+ convolutions. Using only 1 layer seems to be useless.\n  * They run some tests on a network with 34 layers and compare to a 34 layer network without residual blocks and with VGG (19 layers).\n  * They say that their architecture requires only 18% of the FLOPs of VGG. (Though a lot of that probably comes from VGG's 2x4096 fully connected layers? They don't use any fully connected layers, only convolutions.)\n  * A critical part is the change in dimensionality (e.g. from 64 kernels to 128). They test (A) adding the new dimensions empty (padding), (B) using the mentioned linear projection with 1x1 convolutions and (C) using the same linear projection, but on all residual blocks (not only for dimensionality changes).\n  * (A) doesn't add parameters, (B) does (i.e. breaks the pattern of using identity functions).\n  * They use batch normalization before each nonlinearity.\n  * Optimizer is SGD.\n  * They don't use dropout.\n\n* (4) Experiments\n  * When testing on ImageNet an 18 layer plain (i.e. not residual) network has lower training set error than a deep 34 layer plain network.\n  * They argue that this effect does probably not come from vanishing gradients, because they (a) checked the gradient norms and they looked healthy and (b) use batch normaliaztion.\n  * They guess that deep plain networks might have exponentially low convergence rates.\n  * For the residual architectures its the other way round. Stacking more layers improves the results.\n  * The residual networks also perform better (in error %) than plain networks with the same number of parameters and layers. (Both for training and validation set.)\n  * Regarding the previously mentioned handling of dimensionality changes:\n    * (A) Pad new dimensions: Performs worst. (Still far better than plain network though.)\n    * (B) Linear projections for dimensionality changes: Performs better than A.\n    * (C) Linear projections for all residual blocks: Performs better than B. (Authors think that's due to introducing new parameters.)\n  * They also test on very deep residual networks with 50 to 152 layers.\n  * For these deep networks their residual block has the form `1x1 conv -> 3x3 conv -> 1x1 conv` (i.e. dimensionality reduction, convolution, dimensionality increase).\n  * These deeper networks perform significantly better.\n  * In further tests on CIFAR-10 they can observe that the activations of the convolutions in residual networks are lower than in plain networks.\n  * So the residual networks default to doing nothing and only change (activate) when something needs to be changed.\n  * They test a network with 1202 layers. It is still easily optimizable, but overfits the training set.\n  * They also test on COCO and get significantly better results than a Faster-R-CNN+VGG implementation.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.03385"
    },
    "933": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1502.03167",
        "transcript": "### What is BN:\n  * Batch Normalization (BN) is a normalization method/layer for neural networks.\n  * Usually inputs to neural networks are normalized to either the range of [0, 1] or [-1, 1] or to mean=0 and variance=1. The latter is called *Whitening*.\n  * BN essentially performs Whitening to the intermediate layers of the networks.\n\n### How its calculated:\n  * The basic formula is $x^* = (x - E[x]) / \\sqrt{\\text{var}(x)}$, where $x^*$ is the new value of a single component, $E[x]$ is its mean within a batch and `var(x)` is its variance within a batch.\n  * BN extends that formula further to $x^{**} = gamma * x^* +$ beta, where $x^{**}$ is the final normalized value. `gamma` and `beta` are learned per layer. They make sure that BN can learn the identity function, which is needed in a few cases.\n  * For convolutions, every layer/filter/kernel is normalized on its own (linear layer: each neuron/node/component). That means that every generated value (\"pixel\") is treated as an example. If we have a batch size of N and the image generated by the convolution has width=P and height=Q, we would calculate the mean (E) over `N*P*Q` examples (same for the variance).\n\n### Theoretical effects:\n  * BN reduces *Covariate Shift*. That is the change in distribution of activation of a component. By using BN, each neuron's activation becomes (more or less) a gaussian distribution, i.e. its usually not active, sometimes a bit active, rare very active.\n  * Covariate Shift is undesirable, because the later layers have to keep adapting to the change of the type of distribution (instead of just to new distribution parameters, e.g. new mean and variance values for gaussian distributions).\n  * BN reduces effects of exploding and vanishing gradients, because every becomes roughly normal distributed. Without BN, low activations of one layer can lead to lower activations in the next layer, and then even lower ones in the next layer and so on.\n\n### Practical effects:\n  * BN reduces training times. (Because of less Covariate Shift, less exploding/vanishing gradients.)\n  * BN reduces demand for regularization, e.g. dropout or L2 norm. (Because the means and variances are calculated over batches and therefore every normalized value depends on the current batch. I.e. the network can no longer just memorize values and their correct answers.)\n  * BN allows higher learning rates. (Because of less danger of exploding/vanishing gradients.)\n  * BN enables training with saturating nonlinearities in deep networks, e.g. sigmoid. (Because the normalization prevents them from getting stuck in saturating ranges, e.g. very high/low values for sigmoid.)\n\n\n![MNIST and neuron activations](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Batch_Normalization__performance_and_activations.png?raw=true \"MNIST and neuron activations\")\n\n*BN applied to MNIST (a), and activations of a randomly selected neuron over time (b, c), where the middle line is the median activation, the top line is the 15th percentile and the bottom line is the 85th percentile.*\n\n-------------------------\n\n### Rough chapter-wise notes\n\n* (2) Towards Reducing Covariate Shift\n  * Batch Normalization (*BN*) is a special normalization method for neural networks.\n  * In neural networks, the inputs to each layer depend on the outputs of all previous layers.\n  * The distributions of these outputs can change during the training. Such a change is called a *covariate shift*.\n  * If the distributions stayed the same, it would simplify the training. Then only the parameters would have to be readjusted continuously (e.g. mean and variance for normal distributions).\n  * If using sigmoid activations, it can happen that one unit saturates (very high/low values). That is undesired as it leads to vanishing gradients for all units below in the network.\n  * BN fixes the means and variances of layer inputs to specific values (zero mean, unit variance).\n  * That accomplishes:\n    * No more covariate shift.\n    * Fixes problems with vanishing gradients due to saturation.\n  * Effects:\n    * Networks learn faster. (As they don't have to adjust to covariate shift any more.)\n    * Optimizes gradient flow in the network. (As the gradient becomes less dependent on the scale of the parameters and their initial values.)\n    * Higher learning rates are possible. (Optimized gradient flow reduces risk of divergence.)\n    * Saturating nonlinearities can be safely used. (Optimized gradient flow prevents the network from getting stuck in saturated modes.)\n    * BN reduces the need for dropout. (As it has a regularizing effect.)\n  * How BN works:\n    * BN normalizes layer inputs to zero mean and unit variance. That is called *whitening*.\n    * Naive method: Train on a batch. Update model parameters. Then normalize. Doesn't work: Leads to exploding biases while distribution parameters (mean, variance) don't change.\n    * A proper method has to include the current example *and* all previous examples in the normalization step.\n    * This leads to calculating in covariance matrix and its inverse square root. That's expensive. The authors found a faster way.\n\n* (3) Normalization via Mini-Batch Statistics\n  * Each feature (component) is normalized individually. (Due to cost, differentiability.)\n  * Normalization according to: `componentNormalizedValue = (componentOldValue - E[component]) / sqrt(Var(component))`\n  * Normalizing each component can reduce the expressitivity of nonlinearities. Hence the formula is changed so that it can also learn the identiy function.\n  * Full formula: `newValue = gamma * componentNormalizedValue + beta` (gamma and beta learned per component)\n  * E and Var are estimated for each mini batch.\n  * BN is fully differentiable. Formulas for gradients/backpropagation are at the end of chapter 3 (page 4, left).\n\n* (3.1) Training and Inference with Batch-Normalized Networks\n  * During test time, E and Var of each component can be estimated using all examples or alternatively with moving averages estimated during training.\n  * During test time, the BN formulas can be simplified to a single linear transformation.\n\n* (3.2) Batch-Normalized Convolutional Networks\n  * Authors recommend to place BN layers after linear/fully-connected layers and before the ninlinearities.\n  * They argue that the linear layers have a better distribution that is more likely to be similar to a gaussian.\n  * Placing BN after the nonlinearity would also not eliminate covariate shift (for some reason).\n  * Learning a separate bias isn't necessary as BN's formula already contains a bias-like term (beta).\n  * For convolutions they apply BN equally to all features on a feature map. That creates effective batch sizes of m\\*pq, where m is the number of examples in the batch and p q are the feature map dimensions (height, width). BN for linear layers has a batch size of m.\n  * gamma and beta are then learned per feature map, not per single pixel. (Linear layers: Per neuron.)\n\n* (3.3) Batch Normalization enables higher learning rates\n  * BN normalizes activations.\n  * Result: Changes to early layers don't amplify towards the end.\n  * BN makes it less likely to get stuck in the saturating parts of nonlinearities.\n  * BN makes training more resilient to parameter scales.\n  * Usually, large learning rates cannot be used as they tend to scale up parameters. Then any change to a parameter amplifies through the network and can lead to gradient explosions.\n  * With BN gradients actually go down as parameters increase. Therefore, higher learning rates can be used.\n  * (something about singular values and the Jacobian)\n\n* (3.4) Batch Normalization regularizes the model\n  * Usually: Examples are seen on their own by the network.\n  * With BN: Examples are seen in conjunction with other examples (mean, variance).\n  * Result: Network can't easily memorize the examples any more.\n  * Effect: BN has a regularizing effect. Dropout can be removed or decreased in strength.\n\n* (4) Experiments\n* (4.1) Activations over time\n** They tested BN on MNIST with a 100x100x10 network. (One network with BN before each nonlinearity, another network without BN for comparison.)\n** Batch Size was 60.\n** The network with BN learned faster. Activations of neurons (their means and variances over several examples) seemed to be more consistent during training.\n** Generalization of the BN network seemed to be better.\n\n* (4.2) ImageNet classification\n** They applied BN to the Inception network.\n** Batch Size was 32.\n** During training they used (compared to original Inception training) a higher learning rate with more decay, no dropout, less L2, no local response normalization and less distortion/augmentation.\n** They shuffle the data during training (i.e. each batch contains different examples).\n** Depending on the learning rate, they either achieve the same accuracy (as in the non-BN network) in 14 times fewer steps (5x learning rate) or a higher accuracy in 5 times fewer steps (30x learning rate).\n** BN enables training of Inception networks with sigmoid units (still a bit lower accuracy than ReLU).\n** An ensemble of 6 Inception networks with BN achieved better accuracy than the previously best network for ImageNet.\n\n* (5) Conclusion\n** BN is similar to a normalization layer suggested by G\u00fclcehre and Bengio. However, they applied it to the outputs of nonlinearities.\n** They also didn't have the beta and gamma parameters (i.e. their normalization could not learn the identity function).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1502.03167"
    },
    "934": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1508.06576",
        "transcript": "\n  * The paper describes a method to separate content and style from each other in an image.\n  * The style can then be transfered to a new image.\n  * Examples:\n    * Let a photograph look like a painting of van Gogh.\n    * Improve a dark beach photo by taking the style from a sunny beach photo.\n\n### How\n  * They use the pretrained 19-layer VGG net as their base network.\n  * They assume that two images are provided: One with the *content*, one with the desired *style*.\n  * They feed the content image through the VGG net and extract the activations of the last convolutional layer. These activations are called the *content representation*.\n  * They feed the style image through the VGG net and extract the activations of all convolutional layers. They transform each layer to a *Gram Matrix* representation. These Gram Matrices are called the *style representation*.\n  * How to calculate a *Gram Matrix*:\n    * Take the activations of a layer. That layer will contain some convolution filters (e.g. 128), each one having its own activations.\n    * Convert each filter's activations to a (1-dimensional) vector.\n    * Pick all pairs of filters. Calculate the scalar product of both filter's vectors.\n    * Add the scalar product result as an entry to a matrix of size `#filters x #filters` (e.g. 128x128).\n    * Repeat that for every pair to get the Gram Matrix.\n    * The Gram Matrix roughly represents the *texture* of the image.\n  * Now you have the content representation (activations of a layer) and the style representation (Gram Matrices).\n  * Create a new image of the size of the content image. Fill it with random white noise.\n  * Feed that image through VGG to get its content representation and style representation. (This step will be repeated many times during the image creation.)\n  * Make changes to the new image using gradient descent to optimize a loss function.\n    * The loss function has two components:\n      * The mean squared error between the new image's content representation and the previously extracted content representation.\n      * The mean squared error between the new image's style representation and the previously extracted style representation.\n    * Add up both components to get the total loss.\n    * Give both components a weight to alter for more/less style matching (at the expense of content matching).\n\n\n![Examples](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/A_Neural_Algorithm_for_Artistic_Style__examples.jpg?raw=true \"Examples\")\n\n*One example input image with different styles added to it.*\n\n-------------------------\n\n### Rough chapter-wise notes\n\n* Page 1\n  * A painted image can be decomposed in its content and its artistic style.\n  * Here they use a neural network to separate content and style from each other (and to apply that style to an existing image).\n\n* Page 2\n  * Representations get more abstract as you go deeper in networks, hence they should more resemble the actual content (as opposed to the artistic style).\n  * They call the feature responses in higher layers *content representation*.\n  * To capture style information, they use a method that was originally designed to capture texture information.\n  * They somehow build a feature space on top of the existing one, that is somehow dependent on correlations of features. That leads to a \"stationary\" (?) and multi-scale representation of the style.\n\n* Page 3\n  * They use VGG as their base CNN.\n\n* Page 4\n  * Based on the extracted style features, they can generate a new image, which has equal activations in these style features.\n  * The new image should match the style (texture, color, localized structures) of the artistic image.\n  * The style features become more and more abtstract with higher layers. They call that multi-scale the *style representation*.\n  * The key contribution of the paper is a method to separate style and content representation from each other.\n  * These representations can then be used to change the style of an existing image (by changing it so that its content representation stays the same, but its style representation matches the artwork).\n\n* Page 6\n  * The generated images look most appealing if all features from the style representation are used. (The lower layers tend to reflect small features, the higher layers tend to reflect larger features.)\n  * Content and style can't be separated perfectly.\n  * Their loss function has two terms, one for content matching and one for style matching.\n  * The terms can be increased/decreased to match content or style more.\n\n* Page 8\n  * Previous techniques work only on limited or simple domains or used non-parametric approaches (see non-photorealistic rendering).\n  * Previously neural networks have been used to classify the time period of paintings (based on their style).\n  * They argue that separating content from style might be useful and many other domains (other than transfering style of paintings to images).\n\n* Page 9\n  * The style representation is gathered by measuring correlations between activations of neurons.\n  * They argue that this is somehow similar to what \"complex cells\" in the primary visual system (V1) do.\n  * They note that deep convnets seem to automatically learn to separate content from style, probably because it is helpful for style-invariant classification.\n\n* Page 9, Methods\n  * They use the 19 layer VGG net as their basis.\n  * They use only its convolutional layers, not the linear ones.\n  * They use average pooling instead of max pooling, as that produced slightly better results.\n\n* Page 10, Methods\n  * The information about the image that is contained in layers can be visualized. To do that, extract the features of a layer as the labels, then start with a white noise image and change it via gradient descent until the generated features have minimal distance (MSE) to the extracted features.\n  * The build a style representation by calculating Gram Matrices for each layer.\n\n* Page 11, Methods\n  * The Gram Matrix is generated in the following way:\n    * Convert each filter of a convolutional layer to a 1-dimensional vector.\n    * For a pair of filters i, j calculate the value in the Gram Matrix by calculating the scalar product of the two vectors of the filters.\n    * Do that for every pair of filters, generating a matrix of size #filters x #filters. That is the Gram Matrix.\n  * Again, a white noise image can be changed with gradient descent to match the style of a given image (i.e. minimize MSE between two Gram Matrices).\n  * That can be extended to match the style of several layers by measuring the MSE of the Gram Matrices of each layer and giving each layer a weighting.\n\n* Page 12, Methods\n  * To transfer the style of a painting to an existing image, proceed as follows:\n    * Start with a white noise image.\n    * Optimize that image with gradient descent so that it minimizes both the content loss (relative to the image) and the style loss (relative to the painting).\n    * Each distance (content, style) can be weighted to have more or less influence on the loss function.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1508.06576"
    },
    "935": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.06434",
        "transcript": "  * DCGANs are just a different architecture of GANs.\n  * In GANs a Generator network (G) generates images. A discriminator network (D) learns to differentiate between real images from the training set and images generated by G.\n  * DCGANs basically convert the laplacian pyramid technique (many pairs of G and D to progressively upscale an image) to a single pair of G and D.\n\n### How\n  * Their D: Convolutional networks. No linear layers. No pooling, instead strided layers. LeakyReLUs.\n  * Their G: Starts with 100d noise vector. Generates with linear layers 1024x4x4 values. Then uses fractionally strided convolutions (move by 0.5 per step) to upscale to 512x8x8. This is continued till Cx32x32 or Cx64x64. The last layer is a convolution to 3x32x32/3x64x64 (Tanh activation).\n  * The fractionally strided convolutions do basically the same as the progressive upscaling in the laplacian pyramid. So it's basically one laplacian pyramid in a single network and all upscalers are trained jointly leading to higher quality images.\n  * They use Adam as their optimizer. To decrease instability issues they decreased the learning rate to 0.0002 (from 0.001) and the momentum/beta1 to 0.5 (from 0.9).\n\n![Architecture of G](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks__G.png?raw=true \"Architecture of G\")\n\n*Architecture of G using fractionally strided convolutions to progressively upscale the image.*\n\n\n### Results\n  * High quality images. Still with distortions and errors, but at first glance they look realistic.\n  * Smooth interpolations between generated images are possible (by interpolating between the noise vectors and feeding these interpolations into G).\n  * The features extracted by D seem to have some potential for unsupervised learning.\n  * There seems to be some potential for vector arithmetics (using the initial noise vectors) similar to the vector arithmetics with wordvectors. E.g. to generate mean with sunglasses via `vector(men) + vector(sunglasses)`.\n\n\n![Example images (bedrooms)](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks__bedrooms.png?raw=true \"Example images (bedrooms)\")\n\n*Generated images, bedrooms.*\n\n\n![Example images (faces)](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks__faces.png?raw=true \"Example images (faces)\")\n\n*Generated images, faces.*\n\n\n\n### Rough chapter-wise notes\n\n* Introduction\n  * For unsupervised learning, they propose to use to train a GAN and then reuse the weights of D.\n  * GANs have traditionally been hard to train.\n\n* Approach and model architecture\n  * They use for D an convnet without linear layers, withput pooling layers (only strides), LeakyReLUs and Batch Normalization.\n  * They use for G ReLUs (hidden layers) and Tanh (output).\n\n* Details of adversarial training\n  * They trained on LSUN, Imagenet-1k and a custom dataset of faces.\n  * Minibatch size was 128.\n  * LeakyReLU alpha 0.2.\n  * They used Adam with a learning rate of 0.0002 and momentum of 0.5.\n  * They note that a higher momentum lead to oscillations.\n\n* LSUN\n  * 3M images of bedrooms.\n  * They use an autoencoder based technique to filter out 0.25M near duplicate images.\n\n* Faces\n  * They downloaded 3M images of 10k people.\n  * They extracted 350k faces with OpenCV.\n\n* Empirical validation of DCGANs capabilities\n  * Classifying CIFAR-10 GANs as a feature extractor\n    * They train a pair of G and D on Imagenet-1k.\n    * D's top layer has `512*4*4` features.\n    * They train an SVM on these features to classify the images of CIFAR-10.\n    * They achieve a score of 82.8%, better than unsupervised K-Means based methods, but worse than Exemplar CNNs.\n  * Classifying SVHN digits using GANs as a feature extractor\n    * They reuse the same pipeline (D trained on CIFAR-10, SVM) for the StreetView House Numbers dataset.\n    * They use 1000 SVHN images (with the features from D) to train the SVM.\n    * They achieve 22.48% test error.\n\n* Investigating and visualizing the internals of the networks\n  * Walking in the latent space\n    * The performs walks in the latent space (= interpolate between input noise vectors and generate several images for the interpolation).\n    * They argue that this might be a good way to detect overfitting/memorizations as those might lead to very sudden (not smooth) transitions.\n  * Visualizing the discriminator features\n    * They use guided backpropagation to visualize what the feature maps in D have learned (i.e. to which images they react).\n    * They can show that their LSUN-bedroom GAN seems to have learned in an unsupervised way what beds and windows look like.\n  * Forgetting to draw certain objects\n    * They manually annotated the locations of objects in some generated bedroom images.\n    * Based on these annotations they estimated which feature maps were mostly responsible for generating the objects.\n    * They deactivated these feature maps and regenerated the images.\n    * That decreased the appearance of these objects. It's however not as easy as one feature map deactivation leading to one object disappearing. They deactivated quite a lot of feature maps (200) and they objects were often still quite visible or replaced by artefacts/errors.\n  * Vector arithmetic on face samples\n    * Wordvectors can be used to perform semantic arithmetic (e.g. `king - man + woman = queen`).\n    * The unsupervised representations seem to be useable in a similar fashion.\n    * E.g. they generated images via G. They then picked several images that showed men with glasses and averaged these image's noise vectors. They did with same with men without glasses and women without glasses. Then they performed on these vectors `men with glasses - mean without glasses + women without glasses` to get `woman with glasses",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.06434"
    },
    "936": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/OordKK16",
        "transcript": "*Note*: This paper felt rather hard to read. The summary might not have hit exactly what the authors tried to explain.\n\n  * The authors describe multiple architectures that can model the distributions of images.\n  * These networks can be used to generate new images or to complete existing ones.\n  * The networks are mostly based on RNNs.\n\n### How\n  * They define three architectures:\n    * Row LSTM:\n      * Predicts a pixel value based on all previous pixels in the image.\n      * It applies 1D convolutions (with kernel size 3) to the current and previous rows of the image.\n      * It uses the convolution results as features to predict a pixel value.\n    * Diagonal BiLSTM:\n      * Predicts a pixel value based on all previous pixels in the image.\n      * Instead of applying convolutions in a row-wise fashion, they apply them to the diagonals towards the top left and top right of the pixel.\n      * Diagonal convolutions can be applied by padding the n-th row with `n-1` pixels from the left (diagonal towards top left) or from the right (diagonal towards the top right), then apply a 3x1 column convolution.\n    * PixelCNN:\n      * Applies convolutions to the region around a pixel to predict its values.\n      * Uses masks to zero out pixels that follow after the target pixel.\n      * They use no pooling layers.\n      * While for the LSTMs each pixel is conditioned on all previous pixels, the dependency range of the CNN is bounded.\n  * They use up to 12 LSTM layers.\n  * They use residual connections between their LSTM layers.\n  * All architectures predict pixel values as a softmax over 255 distinct values (per channel). According to the authors that leads to better results than just using one continuous output (i.e. sigmoid) per channel.\n  * They also try a multi-scale approach: First, one network generates a small image. Then a second networks generates the full scale image while being conditioned on the small image.\n\n### Results\n  * The softmax layers learn reasonable distributions. E.g. neighboring colors end up with similar probabilities. Values 0 and 255 tend to have higher probabilities than others, especially for the very first pixel.\n  * In the 12-layer LSTM row model, residual and skip connections seem to have roughly the same effect on the network's results. Using both yields a tiny improvement over just using one of the techniques alone.\n  * They achieve a slightly better result on MNIST than DRAW did.\n  * Their negative log likelihood results for CIFAR-10 improve upon previous models. The diagonal BiLSTM model performs best, followed by the row LSTM model, followed by PixelCNN.\n  * Their generated images for CIFAR-10 and Imagenet capture real local spatial dependencies. The multi-scale model produces better looking results. The images do not appear blurry. Overall they still look very unreal.\n\n\n![Generated ImageNet images](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Pixel_Recurrent_Neural_Networks__imagenet_multiscale.png?raw=true \"Generated ImageNet images\")\n\n*Generated ImageNet 64x64 images.*\n\n\n![Image completion](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Pixel_Recurrent_Neural_Networks__occlusion.png?raw=true \"Image completion\")\n\n*Completing partially occluded images.*",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1601.06759"
    },
    "937": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/LiSZ15",
        "transcript": "  * Generative Moment Matching Networks (GMMN) are generative models that use maximum mean discrepancy (MMD) for their objective function.\n  * MMD is a measure of how similar two datasets are (here: generated dataset and training set).\n  * GMMNs are similar to GANs, but they replace the Discriminator with the MMD measure, making their optimization more stable.\n\n### How\n  * MMD calculates a similarity measure by comparing statistics of two datasets with each other.\n  * MMD is calculated based on samples from the training set and the generated dataset.\n  * A kernel function is applied to pairs of these samples (thus the statistics are acutally calculated in high-dimensional spaces). The authors use Gaussian kernels.\n  * MMD can be approximated using a small number of samples.\n  * MMD is differentiable and therefor can be used as a standard loss function.\n  * They train two models:\n    * GMMN: Noise vector input (as in GANs), several ReLU layers into one sigmoid layer. MMD as the loss function.\n    * GMMN+AE: Same as GMMN, but the sigmoid output is not an image, but instead the code that gets fed into an autoencoder's (AE) decoder. The AE is trained separately on the dataset. MMD is backpropagated through the decoder and then the GMMN. I.e. the GMMN learns to produce codes that let the decoder generate good looking images.\n\n![Formula](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Generative_Moment_Matching_Networks__formula.png?raw=true \"Formula\")\n\n*MMD formula, where $x_i$ is a training set example and $y_i$ a generated example.*\n\n\n![Architectures](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Generative_Moment_Matching_Networks__architectures.png?raw=true \"Architectures\")\n\n*Architectures of GMMN (left) and GMMN+AE (right).*\n\n\n### Results\n  * They tested only on MNIST and TFD (i.e. datasets that are well suited for AEs...).\n  * Their GMMN achieves similar log likelihoods compared to other models.\n  * Their GMMN+AE achieves better log likelihoods than other models.\n  * GMMN+AE produces good looking images.\n  * GMMN+AE produces smooth interpolations between images.\n\n![Interpolations](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Generative_Moment_Matching_Networks__interpolations.png?raw=true \"Interpolations\")\n\n*Generated TFD images and interpolations between them.*\n\n\n--------------------\n\n### Rough chapter-wise notes\n\n* (1) Introduction\n  * Sampling in GMMNs is fast.\n  * GMMNs are similar to GANs.\n  * While the training objective in GANs is a minimax problem, in GMMNs it is a simple loss function.\n  * GMMNs are based on maximum mean discrepancy. They use that (implemented via the kernel trick) as the loss function.\n  * GMMNs try to generate data so that the moments in the generated data are as similar as possible to the moments in the training data.\n  * They combine GMMNs with autoencoders. That is, they first train an autoencoder to generate images. Then they train a GMMN to produce sound code inputs to the decoder of the autoencoder.\n\n* (2) Maximum Mean Discrepancy\n  * Maximum mean discrepancy (MMD) is a frequentist estimator to tell whether two datasets X and Y come from the same probability distribution.\n  * MMD estimates basic statistics values (i.e. mean and higher order statistics) of both datasets and compares them with each other.\n  * MMD can be formulated so that examples from the datasets are only used for scalar products. Then the kernel trick can be applied.\n  * It can be shown that minimizing MMD with gaussian kernels is equivalent to matching all moments between the probability distributions of the datasets.\n\n* (4) Generative Moment Matching Networks\n  * Data Space Networks\n    * Just like GANs, GMMNs start with a noise vector that has N values sampled uniformly from [-1, 1].\n    * The noise vector is then fed forward through several fully connected ReLU layers.\n    * The MMD is differentiable and therefor can be used for backpropagation.\n  * Auto-Encoder Code Sparse Networks\n    * AEs can be used to reconstruct high-dimensional data, which is a simpler task than to learn to generate new data from scratch.\n    * Advantages of using the AE code space:\n      * Dimensionality can be explicitly chosen.\n      * Disentangling factors of variation.\n    * They suggest a combination of GMMN and AE. They first train an AE, then they train a GMMN to generate good codes for the AE's decoder (based on MMD loss).\n    * For some reason they use greedy layer-wise pretraining with later fine-tuning for the AE, but don't explain why. (That training method is outdated?)\n    * They add dropout to their AE's encoder to get a smoother code manifold.\n  * Practical Considerations\n    * MMD has a bandwidth parameter (as its based on RBFs). Instead of chosing a single fixed bandwidth, they instead use multiple kernels with different bandwidths (1, 5, 10, ...), apply them all and then sum the results.\n    * Instead of $MMD^2$ loss they use $\\sqrt{MMD^2}$, which does not go as fast to zero as raw MMD, thereby creating stronger gradients.\n    * Per minibatch they generate a small number of samples und they pick a small number of samples from the training set. They then compute MMD for these samples. I.e. they don't run MMD over the whole training set as that would be computationally prohibitive.\n\n* (5) Experiments\n  * They trained on MNIST and TFD.\n  * They used an GMMN with 4 ReLU layers and autoencoders with either 2/2 (encoder, decoder) hidden sigmoid layers (MNIST) or 3/3 (TFD).\n  * They used dropout on the encoder layers.\n  * They used layer-wise pretraining and finetuning for the AEs.\n  * They tuned most of the hyperparameters using bayesian optimization.\n  * They use minibatch sizes of 1000 and compute MMD based on those (i.e. based on 2000 points total).\n  * Their GMMN+AE model achieves better log likelihood values than all competitors. The raw GMMN model performs roughly on par with the competitors.\n  * Nearest neighbor evaluation indicates that it did not just memorize the training set.\n  * The model learns smooth interpolations between digits (MNIST) and faces (TFD).",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/li15.html"
    },
    "938": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1602.02644",
        "transcript": "  * The authors define in this paper a special loss function (DeePSiM), mostly for autoencoders.\n  * Usually one would use a MSE of euclidean distance as the loss function for an autoencoder. But that loss function basically always leads to blurry reconstructed images.\n  * They add two new ingredients to the loss function, which results in significantly sharper looking images.\n\n### How\n  * Their loss function has three components:\n    * Euclidean distance in image space (i.e. pixel distance between reconstructed image and original image, as usually used in autoencoders)\n    * Euclidean distance in feature space. Another pretrained neural net (e.g. VGG, AlexNet, ...) is used to extract features from the original and the reconstructed image. Then the euclidean distance between both vectors is measured.\n    * Adversarial loss, as usually used in GANs (generative adversarial networks). The autoencoder is here treated as the GAN-Generator. Then a second network, the GAN-Discriminator is introduced. They are trained in the typical GAN-fashion. The loss component for DeePSiM is the loss of the Discriminator. I.e. when reconstructing an image, the autoencoder would learn to reconstruct it in a way that lets the Discriminator believe that the image is real.\n  * Using the loss in feature space alone would not be enough as that tends to lead to overpronounced high frequency components in the image (i.e. too strong edges, corners, other artefacts).\n  * To decrease these high frequency components, a \"natural image prior\" is usually used. Other papers define some function by hand. This paper uses the adversarial loss for that (i.e. learns a good prior).\n  * Instead of training a full autoencoder (encoder + decoder) it is also possible to only train a decoder and feed features - e.g. extracted via AlexNet - into the decoder.\n\n### Results\n  * Using the DeePSiM loss with a normal autoencoder results in sharp reconstructed images.\n  * Using the DeePSiM loss with a VAE to generate ILSVRC-2012 images results in sharp images, which are locally sound, but globally don't make sense. Simple euclidean distance loss results in blurry images.\n  * Using the DeePSiM loss when feeding only image space features (extracted via AlexNet) into the decoder leads to high quality reconstructions. Features from early layers will lead to more exact reconstructions.\n  * One can again feed extracted features into the network, but then take the reconstructed image, extract features of that image and feed them back into the network. When using DeePSiM, even after several iterations of that process the images still remain semantically similar, while their exact appearance changes (e.g. a dog's fur color might change, counts of visible objects change).\n\n![Generated images](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Generating_Images_with_Perceptual_Similarity_Metrics_based_on_Deep_Networks__generated_images.png?raw=true \"Generated images\")\n\n*Images generated with a VAE using DeePSiM loss.*\n\n\n![Reconstructed images](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Generating_Images_with_Perceptual_Similarity_Metrics_based_on_Deep_Networks__reconstructed.png?raw=true \"Reconstructed images\")\n\n*Images reconstructed from features fed into the network. Different AlexNet layers (conv5 - fc8) were used to generate the features. Earlier layers allow more exact reconstruction.*\n\n\n![Iterated reconstruction](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Generating_Images_with_Perceptual_Similarity_Metrics_based_on_Deep_Networks__reconstructed_multi.png?raw=true \"Iterated reconstruction\")\n\n*First, images are reconstructed from features (AlexNet, layers conv5 - fc8 as columns). Then, features of the reconstructed images are fed back into the network. That is repeated up to 8 times (rows). Images stay semantically similar, but their appearance changes.*\n\n--------------------\n\n### Rough chapter-wise notes\n\n* (1) Introduction\n  * Using a MSE of euclidean distances for image generation (e.g. autoencoders) often results in blurry images.\n  * They suggest a better loss function that cares about the existence of features, but not as much about their exact translation, rotation or other local statistics.\n  * Their loss function is based on distances in suitable feature spaces.\n  * They use ConvNets to generate those feature spaces, as these networks are sensitive towards important changes (e.g. edges) and insensitive towards unimportant changes (e.g. translation).\n  * However, naively using the ConvNet features does not yield good results, because the networks tend to project very different images onto the same feature vectors (i.e. they are contractive). That leads to artefacts in the generated images.\n  * Instead, they combine the feature based loss with GANs (adversarial loss). The adversarial loss decreases the negative effects of the feature loss (\"natural image prior\").\n\n* (3) Model\n  * A typical choice for the loss function in image generation tasks (e.g. when using an autoencoders) would be squared euclidean/L2 loss or L1 loss.\n  * They suggest a new class of losses called \"DeePSiM\".\n  * We have a Generator `G`, a Discriminator `D`, a feature space creator `C` (takes an image, outputs a feature space for that image), one (or more) input images `x` and one (or more) target images `y`. Input and target image can be identical.\n  * The total DeePSiM loss is a weighted sum of three components:\n    * Feature loss: Squared euclidean distance between the feature spaces of (1) input after fed through G and (2) the target image, i.e. `||C(G(x))-C(y)||^2_2`.\n    * Adversarial loss: A discriminator is introduced to estimate the \"fakeness\" of images generated by the generator. The losses for D and G are the standard GAN losses.\n    * Pixel space loss: Classic squared euclidean distance (as commonly used in autoencoders). They found that this loss stabilized their adversarial training.\n  * The feature loss alone would create high frequency artefacts in the generated image, which is why a second loss (\"natural image prior\") is needed. The adversarial loss fulfills that role.\n  * Architectures\n    * Generator (G):\n      * They define different ones based on the task.\n      * They all use up-convolutions, which they implement by stacking two layers: (1) a linear upsampling layer, then (2) a normal convolutional layer.\n      * They use leaky ReLUs (alpha=0.3).\n    * Comparators (C):\n      * They use variations of AlexNet and Exemplar-CNN.\n      * They extract the features from different layers, depending on the experiment.\n    * Discriminator (D):\n      * 5 convolutions (with some striding; 7x7 then 5x5, afterwards 3x3), into average pooling, then dropout, then 2x linear, then 2-way softmax.\n  * Training details\n    * They use Adam with learning rate 0.0002 and normal momentums (0.9 and 0.999).\n    * They temporarily stop the discriminator training when it gets too good.\n    * Batch size was 64.\n    * 500k to 1000k batches per training.\n\n* (4) Experiments\n  * Autoencoder\n    * Simple autoencoder with an 8x8x8 code layer between encoder and decoder (so actually more values than in the input image?!).\n    * Encoder has a few convolutions, decoder a few up-convolutions (linear upsampling + convolution).\n    * They train on STL-10 (96x96) and take random 64x64 crops.\n    * Using for C AlexNet tends to break small structural details, using Exempler-CNN breaks color details.\n    * The autoencoder with their loss tends to produce less blurry images than the common L2 and L1 based losses.\n    * Training an SVM on the 8x8x8 hidden layer performs significantly with their loss than L2/L1. That indicates potential for unsupervised learning.\n  * Variational Autoencoder\n    * They replace part of the standard VAE loss with their DeePSiM loss (keeping the KL divergence term).\n    * Everything else is just like in a standard VAE.\n    * Samples generated by a VAE with normal loss function look very blurry. Samples generated with their loss function look crisp and have locally sound statistics, but still (globally) don't really make any sense.\n  * Inverting AlexNet\n    * Assume the following variables:\n      * I: An image\n      * ConvNet: A convolutional network\n      * F: The features extracted by a ConvNet, i.e. ConvNet(I) (feaures in all layers, not just the last one)\n    * Then you can invert the representation of a network in two ways:\n      * (1) An inversion that takes an F and returns roughly the I that resulted in F (it's *not* key here that ConvNet(reconstructed I) returns the same F again).\n      * (2) An inversion that takes an F and projects it to *some* I so that ConvNet(I) returns roughly the same F again.\n    * Similar to the autoencoder cases, they define a decoder, but not encoder.\n    * They feed into the decoder a feature representation of an image. The features are extracted using AlexNet (they try the features from different layers).\n    * The decoder has to reconstruct the original image (i.e. inversion scenario 1). They use their DeePSiM loss during the training.\n    * The images can be reonstructed quite well from the last convolutional layer in AlexNet. Chosing the later fully connected layers results in more errors (specifially in the case of the very last layer).\n    * They also try their luck with the inversion scenario (2), but didn't succeed (as their loss function does not care about diversity).\n    * They iteratively encode and decode the same image multiple times (probably means: image -> features via AlexNet -> decode -> reconstructed image -> features via AlexNet -> decode -> ...). They observe, that the image does not get \"destroyed\", but rather changes semantically, e.g. three apples might turn to one after several steps.\n    * They interpolate between images. The interpolations are smooth.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1602.02644"
    },
    "939": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/GregorDGRW15",
        "transcript": "  * DRAW = deep recurrent attentive writer\n  * DRAW is a recurrent autoencoder for (primarily) images that uses attention mechanisms.\n  * Like all autoencoders it has an encoder, a latent layer `Z` in the \"middle\" and a decoder.\n  * Due to the recurrence, there are actually multiple autoencoders, one for each timestep (the number of timesteps is fixed).\n  * DRAW has attention mechanisms which allow the model to decide where to look at in the input image (\"glimpses\") and where to write/draw to in the output image.\n  * If the attention mechanisms are skipped, the model becomes a simple recurrent autoencoder.\n  * By training the full autoencoder on a dataset and then only using the decoder, one can generate new images that look similar to the dataset images.\n\n\n![DRAW Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/DRAW_A_Recurrent_Neural_Network_for_Image_Generation__architecture.png?raw=true \"DRAW Architecture\")\n\n*Basic recurrent architecture of DRAW.*\n\n\n### How\n  * General architecture\n    * The encoder-decoder-pair follows the design of variational autoencoders.\n    * The latent layer follows an n-dimensional gaussian distribution. The hyperparameters of that distribution (means, standard deviations) are derived from the output of the encoder using a linear transformation.\n    * Using a gaussian distribution enables the use of the reparameterization trick, which can be useful for backpropagation.\n    * The decoder receives a sample drawn from that gaussian distribution.\n    * While the encoder reads from the input image, the decoder writes to an image canvas (where \"write\" is an addition, not a replacement of the old values).\n    * The model works in a fixed number of timesteps. At each timestep the encoder performs a read operation and the decoder a write operation.\n    * Both the encoder and the decoder receive the previous output of the encoder.\n  * Loss functions\n    * The loss function of the latent layer is the KL-divergence between that layer's gaussian distribution and a prior, summed over the timesteps.\n    * The loss function of the decoder is the negative log likelihood of the image given the final canvas content under a bernoulli distribution.\n    * The total loss, which is optimized, is the expectation of the sum of both losses (latent layer loss, decoder loss).\n  * Attention\n    * The selective read attention works on image patches of varying sizes. The result size is always NxN.\n    * The mechanism has the following parameters:\n        * `gx`: x-axis coordinate of the center of the patch\n        * `gy`: y-axis coordinate of the center of the patch\n        * `delta`: Strides. The higher the strides value, the larger the read image patch.\n        * `sigma`: Standard deviation. The higher the sigma value, the more blurry the extracted patch will be.\n        * `gamma`: Intensity-Multiplier. Will be used on the result.\n        * All of these parameters are generated using a linear transformation applied to the decoder's output.\n    * The mechanism places a grid of NxN gaussians on the image. The grid is centered at `(gx, gy)`. The gaussians are `delta` pixels apart from each other and have a standard deviation of `sigma`.\n    * Each gaussian is applied to the image, the center pixel is read and added to the result.\n\n\n![DRAW Attention](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/DRAW_A_Recurrent_Neural_Network_for_Image_Generation__attention.png?raw=true \"DRAW Attention\")\n\n*The basic attention mechanism. (gx, gy) is the read patch center. delta is the strides. On the right: Patches with different sizes/strides and standard deviations/blurriness.*\n\n\n### Results\n  * Realistic looking generated images for MNIST and SVHN.\n  * Structurally OK, but overall blurry images for CIFAR-10.\n  * Results with attention are usually significantly better than without attention.\n  * Image generation without attention starts with a blurry image and progressively sharpens it.\n\n![DRAW SVHN Results](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/DRAW_A_Recurrent_Neural_Network_for_Image_Generation__svhn.png?raw=true \"DRAW SVHN Results\")\n\n*Using DRAW with attention to generate new SVHN images.*\n\n----------\n\n### Rough chapter-wise notes\n\n* 1. Introduction\n  * The natural way to draw an image is in a step by step way (add some lines, then add some more, etc.).\n  * Most generative neural networks however create the image in one step.\n  * That removes the possibility of iterative self-correction, is hard to scale to large images and makes the image generation process dependent on a single latent distribution (input parameters).\n  * The DRAW architecture generates images in multiple steps, allowing refinements/corrections.\n  * DRAW is based on varational autoencoders: An encoder compresses images to codes and a decoder generates images from codes.\n  * The loss function is a variational upper bound on the log-likelihood of the data.\n  * DRAW uses recurrance to generate images step by step.\n  * The recurrance is combined with attention via partial glimpses/foveations (i.e. the model sees only a small part of the image).\n  * Attention is implemented in a differentiable way in DRAW.\n\n* 2. The DRAW Network\n  * The DRAW architecture is based on variational autoencoders:\n    * Encoder: Compresses an image to latent codes, which represent the information contained in the image.\n    * Decoder: Transforms the codes from the encoder to images (i.e. defines a distribution over images which is conditioned on the distribution of codes).\n  * Differences to variational autoencoders:\n    * Encoder and decoder are both recurrent neural networks.\n    * The encoder receives the previous output of the decoder.\n    * The decoder writes several times to the image array (instead of only once).\n    * The encoder has an attention mechanism. It can make a decision about the read location in the input image.\n    * The decoder has an attention mechanism. It can make a decision about the write location in the output image.\n  * 2.1 Network architecture\n    * They use LSTMs for the encoder and decoder.\n    * The encoder generates a vector.\n    * The decoder generates a vector.\n    * The encoder receives at each time step the image and the output of the previous decoding step.\n    * The hidden layer in between encoder and decoder is a distribution Q(Zt|ht^enc), which is a diagonal gaussian.\n    * The mean and standard deviation of that gaussian is derived from the encoder's output vector with a linear transformation.\n    * Using a gaussian instead of a bernoulli distribution enables the use of the reparameterization trick. That trick makes it straightforward to backpropagate \"low variance stochastic gradients of the loss function through the latent distribution\".\n    * The decoder writes to an image canvas. At every timestep the vector generated by the decoder is added to that canvas.\n  * 2.2 Loss function\n    * The main loss function is the negative log probability: `-log D(x|ct)`, where `x` is the input image and `ct` is the final output image of the autoencoder. `D` is a bernoulli distribution if the image is binary (only 0s and 1s).\n    * The model also uses a latent loss for the latent layer (between encoder and decoder). That is typical for VAEs. The loss is the KL-Divergence between Q(Zt|ht_enc) (`Zt` = latent layer, `ht_enc` = result of encoder) and a prior `P(Zt)`.\n    * The full loss function is the expection value of both losses added up.\n  * 2.3 Stochastic Data Generation\n    * To generate images, samples can be picked from the latent layer based on a prior. These samples are then fed into the decoder. That is repeated for several timesteps until the image is finished.\n\n* 3. Read and Write Operations\n  * 3.1 Reading and writing without attention\n    * Without attention, DRAW simply reads in the whole image and modifies the whole output image canvas at every timestep.\n  * 3.2 Selective attention model\n    * The model can decide which parts of the image to read, i.e. where to look at. These looks are called glimpses.\n    * Each glimpse is defined by its center (x, y), its stride (zoom level), its gaussian variance (the higher the variance, the more blurry is the result) and a scalar multiplier (that scales the intensity of the glimpse result).\n    * These parameters are calculated based on the decoder output using a linear transformation.\n    * For an NxN patch/glimpse `N*N` gaussians are created and applied to the image. The center pixel of each gaussian is then used as the respective output pixel of the glimpse.\n  * 3.3 Reading and writing with attention\n    * Mostly the same technique from (3.2) is applied to both reading and writing.\n    * The glimpse parameters are generated from the decoder output in both cases. The parameters can be different (i.e. read and write at different positions).\n    * For RGB the same glimpses are applied to each channel.\n\n* 4. Experimental results\n  * They train on binary MNIST, cluttered MNIST, SVHN and CIFAR-10.\n  * They then classfiy the images (cluttered MNIST) or generate new images (other datasets).\n  * They say that these generated images are unique (to which degree?) and that they look realistic for MNIST and SVHN.\n  * Results on CIFAR-10 are blurry.\n  * They use binary crossentropy as the loss function for binary MNIST.\n  * They use crossentropy as the loss function for SVHN and CIFAR-10 (color).\n  * They used Adam as their optimizer.\n  * 4.1 Cluttered MNIST classification\n    * They classify images of cluttered MNIST. To do that, they use an LSTM that performs N read-glimpses and then classifies via a softmax layer.\n    * Their model's error rate is significantly below a previous non-differentiable attention based model.\n    * Performing more glimpses seems to decrease the error rate further.\n  * 4.2 MNIST generation\n    * They generate binary MNIST images using only the decoder.\n    * DRAW without attention seems to perform similarly to previous models.\n    * DRAW with attention seems to perform significantly better than previous models.\n    * DRAW without attention progressively sharpens images.\n    * DRAW with attention draws lines by tracing them.\n  * 4.3 MNIST generation with two digits\n    * They created a dataset of 60x60 images, each of them containing two random 28x28 MNIST images.\n    * They then generated new images using only the decoder.\n    * DRAW learned to do that.\n    * Using attention, the model usually first drew one digit then the other.\n  * 4.4 Street view house number generation\n    * They generate SVHN images using only the decoder.\n    * Results look quite realistic.\n  * 4.5 Generating CIFAR images\n    * They generate CIFAR-10 images using only the decoder.\n    * Results follow roughly the structure of CIFAR-images, but look blurry.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/gregor15.html"
    },
    "940": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DentonCSF15",
        "transcript": "  * The original GAN approach used one Generator (G) to generate images and one Discriminator (D) to rate these images.\n  * The laplacian pyramid GAN uses multiple pairs of G and D.\n  * It starts with an ordinary GAN that generates small images (say, 4x4).\n  * Each following pair learns to generate plausible upscalings of the image, usually by a factor of 2. (So e.g. from 4x4 to 8x8.)\n  * This scaling from coarse to fine resembles a laplacian pyramid, hence the name.\n\n### How\n  * The first pair of G and D is just like an ordinary GAN.\n  * For each pair afterwards, G recieves the output of the previous step, upscaled to the desired size. Due to the upscaling, the image will be blurry.\n  * G has to learn to generate a plausible sharpening of that blurry image.\n  * G outputs a difference image, not the full sharpened image.\n  * D recieves the upscaled/blurry image. D also recieves either the optimal difference image (for images from the training set) or G's generated difference image.\n  * D adds the difference image to the blurry image as its first step. Afterwards it applies convolutions to the image and ends in one sigmoid unit.\n  * The training procedure is just like in the ordinary GAN setting. Each upscaling pair of G and D can be trained on its own.\n  * The first G recieves a \"normal\" noise vector, just like in the ordinary GAN setting. Later Gs recieve noise as one plane, so each image has four channels: R, G, B, noise.\n\n### Results\n  * Images are rated as looking more realistic than the ones from ordinary GANs.\n  * The approximated log likelihood is significantly lower (improved) compared to ordinary GANs.\n  * The generated images do however still look distorted compared to real images.\n  * They also tried to add class conditional information to G and D (just a one hot vector for the desired class of the image). G and D learned successfully to adapt to that information (e.g. to only generate images that seem to show birds).\n\n\n\n![Sampling Process](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Deep_Generative_Image_Models_using_a_Laplacian_Pyramid_of_Adversarial_Networks__pyramid.png?raw=true \"Sampling process\")\n\n*Basic training and sampling process. The first image is generated directly from noise. Everything afterwards is de-blurring of upscaled images.*\n\n\n-------------------------\n\n### Rough chapter-wise notes\n\n* Introduction\n  * Instead of just one big generative model, they build multiple ones.\n  * They start with one model at a small image scale (e.g. 4x4) and then add multiple generative models that increase the image size (e.g. from 4x4 to 8x8).\n  * This scaling from coarse to fine (low frequency to high frequency components) resembles a laplacian pyramid, hence the name of the paper.\n\n* Related Works\n  * Types of generative image models:\n    * Non-Parametric: Models copy patches from training set (e.g. texture synthesis, super-resolution)\n    * Parametric: E.g. Deep Boltzmann machines or denoising auto-encoders\n  * Novel approaches: e.g. DRAW, diffusion-based processes, LSTMs\n  * This work is based on (conditional) GANs\n\n* Approach\n  * They start with a Gaussian and a Laplacian pyramid.\n  * They build the Gaussian pyramid by repeatedly decreasing the image height/width by 2: [full size image, half size image, quarter size image, ...]\n  * They build a Laplacian pyramid by taking pairs of images in the gaussian pyramid, upscaling the smaller one and then taking the difference.\n  * In the laplacian GAN approach, an image at scale k is created by first upscaling the image at scale k-1 and then adding a refinement to it (de-blurring). The refinement is created with a GAN that recieves the upscaled image as input.\n  * Note that the refinement is a difference image (between the upscaled image and the optimal upscaled image).\n  * The very first (small scale) image is generated by an ordinary GAN.\n  * D recieves an upscaled image and a difference image. It then adds them together to create an upscaled and de-blurred image. Then D applies ordinary convolutions to the result and ends in a quality rating (sigmoid).\n\n* Model Architecture and Training\n  * Datasets: CIFAR-10 (32x32, 100k images), STL (96x96, 100k), LSUN (64x64, 10M)\n  * They use a uniform distribution of [-1, 1] for their noise vectors.\n  * For the upscaling Generators they add the noise as a fourth plane (to the RGB image).\n  * CIFAR-10: 8->14->28 (height/width), STL: 8->16->32->64->96, LSUN: 4->8->16->32->64\n  * CIFAR-10: G=3 layers, D=2 layers, STL: G=3 layers, D=2 layers, LSUN: G=5 layers, D=3 layers.\n\n* Experiments\n  * Evaluation methods:\n    * Computation of log-likelihood on a held out image set\n      * They use a Gaussian window based Parzen estimation to approximate the probability of an image (note: not very accurate).\n      * They adapt their estimation method to the special case of the laplacian pyramid.\n      * Their laplacian pyramid model seems to perform significantly better than ordinary GANs.\n    * Subjective evaluation of generated images\n      * Their model seems to learn the rough structure and color correlations of images to generate.\n      * They add class conditional information to G and D. G indeed learns to generate different classes of images.\n      * All images still have noticeable distortions.\n    * Subjective evaluation of generated images by other people\n      * 15 volunteers.\n      * They show generated or real images in an interface for 50-2000ms. Volunteer then has to decide whether the image is fake or real.\n      * 10k ratings were collected.\n      * At 2000ms, around 50% of the generated images were considered real, ~90 of the true real ones and <10% of the images generated by an ordinary GAN. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks"
    },
    "941": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeGDG17",
        "transcript": "  * They suggest a variation of Faster R-CNN.\n  * Their network detects bounding boxes (e.g. of people, cars) in images *and also* segments the objects within these bounding boxes (i.e. classifies for each pixel whether it is part of the object or background).\n  * The model runs roughly at the same speed as Faster R-CNN.\n\n### How\n  * The architecture and training is mostly the same as in Faster R-CNN:\n    * Input is an image.\n    * The *backbone* network transforms the input image into feature maps. It consists of convolutions, e.g. initialized with ResNet's weights.\n    * The *RPN* (Region Proposal Network) takes the feature maps and classifies for each location whether there is a bounding box at that point (with some other stuff to regress height/width and offsets).\n      This leads to a large number of bounding box candidates (region proposals) per image.\n    * *RoIAlign*: Each region proposal's \"area\" is extracted from the feature maps and converted into a fixed-size `7x7xF` feature map (with F input filters). (See below.)\n    * The *head* uses the region proposal's features to perform\n      * Classification: \"is the bounding box of a person/car/.../background\"\n      * Regression: \"bounding box should have width/height/offset so and so\"\n      * Segmentation: \"pixels so and so are part of this object's mask\"\n    * Rough visualization of the architecture:\n      * ![Architecture](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Mask_R-CNN/architecture.jpg?raw=true \"Architecture\")\n  * RoIAlign\n    * This is very similar to RoIPooling in Faster R-CNN.\n    * For each RoI, RoIPooling first \"finds\" the features in the feature maps that lie within the RoI's rectangle. Then it max-pools them to create a fixed size vector.\n    * Problem: The coordinates where an RoI starts and ends may be non-integers. E.g. the top left corner might have coordinates `(x=2.5, y=4.7)`.\n      RoIPooling simply rounds these values to the nearest integers (e.g. `(x=2, y=5)`).\n      But that can create pooled RoIs that are significantly off, as the feature maps with which RoIPooling works have high (total) stride (e.g. 32 pixels in standard ResNets).\n      So being just one cell off can easily lead to being 32 pixels off on the input image.\n    * For classification, being some pixels off is usually not that bad. For masks however it can significantly worsen the results, as these have to be pixel-accurate.\n    * In RoIAlign this is compensated by not rounding the coordinates and instead using bilinear interpolation to interpolate between the feature map's cells.\n    * Each RoI is pooled by RoIAlign to a fixed sized feature map of size `(H, W, F)`, with H and W usually being 7 or 14. (It can also generate different sizes, e.g. `7x7xF` for classification and more accurate `14x14xF` for masks.)\n    * If H and W are `7`, this leads to `49` cells within each plane of the pooled feature maps.\n    * Each cell again is a rectangle -- similar to the RoIs -- and pooled with bilinear interpolation.\n      More exactly, each cell is split up into four sub-cells (top left, top right, bottom right, bottom left).\n      Each of these sub-cells is pooled via bilinear interpolation, leading to four values per cell.\n      The final cell value is then computed using either an average or a maximum over the four sub-values.\n  * Segmentation\n    * They add an additional branch to the *head* that gets pooled RoI as inputs and processes them seperately from the classification and regression (no connections between the branches).\n    * That branch does segmentation. It is fully convolutional, similar to many segmentation networks.\n    * The result is one mask per class.\n    * There is no softmax per pixel over the classes, as classification is done by a different branch.\n  * Base networks\n    * Their *backbone* networks are either ResNet or ResNeXt (in the 50 or 102 layer variations).\n    * Their *head* is either the fourth/fifth module from ResNet/ResNeXt (called *C4* (fourth) or *C5* (fifth)) or they use the second half from the FPN network (called *FPN*).\n    * They denote their networks via `backbone-head`, i.e. ResNet-101-FPN means that their backbone is ResNet-101 and their head is FPN.\n    * Visualization of the different heads:\n      * ![Architecture heads](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Mask_R-CNN/head.jpg?raw=true \"Architecture heads\")\n  * Training\n    * Training happens in basically the same way as Faster R-CNN.\n    * They just add an additional loss term to the total loss (`L = L_classification + L_regression + L_mask`). `L_mask` is based on binary cross-entropy.\n    * For each predicted RoI, the correct mask is the intersection between that RoI's area and the correct mask.\n    * They only train masks for RoIs that are positive (overlap with ground truth bounding boxes).\n    * They train for 120k iterations at learning rate 0.02 and 40k at 0.002 with weight decay 0.0002 and momentum 0.9.\n  * Test\n    * For the *C4*-head they sample up to 300 region proposals from the RPN (those with highest confidence values). For the FPN head they sample up to 1000, as FPN is faster.\n    * They sample masks only for the 100 proposals with highest confidence values.\n    * Each mask is turned into a binary mask using a threshold of 0.5.\n\n### Results\n  * Instance Segmentation\n    * They train and test on COCO.\n    * They can outperform the best competitor by a decent margin (AP 37.1 vs 33.6 for FCIS+++ with OHEM).\n    * Their model especially performs much better when there is overlap between bounding boxes.\n    * Ranking of their models: ResNeXt-101-FPN > ResNet-101-FPN > ResNet-50-FPN > ResNet-101-C4 > ResNet-50-C4.\n    * Using sigmoid instead of softmax (over classes) for the mask prediction significantly improves results by 5.5 to 7.1 points AP (depending on measurement method).\n    * Predicting only one mask per RoI (class-agnostic) instead of C masks (where C is the number of classes) only has a small negative effect on AP (about 0.6 points).\n    * Using RoIAlign instead of RoIPooling has significant positive effects on the AP of around 5 to 10 points (if a network with C5 head is chosen, which has a high stride of 32). Effects are smaller for small strides and FPN head.\n    * Using fully convolutional networks for the mask branch performs better than fully connected layers (1-3 points AP).\n    * Examples results on COCO vs FCIS (note the better handling of overlap):\n      * ![Examples COCO](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Mask_R-CNN/examples_overlap.jpg?raw=true \"Examples COCO\")\n  * Bounding-Box-Detection\n    * Training additionally on masks seems to improve AP for bounding boxes by around 1 point (benefit from multi-task learning).\n  * Timing\n    * Around 200ms for ResNet-101-FPN. (M40 GPU)\n    * Around 400ms for ResNet-101-C4.\n  * Human Pose Estimation\n    * The mask branch can be used to predict keypoint (landmark) locations on human bodies (i.e. locations of hands, feet etc.).\n    * This is done by using one mask per keypoint, initializing it to `0` and setting the keypoint location to `1`.\n    * By doing this, Mask R-CNN can predict keypoints roughly as good as the current leading models (on COCO), while running at 5fps.\n  * Cityscapes\n    * They test their model on the cityscapes dataset.\n    * They beat previous models with significant margins. This is largely due to their better handling of overlapping instances.\n    * They get their best scores using a model that was pre-trained on COCO.\n    * Examples results on cityscapes:\n      * ![Examples Cityscapes](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Mask_R-CNN/examples_cityscapes.jpg?raw=true \"Examples Cityscapes\")\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.06870"
    },
    "942": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HowardZCKWWAA17",
        "transcript": "  * They suggest a factorization of standard 3x3 convolutions that is more efficient.\n  * They build a model based on that factorization. The model has hyperparameters to choose higher performance or higher accuracy.\n\n### How\n  * Factorization\n    * They factorize the standard 3x3 convolution into one depthwise 3x3 convolution, followed by a pointwise convoluton.\n    * Normal 3x3 convolution:\n      * Computes per filter and location a weighted average over all filters.\n      * For kernel height `kH`, width `kW` and number of input filters/planes `Fin`, it requires `kH*kW*Fin` computations per location.\n    * Depthwise 3x3 convolution:\n      * Computes per filter and location a weighted average over *one* input filter. E.g. the 13th filter would only computed weighted averages over the 13th input filter/plane and ignore all the other input filters/planes.\n      * This requires `kH*kW*1` computations per location, i.e. drastically less than a normal convolution.\n    * Pointwise convolution:\n      * This is just another name for a normal 1x1 convolution.\n      * This is placed after a depthwise convolution in order to compensate the fact that every (depthwise) filter only sees a single input plane.\n      * As the kernel size is `1`, this is rather fast to compute.\n    * Visualization of normal vs factorized convolution:\n      * ![architecture](https://github.com/aleju/papers/blob/master/neural-nets/images/MobileNets/architecture.jpg?raw=true \"architecture\")\n  * Models\n    * They use two hyperparameters for their models.\n    * `alpha`: Multiplier for the width in the range `(0, 1]`. A value of 0.5 means that every layer has half as many filters.\n    * `roh`: Multiplier for the resolution. In practice this is simply the input image size, having a value of `{224, 192, 160, 128}`.\n\n### Results\n  * ImageNet\n    * Compared to VGG16, they achieve 1 percentage point less accuracy, while using only about 4% of VGG's multiply and additions (mult-adds) and while using only about 3% of the parameters.\n    * Compared to GoogleNet, they achieve about 1 percentage point more accuracy, while using only about 36% of the mult-adds and 61% of the parameters.\n    * Note that they don't compare to ResNet.\n    * Results for architecture choices vs. accuracy on ImageNet:\n      * ![results imagenet](https://github.com/aleju/papers/blob/master/neural-nets/images/MobileNets/results_imagenet.jpg?raw=true \"results imagenet\")\n    * Relation between mult-adds and accuracy on ImageNet:\n      * ![mult-adds vs accuracy](https://github.com/aleju/papers/blob/master/neural-nets/images/MobileNets/mult-adds_vs_accuracy.jpg?raw=true \"mult-adds vs accuracy\")\n  * Object Detection\n    * Their mAP is a bit on COCO when combining MobileNet with SSD (as opposed to using VGG or Inception v2).\n    * Their mAP is quite a bit worse on COCO when combining MobileNet with Faster R-CNN.\n  * Reducing the number of filters (`alpha`) influences the results more than reducing the input image resolution (`roh`).\n  * Making the models shallower influences the results more than making them thinner.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.04861"
    },
    "943": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eccv/LiuAESRFB16",
        "transcript": "  * They suggest a new bounding box detector.\n  * Their detector works without an RPN and RoI-Pooling, making it very fast (almost 60fps).\n  * Their detector works at multiple scales, making it better at detecting small and large objects.\n  * They achieve scores similar to Faster R-CNN.\n\n### How\n  * Architecture\n    * Similar to Faster R-CNN, they use a base network (modified version of VGG16) to transform images to feature maps.\n    * They do not use an RPN.\n    * They predict via convolutions for each location in the feature maps:\n      * (a) one confidence value per class (high confidence indicates that there is a bounding box of that class at the given location)\n      * (b) x/y offsets that indicate where exactly the center of the bounding box is (e.g. a bit to the left or top of the feature map cell's center)\n      * (c) height/width values that reflect the (logarithm of) the height/width of the bounding box\n    * Similar to Faster R-CNN, they also use the concept of anchor boxes.\n      So they generate the described values not only once per location, but several times for several anchor boxes (they use six anchor boxes).\n      Each anchor box has different height/width and optionally scale.\n    * Visualization of the predictions and anchor boxes:\n      * ![predictions](https://github.com/aleju/papers/blob/master/neural-nets/images/SSD/predictions.jpg?raw=true \"predictions\")\n    * They generate these predictions not only for the final feature map, but also for various feature maps in between (e.g. before pooling layers).\n      This makes it easier for the network to detect small (as well as large) bounding boxes (multi-scale detection).\n    * Visualization of the multi-scale architecture:\n      * ![architecture](https://github.com/aleju/papers/blob/master/neural-nets/images/SSD/architecture.jpg?raw=true \"architecture\")\n  * Training\n    * Ground truth bounding boxes have to be matched with anchor boxes (at multiple scales) to determine correct outputs.\n      To do this, anchor boxes and ground truth bounding boxes are matched if their jaccard overlap is 0.5 or higher.\n      Any unmatched ground truth bounding box is matched to the anchor box with highest jaccard overlap.\n    * Note that this means that a ground truth bounding box can be assigned to multiple anchor boxes (in Faster R-CNN it is always only one).\n    * The loss function is similar to Faster R-CNN, i.e. a mixture of confidence loss (classification) and location loss (regression).\n      They use softmax with crossentropy for the confidence loss and smooth L1 loss for the location.\n    * Similar to Faster R-CNN, they perform hard negative mining.\n      Instead of training every anchor box at every scale they only train the ones with the highest loss (per example image).\n      While doing that, they also pick the anchor boxes to be trained so that 3 in 4 boxes are negative examples (and 1 in 4 positive).\n    * Data Augmentation: They sample patches from images using a wide range of possible sizes and aspect ratios.\n      They also horizontally flip images, perform cropping and padding and perform some photo-metric distortions.\n  * Non-Maximum-Suppression (NMS)\n    * Upon inference, they remove all bounding boxes that have a confidence below 0.01.\n    * They then apply NMS, removing bounding boxes if there is already a similar one (measured by jaccard overlap of 0.45 or more).\n\n### Results\n  * Pascal VOC 2007\n    * They achieve around 1-3 points mAP better results than Faster R-CNN.\n    * ![results pascal](https://github.com/aleju/papers/blob/master/neural-nets/images/SSD/results_pascal.jpg?raw=true \"results pascal\")\n    * Despite the multi-scale method, the model's performance is still significantly worse for small objects than for large ones.\n    * Adding data augmentation significantly improved the results compared to no data augmentation (around 6 points mAP).\n    * Using more than one anchor box also had noticeable effects on the results (around 2 mAP or more).\n    * Using multiple feature maps to predict outputs (multi-scale architecture) significantly improves the results (around 10 mAP).\n      Though adding very coarse (high-level) feature maps seems to rather hurt than help.\n  * Pascal VOC 2012\n    * Around 4 mAP better results than Faster R-CNN.\n  * COCO\n    * Between 1 and 4 mAP better results than Faster R-CNN.\n  * Times\n    * At a batch size of 1, SSD runs at about 46 fps at input resolution 300x300 (74.3 mAP on Pascal VOC) and 19 fps at input resolution 512x512 (76.8 mAP on Pascal VOC).\n    * ![results timings](https://github.com/aleju/papers/blob/master/neural-nets/images/SSD/results_timings.jpg?raw=true \"results timings\")\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-46448-0_2"
    },
    "944": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LinDGHHB16",
        "transcript": "  * They suggest a modified network architecture for object detectors (i.e. bounding box detectors).\n  * The architecture aggregates features from many scales (i.e. before each pooling layer) to detect both small and large object.\n  * The network is shaped similar to an hourglass.\n\n###  How\n  * Architecture\n    * They have two branches.\n    * The first one is similar to any normal network:\n      Convolutions and pooling.\n      The exact choice of convolutions (e.g. how many) and pooling is determined by the used base network (e.g. ~50 convolutions with ~5x pooling in ResNet-50).\n    * The second branch starts at the first one's output.\n      It uses nearest neighbour upsampling to re-increase the resolution back to the original one.\n      It does not contain convolutions.\n      All layers have 256 channels.\n    * There are connections between the layers of the first and second branch.\n      These connections are simply 1x1 convolutions followed by an addition (similar to residual connections).\n      Only layers with similar height and width are connected.\n    * Visualization:\n      * ![architecture](https://github.com/aleju/papers/blob/master/neural-nets/images/Feature_Pyramid_Networks_for_Object_Detection/architecture.jpg?raw=true \"architecture\")\n  * Integration with Faster R-CNN\n    * They base the RPN on their second branch.\n    * While usually an RPN is applied to a single feature map of one scale, in their case it is applied to many feature maps of varying scales.\n    * The RPN uses the same parameters for all scales.\n    * They use anchor boxes, but only of different aspect ratios, not of different scales (as scales are already covered by their feature map heights/widths).\n    * Ground truth bounding boxes are associated with the best matching anchor box (i.e. one box among all scales).\n    * Everything else is the same as in Faster R-CNN.\n  * Integration with Fast R-CNN\n    * Fast R-CNN does not use an RPN, but instead usually uses Selective Search to find region proposals (and applies RoI-Pooling to them).\n    * Here, they simply RoI-Pool from the FPN's output of the second branch.\n    * They do not pool over all scales. Instead they pick only the scale/layer that matches the region proposal's size (based on its height/width).\n    * They process each pooled RoI using two 1024-dimensional fully connected layers (initalizes randomly).\n    * Everything else is the same as in Fast R-CNN.\n\n### Results\n  * Faster R-CNN\n    * FPN improves recall on COCO by about 8 points, compared to using standard RPN.\n    * Improvement is stronger for small objects (about 12 points).\n    * For some reason no AP values here, only recall.\n    * The RPN uses some convolutions to transform each feature map into region proposals.\n      Sharing the features of these convolutions marginally improves results.\n  * Fast R-CNN\n    * FPN improves AP on COCO by about 2 points.\n    * Improvement is stronger for small objects (about 2.1 points).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.03144"
    },
    "945": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2102.09475",
        "transcript": "**Background:** The goal of this work is to indicate image features which are relevant to the prediction of a neural network and convey that information to the user by displaying a counterfactual image animation.\n\n**The Latent Shift Method:** This method works on any pretrained encoder/decoder and classifier which is differentiable. No special considerations are needed during model training. With this approach they want the exact opposite of an adversarial attack but it is using the same idea. They want to perturb the input image so that the classifier reduces its prediction. If they just compute $\\frac{\\partial f}{\\partial x}$ and move the pixels directly then they will get an imperceivable difference like an adversarial attack. Using a decoder they can regularize the transformation so it will only yield value images.\n\nThe encoder takes the input image and encodes it into a latent representation $z$. Then the decoder reconstructs the image and feeds this image into the classifier. The gradient is computed from the output of the classifier with respect to $z$. Subtracting the gradient from z and reconstructing the image generates a counterfactual.\n\nhttps://i.imgur.com/iuZGUTH.gif\n\nThey found that if they change the prediction by -30% the images come out pretty good. So an iterative search along the vector defined by the gradient in the latent space until the prediction is reduced by 30%.\n\nFrom this sequence a 2D image can be reconstructed which is similar to a traditional attribution map by taking the maximum pixel wise difference between every image and the unperturbed reconstruction.\n\nhttps://i.imgur.com/V3PCgXZ.png\n\nThe results look great!\n\nhttps://i.imgur.com/DBki84c.gif\n\nhttps://i.imgur.com/kFfQNKD.gif\n\nIn order to validate if this approach can help spot false positive predictions, two radiologists to evaluate how confident they were in a models predictions. For each image, radiologists viewed the prediction in two ways, using traditional methods or the Latent Shift images. Traditional methods includes the image gradient, guided backprop, and integrated gradients. The Latent Shift Counterfactual includes the animation as well as the 2D version.\n\nhttps://i.imgur.com/TlUBhzL.png\n\nWhat they would like to see, that for true positives, the results are all 5 and for false positives they are all 1. \nWhat they observe however, is that many false positives still cause high confidence in the model predictions but not as much as the true positives. Between these two methods they find for true positives that the latent shift counterfactuals show a significant increase in confidence which is good.\n\n> 0.15\u00b10.95 confidence increase using the Latent Shift method  (p=0.01).\n\nFor false positives they find an increase in confidence but it is not significant. \n\n> 0.04\u00b11.06 increase which is not significant (p=0.57)\n\n**Conclusions:**\n - Latent Shift's ability to generate counterfactuals is pretty good!\n - Vanilla autoencoders are sufficient for some pathologies.\n - StyleGAN and higher quality models should improve performance.\n - IoU analysis may not be the best fit.\n - Explainable AI methods can have an impact on the user confidence in the model.\n\n(Disclaimer: I am the author of this work)\n\nProject Website: https://mlmed.org/gifsplanation/\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2102.09475"
    },
    "946": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1810.02334",
        "transcript": "What is stopping us from applying meta-learning to new tasks? Where do the tasks come from? Designing task distribution is laborious. We should automatically learn tasks!\n\nUnsupervised Learning via Meta-Learning: The idea is to use a distance metric in an out-of-the-box unsupervised embedding space created by BiGAN/ALI or DeepCluster to construct tasks in an unsupervised way. If you cluster points to randomly define classes (e.g. random k-means) you can then sample tasks of 2 or 3 classes and use them to train a model.\n\nWhere does the extra information come from? The metric space used for k-means asserts specific distances. The intuition why this works is that it is useful model initialization for downstream tasks.\n\nThis summary was written with the help of Chelsea Finn.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1810.02334"
    },
    "947": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/corl/ClaveraRS0AA18",
        "transcript": "In terms of model based RL, learning dynamics models is imperfect, which often leads to the learned policy overfitting to the learned dynamics model, doing well in the learned simulator but not in the real world.\n\nKey solution idea: No need to try to learn one accurate simulator. We can learn an ensemble of models that together will sufficiently represent the space. If we learn an ensemble of models (to be used as many learned simulators) we can denoise estimates of performance. In a meta-learning sense these simulations become the tasks. The real world is then just yet another task, to which the policy could adapt quickly.  One experimental observation is that at the start of training there is a lot of variation between learned simulators, and then the simulations come together over training, which might also point to this approach providing improved exploration.\n\nThis summary was written with the help of Pieter Abbeel.",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v87/clavera18a.html"
    },
    "948": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1901.10912",
        "transcript": "How can we learn causal relationships that explain data? We can learn from non-stationary distributions. If we experiment with different factorizations of relationships between variables we can observe which ones provide better sample complexity when adapting to distributional shift and therefore are likely to be causal.\n\nIf we consider the variables A and B we can factor them in two ways:\n\n$P(A,B) = P(A)P(B|A)$ representing a causal graph like $A\\rightarrow B$\n\n$P(A,B) = P(A|B)P(B)$ representing a causal graph like $A \\leftarrow B$\n\nThe idea is if we train a model with one of these structures; when adapting to a new shifted distribution of data it will take longer to adapt if the model does not have the correct inductive bias. For example let's say that the true relationship is $A$=Raining causes $B$=Open Umbrella (and not vice-versa). Changing the marginal probability of Raining (say because the weather changed) does not change the mechanism that relates $A$ and $B$ (captured by $P(B|A)$), but will have an impact on the marginal $P(B)$. \n\nSo after this distributional shift the function that modeled $P(B|A)$ will not need to change because the relationship is the same.  Only the function that modeled $P(A)$ will need to change. Under the incorrect factorization $P(B)P(A|B)$, adaptation to the change will be slow because both $P(B)$ and $P(A|B)$ need to be modified to account for the change in $P(A)$ (due to Bayes rule).\n\nHere a difference in sample complexity can be observed when modeling the joint of the shifted distribution.  $B\\rightarrow A$ takes longer to adapt:\nhttps://i.imgur.com/B9FEmA7.png\n\nHere the idea is that sample complexity when adapting to a new distribution of data is a heuristic to inform us which causal graph inductive bias is correct.\n\nExperimentally this works and they also observe that when models have more capacity it seems that the difference between the models grows.\n\nThis summary was written with the help of Yoshua Bengio.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1901.10912"
    },
    "949": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1811.12889",
        "transcript": "The paper discusses neural module network trees (NMN-trees). Here modules are composed in a tree structure to answer a question/task and modules are trained in different configurations to ensure they learn more core concepts and can generalize.\n\nLonger summary:\n\nHow to perform systematic generalization? First we need to ask how\ngood current models are at understanding language. Adversarial\nexamples show how fragile these models can be. This leads us to\nconclude that systematic generalization is an issue that requires\nspecific attention.\n\nMaybe we should rethink the modeling assumptions being made. We can\nthink that samples can come from different data domains but are\ngenerated by some set of shared rules. If we correctly learned these\nrules then domain shift in the test data would not hurt model\nperformance. Currently we can construct an experiment to introduce\nsystematic bias in the data which causes the performance to suffer.\nFrom this experiment we can start to determine what the issue is.\n\nA recent new idea is to force a model to have more independent units\nis neural module network trees (NMN-trees). Here modules are composed\nin a tree structure to answer a question/task and modules are trained\nin different configurations to ensure they learn more core concepts\nand can generalize.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1811.12889"
    },
    "950": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1811.11347",
        "transcript": "The paper looks at approaches to predicting individual survival time distributions (isd). The motivation is shown in the figure below. Between two patients the survival time varies greatly so we should be able to predict a distribution like the red curve.\n\nhttps://i.imgur.com/2r9JvUp.png\n\nThe paper studies the following methods: \n - class-based survival curves Kaplan-Meier [31]\n - Kalbfleisch-Prentice extension of the Cox (cox-kp) [29]\n - Accelerated Failure Time (aft) model [29]\n - Random Survival Forest model with Kaplan-Meier extensions (rsf-km)\n - elastic net Cox (coxen-kp) [55] \n - Multi-task Logistic Regression (mtlr) [57]\n\nLooking at the predictions of these methods side by side we can observe some systematic differences between the methods:\nhttps://i.imgur.com/vJoCL4a.png\n\nThe paper presents a \"D-Calibration\" metric (distributional calibration) which represents of the method answers this question:\n\n    Should the patient believe the predictions implied by the survival curve?\n\n\nhttps://i.imgur.com/MX8CbZ7.png\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1811.11347"
    },
    "951": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/naacl/JagannathaY16",
        "transcript": "The basic approach is an RNN applied to text to predict a medical event such as an ICD code. It is unclear if the complicated Bi-RNN model is required. \n\nThis has some useful applications such as \n- Adapt old databases\n- Correct errors\n- Upgrade ICD versions\n\nA simple diagram of an RNN applied to medical next is shown below:\n\n\nhttps://i.imgur.com/NPExLqH.png",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/N/N16/N16-1056.pdf"
    },
    "952": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1602.05568",
        "transcript": "This model called Med2Vec is inspired by Word2Vec. It is Word2Vec for time series patient visits with ICD codes. The model learns embeddings for medical codes as well as the demographics of patients.\n\nhttps://i.imgur.com/Zjj6Xxz.png\n\nThe context is temporal. For each $x_t$ as input the model predicts $x_{t+1}$ and $x_{t-1}$ or more depending on the temporal window size. ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1602.05568"
    },
    "953": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.00400",
        "transcript": "This paper demonstrates that Word2Vec \\cite{1301.3781} can extract relationships between words and produce latent representations useful for medical data. They explore this model on different datasets which yield different relationships between words.\n\nhttps://i.imgur.com/hSA61Zw.png\n\nThe Word2Vec model works like an autoencoder that predicts the context of a word. The context of a word is composed of the surrounding words as shown below. Given the word in the center the neighboring words are predicted through a bottleneck in the autoencoder. A word has many contexts in a corpus so the model can never have 0 error. The model must minimize the reconstruction which is how it learns the latent representation.\n\nhttps://i.imgur.com/EMtjTHn.png\n\nSubjectively we can observe the relationship between word vectors:\n\nhttps://i.imgur.com/8C9EVq1.png\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.00400v2"
    },
    "954": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/NiculaeB17",
        "transcript": "The idea in this paper is to develop a version of attention that will incorporate similarity in neighboring bins. This aligned with the work \\cite{conf/icml/BeckhamP17} which presented a different approach to deal with consistency between classes of predictions.\n\nIn this work the closed form softmax function is replaced by a small optimization problem with this regularizer:\n\n$$ +\\lambda \\sum_{i=1}^{d-1} |y_{i+1}-y_i|$$\n\nBecause of this, many of the neighboring probabilities are exactly the same resulting in attention that can be seen as blocks.\n\nhttps://i.imgur.com/oue0x4V.png\n\nPoster:\nhttps://i.imgur.com/gclMjzR.png",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention"
    },
    "955": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1612.09346",
        "transcript": "This work deals with rotation equivariant convolutional filters. The idea is that when you rotate an image you should not need to relearn new filters to deal with this rotation. First we can look at how convolutions typically handle rotation and how we would expect a rotation invariant solution to perform below:\n\n| | |\n| - | - |\n| https://i.imgur.com/cirTi4S.png | https://i.imgur.com/iGpUZDC.png |\n| | | |\n\nThe method computes all possible rotations of the filter which results in a list of activations where each element represents a different rotation. From this list the maximum is taken which results in a two dimensional output for every pixel (rotation, magnitude). This happens at the pixel level so the result is a vector field over the image.\n\n\nhttps://i.imgur.com/BcnuI1d.png\n\nWe can visualize their degree selection method with a figure from https://arxiv.org/abs/1603.04392 which determined the rotation of a building:\n\nhttps://i.imgur.com/hPI8J6y.png\n\nWe can also think of this approach as attention \\cite{1409.0473} where they attend over the possible rotations to obtain a score for each possible rotation value to pass on. The network can learn to adjust the rotation value to be whatever value the later layers will need. \n\n------------------------\n\nResults on [Rotated MNIST](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/MnistVariations) show an impressive improvement in training speed and generalization error:\n\n\n\n\nhttps://i.imgur.com/YO3poOO.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1612.09346"
    },
    "956": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1704.07820",
        "transcript": "In this work they take a different approach to the GAN model \\cite{1406.2661}. In the traditionally GAN model a neural network is trained to up-sample from random noise in a feed forward fashion to generate samples from the data distribution. \n\nThis work instead iteratively permutes an image of random noise similar to Artistic Style Transfer \\cite{1508.06576}.  The image is permuted in order to fool a set of discriminators. To obtain the set of discriminators each is trained starting from random noise until some max $t$ step. \n\n\n1. At first a discriminator is trained to discriminate between the true data and random noise . \n2. Images is then permuted using gradients which aim to fool the discriminator and included in the data distribution as a negative example.\n3. The discriminator is trained on the true data + random noise + fake data from the previous steps\n\nThe images generated at each step are shown below:\n\nhttps://i.imgur.com/kp575s8.png\n\nAfter being trained the model is able to generate a sample by iterating over each trained discriminator and applying gradient updates on from random noise. For this storing only the weights of the discriminators is required.\n\nPoster from ICCV2017:\nhttps://i.imgur.com/vYSSdZx.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1704.07820"
    },
    "957": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/bioinformatics/ChenLNSX16",
        "transcript": "\"This deals with a specific prediction task, namely to predict the expression of specified target genes from a panel of about 1,000 pre-selected \u201clandmark genes\u201d. As the authors explain, gene expression levels are often highly correlated and it may be a cost-effective strategy in some cases to use such panels and then computationally infer the expression of other genes. Based on Pylearn2/Theano.\"\n\nhttps://github.com/uci-cbcl/D-GEX\n\nhttps://followthedata.wordpress.com/2015/12/21/list-of-deep-learning-implementations-in-biology/\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1093/bioinformatics/btw074"
    },
    "958": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.06480",
        "transcript": "In this paper they prior the representation a logistic regression model using known protein-protein interactions. They do so by regularizing the weights of the model using the Laplacian encoding of a graph. \n\nHere is a regularization term of this form:\n\n$$\\lambda ||w||_1 + \\eta w^T L w,$$\n\n#### A small example:\n\nGiven a small graph of three nodes A, B, and C with one edge: {A-B} we have the following Laplacian:\n\n$$\nL = D - A = \n\\left[\\array{\n1 & 0 & 0 \\\\\n0 & 1 & 0\\\\\n0 & 0 & 0}\\right]\n-\n\\left[\\array{\n0 & 1 & 0 \\\\\n1 & 0 & 0\\\\\n0 & 0 & 0}\\right]$$\n\n$$L = \n\\left[\\array{\n1 & -1 & 0 \\\\\n-1 & 1 & 0\\\\\n0 & 0 & 0}\\right]\n$$\n\nIf we have a small linear regression of the form:\n\n$$y = x_Aw_A + x_Bw_B + x_Cw_C$$\n\nThen we can look at how $w^TLw$ will impact the weights to gain insight:\n\n$$w^TLw $$\n\n$$=\n\\left[\\array{\nw_A &\nw_B &\nw_C}\\right]\n\\left[\\array{\n1 & -1 & 0 \\\\\n-1 & 1 & 0\\\\\n0 & 0 & 0}\\right]\n\\left[\\array{\nw_A \\\\\nw_B \\\\\nw_C}\\right] \n$$\n\n$$= \n\\left[\\array{\nw_A &\nw_B &\nw_C}\\right]\n\\left[\\array{\nw_A -w_B \\\\\n-w_A + w_B \\\\\n0}\\right] \n$$\n\n\n\n$$\n= \n(w_A^2 -w_Aw_B ) + \n(-w_Aw_B + w_B^2)\n$$\n\nSo because all terms are squared we can remove them from consideration to look at what is the real impact of regularization.\n\n$$\n= \n(-w_Aw_B ) + \n(-w_Aw_B)\n$$\n\n$$ = -2w_Aw_B$$\n\nThe Laplacian regularization seems to increase the weight values of edges which are connected. Along with the squared terms and the $L1$ penalty that is also used the weights cannot grow without bound.\n\n#### A few more experiments:\n\nIf we perform the same computation for a graph with two edges: {A-B, B-C} we have the following term which increases the weights of both pairwise interactions:\n\n$$ = -2w_Aw_B -2w_Bw_C$$\n\nIf we perform the same computation for a graph with two edges: {A-B, A-C} we have no surprises: \n\n$$ = -2w_Aw_B -2w_Aw_C$$\n\nAnother thing to think about is if there are no edges. If by default there are self-loops then the degree matrix will have 1 on the diagonal and it will be the identity which will be an $L2$ term. If no self loops are defined then the result is a 0 matrix yielding no regularization at all.\n\n#### Contribution:\n\nA contribution of this paper is to use the absolute value of the weights to make training easier. \n\n$$|w|^T L |w|$$\n\nTODO: Add more about how this impacts learning.\n\n\n\n#### Overview\n\nHere a high level figure shows the data and targets together with a graph prior. It looks nice so I wanted to include it.\n\nhttps://i.imgur.com/rnGtHqe.png\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1609.06480v1"
    },
    "959": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/miccai/ChenBMRPK17",
        "transcript": "This work aims to produce more spatially consistent MRI image when the patient is breathing during MRI acquisition. \n\nhttps://i.imgur.com/wWMQa1D.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-319-66185-8_31"
    },
    "960": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/miccai/ZhangYCFHC17",
        "transcript": "This work improves the performance of a segmentation network by utilizing unlabelled data. They use a discriminator (they call EN) to distinguish between annotated and unannotated examples. They then train the segmentation generator (they call SN) based on what will fool the discriminator. \n\nhttps://i.imgur.com/7CfKnh5.png\n\nThree training phases are shown above\n\nThis work is really great. They are using the segmentation to condition the discriminator which will learn to point out flaws when applying the segmentation to the unlabelled examples. Then these flaws in the segmentation are corrected by using the gradients from the discriminator to adjust the segmentation.\n\nIn contrast with other semi-supervised approaches which learn a latent space for all samples, labelled and unlabelled, and then uses this space to learn a classifier or segmentation; this approach looks for the boundaries of the space only. The unlabelled examples are used to bias the representation learned by the segmentation network to conform to the distribution represented by all observed examples.\n\nRead this paper for more: https://arxiv.org/abs/1611.08408\n\nPoster:\nhttps://i.imgur.com/eR5jgwn.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-319-66179-7_47"
    },
    "961": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1709.01507",
        "transcript": "\"The SE module can learn some nonlinear global interactions already known to be useful, such as spatial normalization. The channel wise weights make it somewhat more powerful than divisive normalization as it can learn feature-specific inhibitions (ie: if we see a lot of flower parts, the probability of boat features should be diminished). It also has some similarity to bio inhibitory circuits.\" By jcannell on reddit\n\nSlides: http://image-net.org/challenges/talks_2017/SENet.pdf\n\nSummary by the author Jie Hu:\n\nOur motivation is to explicitly model the interdependence between feature channels. In addition, we do not intend to introduce a new spatial dimension for the integration of feature channels, but rather a new \"feature re-calibration\" strategy. Specifically, it is through learning the way to automatically obtain the importance of each feature channel, and then in accordance with this importance to enhance the useful features and inhibit the current task is not useful features.\n\nhttps://i.imgur.com/vXyBg4j.png\n\nThe above figure is a schematic diagram of our proposed SE module. Given an input $x$, the number of characteristic channels is $c_1$, and the characteristic number of a characteristic channel is $c_2$ by a series of convolution and other general transformations. Unlike traditional CNNs, we then re-calibrate the features we received in the next three operations.\n\nThe first is the Squeeze operation, we carry out the feature compression along the spatial dimension, and turn each two-dimensional feature channel into a real number. The real number has a global sense of the wild, and the output dimension and the number of input channels Match. It characterizes the global distribution of responses on the feature channel, and makes it possible to obtain a global sense of the field near the input, which is very useful in many tasks.\n\nFollowed by the Excitation operation, which is a mechanism similar to the door in a circular neural network. The weight is generated for each feature channel by the parameter $w$, where the parameter w is learned to explicitly model the correlation between the feature channels.\n\nReddit thread: https://www.reddit.com/r/MachineLearning/comments/6pt99z/r_squeezeandexcitation_networks_ilsvrc_2017/\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1709.01507"
    },
    "962": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/miccai/SongCHC17",
        "transcript": "The goal of this work is to classify histopathology images into benign and malignant.  They use the BreaKHis and IICBU 2008 lymphoma datasets.\n\nThey use a VGG network for feature extraction from each image. Then on these VGG feature vectors they learn [Fisher Vectors ](https://prateekvjoshi.com/2014/08/23/image-classification-using-fisher-vectors/) which they use to make a prediction.\n\nIt is unclear why Fisher Vectors are more useful than the fully connected layers of the VGG net that they replace. It is not clear how much analysis was performed for the VGG baseline. Also, as a baseline a VGG network should have been trained from scratch to extract domain specific features. \n\nPoster:\nhttps://i.imgur.com/fgzmeYv.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-319-66179-7_12"
    },
    "963": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/miccai/UzunovaWHE17",
        "transcript": "The authors state that the usual approach to cope with few training samples is data augmentation. They extend a method of modelling the data from \\cite{10.1016/j.media.2017.02.003} and use it to train a neural network. The figure below shows the overview:\n\nhttps://i.imgur.com/joLNyfc.png\n\nAt the core of deformation model they determine a set of $m$ landmarks $s_i$ which they will deform and then perform an affine transformation to warp the image to align to these points. The points are moved in a constrained way. They state the constraint is a \"multi-level B-spline scattered data approximation\".\n\nHere is the poster: https://i.imgur.com/enQQqxC.png\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-319-66182-7_26"
    },
    "964": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.07004",
        "transcript": "Summary by [brannondorsey](https://gist.github.com/brannondorsey/fb075aac4d5423a75f57fbf7ccc12124):\n\n- Euclidean distance between predicted and ground truth pixels is not a good method of judging similarity because it yields blurry images.\n- GANs learn a loss function rather than using an existing one.\n- GANs learn a loss that tries to classify if the output image is real or fake, while simultaneously training a generative model to minimize this loss.\n- Conditional GANs (cGANs) learn a mapping from observed image `x` and random noise vector `z` to `y`: `y = f(x, z)`\n- The generator `G` is trained to produce outputs that cannot be distinguished from \"real\" images by an adversarially trained discrimintor, `D` which is trained to do as well as possible at detecting the generator's \"fakes\".\n- The discriminator `D`, learns to classify between real and synthesized pairs. The generator learns to fool the discriminator.\n- Unlike an unconditional GAN, both the generator and discriminator observe an input image `z`.\n- Asks `G` to not only fool the discriminator but also to be near the ground truth output in an `L2` sense.\n- `L1` distance between an output of `G` is used over `L2` because it encourages less blurring.\n- Without `z`, the net could still learn a mapping from `x` to `y` but would produce deterministic outputs (and therefore fail to match any distribution other than a delta function. Past conditional GANs have acknowledged this and provided Gaussian noise `z` as an input to the generator, in addition to `x`)\n- Either vanilla encoder-decoder or Unet can be selected as the model for `G` in this implementation.\n- Both generator and discriminator use modules of the form convolution-BatchNorm-ReLu.\n- A defining feature of image-to-image translation problems is that they map a high resolution input grid to a high resolution output grid.\n- Input and output images differ in surface appearance, but both are renderings of the same underlying structure. Therefore, structure in the input is roughly aligned with structure in the output.\n- `L1` loss does very well at low frequencies (I think this means general tonal-distribution/contrast, color-blotches, etc) but fails at high frequencies (crispness/edge/detail) (thus you get blurry images). This motivates restricting the GAN discriminator to only model high frequency structure, relying on an `L1` term to force low frequency correctness. In order to model high frequencies, it is sufficient to restrict our attention to the structure in local image patches. Therefore, we design a discriminator architecture \u2013 which we term a PatchGAN \u2013 that only penalizes structure at the scale of patches. This discriminator tries to classify if each `NxN`patch in an image is real or fake. We run this discriminator convolutationally across the image, averaging all responses to provide the ultimate output of `D`.\n- Because PatchGAN assumes independence between pixels seperated by more than a patch diameter (`N`) it can be thought of as a form of texture/style loss.\n- To optimize our networks we alternate between one gradient descent step on `D`, then one step on `G` (using minibatch SGD applying the Adam solver)\n- In our experiments, we use batch size `1` for certain experiments and `4` for others, noting little difference between these two conditions.\n- __To explore the generality of conditional GANs, we test the method on a variety of tasks and datasets, including both graphics tasks, like photo generation, and vision tasks, like semantic segmentation.__\n- Evaluating the quality of synthesized images is an open and difficult problem. Traditional metrics such as per-pixel mean-squared error do not assess joint statistics of the result, and therefore do not measure the very structure that structured losses aim to capture.\n- FCN-Score: while quantitative evaluation of generative models is known to be challenging, recent works have tried using pre-trained semantic classifiers to measure the discriminability of the generated images as a pseudo-metric. The intuition is that if the generated images are realistic, classifiers trained on real images will be able to classify the synthesized image correctly as well.  \n- cGANs seems to work much better than GANs for this type of image-to-image transformation, as it seems that with a GAN, the generator collapses into producing nearly the exact same output regardless of the input photograph.\n- `16x16` PatchGAN produces sharp outputs but causes tiling artifacts, `70x70` PatchGAN alleviates these artifacts. `256x256` ImageGAN doesn't appear to improve the tiling artifacts and yields a lower FCN-score.\n- An advantage of the PatchGAN is that a fixed-size patch discriminator can be applied to arbitrarily large images. This allows us to train on, say, `256x256` images and test/sample/generate on `512x512`.\n- cGANs appear to be effective on problems where the output is highly detailed or photographic, as is common in image processing and graphics tasks.\n- When semantic segmentation is required (i.e. going from image to label) `L1` performs better than `cGAN`. We argue that for vision problems, the goal (i.e. predicting output close to ground\ntruth) may be less ambiguous than graphics tasks, and reconstruction losses like L1 are mostly sufficient.\n \n### Conclusion\n\nThe results in this paper suggest that conditional adversarial networks are a promising approach for many image-to-image translation tasks, especially those involving highly structured graphical outputs. These networks learn a loss adapted to the task and data at hand, which makes them applicable in a wide variety of settings.\n\n### Misc\n\n- Least absolute deviations (`L1`) and Least square errors (`L2`) are the two standard loss functions, that decides what function should be minimized while learning from a dataset. ([source](http://rishy.github.io/ml/2015/04/28/l1-vs-l2-loss/))\n- How, using pix2pix, do you specify a loss of `L1`, `L1+GAN`, and `L1+cGAN`?\n\n### Resources\n- [GAN paper](https://arxiv.org/pdf/1406.2661.pdf)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.07004"
    },
    "965": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZhaoSE17",
        "transcript": "A Critical Paper Review by Alex Lamb:\n\nhttps://www.youtube.com/watch?v=_seX4kZSr_8 ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1702.08396"
    },
    "966": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.00071",
        "transcript": "Here is a video overview:\nhttps://www.youtube.com/watch?v=t-fow6GJepQ\n\nHere is an image of the poster:\nhttps://i.imgur.com/Ti9btj9.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1702.00071"
    },
    "967": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/BeckhamP17",
        "transcript": "\nShort overview from ICML:\nhttps://youtube.com/watch?v=GMG5bFciuIA\n\nLong overview from ICML:\nhttps://youtu.be/o6dtDuldsEo\n",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v70/beckham17a.html"
    },
    "968": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1706.02515",
        "transcript": "\"Using the \"SELU\" activation function, you get better results than any other activation function, and you don't have to do batch normalization. The \"SELU\" activation function is:\n\nif x<0, 1.051\\*(1.673\\*e^x-1.673) if x>0, 1.051\\*x\" \n\nSource: narfon2, reddit\n\n\n```\nimport numpy as np\n\ndef selu(x):\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n    return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha)\n```\nSource: CaseOfTuesday, reddit\n\nDiscussion here: https://www.reddit.com/r/MachineLearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1706.02515"
    },
    "969": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BahdanauCB14",
        "transcript": "One core aspect of this attention approach is that it provides the ability to debug the learned representation by visualizing the softmax output (later called $\\alpha_{ij}$) over the input words for each output word as shown below.\n\nhttps://i.imgur.com/Kb7bk3e.png\n\nIn this approach each unit in the RNN they attend over the previous states, unitwise so the length can vary, and then apply a softmax and use the resulting probabilities to multiply and sum each state. This forms the memory used by each state to make a prediction. This bypasses the need for the network to encode everything in the state passed between units.\n\nEach hidden unit is computed as:\n\n$$s_i = f(s_{i\u22121}, y_{i\u22121}, c_i).$$\n\nWhere $s_{i\u22121}$ is the previous state and $y_{i\u22121}$ is the previous target word. Their contribution is $c_i$. This is the context vector which contains the memory of the input phrase.\n\n$$c_i = \\sum_{j=1} \\alpha_{ij} h_j$$\n\nHere $\\alpha_{ij}$ is the output of a softmax for the $j$th element of the input sequence. $h_j$ is the hidden state at the point the RNN was processing the input sequence.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1409.0473"
    },
    "970": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LempitskyZ10",
        "transcript": "They introduce the concept of counting in images by predicting a density map. Their training only requires dot annotations on the center of objects. Each dot is expanded to a gaussian to form a density. A model is trained to predict this density and then the total count is recovered by integrating over the resulting density map.\n\nThey create a function to produce the density based on quantized dense SIFT features \\cite{lowe03distinctive} from every pixel in the image. A simple version of the definition of $F$ is shown below. Each pixel becomes an $x_p$ vector which is used to train and model to implement the function $F$.\n\n$$\\forall p \\in I, \\hspace{10pt } F(p|w) = wx_p $$\n\nThe obtained quantized dense SIFT features using the [VLFEAT](http://www.vlfeat.org/overview/dsift.html) library. The significant part of the code is shown below:\n\n```\nim = imread(['data/' num2str(j, '%03d') 'cell.png']);\nim = im(:,:,3); %using the blue channel to compute data\n\ndisp('Computing dense SIFT...');\n[f d] = vl_dsift(single(im)); %computing the dense sift descriptors centered at each pixel\n%estimating the crop parameters where SIFTs were not computed:\nminf = floor(min(f,[],2));\nmaxf = floor(max(f,[],2));\nminx = minf(1);\nminy = minf(2);\nmaxx = maxf(1);\nmaxy = maxf(2);   \n\n%simple quantized dense SIFT, each image is encoded as MxNx1 numbers of\n%dictionary entries numbers with weight 1 (see the NIPS paper):\ndisp('Quantizing SIFTs...');\nfeatures{j} = vl_ikmeanspush(uint8(d),Dict);\nfeatures{j} = reshape(features{j}, maxy-miny+1, maxx-minx+1);\nweights{j} = ones(size(features{j}));   \n```\n\nThe benchmark their algorithm using \"Bacterial cells in fluorescence-light microscopy images\". The heatmap to the right shows the predicted density.\n\nhttps://i.imgur.com/Vz463nu.png\n\nThe evaluation is performed by training on $N$ images (with $N$ in a validation set) and the testing on 100 randomly picked images in a hold out set. They show that using more images results in less variance and higher accuracy.\n\nhttps://i.imgur.com/hihfC8V.png\n\nPaper website: http://www.robots.ox.ac.uk/~vgg/research/counting/index_org.html",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4043-learning-to-count-objects-in-images"
    },
    "971": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1409.7495",
        "transcript": "The goal of this method is to create a feature representation $f$ of an input $x$ that is domain invariant over some domain $d$. The feature vector $f$ is obtained from $x$ using an encoder network (e.g. $f = G_f(x)$). \n\nThe reason this is an issue is that the input $x$ is correlated with $d$ and this can confuse the model to extract features that capture differences in domains instead of differences in classes. Here I will recast the problem differently from in the paper:\n\n**Problem:** Given a conditional probability $p(x|d=0)$ that may be different from $p(x|d=1)$:\n\n$$p(x|d=0) \\stackrel{?}{\\ne} p(x|d=1)$$\n\nwe would like it to be the case that these distributions are equal.\n\n$$p(G_f(x) |d=0) = p(G_f(x)|d=1)$$\n\naka:\n\n$$p(f|d=0) = p(f|d=1)$$\n\nOf course this is an issue if some class label $y$ is correlated with $d$ meaning that we may hurt the performance of a classifier that now may not be able to predict $y$ as well as before.\n\nhttps://i.imgur.com/WR2ujRl.png\n\nThe paper proposes adding a domain classifier network to the feature vector using a reverse gradient layer. This layer simply flips the sign on the gradient. Here is an example in [Theano](https://github.com/Theano/Theano):\n\n```\nclass ReverseGradient(theano.gof.Op):\n    ...\n    def grad(self, input, output_gradients):\n        return [-output_gradients[0]]\n```\n\nYou then train this domain network as if you want it to correctly predict the domain (appending it's error to your loss function). As the domain network learns new ways to correctly predict an output these gradients will be flipped and the information in feature vector $f$ will be removed.\n\nThere are two major hyper parameters of the method. The number of dimensions at the bottleneck is one but it is linked to your network. The second is a scalar on the gradient so you can increase or decrease the effect of the gradient on the embedding.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1409.7495"
    },
    "972": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.09410",
        "transcript": "This combines the ideas of recurrent attention to perform object detection in an image \\cite{1406.6247} for multiple objects \\cite{1412.7755} with semantic segmentation \\cite{1505.04366}. \n\nSegmenting subregions is to avoid a global resolution bias (the object would take up the majority of pixels) and to allow multiple scales of objects to be segmented. \n\nHere is a video that demos the method described in the paper:\n\nhttps://youtu.be/BMVDhTjEfBU",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.09410"
    },
    "973": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1411.5752",
        "transcript": "So the hypervector is just a big vector created from a network:\n\n`\"We concatenate features from some or all of the feature\nmaps in the network into one long vector for every location\nwhich we call the hypercolumn at that location. As an\nexample, using pool2 (256 channels), conv4 (384 channels)\nand fc7 (4096 channels) from the architecture of [28] would\nlead to a 4736 dimensional vector.\"`\n\nSo how exactly do we construct the vector? \n\n![](https://i.imgur.com/hDvHRwT.png)\n\nEach activation map results in a single element of the resulting hypervector. The corresponding pixel location in each activation map is used as if the activation maps were all scaled to the size of the original image.\n\nThe paper shows the below formula for the calculation. Here $\\mathbf{f}_i$ is the value of the pixel in the scaled space and each $\\mathbf{F}_{k}$ are points in the activation map. $\\alpha_{ik}$ scales the known values to produce the midway points.\n\n$$\\mathbf{f}_i  = \\sum_k \\alpha_{ik} \\mathbf{F}_{k}$$\n\nThen the fully connected layers are simply appended to complete the vector. \n\nSo this gives us a representation for each pixel but is it a good one? The later layers will have the input pixel in their receptive field. After the first few layers it is expected that the spatial constraint is not strong.\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1411.5752"
    },
    "974": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/SeguiPV15",
        "transcript": "This paper discusses some amazing results. The goal is to learn how to count by end-to-end training. The network input is an image and the output is a count of the objects inside it. They do not perform any direct training using the locations of the objects in the image. \n\nThe reason for avoiding direct training is that labeled data is expensive. Employing a surrogate objective ,such as the count of items in the image, is much cheaper and makes more sense because it is the goal of the system we want to learn. This paper states that it is possible! The discuss experiments on two datasets; one of MNIST digits placed in an image and one with the UCSD Pedestrian Database.  \n\nThe network description seems to be general and they don't report any special constraints on the design  `\"We consider networks of two or more convolutional layers followed by one or more fully connected layers. Each convolutional layer consist of several elements: a set of convolutional filters, ReLU non-linearities, max pooling layers and normalization layers.\"` and `\"We use a five layers architecture CNN with two convolutional layers followed by three fully connected layers\"`. They provide these two tables for their designs:\n\n$$\\begin{array}{c|c|c|c}\n Conv1 &  Conv2 & FC1 & FC2  \\\\ \\hline\n10\\text{x}15\\text{x}15 & 10\\text{x}3\\text{x}3 & 32 & 6 \\\\\n\\text{x2 pool} & \\text{x2 pool} & & \\\\ \\hline\n\\end{array}\\\\\n\\text{CNN arch for numbers}$$\n\n$$\n\\begin{array}{c|c|c|c|c}\n Conv1 &  Conv2 & FC1 & FC2 & FC3 \\\\ \\hline\n8\\text{x}9\\text{x}9 & 8\\text{x}5\\text{x}5 & 128 & 128 & 25 \\\\\n\\text{x2 pool} & \\text{x2 pool} & & \\\\ \\hline\n\\end{array}\\\\\n\\text{CNN arch for people}$$\n\nThey state that they use a method based on hypercolumns \\cite{1411.5752} but the description is not clear at all: `\" Starting with the hypercolumn representation\non the last layer we cluster the resulting hypercolumns\ninto a set of prototypes using an online k-means\nalgorithm. Then, a MIL approach with positive and negative\ninstances with the concept of interest is used.\"`\n\n![](https://i.imgur.com/x2q3E9Y.png)\n\nInteresting work but I wish it was a longer paper with more details. This paper doesn't really give me enough information to reproduce it.\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPRW.2015.7301276"
    },
    "975": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/mm/CastanonCZS15",
        "transcript": "This paper poses the the problem of querying a large corpus of aerial video as a subgraph matching problem. Here the video data has been transformed into a large graph where each frame contains labeled objects such as person, object, or car which become nodes and then the edges are relationships such as time (between sequential video frames) and distance (in current frame and in future frames). \n\nThe reason the graph is built is to we can query it with graphs that represent what we are looking for. The first example in the paper (below) shows an example query (called $Q$). This query asks to \"Find a person near an object then after some time or distance they are still near and there is a car\". \n\n![](http://i.imgur.com/6AKVCYX.png)\n\nThe goal now is to find the most similar subgraphs in the larger graph. The game here is to reduce the complexity of the search into something that is not as bad as the subgraph isomorphism problem. Even though this is worse because we what things that are similar and not necessarily exact to the query.\n\nThey filter the larger graph (that represents the video) into a smaller graph that only includes nodes and edges that can match those in the query graph (This graph is called the coarse graph $C$). Another is to filter the query $Q$ into a smaller graph $T$ which retains nodes and edges that have the most discriminative power.\n\n### WORK IN PROGRESS\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2733373.2806229"
    },
    "976": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1001/jama.2016.9797",
        "transcript": "This paper discusses the impact of the Affordable Care Act (ACA) on the United States. First there is discussion regarding what led up to health care reform followed by the impacts of the bill and the current trends. \n\nOne motivation was that the \"US system left more than 1 in 7 Americans without health insurance coverage in 2008.\" Also, there was an upward trend in how much the economy was spending on healthcare. \n\nThe first results are shown in Figure 1. The number of uninsured dropped after the ACA. Maybe this is the most significant take away but it doesn't capture the quality and cost of care which are also addressed in the paper.\n\n![](https://i.imgur.com/qrQ7nU9.png)\n\nThe paper states \"Before the ACA, the health care system was dominated by 'fee-for-service' payment systems, which often penalized health care organizations and health care professionals who find ways to deliver care more efficiently, while failing to reward those who improve the quality of care. The ACA has changed the health care payment system in several important ways.\" \n\nThe ACA modified payments for medicare services and introduced a \"value-based payment\" system with the goal of reducing overall cost. This is shown in Figure 4. What the plot is showing is the change over a time period. Between 2000-2005 the costs per payer were increasing in all programs. Between 2005-2010 medicaid cost to the person was decreasing and the other programs were still growing but growing slower. Between 2010-2014 the cost to both medicare and medicaid are both in decline. Private insurance costs are still increasing but they are increasing slower than before.\n\n![](https://i.imgur.com/knBTQxK.png)\n\nAnother interesting take away is that \"[t]he rate of hospital-acquired conditions (such as adverse drug events, infections, and pressure ulcers) has declined by 17%.\" This is reflected in the 30-day readmission rates dropping shown in Figure 6.\n\n![](https://i.imgur.com/SmcxfoB.png)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1001/jama.2016.9797"
    },
    "977": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=Mathe_2016_CVPR",
        "transcript": "![](http://i.imgur.com/tIX6HQB.jpg)\n\nThe goal of this paper is to find a specific object in an image. Initially a region proposal algorithm is used to identify candidate regions containing objects. The goal is to avoid processing all of these candidates. The idea here is to use RL to identify the neighboring candidates that should be used as a base to transform to get the next coordinates. \n\nStarting from the center, all candidates windows that are overlapped by a radius around the center are evaluated with the RL policy $\\pi$. The state input to the $\\pi$ function is a combination of the features extracted from a CNN as well as values to track the state of the search such as how many candidates have been evaluated. The candidate that is selected has it's features extracted and these features are then transformed into coordinates of where to look next. Then the processing is repeated for that next point until a proper classification is made or the algorithm decides to stop.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Mathe_Reinforcement_Learning_for_CVPR_2016_paper.pdf"
    },
    "978": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=chen2016scaleaware",
        "transcript": "They represent an image as a tree where leafs are pixels and nodes represent clusters of those pixels. They train by regressing for some possible segmented region $r$ on the following function for every segmentation example and ground truth:\n$$S(r)=\\frac{\\\\#(g) - \\\\#(r)}{\\max(\\\\#(r), \\\\#(g)))}$$\n\nHere $\\\\#(g)$ is the number of pixels in the ground truth and $\\\\#(r)$ is the number of pixels in the example segmentation. What is not explained here is what other information is used because it cannot simple be pixel counts. This function is used to rank the nodes in every path from the root to the leafs in Figure (a). \n\nThe idea for the segmentation is that there is some set of nodes such that you can draw a line shown in Figure (b) which is equivalent to selecting a segmentation. The paper goes on to compute this using a dynamic programming solution based on the fact that the same pixel segmentations will be considered multiple times.\n\n![](http://i.imgur.com/FEky9dK.png)\n\nI think the idea is great but the initial idea for the regression is unclear.",
        "sourceType": "blog",
        "linkToPaper": "http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Chen_Scale-Aware_Alignment_of_CVPR_2016_paper.pdf"
    },
    "979": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LeeS00",
        "transcript": "We want to find two matrices $W$ and $H$ such that $V = WH$. Often a goal is to determine underlying patterns in the relationships between the concepts represented by each row and column. $W$ is some $m$ by $n$ matrix and we want the inner dimension of the factorization to be $r$. So \n\n$$\\underbrace{V}_{m \\times n} = \\underbrace{W}_{m \\times r} \\underbrace{H}_{r \\times n}$$\n\nLet's consider an example matrix where of three customers (as rows) are associated with three movies (the columns) by a rating value.\n\n$$\nV = \\left[\\begin{array}{c c c}\n5 & 4 & 1  \\\\\\\\\n4 & 5 & 1 \\\\\\\\\n2 & 1 & 5\n\\end{array}\\right]\n$$\n\n\nWe can decompose this into two matrices with $r = 1$. First lets do this without any non-negative constraint using an SVD reshaping matrices based on removing eigenvalues:\n\n\n$$\nW = \\left[\\begin{array}{c c c}\n-0.656 \\\\\\\n -0.652 \\\\\\\n -0.379\n\\end{array}\\right],\nH = \\left[\\begin{array}{c c c}\n-6.48 & -6.26 & -3.20\\\\\\\\\n\\end{array}\\right]\n$$\n\nWe can also decompose this into two matrices with $r = 1$ subject to the constraint that $w_{ij} \\ge 0$ and  $h_{ij} \\ge 0$. (Note: this is only possible when $v_{ij} \\ge 0$):\n\n$$\nW = \\left[\\begin{array}{c c c}\n0.388 \\\\\\\\\n0.386 \\\\\\\\\n0.224\n\\end{array}\\right],\nH = \\left[\\begin{array}{c c c}\n11.22 & 10.57 & 5.41  \\\\\\\\\n\\end{array}\\right]\n$$\n\nBoth of these $r=1$ factorizations reconstruct matrix $V$ with the same error. \n\n$$\nV \\approx WH = \\left[\\begin{array}{c c c}\n4.36 & 4.11 & 2.10 \\\\\\\n4.33 & 4.08 & 2.09 \\\\\\\n2.52 & 2.37 & 1.21 \\\\\\\n\\end{array}\\right]\n$$\n\n\nIf they both yield the same reconstruction error then why is a non-negativity constraint useful? We can see above that it is easy to observe patterns in both factorizations such as similar customers and similar movies. `TODO: motivate why NMF is better`\n\n\n\n#### Paper Contribution \n\nThis paper discusses two approaches for iteratively creating a non-negative $W$ and $H$ based on random initial matrices. The paper discusses a multiplicative update rule where the elements of $W$ and $H$ are iteratively transformed by scaling each value such that error is not increased. \n\nThe multiplicative approach is discussed in contrast to an additive gradient decent based approach where small corrections are iteratively applied. The multiplicative approach can be reduced to this by setting the learning rate ($\\eta$) to a ratio that represents the magnitude of the element in $H$ to the scaling factor of $W$ on $H$.\n\n\n\n### Still a draft\n\n\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization"
    },
    "980": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MarietS15a",
        "transcript": "The goal is to compress a neural network based on figuring out the most significant neurons. They sample from Determinantal Point Process (DPP) in order to find set of neurons that have the most dissimilar activations and then project remaining neurons to them in order to reduce number of neurons overall.\n\nDPPs compute the probability of volume of dissimilarity over volume of all neurons:\n\n$$P(\\text{subset } Y) = \\frac{det(L_Y)}{det(L+I)}$$ \n\nMore dissimilarity means higher probability. A simple sample of the neurons outputs are taken given the training set.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05077"
    },
    "981": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jmlr/GlorotB10",
        "transcript": "The weights at each layer $W$ are initialized based on the number of connections they have. Each $w \\in W$  is drawn from a Gaussian distribution with mean $\\mu = 0$ with the variance as follows. \n\n$$\\text{Var}(W) = \\frac{2}{n_\\text{in}+ n_\\text{out}}$$\n\nWhere $n_\\text{in}$ is the number of neurons in the previous layer from the feedforward direction and $n_\\text{out}$ is the number of neurons from the previous layer from the backprop direction.\nReference: [Andy Jones's Blog](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)",
        "sourceType": "blog",
        "linkToPaper": "http://www.jmlr.org/proceedings/papers/v9/glorot10a.html"
    },
    "982": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/cacm/Shamir79",
        "transcript": "This paper defines a scheme to share a secret message with a complete group of people. It requires the group of $k$ people, no less, to combine their secret keys in order to obtain the shared secret. The secret shared is contained in the $a_0, .. a_{k-1}$ coefficients of a polynomial:\n\n$$f(x)=a_0+a_1x+a_2x^2+\\cdots+a_{k-1}x^{k-1}$$\n\nThere is a property of defining polynomials such that 2 points are sufficient to define a line, 3 points are sufficient to define a parabola, 4 points to define a cubic curve, etc. It takes $k$ points to define a polynomial of degree $k-1$\n\nYou can then give out $k$ pairs of input $x$ and output $f(x)$ examples. Given $k$ unique examples of an input $x$ and an output $f(x)$ you can determine what the coefficients were. But only having $k-1$ examples leaves a free variable and without added information it is impossible to know the coefficients. This means all $k$ people must provide their examples in order to determine the secret!",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/359168.359176"
    },
    "983": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=sammon1969mapping",
        "transcript": "This paper presents what is known as `Sammon's mapping`. This method produces points in any $\\mathbb{R}^n$ space using only a distance function between points. You can define any distance function $d^*$ that represents relationships between points. This function can even be non-symmetric. The power is that any relationship encoded into a distance function or distance matrix can be visualized.\n\nFor mapping $n$ points from some dimension in another the algorithm starts by generating $n$ random points in the space (called d-space) that you would like to map the points to. You can just pick these at random because they will be moved later. \n\nThe algorithm then performs gradient decent to minimize the *Sammon's stress* which can also be called the objective function.\n\n$$\n\\text{Sammon's stress} = \\frac{1}{\n\\sum\\limits_{i<j} d^{*}_{ij}} \n\\sum_{i<j}\n\\frac{ ( d^{*}_{ij}-d_{ij})^2}\n{d^{*}_{ij}}\n$$\n\nTo minimize this objective function a partial derivative is taken with respect to each dimension of each point in d-space. For each dimension $y$ the distance between points $p$ and $q$ are modified using a scaled partial derivative and a learning rate. The paper calls this a \"magic factor\" MF but it is referred to today as a learning rate $\\lambda$.\n\n$$y_{pq}' = y_{pq}-\\lambda \\Delta_{pq}$$\n\nThe partial is scaled by the second derivative:\n\n$$\\Delta_{pq}=\\left.\\frac{\\partial E}{\\partial y_{pq}} \\middle/  \\frac{\\partial^2 E}{\\partial y_{pq}^2}\\right.$$\n\nUsing the second derivative might be overkill for this. The objective function should also be minimized using only the first derivative. Possibly using new update rules for stochastic optimization like \\cite{conf/colt/DuchiHS10} or \\cite{journals/corr/KingmaB14} may be more efficient.\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/T-C.1969.222678"
    },
    "984": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/nn/HornikSW89",
        "transcript": "This paper discusses the universal approximation theorem which states: There is a single hidden layer feedforward network that approximates any measurable function to any desired degree of accuracy.\n\nFor any unknown function $f(x)$ there exists a single hidden layer feedforward network $F(x)$ such that $  | F( x ) - f ( x ) | < \\epsilon$ for some number of hidden units. \n\n$F(x)$ takes the following form where $h$ is some nonlinear activation function (relu, tanh, sigmoid). $w_i$ is a vector and $b_i$ and $v_i$ are scalars.\n\n$$  F( x ) =\n  \\sum_{i=1}^{N} v_i h( w_i x + b_i)$$\n\n\nResources: \n\nhttp://deeplearning.cs.cmu.edu/notes/Sonia_Hornik.pdf\n\nhttp://neuralnetworksanddeeplearning.com/chap4.html\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/0893-6080(89)"
    },
    "985": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/ai/KohaviJ97",
        "transcript": "Feature subset selection can be categorized into embedded approaches, filter approaches, and wrapper approaches. This paper presents the wrapper subset selection problem and some algorithms to obtain good subsets. Wrapper subset selection methods are black-box optimization techniques. \n\nFirst let's look at what the wrapper search space looks like in the figure below. We want to find a subset of features which maximize the performance of our classification model so each node in the graph is a subset of all the features. For a set of $n$ features there are $2^n$ unique subsets. A wrapper method approaches the problem by only looking at the graph structure and optionally evaluating each node during a search to determine how well it performs. The edges in the graph represent adding and removing one feature from the subset.\n\n![](http://i.imgur.com/is9WLJ9.png)\n\nKohavi presents two search algorithms hill-climbing and best-first search. Hill-climbing (aka greedy) evaluates all neighbor nodes and picks the best one to start searching from next. Best-first evaluates $k$ neighbors and then picks the best one. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/S0004-3702(97)"
    },
    "986": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jmlr/GuyonE03",
        "transcript": "\"The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data.\"\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.jmlr.org/papers/v3/guyon03a.html"
    },
    "987": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jmlr/Reunanen03",
        "transcript": "This paper discusses an important bias in evaluation of methods using cross-validation. A method that makes decisions based of cross validation can appear to increase overall performance by simply dealing with the bias of cross-validation and not the real problem.",
        "sourceType": "blog",
        "linkToPaper": "http://www.jmlr.org/papers/v3/reunanen03a.html"
    },
    "988": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=freund1999",
        "transcript": "This extends perceptrons \\cite{books/daglib/0066902} but uses what is known as the Hinge loss (aka SVM loss):\n\n$$J_i(w) = max(0,\\gamma -y\\_i f(x\\_i))$$\n\nWhere $\\gamma$ is the margin. $J_i(w)$ is the error given some weight $w$ parameters. $x_i$ and $y_i$ are a training example and correct label. $f(x_i)$ is the perceptron function we are trying learn the best weights for.",
        "sourceType": "blog",
        "linkToPaper": "https://cseweb.ucsd.edu/~yfreund/papers/LargeMarginsUsingPerceptron.pdf"
    },
    "989": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=books/daglib/0066902",
        "transcript": "### Perceptron Classification\n\nThe function of the perceptron takes this form for some weight vector $\\vec{w}$ and bias scalar $b$. Given some input $x$ it will produce a binary prediction.\n\n$$ f(x) = \\left\\{ \\begin{matrix} \n1 & \\text{if } (\\vec{w} \\cdot \\vec{x} + b > 0)  \\\\\n-1 & otherwise  \\\\\n\\end{matrix}\\right. $$\n\n### Perceptron Learning\n\nThe values $w$ and $b$ for this function are learned from the sample data by minimizing the misclassification error of predictions. Our sample data is in the form $(x\\_i,y\\_i)$ where $y\\_i$ the correct label (1 or -1). If the output of $f(x\\_i)$ is equal to $y\\_i$ then multiplying $-y\\_i f(x\\_i)$ will be 1 or -1. If it is incorrect it will be 1. So we can take the $max$ of 0 and this product and then sum them all to get how bad $w$ and $b$ are! $J_i(w,b)$ is the error for that one example. We can sum these together to get the error over all samples.\n\n$$J_i(w,b) = max(0,-y\\_i f(x\\_i))$$\n\n$$J(w,b) = \\frac{1}{N} \\displaystyle\\sum\\_{i=1}^N max(0,-y\\_i f(x\\_i))$$\n\nTo apply Gradient Decent to this problem we calculate the gradient of $J_i(w,b)$ with respect to each $w\\_j \\in w$ so we can know how to adjust it to minimize $J_i(w,b)$ Because we have a $max$ this gradient is annoying and has a split.\n\n$$ \\frac{\\partial J_i}{\\partial w_j}= \n\\left\\{ \\begin{matrix} \n0 & \\text{if } (\\vec{w} \\cdot \\vec{x} + b > 0)  \\\\\ny\\_ix\\_{ij} & otherwise  \\\\\n\\end{matrix}\\right. $$\n\nThis gradient $\\frac{\\partial J_i}{\\partial w_j}$ is then used to adjust $w_j$. By subtracting $\\frac{\\partial J_i}{\\partial w_j}$ from $w_j$ it will adjust the output of $f(x_i)$ such that the error $J_i(w,b)$ is reduced. Generally, subtracting the full gradient will not result in the minimal error. So a fraction of the gradient is subtracted $\\lambda$ normally at a rate of $0.05$ but this term is still a point of debate and generally is set by experience.\n",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "990": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/ipmi/ShenZYYT15",
        "transcript": "They apply a CNN to detect nodules in a 2D section of a CT scan. The network has three input images at different scales. The networks are joined at a *feature layer* before a final output layer.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-319-19992-4_46"
    },
    "991": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/Girshick15",
        "transcript": "This method is based on improving the speed of R-CNN \\cite{conf/cvpr/GirshickDDM14}\n\n1. Where R-CNN would have two different objective functions, Fast R-CNN combines localization and classification losses into a \"multi-task loss\" in order to speed up training.\n2. It also uses a pooling method based on \\cite{journals/pami/HeZR015} called the RoI pooling layer that scales the input so the images don't have to be scaled before being set an an input image to the CNN. \"RoI max pooling works by dividing the $h \\times w$ RoI window into an $H \\times W$ grid of sub-windows of approximate size $h/H \\times w/W$ and then max-pooling the values in each sub-window into the corresponding output grid cell.\"\n3. Backprop is performed for the RoI pooling layer by taking the argmax of the incoming gradients that overlap the incoming values.\n\nThis method is further improved by the paper \"Faster R-CNN\" \\cite{conf/nips/RenHGS15}",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ICCV.2015.169"
    },
    "992": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/xsede/CohenL14",
        "transcript": "Academic Torrents is a BitTorrent service that aims to make it easy for academics to share data via BitTorrent. Specific use cases are during competitions where everyone needs access to data quickly. Also, when a dataset is not available anymore the data can be shared from simple desktop computers and become available globally.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2616498.2616528"
    },
    "993": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/GirshickDDM14",
        "transcript": "The R-CNN method is a way to localize objects in an image. It is restricted to finding one of each object in an image. \n\n1. Regions are generated based on any method including brute force sliding window.\n2. Each region is classified using AlexNet.\n3. The classifications for each label are searched to find the location which expresses that label the most.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/CVPR.2014.81"
    },
    "994": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/spe/Yang91",
        "transcript": "This is a dynamic programming algorithm known as *Simple Tree Matching*:\n\n```\nint simple_tree_match(a,b){\n\n    if (a != b) return 0\n\n    m = the number of first-level sub-trees of a\n    n = the number of first-level sub-trees of b\n    M[i,0] := 0 for i = 0,...,m\n    M[0,j] := 0 for j = 0,...,n\n    for(i := 1 to m){\n        for(i := 1 to n){\n            x := M[i,j-1]\n            y := M[i-1,j]\n            z := M[i-1,j-1]+ simple_tree_match(a_i,b_j)\n            M[i,j] = max(x,y,z)\n        }\n    }\n}\nreturn M[m,n] + 1\n}\n```\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1002/spe.4380210706"
    },
    "995": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/ecoi/StepinskiC14",
        "transcript": "This paper talks about how to compare National Land Cover Database data which can be represented as histograms. The challenge is scaling the computations. The question this paper asks is if a semantically aware histogram comparison is worth the extra computation. It turns out that is does not appear worth it but interesting findings are discussed. \n\n![](http://i.imgur.com/5mKnkQw.png)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/j.ecoinf.2014.07.005"
    },
    "996": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SzegedyLJSRAEVR14",
        "transcript": "This paper introduces the GoogLeNet Inception Architecture The major part of this paper is the *Inception Module* which takes convolutions at multiple layers and provides a good receptive field as well as reducing the overall number of parameters.\n\n![Inception Module](http://i.imgur.com/CfmUmUB.png)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1409.4842"
    },
    "997": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeZRS15",
        "transcript": "This summary is as ridiculous as this network is long. A good implementation of the network is here: https://github.com/dmlc/mxnet/blob/master/example/image-classification/symbol_resnet-28-small.py\n\n\nHere is a visualization of this crazy network:\n\n![](http://josephpcohen.com/w/wp-content/uploads/resnet-28-small.png)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.03385"
    },
    "998": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/IoffeS15",
        "transcript": "A *Batch Normalization* applied immediately after fully connected layers and adjusts the values of the feedforward output so that they are centered to a zero mean and have unit variance.\n\nIt has been used by famous Convolutional Neural Networks such as GoogLeNet \\cite{journals/corr/SzegedyLJSRAEVR14} and ResNet \\cite{journals/corr/HeZRS15}",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/ioffe15.html"
    },
    "999": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/KingmaB14",
        "transcript": "Adam is like RMSProp with momentum. The (simplified) update [[Stanford CS231n]](https://cs231n.github.io/neural-networks-3/#ada) looks as follows:\n\n```\nm = beta1*m + (1-beta1)*dx\nv = beta2*v + (1-beta2)*(dx**2)\nx += - learning_rate * m / (np.sqrt(v) + eps)\n```\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1412.6980"
    },
    "1000": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Cohen13a",
        "transcript": "This paper proposes a method to send messages between cell phones over Bluetooth by using the device name field. This allows devices to communicate directly with each other without pairing. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1307.7814"
    },
    "1001": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/colt/DuchiHS10",
        "transcript": "This is Adagrad. Adagrad is an adaptive learning rate method. Some sample code from  [[Stanford CS231n]](https://cs231n.github.io/neural-networks-3/#ada) is:\n\n```python\n# Assume the gradient dx and parameter vector x\ncache += dx**2\nx += - learning_rate * dx / (np.sqrt(cache) + eps)\n```",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265"
    },
    "1002": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/CohenDB15",
        "transcript": "This idea is so badass! It uses Simple Tree Matching \\cite{journals/spe/Yang91} and extends it to work with HTML and then recursively searches an unseen document to align it with previously seen examples. An overview of the problem of *shift* can be seen on the left of the figure below and  the alignment is shown on the right.\n\nhttp://i.imgur.com/b8EzP42.png",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1505.01303"
    },
    "1003": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/fgr/LoC015",
        "transcript": "The prediction gradient is just $\\frac{\\partial \\mathbf{y}}{\\partial w}$ where $\\mathbf{y}$ is the output before the loss function. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/FG.2015.7163154"
    },
    "1004": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/CohenL016",
        "transcript": "Basically they observe a pattern they call The Filter Lottery (TFL) where the random seed causes a high variance  in the training accuracy:\n\n![](http://i.imgur.com/5rWig0H.png)\n\nThey use the convolutional gradient norm ($CGN$) \\cite{conf/fgr/LoC015} to determine how much impact a filter has on the overall classification loss function by taking the derivative of the loss function with respect each weight in the filter.\n\n$$CGN(k) = \\sum_{i} \\left|\\frac{\\partial L}{\\partial w^k_i}\\right|$$\n\nThey use the CGN to evaluate the impact of a filter on error, and re-initialize filters when the gradient norm of its weights falls below a specific threshold.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.05931"
    },
    "1005": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/af/Maymin11",
        "transcript": "Is the market efficient? This is perhaps the most prevalent question in all of finance. While this paper does not aim to answer that question, it does frame it in an information-theoretic context. Mainly, Maymin shows that at least the weak form of the efficient market hypothesis (EMH) holds if and only if P = NP. \n\nFirst, he defines what efficient market means: \n\"The weakest form of the EMH states that future prices cannot be predicted by analyzing prices from the past. Therefore, technical analysis cannot work in the long run, though of course any strategy could make money randomly.\"\n\nFor $n$ past historical price changes of {UP, DOWN}. Let there be three trading strategies that are neutral, long or short to the market. In order to check if there exists a strategy that statistically significantly makes money, requires checking all $3^n$ possible strategies. Verifying that a strategy is profitable beyond random chance can be done with a linear $O(n)$ pass of the historical data. Thus the problem of finding a profitable strategy is NP. \n\nIf P=NP, then computing a profitable strategy can be done efficiently in polynomial time, since a trader can check each possible strategy in polynomial time. We can then trade based on our results to make the market efficient as well. If the market is efficient, it becomes impossible for a trader to predict future prices based on historical data, as the current price has all publicly available information \"priced in\". A future price would be a random fluctuation of the current price. \n\nDoes an efficient market imply P=NP? \n\nAn example 3-SAT:\n\n$$(a \\lor b \\lor !c) \\land (a \\lor !b \\lor d)$$\n\nThe NP problem of 3-sat can be encoded into the market using order-cancels-order (OCO) orders[^1]. \nWhere each variable is a security and negated variables are sell orders. \n\nPlace these two OCOs in the market.\n\n$$\\text{OCO (buy A, buy B, or sell C)}$$\n$$\\text{OCO (buy A, sell B, or buy D)}$$\n\nAfter some constant time, any outstanding order is cancelled and all positions are liquidated. If the market is efficient, then there exists a way to execute these two OCO orders such that an overall profit is guaranteed. In other words, a market that is efficient allows us to solve an arbitrary 3-SAT problem in polynomial time. \n\n### **Takeaway**:\nThe author links market efficiency with computational complexity. \n\n[^1]: An order-cancels-order is a type of order on two different securities that automatically cancels the other order when one is filled. There is no chance that both orders can be filled. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.3233/AF-2011-007"
    },
    "1006": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/iacr/BellareRRS09",
        "transcript": "Format-preserving encryption is a deterministic encryption scheme that encrypts plaintext of some specified format into ciphertext of the same format. This has a lot of practical use cases such as storing SSN or credit card information, without having to change the underlying schematics of the database or application that stores the data. The protected data is in-differentiable from unprotected data, and still enables some analytics over it, such as with masking (ie, displaying last four digits of a format). For other analytics, and depending on the use cases, differential privacy or FHE should be considered. \n\nThis paper primarily describes construction of a short-space FPE scheme. Starting off with earlier constructions based off of work done by Terence Spies and M. Bellare. The authors have developed an FPE scheme that is tweakable, to enhance security and this value is imperative for ciphertext alphabets of small range. A tweak value is like an initialization vector, chosen by the user. This ensures that with two different tweak values, the same plaintext encrypted twice will encipher to different ciphertexts. \n\nOf particular interest for practical applications is 'cycle-walking' and 'rank-then-encipher'. These are \"meta-techniques\" for building an FPE scheme that essentially ensures you get a ciphertext in the desired ciphertext space. \n\nmisc (pg 9):\n\ncycle-walking: \n\nIf you wanted to encipher on some message space $X$, you would make an FPE scheme:\n$$E: K \\times \\chi \\rightarrow \\chi$$\n\nIf you know how to encipher onto a superset of $X'$, with an FPE scheme:\n$$E' : K \\times \\chi' \\rightarrow \\chi'\\\\$$\nAll you would need to do is keep enciphering the message $X \\in \\chi'$ until you get $X \\in \\chi$\n\nRank-then-Encipher:\n\nEncipher a point $X \\in \\chi$\nmap it to a corresponding point $X' \\in \\chi'$, encipher that point in $\\chi'$ to get a ciphertext $Y' \\in \\chi'$, then map $Y'$ to its corresponding point $Y$ in $\\chi$.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://eprint.iacr.org/2009/251"
    },
    "1007": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/iacr/BrakerskiV11",
        "transcript": "Brakerski and Vaikuntanathan introduce a fully homomorphic encryption scheme (FHE) based solely on the decisional learning with errors (LWE) security assumptions. Moving away from the relatively obscure mathematics of ideal lattices. They introduce relinearization and modulus switching techniques for dimensionality reduction and for removing the \u201csquashing\u201d step of Craig Gentry\u2019s FHE scheme. BV11 and other similar schemes are commonly referred to as \u201cSecond generation FHE\u201d schemes. \n\nReliearnization lowers the dimensions of the produced ciphertext, and allows the construction of a somewhat homomorphic encryption scheme that is capable of evaluating $O(\\log n)$ depth (ie, multiplicative depth, circuit). With every homomorphic multiplication, the ciphertext error is squared. To slow down the growth of the error, BV11 shows that you can choose a new secret key used in re-linearization to have small modulus $p$. Thus taking the ciphertext dimensions from $(n,\\log q)$ down to $(k, \\log p)$, where it is small enough to achieve a bootstrappable scheme, thus making it FHE.\n\n$$\\\\\n$$\n\nBelow is an example of re-linearization for multiplicative homomorphism from the paper. Mostly for my own notes, but may help give additional intuition for others. What confused me was the \"publishing\" of the linear and quadratic terms in $s$. What they mean is that an evaluation key is published, containing the 'encryptions' of the linear and quadratic variables. For example, if $n=2$, this would be the set of encryptions for secret key $s$ :  Enc ${\\{s_1, s_2, s_1s_2, s_1^2, s_2^2\\}}$.\n\n\nGiven a ciphertext $(\\mathbf{a},b)$ and $(\\mathbf{a}',b')$, we can describe homomorphic multiplication with the decryption function $f$:\n\n$$\\begin{align} \nf_{(\\mathbf{a},b)}(x) \\cdot f_{(\\mathbf{a'},b')}(x) &= (b - {\\sum_{i}^n} a[i] \\cdot x[i]) \\cdot (b' - {\\sum_{i}^n} a'[i] \\cdot x[i])\\\\\n&= h_0 + \\sum_{i}^n h_i \\cdot x[i] + \\sum_{i,j}^n h_{i,j} \\cdot x[i]x[j] \\\\\n\\end{align}\n$$\n\nwhere $h_i$ and $h_{i,j}$ are coefficients of the 2-degree polynomial in $\\mathbf{x}$.\n$$\\begin{align} h_0 &= bb' \\\\\nh_i &= -(b\\mathbf{a}'[i] + b'\\mathbf{a}[i]) \\\\\nh_{i,j} &= \\mathbf{a}[i]\\mathbf{a}'[j] \n\\end{align}$$\n\nWith the secret key $s \\in \\mathbb{Z}_q^n$, we evaluate $f$ on $s$ to get the plaintext.\n\nBecause the decryption function needs to know all of the coefficients of the quadratic polynomial to be able to decrypt correctly, the size of the ciphertext jumps from $n+1$ to an order of $\\approx n^2$. \nWhat BV11 shows is a relinearization technique that reduces the size of the ciphertext back down to $n+1$.\n\nTo do this, we create an evaluation key $evk$ that is made up of the encryption of the quadratic terms of $x$. The ciphertexts will be the set $\\{(\\mathbf{a}_i,b), (\\mathbf{a}_{i,j},b)\\}$\n\nNote: $\\mathbf{a}$ is a random vector $\\in \\mathbb{Z}_q^n$\n\nwhere\n\n$$\\begin{align}\nb_{i,j} &\\approx \\langle \\mathbf{a}_{i,j},t\\rangle + s[i] \\cdot s[j] \\\\\nb_{i} &\\approx \\langle \\mathbf{a}_{i},t\\rangle + s[i]\n\\end{align}\n$$\n\nNow we can rewrite the symbolic multiplication of the decryption function as:\n\n$$\\begin{align} \nf_{(\\mathbf{a}_{mult},b_{mult})}(t) &= h_0 + \\sum_{i}^n h_i (b_i - \\langle \\mathbf{a}_i,\\mathbf{t} \\rangle) + \\sum_{i,j}^n h_{i,j} (b_{i,j} - \\langle \\mathbf{a}_{i,j}, \\mathbf{t} \\rangle) \\\\\n\\end{align}\n$$\n\nThis is a linear function in $\\mathbf{t}$! The number of coefficients of this linear equation are at most $n+1$. \n$$\\begin{align} \n\\mathbf{a}_{mult} &= \\sum_{i}^n h_i \\mathbf{a}_i + \\sum_{i,j}^n h_{i,j} \\mathbf{a}_{i,j}\\\\\n\\mathbf{b}_{mult} &= h_0 + \\sum_{i}^n h_i \\mathbf{b}_i + \\sum_{i,j}^n h_{i,j} \\mathbf{b}_{i,j}\n\\end{align}\n$$\n\n## See Also\n[lecture](https://www.youtube.com/watch?v=MB3mSaG6Bro) from MathNet Korea (potato res). ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://eprint.iacr.org/2011/344"
    },
    "1008": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1111/j.1467-9965.1991.tb00002.x",
        "transcript": "Cover's Universal Portfolio is an information-theoretic portfolio optimization algorithm that utilizes constantly rebalanced porfolios (CRP). A CRP is one in which the distribution of wealth among stocks in the portfolio remains the same from period to period. Universal Portfolio strictly performs rebalancing based on historical pricing, making no assumptions about the underlying distribution of the prices. \n\nThe wealth achieved by a CRP over n periods is:\n\n$S_n(b,x^n) = \\displaystyle  \\prod_{n}^{i=1} b \\cdot x$\n\nThe key takeaway:\n\nWhere $\\mathrm{b}$ is the allocation vector. Cover takes the integral of the wealth over the entire portfolio to give $b_{t+1}$. This is what makes it \"universal\". Most implementations in practice do this discretely, by creating a matrix $\\mathrm{B}$ with each row containing a combination of the percentage allocatio, and calculating $\\mathrm{S} = \\mathrm{B\\dot x}$.\n\nCover mentions trading costs will eat away most of the gains, especially if this algorithm is allowed to rebalance daily. Nowadays, there are commission-free brokers. See this summary for Universal Portfolios without transaction costs: \\cite{conf/colt/BlumK97}\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1111/j.1467-9965.1991.tb00002.x"
    },
    "1009": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1145/3178876.3186154",
        "transcript": "This work is a direct improvement of Collaborative Metric Learning. While CML tries to retrieve user and item embeddings in a direct way by placing them in metric space and adjusting with triplet loss, this paper focuses on introduction of latent relational vectors.\n\n A relational vector $r$ must describe relation between user $p$ and item $q$ in a way that  $s(p,q)=\\parallel \\ p + r - q \\parallel \\approx 0$.\n\nVectors $r$ are introduced as a softmax-weighted linear combination of vectors from Latent Relational Attentive Memory (LRAM).\n\nThe net is trained with BPR-like loss via negative sampling $max(0, s(p,q) - s(p', q') + margin)$",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3178876.3186154"
    },
    "1010": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/www/HsiehYCLBE17",
        "transcript": "## Idea\nUse implicit feedback and item features to project users and items into the same latent space to use with kNN later. Learned metric encodes user-item, user-user and item-item relationships.\n## Loss\nUsers and items are represented by vectors $u_i \\in \\mathcal{R}^r, v_i \\in \\mathcal{R}^r$.\n\nWe define euclidean distance as $d(i,j)= \\parallel u_i-v_j\\  \\parallel$\n\nLoss function consists of 3 parts:\n$$\\mathcal{L}=\\mathcal{L}_m + \\lambda_f\\mathcal{L}_f + \\lambda_c\\mathcal{L}_c$$\n\n### Weighted Triplet Loss\nSample user $i$, positive item $j$ and negative item $k$.\n$$\\mathcal{L}_m=\\sum_{i,j,k}w_{ij}[d(i,j)^2-d(i,k)^2+m]_{+}$$\nwhere $[z]_{+}=max(0,z)$ and $m>0$ is the margin size.\n\n$w_{i,j}$ is calculated in WARP fashion, but sampling $U$ negative items for each positive pair $(i,j)$ instead of sampling until imposter is met.\n$$w_{i,j}=log(\\lfloor |Items| \\frac{M}{U}\\rfloor + 1)$$\nwhere $M$ is the number of imposters in $U$ sampled negative items.\n\n### Loss for features\nLet $x_j \\in \\mathcal{R}^m$ denote raw feature vector of item $j$. We want it to be close to corresponding item vector $v_j$.\n$$\\mathcal{L}_f=\\sum_j \\parallel f(x_j) - v_j\\ \\parallel ^2$$\nwhere $f$ is some transformation (MLP with dropout) to process item features.\n\n### Regularization\nkNN is ineffective ineffective in high-dimensional sparse space, so we bound user and item vectors to a unit sphere.\n$$\\parallel \\ u_* \\parallel ^2 \\leq 1$$\n$$\\parallel \\ v_* \\parallel ^2 \\leq 1$$\n$L_2$ norm is not used because it pulls every object toward the origin which does not have any specific meaning in our case. Covariance regularization is used instead to de-correlate dimensions of the learned metric.\n\nThe covariances between all pairs of dimensions $i$ and $j$ form a matrix $C$.\n$$C_{i,j} = \\frac{1}{N} \\sum_n (y_i^n - \\mu_i^n)(y_j^n - \\mu_j^n)$$\nwhere $\\mu_i = \\frac{1}{N}(\\parallel C \\parallel_f - \\parallel diag(C) \\parallel_2^2)$ and $\\parallel \\cdot \\parallel_f$ is the Frobenius norm\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3038912.3052639"
    },
    "1011": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/recsys/XinMPLA17",
        "transcript": "One bad item can reduce perceived quality of recommendation list. Sometimes this may be particularly undesirable such as recommending horror movies to children. Authors argue that this happens when missing not at random data is handled improperly and separate groups of users and items overlap during the process of dimensionality reduction and computation of embeddings. Folding is a metric that measures the severity of described effect in a recommendation model.\n\nTo calculate folding we must introduce the notion of relatedness between user $i$ and item $j$ which captures the likelihood of interaction between $i$ and $j$, regardless of the rating. In a way this is a form of smoothing the interaction matrix. There are different ways to calculate relatedness, but authors propose to solve matrix factorization task using WALS with high weight for missing interactions or use SVD for this purpose.\n\nGiven predicted score and relatedness matrixes $S, R \\in \\mathbb{R}^{m \\times n}$ we can calculate folding as the average across all interactions\n$$Folding = \\frac{1}{mn} \\sum_{i,j}max(0, s_{ij}-r_{ij})$$",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3109859.3109911"
    },
    "1012": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/um/FrumermanSSS19",
        "transcript": "## Idea\n\nWhen we recommend items to users, some of them are not chosen by the user. These rejected recommendations are usually treated as hard mistakes.\n\nAuthors argue that these bad recommendations still may influence user's choice even though they were not picked. For example user didn't click on \"Die Hard\" but watched another Bruce Willis movie. This seems to be a not so bad recommendation after all and maybe we should not penalize it as hard as we usually do.\n\nUltimate goal is to invent a metric that would have good correlation between offline results and real online performance.\n\n## User study\n\nAuthors held a user study, showing people a set of 5 items: watched movie, 3 rejected recommendations and an item chosen after recommendation. Rejected recommendations were generated into 4 groups: \n- only high content similarity\n- only high collaborative similarity\n- only high popularity similarity\n- all medium similarities\n\nThe question was \"**How good is this recommendation 1-5?**\"\n\n| Content | Collaborative | Popularity | Other |\n| ------- | ------------- | ---------- | ----- |\n| 3.8     | 3.52          | 2.93       | 1.99  |\n\n## Proposal\nIf standard precision is\n$$p_u = \\frac{|c_u \\cap r_u|}{|r_u|}$$\nwhere $c_u$ are items chosen by user and $r_u$ items recommended to user, then we can define a refined precision as \n$$p_u^{sim} = p_u + \\frac{\\sum_{i \\in r_u \\setminus c_u}max_{j \\in \\{ c_u:\\ t(u,j) > t(u,i)\\}}sim(i, j)}{|r_u|}$$\nwhere $t(u,i)$ is the time when user $u$ interacted with item $i$.\n\n## Evaluation\nAuthors used Xing dataset containing user interactions with a system for seeking employment opportunities. It contains logs of what was recommended and what was clicked.\n\n### \"Online\" evaluation\nMeasure correlation between different refinement types of precision of RS presented in dataset and actual user clicks.\n\n| Content | Collaborative | Regular |\n| ------- | ------------- | ------- |\n| 0.615   | 0.197         | 0.184   |\n\n### Offline evaluation\nSplit logs 70/30 by time and measure correlation between number of clicks per user on test and metrics on train as if we were training a model on train part.\n\n\n\n| Train clicks | Content | Collaborative | Random |\n| ------------ | ------- | ------------- | ------ |\n| 0.5          | 0.35    | 0.16          | 0.087  |\n\n\n## Open question\nWhat is the best way to calculate item similarity?",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1145/3320435.3320448"
    },
    "1013": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/Sohl-DicksteinW15",
        "transcript": "I spend the week at ICML, and this paper on generative models is one of my favourites so far:\n\nTo be clear: this post doesn't add much to the presentation of the paper, but I will attempt to summarise my understanding of it. Also, I want to make clear that this is not my work.\n\nUnsupervised learning has been one of the most interesting areas of machine learning in the last decades, but it is in the spotlight again since the deep learning crowd started to care about it. Unsupervised learning is hard because evaluating the loss function people want to use (log likelihood) is intractable for most interesting models. Therefore people come up with\n\n- alternative objective functions, such as adversarial training, maximum mean discrepancy, or pseudolikelihood, which can be evaluated for a large class of interesting models\n- alternative optimisation methods or approximate inference methods such as contrastive divergence or variational Bayes\n- models that have some nice properties. This paper is an example of the latter\n\n#### The key idea behind the paper\n\nWhat we typically try to do in representation learning is to map data to a latent representation. While the Data can have arbitrarily complex distribution along some complicated nonlinear manifold, we want the computed latent representations to have a nice distribution, like a multivariate Gaussian.\n\nThis paper takes this idea very explicitly using a stochastic mapping to turn data into a representation: a random diffusion process. If you take any data, and apply Brownian motion-like stochastic process to this, you will end up with a standard Gaussian distributed variable, due to the stationarity of the Brownian motion. Below image shows an example: 2D observations (left) have a complex data distribution along the Swiss roll manifold. If one applies Brownian motion to each datapoint, the complicated structure starts to diffuse, and eventually the data is scrambled to become white noise (right).\n\n![](http://www.inference.vc/content/images/2015/07/Screen-Shot-2015-07-09-at-13-27-41.png)\n\nNow the trick the authors used is to train a dynamical system to inverts this random walk, to be able to reconstruct the original data distribution from the random Gaussian noise. Amazingly, this works, and the traninig objective becomes very similar to variational autoencoders. Below is a figure showing what happens when we try to reconstruct data in the Swiss roll example: The top images from right to left: we start with a bunch of points drawn from random noise (top right). We apply the inverse nonlinear transformation to these points (top middle). Over time points will be pushed towards the original Swiss roll manifold (top left).\n\n`The information about the data distribution is encoded in the approximate inverse dynamical system`\n\nThe bottom pictures show where this dynamical system tries to push points as time progresses.\n\n![](http://www.inference.vc/content/images/2015/07/Screen-Shot-2015-07-09-at-13-30-16.png)\n\nThis is super cool. Now we have a deep generative process that can turn random noise into something that looks like our datapoints. It can generate roughly natural-looking images like these:\n\n![](http://www.inference.vc/content/images/2015/07/Screen-Shot-2015-07-09-at-13-33-38.png)\n\n #### Advantages\n\nIn this model a lot of things that are otherwise hard to do are easy to do:\n\n1. generating/imagining data is straightforward\n2. inference, i.e. calculating the latent representation from data, is simple\n3. you can multiply the distribution with another distribution, making Bayesian calculations for stuff like denoising or superresolution possible.\n\n#### Drawbacks and extensions\n\nI think a drawback of the model is that if you run the diffusion process for too long (i.e. make the model deeper), the mutual information between datapoint and its representation is bound to decrease, due to the stationarity of Brownian motion. I guess this is going to be an important limitation to the depth of these models.\n\nAlso, the latent representations at each layer are assumed to be exactly if the same dimensionality and type as the data itsef. So if we are modeling 100x100 images, then all layers in the resulting network will have 100k nodes. I guess this can be overcome by combining variational autoencoders with this method. Also, you can imagine augmenting your space with extra 'pixels' that are only used for richer representations in the intermediate layers.\n\nAnyway, this is super cool, go read the paper.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/sohl-dickstein15.html"
    },
    "1014": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/GermainGML15",
        "transcript": "This is my second favourite paper from ICML last week, and I think the title really does not do it justice. It is a great idea about training rich, tractable autoregressive generative models of data, and doing so by using standard techniques from autoencoder training with dropout.\n\nCaveat (again): this is not my work, and this blog post does not really add anything new to the paper, only my perspective on it.\n\n#### Unsupervised learning primer (again)\n\nUnsupervised learning is about modelling the probability distribution $p(\\mathbf{x})$ of some data, from which we observe independent samples $\\mathbf{x}_i$. Often, the vector $\\mathbf{x}$ is high dimensional, such as in images, where different components of $\\mathbf{x}$ encode pixel intensities.\n\nTypically, a probability model is specified as $q(\\mathbf{x};\\theta) = \\frac{f(\\mathbf{x};\\theta)}{Z_\\theta}$, where $f(\\mathbf{x};\\theta)$ is some positive function parametrised by $\\theta$. The denominator $Z_\\theta$ is called the normalisation constant which makes sure that $q$ is a valid probability model: it has to sum up to one over all possible configurations of $\\mathbf{x}$. The central problem in unsupervised learning is that for the most interesting models in high dimensional spaces, calculating $Z_\\theta$ is intractable, so crucial quantities such as the model likelihood cannot be calculated and the model cannot be fitted. The community is therefore in search for\n\n- interesting models that have tractable normalisation constants\n- fancy methods to deal with intractable models (pseudo-likelihood, adversarial networks, contrastive divergence)\n\nThis paper is about the former.\n\n#### Core ingredient: autoregressive models\n\nThis paper sidesteps the high dimensional normalisation problem by restricting the class of probability distributions to autoregressive models, which can be written as follows:\n\n$$q(\\mathbf{x};\\theta) = \\prod_{d=1}^{D} q(x_{d}\\vert x_{1:d-1};\\theta).$$\n\nHere $x_d$ denotes the $d^{th}$ component of the input vector $\\mathbf{x}$. In a model like this, we only need to compute the normalisation of each $q(x_{d}\\vert x_{1:d-1};\\theta)$ term, and we can be sure that the resulting model is a valid model over the whole vector $\\mathbf{x}$. But as normalising these one-dimensional probability distributions is a lot easier, we have a whole range of interesting tractable distributions at our disposal.\n\n#### Training multiple models simultaneously\n\nAutoregressive models are used a lot in time series modelling and language modelling: hidden Markov models or recurrent neural networks are examples. There, autoregressive models are a very natural way to model data because the data comes ordered (in time).\n\nWhat's weird about using autoregressive models in this context is that it is sensitive to ordering of dimensions, even though that ordering might not mean anything. If $\\mathbf{x}$ encodes an image, you can think about multiple orders in which pixel values can be serialised: sweeping left-to-right, top-to-bottom, inside-out etc. For images, neither of these orderings is particularly natural, yet all of these different ordering specifies a different model above.\n\nBut it turns out, you don't have to choose one ordering, you can choose all of them at the same time. The neat trick in the masking autoencoder paper is to train multiple autoregressive models all at the same time, all of them sharing (a subset of) parameters $\\theta$, but defined over different ordering of coordinates. This can be achieved by thinking of deep autoregressive models as a special cases of an autoencoder, only with a few edges missing.\n\n![](http://www.inference.vc/content/images/2015/07/Screen-Shot-2015-07-13-at-10-48-54.png)\n\nConsider a fixed ordering of input dimensions. Now take a fully connected autoencoder, which defines a probability distribution $q(\\hat{\\mathbf{x}}\\vert\\mathbf{x};\\theta)$. You can write this as\n\n$$q(\\hat{\\mathbf{x}}\\vert\\mathbf{x};\\theta) = \\prod_{d=1}^{D} q(\\hat{x}_{d}\\vert x_{1:D};\\theta)$$\n\nNote the similarity to the autoregressive equation above, the only difference being that each coordinate now depends on every other coordinate ($x_{1:D}$), rather than only coordinates that precede it in the ordering ($x_{1:d-1}$). To turn this equation into autoregressive equation above, we simply have to remove dependencies of each output coordinate $\\hat{x}_{d}$ on any input coordinate $\\hat{x}_{e}$, where $e>=d$. This can be done by removing edges along all paths from the input coordinate $\\hat{x}_{e}$ to output coordinate $\\hat{x}_{d}$. You can achieve this cutting of edges by multiplying the weight matrices $\\mathbf{W}^{l}$of the autoencoder neural network elementwise by binary masking matrices $\\mathbf{M}^{\\mathbf{W}^{l}}$. Hence the name masked autoencoder.\n\nThe procedure above considered a fixed ordering of coordinates. You can repeat this process for any arbitrary ordering, for which you obtain different masking matrices but otherwise the same procedure. If you train this autoencoder network with randomly sampled masking matrices, you essentially train a family of autoregressive models, each sharing some parameters via the underlying autoencoder network.\n\nBecause masking is similar to the popular dropout training, implementing it is relatively straightforward and requires minimal change to existing autoencoder code. However, now you have a generative model - in fact, a large set of generative models - which has a lot of nice properties for you to enjoy.\n\nThe slight concern\n\nOf course, this would be all too good to be true: a powerful deep generative model that is easy to evaluate and all. I think the problem with this is the following: If you train just one of these autoregressive models, that's tractable, exact and fine. But you really want to combine all (or many) of these becuause individually they are weak.\n\nWhat is the interpretation of training with randomly drawn masking matrices? You can think of it as stochastic gradient descent on the following objective:\n\n$$\\mathbb{E}_{\\mathbf{x}\\sim p}\\mathbb{E}_{\\pi \\sim U} \\log q(\\mathbf{x},\\pi,\\theta)$$\n\nHere, I used $\\pi$ to denote a permutation of the coordinates, and $\\mathbb{E}_{\\pi \\sim U}$ to take an expectation over a uniform distribution over permutations. The distribution $q(\\mathbf{x},\\pi,\\theta)$ is the autoregressive model defined by $\\theta$ and the masking matrices corresponding to permutation $\\pi$. $\\mathbb{E}_{\\mathbf{x}\\sim p}$ denotes averaging over the empirical data distribution.\n\nCombining as a mixture model\n\nOne way to combine autoregressive models is to take a mixture model. In the paper, the authors actually use an ensemble to make predictions, which is analogous to an equal mixture model where the mixture weights are uniform and fixed. The likelihood for this model would be the following:\n\n$$\\mathbb{E}_{\\mathbf{x}\\sim p} \\log \\mathbb{E}_{\\pi \\sim U} q(\\mathbf{x},\\pi,\\theta)$$\n\nNotice that the averaging over permutations now takes place inside the logarithm. By Jensen's inequality, we can say that randomly sampling masking matrices during training amounts to optimising a stochastically estimated lower bound to the likelihood of an equal mixture. This raises the question whether actually learning the weights in such a model would be hard using something like an EM algorithm with a sparsity-enforcing regulariser/prior over mixture weights.\n\nCombining as a product of experts model\n\nCombining these autoregressive models as a mixture is not ideal. In mixture modeling the sharpness of the mixture distribution is bounded by the sharpness of component distributions. Your combined prediction can never be more confident than the your most confident model. In this case, I expect the AR models to be pretty poor models individually, and therefore not to be very sharp, particularly along the first few coordinates in the corresponding ordering.\n\nA better way to combine probabilistic models is via product of experts. You can actually interpret training by random masking matrices as a form of product of experts, but with the global normalisation ignored. I'm not sure if it would be possible/tractable to do anything better than this.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/germain15.html"
    },
    "1015": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DentonCSF15",
        "transcript": "This post is a comment on the Laplacian pyramid-based generative model proposed by researchers from NYU/Facebook AI Research.\n\nLet me start by saying that I really like this model, and I think - looking at the samples drawn - it represents a nice big step towards convincing generative models of natural images.\n\nTo summarise the model, the authors use the Laplacian pyramid representation of images, where you recursively decompose the image to a lower resolution subsampled component and the high-frequency residual. The reason this decomposition is favoured in image processing is the fact that the high-frequency residuals tend to be very sparse, so they are relatively easy to compress and encode.\n\nIn this paper the authors propose using convolutional neural networks at each layer of the Laplacian pyramid representation to generate an image sequentially, increasing the resolution at each step. The convnet at each layer is conditioned on the lower resolution image, and some noise component $z_k$, and generates a random higher resolution image. The process continues recursively until the desired resilution is reached. For training they use the adversarial objective function. Below is the main figure that explains how the generative model works, I encourage everyone to have a look at the paper for more details:\n\n![](http://www.inference.vc/content/images/2015/07/Screen-Shot-2015-07-23-at-11-15-17.png)\n\n#### An argument about Conditional Entropies\n\nWhat I think is weird about the model is the precise amount of noise that is injected at each layer/resolution. In the schematic above, these are the $z_k$ variables. Adding the noise is crucial to defining a probabilistic generative process; this is how it defines a probability distribution.\n\nI think it's useful to think about entropies of natural images at different resolutions. When doing generative modelling or unsuperised learning, we want to capture the distribution of data. One important aspect of a probability distribution is its entropy, which measures the variability of the random quantity. In this case, we want to describe the statistics of the full resolution observed natural image $I_0$. (I borrow the authors' notation where $I_0$ represents the highest resolution image, and $I_k$ represent the $k$-times subsampled version. Using the Laplacian pyramid representation, we can decompose the entropy of an image in the following way:\n\n$$\\mathbb{H}[I_0] = \\mathbb{H}[I_{K}] + \\sum_{k=0}^{K-1} \\mathbb{H}[I_k\\vert I_{k+1}].$$\n\nThe reason why the above decomposition holds is very simple. Because $I_{k+1}$ is a deterministic function of $I_{k}$ (subsampling), the conditional entropy $\\mathbb{H}[I_{k+1}\\vert I_{k}] = 0$. Therefore the joint entropy of the two variables is simply the entropy of the higher resolution image $I_{k}$, that is $\\mathbb{H}[I_{k},I_{k+1}] = \\mathbb{H}[I_{k}] + \\mathbb{H}[I_{k+1}\\vert I_{k}] = \\mathbb{H}[I_{k}]$. So by induction, the join entropy of all images $I_{k}$ is just the marginal entropy of the highest resolution image $I_0$. Applying the chain rule for joint entropies we get the expression above.\n\nNow, the interesting bit is how the conditional entropies $\\mathbb{H}[I_k\\vert I_{k+1}]$ are 'achieved' in the Laplacian pyramid generative model paper. These entropies are provided by the injected random noise variables $z_k$. By the information processing lemma $\\mathbb{H}[I_k\\vert I_{k+1}] \\leq \\mathbb{H}[z_k]$. The authors choose $z_k$ to be uniform random variables whose dimensionality grows with the resolution of $I_k$. To quote them \"The noise input $z_k$ to $G_k$ is presented as a 4th color plane to low-pass $l_k$, hence its dimensionality varies with the pyramid level.\" Therefore $\\mathbb{H}[z_k] \\propto 4^{-k}$, assuming that the pixel count quadruples at each layer.\n\nSo the conditional entropy $\\mathbb{H}[I_k\\vert I_{k+1}]$ is allowed to grow exponentially with resolution, at the same rate it would grow if the images contained pure white noise. In their model, they allow the per-pixel conditional entropy $c\\cdot 4^{-k}\\cdot \\mathbb{H}[I_k\\vert I_{k+1}]$ to be constant across resolutions. To me, this seems undesirable. My intuition is, for natural images, $\\mathbb{H}[I_k\\vert I_{k+1}]$ may grow as $k$ decreases (because the dimensionality gorws), but the per-pixel value $c\\cdot 4^{k}\\cdot \\mathbb{H}[I_k\\vert I_{k+1}]$ should decrease or converge to $0$ as the resolution increases. Very low low-resolution subsampled natural images behave a little bit like white noise, there is a lot of variability in them. But as you increase the resolution, the probability distribution of the high-res image given the low-res image will become a lot sharper.\n\nIn terms of model capacity, this is not a problem, inasmuch as the convolutional models $G_{k}$ can choose to ignore some variance in $z_k$ and learn a more deterministic superresolution process. However, adding unnecessarily high entropy will almost certainly make the fitting of such model harder. For example, the adversarial training process relies on sampling from $z_k$, and the procedure is pretty sensitive to sampling noise. If you make the distribution of $z_k$ unneccessarily high entropy, you will end up doing a lot of extra work during training until the network figures out to ignore the extra variance.\n\nTo solve this problem, I propose to keep the entropy of the noise vectors constant, or make them grow sub-linearly with the number of pixels in the image. This mperhaps akes the generative convnets harder to implement. Another quick solution would be to introduce dependence between components of $z_k$ via a low-rank covariance matrix, or some sort of a hashing trick.\n\n#### Adversarial training vs superresolution autoencoders\n\nAnother weird thing is that the adversarial objective function forgets the identity of the image. For example, you would want your model so that\n\n`\"if at the previous layer you have a low-resolution parrot, the next layer should be a higher-resolution parrot\"`\n\nInstead, what you get with the adversarial objective is\n\n`\"if at the previous layer you have a low-resolution parrot, the next layer should output a higher-dimensional image that looks like a plausible natural image\"`\n\nSo, there is nothing in the objective function that enforces dependency between subsequent layers of the pyramid. I think if you made $G_k$ very complex, it could just learn to model natural images by itself, so that $I_{k}$ is in essence independent of $I_{k+1}$ and is purely driven by the noise $z_{k}$. You could sidestep this problem by restricting the complexity of the generative nets, or, again, to restrict the entropy of the noise.\n\nOverall, I think the approach would benefit from a combination of the adversarial and a supervised (superresolution autoencoder) objective function.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks"
    },
    "1016": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/GoroshinML15",
        "transcript": "I think this paper has two main ideas in there, I see them as independent, for reasons explained below:\n\n- A new penalty function that aims at regularising the second derivative of the trajectory the latent representation traces over time. I see this as a generalisation of the slowness principle or temporal constancy, more about this in the next section.\n\n- A new autoencoder-like method to predict future frames in video. Video is really hard to forward-predict with non-probabilistic models because high level aspects of video are genuinely uncertain. For example, in a football game, you can't really predict whether the ball will hit the goalpost, but the results might look completely different visually. This, combined with L2 penalties often results in overly conservative, blurry predictions. The paper improves things by introducing extra hidden variables, that allow the model to represent uncertainty in its predictions. More on this later.\n\n#### Inductive bias: penalising curvature\n\nThe key idea of this paper is to learn good distributed representations of natural images from video in an unsupervised way. Intuitively, there is a lot of information contained in video, which is lost if you scramble the video and look at statistics individual frames only. The race is on to develop the right kind of prior and inductive bias that helps us fully exploit this temporal information. This paper presents a way, which is called learning to linearise (I'm going to call this L2L).\n\nNaturally occurring images are thought to reside on some complex, nonlinear manifold whose intrinsic dimension is substantially lower than the number of pixels in an image. It is then natural to think about video as a journey on this manifold surface, along some smooth path. Therefore, if we aim to learn good generic features that correspond to coordinates on this underlying manifold, we should expect that these features vary in a smooth fashion over time as you play the video.\n\nL2L uses this intuition to motivate their choice of an objective function that penalises a scale-invariant measure of curvature over time. In a way it tries to recover features that transform nearly linearly as time progresses and the video is played.\n\nIn their notations, $x_{t}$ denotes the data in frame $t$, which is transformed by a deep network to obtain the latent representation $z_{t}$. The penalty for the latent representation is as follows.\n\n$$-\\sum_{t} \\frac{(z_t - z_{t-1})^{T}(z_{t+1} - z_{t})}{\\|z_t - z_{t-1}\\|\\|z_{t+1} - z_{t}\\|}$$\n\nThe expression above has a geometric meaning as the cosine of the angle between the vectors $(z_t - z_{t-1})$ and $(z_{t+1} - z_{t})$. The penalty is minimised if these two vectors are parallel and point in the same direction. In other words the penalty prefers when the latent feature representation keeps its momentum and continues along a linear path - and it does not like sharp turns or jumps. This seems like a sensible prior assumption to build on.\n\nL2L is very similar to another popular inductive bias used in slow feature analysis: the temporal slowness principle. According to this principle, the most relevant underlying features don't change very quickly. The slowness principle has a long history both in machine learning and as a model of human visual perception. In SFA one would minimise the following penalty on the latent representation:\n\n$$\\sum_{t} (z_t - z_{t-1})^{2},$$\n\nwhere the square is applied component-wise. There are additional constraints in SFA, more about this later. We can understand the connection between SFA and this paper's penalty if we plot the penalty for a single hidden feature $z_{t,f}$ at time $t$, keeping all other features and values at neighbouring timesteps constant. This is plotted in the figure below (scaled and translated so the objectives line up nicely).\n\n![](http://www.inference.vc/content/images/2015/09/-RfX4Dp2Y3YAAAAASUVORK5CYII-.png)\n\nAs you can see, both objectives have a minimum at the same location: they both try to force $z_{t,f}$ to linearly interpolate between the neighbouring timesteps. However, while SFA has a quadratic penalty, the learning to linearise objective tapers off at long distances. Compare this to Tukey's loss function used in outlier-resistant robust regression.\n\nBased on this, my prediction is that compared to SFA, this loss function is more tolerant of outliers, which in the temporal domain would mean abrupt jumps in the latent representation. So while SFA is equivalent to assuming that the latent features follow a Brownian-motion-like Ornstein\u2013Uhlenbeck process, I'd imagine this prior corresponds to something like a jump diffusion process (although I don't think the analogy holds mathematically).\n\nWhich one of these inductive biases/priors are better at exploiting temporal information in natural video? Slow Brownian motion, or nearly-linear trajectories with potentially a few jumps Unfortunately, don't expect any empirical answer to that from the paper. All experiments seem to be performed on artificially constructed examples, where the temporal information is synthetically engineered. Nor there is any real comparison to SFA.\n\n#### Representing predictive uncertainty with auxillary variables\n\nWhile the encoder network learns to construct smoothly varrying features $z_t$, the model also has a decoder network that tries to reconstruct $x_t$ and predict subsequent frames. This, the authors agree, is necessary in order for $z_t$ to contain enough relevant information about the frame $x_t$ (more about whether or not this is necessary later). The precise way this decoding is done has a novel idea as well: minimising over auxillary variables.\n\nLet's say our task is to predict a future frame $x_{t+k}$ based on the latent representation $z_{t}$. The problem is, this is a very hard problem. In video, just like in real life, anything can happen. Imagine you're modelling soccer footage, and the ball is about to hit the goalpost. In order to predict the next frames, not only do we have to know about natural image statistics, we also have to be able to predict whether the goal is in or not. An optimal predictive model would give a highly multimodal probability distribution as its answer. If you use the L2 loss with a deterministic feed-forward predictive network, it's likely to come up with a very blurry image, which would correspont to the average of this nasty multimodal distribution. This calls for something better, either a smarter objective function, or a better way of representing predictive uncertainty.\n\nThe solution the authors gave is to introduce hidden variables $\\delta_{t}$, that the decoder network also receives as input in addition to $z_t$. For each frame, $\\delta_t$ is optimised so that only the best possible reconstruction is taken into account in the loss function. Thus, the decoder network is allowed to use $\\delta$ as a source of non-determinism to hedge its bets as to what the contents of the next frame will be. This is one step closer to the ideal setting where the decoder network is allowed to give a full probability distribution of possibilities and then is evaluated using a strictly proper scoring rule.\n\nThis inner loop minimisation (of $\\delta$) looks very tedious, and introduces a few more parameters that may be hard to set. The algorithm is reminiscent of the E-step in expectation-maximisation, and also very similar to the iterated closest point algorithm Andrew Fitzgibbon talked about in his tutorial at BMVC this year.\n\nIn his tutorial, Andrew gave examples where jointly optimising model parameters and auxiliary variables ($\\delta$) is advantageous, and I think the same logic applies here. Instead of the inner loop, simultaneous optimisation helps fixing some pathologies, like slow convergence near the optimum. In addition, Andrew advocates exploiting the sparsity structure of the Hessian to implement efficient second-order gradient-based optimisation methods. These tricks are explained in paragraphs around equation 8 in (Prasad et al, 2010).\n\n#### Predictive model: Is it necessary?\n\nOn a more fundamental level, I question whether the predictive decoder network is really a necessary addition to make L2L work.\n\nThe authors observe that the objective function is minimised by the \"trivial\" solutions $z_{t} = at + b$, where $a,b$ can be arbitrary constants. They then say that in order to make sure features do something more than just discover some of these trivial solutions, we also have to include a decoder network, that uses $z_t$ to predict future frames. I believe this is not necessary at all.\n\nBecause $z_t$ is a deterministic function of $x_t$, and $t$ is not accessible to $z_{t}$ in any other way than through inferring it from $x_t$, as long as $a\\neq 0$, the linear solutions are not trivial at all. If the network discovers $z_{t} = at, a\\neq 0$, you should in fact be very happy (assuming a single feature). The only problems with trivial solutions occur when $z_{t} = b$ ($z$ doesn't depend on the data at all) or when $z$ is multidimensional and several redundant features are sensitive to exactly the same thing.\n\nThese trivial solutions could be avoided the same way they are avoided in SFA, by constraining the overall spatial covariance of $z_{t}$ over the videoclip to be $I$. This would force each feature to vary at least a little bit with data- hence avoiding the trivial constant solutions. It would also force features to be linearly decorrelated - solving the redundant features problem.\n\nSo I wonder if the decoder network is indeed a necessary addition to the model. I would love to encourage the authors to implement their new hypothesis of a prior both with and without the decoder. They may already have tried it without and found it really didn\u2019t work, so it might just be a matter of including those results. This would in turn allow us to see SFA and L2L side-by-side, and learn something about whether and why their prior is better than the sl",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5951-learning-to-linearize-under-uncertainty"
    },
    "1017": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/BengioVJS15",
        "transcript": "Google's team developed scheduled sampling as an alternative training procedure to fit RNNs, and they used it in their competition-winning method for image captioning. While I can't argue with the empirical results (so I won't), I was a bit skeptical about the technique at a fundamental level, so I decided to do a bit of math that resulted in this blog post.\n\nOverall, I have a suspicion that scheduled sampling is a flawed objective function for unsupervised/generative modelling, and I want to use this post to explain why I think so. I hope the comments section will work this time so people can comment and argue otherwise. Please also shoot me an email if you have more to say.\n\n#### Summary of this note\n\n- I have a critical look at scheduled sampling as objective function for training RNNs\n- I show it can lead to pathologies where the RNN learns marginal instead of conditional distributions\n- I explain why I think adversarial training/generative moment matching offers a better alternative\n- Lastly, I include a paragraph in which I apologise for being a di*k again.\n\n#### Strictly proper scoring rules\n\nI've mentioned scoring rules on this blog many times, and my PhD thesis was about them, so saying I'm obsessed with this topic would be a valid observation. But this is important stuff, particularly for unsupervised learning, and particularly as a framework to think about hard concepts like overfitting in generative models.\n\nScoring rules are essentially loss functions for probabilistic models/forecasts. A scoring rule $$S(x,Q)$$ simply measures how bad a probabilistic forecast $Q$ for a variable is in the light of actual observation $x$. In this notation, lower is better. A scoring rule is called strictly proper, if for any $P$, the following holds:\n\n$$\\underset{Q}{\\operatorname{argmax}} \\mathbb{E}_{x\\sim P}S(x,Q) = P$$\n\nIn other words, if you repeatedly sample observations from some true underlying distribution $P$, then the model $Q$ which minimises expected score is $P$. This means that the scoring rule cannot be fooled and that minimising the expected score yields a consistent estimator for $P$. Because I mention consistency, people may dismiss this as a learning theory argument, but it is not. If you are a Bayesian or a deep learning person with no interest in consistency, a scoring rule being strictly proper simply means that it is safe to use it as a loss function. Anything that's not strictly proper is weird and wrong, it will lead to learning the wrong thing.\n\nThis concept is central in unsupervised learning and generative modelling. Unsupervised learning is all about modelling the probability distribution of data, so it's essential that we have loss functions that can measure the discrepancy between our model $Q$, and the true data distribution $P$ in a consistent way.\n\n#### log-likelihood\n\nOne of the most frequently used strictly proper scoring rule is the logarithmic score:\n\n$$S(x,Q) = - \\log Q(x)$$\n\nThis quantity is also known as the negative log-likelihood. Minimising the expected score in an i.i.d scenario yields maximum likelihood estimation, which is known to be a consistent estimator and has nice properties.\n\nOften, the likelihood is impossible to evaluate. Luckily, it is not the only strictly proper scoring rule. In the context of generative models people have used the pseudolikelihood, score matching and moment matching, all of which are examples of strictly proper scoring rules.\n\nTo recap, any learning method that corresponds to minimising a strictly proper scoring rule is fine, everything else can go horribly wrong, even if we feed it infinite data, it might just learn the wrong thing.\n\n#### Scheduled Sampling\n\nAfter successfully establishing myself as a proper-scoring-rule-nazi, let's talk about scheduled sampling (SS). I don't have a lot of space explaining SS in great detail here, only the basic idea. I encourage everyone to read the paper and Hugo's summary above.\n\nSS is a new method to train recurrent neural networks(RNNs) to model sequences. I will use character-by-character models of text as an example. Typically, when you train an RNN, you aim to minimise the log predictive likelihood in predicting the next character in each training sentence, given the prefix string of previous characters. This can be thought of as a special case of maximum likelihood learning, and is all fine, you can actually do this properly without approximations.\n\nAfter training, you use the RNN to generate sample sentences in a recursive fashion: assuming you've already generated $n$ characters, you feed that prefix into the RNN, and ask it to predict the $n+1$st character. The $n+1$st character is then added to the prefix to predict the $n+2$th character, and so on.\n\nThe authors say there is a disconnect between how the model is trained (it's always fed real data) and how it's used (it's always fed synthetic data generated by itself). This, they argue, leads to the RNN being unable to recover from its own mistakes early on in the sentence.\n\nTo address this, the authors propose an alternative training strategy, where every once in a while, the network is given its own synthetic data instead of real data at training time. More specifically, for each character in the training sentences, we flip a coin to decide whether we feed the character from the real training sentence, or whether to feed the model's own prediction as to what that character would have been. The authors claim this makes the model more robust to recovering from mistakes, which is probably true.\n\nAs far as I'm concerned, I'm happy as long as the new training procedure corresponds to a strictly proper scoring rule. But in this case, I have a strong feeling that it does not.\n\n#### case study: sequence of two variables\n\nFor sake of simplicity, let's consider using scheduled sampling to learn the joint distribution of a sequence of just two random variables. This is probably the simplest (shortest) time series I can think of. So SS in this case works as follows: For each datapoint train the network to predict the real $x_1$. Then we flip a coin to decide whether to keep $x_1$ from the datapoint, or to replace it with a sample from the model $Q_{x_1}$. Then we train $Q_{x_2\\vert x_1}$ on the $(x_1,x_2)$ pair obtained this way.\n\nThe scoring rule for selective sampling looks something like this:\n\n$$ S(Q_{x_1,x_2},(x_1,x_2)) = - (1 - \\epsilon) [ \\mathbb{E}_{z \\sim Q_{x_1}} \\log Q_{x_2 \\vert x_1}(x_2 \\vert z) + \\log Q_{x_1}(x_1)] - \\epsilon \\log Q_{x_2 , x_1}(x_1,x_2),$$\n\nwhere $\\epsilon$ is the probability with wich the true $x_1$ is used.\n\nThe authors suggest starting training with $\\epsilon=1$ and annealing it so that by the end of the training $\\epsilon=0$. So as far as the eventual optimum of SS is concerned, we only have to focus on what the first term of the scoring rule does. The second term is the good old log-likelihood so we know that part works.\n\nAfter some math, one can show that scheduled sampling with a fixed $\\epsilon$ minimises the following divergence between the true $P$ and the model $Q$:\n\n$$D_{SS}[P\\|Q] = KL[P_{x_1}\\|Q_{x_1}] + (1-\\epsilon) \\mathbb{E}_{z\\sim Q_{x_1}} KL[P_{x_2}\\|Q_{x_2\\vert x_1=z}] + \\epsilon KL[P_{x_2\\vert x_1}\\|Q_{x_2\\vert x_1}]$$\n\nNow, if $\\epsilon=1$, we recover the Kullback-Leibler divergence between the joint $P_{x_1,x_2}$ and $Q_{x_1,x_2}$, which is what we expect as it corresponds to maximum likelihood estimation. However, as $\\epsilon$ is annealed to $0$, the objective function is somewhat strange, whereby the conditional distribution $Q_{x_2\\vert x_1}$ is pushed to model the marginal distribution $P_{x_2}$, instead of $P_{x_2\\vert x_1}$ as one would expect. One can therefore see that the factorised $Q^{*} = P_{x_1}P_{x_2}$ minimises this objective function.\n\n#### what this means for text modeling\n\nExtrapolating from the two variable case to longer sequences, one can see that the scheduled sampling objective would fail if minimised properly until convergence. Consider the case when the $\\epsilon\\approx 0$ stage is reached in the annealing schedule. Now consider what the RNN has to do to predict the $n$th character in a string during training. It is fed a random prefix string that was generated by itself but never seen any real data. Then the RNN has to give a probabilistic forecast of what the $n$th character in the training sentence is, having seen none of the previous characters in the sentence.\n\nThe optimal model that minimises this objective would completely ignore all the characters in the sentence so far, but keep a simple linear counter that indexes where it is within the sentence. Then it would emit a character from an index-specific marginal distribution of characters. This is the equivalent of the factorised trivial solution above.\n\nYes, such a model would be better at \"recovering from its own mistakes\", because at every character it would start independently from what it has generated so far. But this is at the cost of paying no attention whatsoever as to what the prefix of the sentence was. I believe the reason why this trivial behaviour was not observed in the paper is that the authors did not run the optimisation until convergence, and did not implement the full gradient of the objective function, as they discuss in the paper.\n\n#### Constructive part of criticism\n\n#### What to do instead of SS?\n\nSo the observed problem was that RNNs trained via maximum likelihood are unable to recover from their own mistakes early on in a sentence, when they are used to generate.\n\n`The main reason for the observed problem is that the log-likelihood is a local scoring rule`\n\nThe local property of scoring rules means that at training time we only ever evaluate the model $Q$ on actually observed datapoints. So if the RNN is faced with a prefix subsequence that was not in the dataset, God knows what it's going to complete that sentence with.\n\nThe proper (shall I say strictly proper) way to fix this issue is to use ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks"
    },
    "1018": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Huszar15",
        "transcript": "### Evaluating Generative Models\n\nA key topic I'm very interested in is the choices of objective functions used in unsupervised learning and generative models. The key organising principle should be this: the objective function we use for training a probabilistic model should match the way we ultimately want to use the model. Yet, in unsupervised learning this is often overlooked and I think we lack clarity around what the models are used for and how they should be trained and evaluated. This paper tries to clarify this a bit in the context of generative models. I also want to mention that another ICLR submission this year also deals with this fundamental question: I highly recommend taking a look.\n\nHere, I'm going to consider a narrow definition of generative models: models we actually want to use to generate samples from which are then shown to a human user/observer. This includes use-cases such as image captioning, texture generation, machine translation, speech synthesis and dialogue systems, but excludes things like unsupervised pre-training for supervised learning, semisupervised learning, data compression, denoising and many others. Very often people don't make this distinction clear when talking about generative models which is one of the reasons why there is still no clarity about what different objective functions do.\n\nI argue that when the goal is to train a model that can generate natural-looking samples, maximum likelihood is not a desirable training objective. Maximum likelihood is consistent so it can learn any distribution if it is given infinite data and a perfect model class. However, under model misspecification and finite data (that is, in pretty much every practically interesting scenario), it has a tendency to produce models that overgeneralise.\n\n#### KL divergence as a perceptual loss\n\nGenerative modelling is about finding a probabilistic model $Q$ that in some sense approximates the natural distribution of data $P$. When researchers (or users of their product) evaluate generative models for perceptual quality, they draw samples from it, then - for lack of a better word - eyeball the samples. In visual information processing this is often referred to as no-reference perceptual quality assessment \\citep[see e.,g.\\ ][]{wang2002noreference}. In the paper, I propose that the KL divergence $KL[Q\\| P]$ can be used as an idealised objective function to describe this scenario. This related to maximum likelihood which minimises $KL[P\\|Q]$, but different in fundamental ways which I will explain later.\n\nHere is why I think $KL[Q\\|P]$ should be used: First, we can make the assumption that the perceived quality of each sample is related to the \\emph{surprisal} $-\\log Q_{human}(x)$ under the human observers' subjective prior of stimuli $Q_{human}(x)$. For those of you not familiar with computational cognitive science, this will seem ad-hoc, but it's a relatively common assumption to make when modelling reaction times in experiments for example. We further assume that the human observer maintains a very accurate model of natural stimuli, thus, $Q_{human}(x) \\approx P(x)$. This is a fancy way of saying things like the observer being a native speaker therefore understanding all the nuances in language. These two assumptions suggest that in order to optimise our chances in this Turing test-like scenario, we need to minimise the following cross-entropy or perplexity term:\n\n\\begin{equation} - \\mathbb{E}_{x\\sim Q} \\log P(x) \\end{equation}\n\nThis perplexity is the exact opposite average negative log likelihood $- \\mathbb{E}_{x\\sim P} \\log Q(x)$, with the role of $P$ and $Q$ changed. However, the perplexity alone would be maximised by a model $Q$ that deterministically picks the most likely stimulus. To enforce diversity one can simultaneously try to maximise the Shannon entropy of $Q$. This leaves us with the following KL divergence to optimise:\n\n\\begin{equation} KL[Q\\| P] = - \\mathbb{E}{x\\sim Q} \\log P(x) + \\mathbb{E}{x\\sim Q} \\log Q(x) \\end{equation}\n\nSo if we want to train models that produce nice samples, my recommendation is to try to use $KL[Q\\|P]$ as an objective function or something that behaves like it. How does maximum likelihood compare?\n\n#### Differences between maximum likelihood and $KL[Q\\|P]$\n\nMaximum likelihood is roughly the same as minimising $KL[P\\|Q]$. The differences between minimising $KL[P\\|Q]$ and $K[Q\\|P]$ are well understood and it frequently comes up in the context of Bayesian approximate inference as well. Both divergences ensure consistency, minimising either converges to the true $P$ in the limit of infinite data and a perfect model class. However, they differ fundamentally in the way they deal with finite data and model misspecification (in almost every practical scenario):\n\n$KL[P\\|Q]$ tends to favour approximations $Q$ that overgeneralise $P$. If P is multimodal, the optimal $Q$ will tend to cover all the modes of $P$, even at the cost of introducing probability mass where $P$ has $0$ mass. Practically this means that the model will occasionally sample unplausible samples that don't look anything like samples from $P$.\n$KL[Q\\|P]$ tends to favour under-generalisation. The optimal $Q$ will typically describe the single largest mode of $P$ well, at the cost of ignoring other modes if they are hard to model without covering low-probability areas as well. Practically this means that $KL[Q\\|P]$ will try to avoid introducing unplausible samples, sometimes at the cost of missing the majority of plausible samples under $P$.\nIn other words: $KL[P\\|Q]$ is liberal, $KL[Q\\|P]$ is conservative. In yet other words: $KL[P\\|Q]$ is an optimist, $KL[Q\\|P]$ is a pessimist.\n\nThe problem of course is that $KL[Q\\|P]$ is super hard to optimise beased on a finite sample from $P$. Even harder than maximum likelihood. Not only that, the KL divergence is also not very well behaved, and is not well-defined unless $P$ is positive everywhere where $Q$ is positive. So there is little hope we can turn $KL[Q\\|P]$ into a practical training algorithm.\n\n#### Generalised Adversarial Training\n\nGenerative Adversarial Networks(GANs) train a generative model jointly with an adversarial discriminative model that tries to differentiate between artificial and real data. The idea is, a generative model is good if it can fool the best discriminative model into thinking the generated samples are real. GANs have produced some of the nicest looking samples you'll find on the Internet and got people very excited about generative models again: human faces, album covers, etc.\n\nHow do they come into this picture? It's because they can be understood as approximately minimising the Jensen-Shannon divergence:\n\n\\begin{equation} JSD[P\\|Q] = JSD[P\\|Q] = \\frac{1}{2}KL\\left[P\\middle\\|\\frac{P+Q}{2}\\right] + \\frac{1}{2}KL\\left[Q\\middle\\|\\frac{P+Q}{2}\\right]. \n\\end{equation}\n\nLooking at the equation above you can immediately see how it's related to this topic. JS divergence is a bit like a symmetrised version of KL divergence. It's not $KL[P\\|Q]$, not $KL[Q\\|P]$, but a bit of both. So one can expect that minimising JS divergence would exhibit a behaviour that is kind of halfway between the two extremes explained above. And that means that they would generate better samples than methods trained via maximum likelihood and similar objectives.\n\nWhat's more, one can generalise JS divergence to a whole family of divergences, parametrised by a probability $0<\\pi<1$ as follows:\n\n\\begin{equation} JS_{\\pi}[P\\|Q] = \\pi \\cdot KL[P\\|\\pi P+(1-\\pi)Q] + (1-\\pi)KL[Q\\|\\pi P+(1-\\pi)Q]. \n\\end{equation}\n\nWhat I show in the paper is that by varrying $\\pi$ between the two extremes, one can effectively interpolate between the behaviour of maximum likelihood ($\\pi\\rightarrow 0$) and minimising $KL[Q\\|P]$ ($\\pi\\rightarrow 1$). See the paper for details. This interpolation between behaviours is explained in this main figure below:\n\n![](http://www.inference.vc/content/images/2015/11/Screen-Shot-2015-11-16-at-16-19-10.png)\n\nFor any given value of $\\pi$, we can optimise $JS_{\\pi}$ approximately using an algorithm that is a slightly changed version of the original GAN algorithm. This is because the generalised JS divergence still has an elegant information theoretic interpretation. Consider a communications channel on which we can transmit a single data point of some kind. We toss a coin and with probability $\\pi$, we send a sample from $P$, and with probability $1-\\pi$ we send a sample from $Q$ instead. The receiver doesn't know the outcome of the coinflip, she only observes the sample. The $JS_{\\pi}$ is the mutual information between the observed sample and the coinflip. It is also an upper bound on how well any algorithm can do in guessing the coinflip from the observed sample.\n\nTo implement an adversarial training algorithm for $JS_{\\pi}$ one simply needs to change the ratio of samples the discriminative network sees from $Q$ vs $P$ (or apply appropriate weights during training). In the original method the discriminator network is faced with a balanced classification problem, i.e. $\\pi=\\frac{1}{2}$. It is hard to believe, but this irrelevant-looking modification changes the behaviour of the GAN algorithm dramatically, and can in theory allow the GAN algorithm to approximate both maximum likelihood or $KL[Q\\|P]$.\n\nThis analysis explains why GANs have been so successful in generating very nice looking images, and relatively few weird-looking ones. It is also worth pointing out that the GAN method is still in its infantcy and has many issues and limitations. The main issue is that it is based on sampling from $Q$ which doesn't work well in high dimensions. Hopefully some of these limitations can be overcome and then we should have a pretty powerful framework for training good generative models.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05101"
    },
    "1019": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Turner16a",
        "transcript": "#### Explainability-Accuracy trade-off\n\nMany practical applications of machine learning systems call for the ability to explain why certain predictions are made. Consider a fraud detection system: it is not very useful for a user to see a list of possible fraud attempts without any explanation why the system thought the attempt was fraud. You want to say something like 'the system thinks it's fraud because the credit card was used to make several transactions that are smaller than usual'. But such explanations are not always compatible with our machine learning model. Or are they?\n\nWhen choosing a machine learning model we usually think in terms of two choices:\n\n- accurate but black-box: The best classification accuracy is typically achieved by black-box models such as Gaussian processes, neural networks or random forests, or complicated ensembles of all of these. Just look at the kaggle leaderboards. These are called black-box and are often criticised because their inner workings are really hard to understand. They don't, in general, provide a clear explanation of the reasons they made a certain prediction, they just spit out a probability.\n\n- white-box but weak: On the other end of the spectrum, models whose predictions are easy to understand and communicate are usually very impoverished in their predictive capacity (linear regression, a single decision tree) or are inflexible and computationally cumbersome (explicit graphical models).\n\nSo which ones should we use: accurate black-box models, or less accurate but easy-to-explain white-box models?\n\nThe paper basically tells us that this is a false tradeoff. To summarise my take-home from this poster in one sentence:\n\n`Explainability is not a property of the model`\n\nRyan presents a nice way to separate concerns of predictive power and explanation generation. He does this by introducing a formal framework in which simple, human-readable explanations can be generated for any black-box classifier, without assuming anything about the internal workings of the classifier.\n\nIf you think about it, it makes sense. If you watch someone playing chess, you can probably post-rationalise and give a reason why the person might think it's a good move. But you probably don't have an idea about the algorithm the person was executing in his brain.\n\nNow we have a way to explain why decisions were made by complex systems, even if that explanation is not an exact explanation of what the classifier algorithm actually did. This is super-important in applications such as face recognitions where the only models that seem to work today are large black-box models. As (buzzword alert) AI-assisted decision making is becoming commonplace, the ability to generate simple explanations for black-box systems is going to be super important, and I think Ryan has made some very good observations in this paper.\n\nI recommend everyone to take a look, I'm definitely going to see the world a bit differently after reading this.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.09517"
    },
    "1020": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MakhzaniSJG15",
        "transcript": "#### Summary of this post: \n\n* an overview the motivation behind adversarial autoencoders and how they work * a discussion on whether the adversarial training is necessary in the first place. tl;dr: I think it's an overkill and I propose a simpler method along the lines of kernel moment matching.\n\n#### Adversarial Autoencoders\n\nAgain, I recommend everyone interested to read the actual paper, but I'll attempt to give a high level overview the main ideas in the paper. I think the main figure from the paper does a pretty good job explaining how Adversarial Autoencoders are trained:\n\n![](http://www.inference.vc/content/images/2016/01/Screen-Shot-2016-01-08-at-14-48-25.png)\n\nThe top part of this image is a probabilistic autoencoder. Given the input $\\mathbf{x}$, some latent code $\\mathbf{z}$ is generated by sampling from an encoding distribution $q(\\mathbf{z}\\vert\\mathbf{x})$. This distribution is typically modeled as the output a deep neural network. In normal autoencoders this encoder would be deterministic, now we allow it to be probabilistic.\n\nA decoder network is then trained to decode $\\mathbf{z}$ and reconstruct the original input $\\mathbf{x}$. Of course, reconstruction will not be perfect, but we train the networks to minimise reconstruction error, this is typically just mean squared error.\n\nThe reconstruction cost ensures that the encoding process retains information about the input image, but it doesn't enforce anything else about what these latent representations $\\mathbf{z}$ should do. In general, their distribution is described as the aggregate posterior $q(\\mathbf{z})=\\mathbb{E}_\\mathbf{x} q(\\mathbf{z}\\vert\\mathbf{x})$. Often, we would like this distribution to match a certain prior $p(\\mathbf{z})$. For example. we may want $\\mathbf{z}$ to have independent components and Gaussian distributed (nonlinear ICA,PCA). Or we may want to force the latent representations to correspond to discrete class labels, or binary factors. Or we may simply want to ensure there are 'no gaps' in the latent space, and any random $\\mathbf{z}$ would lead to a viable sample when squashed through the decoder network.\n\nSo there are multiple reasons why one might want to control the aggregate posterior $q(\\mathbf{z})$ to match a predefined prior $p(\\mathbf{z})$. The authors achieve this by introducing an additional term in the autoencoder loss function, one that measures the divergence between $q$ and $p$. The authors chose to do this via adversarial training: they train a discriminator network that constantly learns to discriminate between real code vectors $\\mathbb{z}$ produced by encoding real data, and random code vectors sampled from $p$. If $q$ matches $p$ perfectly, the optimal discriminator network should have a large classification error.\n\n#### Is this an overkill?\n\nMy main question about this paper was whether the adversarial cost is really needed here, because I think it's an overkill. Let me explain:\n\nAdversarial training is powerful when all else fails to quantify divergence between complicated, potentially degenerate distributions in high dimensions, such as images or video. Our toolkit for dealing with images is limited, CNNs are the best tool we have, so it makes sense to incorporate them in training generative models for images. GANs - when applied directly to images - are a great idea.\n\nHowever, here adversarial training is applied to an easier problem: to quantify the divergence between a simple, fixed prior (e.g. Gaussian) and an empirical distribution of latents. The latent space is usually lower-dimensional, distributions better behaved. Therefore, matching to $p(\\mathbf{z})$ in latent space should be considerably easier than matching distributions over images.\n\nAdversarial training makes no assumptions about the distributions compared, other than sampling from them. This comes very handy when both $p$ and $q$ are nasty such as in the generative adversarial network scenario: there, $p$ is the distribution of natural images, $q$ is a super complicated, degenerate distribution produced by squashing noise through a deep convnet. The price we pay for this flexibility is this: when $p$ or $q$ are actually easy to work with, adversarial training cannot exploit that, it still has to sample. (it would be interesting to see if expectations over $p(\\mathbf{z})$ could be computed analytically). So even though in this work $p$ is as simple as a mixture of ten 2D Gaussians, we need to approximate everything by drawing samples.\n\n#### Other things might work: kernel moment matching\n\nWhy can\u2019t one use easier divergences? For example, I think moment matching based on kernel MMD would work brilliantly in this scenario. It would have the following advantages over the adversarial cost.\n\n- closed form expressions: Depending on the choice of the prior $p(\\mathbf{z})$ and kernel used in MMD, the expectations over $p$ may be available in closed form, without sampling. So for example if we use a squared exponential kernel and a mixture of Gaussians as $p$, the divergence from $p$ can be precomputed in closed form that is easy to evaluate.\n\n- no nasty inner loop: Adversarial training requires the discriminator network to be reoptimised every time the generative model changes. So we end up with a gradient descent in the inner loop of a gradient descent, which is anything but nice to work with. This is why it takes so long to get it working, the whole thing is pretty unstable. In contrast, to evaluate MMD, the inner loop is not needed. In fact, MMD can also be thought of as the solution to a convex maximisation problem, but via the kernel trick the maximum has a closed form solution.\n\n- the problem is well suited for MMD: because the distributions are smooth, and the space is nice and low-dimensional, MMD might work very well. Kernel-based methods struggle with complicated manifold-like structure of natural images, so I wouldn't expect MMD to be competitive with adversarial training if it is applied directly in the image space. Therefore, I actually prefer generative adversarial networks to generative moment matching networks. However, here we have an easier problem, simpler space, simpler distributions where MMD shines, and adversarial training is just not needed.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05644"
    },
    "1021": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/NorooziF16",
        "transcript": "The core idea behind this paper is powerfully simple. The goal is to learn useful representations from unlabelled data, useful in the sense of helping supervised object classification. This paper does this by chopping up images into a set of little jigsaw puzzle, scrambling the pieces, and asking a deep neural network to learn to reassemble them in the correct constellation. A picture is worth a thousand words, so here is Figure 1 taken from the paper explaining this visually:\n\n![](http://www.inference.vc/content/images/2016/04/Screen-Shot-2016-04-25-at-08-26-45-1.png)\n\n#### Summary of this note:\n\n- I note the similarities to denoising autoencoders, which motivate my question: \"Can this method equivalently represent the data generating distribution?\"\n- I consider a toy example with just two jigsaw pieces, and consider using an objective like this to fit a generative model $Q$ to data\n- I show how the jigsaw puzzle training procedure corresponds to minimising a difference of KL divergences\n- Conclude that the algorithm ignores some aspects of the data generating distribution, and argue that in this case it can be a good thing\n\n#### What does this learn do?\n\nThis idea seems to make a lot of sense. But to me, one interesting question is the following:\n\n`What does a network trained to solve jigsaw puzzle learn about the data generating distribution?`\n\n#### Motivating example: denoising autoencoders\n\nAt one level, this jigsaw puzzle approach bears similarities to denoising autoencoders (DAEs): both methods are self-supervised: they take an unlabelled data and generate a synthetic supervised learning task. You can also interpret solving jigsaw puzzles as a special case 'undoing corruption' thus fitting a more general definition of autoencoders (Bengio et al, 2014).\n\nDAEs have been shown to be related to score matching (Vincent, 2000), and that they learn to represent gradients of the log-probability-density-function of the data generating distribution (Alain et al, 2012). In this sense, autoencoders equivalently represent a complete probability distribution up to normalisation. This concept is also exploited in Harri Valpola's work on ladder networks (Rasmus et al, 2015)\n\nSo I was curious if I can find a similar neat interpretation of what really is going on in this jigsaw puzzle thing. If you equivalently represent all aspects of a probability distribution this way? Are there aspects that this representation ignores? Would it correspond to a consistent denisty estimation/generative modelling method in any sense? Below is my attempt to figure this out.\n\n#### A toy case with just two jigsaw pieces\n\nTo simplify things, let's just consider a simpler jigsaw puzzle problem with just two jigsaw positions instead of 9, and in such a way that there are no gaps between puzzle pieces, so image patches (now on referred to as data $x$) can be partitioned exactly into pieces.\n\nLet's assume our datapoints $x^{(n)}$ are drawn i.i.d. from an unknown distribution $P$, and that $x$ can be partitioned into two chunks $x_1$ and $x_2$ of equivalent dimensionality, such that $x=(x_1,x_2)$ by definition. Let's also define the permuted/scrambled version of a datapoint $x$ as $\\hat{x}:=(x_2,x_1)$.\n\nThe jigsaw puzzle problem can be formulated in the following way: we draw a datapoint $x$ from $P$. We also independently draw a binary 'label' $y$ from a Bernoulli($\\frac{1}{2}$) distribution, i.e. toss a coin. If $y=1$ we set $z=x$ otherwise $z=\\hat{x}$.\n\nOur task is to build a predictior, $f$, which receives as input $z$ and infers the value of $y$ (outputs the probability that its value is $1$). In other words, $f$ tries to guess the correct ordering of the chunks that make up the randomly scrambled datapoint $z$, thereby solving the jigsaw puzzle. The accuracy of the predictor is evaluated using the log-loss, a.k.a. binary cross-entropy, as it is common in binary classification problems:\n\n$$ \\mathcal{L}(f) = - \\frac{1}{2} \\mathbb{E}_{x\\sim P} \\left[\\log f(x) + \\log (1 - f(\\hat{x}))\\right] $$\n\nLet's consider the case when we express the predictor $f$ in terms of a generative model $Q$ of $x$. $Q$ is an approximation to $P$, and has some parameters $\\theta$ which we can tweak. For a given $\\theta$, the posterior predictive distribution of $y$ takes the following form:\n\n$$ f(z,\\theta) := Q(y=1\\vert z;\\theta) = \\frac{Q(z;\\theta)}{Q(z;\\theta) + Q(\\hat{z};\\theta)}, \n$$\n\nwhere $\\hat{z}$ denotes the scrambled/permuted version of $z$. Notice that when $f$ is defined this way the following property holds:\n\n$$ 1 - f(\\hat{x};\\theta) = f(x;\\theta), \n$$\n\nso we can simplify the expression for the log-loss to finally obtain:\n\n\\begin{align} \\mathcal{L}(\\theta) &= - \\mathbb{E}_{x \\sim P} \\log Q(x;\\theta) + \\mathbb{E}_{x \\sim P} \\log [Q(x;\\theta) + Q(\\hat{x};\\theta)]\\\\ &= \\operatorname{KL}[P\\|Q_{\\theta}] - \\operatorname{KL}\\left[P\\middle\\|\\frac{Q_{\\theta}(x)+Q_{\\theta}(\\hat{x})}{2}\\right] + \\log(2) \\end{align}\n\nSo we already see that using the jigsaw objective to train a generative model reduces to minimising the difference between two KL divergences. It's also possible to reformulate the loss as:\n\n$$ \\mathcal{L}(\\theta) = \\operatorname{KL}[P\\|Q_{\\theta}] - \\operatorname{KL}\\left[\\frac{P(x) + P(\\hat{x})}{2}\\middle\\|\\frac{Q_{\\theta}(x)+Q_{\\theta}(\\hat{x})}{2}\\right] - \\operatorname{KL}\\left[P\\middle\\|\\frac{P(x) + P(\\hat{x})}{2}\\right] + \\log(2) $$\n\nLet's look at what the different terms do:\n\n- The first term is the usual KL divergence that would correspond to maximum likelihood. So it just tries to make $Q$ as close to $P$ as possible.\n- The second term is a bit weird, particularly as comes into the formula with a negative sign. The KL divergence is $0$ if $Q$ and $P$ define the distribution over the set of jigsaw pieces ${x_1,x_2}$. Notice, I used set notation here, so the ordering does not matter. In a way this term tells the loss function: don't bother modelling what the jigsaw pieces are, only model how they fit together.\n- The last terms are constant wrt. $\\theta$ so we don't need to worry about them.\n\n#### Not a proper scoring rule (and it's okay)\n\nFrom the formula above it is clear that the jigsaw training objective wouldn't be a proper scoring rule, as it is not uniquely minimised by $Q=P$ in all cases. Here are two counterexamples where the objective fails to capture all aspects of $P$:\n\nConsider the case when $P=\\frac{P(x) + P(\\hat{x})}{2}$, that is, when $P$ is completely insensitive to the ordering of jigsaw pieces. As long as $Q$ also has this property $Q = \\frac{Q(x) + Q(\\hat{x})}{2}$, all KL divergences are $0$ and the jigsaw objective is constant with respect $\\theta$. Indeed, it is impossible in this case for any classifier to surpass chance level in the jigsaw problem.\n\nAnother simple example to consider is a two-dimensional binary $P$. Here, the jigsaw objective only cares about the value of $Q(0,1)$ relative to $Q(1,0)$, but completely insensitive to $Q(1,1)$ and $Q(0,0)$. In other words, we don't really care how often $0$s and $1$s appear, we just want to know which one is likelier to be on the left or the right side.\n\nThis is all fine because we don't use this technique for generative modelling. We want to use it for representation learning, so it's okay to ignore some aspects of $P$.\n\n#### Representation learning\n\nI think this method seems like a really promising direction for unsupervised representation learning. In my mind representation learning needs either:\n\n- strong prior assumptions about what variables/aspects of data are behaviourally relevant, or relevant to the tasks we want to solve with the representations.\n- at least a little data in a semi-supervised setting to inform us about what is relevant and what is not.\n\nDenoising autoencoders (with L2 reconstruction loss) and maximum likelihood training try to represent all aspects of the data generating distribution, down to pixel level. We can encode further priors in these models in various ways, either by constraining the network architecture or via probabilistic priors over latent variables.\n\nThe jigsaw method encodes prior assumptions into the training procedure: that the structure of the world (relative position of parts) is more important to get right than the low-level appearance of parts. This is probably a fair assumption.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.09246"
    },
    "1022": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/YuK15",
        "transcript": "- I give an overview of the paper which proposes an exponential schedule of dilated convolutional layers as a way to combine local and global knowledge\n- I point out the connection between 2D dilated convolutions and Kronecker products\n- cascades of exponentially dilated convolutions - as proposed in the paper - can be thought of as parametrising a large convolution kernel as a Kronecker product of small kernels\n- the relationship to Kronecker factorisation only holds under particular assumptions, in this sense cascades of exponenetially diluted convolutions are a generalisation of the Kronecker layer (Zhou et al. 2015)\n- I note that dilated convolutions are equivariant under image translation, a property that other multi-scale architectures often violate.\n\n#### Background\n\nThe key application the dilated convolution authors have in mind is dense prediction: vision applications where the predicted object that has similar size and structure to the input image. For example, semantic segmentation with one label per pixel; image super-resolution, denoising, demosaicing, bottom-up saliency, keypoint detection, etc.\n\nIn many such applications one wants to integrate information from different spatial scales and balance two properties:\n\n1. local, pixel-level accuracy, such as precise detection of edges, and\n2. integrating knowledge of the wider, global context\n\nTo address this problem, people often use some kind of multi-scale convolutional neural networks, which often relies on spatial pooling. Instead the authors here propose using layers dilated convolutions, which allow us to address the multi-scale problem efficiently without increasing the number of parameters too much.\n\n#### Dilated Convolutions\n\nIt's perhaps useful to first note why vanilla convolutions struggle to integrate global context. Consider a purely convolutional network composed of layers of $k\\times k$ convolutions, without pooling. It is easy to see that size of the receptive field of each unit - the block of pixels which can influence its activation - is $l*(k-1)+k$, where $l$ is the layer index. So the effective receptive field of units can only grow linearly with layers. This is very limiting, especially for high-resolution input images.\n\nDilated convolutions to the rescue! The dilated convolution between signal $f$ and kernel $k$ and dilution factor $l$ is defined as:\n\n$$ \\left(k \\ast_{l} f\\right)_t = \\sum_{\\tau=-\\infty}^{\\infty} k_\\tau \\cdot f_{t - l\\tau} $$\n\nNote that I'm using slightly different notation than the authors. The above formula differs from vanilla convolution in last subscript $f_{t - l\\tau}$. For plain old convolution this would be $f_{t - \\tau}$. In the dilated convolution, the kernel only touches the signal at every $l^{th}$ entry. This formula applies to a 1D signal, but it can be straightforwardly extended to 2D convolutions.\n\nThe authors then build a network out of multiple layers of diluted convolutions, where the dilation factor $l$ increases exponentially at each layer. When you do that, even though the number of parameters grows only linearly with layers, the effective receptive field of units grows exponentially with layer depth. This is illustrated in the figure below:\n\n![](http://www.inference.vc/content/images/2016/05/Screen-Shot-2016-05-12-at-09-47-12.png)\n\nWhat this figure doesn't really show is the parameter sharing and parameter dependencies across the receptive field (frankly, it's pretty hard to visualise exactly with more than 2 layers). The receptive field grows at a faster rate than the number of parameters, and it is obvious that this can only be achieved by introducing additional constraints on the parameters across the receptive field. The network won't be able to learn arbitrary receptive field behaviours, so one question is, how severe is that restriction?\n\n#### Relationship to Kronecker Products\n\nTo me this whole dilated convolution paper cries Kronecker product, although this connection is never made in the paper itself. It's easy to see that a 2D dilated convolution with matrix/filter $K$ is the same as vanilla convolution with a diluted filter $\\hat{K}_{l}$ which can be represented as the following Kronecker product:\n\n$$ \\hat{K}_l = K \\otimes \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\ \n0 & 0 & \\ddots & & 0 \\\\ \n0 & \\ddots & \\ddots & \\ddots & \\\\ \n0 & & \\ddots & \\ddots & 0 \\\\ \n0 & 0 & 0 & 0 & 0 \n\\end{bmatrix} $$\n\nUsing this, and properties of convolutions and Kronecker products (I suggest beginners to make extensive use of the matrix cookbook) we can even understand something about exponentially iterated dilated convolutions.\n\nLet's assume we apply several layers of dilated convolutions, without nonlinearity, as in Equation 3 of the paper. For simplicity, I assume that that all convolution kernels $K_l, L=1\\ldots L$ are $a\\times a$ in size, the dilation factor at layer $l$ is $a^{l}$, and we only have a single channel throughout ($C=1$). In this case we can show that:\n\n$$ F_{L+1} = K_L \\ast_{a^L} \\left( K_{L-1} \\ast_{a^{(L-1)}} \\left( \\cdots K_1 \\ast_{a} \\left( K_0 \\ast F_0 \\right) \\cdots \\right) \\right) = \\left( K_L \\otimes K_{L-1} \\otimes \\cdots \\otimes K_{0} \\right) \\ast F_0 \n$$\n\nThe left-hand side of this equation is the same construction as in Equation 3 in the paper, but expanded. The right hand side is a single vanilla convolution, but with a convolution kernel that is constructed as the Kronecker product of all the $a\\times a$ kernels $K_l$.\n\nIt turns out Kronecker-factored parametrisations of convolution tensors are already used in CNNs, a quick googling revealed this paper:\n\nShuchang Zhou, Jia-Nan Wu, Yuxin Wu, Xinyu Zhou (2015) Exploiting Local Structures with the Kronecker Layer in Convolutional Networks\nWhat can Kronecker-factored filters represent?\n\nLet's look at what kind of kernels can we represent with Kronecker products, and hence what behaviour should we expect from dilated convolutions. Here are a few examples of $27\\times 27$ kernels that result from taking the Kronecker product of three random $3\\times 3$ kernels:\n\n![](http://www.inference.vc/content/images/2016/05/VzORx0FEfAAAAAElFTkSuQmCC.png)\n\nThese look somehow natural, at least to me. They look like pretty plausible texture patches taken from some pixellated video game. You will notice the repeated patterns and the hierarchical structure. Indeed, we can draw cool self-similar fractal-like filters if we keep taking the Kronecker product of the same kernel with itself, some examples of such random fractals:\n\n![](http://www.inference.vc/content/images/2016/05/YSJIkSZIkLYw3bCRJkiRJkhbGGzaSJEmSJEkL8zeSmRmMrhHPQgAAAABJRU5ErkJggg--.png)\n\nI would say these kernels are not entirely unreasonable for a ConvNet, and if you allow for multiple channels ($C>1$) they can represent pretty nice structured patterns and shapes with reasonable number of parameters.\n\nCompare these filters to another common technique for reducing parameters of convolution tensors: low-rank decompositions (see e.g. Lebedev et al, 2014). Spatially, a low-rank approximation to a square 2D convolution filter can be understood as subsequently applying two smaller rectangular filters: one with a limited horizontal extent and one with limited vertical extent. Here are a few random samples of $27\\times 27$ filters with a rank of 1. These can be represented using the same number of parameters (27) as the Kronecker samples above.\n\n\n\nTo me, these don't look so natural. Notice also that for low-rank representations the number of parameters has to scale linearly with the spatial extent of the filter, whereas this scaling can be logarithmic if we use a Kronecker parametrisation. This is the real deal when using Kronecker products or dilated convolutions.\n\nHere is another cool illustration of the naturalness of the Kronecker approximation, taken out of the Kronecker layer paper:\n\n![](http://www.inference.vc/content/images/2016/05/Screen-Shot-2016-05-12-at-14-58-33.png)\n\nSo in general, parametrising convolution kernels as Kronecker-products seems like a pretty good idea. The dilated convolutions paper presents a more flexible approach than just Kronecker-factors. Firstly, you can add nonlinearities after each layer of dilated convolution, which would now be different from Kronecker products. Secondly, the Kronecker analogy only holds if the dilation factor and the kernel size are the same. In the paper the authors used a kernel size of $3$ and dilation factor of $2$.\n\n#### Final note on translational equivariance\n\nOne desirable property of convolutions is that they are translationally equivariant: if you shift the input image by any amount, the output remains the same, shifted by the same amount. This is a very useful inductive bias/prior assumtion to use in a dense prediction task.\n\nOne way to introduce multiscale thinking to ConvNets is to use architectures that look like the figure below: we first decrease the spatial extent of feature-maps via pooling, then grow them back again via unpooling/deconvolution. Additional shortcut connections ensure that pixel-level local accuracy can be retained. The example below is from the SegNet paper, but there are multiple other papers such as this one on recombinator networks.\n\n![](http://www.inference.vc/content/images/2016/05/conv-deconv.png)\n\nHowever, as soon as you include spatial pooling, the translational equivariance property of the whole network might break. For example the SegNet above is not translationally equivariant anymore: the network's predictions are sensitive to small, single-pixel shifts to the input image, which is undesirable. Thankfully, layers of dilated convolutions are still translationally equivariant, which is a good thing.\n\n#### Summary\n\nThis dilated convolutions idea is pretty cool, and I think these papers are just scratching the surface of this topic. The dilated convolution architecture generalises Kronecker-factored convolutional filters, it allows for very large receptive fields while only growing the number o",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.07122"
    },
    "1023": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SalimansGZCRC16",
        "transcript": "Summary of this post\n\n - How does this minibatch discrimination heuristic work and how does it change the behaviour of the GAN algorithm? Does it change the underlying objective function that is being minimized?\n - the answer is: for the original GAN algorithm that minimises Jensen-Shannon divergence it does change the behaviour in a non-trivial way. One side-effect is assigning a higher relative penalty for low-entropy generators.\n - when using the blended update rule from here, the algorithm minimises the reverse KL-divergence. In this case, using minibatch discrimination leaves the underlying objective unchanged: the algorithm can still be shown to miminise KL divergence.\n - even if the underlying objectives remain the same, using minibatch discrimination may be a very good idea practically. It may stabilise the algorithm by, for example, providing a lower variance estimator to log-probability-ratios.\n\n[Here is the ipython/jupyter notebook](https://gist.github.com/fhuszar/a91c7d0672036335c1783d02c3a3dfe5) I used to draw the plots and test some of the things in this post in practice.\n\n### What is minibatch discrimination?\n\nIn the vanilla Generative Adversarial Networks (GAN) algorithm, a discriminator is trained to tell apart generated synthetic examples from real data. One way GAN training can fail is to massively undershoot the entropy of the data-generating distribution, and concentrate all it's parameters on generating just a single or few examples.\n\nTo remedy this, the authors play with the idea of discriminating between whole minibatches of samples, rather than between individual samples. If the generator has low entropy, much lower than real data, it may be easier to detect this with a discriminator that sees multiple samples.\n\nHere, I'm going to look at this technique in general: modifying an unsupervised learning algorithm by replacing individual samples with i.i.d. minibatches of samples. Note, that this is not exactly what the authors end up doing in the paper referenced above, but it's an interesting trick to think about.\n\n#### How does the minibatch heuristic effect divergences?\n\nThe reason I'm so keen on studying GANs is the connection to principled information theoretic divergence criteria. Under some assumptions, it can be shown that GANs minimise the Jensen-Shannon (JS) divergence, or with a slight modification the reverse-KL divergence. In fact, a recent paper showed that you can use GAN-like algorithms to minimise any $f$-divergence.\n\nSo my immediate question looking at the minibatch discrimination idea was: how does this heuristic change the divergences that GANs minimise.\n\n#### KL divergence\n\nLet's assume we have any algorithm (GAN or anything else) that minimises KL divergence $\\operatorname{KL}[P\\|Q]$ between two distributions $P$ and $Q$. Let's now modify this algorithm so that instead of looking at distributions $P$ and $Q$ of a single sample $x$, it looks at distributions $P^{(N)}$ and $Q^{(N)}$ of whole a minibatch $(x_1,\\ldots,x_N)$. I use $P^{(N)}$ to denote the following distribution:\n\n$$ P^{(N)}(x_1,\\ldots,x_N) = \\prod_{n=1}^N P(x_n) \n$$\n\nThe resulting algorithm will therefore minimise the following divergence:\n\n$$ d[P\\|Q] = \\operatorname{KL}[P^{(N)}\\|Q^{(N)}] \n$$\n\nIt is relatively easy to show why this divergence $d$ behaves exactly like the KL divergence between $P$ and $Q$. Here's the maths for minibatch size of $N=2$:\n\n\\begin{align} d[P\\|Q] &= \\operatorname{KL}[P^{(2)}\\|Q^{(2)}] \\\\ \n&= \\mathbb{E}_{x_1\\sim P,x_2\\sim P}\\log\\frac{P(x_1)P(x_2)}{Q(x_1)Q(x_2)} \\\\ &= \\mathbb{E}_{x_1\\sim P,x_2\\sim P}\\log\\frac{P(x_1)}{Q(x_1)} + \\mathbb{E}_{x_1\\sim P,x_2\\sim P}\\log\\frac{P(x_2)}{Q(x_2)} \\\\ &= \\mathbb{E}_{x_1\\sim P}\\log\\frac{P(x_1)}{Q(x_1)} + \\mathbb{E}_{x_2\\sim P}\\log\\frac{P(x_2)}{Q(x_2)} \\\\ &= 2\\operatorname{KL}[P\\|Q] \\end{align}\n\nIn full generality we can say that:\n\n$$ \\operatorname{KL}[P^{(N)}\\|Q^{(N)}] = N \\operatorname{KL}[P\\|Q] $$\n\nSo changing the KL-divergence to minibatch KL-divergence does not change the objective of the training algorithm at all. Thus, if one uses minibatch discrimination with the blended training objective, one can rest assured that the algorithm still performs approximate gradient descent on the KL divergence. It may still work differently in practice, for example by reducing the variance of the estimators involved.\n\nThis property of the KL divergence is not surprising if one considers its compression/information theoretic definition: the extra bits needed to compress data drawn from $P$, using model $Q$. Compressing a minibatch of i.i.d. samples corresponds to compressing the samples independently. Their codelengths would add up linearly, hence KL-divergences add up linearly, too.\n\n#### JS divergence\n\nThe same thing does not hold for the JS-divergence. Generally speaking, minibatch JS divergence behaves differently from ordinary JS-divergence. Instead of equality, for JS divergences the following inequality holds:\n\n$$ JS[P^{(N)}\\|Q^{(N)}] \\leq N \\cdot JS[P\\|Q] \n$$\n\nIn fact for fixed $P$ and $Q$, $JS[P^{(N)}\\|Q^{(N)}]/N$ is monotonically non-increasing. This can be seen intuitively by considering the definition of JS divergence as the mutual information between the samples and the binary indicator $y$ of whether the samples were drawn from $Q$ or $P$. Using this we have that:\n\n\\begin{align} \\operatorname{JS}[P^{(2)}\\|Q^{(2)}] &= \\mathbb{I}[y ; x_1, x_2] \\\\ &= \\mathbb{I}[y ; x_1] + \\mathbb{I}[y ; x_2 \\vert x_1] \\\\ &\\leq \\mathbb{I}[y ; x_1] + \\mathbb{I}[y ; x_2] \\\\ &= 2 \\operatorname{JS}[P\\|Q] \\end{align}\n\nBelow I plotted the minibatch-JS-divergence $JS[P^{(N)}\\|Q^{(N)}]$ for various minibatch-sizes $N=1,2,3,8$, between univariate Bernoulli distributions with parameters $p$ and $q$. For the plots below, $p$ is kept fixed at $p=0.2$, and the parameter $q$ is varied between $0$ and $1$.\n\n![](http://www.inference.vc/content/images/2016/06/fr9ra2qB2t9stl8sVnqIAAH1GqAAAhM3YsWOVkpKiTz75RD6fL9D-0UcfhbEqAEBfRYe7AABA5DIMQ-vXr9eLL76op556So8--qjOnDmjsrIyGYYR7vIAAL3ESAUAIKxmzpyp9957T52dndqyZYuOHTum4uJi7v4EAIMIIxUAgLCbPn26pk-fHtj2er1hrAYA0FeMVAAAAAAICaECAAAAQEgIFQCAm5JhGFysDQCDhOHnSjgAAAAAIWCkA.png)\n\nYou can see that all divergences have a unique global minimum around $p=q=0.2$. However, their behaviour at the tails changes as the minibatch-size increases. This change in behaviour is due to saturation: JS divergence is upper bounded by $1$, which corresponds to 1 bit of information. If I continued increasing the minibatch-size (which would blow up the memory footprint of my super-naive script), eventually the divergence would reach $1$ almost everywhere except for a dip down to $0$ around $p=q=0.2$.\n\nBelow are the same divergences normalised to be roughly the same scale.\n\n![](http://www.inference.vc/content/images/2016/06/Rle8idcTS3gAAAABJRU5ErkJggg--.png)\n\nThe problem of GANs that minibatch discrimination was meant to fix is that it favours low-entropy solutions. In this plot, this would correspond to the $q<0.1$ regime. You can argue that as the batch-size increases, the relative penalty for low-entropy approximations $q<0.1$ do indeed decrease when compared to completely wrong solutions $q>0.5$. However, the effect is pretty subtle.\n\nBonus track: adversarial preference loss\n\nIn this context, I also revisited the adversarial preference loss. Here, the discriminator receives two inputs $x_1$ and $x_2$ (one synthetic, one real) and it has to decide which one was real.\n\nThis algorithm, too, can be related to the minibatch discrimination approach, as it minimises the following divergence:\n\n$$ d(P,Q) = d(P\\times Q\\|Q\\times P), \n$$\n\nwhere $P\\times Q(x_1,x_2) = P(x_1)Q(x_2)$. Again, if $d$ is the $KL$ divergence, the training objective boils down to the same thing as the original GAN. However, if $d$ is the JS divergence, we will end up minimising something weird, $\\operatorname{JS}[Q\\times P\\| P\\times Q]$",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.03498"
    },
    "1024": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HyvarinenM16",
        "transcript": "\"Aapo did it again!\" - I exclaimed while reading this paper yesterday on the train back home (or at least I thought I was going home until I realised I was sitting on the wrong train the whole time. This gave me a couple more hours to think while traveling on a variety of long-distance buses...)\n\nAapo Hyv\u00e4rinen is one of my heroes - he did tons of cool work, probably most famous for pseudo-likelihood, score matching and ICA. His recent paper, brought to my attention by my brand new colleague Hugo Larochelle, is similarly excellent:\n\n### Summary\n\nTime-contrastive learning (TCL) trains a classifier that looks at a datapoint and guesses what part of the time series it came from.\nit exploits nonstationarity of time series to help representation learning\nan elegant connection to generative models (nonlinear ICA) is shown, although the assumptions of the model are pretty limiting\nTCL is the temporal analogue of representation learning with jigsaw puzzles\nsimilarly to GANs, logistic regression is deployed as a proxy to learn log-likelihood ratios directly from data\nTime-contrastive learning\n\nTime-contrastive learning (TCL) is a technique for learning to extract nonlinear representations from time series data. First, the time series is sliced up into a number of non-overlapping chunks, indexed by \u03c4\u03c4. Then, a multivariate logistic regression classifier is trained in a supervised manner to look at a sample taken from the series at an unknown time and predict \u03c4\u03c4, the index of the chunk it came from. For this classifier, a neural network is used.\n\nThe classifier itself is only a proxy to solving the representation learning problem. It turns out, if you chop off the final linear + softmax layer, the activations in the last hidden layer will learn to represent something fundamental, the log-odds-ratios in a probabilistic generative model (see paper for details). If one runs linear ICA over these hidden layer activations, the resulting network will learn to perform inference in a nonlinear ICA latent variable model.\n\nMoreover, if certain conditions about nonstationarity and the generative model are met, one can prove that the latent variable model is identifiable. This means that if the data was indeed drawn from the nonlinear ICA generative model, the resulting inference network - composed by the chopping off the top of the classifier and replacing it with a linear ICA layer - can infer the true hidden variables exactly.\n\n### How practical are the assumptions?\n\nTCL relies on the nonstationarity of time series data: the statistics of data changes depending on which chunk or slice of the time series you are in, but it is also assumed that data are i.i.d. within each chunk. The proof also assumes that the chunk-conditional data distributions are slightly modulated versions of the same nonlinear ICA generative model, this is how the model ends up identifiable - because we can use the different temporal chunks as different perspectives on the latent variables.\n\nI would say that these assumptions are not very practical, or at least on data such as natural video. Something along the lines of slow-feature analysis, with latent variables that exhibit more interesting behaviour over time would be desirable. Nevertheless, the model is complex enough to make a point, and I beleive TCL itself can be deployed more generally for representation learning.\n\n### Temporal jigsaw\n\nIt's not hard to see that TCL is analogous to a temporal version of the jigsaw puzzle method I wrote about last month. In the jigsaw puzzle method, one breaks up a single image into non-overlapping chunks, shuffles them, and then trains a network to reassemble the pieces. Here, the chunking happens in the temporal domain instead.\n\nThere are other papers that use the same general idea: training classifiers that guess the correct temporal ordering of frames or subsequences in videos. To do well at their job, these classifiers can end up learning about objects, motion, perhaps even a notion of inertia, gravity or causality.\n\nIshan Misra et al. (2016) Unsupervised Learning using Sequential Verification for Action Recognition\nBasura Fernando et al. (2015) Modeling Video Evolution For Action Recognition\nIn this context, the key contribution of Hyvarinen and Morioka's paper is to provide extra theoretical justification, and relating the idea to generative models. I'm sure one can use this framework to extend TCL to slightly more plausible generative models.\n\n### Key takeaway\n\n`Logistic regression learns likelihood ratios`\n\nThis is yet another example of using logistic regression as a proxy to estimating log-probability-ratios directly from data. The same thing happens in generative adversarial networks, where the discriminator learns to represent $\\log P(x) - \\log Q(x)$, where $P$ and $Q$ are the real and synthetic data distributions, respectively.\n\nThis insight provides new ways in which unsupervised or semi-supervised tasks can be reduced to supervised learning problems. \nAs classification is now considered significantly easier than density estimation, direct probability ratio estimation may provide the easiest path forward for representation learning.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1605.06336"
    },
    "1025": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ChenK17",
        "transcript": "This paper generates photographic images from semantic images using progressively growing resolution of the feature maps. The goal is to generate high resolution images while maintaining global structure in the images in a coarse-to-fine procedure.\n\nThe architecture is composed of several refinement modules (as shown in Figure below), where each one maintains the resolution of its input. The output resolution of each module is then doubled when being passed to the next module. The first module has a resolution of $4 \\times 8$ and takes semantic image at this resolution. It produces a feature layer $F_0$ as output. The output is then doubled in resolution and passed together with a downsampled semantic image to the next module that generates feature layer $F_1$ as output. This process continues where each module takes feature layer $F_{i-1}$ together with a semantic image as input and produces $F_i$ as output. The final module outputs 3 channels for RGB image.\n\nhttps://i.imgur.com/M3ucgwI.png\n\nThis process is used to generate high resolution images (images of resolution $1024 \\times 2048$ on Cityscapes dataset are generated) and meanwhile maintains global coordination in the image in a coarse-to-fine process. For example, if the model generates the left red light of a car, the right red light should also be similar. The global structure can then be specified at low resolution where features are close and then maintained while increasing the resolution of the maps.\n\nCreating photographic images is a 1 to n mapping, so a model can output many plausible and at the same time correct outputs. Therefore, pixel-wise comparison of the generated image with the ground truth (GT) from the training set can produce high errors. For example, if the model assigns black color instead of white to a car the error is very high while the output is still correct. Therefore the authors define the cost by comparing features of a pre-trained VGG network as follows:\n\nhttps://i.imgur.com/gIflZLM.png\n\nwhere $l$ is the layer of pre-trained VGG model and $\\lambda_l$ is its corresponding weight, $\\Phi(I)$ and $\\Phi(g(L,\\theta))$  are features of GT image and generated image.\n\nThe following image shows samples of this model:\nhttps://i.imgur.com/coxsdbU.png\n\nIn order to generate more diverse images another variant of this model is proposed, where the final output layers generates $3k$ images ($k$ tuples of RGB images) instead of $3$. The model then optimizes the following loss:\n\nhttps://i.imgur.com/wVQwufn.png\n\nwhere for each class label $c$, the image $u$ among $k$ generated images that generates the least error is selected. The rest of the loss is similar to Eq. (1), with the difference that it considers loss for each feature map $j$ and the difference in features is multiplied (with Hadamard product) in $L_p^l$, which is a mask (0 or 1) of the same resolution as feature map $\\Phi$ and indicates the existence of the class label $c$ in the corresponding feature. In summary, this loss takes the best synthesized image for each class $c$ and penalizes only the corresponding pixels to the class $c$ in the feature maps.\n\nThe following image shows two different samples for the same input:\nhttps://i.imgur.com/TFPWLxa.png\n\nThe model (referred to as CRN) is evaluated by comparing pair-wise samples of CRN with the following cases using Mechanical Turks:\n\n- $\\textbf{GAN and semantic segmentation:}$ a model that uses gan loss plus semantic loss on the generated photographic images.\n- $\\textbf{Image-to-image translation:}$ a model that uses conditional GAN using image-to-image translation network.\n- $\\textbf{Encoder-decoder:}$ a model that uses CRN loss but replaces its architecture with U-Net or Recombinator Networks architecture (where the model has an encode-decoder architecture with skip connections.)\n- $\\textbf{Full-resolution network:}$ a model that uses CRN loss but with a full-resolution network, which is a model that maintains the resolution from input to output.\n- $\\textbf{Image-space loss:}$ a model that uses CRN loss but with loss directly on the RGB values rather than VGG features.\n\nThe first two use different losses and also different architectures, the last three use the same loss as CRN but with different architectures. The Mechanical Turk users rate samples of CRN with its proposed loss more realistic than other approaches. \n\n\nAlthough this paper compares with a model that uses GAN loss and/or semantic segmentation loss, but it would have been better to try these losses on the CRN architecture itself to evaluate better the impact of these losses.\n\nAlso the paper does not show the diverse samples generated by the model (only two samples are shown). More samples of the model's output would show better the effectiveness of the proposed approach in terms of generating diverse samples (impact of using Eq. 3).\n\nIn general I like the proposed approach in using a coarse-to-fine modular resolution increment and find their defined loss and architecture affective.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1707.09405"
    },
    "1026": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1709.06548",
        "transcript": "This paper introduces triangle-GAN ($\\triangle$-GAN) that aims at cross-domain joint distribution matching: The model is shown below\n\nhttps://i.imgur.com/boIDOMu.png\n\nHaving two domains of data $x$ and $y$, there are two generators:\n\n1- $G_x(y)$ which takes $y$ and generates $\\tilde{x}$\n\n2- $G_y(x)$ which takes $x$ and generates $\\tilde{y}$\n\nThere are two discriminators in the model:\n\n1- $D_1 (x,y)$ a discriminator that distinguishes between $(x, y)$ and either of $(x, \\tilde{y})$ or $(\\tilde{x}, y)$.\n\n2- $D_2 (x,y)$ a discriminator that distinguishes between $(x, \\tilde{y})$ and $(\\tilde{x}, y)$.\n\nThe second discriminator is ALI and can be used on un-paired sets of data.\nThe first discriminator is equivalent to a conditional discriminator where the true paired data $(x, y)$ is compared to either $(x, \\tilde{y})$ or $(\\tilde{x}, y)$, where one element in the pair is sampled. This discriminator needs paired $(x, y)$ data for training. \n\nThis model can be used for semi-supervised settings, where a small set of paired data is provided. In this paper it is used for:\n\n-  semi-supervised image classification, where a small subset of CIFAR10 is labelled. $x$ and $y$ are images and class labels here.\n\n- image to image translation on edge2shoes dataset, where only a subset of dataset is paired.\n\n- attribute conditional image generation where $x$ and $y$ domains are image and attributes. CelebA and COCO datasets are used here. In one experiment test-set images are projected to attributes and then given those attributes new images are generated:\n\nOn celebA:\nhttps://i.imgur.com/EX5tDZ0.png\n\nOn COCO:\nhttps://i.imgur.com/GRpvjGx.png\n\nIn another experiment some attributes are chosen (as samples shown below in the first row with different noise) and then another feature is added (using the same noise) to generate the samples in the second row:\n https://i.imgur.com/KeHL8Ye.png\n\nThe triangle gan demonstrates improved performance compared to triple gan in the experiments shown in the paper. It has been also compared with Disco gan (a model that can be trained on un-paired data) and shows improved performance when some percentage of paired data is provided. In an experiment they pair each MNIST digit with its transposed (as $x$, $y$ pairs). Disco-GAN cannot learn correct mapping between them, while triangle-GAN can learn correct mapping since it leverages paired data.\n\nhttps://i.imgur.com/Vz9Zfhu.png\n\nIn general this model is a useful approach for semi-supervised cross-domain matching and can leverage un-paired data (using ALI) as well as paired data (using conditional discriminator). ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1709.06548"
    },
    "1027": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LampleZUBDR17",
        "transcript": "This paper aims at changing the attributes of a face, without manipulating other aspects of the image, such as add/remove glasses, make a person young/old, changed the gender, and hence the name Fader Networks, similar to sliders of audio mixing tools that can change a value linearly to increase/decrease a feature. \n\nThe model is shown below:\n\nhttps://i.imgur.com/fntPmNu.png\n\nAn image $x$ is passed to the encoder and the output of the encoder $E(x)$ is passed to the \ndiscriminator to distinguish whether a feature $y$ is in the latent space or not. The encoded features $E(x)$ and the feature $y$ is passed to the decoder to reconstruct the image $D(E(x))$.\n\nThe AE therefore has two loss: 1- The reconstruction loss between $x$ and $D(E(x))$, and 2- The gan loss to fool the discriminator on the feature $y$ in the encoded space $E(x)$.\n\nThe discriminator tries to distinguish whether a feature $y$ is in the encoded space $E(x)$ or not, while the encoder tries to fool the discriminator. This process leads to removal of the feature $y$ from the $E(x)$ by encoder. The encoded feature $E(x)$ therefore does not have any information on $y$. However, since the decoder needs to reconstruct the same input image, $E(x)$ has to maintain all information, except the feature $y$ and the decoder should get the feature $y$ from the input of the decoder.\n\nThe model is trained on binary $y$ features such as:\nmale/female, young/old, glasses Yes/No, mouth open Yes/No, eyes open Yes/No (some samples from test set below):\nhttps://i.imgur.com/bj9wu6B.png\n\nAt test time, they can change the features continuously and show transition in the features:\nhttps://i.imgur.com/XUD3ZTu.png\n\nThe performance of the model is measured using mechanical turks on two metrics: Naturalness of the images and the accuracy of swapping features on the image. In both FadNet shows better results compared to IcGAN, and FadNet shows very good results on accuracy, however on naturalness the performance drops when some features are swapped.\n\nOn Flowers dataset, FadNet can change colors of the flowers:\nhttps://i.imgur.com/7nvBSEY.png\n\n\nI find the following positive aspects about FadNet:\n\n1- It can change some features while maintaining other features of the image such as identity of the person, background information, etc.\n\n2- The model does not need paired data. In some cases it is impossible to gather paired data (e.g. male/female) or very difficult (young/old).\n\n3- The gan loss is used to remove a feature in the latent space, where that feature can be later specified for reconstruction by decoder. Since GAN is applied to latent space, it can be used to remove features on the data that is discrete (where direct usage of disc on those data is not trivial).\n\nI think these aspects need further work for improvement:\n- When multiple features are changed the blurriness of the image shows up:\nhttps://i.imgur.com/LD5cVbg.png\nWhen only one feature changes the blurriness affect is much less, despite the fact that they use L2-loss for AE reconstruction. I guess also using a high resolution of 256*256 helps make the bluriness of the images less noticeable.\n\n- The model should be first trained only on AE (no gan loss) and then the gan loss in AE is linearly increased to remove a feature. So, it requires a bit of care in training it properly.\n\nOverall, I find it an interesting paper on how to change a feature in an image when one wants to keep other features unchanged.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.00409"
    },
    "1028": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.03431",
        "transcript": "This paper merges a GAN and VAE to improve pose estimation on depth hand images. \n\nThey used paired data (where both depth image ($x$) and pose ($y$) is provided) and merge that with unlabelled data where only depth image ($x$) is given. The model is shown below:\nhttps://i.imgur.com/BvjZekU.png\n\nThe VAE model takes $y$ and projects it to latent space ($z_y$) using encoder and then reconstructs it back to $\\bar y$.\n\nAli is used to map between latent space of VAE $z_y$ and the latent space of GAN $z_x$.\nThe depth image synthesizer takes $z_x$ and generates a depth image $\\bar x$.\n\nThe Discriminator does three tasks: \n\n1-$L_{gan}$: distinguishing between true ($x$) and generated sample ($\\bar x$). \n\n2- $L_{pos}$: predicting the pose of the true depth image $x$. \n\n3: $L_{smo}$: a smoothing loss to enforce the difference between two latent spaces in the generator and the ones predicted by discriminator to be the same (see below for more details). \n\n$\\textbf{Here is how the data flows and losses are defined:}$ Given a pair of labelled data $(x,y)$, the pose $y$ is projected to latent space $z_y$, then projected back to estimate pose $\\bar y$. \nUsing VAE model, a reconstruction loss $L_{recons}$ is defined on pose. \nUsing Ali, the latent variable $z_y$ is projected to $z_x$ and then the depth image $\\bar{x}$ is generated $\\bar{x} = Gen(Ali({z_y}))$. A reconstruction loss between x and $\\bar{x}$ is defined (d_{self}). \nA random noise is samples from pose latent space ($\\hat{z_y}$) and projected to a depth map using $\\hat{x} = Gen(Ali(\\hat{z_y}))$. Discriminator then takes $x$ and $\\hat{x}$. It estimates pose on $x$ using $L_{pos}$. It also distinguishes between $x$ and $\\hat{x}$ with $L_{gan}$. Finally, it measures the $x$ and $\\hat{x}$'s latent space difference $smo(x, \\hat x)$, which should be similar to the distance between $z_y$ and $\\hat{z_y}$, so the smo-loss is: $L_{smo} = || smo(x, \\hat x) - (z_y - \\hat{z_y})||^2 + d_{self}$.\n\n\nIn general the the VAE model and the depth image synthesizer can be considered as the Generator of the network. The total loss can be written as:\n\n$L_G = L_{recons} + L_{smo} - L_{gan}\\\\$\n$L_D = L_{pos} + L_{smo} - L_{gan}\\\\$\n\nThe generator loss contains pose reconstruction, smo-loss, and gan loss on generated depth maps.\nThe discriminator loss contains pose estimation loss, smo-loss, and gan loss on distinguishing fake and real depth images.\n\nNote that in the gen and disc losses all except the gan loss need paired data and the un-labelled data can be used for only gan-loss. However, the unlabelled data would train the lowest layers of the disc (for pose estimation) and the image synthesis part of gen. But for pose estimation (the final target of the paper), training the VAE model, and also mapping between VAE and GAN using Ali, labelled data should be provided. Also note that $ L_{smo}$ trains both generator and discriminator parameters.\n\nIn terms of performance the model improves the results on partially labelled data. On fully labelled data it shows either improvement or comparable results w.r.t to previous models. I find the strongest aspect of the paper in semi-supervised learning where smaller portion of labelled data is provided, However, due to the way parameters are binded together, the model needs some labelled data to train the model completely.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1702.03431"
    },
    "1029": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/TranYL17",
        "transcript": "This paper gets a face image and changes its pose or rotates it (to any desired pose) by passing the target pose as the input to the model. \n\nhttps://i.imgur.com/AGNOag5.png\n\nThey use a GAN (named DR-GAN) for face rotation. The gan has an encoder and a decoder. The encoder takes the image and gets a high-level feature representation. The decoder gets high-level features, the target pose, and some noise to generate the output image with rotated face. \n\nThe generated image is then passed to a discriminator where it says whether the image is real or fake. The disc also has two other outputs: 1- it estimates the pose of the generated image, 2) it estimated the identity of the person.\n\nno direct loss is applied to the generator, it is trained by the gradient that it gets through discriminator to minimize the three objects: 1- gan loss (to fool disc) 2-pose estimation 3- identity estimation.\n\nThey use two tricks to improve the model:\n1- using the same parameters for encoder of generator (gen-enc) and the discriminator (they observe this helps better identity recognition)\n2- passing two images to gen-enc and interpolating between their high-level features (gen-enc output) and then applying two costs on it: 1) gan loss 2) pose loss. These losses are applied through disc, similar to above.\nThe first trick improves gen-enc and second trick improves gen-dec, both help on identification.\n\nTheir model can also leverage multiple image of the same identity if the dataset provides that to get better latent representation in gen-enc for a given identity.\n\nhttps://i.imgur.com/23Tckqc.png\n\nThese are some samples on face frontalization:\nhttps://i.imgur.com/zmCODXe.png\n\nand these are some samples on interpolating different features in latent space: (sub-fig a) interpolating f(x) between the latent space of two images, (sub-fig b) interpolating pose (c), (sub-fig c) interpolating noise:\nhttps://i.imgur.com/KlkVyp9.png\n\nI find these positive aspects about the paper:\n1) face rotation is applied on the images in the wild, 2) It is not required to have paired data. 3) multiple source images of the same identity can be used if provided, 4) identity and pose are used smartly in the discriminator to guide the generator, 5) model can specify the target pose (it is not only face-frontalization).\n\nNegative aspects:\n1) face has many artifacts, similar to artifacts of some other gan models. \n2) The identity is not well-preserved and the faces seem sometime distorted compared to the original person.\n\n They show the models performance on identity recognition and face rotation and demonstrate compelling results.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.11136"
    },
    "1030": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1705.10872",
        "transcript": "This paper learns deep local patch descriptor (for replacing SIFT) by hard negative mining using current mini-batch. It outperforms SIFT and deep competitors on Oxford5K and Paris6K retrieval datasets.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1705.10872"
    },
    "1031": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1606.02228",
        "transcript": "Authors test different variant of CNN architectures, non-linearities, poolings, etc. on ImageNet.\n\nSummary:\n-  use ELU non-linearity without batchnorm or ReLU with it.\n-  apply a learned colorspace transformation of RGB (2 layers of 1x1 convolution ).\n-  use the linear learning rate decay policy.\n-  use a sum of the average and max pooling layers.\n-  use mini-batch size around 128 or 256. If this is too big for your GPU,\ndecrease the learning rate proportionally to the batch size.\n- use fully-connected layers as convolutional and average the predictions for\nthe final decision.\n- when investing in increasing training set size, check if a plateau has not\nbeen reach.\n- cleanliness of the data is more important then the size.\n- if you cannot increase the input image size, reduce the stride in the consequent\nlayers, it has roughly the same effect.\n- if your network has a complex and highly optimized architecture, like e.g.\nGoogLeNet, be careful with modifications.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1606.02228"
    },
    "1032": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.5244/c.29.12",
        "transcript": "- SIFT family is still the best local descriptor, outperforms novel CNN [SiamNet2015] approaches.\n- (adaptive) Hessian-Affine is the best detector with broad applicability (not beaten yet)\n- Affine view synthesis greatly helps for non-geometrical problems.\n- Datasets and WxBS-Matcher available http://cmp.felk.cvut.cz/wbs/\n-  We need more diverse datasets for learning local descriptors than Yosemite and Libert \n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.5244/c.29.12"
    },
    "1033": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/cviu/MishkinMP15",
        "transcript": "For robust wide baseline matching:\n\n1) Use combination of MSER and Hessian-Affine with RootSIFT as a descriptor\n\n2) Do iteratively increasing affine view synthesis  - from sparse to dense\n\nSo you can match both fast for easy pairs and reliably for extreme (80 degrees of view point difference) pairs of same view of the object. Works for non-planar objects as well, much better than ASIFT.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/j.cviu.2015.08.005"
    },
    "1034": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MishkinM15",
        "transcript": "Mean(input) = 0, var(input) =1 is good for learning. Independent input features are good for learning.\nSo:\n\n1) Pre-Initialize network weights with (approximate) orthonormal matrices\n\n2) Do forward pass with mini-batch\n\n3) Divide layer weights by $\\sqrt{var(Output)}$\n\n4) PROFIT! ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.06422"
    },
    "1035": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/stoc/Cesa-BianchiFHHSW93",
        "transcript": "The authors consider online learning of binary values, where each period each of $N$ experts makes a prediction (in $[0,1]$), and the learner must make predictions such that, in hindsight, the learner didn't do much worse than the best expert. The loss function the authors use is $|\\hat{Y}_t - Y_t|$, the the total loss is the sum over all $t$.\n\nThe authors first solution to this problem is an algorithm called MM.  It uses a complex recursively-defined function v which takes exponential time to compute, but which (roughly) computes the anticipated future total loss of each expert for the rest of the game and uses that.  The bound they give for MM is in terms of this $v$ function, so it isn't easily interpreted.  This is analyzed in tremendous detail and under various assumptions.\n\nThe authors then give a more familiar (and implementable) multiplicative weight update algorithm, where a certain non-negative weight is placed on each expert and our prediction at any time is the weighted average of the experts' predictions.  After every period, each expert's weight is multiplied by some function of the loss it incurred that period and a learning rate. They show how, when the weight updates are done using the right function, the algorithm has a nice Bayesian interpretation. This paper is dense (at 59 pages) and so filled with proofs it feels like reading an appendix.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/167088.167198"
    },
    "1036": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/ml/Gentile03a",
        "transcript": "This paper describes a class of algorithms for classification or regression in the on-line setting.  That is, the data is a bunch of pairs $(X_t,Y_t)$ (where X may be a vector), and these data items arrive in some order: the algorithm must predict each $\\hat{Y}_t$ using only the $X_t$ and previously seen pairs. In the regression setting, each mis-prediction has a loss that is like $(Y_t - \\hat{Y}_t)^2$, and in the classification setting $Y_t$ is always 0 or 1 and the loss is $| Y_t - \\hat{Y}_t |$.\n\nRoughly, the algorithm makes linear predictions using some internal weight vector $(\\hat{y} = w  * X)$, and does a gradient-descent like weight update. However, it tries to keep the q-norm (q can be any number) of the weight vector \"small\", preventing the weights themselves from becoming too large. The algorithm is actually simple, and the weight update takes advantage of link functions, which the author defines.  The majority of the paper is focused on deriving loss bounds, showing that the loss incurred by this algorithm isn't much worse than that incurred by the best weight vector, chosen in hindsight.  Typical readers will be interested in the first few pages, as the latter part of the paper is mainly technical proofs.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1023/A:1026319107706"
    },
    "1037": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/siamjo/Freund95",
        "transcript": "This is a fantastic paper: it presents the problem well, the algorithms are intuitive and clearly presented, and the proofs are not overly long.  This describes the online allocation problem: roughly how to use the hypothesis of N \"experts\" to create a hypothesis that isn't much worse than the best expert (alternatively, how to allocate wealth every period among N traders, conditioned on only their past performance, such that at the end you performed almost as well as the best trader chosen in hindsight).  This paper presents the hedge algorithm, which works with general bounded summable loss functions and has only one parameter (a learning rate) to tune: it works by simply decreasing the weight on each expert every period by a factor like (learning_rate)^(loss), where 0 < learning_rate <= 1.  The final bound, where L(hedge) is the loss of the hedge algorithm, L* is the loss of the best expert (the one with minimum loss), and $0<B<=1$ is the learning rate, is:\n\n$$L(hedge) <= \\[  ln(1/B) \\* L\\* + ln(N)  \\] / (1 - B)$$\n\nThis paper also describes boosting and relates it to the hedge algorithm, though in my opinion the description given of the boosting problem and adaboost isn't nearly as good as the explanation of the online allocation problem and Hedge.  In boosting, we have a weak learner, which can learn a function $X \\rightarrow Y$ given some examples, but possibly with high error rate (the precise definition of a weak is, of course, in the paper).  In boosting, a \"master algorithm\" has some set of labelled examples $X_i \\rightarrow Y_i$  (X may be multi-dimensional).  it calls the weak learner many times, giving it different distributions over these examples and looking at the hypotheses created by the weak learner each time.  It then combines these hypothesis into a \"master\" hypothesis that is guaranteed to be \"good\". The paper continues with several extentions of boosting to other domains like regression.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1137/0805014"
    },
    "1038": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigir/CohenS96",
        "transcript": "This paper focuses on empirical results of various algorithms for text classification: my interest in it was its description of the sleeping experts algorithm.  Roughly, in the experts model, there are many \"experts\" making a prediction on every sample, and we must learn which experts to trust: i.e. we will make a prediction based on the weighted predictions of the experts, and the weights might depend on the experts' past peformance.  In \"sleeping experts\", some of the experts make no predictions on some samples. This paper has a good description, with pseudocode, of the sleeping expert algorithm, but does not derive any bounds.  The paper also describes Ripper, an algorithm for learning rule sets.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/243199.243278"
    },
    "1039": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/ml/Blum92",
        "transcript": "This paper develops algorithms and mistake bounds for learning non-noisy boolean functions.  It considers the case when there is a very large number of possible attributes, but each example (and the concept) depends on only a few attributes, so we want ot avoid having to ever explicitly list or consider all possible attributes.  The concept classes it deals with learning are boolean formulas. The paper is a good cite but (for my work) probably not directly applicable.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/BF00994112"
    },
    "1040": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/colt/BlumK97",
        "transcript": "The authors give a MUCH simpler description of constantly rebalanced portfolios (CRPs) and of their performance bound than Cover (above).  They also begin with a nice example of when a CRP has positive return even though neither stock does.  They motivate the problem of performing as well as the best CRP by describing how one can perform nearly as well as the best individual stock (within $1/N$ of the terminal wealth) by simply investing equal-weight initially and holding each stock.  They use the same strategy as Cover for performing nearly as well as the best CRP: spread wealth evenly across CRPs and let it grow within each CRP, and get the same bound: the wealth of the strategy is less than the wealth of the best CRP by no more than $(T+1)^{(N-1)}$ over $T$ periods, with $N$ assets.  They also consider a case of transaction costs to derive a (slightly looser) bound.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/267460.267518"
    },
    "1041": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tit/CoverO96",
        "transcript": "This paper is overly mathematical for what it accomplished (117 equations) but is somewhat interesting.  Cover considers CRPs (constantly rebalanced portfolios), where a certain portion of wealth is \"fixed\" in each stock and the portfolio is rebalanced every period: CRPs can earn positive return even if none of the stock's do. Finding the best CRP represents an optimization over a simplex (since we optimize over the fraction to invest in each stock), while the best stock is just a corner of the simplex, so the best CRP does at least as well as the best stock.  He considers the strategy of initially investing equal money in each CRP (i.e. over the continuous space of CRPs) and then letting the wealth in each CRP grow: doing this, he derives a bound that the terminal wealth of this strategy will only lag the terminal wealth of the best CRP chosen in hindsight by a factor  $(T+1)^{(N-1)}$, when there are $T$ periods and $N$ assets.  Thus the strategy earns the same asymptotic growth as the best CRP.  In practice I found that this technique isn't useful and the bound is very very weak (even over 50 years of data).  However, the idea of buy-and-hold having good asymptotic performance and comparing well to the best portfolio chosen in hindsight is a good one.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/18.485708"
    },
    "1042": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/AgarwalHKS06",
        "transcript": "The authors present an algorithm for investment (picking which stocks to buy).  This online algorithm chooses an investment in N stocks (a point on the N-dimensional simplex) every period.  It works by optimizing, every period, some approximation to the current lossdefining a portfolio loss minus the two-norm of the investment vector.  This algorithm can be easily implemented and the authors claim is has optimal regret with respect to the best CRP (constantly rebalanced portfolio) chosen in hindsight (I leave technical details of the algorithm to those who want to read the paper).  The authors have experiments indicating that the algorithms does better than buy-and-hold on randomly selected S&P stocks.  My personal experiments with this algorithm (using code from one of the authors) indicated that the regret bounds, though optimal, are far too weak to be economically meaningful (the terminal wealth of ONS may lag the best CRP by a factor exponential in $22 \\cdot N \\cdot \\sqrt{T}$, where $N$ is the number of stocks and T the number of periods) and the algorithm performed roughly as well as buy-and-hold on monthly data.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/1143844.1143846"
    },
    "1043": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/colt/Even-DarKMW07",
        "transcript": "Without a doubt this is one of the better papers I've read on this topic: it's well written and their proofs (except the last technical one) are relatively straightforward.  This also cites other useful papers on expert learning (especially the first two references).\n\nThe authors work in the experts allocation framework, where each period each of N experts has a gain in [0,1], and the algorithm must choose a weighting w on the probability simplex and will receive gain w \\cdot g, where g is a vector of the experts' gains.  The algorithms total gain is the sum of its gains over T time steps.  The authors define regret to the best (average, worst) as the amount by which the algorithm's total gain lags that of the best (average, worst) expert. They point out that, while traditional expert weighting algorithm have regret $O(T^{0.5})$ to the best, they also have regret $\\Omega(T^{0.5})$ to the average and worst expert.  They show that any algorithm with $O(T^0.5)$ regret to best has $\\Omega(T^{0.5})$ regret to worst (more detailed bounds are provided) They develop an algorithm with $O((N\\cdot T)^{0.5} \\cdot log(T))$ regret to best and constant regret to average: thus giving up a little regret to the best expert for much better regret to the average. The first of these, algorithms, BestAverage, basically runs an exponential weights algorithm, resetting whenever the best expert diverges sufficiently from the average.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-540-72927-3_18"
    },
    "1044": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jair/KaelblingLM96",
        "transcript": "Great paper! Summarizes unsupervised reinforcement learning techniques, both with a model and model free. Include TD learning, Q learning, exploration vs. exploitation tradeoff, and other details. Not difficult to read for a technical audience. Explanations are clear while avoiding unnecessary detail and the paper has copious references. Granted I'm biased since I took one of the author's courses",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1613/jair.301"
    },
    "1045": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1809.01999",
        "transcript": "## General Framework \nThe take-home message is that the challenge of Reinforcement Learning for environments with high-dimensional and partial observations is learning a good representation of the environment. This means learning a sensory features extractor V to deal with the highly dimensional observation (pixels for example). But also learning a temporal representation M of the environment dynamics to deal with the partial observability. If provided with such representations, learning a controller so as to maximize a reward is really easy (single linear layer evolved with CMA-ES).  \n\nAuthors call these representations a *World model* since they can use the learned environment's dynamics to simulate roll-outs. They show that policies trained inside the world model transfer well back to the real environment provided that measures are taken to prevent the policy from exploiting the world model's inaccuracies. \n\n## Method \n**Learning the World Model**\n![](https://i.imgur.com/tgV17k4.png =600x)\n\nIn this work they propose to learn these representations off-line in an unsupervized manner in order to be more efficient.\nThey use a VAE for V that they train exclusively with the reconstruction loss, that way the learned representations are independent of the reward and can be used alongside any reward. They then train M as Mixture-Density-Network-RNN to predict the next sensory features (as extracted by the VAE) --and possibly the done condition and the reward-- and thus learn the dynamics of the environment in the VAE's latent space (which is likely simpler there than in the pixel space).\nNote that the VAE's latent space is a single Gaussian (adding stochasticity makes it more robust to the \"next state\" outputs of M), whereas M outputs next states in a mixture of Gaussians. Indeed, an image is likely to have one visual encoding, yet it can have multiple and different future scenarii which are captured by the multimodal output of M. \n\n**Training the policy**\n![](https://i.imgur.com/H5vpb2H.png)\n\n* In the real env: \nThe agent is provided with the visual features and M's hidden state (temporal features).\n\n* In the world model: \nTo avoid that the agent exploits this imperfect simulator they increase its dynamics' stochasticity by playing with $\\tau$ the sampling temperature of $z_{t+1}$ in M. \n\n\n## Limitations\nIf exploration is important in the environment the initial random policy might fail to collect data in all the relevant part of the environment and an iterative version of Algorithm 1 might be required (see https://worldmodels.github.io/ for a discussion on the different iterative methods) for the data collection.\n\nBy training V independently of M it might fail to encode all the information relevant to the task. Another option would be to train V and M concurrently so that the reward and $z_{t+1}$'s prediction loss (or next state reconstruction loss) of M flows through V (that would also be trained with its own reconstruction loss). The trade-off is that now V is tuned to a particular reward and cannot be reused.\n\nThe authors argue that since $h_t$ is such that it can predict $z_{t+1}$, it contains enough insight about the future for the agent not needing to *plan ahead* and just doing reflexive actions based on $h_t$. This is interesting but the considered tasks (driving, dodging fireball) are still very reflexive and do not require much planning. \n\n## Results\nWhen trained on the true env, a simple controller with the V and M representations achieve SOTA on car-racing. V + M is better than V alone. \nWhen trained inside the world model, its dynamics' stochasticity must be tuned in order for the policy to transfer well and perform well on the real env: too little stochasticity and the agent overfits to the world model flaws and does not transfer to the real env, too much and the agent becomes risk-averse and robust but suboptimal.  \n\n![](https://i.imgur.com/e8ETSjQ.png)\n\n## Additional ressources \nThorough interactive blog post with additional experiments and discussions: https://worldmodels.github.io/",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1809.01999"
    },
    "1046": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1907.03976",
        "transcript": "## General Framework\nExtends T-REX (see [summary](https://www.shortscience.org/paper?bibtexKey=journals/corr/1904.06387&a=muntermulehitch)) so that preferences (rankings) over demonstrations are generated automatically (back to the common IL/IRL setting where we only have access to a set of unlabeled demonstrations). Also derives some theoretical requirements and guarantees for better-than-demonstrator performance. \n\n## Motivations\n* Preferences over demonstrations may be difficult to obtain in practice. \n* There is no theoretical understanding of the requirements that lead to outperforming demonstrator. \n\n## Contributions\n* Theoretical results (with linear reward function) on when better-than-demonstrator performance is possible: 1- the demonstrator must be suboptimal (room for improvement, obviously), 2- the learned reward must be close enough to the reward that the demonstrator is suboptimally optimizing for (be able to accurately capture the intent of the demonstrator), 3- the learned policy (optimal wrt the learned reward) must be close enough to the optimal policy (wrt to the ground truth reward). Obviously if we have 2- and a good enough RL algorithm we should have 3-, so it might be interesting to see if one can derive a requirement from only 1- and 2- (and possibly a good enough RL algo). \n* Theoretical results (with linear reward function) showing that pairwise preferences over demonstrations reduce the error and ambiguity of the reward learning. They show that without rankings two policies might have equal performance under a learned reward (that makes expert's demonstrations optimal) but very different performance under the true reward (that makes the expert optimal everywhere). Indeed, the expert's demonstration may reveal very little information about the reward of (suboptimal or not) unseen regions which may hurt very much the generalizations (even with RL as it would try to generalize to new states under a totally wrong reward). They also show that pairwise preferences over trajectories effectively give half-space constraints on the feasible reward function domain and thus may decrease exponentially the reward function ambiguity. \n* Propose a practical way to generate as many ranked demos as desired.\n\n## Additional Assumption\nVery mild, assumes that a Behavioral Cloning (BC) policy trained on the provided demonstrations is better than a uniform random policy. \n\n## Disturbance-based Reward Extrapolation (D-REX)\n\n![](https://i.imgur.com/9g6tOrF.png)\n\n![](https://i.imgur.com/zSRlDcr.png)\n\nThey also show that the more noise added to the BC policy the lower the performance of the generated trajs. \n\n## Results\nPretty much like T-REX.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1907.03976"
    },
    "1047": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1904.06387",
        "transcript": "## General Framework\nOnly access to a finite set of **ranked demonstrations**. The demonstrations only contains **observations** and **do not need to be optimal** but must be (approximately) ranked from worst to best. \nThe **reward learning part is off-line** but not the policy learning part (requires interactions with the environment).\n\nIn a nutshell: learns a reward models that looks at observations. The reward model is trained to predict if a demonstration's ranking is greater than another one's. Then, once the reward model is learned, one simply uses RL to learn a policy. This latter outperform the demonstrations' performance.\n\n## Motivations\nCurrent IRL methods cannot outperform the demonstrations because they seek a reward function that makes the demonstrator optimal and thus do not infer the underlying intentions of the demonstrator that may have been poorly executed. \nIn practice, high quality demonstrations may be difficult to provide and it is often easier to provide demonstrations with a ranking of their relative performance (desirableness).\n\n## Trajectory-ranked Reward EXtrapolation (T-REX)\n![](https://i.imgur.com/cuL8ZFJ.png =400x)\n\nUses ranked demonstrations to extrapolate a user's underlying intent beyond the best demonstrations by learning a reward that assigns greater return to higher-ranked trajectories. While standard IRL seeks a reward that **justifies** the demonstrations, T-REX tries learns a reward that **explains** the ranking over demonstrations. \n\n![](https://i.imgur.com/4IQ13TC.png =500x)\n\n\nHaving rankings over demonstrations may remove the reward ambiguity problem (always 0 reward cannot explain the ranking) as well as provide some \"data-augmentation\" since from a few ranked demonstrations you can define many pair-wise comparisons. Additionally, suboptimal demonstrations may provide more diverse data by exploring a larger area of the state space (but may miss the parts relevant to solving the task...)\n\n## Tricks and Tips\nAuthors used ground truth reward to rank trajectories, but they also show that approximate ranking does not hurt the performance much. \n\nTo avoid overfitting they used an ensemble of 5 Neural Networks to predict the reward.\n\nFor episodic tasks, they compare subtrajectories that correspond to similar timestep (better trajectory is a bit later in the episode than the one it is compared against so that reward increases as the episode progresses). \n\nAt RL training time, the learned reward goes through a sigmoid to avoid large changes in the reward scale across time-steps.\n\n## Results\n\n![](https://i.imgur.com/7ysYZKd.png)\n\n![](https://i.imgur.com/CHO9aVT.png)\n\n![](https://i.imgur.com/OzVD9sf.png =600x)\n\nResults are quite positive and performance can be good even when the learned reward is not really correlated with the ground truth (cf. HalfCheetah). \n\nThey also show that T-REX is robust to different ranking-noises: random-swapping of pair-wise ranking,  ranking by humans that only have access to a description of the task and not the ground truth reward. **They also automatically rank the demonstrations using the number of learning steps of a learning expert: therefore T-REX could be used as an intrinsic reward alongside the ground-truth to accelerate training.** \n\n![](https://i.imgur.com/IfOeLY6.png =500x)\n\n**Limitations**\nThey do not show than T-REX can match an optimal expert, maybe ranking demonstrations hurt when all the demos are close to optimality? ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1904.06387"
    },
    "1048": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1406.5979",
        "transcript": "## General Framework\nReally **similar to DAgger** (see [summary](https://www.shortscience.org/paper?bibtexKey=journals/corr/1011.0686&a=muntermulehitch)) but considers **cost-sensitive classification** (\"some mistakes are worst than others\": you should be more careful in imitating that particular action of the expert if failing in doing so incurs a large cost-to-go). By doing so they improve from DAgger's bound of $\\epsilon_{class}uT$ where $u$ is the difference in cost-to-go (between the expert and one error followed by expert policy) to  $\\epsilon_{class}T$ where $\\epsilon_{class}$ is the error due to the lack of expressiveness of the policy class. In brief, by accounting for the effect of a mistake on the cost-to-go they remove the cost-to-go contribution to the bound (difference in the performance of the learned policy vs. expert policy) and thus have a tighter bound. In the paper they use the word \"regret\" for two distinct concepts which is confusing to me: one for the no-regret online learning meta-approach to IL (similar to DAgger) and another one because Aggrevate aims at minimizing the cost-to-go difference with the expert (cost-to-go difference: the sum of the cost I endured because I did not behave like the expert once = *regret*) compared to DAgger that aims at minimizing the error rate wrt. the expert.\n\nAdditionally, the paper extends the view of Imitation learning as an online learning procedure to Reinforcement Learning. \n\n## Assumptions\n**Interactive**: you can re-query the expert and thus reach $\\epsilon T$ bounds instead of $\\epsilon T^2$ like with non-interactive methods (Behavioral Cloning) due to compounding error.\nAdditionally, one also needs a **reward/cost** that **cannot** be defined relative to the expert (no 0-1 loss wrt expert for ex.) since the cost-to-go is computed under the expert policy (would always yield 0 cost).\n\n## Other methods\n**SEARN**: does also reason about **cost-to-go but under the current policy** instead of the expert's (even if you can use the expert's in practice and thus becomes really similar to Aggrevate). SEARN uses **stochastic policies** and can be seen as an Aggrevate variant where stochastic mixing is used to solve the online learning problem instead of **Regularized-Follow-The-Leader (R-FTL)**. \n\n## Aggrevate - IL with cost-to-go\n![](https://i.imgur.com/I1otJwV.png)\n\nPretty much like DAgger but one has to use a no-regret online learning algo to do **cost-sensitive** instead of regular classification. In the paper, they use the R-FTL algorithm and train the policy on all previous iterations. Indeed, using R-FTL with strongly convex loss (like the squared error) with stable batch leaner (like stochastic gradient descent) ensures the no-regret property.\n\nIn practice (to deal with infinite policy classes and knowing the cost of only a few actions per state) they reduce cost-sensitive classification to an **argmax regression problem** where they train a model to match the cost given state-action (and time if we want nonstationary policies) using the collected datapoints and the (strongly convex) squared error loss. Then, they argmin this model to know which action minimizes the cost-to-go (cost-sensitive classification). This is close to what we do for **Q-learning** (DQN or DDPG): fit a critic (Q-values) with the TD-error (instead of full rollouts cost-to-go of expert), argmax your critic to get your policy. Similarly to DQN, the way you explore the actions of which you compute the cost-to-go is important (in this paper they do uniform exploration).\n\n**Limitations**\n\nIf the policy class is not expressive enough and cannot match the expert policy performance this algo may fail to learn a reasonable policy. Example: the task is to go for point A to point B, there exist a narrow shortcut and a safer but longer road. The expert can handle both roads so it prefers taking the shortcut. Even if the learned policy class can handle the safer road it will keep trying to use the narrow one and fail to reach the goal. This is because all the costs-to-go are computed under the expert's policy, thus ignoring the fact that they cannot be achieved by any policy of the learned policy class. \n\n## RL via No-Regrety Policy Iteration -- NRPI\n![](https://i.imgur.com/X4ckv1u.png)\n\nNRPI does not require an expert policy anymore but only a **state exploration distribution**. NRPI can also be preferred when no policy in the policy class can match the expert's since it allows for more exploration by considering the **cost-to-go of the current policy**. \n\nHere, the argmax regression equivalent problem is really similar to Q-learning (where we use sampled cost-to-go from rollouts instead of Bellman errors) but where **the cost-to-go** of the aggregate dataset corresponds to **outdated policies!** (in contrast, DQN's data is comprised of rewards instead of costs-to-go).\nYet, since R-FTL is a no-regret online learning method, the learned policy performs well under all the costs-to-go of previous iterations and the policies as well as the costs-to-go converge. \n\nThe performance of NRPI is strongly limited to the quality of the exploration distribution. Yet if the exploration distribution is optimal, then NRPI is also optimal (the bound $T\\epsilon_{regret} \\rightarrow 0$ with enough online iterations). This may be a promising method for not interactive, state-only IL (if you have access to a reward).\n\n## General limitations\nBoth methods are much less sample efficient than DAgger as they require costs-to-go: one full rollout for one data-point.\n\n## Broad contribution \nSeeing iterative learning methods such as Q-learning in the light of online learning methods is insightful and yields better bounds and understanding of why some methods might work. It presents a good tool to analyze the dynamics that interleaves learning and execution (optimizing and collecting data) for the purpose of generalization. For example, the bound for NRPI can seem quite counter-intuitive to someone familiar with on-policy/off-policy distinction, indeed NRPI optimizes a policy wrt to **costs-to-go of other policies**, yet R-FTL tells us that it converges towards what we want. Additionally, it may give a practical advantage for stability as the policy is optimized with larger batches and thus as to be good across many states and many cost-to-go formulations.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1406.5979"
    },
    "1049": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.15607/rss.2016.xii.029",
        "transcript": "## General Framework\n*wording: car = the autonomous car, driver = the other car it is interacting with* \n\nBuilds a model of an **autonomous car's influence over the behavior of an interacting driver** (human or simulated) that the autonomous car can leverage to plan more efficiently. The driver is modeled by the policy that maximizes his defined objective. In brief, a **linear reward function is learned off-line with IRL on human demonstrations** and the modeled policy takes the actions that maximize the driver's return under this reward function (computed with **Model Predictive Control (MPC)**). \nThey show that a car planning while using this driver learns ways to **influence the driver either towards specified behaviors** or in ways that **achieve higher payoff**. These results also hold when interacting with human **drivers which are loosely approximated by this driver model**.\n\nI believe the **key to this success lies in a learned reward that generalizes well** (linear function with few, clever hand-designed features) to new states as well as the **use of MPC**: by **focusing on modeling goals it captures a reasonable driver's policy** (since the learned reward is accurate) and does not have to deal with concurrent learning dynamics. On the other hand, IL would try to match behavior instead of goals and might not generalize as well to new (unseen) situations. Additionally, MPC enables to coordinate the car-driver interactions over **extended time horizons** (through planning).\n\n**This shows that leveraging an influence model is promising for communication emergence.** *Parallel with: Promoting influence like Jaques et al. is more effective than hoping agents figure it out by themselves like MADDPG* \n\n## Motivations\nPrevious methods use simplistic influence models (\"will keep constant velocity\"), yet the car behavior influences the driver whether the car is aware of it or not. This simplistic model only leads to \"defensive behaviors\" where the car avoids disturbing the driver and therefore does not interact with it and yields suboptimal strategies. Additionally, a simplistic interaction model seems to lead to exclusively to functional actions instead of communicative ones. \n\n## Assumptions\n* **Sequential decision making**: the car acts first which forces a driver response which makes the world transition to a new state (could be alleviated by considering influence over time steps: car's action a time t influences driver's action at time t+1)\n* Approximates the **human as an optimal planner** (but with limited recursive induction) without uncertainty (deterministic model) etc. \n* The car uses **1st-order recursive induction** (\"my action influences the driver but the driver believes that its action won't influence me (that my action is constant no matter what it does)\"): *I see this as assuming \"defensive\" driver and \"aggressive\" car.*\n* **Fully observable** environment (not sure how difficult it would be to wave this)\n* Model Predictive Control (MPC) with L-BFGS requires access to the **differentiable transition function**. \n\n## Method \nModel the interaction as a dynamical system where the car's action influences both the next state and the driver's action.  Uses Theano to backprop through the transition function, and implicit function theorem to derive a symbolic expression of the gradient. \n\n## Results\n* Car leverages driver's model to **influence** the simulated drivers (car has **perfect model**) in **specified ways** (modifying the car's reward function).\n* Car leverages driver's model to **influence** the simulated drivers (car has **perfect model**) in order to be **more efficient wrt its own objective**.\n* Car leverages driver's model to **influence** the human drivers (car has **imperfect model**) in **specified ways**.\n\n*colors: autonomous car is yellow, driver's car is red*\n![](https://i.imgur.com/RK1Gx2P.png)\n![](https://i.imgur.com/F77hSfp.png)\n![](https://i.imgur.com/3elOG9O.png)\n![](https://i.imgur.com/yeSsjiP.png)\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.15607/rss.2016.xii.029"
    },
    "1050": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1011.0686",
        "transcript": "## General Framework\nThe imitation learning problem is here cast into a classification problem: label the state with the corresponding expert action. With this, you can see structured prediction (predict next label knowing your previous prediction) as a degenerated IL problem. They make the **reduction assumption** that you can make the probability of mistake $\\epsilon$ as small as desired on the **training distribution** (expert or mixture). They also assume that the difference in the cost-to-go (Q-value) between expert-action followed by expert policy and any action followed by expert policy is small (which does not hold if one mistake makes you for fall from a cliff for example!). \n\n## Problem definition\nThere are three problems that are highlighted in this paper  **1-interaction between policy and distribution**, **2-violation of the i.i.d assumption**, and **3-distributional shift and resulting compounding error of BC**. The paper implies that **1** implies **2** that implies **3** but they eventually only address **3** with the algorithm they propose (DAgger). \n1. **interaction between policy and distribution**: the fact that the learned policy is present in the expectation (through the state-action distribution) as well as in the loss makes the **objective non-convex** and this even for convex loss functions. **Yet this also implies non i.i.d. samples!** Indeed, even if one could directly sample from the state-action distribution (like having its analytical form or an infinite experience replay buffer) and thus draw i.i.d. samples, the dependency will occur across optimization steps: if I draw a sample and use it to update my policy, I also update the distribution from which I will draw my next sample and then my next sample depends on my previous sample (since it conditioned my policy update).\n2. **violation of the i.i.d. assumption**: So we just discussed that 1. implies non-iid samples across updates (batches) yet there is another form of dependency (inside a batch this time) as in practice we collect samples using rollouts and in a trajectory s' depends on (s,a) so we have a dependency because we collected the **samples sequentially**.\n3. **distribution-shift and compounding error of Behavior cloning (supervised learning)**: During training, you assume iid data and you do not model that the next state you will have to classify (act upon) is influenced by your previous action. But at execution samples are not iid: if you do a mistake you reach a state that you may have never seen (distribution shift) and you cannot recover(compounding error). If the classifier's probability of making a mistake is $\\epsilon$ and the episode length is $T$, then BC can make as many as $\\epsilon T^2$ mistakes (CF Levine's class example with the funambulist and the worst-case scenario: as soon as you make a mistake, you cannot recover and you make mistakes until the end of the episode). **This is what this paper (and the papers it compares to) addresses, they propose a worst-case scenario linear in both** $\\epsilon$ and $T$.\n\n**Additionnal assumptions compared to BC:**During training you are allowed to query the expert as much as you want. \n\n## Other methods\n* **Forward Training**: trains a **non-stationary policy** (one policy for each time step). Each policy is trained to match the expert on the distribution of states encountered at that time-step when doing rollouts with the previously learned policies (there is no distribution shift since each policy is trained on the actual distribution of states it will see).  **Problem**: you have to train $T$ different policies, you cannot stop until you trained all the policies. \n* **Stochastic Mixing Iterative Learning (SMILe)**: iteratively trains a **stochastic mixture of policies**. Each new policy is trained to match the expert on the state distribution of the current mixture, therefore iteratively accounting for the distribution drift, eventually, the distribution should converge. This yields near-linear regret on $T$ and $\\epsilon$. **Problem**: At execution time, we sample a policy from the mixture, which can perform poorly on the given state, especially if it corresponded to a different distribution (early iteration for example). \n\n## DAgger -- Dataset Aggregation\n**Algo** \n![](https://i.imgur.com/FpQAyI2.png =400x)\n$\\beta_i$ is related to the proportion of state distribution that we want to collect similar to the expert distribution. This may help in the early iterations, where the policy is bad and may spend a lot of time collecting datapoints in states that are irrelevant. In practice using $\\beta_i=I(i=0)$ works well. \nIntuitively, by training on generated transitions, DAgger reduces the distribution shift. \n\n**Link to no Regret Online Learning**\nDAgger can be seen as a *Follow-the-Leader* (FTL) algorithm where at iteration $n$ we pick the best policy in hindsight, i.e. with respect to all the transitions seen so far (in previous iterations).\nThe use of $\\beta_i$ is an \"extra trick\" that doesn't exist as is in FTL algos. \n\n*Regret*: the difference between what I did and the best thing that I could have done. \n\nEach iteration of DAgger can be seen as a step of Online Learning (treat mini-batches of trajectories under a single policy as a single online-learning example) where the online algo used is FTL but could actually be any no-regret algo. Indeed, by using the no-regret algo view, the authors show that using no-regret algos (regret decays with $1/N$) as a meta-algorithm for IL (just like FTL for DAgger) yield that, for N big enough (in the order of T), we have a worst-case performance linear in $\\epsilon$, $u$ and $T$. Where it is **assumed** that we can reach an error probability of $\\epsilon$ on any training distribution and a difference in the cost-to-go (-Q-value) of $u$. In other words, if we can guarantee a classification error rate of $\\epsilon$ **on the training distribution** and that the expert is able to recover from a mistake ($u$ is bounded) then the use of a **no-regret online learning algorithm** will yield a policy that will make at most (proportional to) $\\epsilon T u$ errors **on its own state distribution (test distribution)**.\n\n**Requirements**\n* An error rate of $\\epsilon$ on the training distribution\n* A strongly convex loss function between expert and learner labels (0-1 loss is not but binary cross-entropy that is usually used for classification is) so that FTL is indeed a no-regret algo. If the loss in not strongly convex, FTL has no guarantees and we must use another no-regret method\n* The difference in the cost-to-go (Q-value) between expert-action followed by expert policy and any action followed by the expert policy must be small (expert must be able to recover: this does not account for falling from a cliff for example!).\n\n**Results**\n\n![](https://i.imgur.com/DZTK2nu.png =500x)\n![](https://i.imgur.com/rbtyUbY.png =500x)\n![](https://i.imgur.com/ZViGENA.png =500x)\n![](https://i.imgur.com/fExmxeb.png =500x)\n![](https://i.imgur.com/kv1X01w.png =500x)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1011.0686"
    },
    "1051": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/bea/Rei17",
        "transcript": "A neural architecture for detecting off-topic written responses, with respect to visual prompts. The text is composed with an LSTM and then used to condition the image representation. The two representations are then compared to calculate a confidence score for the text being written in response to the prompt image.\n\nhttps://i.imgur.com/FIltq46.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://aclanthology.info/papers/W17-5020/w17-5020"
    },
    "1052": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/bea/FaragRB17",
        "transcript": "Introduces a process for pre-training word embeddings with an objective that optimises them to distinguish between grammatical and ungrammatical sequences. This is then extended to also distinguish between correct and incorrect versions of the same sentence. The embeddings are then used in a network for essay scoring, improving performance compared to previous methods.\n\nhttps://i.imgur.com/1tyrlFB.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://aclanthology.info/papers/W17-5016/w17-5016"
    },
    "1053": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/bea/ReiY17",
        "transcript": "Investigating a range of auxiliary objectives for training a sequence labeling system for error detection. Automatically generated dependency relations and POS tags perform surprisingly well as gold labels for multi-task learning. Learning different objectives at the same time works better than doing them in sequence or switching.\n\nhttps://i.imgur.com/81PvMfj.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://aclanthology.info/papers/W17-5004/w17-5004"
    },
    "1054": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/bea/ReiFYB17",
        "transcript": "Investigating methods for generating artificial data in order to train better systems for detecting grammatical errors. The first approach uses regular machine translation, essentially translating from correct English to incorrect English. The second method uses local patterns with slots and POS tags to insert errors into new text.\n\nhttps://i.imgur.com/xEMm1oM.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://aclanthology.info/papers/W17-5032/w17-5032"
    },
    "1055": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/YannakoudakisRA17",
        "transcript": "Using error detection to improve error correction. A neural sequence labeling model is used to find correctness probabilities for every token, which are then used to rerank possible correction candidates. The process consistently improves the performance of different correction systems.\n\nhttps://i.imgur.com/DMkotr6.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://aclanthology.info/papers/D17-1297/d17-1297"
    },
    "1056": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/ReiBKS17",
        "transcript": "A specialised architecture for detecting metaphorical phrases. Uses a gating mechanism to condition one word based on the other, a neural version of weighted cosine similarity to make a prediction and hinge loss to optimise the model. Achieves high results on detecting metaphorical adjective-noun, verb-object and verb-subject phrases.\n\nhttps://i.imgur.com/p3zyCcJ.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://aclanthology.info/papers/D17-1162/d17-1162"
    },
    "1057": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/Rei17",
        "transcript": "Incorporating an unsupervised language modeling objective to help train a bidirectional LSTM for sequence labeling. At the same time as training the tagger, the forward-facing LSTM is optimised to predict the next word and the backward-facing LSTM is optimised to predict the previous word. The model learns a better composition function and improves performance on NER, error detection, chunking and POS-tagging, without using additional data.\n\nhttps://i.imgur.com/pXLSsAR.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.18653/v1/P17-1194"
    },
    "1058": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/GraveJCGJ17",
        "transcript": "Modification of the 2-level hierarchical softmax for better efficiency. An equation of computational complexity is used to find the optimal number of words in each class. In addition, the most common words are considered on the same level as other classes.\n\nhttps://i.imgur.com/dbKS3gh.png",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v70/grave17a.html"
    },
    "1059": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.06922",
        "transcript": "Learning to translate using two monolingual image captioning datasets and pivoting through images. The model encodes an image and generates a caption in language A, this is then encoded into the same space as language B and the representation is optimised to be similar to the correct image. The model is trained end-to-end using Gumbel-softmax.\n\nhttps://i.imgur.com/lnIsFNb.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1710.06922"
    },
    "1060": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/PlankSG16",
        "transcript": "Doing POS tagging using a bidirectional LSTM with word- and character-based embeddings. They add an extra component to the loss function \u2013 predicting a frequency class for each word, together with their POS tag. Results show that overall performance remains similar, but there\u2019s an improvement in tagging accuracy for low-frequency words.\n\nhttps://i.imgur.com/nwb8dOC.png",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P16/P16-2067.pdf"
    },
    "1061": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SabourFH17",
        "transcript": "An attention-based architecture for combining information from different convolutional layers. The attention values are calculated using an iterative process, making use of a custom squashing function. The evaluations on MNIST show robustness to affine transformations.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules"
    },
    "1062": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1707.05589",
        "transcript": "Comparison of three recurrent architectures for language modelling: LSTMs, Recurrent Highway Networks and the NAS architecture. Each model goes through a substantial hyperparameter search, under the constraint that the total number of parameters is kept constant. They conclude that basic LSTMs still outperform other architectures and achieve state-of-the-art perplexities on two datasets.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1707.05589"
    },
    "1063": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/FangLC17",
        "transcript": "Active learning (choosing which examples to annotate for training) is proposed as a reinforcement learning problem. The Q-learning network predicts for each sentence whether it should be annotated, and is trained based on the performance improvement from the main task. Evaluation is done on NER, with experiments on transferring the trained Q-learning function to other languages.\n\nhttps://i.imgur.com/5rXm5vZ.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://aclanthology.info/papers/D17-1063/d17-1063"
    },
    "1064": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.09645",
        "transcript": "A model for document sentiment classification which can also return sentence-level sentiment predictions. They construct sentence-level representations using a convnet, use this to predict a sentence-level probability distribution over possible sentiment labels, and then combine these over all sentences either with a fixed weight vector or using an attention mechanism. They release a new dataset of 200 documents annotated on the level of sentences and discourse units.\n\nhttps://i.imgur.com/A6YpmLU.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.09645"
    },
    "1065": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tacl/LinzenDG16",
        "transcript": "Investigation of how well LSTMs capture long-distance dependencies. The task is to predict verb agreement (singular or plural) when the subject noun is separated by different numbers of distractors. They find that an LSTM trained explicitly for this task manages to handle even most of the difficult cases, but a regular language model is more prone to being misled by the distractors.\n\nhttps://i.imgur.com/0kYhawn.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://transacl.org/ojs/index.php/tacl/article/view/972"
    },
    "1066": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.00043",
        "transcript": "The model learns to translate using a seq2seq model, an autoencoder objective, and an adversarial objective for language identification.\nThe system is trained to correct noisy versions of its own output and iteratively improves performance.\nDoes not require parallel corpora, but relies on a separate method for inducing a parallel dictionary that bootstraps the translation.\n\nhttps://i.imgur.com/6uXNAgo.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.00043"
    },
    "1067": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1709.07432",
        "transcript": "Updating the parameters in a LSTM language model based on the observed sequence during testing. A slice of text is first processed and then used for a gradient descent update step. A regularisation term is also proposed which draws the parameters back towards the original model.\n\nhttps://i.imgur.com/zikOowE.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1709.07432"
    },
    "1068": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/BryantFB17",
        "transcript": "A toolkit for automatically annotating error correction data with error types. It takes original and corrected sentences as input, aligns them to infer error spans, and uses rules to assign error types. They use the tool to perform fine-grained evaluation of CoNLL-14 shared task participants.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.18653/v1/P17-1074"
    },
    "1069": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/HuARDS17",
        "transcript": "A modular neural architecture for visual question answering. A seq2seq component predicts the sequence of neural modules (eg find() and compare()) based on the textual question, which are then dynamically combined and trained end-to-end. Achieves good results on three separate benchmarks that focus on reasoning about the image.\n\nhttps://i.imgur.com/iOkSh8y.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/ICCV.2017.93"
    },
    "1070": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BritzGLL17",
        "transcript": "Investigates different parameter choices for encoder-decoder NMT models. They find that LSTM is better than GRU, 2 bidirectional layers is enough, additive attention is the best, and a well-tuned beam search is important. They achieve good results on the WMT15 English->German task and release the code.\n\nhttps://i.imgur.com/GaAsTvE.png",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.03906"
    },
    "1071": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/BarrettBKS16",
        "transcript": "The paper explores the usefulness of eye tracking for the task of POS tagging. The assumption is that readers skip quickly over closed class words, and fixate longer on rare on ambiguous words.\n\nThe experiments are performed on unsupervised POS tagging \u2013 a second-order HMM uses constraints on possible tags for each word (based on a dictionary), but no explicit annotated data is required. They show that including the eye tracking features improves performance by quite a bit. Surprisingly, it seems to be better to average eye tracking features over all training tokens of the same type, as opposed to using using the data for each individual token, which means eye tracking is only used during the training stage.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P16/P16-2094.pdf"
    },
    "1072": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/PetersABP17",
        "transcript": "The paper proposes integrating a pre-trained language model into a sequence labeling model. The baseline model for sequence labeling is a two-layer LSTM/GRU. They concatenate the hidden states from pre-trained language models onto the output of the first LSTM layer. This provides an improvement on NER and chunking tasks.\n\nhttps://i.imgur.com/Hso3mL9.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.18653/v1/P17-1161"
    },
    "1073": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GulcehreFXCBLBS15",
        "transcript": "The authors extend a seq2seq model for MT with a language model. They first pre-train a seq2seq model and a neural language model, then train a separate feedforward component that takes the hidden states from both and combines them together to make a prediction. They compare to simply combining the output probabilities from both models (shallow fusion) and show improvement on different MT datasets.\n\nhttps://i.imgur.com/zD9jb4K.png",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1503.03535"
    },
    "1074": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZhangW15b",
        "transcript": "The authors perform a hyperparameter search for a single-layer CNN on 9 different sentence classification datasets.\nThey find that the optimal embedding initialisation, filter size and number of feature maps depends on the dataset and should be chosen through a search; ReLU and tanh are the best activation functions; 1-max pooling is the pooling method; dropout may help when the number of feature maps gets large.\n\nhttps://i.imgur.com/uUXVwb5.png",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1510.03820"
    },
    "1075": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LeeLZ17",
        "transcript": "The authors propose a simplified version of LSTMs. Some non-linearities and weighted components are removed, in order to arrive at the recurrent additive network (RAN). The model is evaluated on 3 language modeling datasets: PTB, the billion word benchmark, and character-level Text8.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.07393"
    },
    "1076": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eacl/InuiRSS17",
        "transcript": "They propose a neural architecture for assigning fine-grained labels to detected entity types. The model combines bidirectional LSTMs, attention over the context sequence, hand-engineered features, and the label hierarchy. They evaluate on Figer and OntoNotes datasets, showing improvements from each of the extensions.\n\nhttps://i.imgur.com/HJL3CYy.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://aclanthology.info/papers/E17-1119/e17-1119"
    },
    "1077": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/MrksicSWTY17",
        "transcript": "They propose neural models for dialogue state tracking, making a binary decision for each possible slot-value pair, based on the latest context from the user and the system. The context utterances and the slot-value option are encoded into vectors, either by summing word representations or using a convnet. These vectors are then further combined to produce a binary output. The systems are evaluated on two dialogue datasets and show improvement over baselines that use hand-constructed lexicons.\n\nhttps://i.imgur.com/G4rm954.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.18653/v1/P17-1163"
    },
    "1078": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/XieWLLNJN17",
        "transcript": "The paper investigates better noising techniques for RNN language models.\n\nhttps://i.imgur.com/cq5Kb0Y.png\n\nA noising technique from previous work would be to randomly replace words in the context or replace them with a blank token. Here they investigate ways of choosing better which words to replace and choosing the replacements from a better distribution, inspired by methods in n-gram smoothing. They show improvement on language modeling (PTB and text8) and machine translation (English-German).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.02573"
    },
    "1079": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/GutierrezSMB16",
        "transcript": "The paper investigates compositional semantic models specialised for metaphors.\n\nhttps://i.imgur.com/OnoJK3h.png\n\nThey construct a dataset of 8592 adjective-noun phrases, covering 23 different adjectives, annotated for being metaphorical or literal. They then train compositional models to predict the phrase vector based on the noun vector, as a linear combination with an adjective-specific weight matrix. They show that it\u2019s better to learn separate adjective matrices for literal and metaphorical uses of each adjective, even though the amount of training data is smaller.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P16/P16-1018.pdf"
    },
    "1080": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eacl/SogaardB17",
        "transcript": "The authors investigate the benefit of different task combinations when performing multi-task learning.\n\nhttps://i.imgur.com/VmD2ioS.png\n\nThey experiment with all possible pairs of 10 sequence labeling datasets, switching between the datasets during training. They find that multi-task learning helps more when the main task quickly plateaus while the auxiliary task does not, likely helping the model out of local minima.\nThere does not seem to be any auxiliary task that would help on all main tasks, but chunking and semantic tagging seem to perform best.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://aclanthology.info/papers/E17-2026/e17-2026"
    },
    "1081": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/YogatamaBDGL16",
        "transcript": "The aim is to have the system discover a method for parsing that would benefit a downstream task.\n\nhttps://i.imgur.com/q57gGCz.png\n\nThey construct a neural shift-reduce parser \u2013 as it\u2019s moving through the sentence, it can either shift the word to the stack or reduce two words on top of the stack by combining them. A Tree-LSTM is used for composing the nodes recursively. The whole system is trained using reinforcement learning, based on an objective function of the downstream task. The model learns parse rules that are beneficial for that specific task, either without any prior knowledge of parsing or by initially training it to act as a regular parser.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1611.09100"
    },
    "1082": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=bojanowski2017enriching",
        "transcript": "They extend skip-grams for word embeddings to use character n-grams. Each word is represented as a bag of character n-grams, 3-6 characters long, plus the word itself. Each of these has their own embedding which gets optimised to predict the surrounding context words using skip-gram optimisation. They evaluate on word similarity and analogy tasks, in different languages, and show improvement on most benchmarks.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1083": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eacl/ClarkSB17",
        "transcript": "They propose using attribute-based vectors for detecting metaphorical word pairs.\n\nhttps://i.imgur.com/dgDjSvu.png\n\nTraditional embeddings (word2vec and count-based) are mapped to attribute vectors, using a supervised system trained on McRae norms. These vectors for a word pair are then given as input to an SVM classifier and trained to detect metaphorical (black humour) vs literal (black dress) word pairs. They show that using the attribute vectors gives higher F score over using the original vector space.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://aclanthology.info/papers/E17-2084/e17-2084"
    },
    "1084": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.05397",
        "transcript": "They describe a version of reinforcement learning where the system also learns to solve some auxiliary tasks, which helps with the main objective.\n\nhttps://i.imgur.com/fmTVxvr.png\n\nIn addition to normal Q-learning, which predicts the downstream reward, they have the system learning 1) a separate policy for maximally changing the pixels on the screen, 2) maximally activating units in a hidden layer, and 3) predicting the reward at the next step, using biased sampling. They show that this improves learning speed and performance on Atari games and Labyrinth (a Quake-like 3D game).\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.05397"
    },
    "1085": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.03530",
        "transcript": "The authors investigate the generalisation properties of several well-known image recognition networks.\n\nhttps://i.imgur.com/km0mrVs.png\n\nThey show that these networks are able to overfit to the training set with 100% accuracy even if the labels on the images are random, or if the pixels are randomly generated. Regularisation, such as weight decay and dropout, doesn\u2019t stop overfitting as much as expected, still resulting in ~90% accuracy on random training data. They then argue that these models likely make use of massive memorization, in combination with learning low-complexity patterns, in order to perform well on these tasks.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.03530"
    },
    "1086": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/WangLL16",
        "transcript": "The authors present a neural model that maps images and sentences into the same space, in order to perform cross-modal retrieval \u2013 find images based on a sentence or find sentences based on an image.\n\nhttps://i.imgur.com/DCFYzN8.png\n\nThe image vectors come from a pre-trained VGG image detection network. The sentence vectors are constructed using Fisher vectors, but they also explore simpler options, such as mean word2vec vectors and tfidf. Both are then mapped through nonlinearities and normalised, and Euclidean distance is used to measure vector similarity. They also investigate the task of mapping noun phrases from the image caption to specific areas of the image.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2016.541"
    },
    "1087": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/DongZ16",
        "transcript": "The authors investigate convolutional networks for essay scoring. They use a two-level convolution \u2013 first over words and then over sentences. Evaluation is performed on the Kaggle ASAP dataset, training separate models on individual topics, and also reporting some cross-topic results.\n\nhttps://i.imgur.com/WmNqgGm.png",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D16/D16-1115.pdf"
    },
    "1088": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/KiddonZC16",
        "transcript": "They describe a neural model for text generation, which keeps track of a checklist of items that need to be mentioned in the text.\n\nhttps://i.imgur.com/yKSIpza.png\n\nThe basic system is an encoder-decoder GRU model for text generation. On top of that, the model uses attention over items that need to be mentioned and items that have already been mentioned, both of which are encoded as vectors. An additional cost objective encourages the checklist to be filled by the end of the text. Evaluation is performed on recipe and dialogue generation.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D16/D16-1032.pdf"
    },
    "1089": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/TaghipourN16",
        "transcript": "The authors construct a neural network for automated essay scoring.\n\nhttps://i.imgur.com/XTWGpmy.png\n\nConvolution window of 3 is passed over the text, which is used as input to an LSTM. The output of the LSTM is averaged over all timesteps and then a single value in the range of [0,1] is predicted as a scaled-down score for the essay. They evaluate by measuring quadratic weighted Kappa on the Kaggle essay scoring dataset.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D16/D16-1193.pdf"
    },
    "1090": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1038/nature20101",
        "transcript": "The DeepMind guys present an extension to the Neural Turing Machine architecture.\n\nhttps://i.imgur.com/4TC6yAp.png\n\nThey call it a Differentiable Neural Computer (DNC) and it uses 1) an attention mechanism to access information in a matrix that acts as a memory, 2) an attention mechanism to save information to that memory, and 3) a transition matrix that stores information about the order in which rows in the memory are modified, in order to better handle sequential information. They test on the bAbI question answering dataset, a graph inference task, and on solving a puzzle of arranging blocks.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1038/nature20101"
    },
    "1091": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/QuFZHB16",
        "transcript": "The authors tackle the problem of domain adaptation for NER, where the label set of the target domain is different from the source domain.\n\nThey first train a CRF model on the source domain. Next, they train a LR classifier to predict labels in the target domain, based on predicted label scores from the model. Finally, the weights from the classifier are used to initialise another CRF model, which is then fine-tuned on the target domain data.\n\nhttps://i.imgur.com/zwSB7qN.png",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D16/D16-1087.pdf"
    },
    "1092": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/KielaVC16",
        "transcript": "The authors compare different image recognition models and image data sources for multimodal word representation learning.\n\nhttps://i.imgur.com/iHwCSks.png\n\nImage recognition models used for vector generation\n\nExperiments are performed on SimLex-999 (similarity) and MEN (relatedness). The performance of different models (AlexNet, GoogLeNet, VGGNet) is found to be quite similar, with VGGNet performing slightly better at the cost of requiring more computation. Using search engines for image sources gives good coverage; ImageNet performs quite well with VGGNet; ESP Game dataset gave the lowest performance. Combining visual and linguistic vectors was found to be beneficial on both English and Italian.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D16/D16-1043.pdf"
    },
    "1093": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/naacl/SchwartzRR16",
        "transcript": "They train word2vec skip-gram embeddings using coordinations as context. They use 11 manual patterns to extract coordinations (eg \u201cX and Y\u201d, \u201ceither X or Y\u201d, etc). From \u201cboats or planes\u201d, \u201cboats\u201d will be a context of \u201cplanes\u201d and \u201cplanes\u201d will be a context of \u201cboats\u201d.\n\nThey evaluate on SimLex-999 and find that this performs badly on nouns. However, it beats normal skip-gram and dependency-based skip-gram on verbs and adjectives.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/N/N16/N16-1060.pdf"
    },
    "1094": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/naacl/JagannathaY16",
        "transcript": "The authors have a dataset of 780 electronic health records and they use it to detect various medical events such as adverse drug events, drug dosage, etc. The task is done by assigning a label to each word in the document.\n\nhttps://i.imgur.com/bZ7yM0z.png\n\nAnnotation statistics for the corpus of health records.\n\nThey look at CRFs, LSTMs and GRUs. Both LSTMs and GRUs outperform the CRF, but the best performance is achieved by a GRU trained on whole documents.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/N/N16/N16-1056.pdf"
    },
    "1095": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1603.00892",
        "transcript": "They describe a method for augmenting existing word embeddings with knowledge of semantic constraints. The idea is similar to retrofitting by Faruqui et al. (2015), but using additional constraints and a different optimisation function.\n\nhttps://i.imgur.com/zedR5FV.png\n\n\nExisting word vectors are further optimised to 1) have high similarity for known synonyms, 2) have low similarity for known antonyms, and 3) have high similarity to words that were highly similar in the original space. They evaluate on SimLex-999, showing state-of-the-art performance. Also, they use the method to improve a dialogue tracking system.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1603.00892"
    },
    "1096": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/naacl/ShutovaKM16",
        "transcript": "They build a system for detecting metaphors (\u201cblind alley\u201d, \u201chonest meal\u201d, etc) from literal word pairs.\n\nhttps://i.imgur.com/Bv1gIb2.png\n\nAnnotated metaphor examples from Tsvetkov et al. (2014), used in this work.\n\nThe basic system uses word embedding similarity \u2013 cosine between the word embeddings. Then they explore variations using phrase embeddings, cos(phrase-word2, word2), which is similar to the operations with word regularities by Mikolov.\n\nFinally, they create vector representations for words and phrases using visual information. The words are used as queries in Google Image Search, and the returned images are passed through an image detection network in order to obtain vector representations. The best final system performs the task separately using linguistic and visual vectors, and then combines the resulting scores.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/N/N16/N16-1020.pdf"
    },
    "1097": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/SpithourakisAR16",
        "transcript": "They create an LSTM neural language model that 1) has better handling of numerical values, and 2) is conditioned on a knowledge base.\n\nhttps://i.imgur.com/Rb6V1Hy.png\n\nFirst the the numerical value each token is given as an additional signal to the network at each time step. While we normally represent token \u201c25\u201d as a normal word embedding, we now also have an extra feature with numerical value float(25). Second, they condition the language model on text in a knowledge base. All the information in the KB is converted to a string, passed through an LSTM and then used to condition the main LM.\n\nThey evaluate on a dataset of 16,003 clinical records which come paired with small KB tuples of 20 possible attributes. The numerical grounding helps quite a bit, and the best results are obtained when the KB conditioning is also added.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D16/D16-1101.pdf"
    },
    "1098": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/ZhangXSDZ16",
        "transcript": "They start with the neural machine translation model using alignment, by Bahdanau et al. (2014), and add an extra variational component.\n\nhttps://i.imgur.com/6yIEbDf.png\n\nThe authors use two neural variational components to model a distribution over latent variables z that captures the semantics of a sentence being translated. First, they model the posterior probability of z, conditioned on both input and output. Then they also model the prior of z, conditioned only on the input. During training, these two distributions are optimised to be similar using Kullback-Leibler distance, and during testing the prior is used. They report improvements on Chinese-English and English-German translation, compared to using the original encoder-decoder NMT framework.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D16/D16-1050.pdf"
    },
    "1099": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/bea/YuanBF16",
        "transcript": "They improve an existing error correction system by re-ranking its predictions. The basic approach uses machine translation to perform error correction on learner texts \u2013 the incorrect text is essentially translated into correct text. Here, they include a ranking SVM to score and reorder the n-best lists from the translation model.\n\nThe reranking features include various internal scores from the translation model, the rank in the original ordering, language model probabilities trained on large corpora, language model scores based on only the n-best list, word-level translation probabilities, and sentence length features. They show improvement on two error correction datasets.\n\nhttps://i.imgur.com/RxAE11a.png\n\nExample output from the models.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/W/W16/W16-0530.pdf"
    },
    "1100": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/YangM16",
        "transcript": "They propose a joint model for 1) identifying event keywords in a text, 2) identifying entities, and 3) identifying the connections between these events and entities. They also do this across different sentences, jointly for the whole text.\n\nhttps://i.imgur.com/ETKZL7V.png\n\nExample of the entity and event annotation that the system is modelling.\n\nThe entity detection part is done with a CRF; the structure of an event is learned with a probabilistic graphical model; information is integrated from surrounding sentences using a Stanford coreference system; and these are all tied together across the whole document using Integer Linear Programming.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1609.03632"
    },
    "1101": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/BingelBS16",
        "transcript": "They incorporate fMRI features into POS tagging, under the assumption that reading semantically/functionally different words will activate the brain in different ways. For this they use a dataset of fMRI recordings, where the subjects were reading a chapter of Harry Potter. The main issue is that fMRI has very low temporal resolution \u2013 there is only one fMRI reading per 4 tokens, and in general it takes around 4-14 seconds for something to show up in fMRI. Nevertheless, they construct token-level vectors by using a Gaussian weighted average, integrate them into an unsupervised POS tagger, and show that it is able to improve performance.\n\nhttps://i.imgur.com/TU60N6w.png",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P16/P16-1071.pdf"
    },
    "1102": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.02533",
        "transcript": "Adversarial examples are datapoints that are designed to fool a classifier. For example, we can take an image that is classified correctly using a neural network, then backprop through the model to find which changes we need to make in order for it to be classified as something else. And these changes can be quite small, such that a human would hardly notice a difference.\n\nhttps://i.imgur.com/pkK570X.png\n\nExamples of adversarial images.\n\nIn this paper, they show that much of this property holds even when the images are fed into the classifier from the real world \u2013 after being photographed with a cell phone camera. While the accuracy goes from 85.3% to 36.3% when adversarial modifications are applied on the source images, the performance still drops from 79.8% to 36.4% when the images are photographed. They also propose two modifications to the process of generating adversarial images  \u2013 making it into a more gradual iterative process, and optimising for a specific adversarial class.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1607.02533"
    },
    "1103": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/naacl/BulatKC16",
        "transcript": "The task is to predict feature norms \u2013 object properties, for example is_yellow and is_edible for the word banana. They experiment with adding in image recognition features, in addition to using distributional word vectors.\n\nAn input word is used to retrieve 10 images from Google, these are passed through an ImageNet classifier to get feature vectors, and then averaged to get a vector representation for that word. A supervised model (partial least-squares regression) is then trained to predict vectors of feature norms based on the input vectors (image-based, distributional, or a combination). Including the image information helps quite a bit, especially for detecting properties like colour and shape.\n\nhttps://i.imgur.com/4TYKhvm.png\n\nExamples of predicted feature norms using the visual features.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/N/N16/N16-1071.pdf"
    },
    "1104": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/SuGMRUVWY16",
        "transcript": "The goal is to improve the training process for a spoken dialogue system, more specifically a telephone-based system providing restaurant information for the Cambridge (UK) area. They train a supervised system which tries to predict the success on the current dialogue \u2013 if the model is certain about the outcome, the predicted label is used for training the dialogue system; if the model is uncertain, the user is asked to provide a label. Essentially it reduces the amount of annotation that is required, by choosing which examples should be annotated through active learning.\n\nhttps://i.imgur.com/dWY1EdE.png\n\nThe dialogue is mapped to a vector representation using a bidirectional LSTM trained like an autoencoder, and a Gaussian Process is used for modelling dialogue success.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P16/P16-1230.pdf"
    },
    "1105": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/JiWTGTG17",
        "transcript": "Proposing character-based extensions to a neural MT system for grammatical error correction. OOV words are represented in the encoder and decoder using character-based RNNs. They evaluate on the CoNLL-14 dataset, integrate probabilities from a large language model, and achieve good results.\n\nhttps://i.imgur.com/r0Bsxp5.png",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.18653/v1/P17-1070"
    },
    "1106": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.04087",
        "transcript": "Inducing word translations using only monolingual corpora for two languages. Separate embeddings are trained for each language and a mapping is learned though an adversarial objective, along with an orthogonality constraint on the most frequent words. A strategy for an unsupervised stopping criterion is also proposed.\n\nhttps://i.imgur.com/HmME09P.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1710.04087"
    },
    "1107": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/ChenBM16",
        "transcript": "Hermann et al (2015) created a dataset for testing reading comprehension by extracting summarised bullet points from CNN and Daily Mail. All the entities in the text are anonymised and the task is to place correct entities into empty slots based on the news article.\n\nhttps://i.imgur.com/qeJATKq.png\n\nThis paper has hand-reviewed 100 samples from the dataset and concludes that around 25% of the questions are difficult or impossible to answer even for a human, mostly due to the anonymisation process. They present a simple classifier that achieves unexpectedly good results, and a neural network based on attention that beats all previous results by quite a margin.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P16/P16-1223.pdf"
    },
    "1108": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1905-10295",
        "transcript": "### Key points\n\n- Instead of just focusing on supervised learning, a self-critique and adapt network provides a unsupervised learning approach in improving the overall generalization. It does this via transductive learning by learning a label-free loss function from the validation set to improve the base model.\n- The SCA framework helps a learning algorithm be more robust by learning more relevant features and improve during the training phase.\n\n### Ideas\n\n1. Combine deep learning models with SCA that help improve genearlization when we data is fed into these large networks.\n2. Build a SCA that focuses not on learning a label-free loss function but on learning quality of a concept.\n\n### Review\n\nOverall, the paper present a novel idea that offers a unsupervised learning method to assist a supervised learning model to improve its performance. Implementation of this SCA framework is straightforward and demonstrates promising results. This approach is finally contributing to the actual theory of meta-learning and learning to learn research field. SCA framework is a new step towards self-adaptive learning systems. Unfortunately, the experimentation is insufficient and provided little insight into how this framework can help in cases where task domains vary in distribution or in concept.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1905.10295"
    },
    "1109": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1805.08296",
        "transcript": "# Keypoints\n- Proposes the HIerarchical Reinforcement learning with Off-policy correction (**HIRO**) algorithm.\n - Does not require careful task-specific design.\n - Generic goal representation to make it broadly applicable, without any manual design of goal spaces, primitives, or controllable dimensions.\n- Use of off-policy experience using a novel off-policy correction.\n- A two-level hierarchy architecture\n - A higher-level controller outputs a goal for the lower-level controller every **c** time steps and collects the rewards given by the environment, being the goal the desired change in state space\n - The lower level controller has the goal given added to its input and acts directly in the environment, the reward received is parametrized from the current state and the goal.\n\n# Background\nThis paper adopts a standard continuous control reinforcement learning setting, in which an agent acts on an environment that yields a next state and a reward from unknown functions. This paper utilizes the TD3 learning algorithm.\n\n## General and Efficient Hierarchical Reinforcement Learning\n\nhttps://i.imgur.com/zAHoWWO.png\n\n## Hierarchy of Two Policies\n\nThe higher-level policy $\\mu^{hi}$ outputs a goal $g_t$, which correspond directly to desired relative changes in state that the lower-level policy $\\mu^{lo}$ attempts to reach. $\\mu^{hi}$ operates at a time abstraction, updating the goal $g_t$ and collecting the environment rewards $R_t$ every $c$ environment steps, the higher-level transition $(s_{t:t+c\u22121},g_{t:t+c\u22121},a_{t:t+c\u22121},R_{t:t+c\u22121},s_{t+c})$ is stored for off-policy training.\n\nThe lower-level policy $\\mu^{lo}$ outputs an action to be applied directly to the environment, having as input the current environment observations $s_t$ and the goal $g_t$. The goal  $g_t$ is given by $\\mu^{hi}$ every $c$ environment time steps, for the steps in between, the goal $g_t$ used by $\\mu^{lo}$ is given by the transition function $g_t=h(s_{t\u22121},g_{t\u22121},s_t)$, the lower-level controller reward is provided by the parametrized reward function $r_t=r(s_t,g_t,a_t,s_{t+1})$.  The  lower-level  transition $(s_t,g_t,a_t,r_t,s_{t+1}, g_{t+1})$ is stored for off-policy training.\n\n## Parameterized Rewards\n\nThe goal $g_t$ indicates a desired relative changes in state observations, the lower-level agent task is to take actions from state $s_t$ that yield it an observation $s_{t+c}$ that is close to $s_t+g_t$. To maintain the same absolute position of the goal regardless of state change, the goal transition model, used between $\\mu^{hi}$ updates every $c$ steps, is defined as:\n\n$h(s_t,g_t,s_{t+1}) =s_t+g_t\u2212s_{t+1}$\n\nAnd the reward given to the lower-level controller is defined as to reinforce reaching a state closer to the goal $g_t$, this paper parametrizes it by the function:\n$r(s_t,g_t,a_t,s_{t+1}) =\u2212||s_t+g_t\u2212s_{t+1}||_2$.\n\n## Off-Policy Corrections for Higher-Level Training\n\nThe higher-level transitions stored $(s_{t:t+c\u22121},g_{t:t+c\u22121},a_{t:t+c\u22121},R_{t:t+c\u22121},s_{t+c})$ have to be converted to state-action-reward transitions $(s_t,g_t,\u2211R_{t:t+c\u22121},s_{t+c})$ as they can be used in standard off-policy RL algorithms, however, since the lower-level controller is evolving, these past transitions do not accurately represent the actions tha would be taken by the current lower-level policy and must be corrected.\n\nThis paper correction technique used is to change the goal $g_t$ of past transitions using an out of date lower-level controller to a relabeled goal $g \u0303_t$ which is likely to induce the same lower-level behavior with the updated $\\mu^{lo}$. In other words, we want to find a goal $g \u0303_t$ which maximizes the probability $\u03bc_{lo}(a_{t:t+c\u22121}|s_{t:t+c\u22121},g \u0303_{t:t+c\u22121})$, in which the $\\mu^{lo}$ is the current policy and the actions $a_{t:t+c\u22121}$ and states $s_{t:t+c\u22121}$ are from the stored high level transition.\n\nTo approximately maximize this quantity in practice, the authors calculated the probability for 10 candidates $g \u0303_t$, eight candidate goals sampled randomly from a Gaussian centered at $s_{t+c}\u2212s_t$, the original goal $g_t$ and a goal corresponding to the difference $s_{t+c}\u2212s_t$.\n\n# Experiments\nhttps://i.imgur.com/iko9nCd.png\n\nhttps://i.imgur.com/kGx8fZv.png\n\nThe authors compared the $HIRO$ method to prior method in 4 different environments:\n- Ant Gather;\n- Ant Maze;\n- Ant Push;\n- Ant Fall.\n\nThey also performed an ablative analysis with the following variants:\n- With lower-level re-labeling;\n- With pre-training;\n- No off-policy correction;\n- No HRL.\n\n# Closing Points\n- The method proposed is interesting in the hierarchical reinforcement learning setting for not needing a specific design, the generic goal representation enables applicability without the need of designing a goal space manually;\n- The off-policy correction method enables this algorithm to be sample efficient;\n- The hierarchical structure with intermediate goals on state-space enables to better visualize the agent goals;\n- The paper Appendix elaborates on possible alternative off-policy corrections.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1805.08296"
    },
    "1110": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1708.09259",
        "transcript": "ScatterNets incorporates geometric knowledge of images to produce discriminative and invariant (translation and rotation) features i.e. edge information. The same outcome as CNN's first layers hold. So why not replace that first layer/s with an equivalent, fixed, structure and let the optimizer find the best weights for the CNN with its leading-edge removed.\nThe main motivations of the idea of replacing the first convolutional, ReLU and pooling layers of the CNN with a two-layer parametric log-based Dual-Tree Complex Wavelets Transform (DTCWT), covered by a few papers, were:\nDespite the success of CNNs, the design and optimizing configuration of these networks is not well understood which makes it difficult to develop these networks\nThis improves the training of the network as the later layers can learn more complex patterns from the start of learning because the edge representations are already present\nConverge faster as it has fewer filter weights to learn\nMy takeaway: a slight reduction in the amount of data necessary for training!\n\nOn CIFAR10 and Caltech-101 with 14 self-made CNN with increasing depth, VGG, NIN and WideResnet:\nWhen doing transfer learning(Imagenet): DTSCNN outperformed (\u201cuseful margin\u201d) all the CNN architectures counterpart when finetuning with only 1000 examples(balanced over classes). While on larger datasets the gap decreases ending on par with. However, when freezing the first layers on VGG and NIN, as in DTSCNN, the NIN results are in par with, while VGG outperforms!\n\nDTSCNN learns faster in the rate but reaches the same target with minor speedup (few mins)\n\nComplexity analysis in terms of weights and operations is missing\n\nDatasets: CIFAR-10 & Caltech-101, is a good start point (further step with a substantial dataset like COCO would be a plus). For other modalities/domains, please try and let me know\n\nGreat work but ablation study is missing such as comparing full training WResNet+DTCWT vs. WResNet\n\n14 citation so far (Cambridge): probably low value per money at the moment\nhttps://i.imgur.com/GrzSviU.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1708.09259"
    },
    "1111": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/pami/DengM01",
        "transcript": "**Introduction**\n\nObject segmentation methods are often produced an imprecise result as objects frequently not always agree with homogeneous regions. Thus this paper provides segmentation of images and videos into homogeneous region in color and texture feature cues called JSEG. Assumptions for the environments used are:\n* Image contains homogeneous color and texture regions\n* Color is quantized \n* There are distinct colors in neighboring regions\n\n**Related work**\n* Present work in image segmentation requires texture model parameter approximation that often needs the homogeneous region to produce good derivation.\n* There is an existing technique for segmentation using motion. However, this method is not dependable in noisy data, insufficiency in affine transformation for close-up motion, and errors in the presence of occlusion.\n\n**Approach**\n* Method consists of two stages that are color quantization and image segmentation spatially.\n* Colors quantized into several appointed classes to distinguish regions by weighting pixels individually using the Lloyd algorithm.\n* Result of quantized colors are assigned labels. These labels or class-map also define the composition of textures.\n\nhttps://i.imgur.com/2AlFD7Z.png\n\n* Class maps are labeled by three symbols that are: *, + and o.\n* Symbols indicate positions where line of segmentations need to be drawn, for example a class map with half of the left region that contains + symbol and the other half contains uniform distribution of \\* and o can be segmented into two regions: one with + symbol and the other is a collection of * and o symbols. \n* Variance from the class map is computed and the value J is computed using the variance of both the same class and different class.\n* Value of J is small when image contains a uniform distribution of color classes and large otherwise.\n* The definition of J initiate an assumption of states of the class labels and specify information where line of segmentation could be drawn.\n* In the segmented region, the mean of J is calculated and the minimized value of J mean is a criterion to segment image given region numbers.\n* In a good segmentation, the value of J means is small as the number of colors that are uniformly distributed is small in the divided region.\n* Algorithm of spatial segmentation contains several stages: calculate J values in each region, growing regions by using seeds, and merging regions once scale has exceeded the threshold. Described as follows:\n\nhttps://i.imgur.com/uZzgrOO.png\n\n* Local J values applied as it has the property to indicate whether an area is in a region or near boundary of a region\n* Windows are used to detect region sizes. Large windows classify boundary of texture cues and small windows classify color or intensity edges.\n* Multiple sizes of windows are utilized with the circular shape of diameter 9 pixels for the smallest window.\n* To grow region using seeds, seeds are set first by finding mean and standard deviation of local J values, setting threshold by adding mean and standard deviation multiplied by preset values, and seed is fixed once it consists of an area larger than predetermined values for each window pixels.\n* Seeds is then grown by: removing empty classifications from fixed seeds before, averaging local J values in unsegmented region where if a region is near to only one seed, it is classified as the corresponding seed's region, calculating J values for smaller region, averaging more local J values in the respected remaining unsegmented region and growing region at the smallest scale.\n* Similarities in color built the merging of a region. As colors have quantized in histogram bins, distance is calculated between two histograms using Euclidean distance in CIE LUV color space.\n* To merge regions, distances are enlisted and pairs with small distance are joined. Next, a new feature vector is computed and process iterates for merging and generating new feature vectors until the maximum threshold is attained.\n* JSEG can be implemented in video data by using movements of objects as indirect constraints for tracking and segmentation. The assumption used for implementation is that videos have been parted into shots and shots are continuous scenes.\n* Video is decomposed in the spatiotemporal domain and grouped to be segmented for consecutive frames.\n* In this paper, 1000 frames are grouped and quantized for its color to generate class-maps\n* In frames that have color textures that are close to each other, they are counted as one object.\n* After seeds are fixed from the frames, tracking is done by assigning initial seed, overlapped seeds are considered to be one, iteratively checking overlapped seeds, and assign time duration for objects\n* To reduce the number of false merges, The value of J track is computed by calculating mean and standard deviance between two frames. When the region is static, the J value will be small and large otherwise\n* The running time for JSEG in video segmentation is equal to the application of image segmentation by grouped frames\n* Overall, parameters that need to be adjusted in using JSEG are color quantization threshold, number of image scales and object duration for video segmentation\n* In video segmentation, in average frames can be grouped as 10 to 15 frames \n\n**Paper contributions**\n* Paper provides a new method called JSEG to segment objects by spatial segmentation and color quantization in images and videos unsupervised.\n* Criteria to evaluate a good segmentation in an image is proposed.\n* Final segmentation is obtained by dividing region based on seed areas from J-image.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/34.946985"
    },
    "1112": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/cvpr.2014.118",
        "transcript": "**Introduction**\n* Salient region is an area where a striking combination of features in images is perceived at the first observation.\n* These features combined to make a region that has a significant distinction with other areas in the image.\n* This paper presents a map of saliency using linear combination of high-dimensional color representation space.\n\n**Related work**\n* Present work in saliency detection is divided into two groups which are taking into account low-level features, and statistical learning methods. Both approach has variety of results with no clear significance which performs better in saliency detection.\n* In the first group of taking into account of low-level features, there are approaches on saliency region detection based on color contrast, Markovian approach, and multi-scale saliency based on superpixels that have drawbacks on the pres.ence of high-contrast edges, unpreserved boundary, and segmentation parameters.\n* Using learning-based methods present are region detection saliency based on the regional descriptor, using graph-based, and sample patches.\n\n**Approach**\n* Paper provides a method of mapping low dimensional color of RGB, CIELab, HSV spaces into high dimensional color.\n* Method identify the presence of superpixel saliency features using the SLIC method with 500 number of superpixels.\n* Feature vector is defined by the location of superpixels and then combined with color which proceeds to color space representation computation.\n* Histogram features are later combined with 8 bins in each of the histogram and distance is computed using the Chi-squared method.\n* Global and local contrast are used using Euclidean distance with variance parameters of 0.25.\n* Histogram of gradients method with 31 dimensions is used to extract shape and texture features in the superpixels. As backgrounds tend to have more blur features in the pixel, separation of backgrounds is done using Singular Value Feature which algorithm is following the concept of Eigen images with weight acquired by using Singular Value Decomposition.\n* 75 dimension of feature vectors are then obtained for saliency detection. These feature maps are combined from all the superpixel operations mentioned. Features included are location features, color features, color histogram features, color contrast features, shape and texture features.\n* Regression algorithm is then applied to feature vectors. As large databases including 2000 images in a dataset are tested, the best present approach for the algorithm is a random forest. An unlimited number of nodes are applied with 200 trees.\n* To construct the transformation into high-dimensional color, a Trimap is built by dividing the starting saliency map to three different regions that are 2x2, 3x3, and 4x4. Then seven level of adaptive thresholding is applied to each subregion which then produces 21-level of the locally thresholded map.\n* Global threshold construct the trimap by the division of local levels, when the levels are more and equal to 18 levels, the map is defined by 1 and when the levels are less and equal than 6 levels, map is defined by 0.\n\nhttps://i.imgur.com/4eF1UZd.png\n\n* High dimensional color is used as it combines all the benefits of color representation properties. Nonlinear color representation in RGB and its gradients, CIELab color representation, and saturation and hue in HSV are combined. This produces 11 color channels.\n* Linear RGB values are not combined as it counteracts with YUV/YIQ color space.\n* Then gamma correction in the range of 0.5 to 2 with 0.5 intervals is applied, generating 44 high dimensional color vector.\n* Background that had been separated from the foreground in trimap is then examined to approximate color coefficients' linear combination using the minimum of least square problem of two matrices. The first matrix is a vector with binary value with 0 represents background and 1 represents foreground. The second matrix is color samples multiplied by a coefficient vector.\n* The map of the saliency region is then built by the summation of color samples applied to the coefficient vector estimation. The whole method is iterated three times to construct a stable saliency map.\n* The map is then refined by spatial information by adding more weights to pixels that contain foreground region. It is defined by exponential to the power of a parameter 0.5 applied to the minimum Euclidean distance for both foreground and background. It is concluded in the image below:\n\nhttps://i.imgur.com/k9heDiw.png\n\n* Algorithm is evaluated using three datasets that contain 5000 images in the MSRA dataset, 1000 images in the ECCSD dataset, and 643 multiple objects in the Interactive co-segmentation dataset.\n* Eight saliency region detection are compared using precision and recall measurements. The algorithms compared are LC, HC, LR, SF, HS, GMR, and DRFI.\n* F-measurement to evaluate performance is computed by application of precision-recall, and a quadratic parameter added by one that is divided by the summation of precision, recall, and the quadratic parameter. The quadratic parameter has a value of 0.3.\n* Algorithm performance placed at the second-best compared to all other methods.\n\n**Notes on The Paper**\n* Paper provides an algorithm that performs well for detecting saliency based on colors by generating features that are high dimensional from low-level features.\n* In high dimensional color space, salient regions are able to be separated from the background (Rahman, I et al. 2016).\n* If the algorithm is further developed using classifier, it is then able to integrate richer features as a higher dimension is present (Borji, A et al. 2017).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/cvpr.2014.118"
    },
    "1113": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eccv/MairHBSH10",
        "transcript": "**Introduction :**\n\nCorners, as feature cues in an image, is defined by two edge intersections. This definition has benefit in allowing precise location of the cue, although it is only valid when locality is maintained and the result is similar to the real corner location\n\n**Related work:**\n* Corner detector method present are SIFT global tracker that is using Difference of Gaussians and SURF that is using Haar wavelet to approximate Hessian determinat. These methods have drawback in high computation\n* FAST method is a corner detector that performs better than conventional corner detectors such as Harris corner detection. Drawbacks of this method is that it depends on the environment, therefore decision tree needs to be always constructed from scratch (ID3 greedy algorithm)\n\n**Approach:**\n* In detecting corners, discretized circle pixels are used to be compared with the center area. A circle with 3.4 diameter is used as test mask\n* Based on accelerated segment test, pixel is identified to be a corner when there are a number of pixel that has different value, either darker or lighter, than threshold of center pixel\n* The number of pixel that is used in this paper is the same as FAST-9 method, which nine size segment as it has the best performance\n* This number has the property to detect corners with some standard such that when different viewpoints are applied, it has the highest number of repeatability to detect corners correctly\n* FAST algorithm is building ternary tree which has three possible states that are darker, lighter, or similar, added by unknown state that leads to $4^N$ configuration space, while FAST-9 has dissimilarity in the circle's thickness which is increased to 3 pixels\n* Proposed corner detection describes the algorithm by testing one of the pixel with one question to pose. If a scenario of a pixel is given and question is evaluated, the next pixel and question in query is determined by the response\n* This algorithm expands the configuration space by adding not lighter and not darker which produces a binary tree representation in which evaluation can be done in each of the nodes, therefore the configuration space has the size of $6^N$\n* Memory is accessed by three types of cost which are second pixel comparison, same row pixel test, and other pixel test\n* To make decision tree optimal, a method that has resemblance with backward induction is formulated. Configuration space is explored using Depth First Search algorithm where each leaf is described whether it has satisfy corner criteria by accelerated segment test\n* Cost of each node is calculated by summation of minimum cost in each pair of child that has the positive or negative value with probability of pixel nodes both parent and child\n* The algorithm calculates the probability of an image whether it has homogeneous and structured areas then proceed to make a decision tree according to it. The distribution of probability contains three probabilities that are mirror state probability and similar state probability\n\nhttps://i.imgur.com/lIHh7gL.png\n\n* Algorithm is improved to make it generic by jumping from one optimized tree to another based on the configuration of the respected leaf once it has termination condition based on the corner criteria\n* This switching method has no cost as it happens in a leaf. However, it affects the time by one test delay. Thus, AGAST can only perform worse than FAST once it needs to jump between either homogeneous or heterogeneous pixels consecutively\n* Corner detection is compared using three pixel mask sizes that are 4, 12 and 16. Comparison is done by applying Gaussian noise and blur to database of variety viewpoints of checkerboard database\n* As limited computation using conventional computers are present, four state configuration space is used using three different arc lengths that are 9, 10 and 11\n* As the mask and arc length enlarged, the more features found. Small arc defines the location of the real corner\n* Large pattern leads to slower computation as it has more process to be done in order to detect the corner or evaluating features and it needs more advanced computing memory\n* Smaller pattern can also lead to elimination of feature detection as the features located near to each other, therefore in smaller feature, post processing technique is removed\n* When database is added by Gaussian blur and noise, the combination of pixel mask of 16 and arc length of 9 is more robust against the disturbance, therefore, repeatability is controlled by arc\n* Decision tree is also evaluated by computing the response time of corner detection. This is done by calculating the tests number that has possible pixel arrangement in a mask \n* Pixels arrangements that have close similarities are grouped. By observing the standard deviation, the group with large number of pixels that are alike has unbalanced decision tree. This happens as possible pixel arrangements are limited. Observed as follows:\n\nhttps://i.imgur.com/DLqCXhy.png\n\n* However, when one tree is used, adaptive tree has better performance than the conventional method\n* In comparison for trees that has different weight, the algorithm that jumps between trees or AGAST is optimized when the value of the weights are 0.1 and 1/3\n* Performance of AGAST is also tested by comparison with FAST-9 where uniform probability distribution are used to make the trees\n* Both algorithm are tested on five scenes, which are laboratory, outdoor, indoor, aerial, and medical. The optimized tree can speed the corner detection up for 13$\\%$ while AGAST can speed up in the range of 23$\\%$ to over 30$\\%$ \n\n**Paper contributions**\n* Paper provides an improved FAST corner detector that is able to dynamically adapt with its environment while also processing an image input\n* AGAST method is improving its predecessors method in saving time spent to process the image and also memory that is being used\n* It is also able to find more keypoints in corner detection",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-642-15552-9_14"
    },
    "1114": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1901-08256",
        "transcript": "Often the best learning rate for a DNN is sensitive to batch size and hence need significant tuning while scaling batch sizes to large scale training. Theory suggests that when you scale the batch size by a factor of $k$ (in the case of multi-GPU training), the learning rate should be scaled by $\\sqrt{k}$ to keep the variance of the gradient estimator constant (remember the variance of an estimator is inversely proportional to the sample size?). But in practice, often linear learning rate scaling works better (i.e. scale learning rate by $k$), with a gradual warmup scheme. \n\nThis paper proposes a slight modification to the existing learning rate scheduling scheme called LEGW (Linear Epoch Gradual Warmup) which helps us in bridging the gap between theory and practice of large batch training.\n\nThe authors notice that in order to make square root scaling work well in practice, one should also scale the warmup period (in terms of epochs) by a factor of $k$. In other words, if you consider learning rate as a function of time period in terms of epochs, scale the periodicity of the function by $k$, while scaling the amplitude of the function by $\\sqrt{k}$, when the batch size is scaled by $k$. The authors consider various learning rate scheduling schemes like exponential decay, polynomial decay and multi-step LR decay and find that square root scaling with LEGW scheme often leads to little to no loss in performance while scaling the batch sizes. In fact, one can use SGD with LEGW with no tuning and make it work as good as Adam.\n\nThus with this approach, one can tune the learning rate for a small batch size and extrapolate it to larger batch sizes while making use of parallel hardwares.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1901.08256"
    },
    "1115": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.2478/pralin-2018-0002",
        "transcript": "**TL;DR:** This paper summarizes some of the practical tips for training a transformer model for MT task, though I believe some of the tips are task-agnostic. The parameters considered include number of GPUs, batch size, learning rate schedule, warmup steps, checkpoint averaging and maximum sequence lengths. \n\n**Framework used for the experiments:** [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor)  \n\nThe effect of varying the most important hyper-parameters on the performances are as follows:\n\n**Early Stopping:** Usually papers don't report the stopping criterion except in some vague terms (like number of days to train). The authors suggest that with a large dataset, even a very large model almost never converges and keeps improving by small amounts. So, keep training your model for long periods if your GPU budget supports such an option.\n\n**Data Preprocessing:** Mostly neural architectures these days use sub word units instead of words. It's better to create the sub word vocabulary using a sufficiently large dataset. Also, its advised to filter datasets based on *max_sequence_length* and store them (for eg. as TFRecords) before training your model and not do the filtering every epoch to save precious CPU time.\n\n**Batch Size:** Computational throughput (number of tokens executed per unit time) increases sub-linearly w.r.t. the batch size, which means after a particular number, increasing the batch size may not be that useful. From performance POV however, increasing the batch size usually leads to faster and better convergence. So, try using the maximum batch size, be it a single or multi-GPU training. Keep in mind, however, that due to random batching you may run out of memory suddenly even after days of training. So, leave some backup memory for such cases while increasing the batch size.\n\n**Dataset size:** Experiments from the paper reinforce the fact that with BIG models, more data is better. While comparing datasets of different sizes, its advised to train the models for long enough periods because the effect of dataset sizes kicks in usually after long periods.\n\n**Model size:** A bigger model even with a smaller batch size performs better than a smaller model with larger batch sizes after a few days of training. For debugging, use the smallest models btw!\n\n**Maximum sequence length:** Decreasing the *max_sequence_length* leads to more examples from the dataset excluded while allowing bigger batch sizes. So, its a trade-off. Often, the presence of more examples off-sets the gains from increasing batch sizes while training for enough time. But even such a gain plateaus after a sufficient sequence length, since very long sentences are often outliers and won't contribute much to performance gains. \n\n**Learning rate and Warm-up steps:** The usual advice of using a not-so-high and not-so-low learning rates apply here. Using large warm-up steps often off-set the damage caused by large learning rates. So does gradient clipping.\n\n**Number of GPUs:** For the fastest convergence, use as many GPUs as available. There would be no noticeable variation in the performances. There is a huge debate on scaling of learning rates while going from single to multiple GPUs, though the authors report that there is no significant variation while using the same learning rates, independent of the batch size (which increases with more GPUs)\n\n**Checkpoint averaging:** Averaging last n (=10) model checkpoints saved at 1hr/30mins intervals almost always leads to better performances. This is similar to Averaged SGD from AWD-LSTM (*Merity et al.*)\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.2478/pralin-2018-0002"
    },
    "1116": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/mia/BarrettM97",
        "transcript": "## **Introduction**\nThis paper presents a novel interactive tool for efficient and reproducible boundary extraction with minimal user input. Despite the user not being very accurate with the manual marking of the seed points, the algorithm snaps the boundary to the nearest strong object edge. Unlike active contour models where the user is unaware of how the final boundary will look like after energy minimization (which, if unsatisfactory, requires the entire process to be repeated again), this algorithm is interactive and therefore the user is aware of the \"live-wire boundary snapping\". Moreover, \"boundary cooling\" and \"on-the-fly training\" are two novel contributions of the paper which help reduce user input while maintaining the stability of the boundary.\n\n## **Method**\nModeling the boundary detection problem as a graph search, the new objective is to find the optimal path between a start node (pixel) to a goal node (pixel), where the total cost of any path is the sum of the local costs of the path. Let the local cost of a directed edge from pixel ${p}$ to a neighboring pixel ${q}$ be\n$$\n    l({p}, {q}) = \\underbrace{0.43f_Z({q})}_{\\text{Laplacian zero crossing}} + \\underbrace{0.43f_G({q})}_{\\text{gradient magnitude}} + \\underbrace{0.14f_D({p}, {q})}_{\\text{gradient direction}}\n$$\nwhere the weights have been empirically chosen by the authors.\n\nThe gradient magnitude $f_G({g})$ needs to be a term so that higher image gradients will correspond to lower costs in order to provide a \"first-order\" positioning of the live wire boundary. As such, the gradient magnitude G is defined as\n\n$$\n    f_G = 1 - \\frac{G}{max(G)}\n$$\nThe Laplacian zero crossing term is a binary edge feature and acts as a \"second order\" fine tuning term for boundary localization.\n\n$$\n  f_Z({q}) = \\left\\{\n  \\begin{array}{@{}ll@{}}\n    0, & \\text{if}\\ I_L({q}) = 0 \\: or \\: a \\: neighbor \\: with \\: opposite \\: sign \\\\\n    1, & \\text{otherwise} \\\\\n  \\end{array}\\right.\n$$\nwhere $I_L$ represents the convolution of the image with a Laplacian edge operator.\n\nThe gradient direction term is associated with penalizing sharp changes in the boundary direction and therefore effectively adds a boundary smoothness constraint.\n\nLet ${D(p)}$ be the unit vector normal to the image gradient at pixel ${p}$. Then the gradient direction cost can be represented as:\n$$\n    f_D({p}, {q}) = \\frac{2}{3\\pi}\\big\\{cos[d_p(({p}, {q})]^{-1} + cos[d_q(({p}, {q})]^{-1}\\big\\}\n$$ where\n$$\n    d_p(({p}, {q}) = {D(p).L(p,q)}\n    $$$$\n    d_p(({p}, {q}) = {L(p,q).D(q)}, \\: and \\text{} \\\\ \n    $$$$\n      {L(p,q)} = \\left\\{\n      \\begin{array}{@{}ll@{}}\n        {q-p}, & \\text{if}\\ {D(p).(q-p)} \\geq 0 \\\\\n        {p-q}, & \\text{otherwise} \\\\\n      \\end{array}\\right.\n$$\nwhere ${L(p,q)}$ represents the normalized the bidirectional edge between pixels ${p}$ and ${q}$ and represents the direction of the link between them so that the difference between ${p}$ and this direction is minimized. Intuitively, this cost is low when the gradient direction of the two pixels are similar to each other and the link between them.\n\nStarting at a user-selected seed point, the boundary finding is continued in the direction of the minimum cumulative cost, which creates a dynamic 'wavefront' along the directions to highest interest (which happen to be along the edges). For a path length of $n$, the boundary growing requires $n$ iterations, which is significantly more efficient than the previously used approaches.\n\nA distribution of a variety of features (such as the image gradient $f_G$ weighted with a monotonically decreasing function (either linear or Gaussian) determines the contribution of each of the closest $t$ pixels, and the algorithm follows the edge of current interest (rather than the strongest) and associates lower costs with current edge features and higher costs for edge features not belonging to the current edge, thereby performing a dynamic or \"on-the-fly\" training.\n\nThe algorithm also exhibits what the paper calls \"data-driven path cooling\" - as the cursor (and therefore the free point) moves further away from the seed point, progressively longer portions of the boundary become fixed and only the new parts of the boundary need to be updated.\n\n## **Results and Discussion**\nUsing the algorithm, the boundaries are extracted in one-fifth of the time required for manually tracing the boundary, while doing so with a 4.4x higher accuracy and 4.8x higher reproducibility. However, the paper admits that boundary detection can be difficult with objects with \"hair like boundaries\". Moreover, this technique cannot be extended to N-dimensional images.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q48317308"
    },
    "1117": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tmi/PluempitiwiriyawejMWH05",
        "transcript": "## **Introduction**\nThis paper presents a stochastic active contour model for image segmentation from cardiac MR images. The proposed algorithms aims to minimize an energy functional by the level set method while incorporating stochastic region based and edge based information as well as shape priors of the heart and local contour properties. Moreover, the algorithm also uses a parameter annealing component to dynamically balance the weightage of the components of the energy functional.\n\n\n## **Method**\nThe paper locates a contour $C$ in a cardiac MR image that segments the image into two groups - the heart and the background. The corresponding objective energy functional can be represented by\n$$\n    J(C) = \\lambda_1 J_1(C) + \\lambda_2 J_2(C) + \\lambda_3 J_3(C) + \\lambda_4 J_4(C)\n$$\n\nLooking at these individual components, we have\n* _Region Based Term: Model Matching_ **$J_1(\\phi)$**: In order to segment the image into 2 regions, let the two regions inside and outside the contour C be represented by $\\Omega_1$ and $\\Omega_2$ respectively. For each region, consider a stochastic model to describe the pixel statistics of that region. Assuming that the pixel intensities of all the pixels in each region are statistically independent, the objective is to minimize the negative log-likelihood  of pixels belonging to the correct regions.\n*  _Edge Based Term_ **$J_2(\\phi)$**: In order for the contour C to be aligned to the prominent edges in the image, the edge map of the image (which can be obtained by various image smoothing methods such as Gaussian kernel blurring, edge-preserving anisotropic diffusions, Min/Max flow algorithms, etc.) has to be minimized.\n* _Heart Shape Prior Term_ **$J_3(\\phi)$**: In order to distinguish between similar looking tissues in the foreground and the background, an elliptical heart shaped prior is used. An ellipse can be described with 5 parameters with certain constraints in the conic equation. \n* _Contour Smoothing Term_ **$J_4(\\phi)$**: In order to obtain a smooth contour of the segmented heart, the total Euclidean arc length of the contour C should be minimized.\n\nThe parameters ($\\lambda_1$, $\\lambda_2$, and $\\lambda_3$) of the energy functional $J(C)$ need to be dynamically updated during the energy minimization. For example, the region-based and the edge-based terms should have a higher weightage in $J(C)$ during the initial steps of the segmentation, and at the later stages, their weightage should be reduced and that of the shape prior should be increased in order to keep the segmented output similar to the desired shape.\n\n## **Results**\n\nThe two metrics used for assessing the performance were Area Similarity and Shape Similarity. The algorithm was tested on 48 images covering 143 contours, including manually annotated contours by an expert on six rat cardiac sequences of eight frames each, and the results indicated excellent segmentation agreement with the manually traced contours.\n\n## **Discussion and Shortcomings**\nSince STACS uses stochastic models instead of deterministic models, it can be applied to a large variety of images, and is especially helpful when distinguishing between visually similar adjacent regions.\n\nSince STACS incorporates both region-based and edge-based information in its energy functional, this makes it more robust to noise as well as reduces the susceptibility to curve initialization.\n\nPerhaps the most highlighting feature of STACS that distinguishes it from other active contour based models is that it incorporates shape based priors into the energy functional. This helps segment the heart from the chest wall, which is especially difficult since the two regions share similar texture.\n\nMoreover, the scheduled parameter annealing adjusts the weights of the components of the energy functional, which helps dynamically vary the importance to different components at different stages of the segmentation process.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q51469012"
    },
    "1118": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/83.902291",
        "transcript": "## **Introduction**\nThis paper presents an active contour model to detect and segment objects in images whose boundaries may or may not be defined by their gradients. The curve evolution is based on the Mumford Shah energy functional and the level set methods, making it less susceptible to curve initialization errors.\n\n\n## **Method**\nUsing an implicit shape representation, the boundary(s) C can be represented using the zero level set of an embedding function $\\phi : \\Omega \\rightarrow$ **R** such that\n\n$$\n    C = \\{x \\in \\Omega \\:|\\: \\phi(x) = 0\\}\n$$\n\nSuch a representation of the boundary does not require a choice of parameterization.\n\nThe embedding function $\\phi(.)$ is defined as \n$$\n    \\phi(x) = \\pm \\: distance(x,C)\n$$\nwhere the sign is either + or - depending upon whether the point is inside or outside the curve. This means that the boundary can be determined by finding out the point at which $\\phi$ changes its sign.\n\nAccording to the level set method [1], the embedding function is zero at all the points on the curve C at any time.\n\n$$\n    \\phi(C(t),\\:t) = 0 \\:\\: \\forall t\n$$\n\nConsider the piecewise Mumford Shah energy functional model [2] with 2 regions $\\Omega_{1}$ and $\\Omega_{2}$ such that $\\Omega = \\Omega_{1} + \\Omega_{2}$.\n\nLet us define the Heaviside step function as \n\n$$\n  H\\phi \\equiv H(\\phi)\\left\\{\n  \\begin{array}{@{}ll@{}}\n    1, & \\text{if}\\ \\phi > 0 \\: \\Rightarrow x \\in \\Omega_{1} \\\\\n    0, & \\text{otherwise}\\ \\Rightarrow x \\in \\Omega_{2} \\\\\n  \\end{array}\\right.\n$$\n\nMoving from an energy functional defined on the boundary C to one defined on the embedding function $\\phi$, we get\n\n$$\n    E(\\phi) = \\int\\limits_{\\Omega_{1}} \\: ({I}(x,y) - \\mu_{1})^{2} \\: dx + \\int\\limits_{\\Omega_{2}} \\: ({I}(x,y) - \\mu_{2})^{2} \\: dx + \\nu \\left| \\partial \\Omega_{1} \\right|\n$$\n\nwhere we approximate the average brightness/intensity of all the pixels in the $\\Omega_{1}$ and $\\Omega_{2}$ regions to be $\\mu_{1}$ and $\\mu_{2}$ respectively. The term $\\left| \\partial \\Omega_{1} \\right|$ represents the length of the boundary and is added as a penalizer/regularizer.\n\nSimplifying this expression, we get\n$$\n    \\begin{align*}\n    E(\\phi) & =\n    \\int\\limits_{\\Omega} \\: \\Big[\\big[({I}(x,y) - \\mu_{1})^{2} - ({I}(x,y) - \\mu_{2})^{2}\\big]H\\phi \\\\\n    & + ({I}(x,y) - \\mu_{2})^{2}\\Big]\\:dx + \\nu \\int\\limits_{\\Omega}\\left| \\nabla H\\phi \\right|\\:dx\n\\end{align*}\n$$\n\nSince $H(\\phi)$ is not differentiable everywhere, we assume a slightly smoothened Heaviside function such that\n\n$$\n    \\frac{\\mathrm{d} H(\\phi)}{\\mathrm{d} \\phi} = \\delta(\\phi)\n$$\n\nOne such choice of the smoothened delta function is\n\n$$\n    \\delta_{\\epsilon}(\\phi) = \\frac{1}{\\pi} \\frac{\\epsilon}{\\epsilon^{2} + \\phi^{2}}, \\: \\epsilon > 0\n$$\n\nNow, the gradient descent equation can be computed as \n$$\n    \\frac{\\partial \\phi}{\\partial t} =  \\delta(\\phi) \\Big[\\: \\nu\\:div\\Big(\\frac{\\nabla \\phi}{\\left|\\nabla \\phi\\right|}\\Big) + ({I}(x,y) - \\mu_{1})^{2} - ({I}(x,y) - \\mu_{2})^{2}\\Big]\n$$\n\nAs the embedding function $\\phi(.)$ evolves over time, the boundary points can be detected when it undergoes a sign change.\n\n\n## **Discussion and Shortcomings**\nAs compared to the edge-based segmentation approaches such as geodesic active contours [3], this method uses region based segmentation, and can, therefore, result in curve evolution that is not just local.\n\nThe segmentation results are not dependent on the initialization of the curve. Because of the choice of the embedding function $\\phi$ as a signed distance function, the curves can split and merge. Moreover, unlike geodesic active contours [3], new curves can also be formed. This is because of the numerical approximation of the delta function - since the delta function never actually goes to 0. This means that objects with interior contours such as a ring can also be segmented, which was not possible with previously available active contour models.\n\n#### [1] S. Osher, and J. A. Sethian, \"Fronts propagating with curvature-dependent speed: Algorithms based on Hamilton-Jacobi formulations,'' *Journal of Computational Physics*, vol. 79, no. 1, pp. 12-49, 1988.\n\n#### [2] D. Mumford, and J. Shah, \"Optimal approximations by piecewise smooth functions and associated variational problems,'' *Communications on Pure and Applied Mathematics*, vol. 42, no. 5, pp. 577-685, 2006.\n\n#### [3] V. Caselles, R. Kimmel, and G. Sapiro, \"Geodesic Active Contours,'' *International Journal of Computer Vision*, vol. 22, no. 1, pp. 61-79, February 1997.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/83.902291"
    },
    "1119": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/ijcv/KassWT88",
        "transcript": "## **Introduction**\nThis paper presents a generalized technique of matching a deformable model to an image through energy minimization. An active contour model called snakes has been proposed which is a continuous and deformable spline always minimizing its energy under the influence of image forces as well as internal energy of the spline and external constraint forces. Because of its dynamic nature, snakes can be applied to image segmentation tasks (edges, lines, and subjective contours) as well as to motion tracking and stereo matching.\n\n## **Method**\nA snake can be parametrically represented as\n$$\n    {v}(s) = (x(s), y(s))\n$$\n\nThe energy functional of a snake consists of the internal and the external energy components.\n$$\n    E_{snake}(C) = \\underbrace{E_{ext}(C)}_{\\text{data term}} + \\underbrace{E_{int}(C)}_{\\text{regularization term}}\n$$\n\nThis can be decomposed into the following components:\n$$\n    E^{*}_{snake} = \\int_{0}^{1} \\Big[\\: E_{int}({v}(s)) + E_{image}({v}(s)) + E_{constr}({v}(s)) \\:\\Big] \\: ds\n$$\n\nwhere the components represent the internal energy of the snake due to its shape and bending, the image forces, and the external constraint forces on the snake respectively.\n\nThe internal energy of the snake can be represented as \n$$\n    E_{int} = \\frac{1}{2} \\{\\alpha (s)\\left | {v}_{s}(s) \\right |^{2} \\:+\\: \\beta (s)\\left | {v}_{ss}(s) \\right |^{2}\\}\n$$\nwhere the first order term and the second order term represent the elastic length and the stiffness of the snake respectively. \n\nThe energy minimization is a $O(n)$ technique in time using sparse matrix methods (semi-implicit Euler method), which is is much faster than a $O(n^{2})$ fully explicit method.\n\nThe energy due to the image forces is composed of three components:\n$$\n    E_{image} = w_{line}E_{line} + w_{edge}E_{edge} + w_{terminal}E_{terminal}\n$$ where\n$$E_{line} = -\\big( G_{\\sigma} * \\nabla^{2}{I} \\big)^{2},$$ $$ E_{edge} = -\\left | \\nabla {I}(x,y) \\right |^{2},$$ $$E_{term} = \\frac{\\partial \\theta}{\\partial {n}_{\\perp}} = \\frac{C_{yy}C_{x}^{2} - 2C_{xy}C_{x}C_{y} + C_{xx}C_{x}^{2}}{(C_{y}^{2} + C_{x}^{2})^\\frac{3}{2}}$$\n\n* The minima of the $E_{line}$ energy functional define the edges of the image. Depending upon the sign of the $w_{line}$ term, the snake tries to align itself to the lightest or the darkest contour near it. Smoothening the image gradient with a Gaussian allows the snake to reach an equilibrium on a blurred energy functional, and when the blurring is slowly reduced, the snake stays in shape despite the small scale textural details because of its own smoothness constraint.\n    \n* Because of the $E_{edge}$ term, the snake is attracted to the contours corresponding to large image gradients.\n    \n* Consider $C(x,y)$ to be a slightly smoothened image obtained by applying a Gaussian filter to it. The $E_{terminal}$ represents the energy corresponding to the curvature of the level contours of $C(x,y)$, and can be calculated as the rate of change of the gradient angle $\\theta$ along the direction of the outer unit normal to the curve. The combination of $E_{edge}$ and $E_{terminal}$ ensures that during the energy minimization, the snake is attracted to the edges and/or the terminations.\n\nSnakes can also be applied to the problem of stereo matching by adding an additional constraint to the energy functional. The constraint is that the disparities in the human visual system do not change too rapidly with space, meaning the following disparity has to be minimized, where the ${v}^{L}(s)$ and the ${v}^{R}(s)$ represent the left and the right snake contours respectively.\n\n$$\n    E_{stereo} = \\big( {v}^{L}_{s}(s) - {v}^{L}_{s}(s) \\big)^{2}\n$$\n\nSimilarly, snakes can also be applied to the problem of motion tracking since the snakes are able to \"lock on\" to salient visual features and can, therefore, track them accordingly across frames.\n\n## **Discussion and Shortcomings**\nThe snakes were the first variational approach to the image segmentation task, and therefore this was a seminal paper in image processing. By introducing the gradient (edge strength) as an energy term, they improved on the shortcomings of the prior published works.\n\nBecause snakes are \"active\" and are therefore always minimizing their energy, they show hysteresis in response to moving input.\n\nThe energy minimization for the snakes is achieved through gradient descent, and since the energy functional is not convex, it can get stuck on local minima. Therefore, a prerequisite for the snakes is that the curve initialization must be done sufficiently close to the desired solution.\n\nMoreover, because of numerical instabilities, as the control points move in time, the curve can have self-intersections which is undesired in image segmentation tasks.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q55934454"
    },
    "1120": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/PenningtonSM14",
        "transcript": "Stanford\u2019s paper on Global Vectors for Word Representation proposes one of few extremely popular word embedding methods in NLP. GloVe takes advantage of both global corpus statistics and local context window methods by constructing the word-context co-occurrence matrix and reducing its dimensionality by preserving as much of the variance as possible. It builds a feature space with additive compositionality while preserving statistically meaningful word occurrence information extracted from the data corpus.\n\nAuthors start with building the counts matrix X, where X_ij is the number of times word w_j appears in the context w_i. The corresponding probabilities can be calculated as P_ij = X_ij/X_i. The GloVe model tries to fit a function F(w_i, w_j, w.hat_k) = P_ik/Pjk, which represents ratio of probabilities of the word w.hat_k appearing in the context of words w_i and w_j respectively. The model is expected to produce large output for w.hat_k relevant to w_i and not w_j and vice versa. When w.hat_k is equivalently relevant or irrelevant to w_i and w_j, the output is close to unity. For practical reasons, the authors simplified the model with a linear equation w_i.T*w.hat_k + b_i + b_j = log(1+X_ik), where the biases b_i, b_j represent log(1+X_i) and log(1+X_j) respectively. As an optimization objective, GloVe uses weighted squared error J = Sum_ij(f(X_ij)*(w_i.T*w.hat_k + b_i + b_j - log(1+X_ik))^2), where the weighting function mitigates the effect of noisy rare word occurrences (X_ij ~ 0). The choice of the weighting function f(x) is somewhat arbitrary and driven by empirical observations. The best performing f(x) = min{1, (x/x_max)^0.75}, with x_max = 100. Authors refer to 0.75 power of the skip-gram model, which appears to be used in negative sampling distribution in skip-gram model, which also functions as a weighting coefficient. The resulting log-bilinear regression model is then optimized with AdaGrad and the combination of the resulting matrices W + W.hat is used as the word embeddings.\n\nAuthors evaluate GloVe performance on a variety of NLP tasks and compare against Skip-Gram, CBOW, SVD and HPCA. Although GloVe demonstrated advantageous metrics in both performance and training time (co-occurrence matrix is built in parallel), the model is not very different from Word2Vec. Another questionable argument of the GloVe authors is that the log squared error function is superior to the cross entropy loss in Skip-Gram and ivLBL models, because of higher robustness in the long tails distributions. However, it is not apparently correct, as Skip-Gram uses stochastic optimization, which inherently mitigates the long tails vulnerability. In fact, GloVe shows similar performance to Word2Vec in numerous NLP problems, yet the later historically gained a larger popularity.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D14/D14-1162.pdf"
    },
    "1121": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/coling/BergerPP96",
        "transcript": "A fundamental paper by Adam Berger and his colleagues at IBM Watson Research Center presents a thorough guide on how to apply maximum entropy (ME) modelling in NLP. Authors follow the principle of maximum entropy and show the optimality of their procedure as well as its duality relationship with the maximum log-likelihood estimation. Importantly, the paper proposes a method for automatic feature selection, perhaps the most critical step in the entire approach. Empirical results from Candide, an IBM\u2019s automatic machine-translation system, demonstrate the capabilities of ME models. Both originality and wide range of possible applications made the paper to stand out and led to the deserved accolades.\n\nMany practical NLP problems impose constraints in terms of data or explicit statements, while requiring minimal assumption on the underlying distributions. Laplace advocated that in this scenario the best strategy is to treat everything as equal as possible, while being consistent with the observed facts. Authors follow Laplace\u2019s philosophy and propose to use entropy as a measure of uniformly distributed uncertainty. They employ the principle of maximum entropy to choose the best performing model, which simply states that the optimal model is the one that has the largest entropy. \n\nFor a given set of features f they model its expected value using the empirical data distribution p(x,y) as p(f) = sum[p(x,y)*f(x,y)] = sum[p(y|x)*p(x)*f(x,y)]. By replacing the conditional probability p(y|x) with its entropy, they obtain the objective function H(p) = -sum[p(x)*p(y|x)*log(p(y|x))]. The optimal distribution p* is then defined as p* = argmax(H(p)). Note that this optimization usually requires a numerical solution. Therefore, authors employ Lagrange multipliers L(p, lambda) = H(p) + sum[lambda_i * (p(f_i) \u2013 p_tilda(f_i))], which yields p_lambda(y|x) = 1/Z * exp(sum[lambda_i * f_i(x,y)]) and L(lambda) = -sum[p(x)*log(Z)] + sum[lambda_i*p(f_i)], where Z is a normalizing constant. The unconstraint dual problem then becomes lambda = argmax(L(lambda)). Interestingly, that L is exactly equal to the log-likelihood of the data sample, which implies that MLE and ME are dual and ensures the optimality of ME, since it converges to the best fit of the data. To estimate the model parameters, authors proposed an iterative scaling method, which update lambda by solving sum[p(x)*p(y|x)*f_i(x,y)*exp(delta_lambda * f(x,y))] = p(f_i). The choice of features f is apparently the most critical step in the procedure. Authors propose a simple approach of feature selection by iteratively adding a feature f_new that maximizes the increase in entropy. They continue expanding the set of features until the cross-validation results converge.\n\nTo evaluate ME approach, the authors applied it to context-sensitive word-translation, sentence segmentation, and word reordering problems. These applications demonstrate the efficacy of ME techniques for performing context-sensitive modelling and the paper undoubtedly deserves the attention it gained. For example, in word reordering the model outperform a simple baseline model by 10%. However, a deeper analysis of the feature selection could benefit it. The proposed feature selection method is computationally expensive, as it iteratively calls optimization subroutine and involves a cross-validation step. Moreover, authors skip the discussion on overfitting and the choice of the initial pool of the features. They imply that the features have to be designed manually, which makes the approach impractical for large diverse problems. Therefore, further developments in dimensionality reduction and feature extraction could benefit ME.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1122": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/naacl/McDonaldPRH05",
        "transcript": "The paper written by an international collaboration between UPenn and CUNI presents an original parsing approach that expands the conventional projective tree-based method to include the non-projective dependencies in text. Authors represent words and their relationships as vertices and edges of a complete directed graph respectively, and then employ Chu-Liu-Edmonds algorithm to find the maximum spanning trees allowing non-projective parses. Importantly, they demonstrated better accuracy for Czech language and higher efficiency (O(n^2)) compared to Eisner\u2019s projective parsing (O(n^3)). Generality and unexpected asymptotical computational simplicity of the proposed approach attracted numerous researchers and led to the best student paper award on HLT/EMNLP conference in 2005.\n\nConventionally, finding a maximum projective spanning tree (MST) corresponds to finding a maximum dependency tree, which could be solved by Chu-Liu-Edmonds or Eisner algorithms. To find the solution, authors define a score s(x,y) = sum[w * f(i,j)], where (i,j) is an edge in a dependency tree y corresponding to a sentence {x}, f is a feature vector, and w is the weight vector that needs to be optimized. To extend the procedure to non-projective trees, authors apply a greedy Chu-Liu-Edmonds algorithm, where for each vertex it leaves only the incoming edge with the highest score. If the resulting graph has cycles, they are replaced with vertices, and the algorithms iterates until it converges to a tree, which must be the MST. Tarjan\u2019s efficient implementation of the algorithms guarantees O(n^2), which dispels the notion that non-projective parsing is \u201charder\u201d than projective parsing that takes O(n^3). To optimize weight vector w, authors employ factored version of Margin Infused Relaxed Algorithm (MIRA), which iteratively updates w for each sample (x_t,y_t) subject to min||w_t+1 \u2013 w_t|| s.t. s(l, j) \u2013 s(k, k) >= 1 for (l,j) in y_t and (k,j) not in y_t.\n\nTo evaluate the proposed approach, the paper shows dependency parsing results for Czech, using the Prague Dependency Treebank that has both projective parsing and non-projective parsing tasks. The method achieves the state of the art performance for the dataset and promises an outstanding generality for numerous other languages and low computation cost. However, the paper vaguely defines the high-dimensional feature vectors f(i,j) that must play a crucial role in the optimization of the weight vector w. Moreover, lower accuracy and completeness of the proposed method for English requires a closer look at the optimization procedure. Finally, as the authors mentioned in the paper, the non-projective parsing does not generalize well to bigger substructures as the search becomes intractable.\n",
        "sourceType": "blog",
        "linkToPaper": "http://acl.ldc.upenn.edu/H/H05/H05-1066.pdf"
    },
    "1123": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=mikolov2013distributed",
        "transcript": "The paper presents the famously known Word2Vec model, which became ubiquitous in numerous NLP applications partially owing to its linear feature space with additive compositionality. To be fair, the paper is an extension to the previously presented work by Tomas Mikolov and his colleagues on distributed representation of words and phrases. The proposed approach is based on the skip-gram model and introduces four novel methods to significantly improve training speed and performance. Particularly, the approach is effective with frequent and rare words and phrases, including idiomatic phrases.\n\nSkip-gram model uses softmax function to define P(w_O | w_I), which becomes impractical when training on a large vocabulary due to expensive calculation of the denominator. To tackle it, the authors employ binary Huffman tree of words and calculate P(w_O | w_I) as a product of the sigmoids along the path from the root to any word w. The argument to the sigmoids is a product of the indicator function and the dot product between the vector representations of the words. The indicator flags whether a word is in the path from the root node to the word w. Tree structure simplifies the calculation from O(W) to average O(logW). Additionally, since Huffman tree assigns short codes to the frequent words, the training gets an extra speed up.\n\nAs an alternative to tree-based hierarchical softmax, the authors introduce an extension to Noise Contrastive Estimation (NCE), which they call Negative Sampling. The idea is to use logistic binary regression to distinguish between negative and positive samples, rather than picking one class from the entire vocabulary. They sample k negative examples from a \u201cnoise distribution\u201d Pn for each positive example. As Pn authors employ unigram distribution U(w)^(3/4)/Z, which works best for both Negative Sampling and Hierarchical Softmax models, although they do not elaborate on the choice of the noise distribution.\n\nTo balance frequent and rare words, authors aggressively subsample words whose frequency is greater than a chosen threshold while preserving the ranking of the frequencies. They demonstrate that a heuristically chosen subsampling strategy accelerates learning and even significantly improves the accuracy of the learned vectors of the rare words. They discard a word w with probability p(w) = 1 \u2013 sqrt(t/f(w)), where t is a manually selected threshold (e.g. 10^-5) and f(w) is the word frequency.\nFinally, authors learn vector representation of the phrases by combining words into individual tokens. They score bigrams as score(w_i,w_j) = (cnt(w_i, w_j)-d)/(cnt(w_i) \u2013 cnt(w_j)) and then choose the ones that have scores above the chosen threshold. To form longer phrases, authors evaluate the scores recursively. The chosen score metric combines words that appear frequently together, and infrequently in other contexts.\n\nAlthough the paper lacks detailed discussion on critical choice of the subsampling and negative sampling, it creates a foundation for explainable linear feature space in NLP model. The model shows the state-of-the-art performance and significantly outperforms other approaches, though it is partially due to a much bigger size of the training dataset. One of the most outstanding results of the paper is the inherent additive compositionality. This directly comes from the use of the softmax that tries to maximize the product of the probabilities. It forces the word vectors to have a linear relationship with the inputs to the softmax and decreases the distance of between the words that appear lose to each other. As a results, the word vectors can be combined as follows: \u201cParis\u201d \u2013 \u201cFrance\u201d + \u201cGermany\u201d = \u201cBerlin\u201d.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1124": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jmlr/BengioDVJ03",
        "transcript": "Yoshua Bengio\u2019s visionary work on probabilistic language modelling had a huge impact on the field of Natural Language Processing. Despite that it was published nearly 20 years ago, it is still relevant to modern NLP solutions. In fact, the subsequent works perfected the state-of-the-art in NLP, although it took more than 10 years for the paper to get a significant attention in the field.\n\nThe authors approach the fundamental problem of exponentially growing number of trainable parameters with the size of corpora. The problem is called \u201cCurse of Dimensionality\u201d and limits the generalization performance of conventional approaches such as N-gram models. For instance, if we want to model a joint distribution of 10 consecutive words with a vocabulary of size 100,000, there are potentially 10^50-1 trainable variables.\n\nThe proposed probabilistic modeling approach scales linearly with the size of the vocabulary. It comprises of two steps: 1) converting words into real-valued feature vector of size m and 2) extract joint probability of the word sequence represented by the feature vectors (total size of (n-1)-by-m). While the first step is implemented as a trainable matrix of size |V|*m, the joint probability function is implemented as two densely connected layers (tanh and softmax activation) with a skip connection between the input of the first layer and the input of the second layers. The first layer has weight matrix of size m*h and the second is (nm+h)*|V|, which results in total of |V|(1+nm+h)+h(1+(n-1)m) trainable variables. The output of the model is then mixed with the output of a tri-gram model.\n\nAs the optimization objective, the authors employ log-likelihood of the next word regularized on the weights of dense layers (biases are excluded, as well as, matrix of real-valued word features embedding matrix of size |V|-by-m). The goal of the optimization is to find the parameters that minimize the perplexity of the training dataset. Eventually we learn the distributed representations of the words and the probability function of a sequence as a function of the distributed word representations.\nThe proposed model improved the out-of-sample perplexity score by 24% for Brown and 8% on AP news datasets compared to the state-of-the-art smoothed trigram models. The best performing architecture comprised h=100, m=30, without a skip connection, but with trigram mixing.\n\nAs a primary drawback of the approach, authors mention significant speed limitations and propose use of shallow networks, time delay and recurrent neural networks to mitigate the problem and improve the performance. They also consider several promising direction for future research such as decomposing the network in sub-networks, representing the conditional probability as a tree, introducing a-priori knowledge, interpretable word embedding, and adjusting the gradient propagation path.\n\nAs a critique, the training did not explicitly target word semantics; although the authors claim that their embedding takes advantage of the inherently learned similarity (close words are expected to have similar feature vectors). For example, a better approach could be a Siamese network with an objective to minimize a distance (e.g. cosine or Euclidian) between \u201csimilar\u201d words, this would also decrease the number of the trainable variables and well as exclude the risky non-regularized word embedding matrix. Secondly, as the authors mentioned, the model could significantly benefit from use of GRU, RNN, LSTM and other network architectures, as well as including prior knowledge. Finally, the model scales linearly with n and |V|, which significantly limits its application.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/papers/v3/bengio03a.html"
    },
    "1125": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.02827",
        "transcript": "The method they use basically tells the robot to reason as follows:\n1. The human gave me a reward function $\\tilde{r}$, selected in order to get me to behave the way they wanted.\n2. So I should favor reward functions which produce that kind of behavior. \n\nThis amounts to doing RL (step 1) followed by IRL on the learned policy (step 2); see the final paragraph of section 4.\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.02827v1"
    },
    "1126": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1902.08605",
        "transcript": "Disclaimer: I am the first author.\n\n# Executive summary\n- The authors propose a new method, [*Centroid Networks*](https://arxiv.org/pdf/1902.08605.pdf), for learning to cluster. \n- Given example clusterings of data, the goal is to learn how to cluster new data following the same criterion.\n- Centroid Networks basically consist of running K-means on Prototypical Network features, plus many tricks.\n- They evaluate Centroid Networks on Omniglot and miniImageNet (supervised few-shot classification benchmarks). \n- Centroid Networks can compete with Prototypical Networks (state of the art in supervised few-shot classification) despite using no supervision at evaluation time (the labels of the support set are completely ignored).\n\n## Pros\n* **Simple training** (non end-to-end, very similar to prototypical networks, with additional tricks).\n* **Very fast clustering** (nearly same running time as prototypical networks).\n* The authors claim that the Sinkhorn K-means formulation is empirically very stable : any initialization is fine as long as symmetries are broken (in practice, they initialize all centroids at 0 and add a small gaussian noise to centroids at each step).\n\n## Cons\n* Clusters **need to be balanced** now. Removing the balanced constraint is future work.\n\n# Setting\n- They frame learning to cluster as a meta-learning problem, **few-shot clustering**. \n- The goal is to cluster K*M images into K clusters of M images. \n- Classes vary across tasks, but class semantics are the same (Omniglot: cluster by character, miniImageNet: cluster by object category).\n- They also define a second task **unsupervised few-shot classification** solely for comparing with supervised few-shot classification methods.\n\n# Method\n\nConceptually, Centroid Networks consist of training *Prototypical Networks* (meta-training), then running *K-means* on top of protonet representations at clustering time (meta-evaluation).  However, the authors propose several tricks that significantly improve upon that baseline:\n- **Center loss**: when pretraining, this extra regularization term penalizes the intra-class variance.\n- **Sinkhorn assignments**: when pretraining, replace the softmax predictions p(y|x) with a formulation based on optimal transport (Sinkhorn distances).\n- **Sinkhorn K-means**: at clustering time, run the Sinkhorn K-means algorithm on the learned representation\n\n# Results on Few-Shot Classification Benchmarks:\n\n- The task is **unsupervised few-shot classification**: cluster a *unlabeled* support set, then predict which clusters new images should be classified into. \n- Target metric is **unsupervised accuracy**.\n- *Unsupervised* few-shot classification is harder than *supervised* few-shot classification because *no labels* are given in the support set.\n- Compare with reference oracle Prototypical networks, which can access labeled support set.\n- Centroid Networks are almost as good as Protonets on Omniglot (99.1% vs. reference 99.7%)\n- Centroid Networks are comparable to Protonets on miniImageNet (53.1% vs. reference 66.9%).\n- The proposed \"tricks\" are useful because Centroid Networks beats K-Means (Protonet feature) baseline.\n\nhttps://i.imgur.com/acQpQeq.png\nhttps://i.imgur.com/FlHf9Ko.png\n\n# Results on Learning to Cluster Benchmarks:\n\n- The task is **few-shot clustering**. After training on 30 alphabets of Omniglot, the task is to cluster 20 new alphabets (20-47 characters, with 20 instances/character). \n- Target metric is **clustering accuracy**.\n- Centroid Networks beat all flavors of Constrained Clustering Networks (86.6% vs. 83.3%)\n- Centroid Networks are about 100 times than CCN faster but less flexible (fixed cluster sizes).\n\nhttps://i.imgur.com/PvH5V1W.png\n\n# Code\n\nThe code is available at https://github.com/gabrielhuang/centroid-networks\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1902.08605"
    },
    "1127": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1901-08746",
        "transcript": "The paper discusses the idea of using BERT pre-trained network in bio-medical domain NLP tasks. It lays a path for future applications of Bert in different business verticals. Sharing some points from the review article I wrote on BioBERT on medium (https://medium.com/@raghudeep/biobert-insights-b4c66fde8fa7).\n\nThe major contribution is a pre-trained bio-medical language representation model for various bio-medical text mining tasks. Tasks such as NER from Bio-medical data, relation extraction, question & answer in the biomedical field.\n\nFor pretraining the model, authors have used a combination of general & medical corpora, which has shown interesting results. They have compared the results of all their combinations.\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1901.08746"
    },
    "1128": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1807.03146",
        "transcript": "What the paper is about:\nKeypointNet learns the optimal set of 3D keypoints and their 2D detectors for a specified downstream task. The authors demonstrate this by extracting 3D keypoints and their 2D detectors for the task of relative pose estimation across views. They show that, using keypoints extracted by KeypointNet, relative pose estimates are superior to ones that are obtained from a supervised set of keypoints.\n\n\nApproach:\nTraining samples for KeypointNet comprise two views (images) of an object. The task is to then produce an ordered list of 3D keypoints that, upon orthogonal procrustes alignment, produce the true relative 3D pose across those views. The network has N heads, each of which extracts one (3D) keypoint (from a 2D image). There are two primary loss terms. A multi-view consistency loss measures the discrepancy between the two sets of extracted keypoints under the ground-truth transform. A relative-pose estimation loss penalizes the angular discrepency (under orthogonal procrustes) of the estimated transform using the extracted keypoints vs the GT transform. Additionally, they require keypoints to be distant from each other, and to lie within the object silhouette.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1807.03146"
    },
    "1129": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1601.02129",
        "transcript": "## Segmented SNN \n\n**Summary**: this paper use 3-stage 3D CNN to identify candidate proposals, recognize actions and localize temporal boundaries.\n\n**Models**: \nthis network can be mainly divided into 3 parts: generate proposals, select proposal and refine temporal boundaries, and using NMS to remove redundant proposals.\n1. generate multiscale(16,32,64,128,256.512) segment using sliding window with 75% overlap. high computing complexity!\n2. network: Each stage of the three-stage network is using 3D convNets concatenating with 3 FC layers.\n  * the proposal network is basically a classifier which will judge if each proposal contains action or not.\n * the classification network is used to classify each proposal which the proposal network think is valid into background and K action categories\n * the localization network functioned as a scoring system which raises scores of proposals that have high overlap with corresponding ground truth while decreasing the others.\n.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1601.02129"
    },
    "1130": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1703.06189",
        "transcript": "## Temporal unit regression network\n\nkeyword: temporal action proposal; computing efficiency \n\n**Summary**: In this paper, Jiyang et al designed a proposal generation and refinement network with high computation efficiency by reusing unit feature on coordinated regression and classification network.  Especially,  a new metric against temporal proposal called AR-F is raised to meet 2 metric criteria: 1. evaluate different method on the same dataset efficiently. 2. capable to evaluate same method's performance across several datasets(generalization capability)\n\n\n**Model**:\n* decompose video and extract feature to form clip pyramid: \n1. A video is first decomposed into short units where each unit has 16/32 frames.\n2. extract each unit's feature using C3D/Two-stream CNN model.\n3. several units' features are average pooled to compose clip level feature. In order to provide context and adaptive for different length action, clip level feature also concatenate surround feature and scaled to different length by concatenating more or fewer clips.\nFeature for a slip is $f_c = P(\\{u_j\\}_{s_u-n_{ctx}}^{s_u})||P(\\{u_j\\}_{s_u}^{e_u})||P(\\{u_j\\}_{e_u}^{e_u+n_{ctx}})  $\n4. for each proposal pyramid, a classifier is used to judge if the proposal contains an action and a regressor is used to provide an offset for each proposal to refine proposal's temporal boundary. \n5. finally, during prediction, NMS is used to remove redundant proposal thus provide high accuracy without changing the recall rate.\n\nhttps://i.imgur.com/zqvHOxj.png\n\n**Training**: There are two output need to be optimized, the classification result and the regression offset. Intuitively, the distance between the proposal and corresponding ground truth should be measured. In this paper, the authors used the L-1 metric for regressor targeted the positive proposals. total loss is measured as follow:\n$L = \\lambda L_{reg}+L_{cls}$\n\n\n$L_{reg} = \\frac{1}{N_{pos}}\\sum_{i = 1}^N*l_s^*|o_{s,i} - o_{s,i}^*+o_{e,i} - o_{e,i}^*|$\n\n\nDuring training, the ratio between positive samples and negative samples is set to 1:10. And for each positive proposal, its ground truth is the one with which it has the highest IOU or which it has IOU more than 0.5.\n\n \n**result**: \n1. Computation complexity: 880 fps using the C3D feature on TITAN X GPU, while 260 FPS using flow CNN feature on the same machine.\n2. Accuracy: mAP@0.5  = 25.6% on THUMOS14\n\n\n**Conclusion**: Within this paper, it generates proposals by generate candidate at each unit with different scale and then using regression to refine the boundary. \n\n\n*However, there are a lot of redundant proposals for each unit which is an unnecessary waste of computing source; Also, proposals are generated with the pre-defined length which restricted its adaptivity to different length action; Finally the proposals are generated on the unit level which will suffer granularity problem*",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1703.06189"
    },
    "1131": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1806.02964",
        "transcript": "## Boundary sensitive network\n### **keyword**: action detection in video; accurate proposal \n\n**Summary**: In order to generate precise temporal boundaries and improve recall with lesses proposals, Tianwei Lin et al use BSN which first combine temporal boundaries with high probability to form proposals and then select proposals by evaluating whether a proposal contains an action(confidence score+ boundary probability).   \n\n**Model**:\n1. video feature encoding: use the two-stream extractor to form the input of BSN. $F = \\{f_{tn}\\}_{n=1}^{l_s} = \\{(f_{S,Tn}, f_{T,t_n}\\}_{n=1}^{l_s)} $\n2. BSN:\n* temporal evaluation: input feature sequence, using 3-layer CNN+3 fiter with sigmoid, to generate start, end, and actioness probability\n* proposal generation: 1.combine bound with high start/end probability or if probility peak to form proposal; 2. use actioness probability to generate proposal feature for each proposal by sampling the actioness probability during proposal region. \n* proposal evaluation: using 1 hidden layer perceptron to evaluate confidence score based on proposal features. \nproposal $\\varphi =(t_s,t_e,p_{conf},p_{t_s}^s,p_{t_e}^e) $ $p_{t_e}^e$ is the end probability,and $p_{conf}$ is confidence score\n\nhttps://i.imgur.com/VjJLQDc.png\n\n**Training**: \n* **Learn to generate probility curve**:\nIn order to calculate the accuracy of proposals the loss in the temporal evaluation is calculated as following: \n $L_{TEM} = \\lambda L^{action} + L ^{start} + L^{end}$; \n$L = \\frac{1}{l_w} \\sum_{i =1}^{l_w}(\\frac{l_w}{l_w-\\sum_i g_i} b_i*log(p_i)+\\frac{l_w}{\\sum_i g_i} (1-b_i)*log(1-p_i))$\n$ b_i = sign(g_i-\\theta_{IoP})$ \n\n\nThus, if start region proposal is highly overlapped with ground truth, the start point probability should increase to lower the loss, after training, the information of ground truth region could be leveraged to predict the accurate probability for start. actions and end probability could apply the same rule.\n\n* **Learn to choose right proposal**:\nIn order to choose the right proposal based on confidence score, push confidence score to match with IOU of the groud truth and proposal is important. So the loss to do this is described as follow:\n$L_p = \\frac{1}{N_{train}} \\sum_{i=1}^{N_{train}}  (p_{conf,i}-g_{iou,i})^2$. $N_{train}$ is number of training proposals and among it the ratio of positive to negative proposal is 1:2.$g_{iou,i}$ is the ith proposal's overlap with its corresponding ground truth.\n    During test and prediction, the final confidence is calculated to fetch and suppress proposals using gaussian decaying soft-NMS. and final confidence score for each proposal is  $p_f = p_{conf}p_{ts}^sp_{te}^e$\n\nThus, after training, the confidence score should reveal the iou between the proposal and its corresponding ground truth based on the proposal feature which is generated through actionness probability, whereas final proposal is achieved by ranking final confidence score.\n\n**Conclusion**: Different with segment proposal or use RNN to decide where to look next, this paper generate proposals with boundary probability and select them using the confidence score-- the IOU between the proposal and corresponding ground truth. with sufficient data, it can provide right bound probability and confidence score. and the highlight of the paper is it can be very accurate within feature sequence. \n\n*However, it only samples part of the video for feature sequence. so it is possible it will jump over the boundary point. if an accurate policy to decide where to sample is used, accuracy should be further boosted. *\n\n* **computation complexity**: within this network, computation includes\n  1. two-stream feature extractor for video samples\n  2. probility generation module: 3-layers cnn for the generated sequence \n  3. proposal generation using combination\n  4. sampler to generate proposal feature\n  5. 1-hidden layer perceptron to generate confidence score.\nmajor computing complexity should attribute to feature extractor(1') and proposal relate module if lots of proposals are generated(3',4')\n\n**Performance**: when combined with SCNN-classifier, it reach map@0.5 = 36.9 on THUMOS14 dataset\n \n\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1806.02964"
    },
    "1132": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1704.06228",
        "transcript": "## Structured segmented network\n\n### **key word**: action detection in video; computing complexity reduction; structurize proposal \n\n**Abstract**: using a temporal action grouping scheme (TAG) to generate accurate proposals, using a structured pyramid to model the temporal structure of each action instance to tackle the issue that detected actions are not complete, using two classifiers to determine class and completeness and using a regressor for each category to further modify the temporal bound. In this paper, Yue Zhao et al mainly tackle the problem of high computing complexity by sampling video frame and remove redundant proposals in video detection and the lack of action stage modeling.  \n\n**Model**: \n1. generate proposals: find continuous temporal regions with mostly high actioness. $P = \\{ p_i = [s_i,e_i]\\}_{i = 1}^N$\n2. splitting proposals into 3 stages: start, course, and end: first augment the proposal by 2 times symmetrical to center, and course part is the original proposal, while start and end is the left part and right part of the difference between the transformed proposal and original one.\n3. build temporal pyramid representation for each stage: first  L samples are sampled from the augmented proposal, then two-stream feature extractor is used on each one of them and pooling features for each stage \n4. build global representation for each proposal by concatenating stage-level representations\n5. a global representation for each proposal is used as input for classifiers \n\n* input  =  ${S_t}_{t = 1} ^{T}$a sequence of T snippet representing the video. each snippet  = the frames + an optical  flow stack\n* network: two linear classifiers; L two-steam feature extractor and several pooling layer  \n* output: category and completeness and modification for each proposals.\nhttps://i.imgur.com/thM9oWz.png\n\n**Training**: \n* joint loss for classifiers: $L_{cls} = -log(P(c_i|p_i)* P(b_i,c_i,p_i)) $\n* loss for location regression:  $\\lambda * 1(c_i>=1, b_i = 1)  L(u_i,\\varphi _i;p_i)$\n\n**Summary**:\nThis paper has three highlights:\n1. Parallel: it uses a paralleled network structure where proposals can be processed in paralleled which will shorten the processing time based on GPU\n2. temporal structure modeling and regression: give each proposal certain structure so that completeness of proposals can be achieved\n3. reduce computing complexity: use two tricks: remove video redundancy by sampling frame; remove proposal redundance\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1704.06228"
    },
    "1133": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.06984",
        "transcript": "### **Keyword**: RNN, serialized model; non-differentiable backpropogarion; action detection in video \n\n\n**Abstract**: This paper uses an end-to-end model which is a recurrent neural network trained by REINFORCE to directly predict the temporal bounds of actions. The intuition is that people will observe moments in video and decide where to look to predict when an action is occurring. After training, Serena et al manage to achieve the state-of-art result by only observing 2% of the video frames.\n\n**Model**: In order to take a long video and output all the instances of given action, they use two parts including an observation network and recurrent network. \n* observation network:  encode the visual representation of video frames.\n     * input:  $ln$ -- the normalized location of the frame + frame $v_{ln}$\n     * network: fc7 feature of finetuned VGG16 network\n     * output: $on$ of 1024 dimension indicate time and frame feature \n     \n* recurrent network: sequentially process the visual representations and decide where to watch next and whether to emit detection.\n##### for each timestep:\n     * input:  $on$ -- the representation of the frame + previous state $h_{n-1}$\n     * network: $d_n = f_d(h_n; \\theta_d)$. $pn = fp(h_n,\\theta_p)$,$fd$ is fc. $fp$ is fc+sigmoid  \n     * output: $d_n = (s_n,e_n,c_n )$as the candidate detection, where $s_n$,$e_n$ is the start and end of the detection, $c_n$ is confidence level;  $p_n$ whether $d_n$ is a valid detection. $l_{n+1}$ where to observe next. all the parameter falls in [0,1]\nhttps://i.imgur.com/SeFianV.png\n\n\n**Training**: in order to learn the supervision annotation in long videos and handle the non-differentiable components, authors use BP to train $d_n$ while use REINFORCE to train $p_n$ and $l_{n+1}$\n* for $d_n$: $L(D) = \\sum_n L_{cls}(d_n) + \\sum_n \\sum_m 1[y_{mn} = 1] L_{loc}(d_n,g_m)$\n* for $p_n,l_{n+1}$: reward $J(\\theta) = \\sum_{a\\in A} p(a) r(a)$ p(a) is the distribution of action and r(a) is the reward for the action. so training needs to maximize this.\n\n\n**Summary**:\nThis paper uses a serialized model which first extract the feature from each frame, then use the frame feature and previous state info to generate the next observation time, detection and detection indicator. Specifically, in order to use previous information, they use RNN to store information and use REINFORCE to train $p_n$ and $l_n$, where the goal is to maximize reward for an action sequence and use Monte-Carlo sampling to numerically calculate the gradient for high dimension function.\n\n**questions**:\n1. why  $p_n$ and $l_n$ are non-differentiable components?\n2. if $p_n$ and $l_n$ are non-differentiable components indeed, how do we come up with REINFORCE to compute the gradient?\n3. why don't we get $p_n$ from $p_n = f_p(h_n, \\theta_p)$ directly but rather use fp as the parameter in bernoulli distribution, similar question can be applied to calculation for $l_{n+1}$ in trainning time.\n\n \n  \n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.06984"
    },
    "1134": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/pami/GhesuGZGMHC19",
        "transcript": "Robust and fast detection of anatomical structures is a prerequisite for both diagnostic and interventional medical image analysis. Current solutions for anatomy detection are typically based on machine learning techniques that exploit large annotated image databases in order to learn the appearance of the captured anatomy. These solutions are subject to several limitations, including the use of suboptimal feature engineering techniques and most importantly the use of computationally suboptimal search-schemes for anatomy detection. To address these issues, we propose a method that follows a new paradigm by reformulating the detection problem\nas a behavior learning task for an artificial agent.\n\nTo this end, Ghesu et al. reformulate the problem of landmark detection as a reinforcement learning task, where an agent is trained to navigate the 3D image space in an efficient way in order to find the landmark as spatially closely as possible. This paper marks the first time that RL has been used for a specific problem like this. \n\nThe authors use Deep Q-Learning (DQN) algorithm as their framework to build the agent. The DQN algorithm uses a Convolutional Neural Network to parameterize the Q* function in a non-linear way. Also, in order to ensure that the agent always finds an object of interest regardless of the size, the authors proposed to use multi-scale search strategy, in which a different CNN works on an image of a different scale. \n\nThe authors also some other interesting techniques like $\\epsilon-$greedy approach, which uses a randomized strategy to choose the actions in every iteration. They also use experience replay where an experience buffer is maintained that records the trajectory of the agent. This buffer is then used to fine-tune the Q network. \n\nThe authors test their method on an internal data set of CT scans, with a few different metrics. The most important metric was failure percentage rate, that was defined if the agent is x mm close to the landmark. The method achieved 0\\% failure percentage rate on detecting landmarks in CT scans. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/TPAMI.2017.2782687"
    },
    "1135": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/miccai/WuKWGLS13",
        "transcript": "Accurate anatomical landmark correspondence is highly critical for medical image registration. Traditionally many of the previous works proposed a number of hand-crafted feature sets that can be used to perform correspondence. However these feature tend to be highly specialized in terms of application area, and cannot be always generalized well to other applications without significant modifications. There have been other works that perform automatic feature extraction, but their reliance on labelled data hinders their ability to perform in cases where there is none. \n\nTo this end Wu et al. propose an unsupervised feature learning method which does not require labelled data. Their approach aims to directly learn the basis filters that can effectively represent all observed image patches. The learnt basis filters are later regarded as general image features representing the morphological structure of the patch. \n\nIn order to learn the basis filters, the authors propose a two-layer convolutional neural network called the  Independent Subspace Analysis (ISA) algorithm. As an extension of ICA, the responses are not required to be all to be mutually independent in ISA. Instead, these responses can be divided into several groups, each of which is called independent subspace. Then, the responses are dependent inside\neach group, but dependencies among different groups are not allowed. Thereby, similar\nfeatures can be grouped into the same subspace to achieve invariance. To ensure the accurate correspondence detection, multi-scale image features are necessary to use. However, it also raised a problem of high-dimensionality in learning features from the large-scale image patches.\n\nThis is achieved by constructing a two-layer network for scaling up the ISA to the large-scale image patches. Specifically, the ISA is first trained in the first layer based on the image patches with smaller scale. After that, a sliding window (with the same scale in the first layer) convolves with each large-scale patch to get a sequence of overlapped small-scale patches. The\ncombined responses of these overlapped patches through the first layer ISA are whitened by PCA and then used as the input to the second layer that is further trained by another ISA. In this way, high-level\nunderstanding of large-scale image patch can be perceived from the low-level image features detected by the basis filters in the first layer. \n\nThe authors compare their work with two other methods, and apply their method on IXI and ADNI datasets. \n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-642-40763-5_80"
    },
    "1136": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tmi/PereiraPAS16",
        "transcript": "Tumor segmentation from brain MRI sequences is usually\ndone manually by the radiologist. Being a highly tedious and error prone task, mainly due to factors such as human fatigue, overabundance of MRI slices per patient, and increasing number of patients, manual operations often lead to inaccurate delineation. Moreover, use of qualitative measures of evaluation by radiologists results in high inter- and intraobserver\nerror rates. There is an evident need for automated systems to perform this task. \n\nTo this end Pereira et al. propose to use a deep learning method called Convolutional Neural Network for predicting a segmentation mask of the patient MRI scans. The approach the segmentation problem as a pixel-wise classification problem, where each pixel in the input MRI scan 2D slice is classified into one of the five categories: background, necrosis, edema, non-enhancing and enhancing region. \n\nThe authors propose two networks, one for High Grade Gliomas (HGG) and one for Low Grade Gliomas (LGG). The HGG network has more number of convolution layers than the LGG due to lack of data. The proposed network architectures are a combination of convolution, relu, and max pooling layers, followed by some fully connected layers in the end. \n\nThe networks are trained using categorical cross-entropy loss function. \n\nThe networks were trained on 2D 33x33 patches extracted from the 2D MRI slices of the brain from the BRATS 2012 dataset. The patches were randomly sampled from the images and the task was to predict the class of the pixel in the middle of the patch. To approach the problem of class imbalance, they sample approximately 40\\% of the patches were normal patches. They also use data augmentation to increase the number of effective patches to train. \n\nIn order to test the performance of their proposed method, as well as understand the impact of various hyperparameters, the authors perform a multitude of tests. The tests included turning data augmentation on/off, using leaky relu instead of relu, changing patch extraction plane, using deeper networks, and using larger kernels. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q39934594"
    },
    "1137": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/neuroimage/SukLS14",
        "transcript": "Alzheimer's Disease (AD) is characterized by impairment of cognitive and memory function, mostly leading to dementia in elderly subjects. For the last decade, it has been shown that neuroimaging can be a potential tool for the diagnosis of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI), and also fusion of different modalities can further provide the complementary information to enhance diagnostic accuracy. Multimodal information like that from MRI and PET can be used to aid in diagnosis of AD in early stages. However most of the previous works in this domain either concentrate on only one domain (MRI or PET), or use hand-crafted features which are then concatenated together to form a single vector. There are increasing evidences that biomarkers from different modalities can provide complementary information in AD/MCI diagnosis. \nIn this paper, Suk et al. propose a Deep Boltzmann Machine (DBM) based method that performs high-level latent and shared feature representation obtained from two neuroimaging modalities (MRI and PET). Specifically they use DBM as a building block, to find a latent hierarchical feature representation from a 3D patch, and then devise a systematic method for a joint feature representation from the paired patches of MRI and PET with a multimodal DBM.\n\nThe method first selects class-discriminative patches from a pair of MRI and PET images, by using a statistical significance test between classes. A MultiModal DBM (MM-DBM) is then built that finds a shared feature representation from the paired patches.  However the MM-DBM is not trained directly on patches, instead, it's trained using binary vectors obtained after running the patches through a Restricted Boltzmann Machine (RBM) which transforms the real-valued observations into binary vectors. \n\nThe MM-DBM network's top hidden layer has multiple entries of the lower hidden layers and the label layer, to extract a shared feature representation by fusing neuroimaging information of MRI and PET. Using this multimodal model, a single fused feature representation is obtained. \n\nUsing this feature representation, A Support Vector Machine (SVM) based classification step is added. Instead of considering all patch-level classifiers' output simultaneously, the output from the SVMs are agglomerated the information of the locally distributed patches by constructing spatially distributed \u2018mega-patches\u2019 under the consideration that the disease-related brain areas are distributed over some distant brain regions with arbitrary shape and size. Following this step, the training data is divided into multiple subsets, and used to train an image-level classifier on each subset individually. The method was tested on ADNI dataset with MR and PET images. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q30399994"
    },
    "1138": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MilletariNA16",
        "transcript": "Medical image segmentation have been a classic problem in medical image analysis, with a score of research backing the problem. Many approaches worked by designing hand-crafted features, while others worked using global or local intensity cues. These approaches were sometimes extended to 3D, but most of the algorithms work with 2D images (or 2D slices of a 3D image). It is hypothesized that using the full 3D volume of a scan may improve segmentation performance due to the amount of context that the algorithm can be exposed to, but such approaches have been very expensive computationally. Deep learning approches like ConvNets have been applied to segmentation problems, which are computationally very efficient during inference time due to highly optimized linear algebra routines. Although these approaches form the state-of-art, they still utilize 2D views of a scan, and fail to work well on full 3D volumes. \n\nTo this end, Milletari et al. propose a new CNN architecture consisting of volumetric convolutions with 3D kernels, on full 3D MRI prostate scans, trained on the task of segmenting the prostate from the images. The network architecture primarily consisted of 3D convolutions which use volumetric kernels having size 5x5x5 voxels. As the data proceeds through different stages along the compression\npath, its resolution is reduced. This is performed through convolution with\n2x2x2 voxels wide kernels applied with stride 2, hence there are no pooling layers in the architecture. The architecutre resembles an encoder-decoder type architecture with the decoder part, also called downsampling, reduces the size of the signal presented as input and increases the receptive field of the features being computed in subsequent network layers. Each of the stages of the left part of the network, computes a number of features which is two times higher than the one of the previous layer. The right portion of the network extracts features and expands the spatial\nsupport of the lower resolution feature maps in order to gather and assemble the necessary information to output a two channel volumetric segmentation. The two features maps computed by the very last convolutional layer, having 1x1x1 kernel size and producing outputs of the same size as the input volume, are converted to probabilistic segmentations of the foreground and background\nregions by applying soft-max voxelwise.\n\nIn order to train the network, the authors propose to use Dice loss function. The CNN is trained end-to-end on a dataset of 50 prostate scans in MRI. The network approached a 0.869 $\\pm$ 0.033 dice loss, and beat the other state-of-art models.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.04797"
    },
    "1139": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/nature/EstevaKNKSBT17",
        "transcript": "Skin cancer is one of the most common cancer type in humans. Primarily, the lesion is diagnosed visually through a series of 2D color images taken of the affected area. This may be followed by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. \n\nTo this end, Esteva et al. propose a deep learning based solution to automate the task of diagnosing lesions of the skin into fine-grained categories. Specifically, they use a GoogleNet Inception v3 CNN architecture which won the ImageNet Large Scale Visual Recognition Challenge in 2014. The method also leverages pre-training, in which an already trained DNN can be fine-tuned on a slightly varied task, which allows the network to leverage the convolutional filters it might have learnt from a much larger dataset. To achieve this, the Inception v3 CNN was fine-tuned from a pre-trained state. The model was initially trained on approximately 1.28 million images with about 1000 classes, from the 2014 ImageNet Large Scale Visual Recognition Challenge. Following which, the network is then fine-tuned on the dermatology dataset. \n\nThe dataset used in the study was obtained clinically from open-access online repositories and Stanford Medical Center. It consists of 127,463 training and validation images, and held out set of 1942 labelled test images. The labels are organized hierarchically in a tree like structure, where each succeeding depth level represents a fine-grained classification of the disease.\n\nThe network is trained to perform three tasks: i) classify the first-level nodes of the taxonomy,\nwhich represent benign lesions, malignant lesions and non-neoplastic. ii)  nine-class\ndisease partition\u2014the second-level nodes\u2014so that the diseases of\neach class have similar medical treatment plans, and finally iii)  using only biopsy-proven images on medically important use cases, whether the algorithm and dermatologists could distinguish malignant versus benign lesions of epidermal (keratinocyte carcinoma compared to benign seborrheic keratosis) or melanocytic (malignant melanoma compared to benign nevus) origin. The CNN achieved 72.1 $\\pm$ 0.9\\% (mean $\\pm$ s.d.) overall accuracy (the average of individual inference class accuracies) and two dermatologists attain 65.56\\% and 66.0\\% accuracy on a subset of the validation set for the first task. The CNN achieves 55.4 $\\pm$ 1.7\\% overall accuracy whereas the same two dermatologists attain 53.3\\% and 55.0\\% accuracy in the second task. For the third task, the CNN outperforms the dermatologists, and obtains an area under the curve (AUC) over 91\\% for each case. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q28528865"
    },
    "1140": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/pami/HuangPM06",
        "transcript": "Shape registration problem have been an active research topic in computational geometry, computer vision, medical image analysis and pattern recognition communities. Also called the shape alignment, it has extensive uses in recognition, indexing, retrieval, generation and other downstream analysis of a set of shapes. There have been a variety of works that approach this problem, with the methods varying mostly in terms of (can be called pillars of registration) the shape representation, transformation and registration criteria that is used. One such method is proposed by Huang et al. in this paper, which uses a novel combination of the three pillars, where an implicit shape representation is used to register an object both globally and locally. For the registration criteria, the proposed method uses Mutual Information based criteria for its global registration phase, while sum-squared differences (SSD) for its local phase. \n\nThe method starts off with defining an implicit, non-parameteric shape representation which is translation, rotation and scale invariant. This makes the first step of the registration pipeline which transforms the input images into a domain where the shape is implicitly defined. The image is first partitioned into three spaces, namely $[\\Omega]$ (the image domain), $[R_S]$ (points inside the shape), $[\\Omega - R_S]$ (points outside the shape), and $[S]$ (points lying on the shape boundary). Using this partition, a function based upon the Lipschitz function $\\phi : \\Omega -> \\mathbb{R}^+$ is defined as:\n\n\\begin{equation}\n    \\phi_S(x,y)\n    \\begin{cases} \n         0 & (x,y) \\in S \\\\\n         + D((x,y), S)>0 & (x,y) \\in [R_s] \\\\\n         - D((x,y), S)<0 & (x,y) \\in [\\Omega - R_s]\n    \\end{cases}\n\\end{equation}\n    \nWhere $D((x,y),S)$ is the distance function which gives the minimum Euclidean distance between point $(x,y)$ and the shape $S$. \n\nGiven the implicit representation, global shape alignment is performed using the Mutual Information (MI) objective function defined between the probability density functions of the pixels in source image and the target image sampled from the domain $\\Omega$. \n\n\\begin{equation}\n\nMI(f_{\\Omega}, g_{\\Omega}^{A}) = \\underbrace{\\mathcal{H}[p^{f_{\\Omega}}(l_1)]}_{\\substack{\\text{Entropy of the}\\\\ \\text{distribution representing $f_{\\Omega}$}}} + \\underbrace{\\mathcal{H}[p^{g_{\\Omega}^{A}}(l_2)]}_{\\substack{\\text{Entropy of the}\\\\ \\text{distribution representing $g_{\\Omega}^{A}$} \\\\ \\text{which is the} \\\\ \\text{transformed source ISR using $A(\\theta)$}}} - \\underbrace{\\mathcal{H}[p^{f_{\\Omega}, g_{\\Omega}^{A}}(l_1, l_2)]}_{\\substack{\\text{Entropy of the}\\\\ \\text{joint distribution}\\\\\\text{representing $f_{\\Omega}, g_{\\Omega}^{A}$}}}\n\n\\end{equation}\n\nFollowing global registration, local registration is performed by embedding a control point grid using the Incremental Free Form Deformation (IFFD) method. The objective function to minimize is used as the sum squared differences (SSD). The local registration is also offset by using a multi-resolution framework, which performs deformations on control points of varying resolution, in order to account for small local deformations in the shape. In case where there is prior information available for feature point correspondence between the two shapes, this prior knowledge can be added as a plugin term in the overall local registration optimization term. \n\nThe method was applied on statistically modeling anatomical structures, 3D face scan and mesh registration. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q50745258"
    },
    "1141": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/pami/JianV11",
        "transcript": "Point pattern matching problem have been an active research topic in computation geometry and pattern recognition communities. These point sets typically arise in a variety of applications, where the problem lies in registration of these point sets which is encountered in stereo matching, feature based image registration and so on. Mathematically, the problem of registering two point sets translates to the following: Let $\\{\\mathcal{M}, \\mathcal{S}\\}$ be two finite set points which need to be registered, where $\\mathcal{M}$ is the \"moving\" point set and $\\mathcal{S}$ is the \"fixed\" or scene set. A transformation $\\mathcal{T}$ is then calculated which can transform points from $\\mathcal{M}$ to $\\mathcal{S}$. \n\nTo this end, Jian and Vemuri propose to use a Gaussian Mixture Model based representation of point sets which are registered together my minimizing a cost function using a slightly modified version of the Iterative Closest Point (ICP) algorithm. In this setting, the problem of point set registration becomes as that of aligning two Gaussian mixture models by minimizing discrepancy between two Gaussian mixtures. The cost function for this optimization is chosen as a closed-form version of the $L_2$ distance between Gaussian mixtures, which allows the algorithm to be computationally efficient. \n\nThe main reason behind choosing Gaussian mixture models to represent discrete point sets was that it directly translates to interpreting point sets as randomly sampled data from a distribution of random point locations. This models the uncertainty of point sets well during feature extraction. The second reason was that hard discrete optimization problems that are encountered in point matching literature become tractable continuous optimization problems. \n\nThe probability density function of a general Gaussian mixture is defined as follows:\n\n\\begin{equation}\n    p(x) = \\sum_{i=1}^{k}w_i \\phi(x|\\mu_i, \\sigma_i)\n\\end{equation}\n\nwhere:\n\n$\\phi(x|\\mu_i, \\sigma_i) = \\dfrac{exp\\left[-\\frac{1}{2}(x - \\mu_i)\\sigma_{i}^{-1}(x-\\mu_i)\\right]}{\\sqrt{(2\\pi)^d |det(\\sigma_i)|}}$\nThe GMM from the given point set is constructed as follows: i) the number of GMM components is equal to the number of points in the point set, and every component is weighted equally, ii) every component's mean vectors is represented by the spatial location of each point, iii) all components have same spherical covariance matrix. \n\nAn intuitive reformulation of the point set registration problem is to solve an optimization problem such that a certain dissimilarity measure between the Gaussian mixtures constructed from the transformed model set and the fixed scene set is minimized. For this method, L2 distance is chosen as the dissimilarity measure for measuring similarity between two Gaussian mixtures. The objective can then be minimized by either a closed-form numerical method (in case the function is convex) or an iterative gradient based method. The method was applied and tested on both rigid and non-rigid point set registration problems. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q51622696"
    },
    "1142": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/42.796284",
        "transcript": "Despite being an ill-posed problem, non-rigid image registration has been the subject of numerous works, which apply the framework on different applications where rigid and affine transformations cannot completely model the variations between image sets. One such application of non-rigid registration is to register pre- and post-contrast breast MR images for estimating contrast uptake, which in turn is an indicator of the tumor malignancy. Due to large variations between the pre- and post-contrast images in terms of patient movement, and breast movement which is both global and local, registration of these images become challenging. Classic methods cannot capture the exact semantics of movements that the images exhibit.\n\nTo this end, the authors propose a non-rigid registration method which combines advantages of voxel-based similarity measures like Mutual Information (MI) as well as non-rigid transformation models of the breast. The method is built using a hierarchical transformation model which capture both the global and local movement of the breast across pre- and post-contrast scans. \n\nThe proposed method consists of two interesting contributions which model the motion of the breast across scans using a global and local model. The global motion model conists of a 3D affine transformation parameterized by 12 degrees of freedom. \n\nThe local model is based upon FFD model, which is based upon B-splines which is a powerful tool for modeling 3D deformable objects. The main idea behind this approach is that FFDs can be used to deform an object by manipulating the underlying mesh of the control points, which is estimated in the form of a B-spline. The formulation exposes a trade-off between computational running time and accurate modelling of the object (breast). In order to achieve the best compromise, a hierarchical multi-resolution approach is implemented in which the resolution of the control mesh is increased along with the image resolution  in a coarse to fine fashion. \n\nIn addition to modeling the movement of the breast, a regularization term is also added to the final optimization function which forces the B-spline based FFD transformation to be smooth. The term is zero in case of affine transformation, and only penalizes non-affine transformations. \n\nThe function that the method optimizes is as follows:\n\n$\\mathcal{C}(\\theta, \\phi) = - \\mathcal{C}_{similarity}\\left( I(t_0), T(I(t))\\right) + \\lambda \\mathcal{C}_{smooth}(T)$\n\nThe optimization of the above objective function is performed in multiple stages, and by using gradient descent algorithm which takes steps towards the gradient vector with a certain step size of $\\mu$. Local optimum is assumed if $||\\nabla \\mathcal{C}|| <= \\epsilon$.\n\nIn order to assess the quality of the proposed method, the method is tested on both clinical and volunteer 3D MR data. The results show that the rigid and affine only transformations based methods perform significantly worse than the proposed method. Moreover, it was shown that the results improve with better control point resolution. However the use of SSD as a quantitative metric is debatable since the contrast enhanced and pre-contrast images will have varying distributions of intensity values. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/42.796284"
    },
    "1143": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/miccai/TangC07",
        "transcript": "Image registration has been well studied problem in medical image analysis community, with rigid registration taking much of the spotlight. In addition to rigid registration, non-rigid registration is of great interest due to it's applications in inter-patient modality registration where deformations of organs are highly pronounced. However non-rigid registration is an ill-posed problem with numerous degrees of freedom, which makes finding the best transformation from source to final image very difficult. To counter this, some methods were proposed which constraint the non-rigid transformation $T$ to be within certain bounds, which is not always ideal. \n\nTo this end, Tang et al. propose a novel framework which encapsulates the problem of non-rigid image registration into a graph-cut framework, which guarantees a global maxima (or minima) under certain conditions. The formulation requires that each pixel in source image has a displacement label (whcih is a vector) indicating its corresponding position in the floating image, according to an objective function. A smoothness constraint is also added to ensure that the values of the transformation function $T$ are meaningful and stay within natural limits (no large displacement should occur between absolute neighbouring pixels). The authors propose the following formulation as their objective function, which they then solve using graph-cuts methods:\n\n$D^* = argmin_{D} \\sum_{x\\in X}||I(x) - J(x + D(x))|| + \\lambda \\sum_{(x,y)\\in \\mathcal{N}}||D(x) - D(y)||$\n\nThe equation above is not fully discretized, in the sense that $D$ is still unbounded and can vary from $[-\\infty, \\infty]$. To allow for optimization using graph-cuts, the transformation function $D$ is mapped to a finite set $\\mathcal{W} = \\{0, \\pm s, \\pm 2s...\\pm ws\\}^d$. Using this discretization, the equation above can be solved using graph-cuts via a sequence of alpha-expansion. $\\alpha$-expansion is a two label problem where the cost of assigning a label $\\alpha$ is calculated on the basis of the previous label of the pixel. Different costs are assigned to different scenarios where the previous label may keep it's original label, or change to new label $\\alpha$. This is important since it is imperative that the cost conditions satisifes the inequality given by Kolmogorov \\& Zabih which then guarantees a global optima. \n\nThe method was tested on on MR data from BrainWeb dataset which were affinely pre-registered and intensity normalized to be within 0 and 255. The method demonstrated good qualitative results when compared two state-of-art methods DEMONS and FFD, where the average intensity differences for the proposed method was much lower than the competition, while the tissue overlap was higher. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/978-3-540-75757-3_111"
    },
    "1144": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/pieee/MaesVS03",
        "transcript": "Current medical imaging modalities like Computed Tomography (CT), Magnetic Resonance Imaging (MRI) or Positron Emission Tomography (PET) has allowed minimally-invasive imaging of internal organs. Rapid advancement in these technologies have lead to an influx of data, which, along with rising clinical need, has lead towards a need for quantitative image interpretation in routine practice. Some of the applications include volumetric measurements of regions of the brain, surgery or radiotherapy planning using CT/MRI images.\n\nThis influx of data have opened up avenues for using multi-modality images for making decisions. However this is not always straightforward as the imaging parameters, scanner types, patient movement, or anatomical movement make the images miss-aligned against each other, making direct comparison between, say, as CT and MRI image tricky. This is formally known as the problem of image registration, and to this end, numerous computational methods have been proposed, which this paper surveys. \n\nOut of the methods proposed for both inter- and intra-patient modality registration, Mutual Information based objective maximization strategy has been extremely successful at computing the registration between 3D multi-modal medical images of various organs from the images. Mutual Information (MI) stems from the field of information theory, pioneered by Shannon, which when applied in the context of medical image registration, postulates that the MI between two images (say CT and MRI) is maximum when the images are aligned. \n\nThe basic formulation of a MI based registration algorithm is as follows: Let $\\mathcal{A}$ and $\\mathcal{B}$ be two images which are geometrically related according to a transformation $T_\\alpha$, such that voxels $p$ in $\\mathcal{A}$ with intensity $a$ physically correspond to voxels $T_\\alpha(p)$ in $\\mathcal{B}$ with intensity $b$. The relationship between $p_{AB}(a,b)$ between $a$ and $b$, and hence their MI depends on $T_\\alpha$. The MI criterian postulates that the MI for images that are geometrically aligned is maximum:\n\n\\begin{equation}\n    \\alpha^* = argmax_{\\alpha} I(A,B)\n\\end{equation}\n\nWhere $A$ and $B$ are two discrete random variables, and $I(A,B) = \\sum_{a,b}p_{AB}(a,b) log \\frac{p_{AB}(a,b)}{p_A(a).p_B(b)}$. \n\nAn optimization algorithm is utilized to find a parameter set $\\alpha^*$ that maximizes the MI between $A$ and $B$. Classically, Powell's multidimensional direction set method is used to otipimize the objective function, but other methods do exist as well. \n\nAlthough the formulation of MI criterion suggests that spatial dependence of image intensities are not taken into account, is in fact essential for the criterion to be well-behaved around the registration solution. MI does not rely on pure intensity values to measure correspondence between images, but rather on their joint distribution and the relationship of occurrence. It also does not impose any modality specific constraints which makes it general enough to be applied to any problem formulation (inter- or intra modality). \n\nSome of the areas where MI based image registration may fail is when there is insufficient information in images due to low resolution, low number of images, images not spatially invariant, images with shading artifacts. In some of these cases, MI based criterion will have multiple local optimals. ",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1145": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tmi/DaviesTCWT02",
        "transcript": "Active Shape Models brought with them the ability to intelligentally deform to various intra-shape variations according to a labelled training set of landmark points. However the dependence of such methods on a low-noise training set marked manually poses challenges due to inter-observer differences which becomes even more pronounced in higher-dimensions (3D). To this end, the authors propose a method that addresses this problem, but introducing automatic shape modelling. \n\nThe method is based upon the idea of Occam's Razor, or more formally, The minimum description length (MDL). It is the principle formalization of Occam's razor in which the best hypothesis (a model and its parameters) for a given set of data is the one that leads to the best compression of the data. This essentially means that the MDL characteristic can be used to learn from a set of data points, the best hypothesis that fully describes the training data set, but in a compressed form. The authors use a simple two-part coding formulation of MDL, which although does not guarantee a minimum coding length,but does provide a computationally simple functional form to evaluate which is suitable to be used as an objective function for numerical optimization. \n\nThe proposes objective function is as follows:\n\n$F = \\sum_{p=1}^{n_g}D^{(1)}\\left(\\hat{Y}^p, R, \\delta \\right) + \\sum_{q=n_g + 1}^{n_g + n_{min}}D^{2}\\left(\\hat{Y}^q, R, \\delta \\right)$\n \n The algorithm proceeds by first parameterizing a single shape using a recursive algorithm. Once the recursive parameterization is complete, optimization of the objective function presented above proceeds. The algorithm first generates a parameterization for each shape recursively, to the same level. Then shapes are sampled according to the correspondence defined by the parameterization. Once this is done, a model is built automatically from the above sampled shapes. This model is then used to calculate the objective function.\n The parameterization is changed as to converge to an optimal value for the objective function. \n \n In order to change the parameterization of the model to converge to an optimal value of objective function, a ``reference\" shape is chosen in order to avoid having the points converge to a bad a local minima (all points collapse to single part of the boundary). Due to the non-convex nature of the objective function, optimization is performed using genetic algorithm. \n \n The method was tested both qualitatively and quantitatively on several sets of outlines of 2-D biomedical objects. Multiple anatomical sites in human body were chosen to test the model to provide an idea of how the method performs in a variety of shape settings. Quantitatively the models were shown to be highly compact in terms of the MDL. Qualitatively, the models were able to generate shapes that respected the overall shape of the training set, while still maintaining a good amount of deformation without going haywire. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q48575060"
    },
    "1146": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/miccai/FrangiNVV98",
        "transcript": "Delineation of vessel structures in human vasculature forms the precursor to a number of clinical applications. Typically, the delineation is performed using both 2D (DSA) and 3D techniques (CT, MR, XRay Angiography). However the decisions are still made using a maximum intensity projection (MIP) of the data. This is problematic since MIP is also affected by other tissues of high intensity, and low intensity vasculature may never be fully realized in the MIP compared to other tissues. This calls for a need for a type of vessel enhancement which can be applied prior to MIP to ensure MIP of the imaging have significant representation of low intensity vessels for detection. It can also facilitate volumetric views of vasculature and enable quantitative measurements. \n \n To this end, Frangi et al. propose a vessel enhancement method which defines a \"vesselness measure\" by using eigenvalues of the Hessian matrix as indicators. The eigenvalue analysis of Hessian provides the direction of the smallest curvature (along the tubular vessel structure). The eigenvalue decomposition of a Hessian on a spherical neighbourhood around a point $x_0$ maps an ellipsoid with the axis represented by the eignevectors and their magnitude represented by their corresponding eigenvalues. The method provides a framework with three eigenvalues $|\\lambda_1| <= |\\lambda_2| <= |\\lambda_3|$ with heuristic rules about their absolute magnitude in the scenario where a vessel is present. Particularly, in order to derive a well-formed ``vessel measure\" as a function of these eigenvalues, it is assumed that for a vessel structure, $\\lambda_1$ will be very small (or zero). The authors also add prior information about the vessel in the sense that the vessels appear as bright tubes in a dark background in most images. Hence they indicate that a vessel structure of this sort must have the following configuration of $\\lambda$ values $|\\lambda_1| \\approx 1$, $|\\lambda_1| << |\\lambda_2|$, $|\\lambda_2| \\approx |\\lambda_3|$. Using a combination of these $\\lambda$ values, as well as a Hessian-based function, the authors propose the following vessel measure:\n$\\mathcal{V}_0(s) = \\begin{cases} \n       0 \\quad \\text{if} \\quad \\lambda_2 > 0 \\quad \\text{or} \\quad \\lambda_3 > 0\\\\\n      (1 - exp\\left(-\\dfrac{\\mathcal{R}_A^2}{2\\alpha^2}\\right))exp\\left(-\\dfrac{\\mathcal{R}_B^2}{2\\beta^2}\\right)(1 - exp\\left(-\\dfrac{S^2}{2c^2}\\right))\n   \\end{cases}$\nThe three terms that make up the measure are $\\mathcal{R}_A$, $\\mathcal{R}_B$, and $S$. The first term $\\mathcal{R}_A$ refers to the largest area cross section of the ellipsoid represented by the eigenvalue decomposition. It distinguishes between plate-like and line-like structures. The second term $\\mathcal{R}_B$ accounts for the deviation from a blob-like structure, but cannot distinguish between line- and a plit-like pattern. The third term $S$ is simply the Frebenius norm of the Hessian matrix which accounts for lack of structure in the background, and will be high when there is high contrast compared to background. The vesselness measure is then analyzed at different scales to ensure that vessels of all sizes get detected. \n\nThe method was applied on 2D DSA images which are obtained from X-ray projection before and after contrast agent is injected. The method was also applied to 3D MRA images. The results showed promising background suppression when vessel enhancement filtering was applied before performing MIP. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1007/BFb0056195"
    },
    "1147": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/cviu/CootesTCG95",
        "transcript": "Object detection in 2D scenes have mostly been performed using model-based approaches, which model the appearance of certain objects of interest. Although such approaches tend to work well in cluttered, noisy and occluded settings, the failure of such models  to adapt to intra-object variability that is apparent in many domains like medical imaging, where the organ shapes tend to vary a lot, have lead to a need for a more robust approach. To this end, Cootes et al. propose a training based method which adapts and deforms well to per-object variations according to the training data, but still maintains rigidity across different objects.\n\nThe proposed method relies on a hand-labelled  training set featuring a set of points called \"landmark points\" that describe certain specific positions of any object. For example, for a face the points may be \"noise end, left eyebrow start, left eyebrow mid, left eyebrow end\" and so on. Next, the landmark points across the whole training set are algined using affine transformations by minimizing a weighted-sum  of squares difference (SSD) between corresponding landmark points amongst training examples. The optmization  function (SSD) is weighted using the apparent variance of each landmark point. The higher the variance across training samples, the lower the weight. In order to ``summarize\" the shape in the high-dimensional space of landmark point vectors, the proposed method uses Principal Component Analysis (PCA). PCA provides the eigenvectors which point to the direction of highest change in points in $2n$-dimensional space, while the corresponding eigenvalues provide the significane of each eigenvector. The best $t$ eigenvectors are chosen such that they describe a certain perctange of variance of the data. Once this is done, the model becomes capable of producing any shape by deriving from the mean shape of the object, using the equation:\n\n$x = \\bar{x} + Pb$\n\nwhere $\\bar{x}$ is the mean shape, $P$ = matrix of $t$ eigenvectors and $b$ = vector of free weights that can be tuned to generate new shapes. The values of $b$ are constrained to stay within boundaries determined using the training set, which essentially forms the basis of the argument that the model only deforms as per the training set. \n\nThe method was tested on a variety shapes, namely resistor models in electric circuits, heart model, worm model, and hand model. The models thus generated were robust and could successfully generate new examples by varying the values of $b$ on a straight line. However for worm-model, it was found that varying the values of $b$ only along a line may not be always suitable, especially in cases where the different dimensions of $b$ may have some existing non-linear relationship. \n\nOnce a shape model is generated, it is used to detect objects/shapes from new images. This is done by first initializing the model points on the image. The model points are then adjusted to the shape by using information from the image like edges. The adjustment is performed iteratively, by applying constraints on the calculated values of $dX$ and $dB$ so that they respect the training set. The iterations are performed until convergence of the model points to the actual shape of interest in the image. \n\nOne drawback of the proposed method is its high sensitivity to noise in training data annotations. Also, the relationship between various variables in $b$ is not entirely clear, and may negatively affect models when  there exists a non-linear relationship. Also, the final convergence is somewhat dependent upon the initialization of the model points, and depend on local edge features for guidance, which may fail in some instances. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q57532916"
    },
    "1148": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/pami/Grady06",
        "transcript": "Image segmentation have been a topic of research in computer vision domain for decades. There have been a multitude of methods proposed for segmentation, but most have been dependent on a high level user input which guides the contour or boundaries towards the real boundaries. In order to come close to a fully automated or partially automated solution, \na novel method is proposed for performing multilabel,\ninteractive image segmentation using Random Walk algorithm as the fundamental driver of segmentation. The problem is formulated as follows: given a small number\nof pixels with user-defined (or pre-defined) labels, assign the the probability that a random walker starting at each unlabeled pixel will first reach one of the pre-labeled pixels. The current pixel is then assigned the label corresponding to the max of this probability. This leads to high-quality segmentations of an image into $K$ different components. The algorithm is based on image graphs, where image pixels are represented as graphs connected  by edges to its 8-connected neighbours. \n\nIn this paper, a novel approach to $K$-class image\nsegmentation problem is proposed which utilizes user-defined seeds representing the example regions of the image belonging to $K$ objects. Each seed specifies a location with a user-defined label. The algorithm labels an\nunseeded pixel by resolving the question: Given a random\nwalker starting at this location, what is the probability that it first reaches each of the K seed points? It will be shown\nthat this calculation may be performed exactly without the\nsimulation of a random walk. By performing this calculation,\nthe algorithm assigns a K-tuple vector to each pixel that specifies the probability that a random walker starting from each unseeded pixel will first reach each of the K seed points. A final\nsegmentation may be derived from these K-tuples by selecting\nfor each pixel the most probable seed destination for a random\nwalker.\n\nThe graph weights are determined to be a function of the pixel intensities, specifically $w_{ij}$ = $exp(-(g_i - g_j)^2)$. \nThe algorithm works by biasing the random walker to avoid crossing sharp intensity gradients, which leads to a quality segmentation that respects object boundaries (including weak boundaries).\n\nThe algorithm exposes only one free variable $\\beta$, and can be combined with other approaches involving pre- and post-filtering techniques. Additionally, the algorithm provides on-the-fly correction of previous detected boundary in an computationally efficient way. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q34576487"
    },
    "1149": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/ijcv/BoykovF06",
        "transcript": "Over the last decade and a half, a plethora of image segmentation algorithms have been proposed, which can be categorized into belonging to roughly four categories, represented by a combination of two labels: explicit or implicit boundary representation, and variational or combinatorial methods. While classic methods like Snakes [1] and Level-Sets [2] belong to explicit/variational and implicit/variational category, there have been another set of algorithms falling under the combinatorial domain, which are DP or path-based algorithms which are  explicit/combinatorial, and finally Graph-Cuts, which are implicit/combinatorial. The main difference between the categories is the space of solutions where search is performed. For variational methods,  the search space is $\\mathcal{R}^\\infty$,  while for combinatorial methods the search space is confined to $\\mathcal{Z}^n$. An obvious advantage of combinatorial methods seem to better computational performance, but they also provide a globally optimal solution, which is global to the image. This makes the algorithm performance independent of numerical stability design decisions, and only dependent on the quality of global  descriptors. Hence the algorithms provide a highly generalized, globally optimal framework which can be applied to a variety of problems, including image segmentation. \n\nGraph-Cut methods are based upon the $s-t$ decomposition of a given image graph (where pixels are nodes and edges are formed between 8-connected neighbours).  An $s-t$ decomposition of a graph $\\mathcal{G} = \\{V,E\\}$ is a subset of edges $C \\subset E$ such that the graph gets completely  separated into individual  components $s$ and $t$. The divided nodes are assigned to two terminal nodes, representing foreground and background of the image.  The best-cut in an image graph is optimal if the cost of a cut (defined as  $|C| = \\sum_{e\\in C}w_e$) is minimal. This corresponds to segmenting an image with a desirable balance of boundary and regional properties. \n\nGraph-Cut exposes a general segmentation energy function (which constitutes the ``cost\") as a combination of a regional term and boundary term, given as $E(A) = \\lambda.R(A) + B(A)$.  Regional term can be used to model apriori distribution of the pixel classes (probability of the pixel belonging to background or foreground). The boundary term can be represented by any boundary feature like local intensity gradients, zero-crossing, gradient direction or geometric costs. Although the region and boundary terms force the algorithm to find a boundary which strikes a good balance between the two, sometimes the lack of information for either or both terms may lead to incorrect segmentation.  To offset these terms, hard constraints can be applied to the energy function. The constraints can be anything, for instance, a term that defines that pixels of particular intensities would belong to either foreground or background. The constraint can also be shape based, where shapes like circles or ellipses can be forced upon the final segmentation. \n\nThe proposed algorithm was applied on a variety of 2D and 3D images. Note that graph-cut can  be generalized to N-D segmentation problems as well. A number of qualitative observations are reported. However there is a lack of quantitative foundation of the algorithm performance, compared to other  state-of-art algorithms not necessarily from the same category as graph-cut. \n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q56221806"
    },
    "1150": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/mia/BarrettM97",
        "transcript": "Edge,  contour or boundary detection in 2D images have been an area of active research,  with a variety of different algorithms. However due to a wide variety of image types and content, developing automatic segmentation algorithms have been challenging, while manual segmentation is tedious and time consuming. Previous algorithms approaching this task have tried to incorporate higher level constraints, energy functional (snakes), global  properties (graph based). However the approaches still do not entirely fulfill the fully automated criteria due to a variety of reasons. \n\nTo this end, Barrett et al. propose a graph-based boundary extraction algorithm called Interactive Live-Wire, which is an extension to the original live-wire  algorithm presented in Mortensen et al. [1]. The algorithm is built upon a reformulation of the segmentation approach into graphs, particularly, an image $I$ is converted to an undirected graph with edges from a pixel $p$ to all it's neighbouring 8-connected pixels. Each pixel or node is assigned a local cost according to a function (described later). The segmentation task then becomes a problem where there needs to be a shortest path from a pixel $p$ (seed) to another pixel $q$ (free goal point), where the cumulative cost of path is minimum. The local cost function is defined as:\n\n$l(p,q) = w_G.f_G(q) + w_Z.f_Z(q) + w_D.f_D(p,q)$\n\nwhere $w_i, i = {G, Z, D}$ are weight coefficients controlling relative importance of the terms. The three terms that make up the local cost function are gradient magnitude ($f_G(q)$), Laplacian Zero-Crossing feature ($f_Z(q)$), and Gradient Direction feature ($f_D(p,q)$). The first term $f_G(q)$ defines a strong measure of edge strength, and is heavily weighted. The term $f_Z(q)$ provides a second degree measure for edge strength in the form of zero-crossing information. The third term $f_D(p,q)$ adds a smoothness constraint to the live-wire boundary by adding high cost for rapidly changing gradient directions. \n \n The algorithm also exhibits some other features, namely boundary freezing, on-the-fly learning and data-driven cooling. Boundary freezing proves useful when Live-wire segmentation digresses from the desired object boundary during interactive mode. The boundary can be \"frozen\" right before the digression point by specifying another seed point, until which the boundary is frozen and not allowed to be changed. On-the-fly learning provides robustness to the method by learning the underlying cost distribution of a known \"good\" boundary, and using that to guide the live-wire further to follow similar distribution. Data-driven path cooling allows the live-wire to generate new seed points automatically as a function of image data and path properties. Pixels on \"stable\" paths will cool down and eventually freeze, producing new seed points. \n The results report that average times taken for  segmenting a region using Live-Wire was roughly 4.6 times less than manual human tracing time. Live-Wire provided same amount of accuracy as manual tracing would in a fraction of time, with high reproducibility. \n \n However,  the method does not provide a way to ``snap out\" of an \\textit{automatically} frozen live-wire segmentation. On-the-fly training can fail at instances where the edges of the object change too fast, and not much implementation related information is provided, especially for freezing and training parts. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q48317308"
    },
    "1151": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tmi/PluempitiwiriyawejMWH05",
        "transcript": "Automated segmentation of various anatomical structures of interest from medical images has been a well grounded field of research in medical imaging. One such problem is related to segmenting whole heart region from a sequence of magnetic resonance imaging (MRI), which is currently done manually, and is time consuming and tedious. Although many automated techniques exist for this, the task remains challenging due to the complex nature of the problem, partly because of low contrast between heart and nearby tissue. Moreover many of the methods are unable to incorporate prior information into the process. To this end, Pluempitiwiriyawej et al. proposed a version of active contour energy minimization based method to segment the whole heart region, including the epicardium, and the left and right ventricular endocardia. \n\nThe proposed method follows the framework laid out by Chan and Vese\\cite{Chan2001}. However Pluempitiwiriyawej et al. propose a modified energy function, which consists of four energy terms. The energy function is given below, where $C$ is the contour represented as a level set function $\\phi(x,y)$:\n\n$J(C) = \\lambda_1 J_1(C) + \\lambda_2 J_2(C) + \\lambda_3 J_3(C) + \\lambda_4 J_4(C)$\n\nThe coefficients $\\lambda_{1..4}$ determine the weight of terms $J_{1..4}$. The first term $J_1(C)$ is designed to add stochastic models $\\mathcal{M}_1, \\mathcal{M}_2$ corresponding to the regions inside and outside of the active contour $C$. The models dictate the probability distribution from which the image intensities making up the inside and outside region of the contour are sampled. The negative log of this term is minimized, which essentially maximizes the probability $p(u | C, \\mathcal{M}_1, \\mathcal{M}_2)$ given the active contour $C$, and the models $\\mathcal{M}_1, \\mathcal{M}_2$. \n\nThe second term $J_2(C)$ is designed similar to the classical Snakes\\cite{Kass1988} in the sense that it uses edges to guide the contour towards the structure of interest. For this term, a simple edge map is used after convolving with a Gaussian filter which smooths out the noise. The term $J_3(C)$ encodes an shape prior which constraints the contour to follow an elliptical shape, and guides it in conjunction with the region and edge information. \n\nThe final term $J_4(C)$ which encodes the total Euclidean arc length of the contour. This forces the contour to be ``smooth\", without rough edges. \n\nThe process of minimizing the energy function follows a three-task approach. The first task is to estimate the stochastic model parameters $\\mu_k, \\sigma^2_k$, and is performed by fixing the position of initial contour $C$, taking derivatives of $J$ w.r.t stochastic model parameters, and solving by equating to zero. The second task estimates the parameters of the ellipse using least squares method. The third and final task involves the contour using the estimated parameters in task one and two, such that it minimizes the function $J$. The method also performs stochastic relaxation, by dynamically changing the values of parameters $\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4$ as the optimization process proceeds. The intuition is that when the optimization starts, the edge and region terms must guide the contour, and as the process proceeds to it's end, the shape prior and contour length term should carry more weight to regularize the effective shape of the contour. \n\nThe study used 48 MRI studies acquired by imaging rat hearts, and compared the proposed method with two earlier methods, namely Xu and Prince's GVF \\cite{ChenyangXu1998}, and Chan and Vese \\cite{Chan2001}. The authors also design a new quantitative metric, which is a modification of the Chamfer matching \\cite{Barrow} technique. The reported results are observed to be in excellent agreement with the gold standard hand-traced contours. However the similarity values for other methods against human gold-standard were not reported. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.wikidata.org/entity/Q51469012"
    },
    "1152": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1109/83.902291",
        "transcript": "Typically, the energy minimization or snakes based object detection frameworks evolve a parametrized curve guided by some form of image gradient information. However due to heavy reliance on gradients, the approaches tend to fail in scenarios where this information is misleading or unavailable. This cripples the snake and renders it unusable as it gets stuck in a local-minima away from the actual object. Moreover, the parametrized snake lacks the ability to model multiple evolving curves in a single run. In order to address these issues, Chan and Vese introduced a new framework which utilized region based information to guide a spline, and tries to solve the minimal partition problem formulated by Mumford and Shah. \n\nThe framework is built upon the following energy equation, where $C$ is a level-set formulation of the curve:$F(c1, c2, C) = \\mu . \\text{Length}(C) + v . \\text{Area}(inside(C))\\\\\n        \\lambda_1 \\int_{inside(C)}|u_0(x,y) - c_1|^2 dxdy + \\lambda_2 \\int_{outside(C)}|u_0(x,y) - c_2|^2 dxdy$\nThe framework essentially divides the image into two regions (per curve), which are referred to as inside and outside of the curve. The first two terms of the equation control the physical aspects of the curve, particularly the length, and area inside the curve, with their contributions controlled by two parameters $\\mu$ and $v$. \n\nThe image forces in this equation correspond to the third and fourth terms, which are identical but work in respective regions identified by the curve. The terms use $c_1$ and $c_2$, which are the mean intensity values inside and outside the curve respectively to guide the curve towards a minima where the both regions are consistent with respect to the mean intensity values. \n\nThe proposed framework also utilizes an improved representation of the curve in the form of a level set function $\\phi(x, y)$, which has many numerical advantages and naturally supports multiple curves evolved during a single run, as compared to the traditional snakes model where only one curve can be evolved in a single run.  The unknown function $\\phi$ is computed using Euler-Lagrange equations formulated using modified Heaviside function $H$, and Dirac measure $\\delta$.\n\nThe proposed framework was applied on numerous challenging 2D images with varying degree of difficulties. The framework was also capable of segmenting point clouds decomposed into 2D images, objects with blurred boundaries, and contours without gradients, all without requiring image denoising. \n\nDue to the formulation of $\\phi$ approximation routine, the framework has tendencies to find actual global minima independent of the initial position of the curve. However the choice of multiple parameters namely $\\lambda_{1,2}, \\mu, v$ is done heuristically, and seem to be problem dependent. Also, the framework's implicit dependency on absolute image intensities in regions inside and outside of curve sometimes fail in very specific cases where the averages tend to be zero, though the authors proposed to use image curvature and orientation information from the initial image $u_0$.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/83.902291"
    },
    "1153": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1007/bf00133570",
        "transcript": "Low level tasks such as edge, contour and line detection are an essential precursor to any downstream image analysis processes. However, most of the approaches targeting these problems work as isolated and autonomous entities, without using any high-level image information such as context, global shapes, or user-level input. This leads to errors that can further propagate through the pipeline without providing an opportunity for future correction. In order to address this problem, Kass et al. investigate the application of an energy minimization based framework for edge, line and contour detection in 2D images. \n\nAlthough energy minimization had earlier been utilized for similar tasks, Kass et al\u2019s framework exposes a novel external force factor, which allows external forces or stimuli to guide the ``snake\" towards a \u201ccorrect answer\u201d. Moreover, the framework exhibits an active behaviour since it is designed to always minimize the energy functional. A \u201csnake\u201d is a controlled continuity spline which is under the influence of forces in the energy functional. The energy functional is made up of three terms, where $v(s)$ is a parametrized snake. \n$    E_{snake}^{*} = \\int_{0}^{1}E_{internal}(v(s)) +  E_{image}(v(s)) + E_{constraint}(v(s)) $\n\nThe internal energy force term is entirely dependent on the shape of the curve, which constraints the snake to be \u201ccontinuous\u201d and well behaved. The force further encapsulates two terms which control the degree of stretchness of the curve (represented by the first derivative of the image), and ensure that the curve does not have too many bends (using the second derivative of the image). \n\nThe image energy force term controls what kind of salient features does the snake track. The force encapsulates three terms, corresponding to the presence of lines, edges and a termination term. The line term uses raw image intensity as energy term, while the edge term uses a negative square of image gradient to make the snake attracted to contours with large image gradients. Further, a termination term allows the snake to find terminations of line segments and corners in the image. The combination of line and termination terms forces the snake to be attracted to edges and terminations. \n\nThe constraint term is used to model external stimuli, which can come from high-level processes, or through user intervention. The framework allows the application of a spring to the curve to constraint the snake or move it in a desired direction. \n\nIn order to test the framework, a user interface called ``Snake Pit\" was created which allowed user control in the form of a spring attachment. The overall approach was tested on a number of different images, including the ones with contour illusion. The framework was also extended for application to stereo matching and motion tracking. For stereo, an extra energy term is added to constraint the snakes in two disparate images to stay close in the coordinate space, which models the fact that disparities in images do not change too rapidly. \nHowever the framework suffers when subjected to a high rate-of-change in both stereo matching and motion tracking problems.\n\nThe proposed framework performed acceptably in many challenging scenarios. However the framework's underlying assumption about following edges and contours using image gradients may fail in cases where there is not much gradient information present in images. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/bf00133570"
    },
    "1154": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1806.00340",
        "transcript": "The paper presents a model-agnostic extension of deep learning classifiers based on a RNN  with a visual attention mechanism for report generation. \n![](https://i.imgur.com/3TQb5TG.png)\nOne of the most important points in this paper is not the model, but the dataset they itself: Luke Oakden-Rayner, one of the authors, is a radiologist and worked a lot to educate the public on current medical datasets ([chest x-ray blog post](https://lukeoakdenrayner.wordpress.com/2017/12/18/the-chestxray14-dataset-problems/)), how they are made and what are the problems associated with them. In this paper they used 50,363 frontal pelvic X-rays, containing 4,010 hip fractures, the original dataset contained descriptive sentences, but these had highly inconsistent structure and content. A radiologist created a new set of sentences more appropriate to the task, from their [blog post](https://lukeoakdenrayner.wordpress.com/2018/06/05/explain-yourself-machine-producing-simple-text-descriptions-for-ai-interpretability/):\n> We simply created sentences with a fixed grammatical structure and a tiny vocabulary (26 words!). We stripped the task back to the simplest useful elements. For example: \u201cThere is a mildly displaced comminuted fracture of the left neck of the femur.\u201d Using sentences like that we build a RNN to generate text*, on top of the detection model.\n\n>And that is the research in a nutshell! No fancy new models, no new maths or theoretical breakthroughs. Just sensible engineering to make the task tractable.\n\nThis paper shows the importance of a well-built dataset in medical imaging and how it can thus lead to impressive results:\n![](https://i.imgur.com/5BVU9WF.png)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1806.00340v1"
    },
    "1155": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1804.02341",
        "transcript": "This paper proposes a new training method for multi-agent communication settings. They show the following referential game: A speaker sees an image of a 3d rendered object and describes it to a listener. The listener sees a different image and must decide if it is the same object as described by the speaker (has the same color and shape). The game can only be completed successfully if a communication protocol emerges that can express the color and shape the speaker sees.\n\nThe main contribution of the paper is the training algorithm. The speaker enumerates the message that would maximise its own understanding of the message given the image it sees (in a greedy way, symbol by symbol). The listener, given the image and the message, predicts a binary output and is trained using maximum likelihood given the correct answer. Only the listener is updating its parameters - therefore the speaker and listener change roles every number of rounds.\n\nThey show that a compositional communication protocol has emerged and evaluate it using zero-shot tests.\n\n[Implemenation of this paper in pytorch](https://github.com/benbogin/obverter)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1804.02341v1"
    },
    "1156": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.21105/joss.00676",
        "transcript": "Modeling and simulation of proton-exchange membrane fuel cells (PEMFC) may work as a powerful tool in the Research & development of renewable energy sources. The Open-Source PEMFC Simulation Tool (OPEM) is a modeling tool for evaluating the performance of proton exchange membrane fuel cells. This package is a combination of models (static/dynamic) that predict the optimum operating parameters of PEMFC. OPEM contained generic models that will accept as input, not only values of the operating variables such as anode and cathode feed gas, pressure and compositions, cell temperature and current density, but also cell parameters including the active area and membrane thickness. In addition, some of the different models of PEMFC that have been proposed in the OPEM, just focus on one particular FC stack, and some others take into account a part or all auxiliaries such as reformers. OPEM is a platform for collaborative development of PEMFC models.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.21105/joss.00676"
    },
    "1157": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.21105/joss.00729",
        "transcript": "PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters.\tPyCM is the swiss-army knife of confusion matrices, targeted mainly at data scientists that need a broad array of metrics for predictive models and an accurate evaluation of large variety of classifiers.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.21105/joss.00729"
    },
    "1158": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1016/j.compeleceng.2008.12.005",
        "transcript": "## IDS approaches for events\n** Misuse-based: **\nDetects events that violate system policy. *Snort* is a signature-based open-source IDS system used for misuse detection in this research. \n\n** Anomaly-based: ** \nDetects events that contain abnormal activity. Uses statistical, heuristic and data mining methods.  *Packet header anomaly detector (PHAD)* [1] and *Network traffic anomaly detector (NETAD)* [2] are used as *Snort* preprocessors for anomaly detection.\n\n\n## Hybrid architecture\n**Preprocessors** (for *Snort)*: Can filter and modify packets before it reaches main detection engine. They can also generate alerts individually.\n\nIn section 4, researchers explain *Snort* software details for preprocessor source-code integration and their implementation of the hybrid system.\n\n## Evaluation of hybrid system\n*MIT Lincoln Laboratory IDEVAL dataset* used for evaluation of IDS system. A more detailed explanation and simulation environment information about the dataset can be found in section 5, as well in reference articles [3-5].\n\nhttps://i.imgur.com/SrAvySJ.png\n\nResearchers evaluated *Snort* only, *Snort + PHAD* and *Snort + PHAD + NETAD * (hybrid IDS) variations and compared detection capabilities of each system based on detected intrusion counts. Results can be seen in the figure. Based on results hybrid IDS is the most successful on *IDEVAL dataset*. \n\n## References\n\n[1] Mahoney MV, Chan PK. PHAD: packet header anomaly detection for identifying hostile network traffic. Florida Institute of Technology Technical Report, CS-2001-04.\n\n[2] Mahoney MV. Network traffic anomaly detection based on packet bytes. In Proceedings of ACM-SAC; 2003.\n\n[3] Lippman R, Haines JW, Fried DJ, Korba J, Das K. Analysis and results of the 1999 DARPA off-line intrusion detection evaluation. In Proceedings of the\nthird international workshop on recent advances in intrusion detection, Toulouse, France; 2\u20134 October 2000. p. 162\u201382.\n\n[4] Haines JW, Lippman R, Fried DJ, Zissman MA, Tran E, Boswell SB. 1999 DARPA intrusion detection evaluation: design and procedures. MIT Lincoln Laboratory Technical Report, TR-1062, Massachusetts, USA; 2001.\n\n[5] Data set, DARPA intrusion detection evaluation data set; 1999. <http://www.ll.mit.edu/IST/ideval/data/1999/1999_data_index.html>\n.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/j.compeleceng.2008.12.005"
    },
    "1159": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.06559",
        "transcript": "The paper combines reinforcement learning with active learning to learn when to request labels to improve prediction accuracy. \n\n- The model can either predict the label at time step $t$ or request it in the next time step, in form of a one-hot vector output of an LSTM with the previous label (if requested) and the current image as an input.\n- A reward is issued based on the outcome of requesting labels (-0.05), or correctly (+1) or incorrectly(-1) predicting the label.\n- The optimal strategy involves storing class embeddings and their labels in memory and only requesting labels if a unseen class is encountered.\n \nThe model is evaluated against the *Omniglot* dataset and learns a non-naive strategy to request fewer labels the more data of a class was encountered, using a learned uncertainty measure.\nThe magnitude of the reward for incorrect labeling decides on the amount of requested labels and can be used to maximize the accuracy during prediction.\n\n\n## Active Learning\nActive Learning is a  special case of semi-supervised learning, which aims to reduce the amount of supervision needed during training. The model typically selects which datapoints to label by applying different metrics like most information content, highest uncertainty or other heuristics.\n\n## Reinforcement Learning\nReinforcement learning agents try to learn an optimal policy $\\pi^*(s_t)$ for a state $s_t$ at time $t$ that will maximize future rewards issued by the environment, by choosing an action $a_t$. The policy is represented by a function $Q^*(s_t, a_t)$, which can be approximated and learned in form of a neural network.\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1702.06559"
    },
    "1160": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ChristianoLBMLA17",
        "transcript": "- explore RL systems with (non-expert) human preferences between pairs of trajectory segments;\n- run experiments on some RL tasks, namely **Atari** and **MuJoCo**, and show effectiveness of this approach;\n- advantages mentioned: \n\t- no need to access to the reward function; \n\t- less than 1% feedback needed -> reduce the cost of human oversight;\n\t- can learn complex novel behaviors.\n\n## Introduction\n\n**Challenges**\n\n- goals complex, pooly-defined or hard to specify;\n- reward function -> behaviors that optimize reward function without achieving goals;\n\n> This difficulty underlines recent concerns about misalignment between reward values and the objectives of RL systems.\n\n**Alternatives**\n\n- inverse RL: extract a reward function from demonstrations of desired tasks;\n- imitation learning: clone the demonstrated behavior;\n\n*con: not applicable to behaviors that are hard to demonstrate for humans*\n\n- use human feedback as a reward function;\n\n*con: require thousands of hours of experience, prohibitively expensive*\n\n**Basic idea**\n\nhttps://i.imgur.com/3R7tO7R.png\n\n**Contributions**\n\n1. solve tasks for which we can only recognize but not demonstrate the desired behaviors;\n2. allow non-expert agent training;\n3. scale to larger problems;\n4. economical with user feedback.\n\n**Related work**\n\ntwo lines of work: (1) RL from human ratings or rankings; (2) general problemn of RL from preferences rather than absolute reward values.\n\n*close-related paper:*\n\n(1) [Active preference learning-based\nreinforcement learning](https://arxiv.org/abs/1208.0984); (2) [Programming by\nfeedback](http://proceedings.mlr.press/v32/schoenauer14.pdf); (3) [A Bayesian approach for policy learning from\ntrajectory preference queries](https://papers.nips.cc/paper/4805-a-bayesian-approach-for-policy-learning-from-trajectory-preference-queries).\n\n*diffs with (1)-(2)*: \n\na) elicit preferences over whole trajectories rather than short clips; b) change training procedure to cope with nonlinear reward models and modern deep RL.\n\n*diffs with (3)*: \n\na) fit reward function by Bayesian inference; b) produce trajectories using MAP estimate of the target policy instead of RL -> involve 'synthetic' human feedback drawn from Bayesian model.\n\n## Preliminaries and Method\n\n**Agent goal**\n\nto produce trajectories which are preferred by the human, while making as few queries as possible to the human.\n\n**Work flow**\n\nat each point maintains two deep NNs - policy *pi*: O -> A; reward estimate *r\\_hat*: O x A -> R. \n\n*Update procedure (asyn):*\n\n1. policy *pi* => env => trajectories *tau* = {*tau^1*,..., *tau^i*}. Then update *pi* by a traditional RL algorithm to maximize the sum of predicted rewards *r\\_t* = *r\\_hat*(*o\\_t*, *a\\_t*);\n2. select segment pairs *sigma* = (*sigma^1*, *sigma^2*) from *tau*. *sigma* => human comparison => labeled data;\n3. update *r\\_hat* with labeled data by supervised learning.\n\n> step 1 => trajectory *tau* => step 2 => human comparison => step 3 => parameters for *r\\_hat* => step 1 => ....\n\n**Policy optimization (step 1)**\n\n*subtlety:* non-stationary reward function *r\\_hat* -> methods robust to changes in reward function.\n\nA2C => Atari, TRPO => MuJoCo.\n\nuse parameter settings that work well for traditional RL tasks; only adjust the entropy bonus for TRPO (improve inadequate exploration); normalize rewards to zero mean and constand std. \n\n**Preference eliciation (step 2)**\n\nclips of trajectory segments for 1 to 2 seconds long.\n\n*data struct:* triples (*sigma^1*, *sigma^2*, *mu*), *mu* - distribution over {1, 2}.\n\none preferable over the others -> *mu* puts all mass on that choice; equally preferable -> *mu* uniform; incomparable -> skip saving triples.\n\n**Fitting the reward function (step 3)**\n\n*assumption:* human\u2019s probability of preferring a segment *sigma^i* depends exponentially on the value of the latent reward summed over the length of the clip.\n\nhttps://i.imgur.com/2ViIWcL.png\n\n(no discount of reward <- human being indifferent about when things happen in the trajectory segment; could consider discounting.)\n\nhttps://i.imgur.com/cpdTZW6.png\n\n*modifications:*\n\n1. ensemble of predictors -> independently normalize base predictors and then average results;\n2. validation set ratio 1/e; employ *l\\_2* regularization, and tune regularization coefficient -> validation loss = 1.1~1.5 training loss; dropout in some domains;\n3. assume 10% chance that human responds uniformly at random;\n\n**Selecting queries**\n\n*pipeline:* sample trajectory segments of length k -> predict preference by base reward predictor in our ensemble -> select trajectories with the highest variance across ensemble members\n\n*future work:* query based on the expected value of information of query.\n\n*Related articles:* \n\n1. [APRIL: Active Preference-learning based Reinforcement Learning](https://arxiv.org/abs/1208.0984)\n2. [Active reinforcement learning: Observing rewards at a cost](http://www.filmnips.com/wp-content/uploads/2016/11/FILM-NIPS2016_paper_30.pdf)\n\n> At each time-step, the agent chooses both an action and whether to observe the\nreward in the next time-step. If the agent chooses to observe the reward, then it\npays the \u201cquery cost\u201d c > 0. The agent\u2019s objective is to maximize total reward\nminus total query cost.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences"
    },
    "1161": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1705.08245",
        "transcript": "- *issue:* RL on real systems -> sparse and slow data sampling;\n- *solution:* pre-train the agent with the EGAN;\n- *performance:* ~20% improvement of training time in the beginning of learning compared to no pre-training; ~5% improvement and smaller variations compared to GAN pre-training.\n\n## Introduction\n\n5G telecom systems -> fufill ultra-low latency, high robustness, quick response to changed capacity needs, and dynamic allocation of functionality.\n\n*Problems:*\n\n1. exploration has an impact on the service quality in real-time service systems;\n2. sparse and slow data sampling -> extended training duration.\n\n## Enhanced GAN\n\n**Fomulas**\n\nthe training data for RL tasks:\n\n$$x = [x_1, x_2] = [(s_t,a),(s_{t+1},r)]$$\n\nthe generated data:\n\n$$G(z) = [G_1(z), G_2(z)] =  [(s'_t,a'),(s'_{t+1},r')] $$\n\nthe value function for GAN:\n\n$$V(D,G) = \\mathbb{E}_{z \\sim p_z(z)}[\\log(1-D(G(z)))] + \\lambda D_{KL}(P||Q)$$\n\nwhere the regularization term $D_{KL}$ has the following form:\n\n$$D_{KL}(P||Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}$$\n\n**EGAN structure**\n\nhttps://i.imgur.com/FhPxamJ.png\n\n**Algorithm**\n\nhttps://i.imgur.com/RzOGmNy.png\n\nThe enhancer is fed with training data *D\\_r(s\\_t, a)* and *D\\_r(s\\_{t+1}, r)*, and trained by supervised learning. After GAN generates synthetic data *D\\_t(s\\_t, a, s\\_{t+1}, r)*, the enhancer could enhance the dependency between *D\\_t(s\\_t, a)* and *D\\_t(s\\_{t+1}, r)* and update the weights of GAN.\n\n## Results\n\ntwo lines of experiments on CartPole environment involved with PG agents: \n\n1. one for comparing the learning curves of agents with no pre-training, GAN pre-training and EGAN pre-training. => Result: EGAN > GAN > no pre-training\n\n2. one for comparing the learning curves of agents with EGAN pre-training for various episodes (500, 2000, 5000). => Result: 5000 > 2000 ~= 500\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.08245v2"
    },
    "1162": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/sigart/Sutton91",
        "transcript": "Main idea: planning is 'trying things in your head' using an internal model of the world\n\n#### Diagram\n\nhttps://i.imgur.com/vDAobu1.png\n\n#### Generic algorithm\n\n- step 1-3: standard reinforcement learning agent\n- step 4: learning of domain knowledge - action model\n- step 5: RL from hypothetical, model-generated experiences - planning\n\nhttps://i.imgur.com/G6dZ00F.png\n\n#### Action model\n\ninput: state, action; output: immediate resulting state and reward\n\nsearch control: how to select hypothetical state and action\n\n## Potential problems\n\n1. reliance on supervised learning\n2. hierarchical planning\n3. ambiguous and hidden state\n4. ensuring variety in action\n5. taskability\n6. incorporation of prior knowledge",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/122344.122377"
    },
    "1163": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.05952",
        "transcript": "this paper: develop a framework to replay important transitions more frequently -> learn efficienty\n\nprior work: uniformly sample a replay memory to get experience transitions\n \nevaluate: DQN + PER outperform DQN on 41 out of 49 Atari games\n\n## Introduction\n\n**issues with online RL:** (solution: experience replay) \n\n1. strongly correlated updates that break the i.i.d. assumption\n2. rapid forgetting of rare experiences that could be useful later\n\n**key idea:** \n\nmore frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error\n\n**issues with prioritization:**\n\n1. loss of diversity -> alleviate with stochastic prioritization\n2. introduce bias -> correct with importance sampling\n\n## Prioritized Replay\n\n**criterion:**\n\n- the amount the RL agent can learn from a transition in its current state (expected learning progress) -> not directly accessible\n- proxy: the magnitude of a transition\u2019s TD error ~= how far the value is from its next-step bootstrap estimate\n\n**stochastic sampling:**\n\n$$P(i)=\\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}$$\n\n*p_i* > 0: priority of transition *i*; 0 <= *alpha* <= 1 determines how much prioritization is used.\n\n*two variants:*\n\n1. proportional prioritization: *p_i* = abs(TD\\_error\\_i) + epsilon (small positive constant to avoid zero prob)\n2. rank-based prioritization: *p_i* = 1/rank(i); **more robust as it is insensitive to outliers**\n\nhttps://i.imgur.com/T8je5wj.png\n\n**importance sampling:**\n\nIS weights: \n\n$$w_i = \\left(\\frac{1}{N} \\cdot \\frac{1}{P(i)}\\right)^\\beta $$\n\n- weights can be folded into the Q-learning update by using $w_i*\\delta_i$ instead of $\\delta_i$\n- weights normalized by $\\frac{1}{\\max w_i}$",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.05952"
    },
    "1164": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1406.2661",
        "transcript": "GAN - derive backprop signals through a **competitive process** invovling a pair of networks;\n\nAim: provide an overview of GANs for signal processing community, drawing on familiar analogies and concepts; point to remaining challenges in theory and applications.\n\n## Introduction\n\n- How to achieve: implicitly modelling high-dimensional distributions of data\n- generator receives **no direct access to real images** but error signal from discriminator\n- discriminator receives both the synthetic samples and samples drawn from the real images\n- G: G(z) -> R^|x|, where z \\in R^|z| is a sample from latent space, x \\in R^|x| is an image\n- D: D(x) -> (0, 1). may not be trained in practice until the generator is optimal\n\nhttps://i.imgur.com/wOwSXhy.png\n\n## Preliminaries\n\n- objective functions J_G(theta_G;theta_D) and J_D(theta_D;theta_G) are **co-dependent** as they are iteratively updated\n- difficulty: hard to construct likelihood functions for high-dimensional, real-world image data\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1406.2661"
    },
    "1165": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1101/262501",
        "transcript": "Single cells behave in complex and sometimes seemingly random ways. We have applied Generative Adversarial Networks (GANs), a form of artificial intelligence, to make sense of the way genes are controlled in skin cells. A GAN involves two separate neural networks, a \u2018generator\u2019 and a \u2018discriminator\u2019. The generator simulates cells and the discriminator tries to tell the difference between the fake data created by the simulator, and data from real cells. As they compete against each other they improve at their tasks and provide new insights into the way cells behave.   \n\nOur neural network approach allows us to understand the relationship between different genes and how this contributes to cell behaviour. One of the networks, the generator, is responsible for simulating cells and we use this to predict how genes are controlled under different conditions, effectively simulating what would previously have been laborious and painstaking experiments. GANs also make it possible to compare data from multiple labs produced in different conditions, opening up the opportunity to answer new questions.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1101/262501"
    },
    "1166": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.09477",
        "transcript": "As in Q-learning, modern actor-critic methods suffer from value estimation errors due to high bias and variance. While there are many attempts to address this in Q-learning (such as Double DQN), not much was done in actor-critic methods.\n\nAuthors of the paper propose three modifications to DDPG and empirically show that they help address both bias and variance issues:\n\n* 1.) Clipped Double Q-Learning:  \n  Add a second pair of critics $Q_{\\theta}$ and $Q_{\\theta_\\text{target}}$ (so four critics total) and use them to upper-bound the value estimate target update: $y = r + \\gamma \\min\\limits_{i=1,2} Q_{\\theta_{target,i}}(s', \\pi_{\\phi_1}(s'))$\n* 2.) Reduce number of policy and target networks updates, and magnitude of target networks updates: $\\theta_{target} \\leftarrow \\tau\\theta + (1-\\tau)\\theta_{target}$\n* 3.) Inject (clipped) random noise to the target policy: $\\hat{a} \\leftarrow \\pi_{\\phi_{target}}(s) + \\text{clip}(N(0,\\sigma), -c, c)$\n\nImplementing these results, authors show significant improvements on seven continuous control tasks, beating not only reference DDPG algorithm, but also PPO, TRPO and ACKTR.\n\nFull algorithm from the paper:\n\nhttps://i.imgur.com/rRjwDyT.png\n\nSource code: https://github.com/sfujim/TD3",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.09477v1"
    },
    "1167": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1038/s41698-017-0029-7",
        "transcript": "The overarching goal of precision oncology is to use personal genomic information, like gene expression data from a tumor biopsy, to devise individualized treatment plans. Network-based analytics support this goal in three ways. First, by helping us to identify malfunctioning gene pathways by observing patient-specific gene expression levels in relation to one another. Second, by enabling us to predict the phenotype, or observable characteristics, of a patient's cancer. Third, by recommending drugs to re-purpose as chemotherapies that targeting a specific gene or protein that has been dysregulated in a pathway. \n\n## Identifying Pathological Gene Pathways\nCancer is best characterized as a disease caused by frequently mutated genes that cause dysregulated biological pathways. There are many kinds of biological pathways, such as metabolic pathways that may synthesize useful molecules or break down complex molecules, or a gene-regulatory pathway wherein molecules like proteins, mRNA, or DNA regulate the production, expression, and activity of other molecules thereby affecting the behaviour of the cell.\n\nSystems biology makes use of many types of pathways, which may be viewed as molecular networks like the gene-regulatory network, the protein-protein interaction network, and the cellular signal transduction network. One goal of network-based analysis of personal genomic profiles is to identify network modules that are causative of cancer and informative of cancer phenotype. This paper outlines three ways of integrating molecular networks to make these predictions: model-based integration, pre-processing integration, and post-analysis integration.\n\nThe authors use the phrase **pre-processing integration** to indicate a two-step process. The first step detects molecular sub-networks by using highly discriminative genes as seed genes in a greedy search to find discriminative sub-networks. The gene expression in each sub-network is then normalized as one feature value for classification with logistic regression or another method.  This method allows you to customize your sub-network features, but generally does not produce optimal predictions.\n\nAn example application of this type of method comes from a paper called [Network-based classification of breast cancer metastasis](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2063581/). Their goal is to determine which gene pathways give rise to metastasis. They note that previous methods had identified certain genes as biomarkers correlated with metastasis. Their approach was to use these genes as seeds for an algorithm to identify discriminative sub-networks in a protein-protein interaction network.\nhttps://i.imgur.com/WCf18fZ.png\n\nTo conduct the **post-analysis integration** of oncogenic alterations in networks, first analyze the patient genomic profiles to generate a list of oncogenic alterations. Second, map those alterations to the network as seed genes for the module analysis. This method is highly informative of cancer mechanisms in the network, but relies on accurate identification of oncogenic alterations in the patient data.\nhttps://i.imgur.com/5CbqQTg.png\n\nWhen using a **model-based integration** approach, you incorporate the molecular network into the machine learning model as a regularizing graph Laplacian. The coefficients that the model learns form dense sub-networks that are used to enable the model to predict patient survival or cancer phenotype. This form of framework has a global optimization strategy and generally produces the best outcome predictions.\nhttps://i.imgur.com/vRH1K8R.png\n\n## Methods for Drug Re-purposing\nIf you want to re-purpose a drug for use against a cancer target, there are three networks that are useful to inform your work: drug-drug similarity, drug-target relations, and gene-gene relations. Additionally, there are three types of methods that you can use with this data to create a predictive model: graph connectivity measures, link prediction models, and network based classification methods.\n\n## Graph Connectivity Methods\nSeveral studies have shown that drugs sharing similar chemical structures, side-effects, or similarities in gene-expression profiles following drug treatment, can be good candidates for re-purposing. It follows that we may combine a drug-drug interaction network, a drug-target interaction network, and a target-target interaction network to estimate the likelihood that a specific drug may be re-purposed for a specific target. The proxy that is used for similarity is simply length of the path from the query drug in the drug-drug interaction network to the target in the target-target interaction network.\nhttps://i.imgur.com/abmHkKl.png\n\n## Link Prediction Models\nThe link prediction model is close to the Graph Connectivity method, but relies on more advanced similarity measures and the global graph structure to determine relations between drugs and targets. For example, one paper using a link prediction model for drug repurposing uses drug-drug two dimensional structural similarity, target-target genomic sequence similarity, and a dataset of known drug-target interactions. Once these measures have been calculated two established methods are used to predict new drug-target relations: matrix completion and random walks.\nhttps://i.imgur.com/kElewfE.png\n\n## Network-based Classification Methods\nHere we view drug repositioning as a classification problem and apply standard classification methods such as SVM. The inputs to the classifier are network topological features (local neighborhood) and the labels are from a dataset of drug-target relations describing whether a specific drug is effective against a specific target. You use the classifier to make predictions on the gene neighborhoods present in your test set.\nhttps://i.imgur.com/cwfJr8t.png\n\n## Network-Based Analysis of TCGA Mutation Data \nThe paper also conducts a network-based analysis mutated genes in 31 cancer genome projects in TCGA using enriched KEGG pathways. The authors find that they are able to re-identify an important signalling pathway that requires the gene AMPK and is known to be affected in breast cancer (BRCA) and uterine corpus endometrial cancer (UCEC).\n\n## Conclusion\nThis article surveys precision oncology, primarily discussing drug re-positioning and alorithms for integrating molecular networks with patient data. It also contains a case-study on ovarian cancer, and a network-based analysis of TCGA data using gene mutations.\n\n## Future Work\nThe authors outline several directions for future work. One direction is to explore higher-resolution graphs that distinguish between isoforms of genes and capture more complex relationships between molecules. Another is to research heterogeneous cell populations in tumors using single-cell RNA sequencing to differentiate cells with different drug targets. Finally, it is essential to develop a standardized software platform that integrates biomedical, biological network data, and analytic software to support comprehensive network-based analysis of patient genomic data and drug re-positioning.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1038/s41698-017-0029-7"
    },
    "1168": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1712.02555",
        "transcript": "Since all algorithms can be modeled as multiple conditional branch operations, this paper allows you to incorporate conventional algorithms into neural networks by dynamically building the neural computation graph based on outputs of these algorithms.\n\nThey obtain near SOTA on Quora Duplicate Questions and SQuAD without heavily fine tuning the architecture to each problem.\n\nOne limitation is that the algorithm itself is not affected by the learning process and so cannot be learned.\n\nThis method provides a nice way to incorporate non-differentiable code into differentiable computation graphs which can be learned via backprop like learning mechanisms.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1712.02555"
    },
    "1169": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.07283",
        "transcript": "The paper starts with the BNN with latent variable and proposes an entropy-based and a variance-based measure of prediction uncertainty. For each uncertainty measure, the authors propose a decomposition of the aleatoric term and epistemic term. A simple regression toy experiment proves this decomposition and its measure of uncertainty. Then the author tries to improve the regression toy experiment performance by using this uncertainty measure into an active learning scheme. For each batch, they would actively sample which data to label. The result shows that using epistemic uncertainty alone outperforms using total certainty, which both outperforms simple gaussian process. The result is understandable since epistemic is directly related to model weight uncertainty, and sampling from high aleatoric uncertain area does help supervised learning.\n\nThen the authors talk about how to extend the model based RL by adding a risk term which consider both aleatoric term and epistemic term, and its related to model-bias and noise aversion. The experiments on Industrial Benchmark shows the method is able prevent overfitting the learned model and better transfer to real world, but the method seems to be pretty sensitive to $\\beta$ and $\\gamma$. ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1710.07283"
    },
    "1170": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=kirkpatrick2016overcoming",
        "transcript": "This paper proposes a simple method for sequentially training new tasks and avoid catastrophic forgetting. The paper starts with the Bayesian formulation of learning a model that is\n$$\n\\log P(\\theta | D) = \\log P(D | \\theta) + \\log P(\\theta) - \\log P(D)\n$$\nBy switching the prior into the posterior of previous task(s), we have\n$$\n\\log P(\\theta | D) = \\log P(D | \\theta) + \\log P(\\theta | D_{prev}) - \\log P(D)\n$$\nThe paper use the following form for posterior\n$$\nP(\\theta | D_{prev}) = N(\\theta_{prev}, diag(F))\n$$\nwhere $F$ is the Fisher Information matrix $E_x[ \\nabla_\\theta \\log P(x|\\theta) (\\nabla_\\theta \\log P(x|\\theta))^T]$. Then the resulting objective function is\n$$\nL(\\theta) = L_{new}(\\theta) + \\frac{\\lambda}{2}\\sum F_{ii} (\\theta_i - \\theta^{prev*}_i)^2\n$$\nwhere $L_{new}$ is the loss on new task, and $\\theta^{prev*}$ is previous best parameter. It can be viewed as a distance which uses Fisher Informatrix to properly scale each dimension, and it further proves that the Fisher Information matrix is important in the experienment by comparing with simple $L_2$ distance.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1612.00796"
    },
    "1171": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/DziugaiteR17",
        "transcript": "This paper proposes a method to obtain a non-vacuous bound on generalization error by optimizing the PAC-Bayes bound directly. The interesting part is that the authors leverage the black magic of neural net itself to bound the neural net. In order to find the optimal Q, the authors' loss function is an empirical err term plus the $KL(Q|P)$, where they choose the prior $P$ to be $N(0, \\lambda I)$, and they also provide justification for choosing the right $\\lambda$. Overally, this objective is similar to the variational inference in the Bayesian neural net, and the author is able to obtain a test error bound of $17\\%$ on MNIST, while the tradition bounds will be mostly meaningless.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.11008"
    },
    "1172": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/ZhuFULL17",
        "transcript": "[FashionGAN][1] works as follows. Given an input image of a person and a sentence describing an outfit, the model tries to \"redress\" the person in the image.\nThe Generator in the model is stacked. \n* The first stage of the generator gets as input a low resolution version of the segmentation of the input image (which is obtained independently) and the design encoding, and generates a **human segmentation map** (not dressed). \n* Then in the second stage, the model renders the generated image using another generator conditioned on the design encoding. It adds region specific texture using the segmentation map and generates the final image.\n\n![FashionGAN Model](https://i.imgur.com/DzwB8xm.png \"FasionGAN model\")\n\nThey added sentence descriptions to a subset of the [DeepFashion dataset][2] (79k examples).\n\n[1]:http://mmlab.ie.cuhk.edu.hk/projects/FashionGAN/\n[2]:http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/ICCV.2017.186"
    },
    "1173": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=muller2002step-size-adapt",
        "transcript": "The idea of the paper is to use reinforcement learning methods to learn a step size adaption rule to be used with evolutionary strategies.\n\nThe authors use SARSA \\cite{Rummery:1994} to learn a step size adaption. Their goal is to learn an adaption rule similar to the $1/5$ success rule \\cite{schwefel1995evolution-and-o}, which updates the step size every $n$ mutations. It checks the number of successes over the preceding $10n$ mutations. If the number is less than $2n$ the step lengths are multiplied by a factor $0.85$ otherwise they are divided by $0.85$.\nSARSA learns a policy which decides (after every $n$ mutations) to either divide the step size; multiply the step size or keep the step size constant (using the same adaption factor of $0.85$)\n\nThey evaluate their approach on the Rosenbrock function in several dimensions as well on optimization of the sphere. The optimization is terminated as soon as the optimized function value is smaller than a threshold $f < 10 ^{-10}$. If this criterion is not met within $N$ generations, the run is counted as not converged.\n\nThe authors consider four reward funcions to learn the desired policy:\n$$\\begin{array}{ll}\nR_1(s_t)=\\left\\{\n\\begin{array}{ll}\n+1&\\mathrm{SR}\\;\\mathrm{increased}\\\\\n\\;\\;\\;0&\\mathrm{SR}\\;\\mathrm{unchanged}\\\\\n-1&\\mathrm{SR}\\;\\mathrm{decreased}\n\\end{array}\\right\\. &\nR_2(s_t)=f_{s_t} - f_{s_{t-1}}\\\\\nR_3(s_t)=\\mathrm{sgn}(f_{s_t} - f_{s_{t-1}})&\nR_4(s_t)=||x_{s_t}-x_{s_{t-1}}||\\cdot\\mathrm{sgn}(f_{s_t} - f_{s_{t-1}})\n\\end{array}$$\nwhere SR is the convergence rate,  $R(s_t)$ is the reward at step $t$, $s_t$ the current state, $s_{t-1}$ the state at which the previous reward was computed and $x_{s_t}$ the point at which $f$ is evaluated at time step $t$. Additionally they constrain the step size to lie in the interval $\\left[10^{-15}, 10^{15}\\right\\]$, giving a reward of $-1$ if the step size is out of bounds.\n\nWith their experiments they demonstrated that they could learn a policy that is very close to the $1/5$ rule.\nThe combined RL-ES approach converged slower (e.g. $R_1$ $3$ to $7$ times slower on the Sphere function and $R_2\\approx 3$times slower) than the pure ES approach and the following table shows that different rewards lead to different convergence rates.\n\nProblem | $N$ | $R_1$ conv. rate |$R_2$ conv. rate |$R_3$ conv. rate |$R_4$ conv. rate\n---------:|:------:|------------------------:|----------------------:|-----------------------:|-----------------------:\nSph 1D| $10^4$| $0.72$|$1.00$|$0.96$|$1.00$\nSph 5D| $10^4$| $0.64$|$0.98$|$0.94$|$0.99$\nSph 20D| $10^4$| $0.50$|$0.92$|$0.90$|$0.94$\nSph 80D| $10^5$| $0.57$|$1.00$|$0.99$|$1.00$\nRos 2D| $10^6$| $0.63$|$0.99$|$0.80$|$1.00$\nRos 5D| $10^7$| $0.77$|$1.00$|$0.77$|$1.00$\n\nwhere $N$ is the number of generations within which the optimized function value has to be lower than $10^{-10}$.\n\nThe authors demonstrated in a small setting that they were able to learn a well tested adaptation rule. They demonstrated neatly that the choice of the reward function is crucial in learning and that similar rewards can lead to very different results. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.cs.vu.nl/ci/ec/catalog/src/muller-2002.pdf"
    },
    "1174": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SchulmanLMJA15",
        "transcript": "The authors present an iterative approach for optimizing policies with guaranteed monotonic improvement.\nTRPO is similar to natural policy gradient methods and can be applied effectively in optimization of large nonlinear policies.\n\n\\cite{KakadeL02} gave monotonic improvement guarantees for mixture of policies $\\pi_{new}(a|s)=(1-\\alpha)\\pi_{old}(a|s) + \\alpha\\pi'(a|s)$ where $\\pi'=\\mathrm{arg}\\max_{\\pi'}L_{\\pi_{old}}(\\pi')$ is the approximated expected return of a policy $\\pi'$ in terms of the advantage over $\\pi_{old}$, as $\\eta(\\pi_{new})\\geq L_{\\pi_{old}}(\\pi_{new}) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2$ with $\\eta$ the true expected return and $\\epsilon$ the maximally expected advantage.\n\nThe authors extend this approach to be applicable for all stochastic policy classes by replacing $\\alpha$ with a distance measure between two policies $\\pi_{new}$ and $\\pi_{old}$.\nAs distance measure they use the maximal Kullback\u2013Leibler divergence $D_{KL}^{\\max}(\\pi_{new},\\pi_{old})$ and show that $\\eta(\\pi_{new})\\geq L_{\\pi_{old}}(\\pi_{new}) -CD_{KL}^{\\max}(\\pi_{new},\\pi_{old})$, with $C= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}$.\n\nFrom this follows, that one is guaranteed to improve the true objective $\\eta$ when performing the following maximaization $\\mathrm{maximize}_\\pi\\left[L_{\\pi_{old}}(\\pi)-CD_{KL}^{\\max}(\\pi,\\pi_{old})\\right]$. In practice however $C$ would only allow for small steps. Thus constraining $\\mathrm{maximize}_\\pi L_{\\pi_{old}}(\\pi)$ subject to $D_{KL}^{\\max}(\\pi,\\pi_{old}) \\leq \\delta$ allows for larger steps in a **Trust Region**\n\nDue to the large number of constraints this problem is impractical to solve, which is why the authors replace the maximum KL divergence with approximated average KL.\n\nTRPO then works as follows:\n 1. Use a rollout procedure to collet a set of state-action-pairs wit Monte Carlo estimates of their $Q$-Values\n 2. Average over the samples to construct the estimate objective $L_{\\pi}$ as well as the constraint\n 3. Approximately solve the constrained optimization problem to update the policy parameters.\n They use the conjugate gradient algorithm followed by a linesearch.\n\nTheir experiments support the claim that TRPO is able to effectively optimize large nonlinear policies.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1502.05477"
    },
    "1175": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/Kawaguchi16",
        "transcript": "# Main Results (tl;dr)\n## Deep *Linear* Networks\n1. Loss function is **non-convex** and non-concave\n2. **Every local minimum is a global minimum**\n3. Shallow neural networks *don't* have bad saddle points\n4. Deep neural networks *do* have bad saddle points\n\n## Deep *ReLU* Networks\n* Same results as above by reduction to deep linear networks under strong simplifying assumptions\n* Strong assumptions:\n  * The probability that a path through the ReLU network is active is the same, agnostic to which path it is.\n  * The activations of the network are independent of the input data and the weights.\n\n## Highlighted Takeaways\n* Depth *doesn't* create non-global minima, but depth *does* create bad saddle points.\n* This paper moves deep linear networks closer to a good model for deep ReLU networks by discarding 5 of the 7 of the previously used assumptions. This gives more \"support\" for the conjecture that deep ReLU networks don't have bad local minima.\n* Deep linear networks don't have bad local minima, so if deep ReLU networks do have bad local minima, it's purely because of the introduction of nonlinear activations. This highlights the importance of the activation function used.\n* Shallow linear networks don't have bad saddles point while deep linear networks do, indicating that the saddle point problem is introduced with depth beyond the first hidden layer.\n\nBad saddle point\n: saddle point whose Hessian has no negative eigenvalues (no direction to descend)\n\nShallow neural network\n: single hidden layer\n\nDeep neural network\n: more than one hidden layer\n\nBad local minima\n: local minima that aren't global minima\n\n# Position in Research Landscape\n* Conjecture from 1989: For deep linear networks, every local minimum is a global minimum: [Neural networks and principal component analysis: Learning from examples without local minima (Neural networks 1989)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.408.1839&rep=rep1&type=pdf)\n  * This paper proves that conjecture.\n* Given 7 strong assumptions, the losses of local minima are concentrated in an exponentially (with dimension) tight band: [The Loss Surfaces of Multilayer Networks (AISTATS 2015)](https://arxiv.org/abs/1412.0233)\n* Discarding some of the above assumptions is an open problem: [Open Problem: The landscape of the loss surfaces of multilayer networks (COLT 2015)](http://proceedings.mlr.press/v40/Choromanska15.pdf)\n  * This paper discards 5 of those assumptions and proves the result for a strictly more general deep nonlinear model class.\n\n# More Details\n\n## Deep *Linear* Networks\n* Main result is Result 2, which proves the conjecture from 1989: every local minimum is a global minimum.\n* Not where the strong assumptions come in\n* Assumptions (realistic and practically easy to satisfy):\n  * $XX^T$ and $XY^T$ are full rank\n  * $d_y \\leq d_x$ (output is lower dimension than input)\n  * $\\Sigma = YX^T(XX^T )^{\u22121}XY^T$ has $d_y$ distinct eigenvalues \n  * specific to the squared error loss function\n* Essentially gives a comprehensive understanding of the loss surface of deep linear networks\n\n## Deep ReLU Networks\n* Specific to ReLU activation. Makes strong use of its properties\n* Choromanska et al. (2015) relate the loss function to the Hamiltonian of the spherical spin-glass model, using 3 reshaping assumptions. This allows them to apply existing random matrix theory results. This paper drops those reshaping assumptions by performing completely different analysis.\n* Because Choromanska et al. (2015) used random matrix theory, they analyzed a random Hessian, which means they need to make 2 distributional assumptions. This paper also drops those 2 assumptions and analyzes a deterministic Hessian.\n* Remaining Unrealistic Assumptions:\n  * The probability that a path through the ReLU network is active is the same, agnostic to which path it is.\n  * The activations of the network are independent of the input data and the weights.\n\n# Related Resources\n* [NIPS Oral Presentation](https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-without-Poor-Local-Minima)",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima"
    },
    "1176": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=Todorov:2002",
        "transcript": "# Introduction\nThis research examines the way that the brain's motor system carries out movement tasks by looking at the trajectory of human subjects' arms during (mostly) reaching and pistol-aiming movements. The authors' observation was that even though subjects were able to reliably to carry out these movements, the way in which they did so (i.e the trajectory of their arm) varied considerably between trials.\n\nPrevious models for motor coordination suggest that the brain strictly separates motor planning and motor execution. That is to say, the brain decides in advance, how to move its limbs in order to carry out a task and then follows that sequence to complete a movement. However this model doesn't make sense given that the motor system is still able to complete movements even in the presence of unforeseen perturbations.\n\nInstead, the authors propose a theory for motor coordination based on stochastic optimal feedback control. They suggest that motor coordination is implemented as a feedback control loop where both the motor signals and the sensory feedback are subject to noise and transmission delay. To complete a movement, the motor system comes up with an optimal feedback control law which iteratively calculates the motor outputs throughout a given task based on the instantaneous state of the system. The system defines this 'optimal control law' as being that which maximises task performance while minimising the total effort expended in carrying out the movement.\n\n# Method\nThe authors ran simulations for simple movement tasks. They used optimal control laws to drive the simulations and then compared the results with measurements taken from human subjects doing the same movement tasks. It was found that the simulations showed the same variability in their joint trajectories along task-irrelevant dimensions as the human subjects in the practical experiments. \n\n# Results\nThis research concluded that the control algorithm implemented by the motor system can be explained by the principle of minimal intervention defined in the optimal control framework. The principle of minimal intervention dictates that control effort should only expended on state dimensions that are relevant for completing the task at hand. This minimises the total control effort and avoids the possibility of degrading task performance by attempting to correct irrelevant errors with noisy control signals.",
        "sourceType": "blog",
        "linkToPaper": "?name=tom89"
    },
    "1177": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1510.09142",
        "transcript": "This paper shows how a family of reinforcement learning algorithms known as value gradient methods can be generalised to learn stochastic policies and deal with stochastic environment models.\n\nValue gradients are a type of policy gradient algorithm which represent a value function either by:\n* A learned Q-function (a critic)\n* Linking together a policy, an environment model and reward function to define a recursive function to simulate the trajectory and the total return from a given state.\n\nBy backpropagating though these functions, value gradient methods can calculate a policy gradient. This backpropagation sets them apart from other policy gradient methods (like REINFORCE for example) which are model-free and sample returns from the real environment.\n\nApplying value gradients to stochastic problems requires differentiating the stochastic bellman equation:\n\\begin{equation} \nV ^t (s) = \\int \\left[ r^t + \u03b3 \\int  V^{t+1} (s) p(s' | s, a) ds'  \\right] p(a|s; \u03b8) da\n\\end{equation}\n\nTo do that, the authors use a trick called re-parameterisation to express the stochastic bellman equation as a deterministic function which takes a noise variable as an input. To differentiate a re-parameterised function, one simply samples the noise variable then computes the derivative as if the function were deterministic. This can then be repeated $ M $ times and averaged to arrive at a Monte Carlo estimate for the derivative of the stochastic function.\n\nThe re-parameterised bellman equation is: \n\n$ V (s) = \\mathbb{E}_{ \\rho(\\eta) } \\left[ r(s, \\pi(s, \\eta; \\theta)) + \\gamma \\mathbb{E}_{\\rho(\\xi) }  \\left[ V' (f(s, \\pi(s, \\eta; \\theta), \\xi)) \\right]  \\right] $\n\nIt's derivative with respect to the current state and the policy parameters is:\n\n$ V_s = \\mathbb{E}_{\\rho(\\eta)} \\[ r_\\textbf{s} + r_\\textbf{a} \\pi_\\textbf{s} + \\gamma \\mathbb{E}_{\\rho(\\xi)} V'_{s'} (\\textbf{f}_\\textbf{s} + \\textbf{f}_\\textbf{a} \\pi_\\textbf{s}) \\] $\n\n$ V_\\theta = \\mathbb{E}_{\\rho(\\eta)} \\[ r_\\textbf{a} \\pi_\\theta + \\gamma \\mathbb{E}_{\\rho(\\xi)} \\[ V'_{\\textbf{s'}} \\textbf{f}_\\textbf{a} \\pi_\\textbf{s} + V'_\\theta\\] \\] $\n\nBased on these relationships the authors define two algorithms; SVG(\u221e), SVG(1) \n\n* SVG(\u221e) takes the trajectory from an entire episode and starting at the terminal state accumulates a gradients $V_{\\textbf{s}} $ and $ V_{\\theta} $ using the expressions above to arrive at a policy gradient. SVG(\u221e) is on-policy and only works with finite-horizon environments\n\n* SVG(1) trains a value function then uses its gradient as an estimate for $ V_{\\textbf{s}} $ above. SVG(1) also uses importance weighting so as to be off-policy and can work with infinite-horizon environments.\n\nBoth algorithms use an environment model which is trained using an experience replay database. The paper also introduces SVG(0) which is a similar to SVG(1), but is model-free.\n\nSVG was analysed using several MuJoCo environments and it was found that:\n* SVG(\u221e) outperformed a BBPT planner on a control problem with a stochastic model, indicating that gradient evaluation using real trajectories is more effective than planning for stochastic environments\n* SVG(1) is more robust to inaccurate environment models and value functions than SVG(\u221e)\n* SVG(1) was able to solve several complex environments\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1510.09142"
    },
    "1178": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.04216",
        "transcript": "Aim: generate realistic-looking synthetic data that can be used to train 3D Human Pose Estimation methods. Instead of rendering 3D models, they choose to combine parts of real images.\n\nInput: RGB images with 2D annotations + a query 3D pose.\nOutput: A synthetic image, stitched from patches of the images, so that it looks like a person in the query 3D pose.\n\nSteps:\n- Project 3D pose on random camera to get 2D coords\n- For each joint, find an image in the 2D annotated dataset whose annotation is locally similar\n- Based on the similarities, decide for each pixel which image is most relevant.\n- For each pixel, take the histogram of the chosen images in a neighborhood, and use this as blending factors to generate the result.\n\nThey also present a method that they trained on this synthetic dataset.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.04216"
    },
    "1179": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.00434",
        "transcript": "## Task\nThey introduce a dense version of the human pose estimation task: predict body surface coordinates for each pixel in an RGB image.\n\nBody surface is representated on two levels:\n- Body part label (24 parts)\n    - Head, torso, hands, feet, etc.\n    - Each leg split in 4 parts: upper/lower front/back. Same for arms.\n- 2 coordinates (u,v) within body part\n    - head, hands, feet: based on SMPL model\n    - others: determined by Multidimensional Scaling on geodesic distances\n\n## Data\n* They annotate COCO for this task\n    - annotation tool: draw mask, then click on a 3D rendering for each of up to 14 points sampled from the mask\n    - annotator accuracy on synthetic renderings (average geodesic distance)\n       - small parts (e.g. feet): ~2 cm\n       - large parts (e.g. torso): ~7 cm\n\n## Method\n\nFully-convolutional baseline\n  - ResNet-50/101\n  - 25-way body part classification head (cross-entropy loss)\n  - Regression head with 24*2 outputs per pixel (Huber loss)\n\nRegion-based approach\n  - Like Mask-RCNN\n  - New branch with same architecture as the keypoint branch\n  - ResNet-50-FPN (Feature Pyramid Net) backbone\n\nEnhancements tested:\n\n- Multi-task learning\n  - Train keypoint/mask and dense pose task at once\n  - Interaction implicit by sharing backbone net\n\n- Multi-task *cross-cascading*\n  - Explicit interaction of tasks\n  - Introduce second stage that depends on the first-stage-output of all tasks\n\n- Ground truth interpolation (distillation)\n  - Train a \"teacher\" FCN with the pointwise annotations\n  - Use its dense predictions as ground truth to train final net\n  - (To make the teacher as accurate as possible, they use ground-truth mask to remove background)\n\n## Results\n\n**Single-person results (train and test on single-person crops)**\n\nPointwise eval measure:\n   - Compute geodesic distance between prediction and ground truth at each annotated point\n   - For various error thresholds, plot percentage of points with lower error than the threshold\n   - Compute Area Under this Curve\n\nTraining (non-regional) FCN on new dataset vs. synthetic data improves AUC10 from 0.20 to 0.38\n\nThis paper's FCN method vs. model-fitting baseline\n- Baseline: Estimate body keypoint locations in 2D (usual \"pose estimation\" task) + fit 3D model\n- AUC10 improves from 0.23 to 0.43\n- Speed: 4-25 fps for FCN vs. model-fitting taking 1-3 minutes per frame (!).\n\n**Multi-person results**\n\n- Region-based method outperforms FCN baseline: 0.25 -> 0.32\n    - FCN cannot deal well with varying person scales (despite multi-scale testing)\n- Training on points vs interpolated ground-truth (distillation) 0.32 -> 0.38\n- AUC10 with cross-task cascade: 0.39\n\nAlso: Per-instance eval (\"Geodesic Point Similarity\" - GPS)\n   - Compute a Gaussian function on the geodesic distances\n   - Average it within each person instance (=> GPS)\n   - Compute precision and recall of persons for various thresholds of GPS\n   - Compute average precision and recall over thresholds\n\nComparison of multi-task approaches:\n1. Just dense pose branch (single-task) (AP 51)\n2. Adding keypoint (AP 53) OR mask branch (multi-task without cross-cascade) (AP 52)\n3. Refinement stage without cross-links (AP 52)\n4. Multi-task cross-cascade (keypoints: AP 56, masks: AP 53)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1802.00434"
    },
    "1180": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.05431",
        "transcript": "* Presents an architecture dubbed ResNeXt\n* They use modules built of\n    * 1x1 conv\n    * 3x3 group conv, keeping the depth constant. It's like a usual conv, but it's not fully connected along the depth axis, but only connected within groups\n    * 1x1 conv\n    * plus a skip connection coming from the module input\n\n* Advantages:\n    * Fewer parameters, since the full connections are only within the groups\n    * Allows more feature channels at the cost of more aggressive grouping\n    * Better performance when keeping the number of params constant\n\n* Questions/Disadvantages:\n    * Instead of keeping the num of params constant, how about aiming at constant memory consumption? Having more feature channels requires more RAM, even if the connections are sparser and hence there are fewer params\n    * Not so much improvement over ResNet",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.05431"
    },
    "1181": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/TarvainenV17",
        "transcript": "* Semi-supervised method\n* There is a teacher net and a student net, with identical architecture.\n* The teacher makes predictions on unlabeled data, which are used as ground-truth for training the student net.\n* After each gradient descent update on the student, the teacher's weights are updated so that it becomes an exponential moving average of the weights of the student at previous timesteps. It's called a \"mean teacher\" because of this moving average.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results"
    },
    "1182": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1712-04440",
        "transcript": "* It's a semi-supervised method (the goal is to make use of unlabeled data in addition to labeled data).\n* They first train a neural net normally, in the supervised way, on a labeled dataset.\n* Then **they retrain the net using *its own predictions* on the originally unlabeled data as if it was ground truth** (but only when the net is confident enough about the prediction).\n  * More precisely they retrain on the union of the original dataset and the examples labeled by the net itself. (Each minibatch is on average 60% original and 40% self-labeled)\n* When making these predictions (that will subsequently used for training), they use **multi-transform inference**.\n  * They apply the net to differently transformed versions of the image (mirroring, scaling), transform the outputs back accordingly and combine the results.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1712.04440"
    },
    "1183": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BojanowskiJLS17",
        "transcript": "\nAn algorithm named GLO is proposed in this paper. The objective function of GLO:\n\n$$\\min_{\\theta}\\frac{1}{N}\\sum_{i=1}^N\\[\\min_{z_i}l(g^\\theta(z_i),x_i)\\]$$\n\nThis idea dates back to [Dictionary Learning](https://en.wikipedia.org/wiki/Sparse_dictionary_learning). ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/81449a31e07ad388801379c804b73e6d1f044ce2)\nIt can be viewed as a nonlinear version of the dictionary learning by \n1. replace the dictionary $D$ with the function $g^{\\theta}$. \n2. replace $r$ with $z$.\n3. use $l_2$ loss function.\n\nAlthough in this way, the generator could be learned without the hassles caused by GAN objective, there could be problems. \n\nWith this method, the space of the latent vector $z$ could be structured. Although the author project $z$ to a unit ball if it falls outside, there is no guarantee that the trained $z$ would remain a Gaussian distribution that it was originally initialized from. \n\nThis could cause the problem that not every sampled noise could reconstruct a valid image, and the linear interpolation could be problematic if the support of the marginalized $p(z)$ is not a convex set.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1707.05776"
    },
    "1184": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/BerantCFL13",
        "transcript": "* traditional semantic parsers get around need for annotated logical forms by increasing number of logical predicates\n* semantic parser must combine predicates into coherent logical form\n* parser define few simple composition rules which over-generate and use model features to simulate soft rules and categories\n* use POS tag features and features on denotations of predicted logical forms\n* database is queried using logical language: lambda-dependency-based compositional semantics\n* given utterance, semantic parser constructs a distribution over possible derivations, each derivation specifying application of a set of rules that culminates in the logical form at root of tree\n* derivations constructed recursively by mapping natural language phrases to knowledge base predicates and small set of composition rules\n* to produce manageable set of predicates per utterance, construct a lexicon that maps natural language phrases to logical predicates by aligning large text corpus to Freebase; also generate logical predicates compatible with neighboring predicates using bridging operation\n",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology//D/D13/D13-1160.pdf"
    },
    "1185": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1146/annurev-linguist-030514-125312",
        "transcript": "* logical approaches rely on techniques from proof theory and model-theoretic semantics; primarily concerned with inference, ambiguity, vagueness, and compositional interpretation\n* statistical approaches derive their tools from algorithms and optimization and tend to focus on word meanings, vector space models, and other broad notions of semantic content\n* principle of compositionality: meaning of complex syntactic phrase is function of meanings of its constituent phrases\n* heart of grammar is its lexicon\n* generate a grammar exponential in the length of the sentence; dynamic programming can mitigate problems for parsing\n* learning via denotations in general results in increased computational complexity\n* learning from denotations offers advantage of being able to define features on denotations\n* semantic representations can also be distributed representations (rather than logical forms)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1146/annurev-linguist-030514-125312"
    },
    "1186": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/LiangJK11",
        "transcript": "* Supervised semantic parsers\n* First must map questiosn into logical forms and this requires data with manually labeled semantic forms\n* all we really care about is resulting denotation for a given input, so are free to choose how we represent logical forms\n* introduce new semantic representation: dependency-based compositional semantics\n* represent logical forms as DCS trees where nodes represent predicates (State, Country, Genus, ...) and edges represent relations \n* such a form allows for a transparency between syntactics and semantics and hence a streamlined framework for program induction\n* denotation at root node\n* trees mirror syntactic dependency structure, facilitating parsing but also enable efficient computation of denotations defined on a given tree\n* to handle divergence between syntactic and semantic scope in some more complicated expressions, mark nodes low in tree with *mark* relation (E, Q, or C) and then invoke it higher up with *execute* relation to create desired semantic scope\n* discriminative semantic parsing model placing a log-linear distribution over the set of permissible DCS trees given an utterance",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aclweb.org/anthology/P11-1060"
    },
    "1187": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/AngeliM14",
        "transcript": "Seeks to tackle database completion using Maccartney's natural logic. Approach does not require explicit alignment between premise and query and allows imprecise inferences at an associated cost learned from data. Casts transformation from query to supporting premise as a unified search problem where each step may have associated with it a cost reflecting confidence in the step. System allows for unstructured text as input to database, without a need to specify a schema or domain of text.\n  \n## Summary\nRepresent Maccartney's inference model as a finite state machine that can be collapsed into three states. Represent acquisition of new premises in knowledge base as search over FSA, with nodes representing candidate facts and edges as mutations of these facts with associated costs.  The confidence is a path is then computed using the cost vector and associated feature vector (representing the distance between the endpoints of two transitions). Model was tested on FraCaS entailment corpus as well as a corpus of OpenIE extractions.\n\n## Future Work\nSearch process does not have full access to the parse tree so struggles with issues of alignment. ",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/D/D14/D14-1059.pdf"
    },
    "1188": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=Dagan:2005",
        "transcript": "Paper proposes an abstract generic task to frame the textual entailment problem. \n## Summary\nGenerated a dataset of text snippets from general news domain, annotated by humans with entailment properties. Annotators generated hypotheses for certain text corpora by converting questions and text phrases across various domains including QA, information extraction, reading comprehension, machine translation, and paraphrase acquisition. Sixteen submissions made to the challenge encompassing a wide variety of entailment inference systems. Basic kinds of features for the system include stemming, lemmatization, POS tagging, and some sort of statistical weighting. Other features included making use of higher-level lexical relationships via Wordnet or evaluating distance between syntactic structures of hypothesis and premise.\n## Future Work\nWished to improve the challenge by dealing with multi-valued annotation, relaxing assumptions on assumed background knowledge, providing entailment subtasks, and offering a wider variety of inference scope.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.cs.biu.ac.il/~glikmao/rte05/"
    },
    "1189": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1406.1827",
        "transcript": "The paper wished to see whether distributed word representations could be used in a machine learning setting to achieve good performance in the task of natural language inference (also called recognizing textual entailment). \n\n## Summary\nPaper investigates the use of two neural architectures for identifying the entailment and contradiction logical relationships. In particular, the paper uses tree-based recursive neural networks and tree-based recursive neural tensor networks relying on the notion of compositionality to codify natural language word order and semantic meaning. The models are first tested on a reduced artificial data set organized around a small boolean structure world model where the models are tasked with learning the propositional relationships. Afterwards these models are tested on another more complex artificial data set where simple propositions are converted into more complex formulas. Both models achieved solid performance on these datasets although in the latter data set the RNTN seemed to struggle when tested on larger-length expressions. The models were also tested on an artificial dataset where they were tasked with learning how to correctly interpret various quantifiers and negation in the context of natural logics. Finally, the models were tested on a freely available textual entailment dataset called SICK (that was supplemented with data from the Denotation Graph project). The models achieved reasonably good performance on the SICK challenge, showing that they have the potential to accurately learn distributed representations on noisy real-world data. \n    \n## Future Work\nThe neural models proposed seem to show particular promise for achieving good performance on very natural language logical semantics tasks. It is firmly believed that given enough data, the neural architectures proposed have the potential to perform even better on the proposed task. This makes acquisition of a more comprehensive and diverse dataset a natural next step in pursuing this modeling approach. Further even the more powerful RNTN seems to show rapidly declining performance on larger expressions which leaves the question of whether stronger models or learning techniques can be used to improve performance on considerably-sized expressions. In addition, there is still the question as to how these architectures actually encode the natural logics they are being asked to learn.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1406.1827"
    },
    "1190": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/emnlp/FitzGeraldAZ13",
        "transcript": "  * Efficiently find fraction of referring expressions for scenes that are used; estimate associated likelihoods\n  * Learn probability distribution over set of logical expressions that select a target set of objects in a world state\n  * Model as globally normalized log-linear model using features of logical form *z*\n  * Distinction for plural entities in generated logical forms\n  * globally optimized log-linear model, conditioned on state S and set of target objects G\n  * Three kinds of features: logical expression structure features, situated features, and a complexity feature\n  * learning two models--one for a global logical form for world-state model; and one learning a series of classifiers for each \n  * learn codebooks and associated sparse codes",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology//D/D13/D13-1197.pdf"
    },
    "1191": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1206.6423",
        "transcript": "* Task of extracting representations of language tied to physical world\n* New grounded concepts from a set of scenes containing only sentences, images, and indications of what objects being referred to\n* System includes: \n  * *Semantic parsing model* \n    * Defines distribution over logical meaning representations for each given sentence\n  * Set of visual attribute classifiers for each possible object in scene\n  * Joint model learning mapping from logical constants in logical form to set of visual attribute classifiers\n  * Extracted depth and RGB values from images as features (shape and color attributes)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1206.6423"
    },
    "1192": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/aaai/EvansSG16",
        "transcript": "### Main ideas\n\n**Key problem:** To infer our preferences, even though our behavior may systematically diverge from them. Examples: a person who smokes event though they prefer not to (but are unable to quit) or somebody who would like to eat healthily, but regularly succumbs to temptation of donuts (which they consider unhealthy).\n\n**Proposed solution:** Modeling human biases directly into reasoning about given agent's behaviour. In the proposed solution [hyperbolic discounting](https://en.wikipedia.org/wiki/Hyperbolic_discounting) is used to account for our time inconsistency.\n\n### Details\n\nImagine a grid-world in which an agent moves around the grid to find a place to eat. https://i.imgur.com/dxL8fA1.png\n\nAn agent is a tuple: $(p(s), U, Y, k, \\alpha)$, where:\n* $s\\in S$ is a state of the world, it is not described in detail in the paper, but, among others, it consists of things like: noodle place is open, vegetarian place is closed.\n* $p(s)$ is agent's belief about which state of the world it is in - it is modeled as a probability distribution over states.\n* $U$ is agent's (deterministic) utility function - this is the thing we would like to learn the most by observing agent's actions - $U: S \\times A \\rightarrow \\mathbb{R}$, we assign utilities to actions, $a\\in A$, in world states $s$.\n* Agent chooses actions stochastically where probability ($C(a;s)$) is proportional to their exponentiated expected utility: $C(a;s) \\propto \\exp^{\\alpha EU_{s}[a]}$ or for discounting agents: $C(a;s) \\propto \\exp^{\\alpha EU_{s,d}[a]}$, where $\\alpha$ is noise parameter (the lower it is the more randomly agent behaves). Expected utility is described below.\n* $Y$ is a variable that denotes the kind of agent:\n    * not discounting agent - as its name suggests it won't discount the utility of future actions regardless of the delay, so its expected utility is: $EU_s[a] = U(s,a) + \\mathbb{E}_{s',a'}[EU_s'[a']]$, where $s'$ is a state in which agent ends after choosing action $a$ from state $s$ and $a'$ is action choosen in $s'$,\n    * discounting naive agent - it discounts utility of future actions based on the delay $d$: $EU_{s,d}[a] =\\frac{1}{1+kd} U(s,a) + \\mathbb{E}_{s',a'}[EU_{s',d+1}[a']]$, where $k$ is a discount rate (part of agent's description (see tuple above)). \n    Because of the discounting the utility of actions changes with time. It is possible then that an agent who decided to go to vegetarian cafe will change its decision once it is next to donut store. This is shown on the left in the image above. If the agent wanted to go to donut place it could've gone to the closer one. That is why it is called the naive agent - it doesn't take into account that the utility of its action changes and ends up doing things that it didn't plan.    \n    * discounting sophisticated agent - its expected utility is also discounted like in naive case, but it chooses future actions $a'$ as if the delay $d$ was $0$. In a sense, it knows that its future self will look at the immediate utility of actions rather then at the utilities they have now. Thanks to that it can for example choose a different path to vegetarian restaurant. It knows that it future self would end up in donut place if it went next to it.\n* $k$ is the discount rate (see dicounting naive agent description).\n* $\\alpha$ is the noise parameter described above together with actions.\n\nGiven a sequence of actions done by some agent, we want to infer its preferences. In the paper this is translated into: given a sequence of actions done by some agent update your probability distribution over agent tuples. We start with uniform distribution (zero knowledge) and do bayesian updates with consecutive actions. The model described above will be considered good if probability mass after the updates concentrates on the kinds of agents that a human would infer after seeing the same.\n\n### Results\n\nAccording to experiments the model performs well, which means that it assigns high probabilities to the kind of agents that humans describe after seeing the same actions. \n\nFor example, after seeing actions from the image above the model as well as human subjects would rate highly explanations of giving in to temptation (in case of naive planner) or avoiding temptation (in case of sophisticated one). The result holds for more complex scenarios: \n* inference with uncertainty - agent might have inaccurate beliefs, for example it might 'think' that the noodle place is open when in fact it's closed, \n* inference from multiple episodes - even though in two out of three cases agent chooses donuts both human subjects and the model assign high probability to the case where the vegetarian place is preferred (among others, they generally agree over variety of explanations).\n\n**Conclusion:** If we want to be able to delegate some of our decisions to ai systems then it is necessary that they are able to learn our preferences despite inconsistencies in our behaviour. The result presented in the paper shows that modeling our biases directly is a feasible direction of research.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12476"
    },
    "1193": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/ijcai/StrubVMPCP17",
        "transcript": "The authors proposed a end-to-end way to learn how to play a game, which involves both images and text, called GuessWhat?!. They use both supervised learning as a baseline and reinforcement learning to improve their results.\n\n**GuessWhat Rules :**\n\n*From the paper :* \"GuessWhat?! is a cooperative two-player game in which both players see the picture of a rich visual scene with several objects. One player \u2013 the oracle \u2013 is randomly assigned an object (which could be a person) in the scene. This object is not known by the other player \u2013 the questioner \u2013 whose goal is to locate the hidden object. To do so, the questioner can ask a series of yes-no questions which are answered by the oracle\"\n\n**Why do they use reinforcement learning in a dialogue context ?**\n\nSupervised learning in a dialogue system usually brings poor results because the agent only learns to say the exact same sentences that are in the training set. Reinforcement learning seems to be a better option since it doesn't try to exactly match the sentences, but allow more flexibility as long as you get a positive reward at the end. The problem is : In a dialogue context, how can you tell that the dialogue was either \"good\" (positive reward) or \"bad\" (negative reward). In the context of the GuessWhat?! game, the reward is easy. If the guesser can find the object that the oracle was assigned to, then it gets a positive reward, otherwise it gets a negative reward.\n\nThe dataset is composed of 150k human-human dialogues.\n\n**Models used** \n\n*Oracle model* : Its goal is to answer by 'yes' or 'no' to the question asked by the agent.\nThey are concatenating :\n- LSTM encoded information of the question asked\n- Information about the location of the object (coordinate of the bounding box)\n- The object category\n\nThen the vector is fed to a single hidden layer MLP\nhttps://i.imgur.com/SjWkciI.png\n\n*Question model* : The questionner is split in two models :\n- The question generation :\n    - **Input** : History of questions already asked (if questions were asked before) and the beginning of the question (if this is not the first word of the question) \n    - **Model** : LSTM with softmax\n    - **Output** : The next word in the sentence\n- The guesser\n    - **Input** : The image + all the questions + all the answers\n    - **Model** : MLP + softmax\n    - **Output** : Selection of one object among the set of all objects in the image.\n\n**Training procedure** :\n\nTrain all the components above, in a supervised way.\nOnce the training is done, you have a dialogue system that is good enough to play on it's own, but the question model is still pretty bad. To improve it, you can train it using REINFORCE Algorithm, the reward being positive if the question model guessed the good object, negative otherwise.\n\n**Main Results :**\n\nThe results are given on both new objects (images have been already seen, but the objected selected had never been selected during training) and new images.\nThe results are in % of the human score, not in absolute accuracy (100% means human-level performance).\n\n|                       | New objects | New images |\n|-----------------------|-------------|------------|\n| Baseline (Supervised) | 53.4%        | 53%         |\n| Reinforce             | 63.2%        | 62%         |\n\nWe can improvement using the REINFORCE algorithm. This is mainly because supervised algorithm doesn't know when to stop asking questions and give an answer. On the other hand REINFORCE is more accurate but tends to stop too early (and giving wrong answers)\n\nOne last thing to point out regarding the database : The language learned by the agent is still pretty bad, the question are mostly \"Is it ... ?\" and since the oracle only answers yes/no questions, the interaction is relatively poor.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.24963/ijcai.2017/385"
    },
    "1194": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1602.02644",
        "transcript": "This paper proposed a class of loss functions applicable to image generation that are based on distance in feature spaces:\n\n$$\\mathcal{L} = \\lambda_{feat}\\mathcal{L}_{feat} + \\lambda_{adv}\\mathcal{L}_{adv} + \\lambda_{img}\\mathcal{L}_{img}$$\n\n### Key Points\n- Using only l2 loss in image space yields over-smoothed results since it leads to averaging all likely locations of details.\n- L_feat measures the distance in suitable feature space and therefore preserves distribution of fine details instead of exact locations.\n- Using only L_feat yields bad results since feature representations are contractive. Many non-natural images also mapped to the same feature vector.\n- By introducing a natural image prior - GAN, we can make sure that samples lie on the natural image manifold.\n\n### Model\n\nhttps://i.imgur.com/qNzMwQ6.png\n\n### Exp\n- Training Autoencoder\n- Generate images using VAE\n- Invert feature\n\n### Thought\nI think the experiment section is a little complicated to comprehend. However, the proposed loss seems really promising and can be applied to many tasks related to image generation.\n\n### Questions\n- Section 4.2 & 4.3 are hard to follow for me, need to pay more attention in the future",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1602.02644"
    },
    "1195": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/NguyenDYBC16",
        "transcript": "\nThis paper performs activation maximization (AM) using Deep Generator Network (DGN), which served as a learned natural iamge prior, to synthesize realistic images as inputs and feed it into the DNN we want to understand.\nBy visualizing synthesized images that highly activate particular neurons in the DNN, we can interpret what each of neurons in the DNN learned to detect.\n\n### Key Points\n\n- DGN (natural image prior) generates more coherent images when optimizing fully-connected layer codes instead of low-level codes. However, previous studies showed that low-level features results in better reconstructions beacuse it contains more image details. The difference is that here DGN-AM is trying to synthesize an entire layer code from scratch. Features in low-level only has a small, local receptive field so that the optimization process has to independently tune image without knowing the global structure. Also, the code space at a convolutional layer is much more high-dimensional, making it harder to optimize.\n\n- The learned prior trained on ImageNet can also generalize to Places.\n- It doesn't generalize well if architecture of the encoder trained with DGN is different with the DNN we wish to inspect.\n- The learned prior also generalizes to visualize hidden neurons, producing more realistic textures/colors.\n- When visualizing hidden neurons, DGN-AM trained on ImageNet also generalize to Places and produce similar results as [1].\n- The synthesized images are showed to teach us what neurons in DNN we wish to inspect prefer instead of what prior prefer.\n\n### Model\n\n![](https://cloud.githubusercontent.com/assets/7057863/21002626/b094d7ae-bd61-11e6-8c95-fd4931648426.png)\n\n### Thought\nSolid paper with diverse visualizations and thorough analysis.\n\n### Reference\n[1] Object Detectors Emerge In Deep Scene CNNs, B.Zhou et. al.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6519-synthesizing-the-preferred-inputs-for-neurons-in-neural-networks-via-deep-generator-networks"
    },
    "1196": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/AytarVT16",
        "transcript": "This paper developed a semantically rich representation for natural sound using unlabeled videos as a bridge to \ntransfer discriminative visual knowledge from well-established visual recognition models into the sound modality.\nThe learned sound representation yields significant performance improvements on standard benchmarks for acoustic \nscene classification task.\n\n### Key Points\n\n- The natural synchronization between vision and sound can be leveraged as a supervision signal for each other.\n- Cross-modal learning can overcome overfitting if the target modal have much fewer data than other modals, which is essential for deep networks to work well.\n- In the sound classification task, **pool5** and **conv6** extracted from SoundNet achieve best performance.\n\n### Model\n- The authors proposed a student-teacher training procedure to transfer discriminative visual knowledge from visual recognition models \ntrained on ImageNet and Places into the SoundNet by minimizing KL divergence between their predictions.\n![](https://cloud.githubusercontent.com/assets/7057863/20856609/05fe12d6-b94e-11e6-8c92-995ee84fe0d7.png)\n- Two reasons to use CNN for sound: 1. invariant to translations; 2. stacking layers to detect higher-level concepts.\n\n\n### Exp\n\n- Adding a linear SVM upon representation learned from SoundNet outperforms other existing methods 10%.\n- Using lots of unlabeled videos as supervision signals enable the deeper SoundNet to work, or otherwise the 8-layer networks \nperforms poorly due to overfitting.\n- Simultaneous Using Places and ImageNet as supervision beats using only one of them 3%.\n- Multi-modal recognition models use visual and sound data together yields 2% gain in classification accuracy.\n\n### Thought\nI think this paper is really complete since it contains good intuition, ablation analysis, representation visualization, hidden unit visualization, and significent performance imporvements.\n\n### Questions\n- Although paper said that \"To handle variable-temporal-length of input sound, this model uses a fully convolutional network and produces an output over multiple timesteps in video.\", but the code seems to set the length of each excerpts fixed to 5 seconds.\n- It looks not clear for me about the data augmentation technique used in training.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6146-soundnet-learning-sound-representations-from-unlabeled-video"
    },
    "1197": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/FinnAL17",
        "transcript": "The authors propose an algorithm for meta-learning that is compatible with any model trained with gradient descent, and show that it works on various domain including supervised learning and reinforcement learning. This is done by explicitly train the network such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task.\n\n### Key Points\n\n- MAML is actually finding a good **initialization** of model parameters for several tasks.\n- Good initialization of parameters means that it can achieve good performance on several tasks with small number of gradient steps.\n\n### Method\n- Simultaneously optimize the **initialization** of model parameters of different meta-training tasks, hoping that it can quickly adapt to new meta-testing tasks.\n\n![](https://cloud.githubusercontent.com/assets/7057863/25161911/46f2721e-24f1-11e7-9fba-8bc2f0782204.png)\n\n- Training procedure:\n\n![](https://cloud.githubusercontent.com/assets/7057863/25161749/8d00902a-24f0-11e7-93a8-6a9b74386f55.png)\n\n\n\n### Exp\n\n- It acheived performance that is comparable to the state-of-the-art on classification/regression/reinforcement learning tasks.\n\n### Thought\nI think the experiments are thorough since they proved that this technique can be applied to both supervised and reinforcement learning. However, the method is not novel provided that [Optimization a A Midel For Few-shot Learning](https://openreview.net/pdf?id=rJY0-Kcll) already proposed to learn initialization of parameters.",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v70/finn17a.html"
    },
    "1198": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/GregorDGRW15",
        "transcript": "#### Problem addressed: \nGenerate images with recurrent neural networks\n\n#### Summary: \nThis paper propose an architecture for image generation. The model itself is similar to variational autoencoder, but both the encoder and decoder are implemented with recurrent neural networks, in particular LSTM. It also has two new components: \n\n1. a reader that select an area of interest for the next recurrence and \n\n2. a writer that write to that particular area. They believe this mimics the attention and demonstrated this on a cluttered mnist dataset.\n\n#### Novelty:\nUsing RNNs for image generation and selective attention of the region of interest\n\n#### Drawbacks:\nIdea seems over complicated and the image generation performance is not that good on real image datset such as CIFAR.\n\n#### Datasets:\nMNIST, SVHN, CIFAR\n\n#### Additional remarks:\nPresentation video available on cedar server\n\n#### Presenter:\nYingbo Zhou",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v37/gregor15.html"
    },
    "1199": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DentonCSF15",
        "transcript": "#### Problem addressed: \nLearn to generate sharp high resolution images\n\n#### Summary: \nThe authors applied generative adversarial networks (GANs) for image modeling. Instead of learning one model for a high resolution image directly, they learn it in a hierachical way using pyramid decomposition of the image. The image is decomposed to smaller versions that contains all low frequency information and its corresponding high frequency ones. It can also be regarded as a similar idea from lossless compression, where one have both compressed version of the image and the corresponding error so that once the compression is given the image can be reconstructed perfectly. The quantitative and qualitaive performance are impressive, and is so that the best image generative model for high resolution images.\n\n#### Novelty:\nUsing laplacian pyramid decomposition that enables the generation of sharp high resolution images possible\n\n#### Drawbacks:\nTraining is stagewise and slow, and levels of decomposition is fixed for all kinds of images.\n\n#### Datasets:\nCIFAR-10, STL, SUN\n\n#### Additional remarks:\nPresentation video available on cedar server\n\n#### Resources:\nOther relevant work to this one: Conditional Generative Adversarial Nets, Conditional generative adversarial nets for convolutional face generation\n\n#### Presenter:\nYingbo Zhou",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks"
    },
    "1200": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/GoodfellowPMXWOCB14",
        "transcript": "#### Problem addressed: \nlearning data distribution using non-parametric models\n\n#### Summary: \nThe authors propose learning generative model as playing an adversarial game, where one player is the generative model and the other is a discriminative model. The discriminative model learns to differentiate samples obtained from the generative model with true samples from the dataset, and the generative model tries to improve its data likelihood. They provide theoretical result that when one use such adversarial objective the global optimum of generative model will be the data distribution. Good generative performance is presented both quantitatively and qualitatively.\n\n#### Novelty:\nTreating learning generative model as playing a two player adversarial game, although such idea is kind of there in the training of energy based models, this paper is the first to successfully demonstrate this power.\n\n#### Drawbacks:\nAs with any of the non-parametric generative models, the evaluation of data probability is not very easy.\n\n#### Datasets:\nMNIST, TFD, CIFAR-10 (only qualitative)\n\n#### Additional remarks:\nPresentation video available on cedar server\n\n#### Presenter:\nYingbo Zhou",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5423-generative-adversarial-nets"
    },
    "1201": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/0001JRVSDHT15",
        "transcript": "#### Problem addressed: \nImage Segmentation, Pixel labelling, Object recognition\n\n#### Summary: \nThe authors approximate the CRF inference procedure using the mean field approximation. They use a specific set of unary and binary potentials. Each step in the mean field inference is modelled as a convolutional layer with appropriate filter sizes and channels. The mean field inference procedure requires multiple iterations (over time) to achieve convergence. This is exploited to model the whole procedure as CNN-RNN. The unary potentials and initial pixel labels are learnt using a FCN. The authors train the FCN and CNN-RNN separately and jointly and find that joint training gives the better performance of the two on the VOC2007 dataset.\n\n#### Novelty:\nFormulating the mean field CRF inference procedure as a combination of CNN and RNN. Joint training procedure of a fully convolutional network (FCN) + CRF as RNN to perform pixel labelling tasks\n\n#### Drawbacks:\nDoes not scale with number of classes. No theoretical justification for success of joint training, only empirical justification\n\n#### Datasets:\nVOC2012, COCO\n\n#### Additional remarks:\nPresentation video available on cedar server\n\n#### Resources:\nhttp://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf\n\n#### Presenter:\nBhargava U. Kota",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.179"
    },
    "1202": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/WangZSZ14",
        "transcript": "#### Problem addressed: \nDenoising images, Recognition in cluttered images, Segmentation of objects of interest\n\n#### Summary: \nThe authors propose an architecture to perform image denoising and segmentation for MNIST variation datasets and MNIST-2. Their work involves producing a 'cognitive' bias which is like a prior assumption that a certain class is present in the input image. Their network generates a denoised image given the prior class distribution at each iteration. A regular classifier is used at the end of generation process for termination condition. The generated image is gated with the input to prevent hallucination due to cognitive bias. MNIST-2 is created by the authors by superimposing 2 digits on the same image.\n\n#### Novelty:\nIntegrates cognitive bias and feature extraction in the same network\n\n#### Drawbacks:\nDoes not scale with number of classes. Works only on binarized images due to gating and masking\n\n#### Datasets:\nMNIST variations, MNIST-2\n\n#### Resources:\nhttp://papers.nips.cc/paper/5268-attentional-neural-network-feature-selection-using-cognitive-feedback.pdf\n\n#### Presenter:\nBhargava U. Kota",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5268-attentional-neural-network-feature-selection-using-cognitive-feedback"
    },
    "1203": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/SunWT13",
        "transcript": "#### Problem addressed: \nEstimation of the position of facial keypoints\n\n#### Summary: \nThe authors propose an effective convolutional network cascade for facial point detection. Deep convolutional networks at the first level provide highly robust initial estimations, while shallower convolutional networks at the following two levels finely tune the initial prediction to achieve high accuracy. The method therefore can avoid local minimum caused by ambiguity and data corruption. Networks at the first level are deep convolutional networks with four convolutional stages, absolute value rectification, and locally shared weights. Networks at the second and third levels share a common shallower structure. Since they are designed to extract local features, deep structures and locally sharing weights are unnecessary.\n\n#### Novelty:\nThree-level carefully designed convolution networks\n\n#### Drawbacks:\nHigh computational cost because of deep model\n\n#### Datasets:\nBioID, LFPW\n\n#### Resources:\nhttp://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr13.pdf\n\n#### Presenter:\nNeeti Narayan",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2013.446"
    },
    "1204": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1502.03167",
        "transcript": "#### Problem addressed: \nStrategy for training deep neural networks\n\n#### Summary: \nThe input distribution (to every layer) undergoes constant changes while training a deep network. The authors call this internal covariate shift in the input distribution. The authors claim this leads to slow learning of optimal model parameters. In order to overcome this, they introduce the idea of normalizing the input of every layer a part of the optimization strategy. Specifically, they reparameterize the input to each layer so that it is whitened and thus has non-changing distribution at every iteration.\n \nThey apply 2 approximation in their strategy: \n\n1. this normalization is done for every mini-batch of training data, \n\n2. the input dimensions are assumed to be uncorrelated. \n\nFinally, the output of last layer is mean subtracted and variance normalized (these can be back-propagated while training). Additionally, the authors also introduce 2 learnable scalar parameters $(r,b)$ per dimension such that the final input to a layer is $y=rg(BN(x))+b$ where g is the activation function.\n \n The advantage of BN apart from the intuition mentioned above is that it allows higher learning rate and network behavior remains unaffected by the scale of the parameters W and bias. The authors also empirically show that BN acts as a regularizer since optimization without dropout yields at par performance.\n\n#### Novelty:\nPrevious work only focused on whitening in 1st layer input. This work extends this idea to all layers and suggests a practical approach for applying this idea to real world data.\n\n#### Datasets:\nImagenet\n\n#### Resources:\npresentation video available on cedar server\n\n#### Presenter:\nDevansh Arpit",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1502.03167"
    },
    "1205": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1311.1780",
        "transcript": "#### Problem addressed: \nA new type of activation function\n\n#### Summary: \nThis paper propose a new activation function that computes a Lp norm from multiple projections on an input vector. The p value can be learned from training example, and can also be different for each hidden unit. The intuition is that 1) for different datasets there may exist different optimal p-values, so it make more sense to make p tunable; 2) allowing different unit take different p-values can potentially make the approximation of decision boundaries more efficient and more flexible. The empirical results support these two intuitions, and achieved comparable results on three datasets.\n\n#### Novelty:\nA generalization of pooling but applied through channels, when the data and weight vector dot product plus bias is constrained to non-negative case, the $L_\\infty$ is equivalent to maxout unit.\n\n#### Drawbacks:\nEmpirical performance is not very impressive, although evidence of supporting the intuition occurs.\n\n#### Datasets:\nMNIST, TFD, Pentomino\n\n#### Resources:\nhttp://arxiv.org/abs/1311.1780\n\n#### Presenter:\nYingbo Zhou",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1311.1780"
    },
    "1206": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GalG15",
        "transcript": "#### Problem addressed: \nBayesian approximation of neural netowrks\n\n#### Summary: \nThis paper gives an alternative view of dropout as Bayesian approximation, which allow one to obtain uncertainty from the predictions. The result is surprisingly simple, both the predictive mean and variance can be obtained by calculating the mean and variance (with some minor adjustment) of multiple passes through the network with dropout.\n\n#### Novelty:\nA new interpretation of dropout as a Bayesian approximation.\n\n#### Drawbacks:\nSome computational overhead, since calculating the predictive mean and variance need multiple passes through the network.\n\n#### Datasets:\nMNIST, solar irradiance, Maunua Loa Co2,\n\n#### Resources:\nPaper: http://arxiv.org/pdf/1506.02142v1.pdf\n\nBlog post: http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html\n\n#### Presenter:\nYingbo Zhou\"",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1506.02142"
    },
    "1207": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/JaderbergSZK15",
        "transcript": "#### Problem addressed: \nA module to spatially transform feature maps conditioned on feature maps itself. Attempts to improve rotation, scale and shift invariance in neural nets.\n\n#### Summary: \nThis paper introduces Spatial Transformer Networks (STN) for rotation, shift and scale invariance. The module consists of three parts - Localization function, Grid point generation and Sampling. Each of these modules are differentiable and can be inserted at any point in a standard neural network architecture. The constraints are that the learnt spatial transform must be parametrized. Localization function learns these parameters by looking at the previous layer output (typically HxWxC for convolutional layers) and regressing to the parameters using FC layers or convolutional layers. The source grid generator parameters are learnt the same way. Given these two, the output of the STN is constructed by sampling (using any differentiable kernel) the source grid and using the transform parameters.\n\n#### Novelty:\nA new module is introduced to increase invariance to rotation, scale and shift\n\n#### Drawbacks:\nSince only some points in the source feature maps are selected due to grid generation it is unclear how the error is backpropagated to previous layers\n\n#### Datasets:\nDistorted MNIST, CUB-200-2011 Birds, SVHN\n\n#### Resources:\nhttp://arxiv.org/pdf/1506.02025v1.pdf\n\n#### Presenter:\nBhargava U. Kota",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5854-spatial-transformer-networks"
    },
    "1208": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1412.6572",
        "transcript": "#### Problem addressed: \nA fast way of finding adversarial examples, and a hypothesis for the adversarial examples\n\n#### Summary: \nThis paper tries to explain why adversarial examples exists, the adversarial example is defined in another paper \\cite{arxiv.org/abs/1312.6199}. The adversarial example is kind of counter intuitive because they normally are visually indistinguishable from the original example, but leads to very different predictions for the classifier. For example, let sample $x$ be associated with the true class $t$. A classifier (in particular a well trained dnn) can correctly predict $x$ with high confidence, but with a small perturbation $r$, the same network will predict $x+r$ to a different incorrect class also with high confidence.\n \n This paper explains that the exsistence of such adversarial examples is more because of low model capacity in high dimensional spaces rather than overfitting, and got some empirical support on that. It also shows a new method that can reliably generate adversarial examples really fast using `fast sign' method. Basically, one can generate an adversarial example by taking a small step toward the sign direction of the objective. They also showed that training along with adversarial examples helps the classifier to generalize.\n\n#### Novelty:\nA fast method to generate adversarial examples reliably, and a linear hypothesis for those examples.\n\n#### Datasets:\nMNIST\n\n#### Resources:\nTalk of the paper https://www.youtube.com/watch?v=Pq4A2mPCB0Y\n\n#### Presenter:\nYingbo Zhou",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1412.6572"
    },
    "1209": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jmlr/GlorotB10",
        "transcript": "#### Problem addressed: \nInstead of approaching the problem why pre-training works, this paper addresses why traditional way of training deep NNs dont work.\n\n#### Summary: \nThe main focus of this paper is to empirically study why deep nets dont work with backprop without any pre-training. To analyse this, the authors mainly study the trends of activations and gradient strength across layers vs training iteration using simple backprop. Their study shows that the higher layer units saturate to 0 in the case of Sigmoid which prevents any backpropagated gradients to lower layers. It takes a lot of iterations to get out of saturation after which the lower layers start to learn.\n \n For this reason the authors suggest using activations symmetric around 0 to avoid saturation, like Tanh and Softsign. For Tanh, they find that units of every layer initialized on either part of 0 start saturating (to respective sides) one after the other starting from lower layer to higher layer. For Softsign on the other hand, units from all layers move towards saturation together. Further the histogram of final activations suggest that Tanh units have a peak at both 0 and -1,+1 saturation, while Softsign units generally lie in the linear region. Note that the linear region in Tanh/Softsign has activation gradients-- hence propagates information.\n \n The most interesting part of this study is the way the authors analyse the flow of information from the input layer to the top layer and vice versa. While the forward prop transmits the information about input to higher layers, backward prop transmits the error gradient. They measure the flow of information in terms of the variance of activation (forward) and gradients (backwards) for different layers. Since we would want the information flow to be equal at all layers, the variance should also be the same. So they propose to initialize the weight vectors such that this variance is preserved across layers. They call this \"\"Normalized Initialization\"\". Their empirical results show that both activations and gradients (hence information) at all layers have better propagation with their initialization.\n\n#### Novelty:\nAnalysis of activation values and back-prop gradient across layers for analyzing training difficulties. Also, a new weight initialization method.\n\n#### Drawbacks:\nThe variance study for activation/gradient is done for linear networks but applied to Tanh and Softsign. How is this justified?\n\n#### Datasets:\nShapeset 3x2, MNIST, CIFAR-10\n\n#### Presenter:\nDevansh Arpit",
        "sourceType": "blog",
        "linkToPaper": "http://www.jmlr.org/proceedings/papers/v9/glorot10a.html"
    },
    "1210": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1311.6184",
        "transcript": "#### Problem addressed: \nEvaluation and comparison of generative models\n\n#### Summary: \nThis paper improves upon an existing non parametric estimator by sampling from hidden variables instead of features. They present an unbiased estimator and prove it asymptotically converges to true distribution with number of samples. They also prove that the expected value of unbiased estimator is a lower bound on the true distribution. They also present a biased estimator with a different sampling scheme. They empirically validate their estimators using MNIST dataset on different generative models\n\n#### Novelty:\nSampling from hidden space for non-parametric estimation\n\n#### Drawbacks:\nThis method works only for models which have hidden variables. Application for deep networks is not clear. Procedure for sampling from hidden variables is not explicitly mentioned. Assumes that P(x|h) is easily calculated from the model\n\n#### Datasets:\nMNIST\n\n\n#### Resources:\npaper: http://arxiv.org/pdf/1311.6184v4.pdf\n\n#### Presenter:\nBhargava U. Kota",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1311.6184"
    },
    "1211": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/RaikoLCB14",
        "transcript": "#### Problem addressed: \nFully visible Bayesian network learning\n\n#### Summary: \nThis paper is very similar to the order agnostic NADE paper, it generalized the idea of order agnostic NADE and extended to k iterations. The difference between this work and the previous NADE work is: 1, instead of totally mask out the variables to compute, it instead provide the data mean for those variables; 2. mask is not supplied to the network; 3. it employed a walk-back like scheme, where the prediction is completed in k iterations.\n\n#### Novelty:\nIt is a generalization of NADE models.\n\n#### Drawbacks:\nTraining would be slow, and with large k, the challenge of training very deep net remains.\n\n#### Datasets:\nbinary mnist, caltec-101 silhouettes\n\n#### Additional remarks:\n\n\n#### Resources:\nimplementation is at https://github.com/yaoli/nade_k\n\n#### Presenter:\nYingbo Zhou",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5277-iterative-neural-autoregressive-distribution-estimator-nade-k"
    },
    "1212": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/UriaML14",
        "transcript": "#### Problem addressed: \nFully visible Bayesian network learning\n\n#### Summary: \nThis work is an extension of the original NADE paper. As oppose to using a prefixed random fully visible connected Bayesian network (FVBN), they try to train a factorial number of all possible FVBN by optimizing a stochastic version of the objective, which is an unbiased estimator. The resultant model is very easy to do any type of inference, in addition, since it is trained on all orderings, the ensemble generation of NADE models are also very easy with no additional cost. The training is to mask out the variables that one wants to predict, and maximize the likelihood over training data for the prediction of those missing variables. The model is very similar to denoising autoencoder with Bernoulli type of noise on the input. One drawback of this masking is that the model has no distinction between a masked out variable and a variable that has value 0. To overcome this, they supply the mask as additional input to the network and showed that this is an important ingredient for the model to work.\n\n#### Novelty:\nProposed order agnoistic NADE, which overcome several drawbacks of original NADE.\n\n#### Drawbacks:\nThe inference at test time is a bit expensive.\n\n#### Datasets:\nUCI, binary MNIST\n\n#### Additional remarks:\n\n\n#### Resources:\nThe first author provided the implementation on his website\n\n#### Presenter:\nYingbo Zhou",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v32/uria14.html"
    },
    "1213": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/TangSS14",
        "transcript": "#### Problem addressed: \nTraining specific generative model under milder unsupervised assumption\n\n#### Summary: \nThis paper implemented an attention based scheme for learning generative models, which could make the unsupervised learning more applicable in practice. In common unsupervised settings, one would assume that the unsupervised data is already in the desired format that one could used directly, which could be a very strong assumption. In this work, the demonstrated a specific application of the idea for training face models. They use a canonical low resolution face model that model the object in mind, alone with a search scheme that resembles the attention to search the face region in a high resolution image. The whole scheme is formalized as a full probabilistic model, and the attention is thus implemented as a inference through the model. The probabilistic model is implemented using RBMs. For inference, they imployed hybrid Monte Carlo, as with all MCMC methods, it is hard for the sampling methods to go between modes that are separated by low density areas. To overcome this, they instead used convnet to propose the moves and used hmc with the convnet initilized states so that the full system is still probabilistic. The result demonstrated are pretty interesting.\n\n#### Novelty:\nThe application of visual attention using RBM with probabilistic inference.\n\n#### Drawbacks:\nThe inference do need a good convnet for initilization, and it seems the mixing of Markov chain is a big problem.\n\n#### Datasets:\nCaltech and CMU face dataset\n\n#### Resources:\nA neurobiological model of visual attention and\n invariant pattern recognition based on dynamic routing of information, is the paper of visual attention\n\n#### Presenter:\nYingbo Zhou",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5345-learning-generative-models-with-visual-attention"
    },
    "1214": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SocherHPNM11",
        "transcript": "#### Problem addressed: \nParaphrase Detection using Recursive Auto Encoders\n\n#### Summary: \nThe authors present the usage of autoencoders to learn recursive tree (in a semantic sense) structures in data. They use deep recursive autoencoders to learn a representation for natural language sentences in unsupervised and semi-supervised frameworks. They then introduce a pooling scheme on top of this representation to handle sentences of varying length and determine if they are paraphrases of each other and achieve state of the art results on MSRP paraphrase corpus.\n\n#### Novelty:\nThe idea of unfolding recursive autoencoders (RAE),\n Pooling to handle sentences and representations of varying sentence lengths.\n\n#### Datasets:\nMSRP paraphrase corpus\n\n#### Additional remarks:\nThe main idea was introduced in Semi-Supervised Recursive\n Autoencoders for Predicting Sentiment Distributions, R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. In EMNLP, 2011. Please refer that for training of RAEs.\n\n#### Resources:\npaper: http://www.socher.org/uploads/Main/SocherHuangPenningtonNgManning_NIPS2011.pdf\n\n#### Presenter:\nBhargava U. Kota",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection"
    },
    "1215": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/CravenS95",
        "transcript": "#### Problem addressed: \nNeural nets are good at learning complex functions but they are blackboxes since they cannot be used to reason why a decision is made. This paper addresses this problem by learning a decision tree that approximates the hypothesis learned by the NN.\n\n#### Summary: \nWhile NNs have been shown to be able to learn complex functions efficiently, we cannot find the logic/reason why a particular test sample is classified into a particular class. Thus when a net when a mistake, we cannot know the reason behind it to rectify it. This raises critical concerns regarding the reliability of such learning algorithms. This paper introduces the idea of inducing a decision tree that approximates the hypothesis of the trained network. This has 2 advantages:\n 1. the decision tree can query as many test cases as it needs from the trained NN as the task is to learn the parent hypothesis.\n 2. This implies much more training examples for leaf nodes as compared to conventional tree algorithms.\n \n Another difference from traditional methods is that for splitting a node, the authors use 'm-of-n expressions'. Here, a conjunction of n conditions are used at least m of which must be satisfied to go on one side or the other. These conditions are derived in a greedy manner.\n \n The evaluation criteria of a node is to check its fidelity with the NN and check how many training examples reach a node. If too many examples reach a node then it is not performing the task of classification.\n\n#### Novelty:\nlearning a decision tree to approximate the hypothesis learned by a NN, the algorithm to train the decision tree\n\n#### Drawbacks:\n1. As this paper is from 1996, the empirical results are very weak since the dataset sizes are within a few hundreds while the feature dimensions are less than 100.\n 2. The algorithm is also not explained very clearly.\n 3. No qualitative results are shown for the core claim of reasoning the decisions of a NN.\n\n#### Datasets:\nCongressional voting dataset, Cleveland heart-disease dataset, UC-Irvine database, Un-named dataset for 'recognizing protein-coding regions in DNA'\n\n#### Additional remarks:\nSince every node is a raw feature and reasoning is being done on the same, this algorithm seems more suitable for Medical data.\n \n A good extension of this work would be to learn a decision tree that approximates a NN hypothesis in a more semantic feature space.\n\n\n#### Presenter:\nDevansh Arpit",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://nips.djvuzone.org/djvu/nips08/0024.djvu"
    },
    "1216": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/BengioMDR13",
        "transcript": "#### Problem addressed: \nIt has been empirically observed that deep representations lead to better mode mixing when sampled using MCMC. The authors present a set of hypotheses as to why this happens and confirm them empirically.\n\n#### Summary: \nThe paper claims that deep representations (specially from parametric models) disentangle the factors of variations in the raw feature space. This disentangling leads to better \"\"mode mixing\"\" during MCMC sampling. For eg., in faces, the factors of variation could be identity-pose-illumination. If the higher layer learns these features then changing the representation in this space starting from a \"\"valid\"\" point would lead to changes in each of these factors directly and hence will produce \"\"valid\"\" images, which in the original feature space would be far apart; thus better mode mixing. This hypothesis is explained using 2 additional ones: (a) the manifold structure of the \"\"valid\"\" data is flattened in the higher layer space, and (b) the fraction of total volume occupied by high probability (valid) points is larger in the higher layer space. While (a) should lead to better interpolation in higher layer space, (b) should lead to more valid points in a parzen window around any known sample. These are confirmed experimentally.\n\n#### Novelty:\nnovel intuitions why deep representations are good for generative modeling.\n\n#### Drawbacks:\nno theoretical justification\n\n#### Datasets:\nMNIST, Toronto Face dataset (TFD)\n\n#### Additional remarks:\nused DBN and Deep CAE for experiments on the datasets\n\n#### Presenter:\nDevansh Arpit",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v28/bengio13.html"
    },
    "1217": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/SocherLNM11",
        "transcript": "#### Problem addressed: \nLearning recursive structure from natural data - sentences and images\n\n#### Summary: \nThis paper introduces recursive neural networks in order to learn recursive structure in data. This recursive structure is illustrated by using examples of scene understanding using superpixel labels and parsing of natural language sentences. The authors go on define a novel neural network architecture, an objective function based on max-margin estimation and provide methods for backpropagation to train this network. They then evaluate its performance for accuracy in pixel labelling on the Stanford background dataset as well as for parsing sentences of Wall Street Journal section of the Penn TreeBank database. They achieve state-of-the-art and near state-of-the-art results respectively.\n\n#### Novelty:\nIntroduction of recursive neural networks, framing an objective function to learn a recursive tree structure from data\n \n State of the art results on scene understanding and pixel labels for Stanford background dataset\n\n#### Drawbacks:\nComputational analysis of method not provided Not many details about the backpropagation method.\n\n#### Datasets:\nStanford background dataset, Penn TreeBank\n\n#### Additional remarks:\nThe thesis of the first author, Chapter 3 was referred during presentation. This paper almost identical to that and the thesis provides slightly more details.\n\n#### Resources:\npaper: http://www-nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf\n \n thesis: http://nlp.stanford.edu/~socherr/thesis.pdf\n\n#### Presenter:\nBhargava U. Kota",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1218": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1305.2982",
        "transcript": "#### Problem addressed: \nGradient estimation for stochastic neurons\n\n#### Summary: \nThis paper proposed an unbiased estimator of stochastic units so that one can use gradient based learning. In addition, it also proposed a simple, biased estimator called straight through.\n\n#### Novelty:\nA new approach for estimating gradient for stochastic units.\n\n#### Drawbacks:\nThe proposed unbised estimator seems to have large variance, and the biased one seems not performing very well in practice\n\n#### Presenter:\nYingbo Zhou",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1305.2982"
    },
    "1219": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HintonVD15",
        "transcript": "#### Problem addressed: \nTraditional classifiers are trained using hard targets. This not only calls for learning a very complex function (due to spikes) but also ignores the relative similarity between classes, e.g., truck is more probable to be misclassified as a car instead of a cat. Instead the classifier is forced to assign both the car and cat to a single target value. This leads to poor generalization. This paper addresses this problem.\n\n#### Summary: \nIn order to address the aforementioned problems, the paper proposes a method to generate soft labels for each sample by first training a cubersome/large/complex classifier like dropout at a high \"\"temperature\"\" in so that it generates soft probabilities for every sample which represents its membership to each class. It then trains a vanilla NN initially at a high temperature and then at a low one using the generated soft labels on either the same training data or a transfer data. By doing so the simpler (student) model performs similar to the complex (teacher) model.\n\n#### Novelty:\ntechnique for generating soft labels for classes for training a much simpler classifier compared to currently used large and complex methods like dropout/conv-nets.\n\n#### Drawbacks:\nI believe a major drawback of this paper is that it entails learning a complex classifier for generating soft labels. Another drawback is that it is incapable of using unlabeled data.\n\n#### Datasets:\nMNIST, JFT (internal google image dataset)\n\n#### Additional remarks:\n\n\n#### Resources:\nhttps://www.youtube.com/watch?v=7kAlBa7yhDM\n\n#### Presenter:\nDevansh Arpit",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1503.02531"
    },
    "1220": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jmlr/LarochelleM11",
        "transcript": "#### Problem addressed: \nDensity estimation\n\n#### Summary: \nThis paper presented a tractable density estimator inspired from RBM. The inspiration comes from the procedure of using mean field approximation for the actual RBM probablities. It turns out that one step of the mean field approximation just corresponds to a one-hidden layer feed forward nerual networks with tied weights.\n\n#### Novelty:\nFind the link between the one step mean field approximation on RBM to a one layer neural network, and come up with a tractable density estimator that performs well.\n\n#### Drawbacks:\nThere still a need to pre-select an ordering of variables before running the algorithm.\n\n#### Datasets:\nMNIST, binary observation datasets from Larochelle 2010\n\n#### Additional remarks:\nThe KL divergence minimization derivation is quite tricky (eq 7, 8)\n\n#### Resources:\n\n\n#### Presenter:\nYingbo Zhou",
        "sourceType": "blog",
        "linkToPaper": "http://www.jmlr.org/proceedings/papers/v15/larochelle11a/larochelle11a.pdf"
    },
    "1221": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1312.6114",
        "transcript": "#### Problem addressed: \nVariational learning of Bayesian networks\n\n#### Summary: \nThis paper present a generic method for learning belief networks, which uses variational lower bound for the likelihood term.\n\n#### Novelty:\nUses a re-parameterization trick to change random variables to deterministic function plus a noise term, so one can apply normal gradient based learning\n\n#### Drawbacks:\nThe resulting model marginal likelihood is still intractible, may not be very good for applications that require the use of actual values of the marginal probablities\n\n#### Datasets:\nMNIST, Frey face\n\n#### Additional remarks:\nExperimentally compared with wake sleep algorithm on logliklihood lower bound as well as estimated marginal likelihood\n\n#### Resources:\nImplementation: https://github.com/y0ast/Variational-Autoencoder\n\n#### Presenter:\nYingbo Zhou",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1312.6114"
    },
    "1222": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/ShrikumarGK17",
        "transcript": "### Summary\n\nThe motivation of this paper is to make neural network interpretable so that they can be adopted in fields where interpretability is essential (ie: medical field). Thus, this paper present _DeepLIFT_, a method to interpret neural networks by **decomposing** the output prediction given a specific input by backpropagating the _contribution_ of all the neurons to every input features. The _contribution_ of a neuron is determined by comparing the activation of this neuron to a _reference activation_. This _reference activation_ is determined arbitrarily by a domain expert. Moreover, the authors argue that in some case, giving separate consideration to positive and negative contributions can reveal dependencies that are missed by other approaches. The authors show that their approaches can capture some dependencies that a gradient-based method cannot.\n\n### Computing the contribution of a neuron\n\nGiven the following notation:\n* $t$: Target output neuron\n* $t^0$: Reference activation of $t$\n* $x_1, x_2, ..., x_n$: Set of neurons\n* $\\Delta t$: The difference-from-reference of a target\n* $\\Delta x$: The difference-from-reference of an input\n* $C_{\\Delta x_i,\\Delta t}$: Contributions scores of a neuron\n\n$$\\Delta t = t - t^0$$\n$$\\Delta t = \\sum_{i=1}^n C_{\\Delta x_i \\Delta t}$$\n\nThe advantage of the _difference from reference_ against purely gradient method is that the _diference from reference_ avoid all discontinuities as seen in the following figure\n\nhttps://i.imgur.com/vLZytJT.png\n\n### \"Backpropagating\" the contribution to the input\n\nTo compute the contribution to the input, the authors use a concept similar to the chain rule. Given a _multiplier_ $m_\\Delta x _\\Delta t$ computed as following:\n\n$$m_{\\Delta x \\Delta t} = \\frac{C_{\\Delta x \\Delta t}} {\\Delta x}$$\n\nGiven $z$ the output of a neuron, $y_j$ one neuron in the hidden layer before $z$ and $x_i$ one neuron at the input, before $y_j$. We can compute $m_{\\Delta x_i \\Delta z}$ as following:\n\n$$m_{\\Delta x_i \\Delta z}=\\sum_j m_{\\Delta x_i \\Delta y_j} m_{\\Delta y_j \\Delta z}$$\n\n### Computing the contribution score\n\nThe authors argues that it can be beneficial in some case to separate the positive and negative contributions. ie:\n$$\\Delta _{x_i} = \\Delta _{x_i}^+ + \\Delta _{x_i}^-$$\n$$C_{\\Delta _{x_i} \\Delta _t} = C_{\\Delta _{x_i}^+ \\Delta _t} + C_{\\Delta _{x_i}^- \\Delta _t}$$\n\nThe authors propose three similars techniques to compute the contribution score\n\n1. A linear rule where one does not take into consideration the nonlinearity function such that $C_{\\Delta _{x_i} \\Delta _t} = w_i \\Delta _{x_i}$\n2. The _rescale rule_ applied to nonlinear function (ie: $y=f(x)$). If $\\Delta _y = 0$ or is very close (less than $10^{-7}$), then the authors use the gradient instead of the multiplier.\n3. The _Reveal Cancel rule_ is similar than the _rescale rule_, but threat the positive and negative example differently. This allows to capture dependencies (ie: min/AND) that cannot be captured by _rescale rule_ or other method. The difference from reference can be computed as follow:\n$$\\Delta y^+ = \\frac{1}{2}(f(x^0 + \\Delta x^+) - f(x^0)) + \\frac{1}{2}(f(x^0 + \\Delta x^+ + \\Delta x^-) - f(x^0+ \\Delta x^-)$$\n$$\\Delta y^- = \\frac{1}{2}(f(x^0 + \\Delta x^-) - f(x^0)) + \\frac{1}{2}(f(x^0 + \\Delta x^+ + \\Delta x^-) - f(x^0+ \\Delta x^+)$$",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v70/shrikumar17a.html"
    },
    "1223": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.18653/v1/p16-1078",
        "transcript": "This work extends sequence-to-sequence models for machine translation by using syntactic information on the source language side. This paper looks at the translation task where English is the source language, and Japanese is the target language. The dataset is the ASPEC corpus of scientific paper abstracts that seem to be in both English and Japanese? (See note below). The trees for the source (English) are generated by running the ENJU parser on the English data, resulting in binary trees, and only the bracketing information is used (no phrase category information).\n\nGiven that setup, the method is an extension of seq2seq translation models where they augment it with a Tree-LSTM to do the encoding of the source language. They deviate from a standard Tree-LSTM by running an LSTM across tokens first, and using the LSTM hidden states as the leaves of the tree instead of the token embeddings themselves. Once they have the encoding from the tree, it is concatenated with the standard encoding from an LSTM. At decoding time, the attention for output token $y_j$ is computed across all source tree nodes $i$, which includes $n$ input token nodes and $n-1$ phrasal nodes, as the similarity between the hidden state $s_j$ and the encoding at node $i$, then passed through softmax. Another deviation from standard practice (I believe) is that the hidden state calculations $s_j$ in the decoder are a function of the previous output token $y_{t-1}$, the previous time steps hidden state $s_{j-1}$ and the previous time step's attention-modulated hidden state $\\tilde{s}_{j-1}$.\n\nThe authors introduce an additional trick for improving decoding performance when translating long sentences, since they say standard length normalization did not work. Their method is to compute a probability distribution over output length given input length, and use this to create an additional penalty term in their scoring function, as the log of the probability of the current output length given input length.\n\nThey evaluate using RIBES (not familiar) and BLEU scores, and show better performance than other NMT and SMT methods, and similar to the best performing (non-neural) tree to sequence model.\n\nImplementation: They seem to have a custom implementation in C++ rather than using a DNN library. Their implementation takes one day to run one epoch of training on the full training set. They do not say how many epochs they train for.\n\nNote on data: We have looked at this data a bit for a project I'm working on, and the English sentences look like translations from Japanese. A large proportion of the sentences are written in passive form with the structure \"X was Yed\" e.g.. \"the data was processed, the cells were cultured.\" This looks to me like they translated subject-dropped Japanese sentences which would have the same word order, but are not actually passive! So that raises for me the question of how representative the source side inputs are of natural English.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.18653/v1/p16-1078"
    },
    "1224": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/BelinkovDDSG17",
        "transcript": "This paper attempts to open up the black box of neural machine translation models and inspect what the representations look like, specifically with respect to morphology. The technique they use is to train word-based and character-based seq2seq-style models on multiple source-target language pairs, of varying morphological complexity, and then ignore the target side to focus on the representations learned about the source language. Once they have an encoder trained to generate these representations, they attempt to use the encoder to create feature representations for external tasks that directly evaluate for morphology and part of speech information. (Contrast this with methods that may, for example, try to inspect activation patterns of individual neurons in a trained model.)\n\nThe first experiment shows that representations learned from character-based models are superior for POS tagging in the source language. The gap is bigger for morphologically rich languages like Arabic. The same result holds for morphological tagging. For infrequent words the gap is especially large -- the system can memorize morphological information for frequent words. They also show that the increases in accuracy are due to getting  prevoiusly unseen words correct (both for POS and morph prediction) and that the biggest increase in accuracy is in predicting plural and determined noun categories. Next, they show that in a deeper network, the middle layer (of 3) has the best representations for predicting pos/morph information. The authors suggest the higher layers are more focused on semantics or other higher abstractions.\n\nOverall, this work empirically confirms some conventional wisdom, that character representations are better for unseen words because of their ability to represent morphology.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.18653/v1/P17-1080"
    },
    "1225": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/IyyerMBD15",
        "transcript": "The authors explore the properties of \"Deep Averaging Networks\" on text classification problems, specifically sentiment and question answering tasks. DANs extend neural bag of words models, starting with a document representation that is the average of the word embeddings in that document, but extending to multiple feed-forward layers. The authors argue that these models are much simpler and faster to train than syntax and composition-based RNNs, while obtaining similar performance. Since this paper is actually arguing for simpler models, there is little technically here to understand, so the real contribution of the paper are the interesting experiments exploring how the DANs represent various phenomena. They show that differences between graded sentiment words (awesome, cool, ok, underwhelming, the worst) are magnified as layers are added. This shows the benefit of depth relative to a neural bag of words. Then they compare against RNNs with examples containing negation and contrastive conjunctions (e.g., but), which are traditionally modeled syntactically. They show that existing methods that we think can represent syntax/composition in fact are not strong enough. Something like \"not bad\" fully exposes the DAN -- it doubles the negation. But while the RNN-based models can learn not to simply double the negation, they are not powerful enough to reverse the polarity and get the example correct.\n\nFinally, the authors introduce one novel mechanism for improving training, \"word dropout.\" Similar to standard dropout, they randomly sample a subset of words at the input layer that are not used as part of the document representation. This gives the network multiple looks at each example with part of its feature space removed. Another way to think of this is data augmentation where new training instances are created by sampling feature vectors from existing data points with some features missing.\n",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P15/P15-1162.pdf"
    },
    "1226": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/UpadhyayFDR16",
        "transcript": "This paper looks at four different ways of training cross-lingual embeddings and evaluates them on downstream tasks to investigate which types of learning are suitable for which tasks. Since they require different amounts and quality of supervision this is important to understand. If it only works when there are word-level alignments then cross-lingual embeddings won't help with low-resource languages. On the other hand, if methods trained by comparable corpora only are effective for some downstream tasks, then we have some hope for low resource languages.\n\nThe four methods are: 1) a skip-gram like method (Biskip) that uses word-aligned corpora, replacing words for aligned words in the target language, and predicting the source words in the context. This requires sentence aligned corpora, and I believe the word alignments are automatic given that. 2) A model that computes sentence embeddings from component word embeddings and optimizes the loss function to minimize the difference between aligned sentences. (BiCVM) This of course reqiures aligned sentences as well. 3) A projection-based method (BiCCA) that takes two monolingual (independent) word vectors and learns projections into a shared space. They use a translation dictinoary to find the subset of words that align, and use CCA to learn a mapping that respects those alignment. This projection can then be applied to all words in the dictionary. This method does not require similar corpora. 4) A method that uses comparable corpora (similar documents in each language, ala wikipedia) to create pseudo-documents that randomly samples words from each languages document. Once these documents are created they train with word2vec skipgram.\n\nThese methods are evaluated on a few NLP tasks, including monolingual word similarity, cross-lingual dictinoary induction, cross-lingual document classification, and cross-lingual dependency parsing.\nOf the models that require sentence alignments, BiSkip usually beats BiCVM. BiSkip is best for most of the semantic tasks. BiCCA is as good or better for dependency parsing, suggesting that cheaper methods might be ok for syntactic tasks. One major caveat is that the Chinese dependency parsing results are terrible, meaning that this style of training for dependency parsers probably only works when the languages have similar structure. So the benefits to parsing low resource languages may be minimal even though the supervision required to create the embeddings is low.",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/P/P16/P16-1157.pdf"
    },
    "1227": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/DaumeJ11",
        "transcript": "Read this because it was cited by Zhang et al. 2017 and the title looked interesting. The setting is machine translation where you have pairs in one domain (europarl) but need to do translation in another (biomed). They quantify how much of the performance loss is due to vocabulary differences in the two domains. First, an oracle is created which using both domains to train. Second, an OOV oracle is created that removes words that their mining approach could not possibly find, to see what the essential limit of their approach is. \n\nTheir approach, then, uses non-parallel domain texts to create word similarity matrices for new terms. They compute a context based similarity matrix first. This involves creating feature vectors for each word based on the contexts in which it appears, and then computing similarity between all word pairs. Then they create an orthography-based similarity matrix using character n-grams within each word as a feature vector and computing similarity between all word pairs. They sum these matrices to get similarity matrix. They build a bipartite graph between existing word pairs where word in source language is connected to its translation words with edge weighted by unigram translation probability. This graph is reduced to single pairs (one edge between word pairs) with the Hungarian algorithm, and they use CCA to estimate the projection using these training pairs (one projection for each language). These projections are then applied to _all_ words to get new word representations. They then explore a few ways to integrate these new scores with existing MT systems, and find that they don't get improvement just by calling them scores, they also need to add features indicating when they are real scores and when they are mined scores.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aclweb.org/anthology/P11-2071"
    },
    "1228": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/coling/CaoZZM16",
        "transcript": "A joint model for training bilingual embeddings without supervision. As background information they point out that embeddings in different languages tend to be linear transforms of each other. Then they create a new network and objective that builds on CBOW training, but has two terms in the loss function: The traditional loss for the CBOW language model, and an additional loss over the mean and variance of each dimension of the embedding space. The hypothesis, I guess, is that constraining each dimension of each languages embeddings to be similar (mean and variance) will put them in the same space from the start (i.e., no transformation needed). During training, they then sample a word randomly from the source (or target) language, feed forward, and compute the distribution-based gradient using the target (or source) means as the gold, and add it to the standard gradient.\nThey evaluate on English-French and English-Chinese using aligned corpora, and get better, though not good, performance.\n\nThere are two issues that I can think of that might affect the (lack of) accuracy. First, the mean/variance similarity constraints might be not constrained enough. Second, the fact that the other languages in-progress embedding statistics are used as gold may cause it to get stuck in local optima? FWIW,  Zhang et al '17 (ACL) also question the distributional assumptions (that each dimension is Gaussian-distributed).",
        "sourceType": "blog",
        "linkToPaper": "http://aclweb.org/anthology/C/C16/C16-1171.pdf"
    },
    "1229": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.18653/v1/p17-2003",
        "transcript": "(Reposting under ACL 2017 version)\n\nKind of a response/deeper dive into the durret/klein \"easy victories\" paper. Suggests that a) lexical features they used (\"easy victories\") are very prone to overfitting. They first show that several state of the art systems that use lexical features, trained on CoNLL data, perform poorly on wikiref, which was annotated using the same guidelines. Meanwhile the stanford sieve system performs about the same on both.\nThen they show that a high percentage of gold standard linked headwords in the test set have been seen in the training set, and that a much lower percentage of errors are in the training set, implying that lexical features just allow you to memorize what kinds of things can be linked.\nThey suggest development of robust features, including using embeddings as lexical features, using lexical representations only for context, and on the evaluation side, using test sets that are different domains than the training set.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.18653/v1/p17-2003"
    },
    "1230": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1704.06779",
        "transcript": "Kind of a response/deeper dive into the durret/klein \"easy victories\" paper. Suggests that a) lexical features they used (\"easy victories\") are very prone to overfitting. They first show that several state of the art systems that use lexical features, trained on CoNLL data, perform poorly on wikiref, which was annotated using the same guidelines. Meanwhile the stanford sieve system performs about the same on both.\nThen they show that a high percentage of gold standard linked headwords in the test set have been seen in the training set, and that a much lower percentage of errors are in the training set, implying that lexical features just allow you to memorize what kinds of things can be linked.\nThey suggest development of robust features, including using embeddings as lexical features, using lexical representations only for context, and on the evaluation side, using test sets that are different domains than the training set.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1704.06779"
    },
    "1231": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/ZhangLLS17",
        "transcript": "Multilingual embeddings are useful for creating embeddings for low resource languages for things like transfer learning (e.g., learning a POS tagger in a low-resource language using training data from a high resource language). However, they typically require some small amount of supervision in the form of aligned corpora, seed pairs, or dictionaries. This approach attempts to learn a mapping from a source embedding space into a target embedding space without supervision.\n\nThe approach uses two networks a la adversarial training. One network (the generator) is parameterized by a projection matrix that attempts to map source words into the target space. The other network (the discriminator) attempts to discriminate true target embeddings from projected source embeddings. Since adversarial training is known to be unstable (a \"research frontier\" as the authors say), quite a bit of the paper describes tricks and training methods the authors investigated to get training to converge and understand how to select models.\n\nThey evaluate on many pairs, including both similar and dissimilar language pairs, and get very nice results. In summary, better than seed-based approaches with 0-100 seeds, competitive with 100-1000 seeds. Much of what would be traditional discussion is instead devoted to details of training regimen, so unfortunately there is little discussion of why this works. Given the difficulty one might encounter attempting to train this, I think it might be a little preliminary to try using this for applications, but continued research in training adversarial networks for NLP and properties of embedding spaces could potentially make this approach reliable enough for real applications.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.18653/v1/P17-1179"
    },
    "1232": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/FangC17",
        "transcript": "They get multilingual alignments from dictionaries, then train a Bilstm pos tagger in source language, then automatically tag many tokens in the target language, then manually annotate 1000 tokens in target language, then train a system with combined loss over distant tagging and gold tagging. They add an additional output layer that is learned for the gold annotations.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.18653/v1/P17-2093"
    },
    "1233": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Lopez-PazR17",
        "transcript": "#### About APAD ####\n_This summer I've been Interning at an AI lab at BCM, working under\nAnkit Patel in his incredible Neuroscience-meets-Deep-Learning group. \nThis field is moving faster than anything else out there. Keeping intro short, \nso full post + inline pictures are on my [blog](http://rayraycano.github.io/data%20science/tech/2017/07/31/A-Paper-a-Day-GEM.html)_\n\n## Basics ##\n* __Paper__: [Gradient Episodic Memory for Continuum Learning][GED]\n* __Authors__: David Lopez-Paz, Marc'Aurelio Ranzato \n* __Organizaitons__: Facebook AI Research (FAIR)\n* __Topic__: Coninuum Learning\n* __In One Senetence__: _Paz and Ranzato define sorely needed metrics to the\nsubfield of Continual Learning while developing a gradient-inspired update rule \nthat avoids catastrophic forgetting_\n\n### Background ###\n\nPaz and Ranzato's continuum learning targets a more general problem of \n_catastrophic forgetting_, which the authors describe as \"the poor ability of models to\nquickly solve new problems, without forgetting previously acquired knowledge.\"\nRecently this has been a hot topic in AI recently, as a flurry of \npapers in early summer were released discussing this topic ([Elastic Weight Consolidation][EWC],\n[PathNet][PathNet], [iCaRL][iCARL], [Sluice Network][Sluice Network], [Intelligent Synapses][Intelligent Synapses]).\nAvoiding catastrophic forgetting and achieving nontrivial _backwards transfer_ (BT) and _forward transfer_ (FT) are major goals for\ncontinual learning models, and in addition, general AI. \n\n__Analogy Alert!__ _As Ankit explained to me originally: \nIf you know how to play tennis, your experience \n*should* aid your ability to pick up ping pong (FT). \nIn addition, when you return to tennis, your aptitude\nin tennis shouldn't decrease (some atheletes argue that they get better\nat their primary sport because they've played secondary sports, i.e. BT)._ \n \n## Paper Accomplishments ##\n\n1. Of the 5 papers mentioned above, 0 of them formally define metrics for a\ncontinual learner. \n2. The gradient-aligning update rule is quite clever and pretty cool.\n\n### The Metrics ###\nFirst, let's take a look at their formal definitions for FT and BT. \nThey're displayed below. The notation is a bit confusing, so I've done my\n best to parse it.\n \n[Backward And Forward Transfer LaTeX](https://www.dropbox.com/s/qrj6sxkfruj42uk/Screen%20Shot%202017-07-31%20at%204.56.05%20PM.png?dl=1)\n\nT is the total number of tasks, enumerated from 1 to T. The bi vector is the random initialization for each task. I've omitted accuracy from this discussion because it seem too novel in the context of this paper\n\n* Assume a fixed sequence of tasks (Numbered 1 through T)\n* Forward transfer is the average accuracy of some task, task _i_, after each \ntask in the sequence preceding _i_ is completed. \n    * Record the score for task _i_ upon random initialization\n    * Learn task 1, record your score for task _i_. Learn task 2, record your\n     score for task _i_, etc., up until task _i-1_.\n    * Subtract from their score upon initialization and average these scores.\n* Backwards Transfer is the average accuracy change for task _i_ after each \ntask afterwards has been completed\n    * Record the score for task _i_ after learning it\n    * Learn task _i+1_. Now record the score for task _i_. Learn task _i+2_. Record the\n    score for task _i_, etc.\n    * For each score of task _i_ that recorded after completeing _i_, subtract from the first\n    score for task _i_. Average these Scores\n\n    \n#### Gripes about the Metrics ####\nIn my opinion, these metrics don't generalize well. Continuum Learning (which I presume\nis less general than _Continual_ Learning) specifies a sequence of tasks,\n meaning it is sensitive to the order of tasks. In their experiments section, \n they use tasks that theoretically don't depend on the order in which they're learned, so in the \n scheme of their paper this point is moot. However, Continual Learning in general\nhas no specification on order. Other papers concerning this topic have not discussed\ntask curriculums at all, while this paper glosses over it. \n\n__A metric I prefer: randomly sample from a pool of tasks\n_n_ times. Learn these _n_ tasks in an arbitrary order. Lastly, evaluate accuracy on _i_ (for forward transfer).__ \n(This can be done over multiple trials to get a robust average)\n\n### Gradient-Aligned Updates ###\n\nFor those new to Machine Learning, much of Deep Learning is powered by the \n[__backpropagation algorithm__][backprop]. This algorithm calculates an update that\nwill improve the accuracy for the problem based on an error metric. It does this by calculating what's \ncalled a gradient. \n\n__Analogy Alert!__ : _You shoot a dart at a board. You shot low by some distance __d__. \nYou correct your mechanics, backwards reasoning from the missed distance, to your \nrelease point, and from there perhaps your throwing velocity. \nYou can think of these corrections as the gradient, and the linked modification of\nall the preceding components as an implementation\nof the [chain rule][chainrule]. Disclaimer: \n There is no evidence that the brain implements backpropagation_\n\nSo what does Gradient Episodic Memory (GEM) do exactly? Let's start with the Memory\npart and go from there\n\n* _Memory_: Recall those sequence of tasks we mentioned earlier? Well for each of those\ntasks, let's make sure we don't forget them. We'll keep a portion of them in memory. \n\n* _Episodic_: Let's replay these memories to make sure we're not damaging our accuracy\non these tasks when we learn new ones. By playing them over again, we're basically going through an episode\n\n* _Gradient_: When we look at the episode again, let's make sure the gradient doesn't\ngo the wrong way. What this means is: let's not unlearn what we learned on the \nprevious task. \n\n### So How is it Done? ###\n\n[Gradient Update Equation][Gradient Update Equation]\n\ng is the gradient for the current task, while gk is the gradient for each previous task, calculated over the episodes in memory (Mk). The big < > notation is a dot product operator\n\nDot Products! In order for a gradient update to take place, they compute the dot product\nof the current learning task with all the previous tasks in memory. The update is allowed\nto take place if the gradient is greater than or equal to 0 for all the episodes. This translates\ninto constraining your update for one task to not conflict with an update for the previous task.\n\nWhat if the gradient is going the wrong way? Paz and Ronzato take this gradient update\nand project it to the closest possible vector that doesn't go the wrong way \n(The proof is in the pudding, eqn. 8 - 13 in the paper. It formulates the optimization as a projection\non to a cone).\n \n[Gradient Update Projection Example][Gradient Update Projection Example]\n\n Above shows the graphical representation of the gradient update conditions, with the blue\n    line being the update for the first task, while the red line is the gradient update for the current task. The right side\n    shows an approximation of the optimized projection for the gradient when the dot product is negative.\n\n### Does it work? ###\nYes. Well, kind of. \n\nThe focus of this paper was to minimize Backwards Loss (aka\nmaximize Backwards Transfer). In this sense, they appear to succeed. However, \nthe small improvements lack error bars, making an unconvincing case (#DLNeedsErrorBars). \nForward Transfer is negligible on all but one experiment (there were three total).\n\n[Results from Paper]\n\nThe plots above show performance. The right hand side demonstrates\nthe accuracy on the first task as the consequent tasks are learned, which each different\ncolored bar indicating a start to learning for a new task.\n\n#### Knitpicking ####\n\n* The experiments compare against Elastic Weight Consolidation (EWC). However, \nEWC was tested and optimized for Reinforcement Learning and Atari Games. I wonder\nif an earnest job of optimizing EWC for the tasks at hand was done. \n\n* There is still no metric for parameter conservation as a result of continual/shared learning. \nA curve showing the change in accuracy across a set of tasks while increasing \nthe size of the overall network would be nice. It would be interesting to compare \nall the papers on this metric. You could also evaluate the similarity of tasks\n(or how well a network learns similarities in tasks) through this method.\n\n### Summary ###\nCool Method.  Nice Paper. Less than satisfying results. But in general a\nsolid step forward for continual learning/overcoming catastrophic\nforgetting. \n\n\n[EWC]: https://arxiv.org/pdf/1612.00796.pdf\n[PathNet]: https://arxiv.org/pdf/1701.08734.pdf\n[iCARL]: https://arxiv.org/pdf/1611.07725.pdf\n[Sluice Network]: https://arxiv.org/pdf/1705.08142.pdf\n[Intelligent Synapses]: https://openreview.net/pdf?id=rJzabxSFg\n[Metrics]: https://www.dropbox.com/s/qrj6sxkfruj42uk/Screen%20Shot%202017-07-31%20at%204.56.05%20PM.png?dl=0\n[Gradient]: https://www.dropbox.com/s/jye3b3mco5fs277/Screen%20Shot%202017-07-31%20at%204.55.52%20PM.png?dl=0\n[Results]: https://www.dropbox.com/s/qvr95xydhtlnijw/Screen%20Shot%202017-07-31%20at%204.55.41%20PM.png?dl=0\n[GED]: https://arxiv.org/pdf/1706.08840.pdf\n[backprop]:https://en.wikipedia.org/wiki/Backpropagation\n[chainrule]:https://en.wikipedia.org/wiki/Chain_rule\n[Gradient Update Equation]:https://www.dropbox.com/s/jye3b3mco5fs277/Screen%20Shot%202017-07-31%20at%204.55.52%20PM.png?dl=1\n[Gradient Update Projection Example]: https://www.dropbox.com/s/jkdkk8bmz6btl77/Screen%20Shot%202017-07-31%20at%207.36.14%20PM.png?dl=1 \"Projection Update\"\n[Results from Paper]:https://www.dropbox.com/s/qvr95xydhtlnijw/Screen%20Shot%202017-07-31%20at%204.55.41%20PM.png?dl=1\n\n \n ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.08840"
    },
    "1234": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SungZXHY17",
        "transcript": "At the core of an actor-critic algorithm is an idea of approximating an objective function $Q(s, a)$ (or Q function) with a trainable function $Q_\\theta$ called a critic. An actor $\\pi_\\phi$ is then trained to maximize $Q_\\theta$ instead of the original $Q$. Often, a pair of $Q_\\theta$ and $\\pi_\\phi$ are trained separately for each task $\\mathcal{T}$. In this paper, the authors cleverly propose to share a single critic $Q_\\theta$ across multiple tasks $\\mathcal{T}_1, \\ldots, \\mathcal{T}_L$ by parametrizing it to be dependent on a task. That is, $Q_\\theta(s, a, z)$, where $z$ is a task vector. \n\nFor simplicity, consider a supervised learning task (as in Sec. 3.2), where $Q^t$ is an objective function for the $t$-th task and takes as input a training example pair $(x, y)$. A single critic $Q_\\theta$ is then trained to approximate $L$-many such objective functions, i.e., $\\arg\\min_{\\theta} \\sum_{t=1}^L \\sum_{(x,y)} (Q^t(x,y) - Q_\\theta(x,y,z^t))^2$. The task vector $z^t$ is obtained by a task encoder (TAEN) which takes as input a minibatch of training examples of the $t$-th task and outputs the task vector. The TAEN is trained together with the $Q_\\theta$, and all the $L$ actors $\\pi^t$. \n\nOnce the critic (or meta-critic, as referred to by the authors) is trained, a new actor $\\pi_\\phi^{L+1}$ can be trained based on a small set of training examples in the following steps. First, the small set of training examples are used to compute a new task vector $z^{L+1}$. Second, a new critic is computed: $Q_\\theta(\\cdot,\\cdot,z^{L+1})$. Third, the new actor is trained to maximize the new critic. \n\nOne big lesson I learned from this paper is that there are different ways to approach meta-learning. In the context of iterative learning of neural nets, I've only thought of meta-learning as learning to approximate an update direction as from https://arxiv.org/abs/1606.04474, i.e., $\\phi \\leftarrow \\phi + g_\\theta(\\phi, x, y)$. This paper however suggests that you can instead learn an objective function, i.e., $\\phi \\leftarrow \\phi + \\eta \\nabla_{\\phi} Q_\\theta(x, \\pi_{\\phi}(x), z(D))$, where $z(D)$ is a task vector obtained from new data $D$. \nThis is interesting, as it maximally reuses any existing technique from gradient-based learning and frees the meta-learner from having to re-learn them again. \n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.09529"
    },
    "1235": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.05510",
        "transcript": "#### Summary:\nThe main point of the paper is to show the automatic tranformation process of a java project to run on AWS lambda. For the transformation process a self developed tool named Podilzer is used to perform the tests. Further a comparison of the execution times and cost factors is made, to show if it's valuable to run Java functions on AWS lambda.\n\n#### Good points:\nThe pipeline process is well described and good understandable. The developed tool called Podilizer implements this pipeline process and is also as open source project available. The experiments are also available on the openscience platform including scripts and code, this grantees repeatability of the conducted tests. \n\n#### Major comments:\nOne of the main problems with FaaS in general is preserving the state of an application. This challenge is described well. I wished to get a bit more insight what other problematic functions exist and what the approach of transforming those would be. \nThe java projects which were used to conduct the results are available but not described in the paper. Therefore the transformation times are difficult to assess, before a study of the projects itself. Further  the execution performance of the same applications were compared locally and on different cloud offerings. On AWS Lambda the execution performance is significantly higher then on all the other platforms. Probably the times also include network latency, therefore it would be also interesting to see the actual execution times on Lambda itself. \n\n#### Minor comments:\nGenerally good grammar, some minor typos. \n\n#### Recommendations:\nDescribe the problems of transforming existing functions in more extend. Include also the \u201creal\u201d execution times of the AWS Lambda functions, to have a better comparison between the run times. \nFinally proofread and publish after corrections have been made",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1702.05510v1"
    },
    "1236": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1702.06925",
        "transcript": "I like the idea proposed in this paper - training on a label-rich domain and transfer the representation to a label-limited domain, but would like to extend it to data more than faces such as transferring the object attributes.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1702.06925"
    },
    "1237": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1604.08612",
        "transcript": "I enjoyed reading this paper. However I have a problem with the core statement of the problem which is described by the following sentence. \n\n\"However, our visual experience is not at all like this. We experience the world as fully\ndetailed and there is currently no scientific explanation of this.\"\n\nWho says we experience the world this way? I don't feel that I do at all. Let's say I'm looking at a large picture made of random pixels each of which is white or black at random. I first perceive the fact that I am looking at something with no real information. I know there are white and black pixels everywhere but I don't actually store which are white and which are black because I see no information in it and don't think it would be useful. I can focus on any small part of it and truly record what the arrangement is. If I close my eyes I can probably remember a 3x3 block accurately enough which is only 9 bits of information. If I practice for a long while, I could probably get a somewhat larger block. But I doubt I could get much higher than a 5x5 block of pixels. \n\nIf I look at the scene around me right now, I see my familiar living room scene. It's not random like before. Again I can focus on any part of it, say a book on my bookshelf, and read the title. And there are lots of books. I can do the same for any of them but not all of them at once. I know they are have titles but I am under no illusion that I see them all at once and record the information. I know my eye and brain is not taking a 64 Megapixel picture every second like a video recorder. \n\nThe other things in the room that show up in my peripheral vision can be summarized easily. I know there is a piano. It's large and brown and to the left side of the room. I know there are lamps and a beer glass. I can close my eyes and answer questions about the things in the room. I've compressed the useful information into a model and ignored the rest just as a JPEG or Autoencoder reduces the size of an image but keeps most of the relevant information. Perhaps my brain has approximately memorized the whole picture at low resolution. But I'm under no illusion that I'm really seeing it all at once at high resolution. \n\nWhether my eyes are open or closed I have this reduced model of what I'm looking at. I know from experience that there is detail everywhere without seeing it all at once. Wherever I focus my gaze I do see the detail and so I come to believe there is indeed detail everywhere even if I can't see and store all of it at once. \n\nThe point is that there is no scientific problem here. Our brain very likely does something like what artificial neural networks do. They identify summarizing features which characterize most things at low resolution keeping at most only a small part of the field in high focus. Our brain being active can choose to query any other part of this to get higher resolution on that part. When it does it overwrites whatever it is that we were focusing on before. \n\nSo in summary, I think the problem arises only from a confusion between \"knowing that a picture has high-resolution information everywhere\" and \"actually recording that information\". I certainly don't see any need to propose a supernatural explanation of vision. We're only beginning to understand how the brain and other artificial neural networks process information and in the latter we are often surprised how well they can do. \n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.08612v2"
    },
    "1238": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1712.09913",
        "transcript": "- Presents a simple visualization method based on \u201cfilter normalization.\u201d\n- Observed that __the deeper networks become, neural loss landscapes become more chaotic__; causes a dramatic drop in generalization error, and ultimately to a lack of trainability.\n- Observed that __skip connections promote flat minimizers and prevent the transition to chaotic behavior__; helps explain why skip connections are necessary for training extremely deep networks.\n- Quantitatively measures non-convexity.\n- Studies the visualization of SGD optimization trajectories.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1712.09913"
    },
    "1239": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/IandolaMAHDK16",
        "transcript": "While preserving accuracy,\n- Network architecture improvement decreases parameters 51X (240MB to 4.8MB).\n- By using Deep Compression, parameters shrinks more 10X more  (4.8MB to 0.47MB).\n\nEven improves more accuracy for about 2% by using Simple Bypass (shortcut connection).\n\nThey show insightful architectural design strategies;\n1. Less 3x3 filters to decrease size,\n2. Decrease input channels also to decrease size,\n3. Downsample late to have larger activation maps to lead to higher accuracy.\n\nAnd great insights about CNN design space exploration by parametrize microarchitecture,\n- Squeeze Ratio to find good balance between weight size and accuracy.\n- 3x3 filter percentage to find enough number of it.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.07360"
    },
    "1240": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.07128",
        "transcript": "- Result of thourough research which not only covers major research, but also compares under same criteria/ dataset; This is also a great survey.\n- Train on 32-bit FP model, run 8-bit model. No retraining required to convert to 8-bit w/o loss in accuracy.\n- Provides comparison concerning computing resource, it's useful to design for typical (ARM) microcontroller systems.\n- MobileNet inspired DS-CNN runs small and accurate, achieves the best accuracies of 94.4% ~ 95.4%. Maybe SOTA.\n- Apatche licensed code/ pretrained models are available at https://github.com/ARM-software/ML-KWS-for-MCU.\n\nhttps://i.imgur.com/qahXKBn.png",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1711.07128v3"
    },
    "1241": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1710.09412",
        "transcript": "Very efficient data augmentation method. Linear-interpolate training set x and y randomly at every epoch.\n```python\nfor (x1, y1), (x2, y2) in zip(loader1, loader2):\n    lam = numpy.random.beta(alpha, alpha)\n    x = Variable(lam * x1 + (1. - lam) * x2)\n    y = Variable(lam * y1 + (1. - lam) * y2)\n    optimizer.zero_grad()\n    loss(net(x), y).backward()\n    optimizer.step()\n```\n- ERM (Empirical Risk Minimization) is $\\alpha = 0$ version of mixup, i.e. not using mixup.\n- Reduces the memorization of corrupt labels.\n- Increases robustness to adversarial examples.\n- Stabilizes the training of GAN.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1710.09412"
    },
    "1242": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1802.03043",
        "transcript": "To keep it simple, this figure shows the basic idea.\n\nhttps://i.imgur.com/a2I4EGY.png",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1802.03043v1"
    },
    "1243": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1612.08242",
        "transcript": "YOLOv2 is improved YOLO;\n - can change image size for varying tradeoff between speed and accuracy;\n - uses anchor boxes to predict bounding boxes;\n - overcomes localization errors and lower recall not by bigger nor ensemble but using variety of ideas from past work (batch normalization, multi-scaling and etc) to keep the network simple and fast;\n - \"With batch nor-malization we can remove dropout from the model without overfitting\"\n -  gets 78.6 mAP at 40 FPS.\n\nYOLO9000;\n - uses WordTree representation which enables multi-label classification as well as making classification dataset also applicable to detection;\n - is a model trained simultaneously both for detection on COCO and classification on ImageNet;\n - is validated for detecting not labeled object classes;\n - detects more than 9000 different object classes in real-time.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1612.08242"
    },
    "1244": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.01838",
        "transcript": "__Edit__: The paper has a newer version at https://openreview.net/pdf?id=B1YfAfcgl, where I also copied this review over with some changes.\n\n__Note__: An earlier version of the review (almost identical to the present one) for an earlier version of the paper (available on arXiV) can be found here: http://www.shortscience.org/paper?bibtexKey=journals/corr/1611.01838#csaba\nThe only change concerns relation to previous work.\n\n__Problem__: The problem considered is to derive an improved version of SGD for training neural networks (or minimize empirical loss) by modifying the loss optimized to that the solution found is more likely to end up in the vicinity of a minimum where the loss changes slowly (\"flat minima\" as in the paper of Hochreiter and Schmidhuber from 1997). \n\n__Motivation__: It is hypothetised that flat minima \"generalize\" better. \n\n__Algorithmic approach__: Let $f$ be the (empirical) loss to be minimized. Modify this to $$\\overline{f}(x) = \\rho f(x) - \\log \\int \\exp(-\\frac{\\gamma}{2}\\||z\\||^2-f(x+z))\\,dz$$ with some $\\rho,\\gamma>0$ tunable parameters. For $\\||z\\||^2\\gg \\frac{1}{\\gamma}$, the term $\\exp(-\\frac{\\gamma}{2}\\||z\\||^2)$ becomes very small, so effectively the second term is close to a constant times the integral of $f$ over a ball centered at $x$ and having a radius of $\\propto \\gamma^{-1/2}$. This is a smoothened version of $f$, hence one expects that by making this term more important then the first term, a procedure minimizing $\\bar f$ will be more likely to end up at a flat minima of $f$. Since the gradient is somewhat complicated, an MCMC algorithm is proposed (\"stochastic gradient Langevin dynamics\" from Welling and Teh, 2011).\n\n__Results__: There is a theoretical result that quantifies the increased smoothness of $\\overline{f}$, which is connected to stability and ultimately to generalization through citing a result of Hardt et al. (2015). Empirical results show better validation error on two datasets: MNIST and CIFAR-10 (the respective networks are LeNet and All-CNN-C). The improvement is in terms of reaching the same validation error as with an \"original SGD\" but with fewer \"epochs\".\n\n__Soundness, significance__: The proof of the __theoretical result__ relies on an arbitrary assumption that there exists some $c>0$ such that no eigenvalue of the hessian of $f$ lies in the set $[-2\\gamma-c,c]$ (the reason for the assumption is because otherwise a uniform improvement cannot be shown). For $\\rho=0$ the improvement of the smoothness (first and second order) is a factor of $1/(1+c/\\gamma)$. The proof uses Laplace's method and is more a sketch than a rigorous proof (error terms are dropped; it would be good to make this clear in the statement of the result).\n\nIn the experiments the modified procedure did not consistently reach a smaller validation error. The authors did not present running times, hence it is unclear whether the procedure's increased computation cost is offset by the faster convergence. \n\n__Evaluation__: It is puzzling why a simpler smoothing, e.g., $$\\overline{f}(x) = \\int f(x+z) g(z) dz$$ (with $g$ being the density of a centered probability distribution) is not considered. The authors note that something \"like this\" may be infeasible in \"deep neural networks\" (the note is somewhat vague). However, under mild conditions, $\\frac{\\partial}{\\partial x} \\overline{f}(x) = \\int \\frac{\\partial}{\\partial x} f(x+z) g(z) dz$, hence, for $Z\\sim g(\\cdot)$, $\\frac{\\partial}{\\partial x} f(x+Z)$ is an unbiased estimate of $\\frac{\\partial}{\\partial x} \\overline{f}(x)$, whose calculation is as cheap as that of vanilla SGD. Also, how much the smoothness of $f$ changes when using this approach is quite well understood.\n\n__Related work__: Hochreiter and Schmidhuber argued a long time ago for the benefits of \"flat minima\" in an  identically titled Neural computation paper that appeared in 1997. This reviewer may not agree with the arguments in this paper, but the paper is highly relevant and should be cited. It is also strange that the specific modification that appears in this paper was proposed by others (Baldassi et al.), whom this paper also cites, but without giving these authors credit for introducing local entropy as a smoothing technique. ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.01838"
    },
    "1245": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1612.06299",
        "transcript": "Table 4, 5, with only $.5\\%$ of the pixels, you can get to $90\\%$ missclassification, and it is a blackbox attack.\n\n#### LocSearchAdv Algorithm\nFor $R$ rounds, at each round find $t$ top pixels that if you were to perturb them without bounds they could affect the classification the most. Then perturb each of the $t$ pixels such that they stay within the bounds (the magnitude of perturbation is a fixed value $r$). The top $t$ pixels are chosen from a subset of $P$ which is around $10\\%$ of pixels; at the end of each round $P$ is updated to be the neighborhood of size $d\\times d$ around the last $t$ top pixels.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1612.06299"
    },
    "1246": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1609.00150",
        "transcript": "Assume you have both label and reward (some flexibility in accepting the output) as is the case for hamming distance and negative edit distance.\n\nAssume the output space is discrete, it can be a quantized n-dim space.\n\nAssume that given an input and a set of labels, computing the probability of the labels is $O(L)$, meaning that most of the computation is independent of the label. (not sure about this! You certainly don't want to sample from the network)\n\nSuppose that you want to maximize an approximate objective which is a tradeoff between optimizing the reward or the label and you control this tradeoff by one parameter $\\tau$ which you fix beforehand. This actually only directly affects the reward.\n\nThen you can compute the loss, by sampling from each discrete value of reward and then sampling from those labels that have such reward.\n\nTherefore, you can quickly approximate the loss and the gradient through sampling. And the samples are independent of the output of the network, but rather fixed given $\\tau$ and ground-truth. This is as opposed to sampling the network and sampling around the predicted label.\n\n#### 2.2 Sampling from the exponentiated payoff distribution\n\nNote that $q$ should be defined such that the entropy is fixed given $\\tau$. The paper notes that given the ground-truth label, if the loss is hamming distance or negative edit distance, then samples can be put into buckets of same loss (loss is discrete). Then One possible family of distributions controlled by $\\tau$ is given by a normalized count reweighted by $\\exp\\{e/\\tau\\}$ for each bucket. By varying $\\tau$, you can go from uniform distribution ($\\tau=\\infty$) to delta ($\\tau=0$). Eq. 11 shows the simplified count for each bucket to be used for normalization and further sampling. As noted above the equation, edge cases like repetitions is ignored in this count to make the computation easy.\n\nGiven a fixed entropy $q$, then you can compute the RML loss by sampling from $q$ by first selecting an edit distance, and then sampling from the corresponding bucket.\n\n#### Footnotes\nI was not satisfied with the results. I didn't quite follow section 3.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1609.00150"
    },
    "1247": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BabenkoAL16",
        "transcript": "**Motivation**\n\nA possible approach to industrialise deep and shallow embeddings in various retrieval tasks is to decouple the representations of documents (video clips/items in recommendations) and queries (users in recsys). Under this scheme, the degree of matching (\"relevance\") $r$ of the document with representation $x$ and the query with representation $q$ is calculated as a softmax function: \n$$\nr := \\frac{e^{q^Tx}}{\\sum_i e^{q^Tx_i}}\n$$\nAt learning time, both representations are independent until the last softmax layer. This allows a convenient run-time scheme: the representations of the documents are pre-calculated and stored in some form of index, and the query-document relevance is calculated online, at the query-time. As for the fixed query the relevance is monotone w.r.t. the dot product $q^Tx$, the problem of efficient ranking boils down to efficiently finding documents/items with high values of this dot product.\n\nSuch an approach is used in papers describing Youtube [1] and Yahoo [2] systems.\n\nThis leads to the following **problem statement** (informal): given a dataset of items $\\mathbb{X}$ and a sample of queries $\\mathbb{Q}$, how can one compress the dataset $\\mathbb{X}$ such that the compressed dataset allows to calculate dot product queries w.r.t. uncompressed $q$ with as small distortion as possible? In this paper, squared Euclidean distance similarity queries are also considered.\n\nFormally, we're looking for a compressed representation $x$ for $\\hat x$ which minimises the error:\n\n(in the case of dot-product)\n$$\nL_{dot} = \\sum_{x \\in \\mathbb{X}, q \\in \\mathbb{Q}} \\left(q^Tx - q^T \\hat x \\right)^T\\left(q^Tx - q^T \\hat x \\right)\n$$\n(in the case of square of squared Euclidean distance)\n$$\nL_{Eucl} = \\sum_{x \\in \\mathbb{X}, q \\in \\mathbb{Q}} \\left((q - x)^2 - (q - \\hat x)^2 \\right)^2\n$$\n\nIn the centre of the paper is the observation that these two *pairwise* loss functions can be reduced to per-point distortion losses in some modified space. In the case of $L_{dot}$:\n$$\nL_{dot} = \\sum_{x \\in \\mathbb{X}, q \\in \\mathbb{Q}} \\left(q^Tx - q^T \\hat x \\right)^T\\left(q^Tx - q^T \\hat x \\right) = \\sum_{x \\in \\mathbb{X}}(x - \\hat x)^T \\left( \\sum_{q \\in \\mathbb{Q}} q q^T \\right)(x - \\hat x)\n$$\nSince $ \\left( \\sum_{q \\in \\mathbb{Q}} q q^T \\right)$ is a semi-definite positive matrix, we can factorise it as $C^TC$ and plug it back:\n$$\nL_{dot} =\\sum_{x \\in \\mathbb{X}}(Cx - C\\hat x)^T(Cx - C\\hat x)=\\sum_{z \\in \\mathbb{Z}}(z - \\hat z)^T(z - \\hat z)\n$$\nwhere $\\mathbb{Z} = \\{z = C x | x \\in \\mathbb{X}\\}$ is a dataset obtained by modifying $\\mathbb{X}$ by applying matrix $C$. That reduces the problem of minimising the distortion of dot-product estimates to the problem of reducing the distortion of individual points in $\\mathbb{Z}$.\n\nAssuming that we have an efficient way to compress and store individual points when minimising squared distance distortion $(z - \\hat z)^2$, we can turn $\\mathbb{X}$ into $\\mathbb{Z}$, then compress & index $\\mathbb{Z}$.\nLuckily, we have Product Quantisation, Optimised Product Quantisation, Additive Product Quantisation, etc to make the required per-point compression $z \\rightarrow \\hat z$, that minimise the per-point loss.\n\nHow one can use compressed $\\mathbb{Z}$ at run-time? The query itself must be modified: $q \\rightarrow r = (C^{-1})^Tq$. Then, by finding an estimate of $r^T z$ we will find our estimate $q^Tx$:\n\n$$\nr^T z = q^TC^{-1}z = q^TC^{-1} \\cdot C x = q^T x \n$$\n\nSimilar reduction is performed for the pairwise squared Euclidean distance loss.\n\nNext, the authors demonstrate the the obtained estimates for the distance and dot product are unbiased when the underlying compression in $\\mathbb{Z}$-space is performed by Optimised Product Quantisation (OPQ).\n\nThe paper concludes with several experiments that demonstrate that the Pairwise Quantisation better recovers dot-product and squared Euclidean distance than OPQ.\n\n**Overall** the approach seems to be practical, elegant and addresses an important problem. One possible issue is that (authors mention it) the optimisation is performed w.r.t. *all* pairs of sampled queries and datapoints. In practical applications, one only needs to accurately recover dot-product/distance to the closest vectors. For instance, the correct ranking of the documents on positions 100-200 does not matter; while positions 1-10 are extremely important.\n\n[1] https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/45530.pdf\n\n[2] http://www.kdd.org/kdd2016/papers/files/adf0361-yinA.pdf",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.01550"
    },
    "1248": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BertasiusTYS16",
        "transcript": "#### Introduction\nMost recent semantic segmentation algorithms rely (explicitly or implicitly) on FCN. However, the large receptive field and many pooling layers lead to low spatial resolution in the deep layers. On top of that, the lack of explicit pixelwise grouping mechanism often produces spatially fragmented and inconsistent results. In order to solve this, the authors proposed a Convolutional Random Walk Networks (RWNs) to diffuse the FCN potentials in a random walk fashion based on learned pixelwise affinities to enforce the spatial consistency of segmentation. One main contribution by the authors is that RWN needs only 131 additional parameters than the DeepLab architecture and yet outperform DeepLab by 1.5% on Pascal SBD dataset.\n\n##### __1. Review of random graph walks__\nIn graph theory, an undirected graph is defined as $G=(V,E)$ where $V$ and $E$ are vertices and edges respectively. Then a random walk in a graph is characterized by the transition probabilities between vertices. Let $W$ be a $n \\times n$ symmetric *affinity* matrix where $W_{ij}$ encodes the similarity of nodes $i$ and $j$ (usually with Gaussian affinities). Then, the random walk transition matrix, $A$ is defined as $A = D^{-1}W$ where $D$ is a $n \\times n$ diagonal *degree* matrix. Let $y_t$ denotes the nodes distribution at time $t$, the distribution after one step of random walk process is $y_{t+1}=Ay_{t}$. The random walk process can be iterated until convergence.\n\n##### __2. Overall architecture__\nThe overall architecture consists of 3 branches:\n* semantic segmentation branch (which is FCN)\n* pixel-level affinity branch (to learn affinities)\n* random walk layer (diffuse FCN potentials based on learned affinities)\n![RWN](http://i.imgur.com/au5PoY2.png)\n\n##### __A) Semantic segmentation branch__\nThis authors employed DeepLab-LargeFOV FCN architecture as the semantic segmentation branch. As a result, the resolution of $fc8$ activation will be of 8 times lower than that of the original image. Let $f \\in \\mathbb{R}^{n \\times n \\times m}$ denote the $fc8$ activations where $n$ refers to height/ width of image and $m$ denotes the features dimension. \n\n##### __B) Pixelwise affinity branch__\nHand-crafted affinities are usually in the form of Gaussian, i.e. $\\exp\\frac{(x-y)^2}{\\sigma^2}$ where $x$ and $y$ are usually pixel intensities while $\\sigma$ control the smoothness. In this work, the authors argued that the learned affinities work better than the hand-crafted color affinities. Apart from RGB features, $conv1\\texttt{_}1$ (64 dimensional) and $conv1\\texttt{_}2$ (64 dimensional) are also employed to build the affinities. In particular, the 3 features are first downsampled by 8 times to match that of $fc8$ and concatenated to form a matrix of $n \\times n \\times k$ where $k=131$ (since 3+64+64=131). Then, the $L1$ pairwise distance is computed for __each__ dimension to form a __sparse__ matrix, $F \\in \\mathbb{R}^{n^2 \\times n^2 \\times 131}$ (the sparsity is due to the fact the distance is computed for pixel pairs within radius of $R$ only). A $1 \\times 1 \\times 1$ $conv$ is attached (dimension of kernel is therefore 131, which attributes to the only additional learned parameters in this work) followed by an $\\exp$ layer, forming a sparse affinity matrix, $W \\in \\mathbb{R}^{n^2 \\times n^2 \\times 1}$. An Euclidean loss layer is attached to optimize w.r.t. the ground truth pixel affinities obtained from semantic segmentation annotations.\n\n##### __C) Random walk layer__\nThe random walk layer diffuses the $fc8$ potentials from semantic segmentation branch using the learned pixelwise affinity $W$. First, the random walk transition matrix $A$ is computed by row-normalizing $W$. The diffused segmentation prediction is therefore $\\hat{y}=A^tf$ to simulate $t$ random walk steps. The random walk layer is finally attached to a softmax layer (with cross-entropy loss) and trained end-to-end.\n\n##### 3. Discussion\n* Although RWN demonstrates the improvement of the coarse prediction, post-processing such as Dense-CRF or Graph Cuts is still required. \n* The authors showed that the learned affinity is better than the hand-crafted color affinities. This is probably due to the findings that $conv1\\texttt{_}2$ features helped improving the prediction.\n* The authors observed that a single random walk steps is the optimal.\n* For the pixelwise affinity branches, only $conv1\\texttt{_}1$, $conv1\\texttt{_}2$ and RGB cues are used due to their same spatial dimension as the original image. Intuitively, only low level features are required to ensure that higher level features (from later layers) won't diffuse across boundaries (which is encoded in earlier layers).\n\n#### Conclusion\nThe authors proposed a RWN that diffuses the higher level (more abstract) features based on __learned__ pixelwise affinities (lower level cues) in a random walk fashion.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1605.07681"
    },
    "1249": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1506.03478",
        "transcript": "#### Introduction\nThis [paper](https://github.com/lucastheis/ride) introduces *recurrent image density estimator* (RIDE), a generative model by combining a *multidimensional* recurrent neural network with mixtures of experts to model the distribution of natural image. In this work, the authors used  *spatial* LSTMs (SLSTM) to capture the semantics in the form of hidden states where these hidden vectors are then fed into a factorized *factorized mixtures of conditional Gaussian scale mixtures* (MCGSMs) to predict the state of the corresponding pixels.\n\n##### __1. Spatial long short-term memory (SLSTM)__\nThis is a straightforward extension of the multidimensional RNN in order to capture long range interaction. Let $\\mathbf{x}$ be a grayscale image patch and $x_{ij}$ be the intensity of pixel at location ${ij}$. At each location $ij$, each LSTM unit perform the following operations:\n\n$\\mathbf{c}_{ij} = \\mathbf{g}_{ij} \\odot \\mathbf{i}_{ij} + \\mathbf{c}_{i,j-1} \\odot \\mathbf{f}^c_{ij} + \\mathbf{c}_{i-1,j} \\odot \\mathbf{f}^r_{ij} $ \n\n$\\mathbf{h}_{ij} = \\tanh(\\mathbf{c}_{ij} \\odot \\mathbf{o}_{ij})$\n\n$\\begin{pmatrix} \n\\mathbf{g}_{ij} \\\\ \\mathbf{o}_{ij} \\\\ \\mathbf{i}_{ij} \\\\ \\mathbf{g}_{ij}\\\\ \\mathbf{f}_{ij}^r\\\\ \\mathbf{f}_{ij}^c\n\\end{pmatrix} = \\begin{pmatrix} \\tanh \\\\ \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\sigma\\\\ \\sigma  \\end{pmatrix} T_{\\mathbf{A,b}}  \\begin{pmatrix} \\mathbf{x}_{<ij} \\\\ \\mathbf{h}_{i,j-1} \\\\ \\mathbf{h}_{i-1,j} \\end{pmatrix} $\n\nwhere $\\mathbf{c}_{ij}$ and $\\mathbf{h}_{ij}$ are memory units and hidden units respectively. Note that, there are 2 different forget gates $\\mathbf{f}^c_{ij}$ and $\\mathbf{f}^r_{ij}$ for the 2 preceding memory states $\\mathbf{c}_{i,j-1}$ and $\\mathbf{c}_{i-1,j}$. Also note that $\\mathbf{x}_{<ij}$ here denotes a set of *causal neighborhood* by applying Markov assumption.\n\n![ride_1](http://i.imgur.com/W8ugGvl.png)\nAs shown in Fig. C, although the prediction of a pixel depends only on its neighborhood (green) through feedforward connections, there is an indirect connection to a much larger region (red) via recurrent connections.\n\n##### __2. Factorized mixtures of conditional Gaussian scale mixtures__\nA generative model can usually be expressed as $p(\\mathbf{x};\\mathbf{\\theta}) = \\prod_{i,j} p(x_{ij}|\\mathbf{x}_{<ij}; \\mathbf{\\theta})$ using chain rule. One way to improve the representational power of a model is to introduce different sets of parameters for each pixel, i.e. $p(\\mathbf{x}; \\{ \\mathbf{\\theta} \\}) = \\prod_{i,j} p(x_{ij}|\\mathbf{x}_{<ij}; \\mathbf{\\theta}_{ij})$. However, untying shared parameters will lead to drastic increase of parameters. Therefore, the author applied 2 simple common used assumptions:\n1. __Markov  assumption__: $\\mathbf{x}_{<ij}$ is limited to small neighborhood around $x_{ij}$ (causal neighborhood)\n2. __Stationary and shift invariance__: the same set of $\\mathbf{\\theta}_{ij}$ is used for every location ${ij}$ which corresponds to recurrent structure in RNN.\n\nTherefore, the hidden vector from SLSTMs can be fed into the MCGSM to predict the state of corresponding label, i.e. $p(x_{ij} | \\textbf{x}_{<ij}) = p(x_{ij} | \\textbf{h}_{ij})$.\n\nThe conditional distribution distribution in MCGSM is represented as a mixture of experts:\n\n$p(x_{ij} | \\mathbf{x}_{<ij}; \\mathbf{\\theta}_{ij}) = \\sum_{c,s} p(c, s | \\mathbf{x}_{<ij}, \\mathbf{\\theta}_{ij}) p (x_{ij} | \\mathbf{x}_{<ij}, c, s, \\mathbf{\\theta}_{ij})$.\n\nwhere the first and second term correspond to gate and experts respectively. To further reduce the number of parameters, the authors proposed using a *factorized* MCGSM in order to use larger neighborhoods and more mixture components. (*__Remarks__: I am not too sure about the exact training of MCGSM, but as far as I understand, the MCGSM is firstly trained end-to-end with SLSTM using SGD with momentum and then finetuned using L-BFGS after each epoch by fixing the parameters of SLSTM.*)\n\n* For training:\n\n```\nfor n in range(num_epochs):\n\tfor b in range(0, inputs.shape[0] - batch_size + 1, batch_size):\n\t\t# compute gradients\n\t\tf, df = f_df(params, b)\n\n\t\tloss.append(f / log(2.) / self.num_channels)\n\n\t\t# update SLSTM parameters\n\t\tfor l in train_layers:\n\t\t\tfor key in params['slstm'][l]:\n\t\t\t\tdiff['slstm'][l][key] = momentum * diff['slstm'][l][key] - df['slstm'][l][key]\n\t\t\t\tparams['slstm'][l][key] = params['slstm'][l][key] + learning_rate * diff['slstm'][l][key]\n\n\t\t# update MCGSM parameters\n\t\tdiff['mcgsm'] = momentum * diff['mcgsm'] - df['mcgsm']\n\t\tparams['mcgsm'] = params['mcgsm'] + learning_rate * diff['mcgsm']\n```\n\n* Finetuning (part of the code)\n\n```\nfor l in range(self.num_layers):\n\tself.slstm[l] = SLSTM(\n\t\tnum_rows=hiddens.shape[1],\n\t\tnum_cols=hiddens.shape[2],\n\t\tnum_channels=hiddens.shape[3],\n\t\tnum_hiddens=self.num_hiddens,\n\t\tbatch_size=min([hiddens.shape[0], self.MAX_BATCH_SIZE]),\n\t\tnonlinearity=self.nonlinearity,\n\t\textended=self.extended,\n\t\tslstm=self.slstm[l],\n\t\tverbosity=self.verbosity)\n\n\thiddens = self.slstm[l].forward(hiddens)\n\n# finetune with early stopping based on validation performance\nreturn self.mcgsm.train(\n\thiddens_train, outputs_train,\n\thiddens_valid, outputs_valid,\n\tparameters={\n\t\t'verbosity': self.verbosity,\n\t\t'train_means': train_means,\n\t\t'max_iter': max_iter})\n```",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1506.03478"
    },
    "1250": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1608.03609",
        "transcript": "From this frame of a video to next frame, maybe the pixel change a lot, but the semantic content changes slowly. Reflect on the the neutral network, shallow layers change more than deeper layers. \n\nSo they use a clock to decide if need to update the deeper layers or just use the previews output result. \n\nThe clock triggers by the differences of output of some layer on previous and next frames. The condition of clock execution can be fixed or learned. ",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1608.03609"
    },
    "1251": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1607.08284",
        "transcript": "When high-mass (>= 8 Msun) stars end their lives in blinding explosions known as core-collapse supernovae, they can rip through the fabric of space-time and create black holes with similar masses, known as stellar-mass black holes. These vermin black holes dwarf in comparison to their big brothers, supermassive black holes that typically have masses of 10\u2076-10\u2079 Msun. However, as vermin usually do, they massively outnumber supermassive black holes. It is estimated that 10\u2078-10\u2079 stellar mass black holes are crawling around our own Milky Way, but we\u2019ve only caught sight of a few dozens of them.\n\nAs black holes don\u2019t emit light, we can only infer their presence from their effects on nearby objects. All stellar mass black holes detected so far reside in binary systems, where they actively accrete from their companions. Radiation is emitted as matter from the companion falls onto the accretion disk of the black hole. Isolated black holes don\u2019t have any companions, so they can only accrete from the surrounding diffuse interstellar medium, producing a very weak signal. That is why isolated black holes, which make up the bulk of the black hole population, have long escaped our discovery. Perhaps, until now.\n\nThe authors of today\u2019s paper turned the intense gravity of black holes against themselves. While isolated black holes do not produce detectable emission, their gravity can bend and focus light from background objects. This bending and focusing of light through gravity is known as gravitational lensing. Astronomers categorize gravitational lensing based on the source and degree of lensing: strong lensing (lensing by a galaxy or a galaxy cluster producing giant arcs or multiple images), weak lensing (lensing by a galaxy or a galaxy cluster where signals are weaker and detected statistically), and microlensing (lensing by a star or planet). During microlensing, as the lens approaches the star, the star will brighten momentarily as more and more light is being focused, up until maximum magnification at closest approach, after which the star gradually fades as the lens leaves. This effect is known as photometric microlensing (see this astrobite). Check out this microlensing simulation, courtesy of Professor Scott Gaudi at The Ohio State University: the star (orange) is located at the origin, the lens (open red circle) is moving to the right, the gray regions trace out the lensed images (blue) as the lens passes by the star, while the green circle is the Einstein radius. The Einsten radius is the radius of the annular image when the observer, the lens, and the star are perfectly aligned.\n\nSomething more subtle can also happen during microlensing, and that is the shifting of the center of light (on a telescope\u2019s detector) relative to the true position of the source \u2014 astrometric microlensing. While photometric microlensing has been widely used to search for exoplanets and MACHOs (massive astrophysical compact halo objects), for instance by OGLE (Optical Gravitational Lensing Experiment), astrometric microlensing has not been put to good use as it requires extremely precise measurements. Typical astrometric shifts caused by stellar mass black holes are sub-milliarcsecond (sub-mas), whereas the best astrometric precision we can achieve from the ground is typically ~1 mas or more. Figure 1 shows the signal evolution of photometric and astrometric microlensing and the astrometric shifts caused by different masses.\n\nhttps://astrobites.org/wp-content/uploads/2016/08/fig1-2.png\n\nFig. 1 \u2013 Left panel shows an example of photometric magnification (dashed line) and astrometric shift (solid line) as function of time since the closest approach between the lens and the star. Note that the peak of the astrometric shift occurs after the peak of the photometric magnification. Right panel shows the astrometric shift as a function of the projected separation between the lens and the star, in units of the Einstein radius, for different lens masses. [Figure 1 in paper]\n\nIn this paper, the authors used adaptive optics on the Keck telescope to detect astrometric microlensing signals from stellar mass black holes. Over a period of 1-2 years, they monitored three microlensing events detected by the OGLE survey. As astrometric shift reaches a maximum after the peak of photometric microlensing (see Figure 1), astrometric follow-up was started post-peak for each event. The authors fit an astrometric lensing model to their data, not all of which were taken under good observing conditions. Figure 2 shows the results of the fit: all three targets are consistent with linear motion within the uncertainties of their measurements, i.e. no astrometric microlensing. Nonetheless, as photometric microlensing is still present, the authors used their astrometric model combined with a photometric lensing model to back out various lensing parameters, the most important one being the lens masses. They found one lens to have comparable mass as a stellar-mass black hole, although verification would require future observations.\n\nhttps://astrobites.org/wp-content/uploads/2016/08/fig2-2.png\n\nDespite not detecting astrometric microlensing signals, the authors demonstrated that they achieved the precision needed in a few epochs; had the weather goddess been on their side during some critical observing periods, some signals could have been seen. This study is also the first to combine both photometric and astrometric measurements to constrain lensing event parameters, ~20 years after this technique was first conceived. For now, we\u2019ll give stellar-mass black holes a break, but it won\u2019t be long until we catch up.\n\nby Suk Sien Tie\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1607.08284v1"
    },
    "1252": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/ChenZW13",
        "transcript": "Idea:\n- Fast: simply a linear projection from image feature to semantic tag: learn linear projection **W**. Especially, in testing time, it is almost O(1) complexity compared with the nearest neighbor methods with at least O(N) complexity. \n- Enrich incomplete tags: learn tag enrichment projection **B** that turns on likely co-occurring tags with existing ones.\nhttps://i.imgur.com/I1DUInN.png\n\nMarginalized blank-out regularization\n- Assume observed tags are corrupted, approximate the unknown corrupting distribution with piecewise uniform distribution.\n- Stacking: multi-layer linear projection. Reconstruct tags that do not co-occur together but tend to appear within similar contexts. \n- Rare tags and Non-Uniform Corruption: only optimize tags that have recall below some threshold in the validation set.\n",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v28/chen13j.html"
    },
    "1253": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GordoARL16",
        "transcript": "\n**Contributions:**\n- Triplet ranking loss, implemented in three-stream Siamese network\n-  Integrate region proposal network in system. All operations are derivative, making the system end-to-end trainable. \n- Proposed dataset cleaning method, which is critical for performance boost.\n- Performance surpasses previous global descriptors and most of local based descriptors in Landmarks dataset.\n\n**Training:**\n- Sample triplets, triplet hinge loss:\n $L(I_q, I^+, I^-)=max(0, m+q^Td^- - q^Td^+)$\n- Since only convolutional layers are used in CNN, and aggregation does not require a fixed input size, full image resolution could be used.\n\n**Network data flow:**\n- Use convolutional layers of pre-trained network to extract activation features.\n- Max-pooling in different regions, using multi-scale rigid grid with overlapping cells. Note that ROI pooling is differentiable.\n- L2 normalize region features, whiten with PCA and l2-normalize again. PCA projection can be implemented with a shifting and a FC layer.\n- Aggregate: sum and l2 normalize.\n- Dot product similarity of image vector is approximately many-to-many region matching.\n\n**Region Proposal Network**\n- Objective function is multi-task loss, which combines classification loss and regression loss.\n- When applied, need to perform non-maximum suppression, keep top K proposals for each image.\n\n**Landmark Dataset Cleaning**\n- Construct image graph, with edges as similarity score. The score is computed offline, using invariant keypoint matching and spatial verification.\n- Extract connected components in graph. They correspond to differnt profiles of a landmark.\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.01325"
    },
    "1254": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=thies2016face",
        "transcript": "https://www.youtube.com/watch?v=_S1lyQbbJM4",
        "sourceType": "blog",
        "linkToPaper": "http://www.graphics.stanford.edu/~niessner/papers/2016/1facetoface/thies2016face.pdf"
    },
    "1255": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/cgf/FerstlAWWT16",
        "transcript": "https://www.youtube.com/watch?v=nfPBT71xYVQ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1111/cgf.12825"
    },
    "1256": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=IizukaSIGGRAPH2016",
        "transcript": "https://www.youtube.com/watch?v=MfaTOXxA8dM",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf"
    },
    "1257": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/KarpathyF14",
        "transcript": "https://www.youtube.com/watch?v=e-WB4lfg30M\n\n\nThis technique is a combination of two powerful machine learning algorithms:\n- convolutional neural networks are excellent at image classification, i.e., finding out what is seen on an input image,\n- recurrent neural networks that are capable of processing a sequence of inputs and outputs, therefore it can create sentences of what is seen on the image.\n\nCombining these two techniques makes it possible for a computer to describe in a sentence what is seen on an input image.  ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1412.2306"
    },
    "1258": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1016/1044-0305(94)80016-2",
        "transcript": "In proteomics, a popular method to identify peptides is mass spectrometry. An experimental tandem mass spectrum consists of mass peaks that stem from fragmenting a peptide to fragment ions. \nTo identify the peptide, that produced the experimental spectrum, all peptides that could produce the seen fragment ions must be analysed. Unfortunately, the space of possible peptides to search against is large.  The paper simplifies this problem by searching against a peptide database.\n\nThe approach describes a 4-step method:\n\n- Data reduction: only the 200 most abundant peaks of the experimental spectrum are kept\n- Search: peptides with a mass similar to the experimental spectrum's precursor are selected\n- Scoring: the score for each selected peptide is based on the number of predicted fragment ions seen in the experimental spectrum.\n- Cross-correlation: For the top 500 scoring peptides, theoretical spectra is constructed. These spectra are are evaluated by cross-correlation with the experimental spectrum. The top scoring peptide is considered the identified peptide.\n\nSince a database of known peptides is needed for this approach, it can not be used for de-novo peptide identification, meaning identifying peptides for species where no sequenced genome or proteome is available.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/1044-0305(94)80016-2"
    },
    "1259": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1021/ac051319a",
        "transcript": "To identify a peptide, an experimental mass spectrum of the peptide is matched against a *protein sequence database*. More specifically, the spectrum is compared against artificially generated *hypothetical spectra* that are based on the databases collection of known possible peptides. PepHMM is a scoring function to compare an experimental spectrum with a hypothetical spectrum. It estimates the likelihood that the experimental spectrum comes from the same peptide that was used to generate the hypothetical spectrum. Only $b$, $y$, $b-H_2O$, $y-H_2O$, $a$, $b^{2+}$, and $y^{2+}$ ions are considered for the comparison.\n\nPepHMM distinguishes\n- **matches**: experimental peaks that correspond to hypothetical ions\n- **missing**: meaning hypothetical ions that have no corresponding experimental peaks\n- noisy peaks: meaning peaks that do not correspond to any hypothetical ion. \n\nMatches and missing are modelled by a Hidden Markov Models (HMM). For one specific fragmentation, a hidden state represents which of the ions have been observed and which are missing. For example, for the first possible fragmentation, the $b_1$ ion is not observed, but the corresponding $y_n$ ion is. PepHMM only considers exactly five of all possible fragmentations: the first two, the middle and the last two. Parameters of the HMM include the parameters for the assumed distributions (exponential for peak intensities and normal for match tolerance), respectively for each ion-type.\n\nThe complete database matching has the following steps.\n1. The search space is limited by the experimental spectrum's precursor mass. This limits the number of potential peptides.\n2. For each potential peptide a hypothetical spectrum is generated.\n3. For each hypothetical spectrum, a **probabilistic score** is calculated with PepHMM as well as a **Z-score** (by simulating 500 peptides with a similar precursor) and an **E-score**, which is a ranks the peptides by their Z-score.\n\nPepHMM's parameters are trained by Expectation Maximization (EM). The matching results are compared to MASCOT. PepHMM outperforms MASCOT in accuracy in two different tests. Furthermore the number of predicted peptide sequences is compared between PepHMM, MASCOT, and SEQUEST. PepHMM, has the most predictions, but there is also a big overlap in predictions between the three compared. \n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1021/ac051319a"
    },
    "1260": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1198/004017008000000064",
        "transcript": "This paper implements a parameter expansion technique with Markov Chain Monte Carlo (MCMC) for the multivariate probit model (MVP). Since the covariance matrix is not identifiable from multivariate ordinal data, a correlation matrix restriction is appropriate with MVP. This is a problem for Bayesian inference, as the structural constraints on correlation matrices make the selection of a good prior and the estimation of a posterior difficult.   \n\nThe main advantage of parameter expansion with this model is that we can draw the covariance matrix in the expanded parameter space from an inverse Wishart distribution with a simple Gibbs sampler. At each iteration, all parameters are drawn from the expanded space and rescaled to their original form. Since the parameter draws are only dependent on the latest latent variable, the dependence between iterations is reduced. This technique improves the mixing and convergence rate of the MCMC.\n\nParameter expansion for the MVP model is also easy to implement for stochastic search variable selection (SSVS). An indicator variable $\\lambda_i$ is included as a parameter, where each 1 in $\\lambda_i$ is associated to a covariate selected for the model.\n\nGibbs sampler algorithm:\n\nLet $q$-variate vector $Z_i \\sim MVN( \\beta X_i, R )$ be a latent variable, truncated in all dimensions according to the corresponding cutpoints, where $R$ is a $q \\times q$ correlation matrix.\n\nLet $\\beta$ be a $q \\times p$ regression coefficient matrix, $\\gamma$ be the set of cutpoints where $\\gamma_0 = -\\infty$, $\\gamma_1 = 0$ and $\\gamma_k = \\infty$, and $k$ be the number of categories for the response.\n\n\nMap $Z $ onto $W$:\n\nCycle through draws of the univariate truncated $Z_{i,j} \\sim N(\\mu_{i,j} - \\frac{1}{R^{-1}_{j,j}}R^{-1}_{j,-j}(Z_{i,-j}-\\mu_{i,-j}),  \\frac{1}{R^{-1}_{j,j}})$, where $\\mu_i = \\beta X_i$, to get the truncated multivariate normal distribution of $W_i$.\n\n$R \\rightarrow \\Sigma$:\n\nDraw $\\Sigma | Z \\sim IW(n-p+q-1, WW^T - WX^T(XX^T)^{-1}XW^T)$\n\n$\\beta \\rightarrow \\alpha$:\n\nDraw $\\alpha | W, \\Sigma \\sim N_{q,p}(WX^T(XX^T)^{-1}, \\Sigma \\otimes (XX^T)^{-1})$\n\nwhere $N_{q,p}$ is a matrix variate normal distribution.\n\n$\\gamma \\rightarrow \\theta$:\n\nDraw $\\theta_{j,k+1} | W, Y \\sim Unif(max_i(\\{W_{ij}|Y_{ij}=c\\}), min_i(\\{W_{ij}|Y_{ij} = c + 1\\})) $ for all free cutpoints.\n\nRescale the parameters back to the original space:\n\n$Z_{i,j} = \\dfrac{W_{i,j}}{\\sqrt{\\Sigma_{j,j}}}$\n\n$R_{i,j} = \\dfrac{\\Sigma_{i,j}}{\\sqrt{\\Sigma_{i,i}\\cdot\\Sigma_{j,j}}}$\n\n$\\beta_{k,j} = \\dfrac{\\alpha_{k,j}}{\\sqrt{\\Sigma_{j,j}}}$\n\n$\\gamma_{j,k+1} = \\dfrac{\\theta_{j,k+1}}{\\sqrt{\\Sigma_{j,j}}}$\n\nRepeat.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1198/004017008000000064"
    },
    "1261": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LeeS00",
        "transcript": "NMF aims to find two matrices W and H, such that V=W H.There are two cost functions as follows:\n\nLeast-squares error: $||V \u2013 WH||^2$\n\nDivergence: $D(V || WH)$\n\nHowever, when we try to optimize the  the functions $||V \u2013 WH||^2$ and  $D(V || WH)$. They are convex in W only or H only, and they are not convex in both variables together. \nTo solve this problem, the authors build a new update rules called \"multiplicative versus additive update rules\"  and  $||V \u2013 WH||^2$ and $D(V || WH)$  are non-increasing under the update rules. \n\nMultiplicative versus additive update rules: $H_{\\alpha \\mu}\\leftarrow H_{\\alpha \\mu} + \\eta_{\\alpha \\mu}\\bigg[\\sum\\limits_{i} W_{i\\alpha}\\frac{V_{i\\mu}}{(WH)_{i\\mu}}-\\sum\\limits_{i} W_{i\\alpha}\\bigg]$\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization"
    },
    "1262": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LeeS00",
        "transcript": "From this paper, my understanding is that NMF performance an unsupervised learning algorithm which decompose the data matrix into two nonnegative sub matrices. The final result is a compress of the original data matrix. \"Each data vector $v$ can be generated by a linear combination of the columns of $W$, weighted by the components of $h$. Therefore $W$ can be regarded as containing a basis\nthat is optimized for the linear approximation of the data in $V$. Since relatively few basis vectors are used to represent many data vectors, good approximation can only be achieved.\" Here I regard this method can generates new features for current dataset and could be used in dimension reduction like PCA but with non-negative result. \n\nTo achieve the factorization, they proposed two cost functions, one is just ordinary euclidean distance between original dataset and the production of two factorized small matrices. The other one used the KL-divergence to perform the same goal. The only difference is the later one is not symmetric. \n\nDespite the difference of cost function, the updating formula are similar. The idea, in my understanding, comes from the ordinary gradient descent. They connected the updating rule: $H_{av} = H_{av}+\\eta_{a\\mu}[(W^TV)_{a\\mu}-(W^TWH)_{a\\mu}]$ with gradient descent which $\\eta_{a\\mu}$ like a learning rate and the multiplier looks like a tiny difference or the first order of cost function at variable $H$. \n\nFinally, they utilized the concept of auxiliary function and updating rule $h^{t+1} = arg\\min_h G(h, h^t)$ to guarantee the nonincreasing of cost function $F$. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization"
    },
    "1263": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LeeS00",
        "transcript": "Two different multiplicative algorithms for NMF are proposed. Both of them try to rescaled the gradient descent, and choose the optimal rescaling factor to update the W and H in each iteration.\nLike W\u2190\u03b1W     And  H\u2190 \u03b2H\n\nThe authors propose two different algorithms based on the definition of the cost functions which help to quantify the quality of the approximation. The first one is constructed based on the Euclidean distance: \u3016||A-B||\u3017^2 . The second one is based on the divergence: D(A||B). Then the new algorithms can be regarded as minimize\u3016 ||V-WH||\u3017^2 with the respect to W and H, and minimize D(V||WH) with the respect to W and H by using gradient descent. For each iteration of the gradient descent, the authors design new update rules (Theorem 1 and Theorem 2) to get the optimal new W and H.\nThe authors provide the proofs of convergence to show that the proposed Theorem 1 and Theorem 2 can help to get the optimal results.\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization"
    },
    "1264": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LeeS00",
        "transcript": "Algorithms for Non-Negative Matrix Factorization \u2013 Short Science Summary:\n\n\nThis paper analyzes two separate iterative methods for calculating NMF decompositions: one that minimizes least-squares error and one that minimizes generalized KL-divergence.\n\nThe cost functions given for the two methods are as follows:\n\nLeast-squares error:\n||A \u2013 B||^2 = \u2211_(ij) (Aij*Bij)^2\n\nDivergence:\nD(A||B) = \u2211_ij (Aij log\u2061(Aij/Bij) - Aij+Bij)\n\nThe authors found that using the below multiplicative update rules for each iteration of the corresponding optimization algorithm results in a good compromise between run speed and ease of implementation:\n\n[Thm 1]\n\n[Thm 2]\n\nThe authors go on to prove convergence to at least a locally optimal solution for both of the above choices of cost function.\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization"
    },
    "1265": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/KaiserS15",
        "transcript": "# Neural GPUs Learn Algorithms\n\nAuthors: \u0141ukasz Kaiser, Ilya Sutskever. http://arxiv.org/abs/1511.08228\n\nThis originally appeared here: https://github.com/ProofByConstruction/better-explanations/blob/master/summaries/1511.08228.md\n\n## Short Version\n\nUsing convolutions on embedded symbols in a recurrent-looking setting allows training of what is essentially a cellular automaton which can perform various algorithms and generalize to sequences of very long lengths.\n\n## Problem Setting\n\nWe want to teach neural nets to learn algorithms (e.g. copy, reverse, binary addition or multiplication, etc.). Other approaches to this include sequence to sequence modeling (usually via some form of [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)). Related work of seq-seq approaches includes [Neural Turing Machines](http://arxiv.org/abs/1410.5401), [Stack RNNs](http://arxiv.org/abs/1503.01007), [Pointer Networks](http://arxiv.org/abs/1506.03134), and [Grid LSTM](http://arxiv.org/abs/1507.01526). Related but different, see [Neural Programmer-Interpreters](http://arxiv.org/abs/1511.06279).\n\nThese types of problems are particularly difficult for neural approaches because as sequence length grows, so too does the necessary computation. Furthermore generalization can be difficult, since it's possible for networks to effectively learn length-dependent rules that break down on longer sequences. This mostly comes from the fact that it's difficult to encode a symbolic, rule-based algorithmic approach in a neural net (hence all of the different approaches above and the active research in this area).\n\nThe Neural GPU is somewhat different in its approach compared to other sequence-sequence models in that it don't exactly take in or generate a sequence, but reads in an entire sequence as a static object and generates a fixed size image length equal to that of the input (with special padding characters to indicate gaps). These inputs and outputs are sized dependent on the problem (longer sequences get bigger representations). In this way, the Neural GPU looks much more like a traditional feedforward convolutional net with variable input/output size (and variable computational depth, but we'll get to that later).\n\n## Architecture\n\nThe three stages are\n - an embedder which takes a sequence of symbols to an input \"image\"\n - a stack of convolutional [GRUs](http://arxiv.org/abs/1412.3555) (Gated Recurrent Units) which at each step progressively process their input\n - a decoder which effectively does the reverse of the embedder (takes output representation to symbols)\n\n### Encoder\n\nThe input is a sequence of length n over set of symbols (e.g. {0, 1, +, PAD (=P)}, input is (in reverse-binary) 1010+0111, output, constrained to be the same size, should be 11001PPPP) of size I. We first map the symbols to vectors of length m (which will be the number of channels of our mental image) by looking up each symbol in an embedding matrix of size I*m. Call these embeddings {e_i}. We then create an initial \"mental image\" which is a rectangular volume with width w (set to be 4), height n (the length of the input sequence) and depth m (the embedding size). In the first column we insert the e_i's depthwise, and set all other cells in the volume to be 0.\n\nNote that the size (height) of the input volume is dependent on sequence length. This could be a problem in other architectures, but since everything is done by convolution (as we'll see shortly) this architecture is able to exploit an adaptive-size input rather than reading in sequences (as I imagine convolutional text models might)\n\n### CGRU\n\nThe processing done by the network is over n (the length of the input sequence) time steps using L layers (L=2) of [GRUs](http://arxiv.org/abs/1412.3555), however this is a bit misleading because we only feed input in once (encoded as state), and if we unroll the computation for the fixed length of n time steps, it looks like a purely feedforward net. We might ask why use the GRU architecture at all (instead of only weight sharing across layers) and my guess is that the update and reset gates help in training over long unrollings (ie without them we might experience vanishing gradients -- though it might be worth trying relu activated convolutional layers with shared weights across layers).\n\nAdditionally, we structure the operation occurring in the GRU by forcing it to be a convolution (instead of e.g. fully connected). Intuitively what this CGRU (Convolutional GRU) is doing is processing the \"mental image\" (which remains the same shape over all time steps) in the same way at each point in time, thus bearing resemblance to cellular automata.\n\n### Decoder\n\nWe consider the output much like the input and read out only the first column (of height n, the output sequence length, and depth m, the embedding size). We use a decoder matrix O, of shape m*I which takes the m-dimensionally represented characters and maps to I logit probabilities.\n\n$$l_k = O s_{fin}[0, k, :] = O c_k = [l_k^1, l_k^2, \\dots, l_k^I]$$\n\n  The output at each slot in the output sequence is then whichever character is most probable. For each character, its loss is the probability of the target character (where the probability is softmax over the logits). The loss is then the sum of the logs (~product) of these over all characters\n\n$$L = - \\sum_{k \\in [1, n]} \\log p(c_k^{target}) = - \\sum_{k \\in 1, n} \\log \\frac{e^{l_k^{target}}}{\\sum_j e^{l_k^j}}$$\n\n## Training\n\n There are a bunch of tricks employed in training these.\n  - Dropout (between 6%-13.5%), noise added to gradients (gaussian with mean 0, variance ~ 1/sqrt(step number)), gradient clipping\n  - Grid search for hyperparameter tuning\n  - Curriculum design - the distribution over lengths of presented examples during training shifts towards more difficult problems as mastery of easier ones increases (note that this curriculum is designed by hand)\n  - \"Parameter sharing relaxation\": each GRU is allowed to have different weights at each time step for 6 steps, then cycles that same set of weights (step 7 uses weights at step 1). This allows for more variation in the weights so that the network can achieve a better fit. To get a single weight matrix there's a penalty that progressively increases which is proportional to the differences of weights from the mean.\n\n## Other Notes\n\n  - The number of steps the network is run for is equal to the length of the input sequence (for algorithms which require more computational steps, this architecture is therefore insufficient for perfect calculation). I'm not sure if this is a trick, or just a decision for architectural simplicity. Regardless, it removes the need to learn a stopping mechanism.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.08228"
    },
    "1266": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/etra/AttarWSP16",
        "transcript": "Using a new search task to study the optimal recognition point. We found that the contextual design can affect of fixation landing position. \n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2857491.2857517"
    },
    "1267": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/pvldb/BailisDFGHS13",
        "transcript": "Serializability is the gold standard of consistency, but databases have always provided weaker consistency modes (e.g. Read Committed, Repeatable Read) that promise improved performance. In this paper, Bailis et al. determine which of these weaker consistency models can be implemented with high availability.\n\nFirst, why is high availability important?\n\n1. Partitions. Partitions happen, and when they do non-available systems become, well, unavailable.\n2. Latency. Partitions may be transient, but latency is forever. Highly available systems can avoid latency by eschewing coordination costs.\n\nSecond, are weaker consistency models consistent enough? In short, yeah probably. In a survey of databases, Bailis finds that many do not employ serializability by default and some do not even provide full serializability. Bailis also finds that four of the five transactions in the TPC-C benchmark can be implemented with highly available transactions.\n\nAfter defining availability, Bailis presents the taxonomy of which consistency can be implemented as HATs, and also argues why some fundamentally cannot. He also performs benchmarks on AWS to show the performance benefits of HAT.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.vldb.org/pvldb/vol7/p181-bailis.pdf"
    },
    "1268": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/osdi/LiAPSAJLSS14",
        "transcript": "A parameter server is more or less a distributed key-value store optimized for training machine learning models. For example, imagine we're learning a weight vector $w = (w_1, w_2, w_3)$ using logistic regression. We can distribute $w$ across two shards of the parameter server where one shard stores $(1, w_1)$ and the other stores $(2, w_2)$ and $(3, w_3)$. Worker nodes can then read parts of the weight vector, perform some computation, and write back parts of the weight vector.\n\nThis paper presents an optimized parameter server with the following features:\n\n1. Efficient communication.\n2. Flexible consistency models.\n3. Elastic scalability.\n4. Fault tolerance and durability.\n5. Ease of use.\n\n#### Machine Learning\n\nMany machine learning algorithms try to find a weight vector $w \\in \\mathbb{R}^d$ that minimizes a regularized cost function of the following form:\n\n$$ F(w) = \\Omega(w) + \\sum_{i=1}^n l(x_i, y_i, w) $$\n\nWhen $n$ and $d$ get really big, it becomes intractable to run these algorithms on a single machine. Instead, we have to parallelize the algorithm across multiple machines.\n\nAs an example, consider how to perform distributed batch gradient descent across $m$ workers. We initialize $w$ and store it in a parameter server. Concurrently, each worker begins by reading $\\frac{1}{m}$ of the training data. Then, every worker reads $w$ from the parameter server and computes a gradient with respect to its local training data (actually, it only needs to read the relevant parts of $w$). Then, it pushes its gradient to the parameter server. Once the server receives gradients from every worker, it aggregates them together and updates $w$. Finally, workers pull the updated $w$ and loop.\n\n#### Architecture\n\nA parameter server consists of a bunch of servers that store weights and a bunch of workers that perform computations with the weights (e.g. compute gradients). Servers are organized into a server group managed by a server manager. Workers are organized into multiple worker groups, and each worker group is managed by a task scheduler. The server manager manages which data is assigned to which server. The task scheduler assigns tasks to workers and monitors progress.\n\nParameters are stores as key-value pairs. For example, a weight vector $w \\in \\mathbb{R}^d$ can be stored as a set of pairs $(i, w_i)$ for $1 \\leq 1 \\leq d$. To store sparse vectors more efficiently, only non-zero entries of $w$ must be explicitly stored. If a pair $(i, w_i)$ is missing, the parameter server assumes $w_i = 0$.\n\nMost machine learning algorithms do not update individual entries of a weight vector at a time. Instead, they typically update part of a matrix or part of a vector. To take advantage of this workload pattern, the parameter server allows workers to read and write ranges of values instead of single values at a time. This reduces network overhead.\n\nIn addition to pushing and pulling entries of  $w$, workers can also register user-defined functions to run at a server. For example, a server can compute the gradient of a regularization term.\n\nBy default, the parameter server runs tasks asynchronously. That is, if a worker issues a pull or push request, it does not block. However, the parameter server also allows workers to explicitly mark dependencies between different requests which forces them to serialize.\n\nSome algorithms are robust to weir inconsistencies. These algorithms can often run faster with weaker consistency. The parameter server provides three levels of consistency:\n\n1. Sequential consistency in which every request is totally serialized.\n2. Eventual consistency in which requests are run whenever they please.\n3. Bounded delay in which a request is delayed until all tasks that began \u03c4 time ago have completed.\n\nUsers can also specify that a certain consistency model only apply to a certain subset of key-value pairs as specified by a filter.\n\n#### Implementation\n\nData is partitioned across servers using consistent hashing, and the server manager records the assignment of key ranges to machines. When a new server joins:\n\n1. The server manager assigns a new key range to the server.\n2. The server fetches its data from other servers.\n3. The server manager broadcasts the update to the other servers who relinquish data they no longer own.\n\nThe parameter server uses chain replication to replicate data. Each node forms a chain with the $k$ previous nodes in the hashing ring. Workers send updates to the master which is chain replicated to the next $k$ servers.\n\nLogically, the parameter server tags each key-value pair with a vector clock (though honestly, I'm not exactly sure I understand why). Physically, each range of key-value pairs is associated with a vector clock. This range-based vector clock avoids storing redundant vector clock information.\n\nMessages sent from workers to servers are compressed with Snappy. Moreover, servers cache parts of messages, and workers can send a hash instead of a whole message if they the think a server has it cached.\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li_mu"
    },
    "1269": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/usenix/OngaroO14",
        "transcript": "Modelling a distributed system as a replicated state machine provides the illusion that the distributed system is really just a single machine. At the core of the replicated state machine approach is a replicated log that is kept consistent by a consensus algorithm. Traditionally, consensus has been synonymous with Paxos. Paxos is taught in schools, and most consensus algorithm implementations are based on Paxos. However, Paxos has two main disadvantages:\n\n1. It is hard to understand. Single-decree Paxos is nuanced, and composing single-decree Paxos into multi-Paxos is confusing.\n2. It is hard to implement efficiently. Multi-Paxos is not very well described in the literature, and the algorithm is difficult to implement efficiently without modification.\n\nThis paper presents the Raft consensus algorithm. Raft provides the same performance and safety as multi-Paxos but it is designed to be much easier to understand.\n\nBasics. Every node in a raft cluster is in one of three states: leader, follower, or candidate. The leader receives requests from users and forwards them to followers. Followers are completely passive and receive messages from leaders. Candidates perform leader elections in an attempt to become a leader. In normal operation, there is a single leader, and every other node is a follower.\n\nRaft proceeds in a series of increasingly numbered terms. Each term consists of a leader election followed by (potentially) normal operation. There is exactly one leader elected per term. Moreover, each node participates in monotonically increasing terms. When a node sends a message in Raft, it annotates it with its term. If a leader receives a message from a later term, it immediately becomes a follower. Nodes ignore messages annotated with older terms.\n\nRaft uses two RPCs: RequestVote (for leader election) and AppendEntries (for replication and heartbeats).\n\nLeader Election. Leaders periodically send heartbeats (AppendEntries RPCs without any entries) to followers. As long as a follower continues to receive heartbeats, it continues to be a follower. If a follower does not receive a heartbeat after a certain amount of time, it begins leader election: it increments its term, enters the candidate state, votes for itself, and sends RequestVote RPCs in parallel to all other nodes. Either,\n\n1. It wins. Nodes issue a single vote per term on a first come first serve basis. If a candidate receives a vote from a majority of the nodes, then it becomes leader.\n2. It hears from another leader. If a candidate receives a message from another leader in a term at least as large as it, it becomes a follower.\n3. It times out. It's possible that a split vote occurs and nobody becomes leader in a particular term. If this happens, the candidate times out after a certain amount of time and begins another election in the next term.\n\nLog Replication. During normal operation, a leader receives a request from a client, appends it to its log annotated with the current term, and issues AppendEntries to all nodes in parallel. An entry is considered committed after it is replicated to a majority of the nodes. Once a log entry is committed, all previous log entries are also committed. Once a log entry is committed, the leader can apply it and respond to the user. Moreover, once an entry is committed, it is guaranteed to eventually execute at all available nodes. The leader keeps track of the index of the largest committed entry and sends it to all other nodes so that they can also apply log entries.\n\nRaft satisfies a powerful log matching invariant:\n\n1. \"If two entries in different logs have the same index and term, then they store the same command.\"\n2. \"If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.\"\n\n1 is ensured by the fact that a single leader is elected for any given term, the fact that a leader only creates a single log entry per index, and the fact that once a log entry is created, it never changes index. 2 is ensured by a runtime check. When a leader sends an AppendEntries RPC for a particular index, it also sends its log entry for the previous index. The follower only applies the AppendEntries RPC if it agrees on the previous index. Inductively, this guarantees 2.\n\nFollowers may have missing or extraneous log entries. When this happens, the leader identifies the longest prefix on which the two agree. It then sends the rest of its log. The follower overwrites its log to match the leader.\n\nSafety. The protocol described so far is unsafe. If a new leader is elected, it can accidentally force followers to overwrite committed values with uncommitted values. Thus, we must ensure that leaders contain all committed entries. Other consensus algorithms ensure this by shipping committed values to newly elected leaders. Raft takes an alternative approach and guarantees that if a leader is elected, it has every committed entry. To ensure this, Raft must restrict which nodes can be elected.\n\nA follower rejects a RequestVote RPC if the requesting candidate's log is not as up-to-date as its log. One log is as up-to-date as another if its last entry has a higher term or has the same term but is longer.\n\nSince a candidate must receive a majority of votes and committed values have been replicated to a majority of nodes, a candidate must contact a node with all committed values during its election which will prevent it from being elected if it doesn't have all the committed log entries.\n\nTo prevent another subtle bug, leaders also do not directly commit values from previous terms. They only commit values from their own term which indirectly commits previous log entries from previous terms.\n\nCluster Membership Changes. A Raft cluster cannot be instantaneously switched from one configuration to another. For example consider a cluster moving from 3 to 5 nodes. It's possible that two nodes are elected master for the same term which can lead to a safety violation. Instead, the cluster transitions to a joint consensus phase where decisions require a majority from both the old and new configuration. Once a majority of nodes accept the new configuration, the cluster can transition to it.\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://www.usenix.org/conference/atc14/technical-sessions/presentation/ongaro"
    },
    "1270": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tocs/BaumannPH15",
        "transcript": "When running an application in the cloud, users have to trust (i) the cloud provider's software, (ii) the cloud provider's staff, and (iii) law enforcement with the ability to access user data. Intel SGX partially solves this problem by allowing users to run small portions of program on remote servers with guarantees of confidentiality and integrity. Haven leverages SGX and Drawbridge to run entire legacy programs with shielded execution.\n\nHaven assumes a very strong adversary which has access to all the system's software and most of the system's hardware. Only the processor and SGX hardware is trusted. Haven provides confidentiality and integrity, but not availability. It also does not prevent side-channel attacks.\n\nThere are two main challenges that Haven's design addresses. First, most programs are written assuming a benevolent host. This leads to Iago attacks in which the OS subverts the application by exploiting its assumptions about the OS. Haven must operate correctly despite a malicious host. To do so, Haven uses a library operation system LibOS that is part of a Windows sandboxing framework called Drawbridge. LibOS implements a full OS API using only a few core host OS primitives. These core host OS primitives are used in a defensive way. A shield module sits below LibOS and takes great care to ensure that LibOS is not susceptible to Iago attacks. The user's application, LibOS, and the shield module are all run in an SGX enclave.\n\nSecond, Haven aims to run unmodified binaries which were not written with knowledge of SGX. Real world applications allocate memory, load and run code dynamically, etc. Many of these things are not supported by SGX, so Haven (a) emulated them and (b) got the SGX specification revised to address them.\n\nHaven also implements an in-enclave encrypted file system in which only the root and leaf pages need to be written to stable storage. As of publication, however, Haven did not fully implement this feature. Haven is susceptible to replay attacks.\n\nHaven was evaluated by running Microsoft SQL Server and Apache HTTP Server.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2799647"
    },
    "1271": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigmod/ToshniwalTSRPKJGFDBMR14",
        "transcript": "Storm is Twitter's stream processing system designed to be scalable, resilient, extensible, efficient, and easy to administer. In Storm, streams of tuples flow through (potentially cyclic) directed graphs, called topologies, of processing elements. Each processing element is either a spout (a source of tuples) or a bolt (a tuple processor).\n\nStorm Overview. Storm runs on a cluster, typically over something like Mesos. Each Storm cluster is managed by a single master node knows as a Nimbus. The Nimbus oversees a cluster of workers. Each worker runs multiple worker processes which run a JVM which run multiple executors which run multiple tasks:\n\n```\nworker\n+--------------------------------------------------------------+\n| worker process                 worker process                |\n| +---------------------------+  +---------------------------+ |\n| | JVM                       |  | JVM                       | |\n| | +-----------------------+ |  | +-----------------------+ | |\n| | | executor   executor   | |  | | executor   executor   | | |\n| | | +--------+ +--------+ | |  | | +--------+ +--------+ | | |\n| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |\n| | | | |task| | | |task| | | |  | | | |task| | | |task| | | | |\n| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |\n| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |\n| | | | |task| | | |task| | | |  | | | |task| | | |task| | | | |\n| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |\n| | | +--------+ +--------+ | |  | | +--------+ +--------+ | | |\n| | +-----------------------+ |  | +-----------------------+ | |\n| +---------------------------+  +---------------------------+ |\n| supervisor                                                   |\n| +----------------------------------------------------------+ |\n| |                                                          | |\n| +----------------------------------------------------------+ |\n+--------------------------------------------------------------+\n```\n\nUsers specify a topology which acts as a logical topology. Storm exploits data parallelism by expanding the logical topology into a physical topology in which each logical bolt is converted into a replicated set of physical bolts. Data is partitioned between producer and consumer bolts using one of the following partitioning scheme:\n\n - shuffle: Data is randomly shuffled.\n - fields: Data is hash partitioned on a subset of fields.\n - all: All data is sent to all downstream bolts.\n - global: All data is sent to a single bolt.\n - local: Data is sent to a task running on the same executor.\n\nEach worker runs a supervisor which communicates with the Nimbus. The Nimbus stores its state in Zookeeper.\n\nStorm Internals.\n\nNimbus and Zookeeper. In Storm, topologies are represented as Thrift objects, and the Nimbus is a Thrift server which stores topologies in Zookeeper. This allows topologies to be constructed in any programming language or framework. For example, Summingbird is a Scala library which can compile dataflows to one of many data processing systems like Storm or Hadoop. Users also send over a JAR of the code to the Nimbus which stores it locally on disk. Supervisors advertise openings which the Nimbus fills. All communication between workers and the Nimbus is done through Zookeeper to increase the resilience of the system.\n\nSupervisor. Each worker runs a supervisor process which is responsible for communicating with the Nimbus, spawning workers, monitoring workers, restarting workers, etc. The supervisor consists of three threads: (1) a main thread, (2) an event manager thread, and (3) a process event manager thread. The main thread sends heartbeats to the Nimbus every 15 seconds. The event manager thread looks for assignment changes every 10 seconds. The process event manager thread monitors workers every 3 seconds.\n\nWorkers and Executors. Each executor is a thread running in a JVM. Each worker process has a thread to receive tuples and thread to send tuples. The receiving thread multiplexes tuples to different tasks' input queues. Each executor runs (1) a user logic thread which reads tuples from its input queue and processes them and (2) an executor send thread which puts outbound tuples in a global outbound queue.\n\nProcessing Semantics. Storm provides at most once and at least once semantics. Each tuple in the system is assigned a unique 64 bit identifier. When a bolt processes a tuple, it can generate new tuples. Each of these tuples is also given a unique identifier. The lineage of each tuple is tracked in a lineage tree. When a tuple leaves the system, all bolts that contributed to it are acknowledged and can retire their buffered output. Storm implements this using a memory-efficient representation that uses bitwise XORs.\n\nCommentary. The paper doesn't mention stateful operators.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2588555.2595641"
    },
    "1272": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigmod/0002KBDHKFG15",
        "transcript": "Strong consistency is easy to reason about, but typically requires coordination which increases latency. Weaker consistency can improve performance but is difficult to reason about. This paper presents a program analysis and distributed protocol to run transactions with coordination-free strong consistency.\n\nAnalysing Transactions. We model a database as a finite map from objects to integers. Transactions are ordered sequences of reads, writes, computations, and prints; this is formalized below. A transaction T executes on a database D to produce a new database D' state and a log G' of printed values. Formally, eval(D, T) = <D', G'>.\n\nSymbolic tables categorize the behavior of a transaction based on the initial database sate. Formally, a symbolic table for transaction T is a binary relation Q of pairs <P, T'> where P is a formula in first order logic describing the contents of a database, and T' is a transaction such that T and T' are observationally equivalent when run on databases satisfying P. A symbolic table can also be built for a set of transactions.\n\nFormally, transactions are expressed in a language L which is essentially IMP with database reads, database writes, and prints. A somewhat simple recursive algorithm walks backwards through the program computing symbolic tables. Essentially, the algorithm traces all paths through the programs control flow.\n\nThere is also a higher-level language L++ which can be compiled to L.\n\nHomeostasis Protocol. Assume data is partitioned (not replicated) across a cluster of K nodes. We model a distributed database as a pair <D, Loc> where D is a database and Loc is a function from objects to an index between 1 and K. Each transaction T runs on a site i; formally, l(T) = i. For simplicity, we assume that transactions only write to objects local to the site it is running on.\n\nEach transaction runs on some site. It reads fresh versions of values on the site and stale versions of values on other sites. Nodes establish treaties with one another such that operating with stale data does not affect the correctness of the transaction. This is best explained by way of example. Imagine the following transaction is running on a site where x is remote.\n\n```\nx' = r(x)\nif x' > 0:\n    write(y = 1)\nelse:\n    write(y = 2)\n```\n\nIf we establish the treaty x > 0, then it doesn't matter what the actual value of x is. We now formalize this notion.\n\nGiven a database D, a local-remote partition is a function p from objects to booleans. We can represent a database D with respect to a local-remote p as a pair (l, r) where l is a vector of values x such that p(x), and r is a vector of values x such that not p(x). In words, we can model a database as disjoint sets of local and remote values.\n\nWe say <(l, r), G> = <(l', r') G'> if l = l' and r = r'. Given a database D, local-remote partition p, transaction T, and set of vectors L and R, we say (L, R) is a local-remote slice (LR-slice) for T if Eval((l, r), T) = Eval((l, r'), T) for all l in L and r, r' in R. In words, (L, R) is a local-remote slice for T if T's output depends only on the values of local values.\n\nA global treaty Gamma is a subset of possible database states. A global treaty is valid for a set of transactions {T1, ..., Tn} if ({l | (l, r) in Gamma}, {r | (l, r) in Gamma}) is an LR-slice for all T.\n\nThe homoeostasis protocol proceeds in rounds where each round has three phases:\n\n1.  Treaty generation The system generates a treaty for the current database state.\n2. Normal execution. Transactions can execute without coordination reading a snapshot of remote values. After each site executes a transaction, it checks that it does not bring the database to a state outside the treaty. If it doesn't, the transaction is committed. If it does, we enter the next phase.\n3. Cleanup. All sites synchronize and communicate all values that have changed since the last round. All sites then run the transaction that caused the violation. Finally, we enter the next round.\n\nGenerating Treaties. Two big questions remain: how do we generate treaties, and how do we enforce treaties?\n\nGiven an initial database state D, we could always pick Gamma = {D}. This requires that we synchronize after every single database modification. We want to pick the treaties that let us run as long as possible before synchronizing. We can pick the predicate P in the symbolic table that D satisfies but this isn't guaranteed to be a valid treaty. Instead we take the predicate P and divide it into a set of local treaties P1, ..., PK where the conjunction of all local treaties imply the global treaty. Moreover, each local treaty must be satisfied by the database. The conjunction of the local treaties is our global treaty and is guaranteed to be valid.\n\nFinding good local treaties is not easy. In fact, it can be undecidable pretty easily. We limit ourselves to linear arithmetic and leverage SMT solvers to do the heavy lifting for us. First, we decompose the global treaty into a conjunction of linear constraints. We then generate templates from the constraints and instantiate them using Z3.\n\nHomeostasis in Practice. Roy et al. present a homoeostasis prototype. An offline preprocessing component takes in L++ transactions and computes join symbolic tables, using tricks to keep the tables small. It them initializes global and local treaties. The online execution component executes the homeostasis protocol described above. It is implemented in Java over MySQL. The analysis uses ANTLR-4 and Z3.\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2723372.2723720"
    },
    "1273": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cidr/KornackerBBBCCE15",
        "transcript": "Impala is a distributed query engine built on top of Hadoop. That is, it builds off of existing Hadoop tools and frameworks and reads data stored in Hadoop file formats from HDFS.\n\nImpala's CREATE TABLE commands specify the location and file format of data stored in Hadoop. This data can also be partitioned into different HDFS directories based on certain column values. Users can then issue typical SQL queries against the data. Impala supports batch INSERTs but doesn't support UPDATE or DELETE. Data can also be manipulated directly by going through HDFS.\n\nImpala is divided into three components.\n\n1. An Impala daemon (impalad) runs on each machine and is responsible for receiving queries from users and for orchestrating the execution of queries.\n2. A single Statestore daemon (statestored) is a pub/sub system used to disseminate system metadata asynchronously to clients. The statestore has weak semantics and doesn't persist anything to disk.\n3. A single Catalog daemon (catalogd) publishes catalog information through the statestored. The catalogd pulls in metadata from external systems, puts it in Impala form, and pushes it through the statestored.\n\nImpala has a Java frontend that performs the typical database frontend operations (e.g. parsing, semantic analysis, and query optimization). It uses a two phase query planner.\n\n1. Single node planning. First, a single-node non-executable query plan tree is formed. Typical optimizations like join reordering are performed.\n2. Plan parallelization. After a single node plan is formed, it is fragmented and divided between multiple nodes with the goal of minimizing data movement and maximizing scan locality.\n\nImpala has a C++ backed that uses Volcano style iterators with exchange operators and runtime code generation using LLVM. To efficiently read data from disk, Impala bypasses the traditional HDFS protocols. The backend supports a lot of different file formats including Avro, RC, sequence, plain test, and Parquet.\n\nFor cluster and resource management, Impala uses a home grown Llama system that sits over YARN.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper28.pdf"
    },
    "1274": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eurosys/VermaPKOTW15",
        "transcript": "Borg is Google's cluster manager. Users submit jobs, a collection of tasks, to Borg which are then run in a single cell, many of which live inside a single cluster. Borg jobs are either high priority latency-sensitive production jobs (e.g. user facing products and core infrastructure) or low priority non-production batch jobs. Jobs have typical properties like name and owner and can also express constraints (e.g. only run on certain architectures). Tasks also have properties and state their resource demands. Borg jobs are specified in BCL and are bundled as statically linked executables. Jobs are labeled with a priority and must operate within quota limits. Resources are bundled into allocs in which multiple tasks can run. Borg also manages a naming service, and exports a UI called Sigma to developers.\n\nCells are managed by five-way replicated Borgmasters. A Borgmaster communicates with Borglets running on each machine via RPC, manages the Paxos replicated state of system, and exports information to Sigma. There is also a high fidelity borgmaster simulator known as the Fauxmaster which can used for debugging.\n\nOne subcomponent of the Borgmaster handles scheduling. Submitted jobs are placed in a queue and scheduled by priority and round-robin within a priority. Each job undergoes feasibility checking where Borg checks that there are enough resources to run the job and then scoring where Borg determines the best place to run the job. Worst fit scheduling spreads jobs across many machines allowing for spikes in resource usage. Best fit crams jobs as closely as possible which is bad for bursty loads. Borg uses a scheduler which attempts to limit \"stranded resources\": resources on a machine which cannot be used because other resources on the same machine are depleted. Tasks that are preempted are placed back on the queue. Borg also tries to place jobs where their packages are already loaded, but offers no other form of locality.\n\nBorglets run on each machine and are responsible for starting and stopping tasks, managing logs, and reporting to the Borgmaster. The Borgmaster periodically polls the Borglets (as opposed to Borglets pushing to the Borgmaster) to avoid any need for flow control or recovery storms.\n\nThe Borgmaster performs a couple of tricks to achieve high scalability.\n\n - The scheduler operates on slightly stale state, a form of \"optimistic scheduling\".\n - The Borgmaster caches job scores.\n - The Borgmaster performs feasibility checking and scoring for all equivalent jobs at once.\n - Complete scoring is hard, so the Borgmaster uses randomization.\n\nThe Borgmaster puts the onus of fault tolerance on applications, expecting them to handle occasional failures. Still, the Borgmaster also performs a set of nice tricks for availability.\n\n - It reschedules evicted tasks.\n - It spreads tasks across failure domains.\n - It limits the number of tasks in a job that can be taken down due to maintenance.\n - Avoids past machine/task pairings that lead to failure.\n\nTo measure cluster utilization, Google uses a cell compaction metric: the smallest a cell can be to run a given workload. Better utilization leads directly to savings in money, so Borg is very focused on improving utilization. For example, it allows non-production jobs to reclaim unused resources from production jobs.\n\nBorg uses containers for isolation. It also makes sure to throttle or kill jobs appropriately to ensure performance isolation.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2741948.2741964"
    },
    "1275": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigmod/AlvaroRH15",
        "transcript": "Building fault tolerant systems is hard, like really hard. There are are a couple approaches to building fault-tolerant systems, but none are perfect.\n\n\n - In a bottom-up approach, we verify individual components of a system are fault-tolerant and then glue the components together. Unfortunately, fault-tolerance is not closed under composition: the combination of two fault-tolerant systems may not be fault tolerant.\n - In a top-down, we can inject faults into an existing system and see whether or not it fails. Fault-injection can discover shallow bugs, but has trouble surfacing bugs that require more complex failure scenarios (e.g. a network partition followed by a particular sequence of machine failures).\n\nLineage-driven Fault Injection (LDFI) is a top-down approach which uses lineage and fault injection to carefully discover fault-tolerance bugs in distributed systems. If a bug is found, the lineage of the bug is given to the user to help discover the root cause of the bug. If no bugs are found, LDFI provides some guarantees that there are no possible bugs for that particular configuration.\n\nIn a nutshell, LDFI takes a program written in something like Bloom, inputs to the program, and some parameters (e.g. to bound the length the execution, to bound the number of faults, etc.). It runs the given program on the given input and computes a provenance graph of the output. It then carefully selects a small number of faults that invalidate every derivation. It then injects these faults into the system to see if it surfaces a bug. This is repeated until a bug is found or no such bugs exist.\n\nIn this paper, Alvaro presents LDFI and an LDFI implementation named Molly.\n\n#### System Model\n\nLDFI injects faults, but not every kind of fault. We'll explore dropped messages, failed nodes, and network partitions. We won't explore message reordering or crash recovery. While we sacrifice generality, we gain tractability.\n\nLDFI is governed by three parameters:\n\n1. LDFI does not operate over arbitrarily long executions. A parameter EOT (end of time) prescribes the maximum logical time of any execution examined by LDFI.\n2. Distributed systems typically tolerate some number of faults, but cannot possible tolerate complete message loss for example. A parameter EFF (end of finite failures) < EOT sets a logical time after which LDFI will not introduce faults. The time between EFF and EOT allows a distributed system to recover from message losses.\n3. A parameter Crashes sets an upper limit on the number of node crashes LDFI will consider.\n\nA failure specification is a three-tuple (EOT, EFF, Crashes). Molly will automatically find a good failure specification by repeatedly increasing EOT until programs can create meaningful results and increasing EFF until faults occur.\n\nWe assume programs are written in Dedalus and that pre- and postconditions are expressed as special relations pre and post in the program.\n\n#### LDFI\n\nConsider an interaction between Molly and a user trying to implement a fault-tolerant broadcast protocol between three nodes A, B, and C where A begins with a message to broadcast. Our correctness condition asserts that if a message is delivered to any non-failed node, it is delivered to all of them.\n\n - Implementation 1. Assume a user writes an incredibly naive broadcast protocol in which A sends a copy of the message to B and C once. Molly drops the message from A to B inducing a bug.\n - Implementation 2. The author user then amends the program so that A continually sends the message to B and C. Molly drops the message from A to B and then crashes A. C has the message but B doesn't: a bug.\n - Implementation 3. The author then amends the program so that all three nodes continuously broadcast the message. Now, Molly cannot find a set of dropped messages or node crashes to prevent all three nodes from obtaining the message.\n - Implementation 4. While implementation 3 is fault-tolerant it is also inefficient. The user amends the protocol so that nodes broadcast messages until they receive an ack from the other nodes. Molly can devise a sequence of message drops and node crashes to prevent the message from being delivered to all three nodes, but when it runs the system again with the same faults, the messages still appear.\n - Implementation 5. A \"classic\" implementation of broadcast in which a node broadcasts any message it receives once is found to be buggy by Molly.\n\n#### Molly\n\nMolly begins by rewriting a Dedalus program into a Datalog program.\n\nEach relation is augmented with a time column.\n\n```\nfoo(A,B) ==> foo(A,B,T)\nfoo(B,C) ==> bar(B,C,T)\nbaz(A,C) ==> baz(A,C,T)\n```\nThe time column of every predicate in the body of a rule is bound to the same variable T.\n\n```\n_ :- foo(A,B),   bar(B,C) ==>\n_ :- foo(A,B,T), bar(B,C,T)\n\n```\n\nThe head of every deductive rule is bound to T.\n\n```\nbaz(A,C)   :- foo(A,B),   bar(B,C) ==>\nbaz(A,C,T) :- foo(A,B,T), bar(B,C,T)\n```\n\nThe head of every inductive rule is bound to T + 1.\n\n```\nbaz(A,C)     :- foo(A,B),   bar(B,C) ==>\nbaz(A,C,T+1) :- foo(A,B,T), bar(B,C,T)\n```\n\nFor asynchronous rules, we introduce a Clock(From, To, T) relation which contains an entry (n, m, T) if node n sent a message to m at time T. Then, the body of asynchronous rules at node n whose heads are destined for node n are augmented with a Clock(n, m, t) predicate while the head is augmented with T + 1. Molly can add and remove entries from Clock to simulate faults.\n\n```\nfoo(A,B)     :- foo(B,A) ==>\nfoo(A,B,T+1) :- foo(B,A,T), Clock(B,A,T)\n```\n\nIt then rewrites the Datalog program to maintain its own provenance graph, and extracts lineage from the graph via recursive queries that walk the graph.\n\nGiven an execution of the Datalog program, Molly generates a CNF formula where the disjuncts inside each conjunct x1 or .. or xn represent a message drop or node failure that would invalidate a particular derivation. If all derivations can be invalidated, then the formula is unsatisfiable and the program is fault-tolerant. If the formula is satisfiable, then each satisfied conjunct represents a counterexample of the derivation. Molly uses a SMT solver to solve these formulas.\n\n\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2723372.2723711"
    },
    "1276": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/eurosys/BalegasDFRPNS15",
        "transcript": "Many distributed databases geo-replicate data for (i) lower read latency and (ii) higher fault tolerance in the face of an extreme failure (e.g. lightning striking a data center). Implementing strong consistency over a geo-replicated system can incur tremendous write latency, as updates have to coordinate between geographically distant data centers. On the other hand, weak consistency is a real brain buster. This paper introduces a new consistency model between weak and strong consistency, explicit consistency, which takes into account user specified invariants. It also presents an explicitly consistent system which\n\n1. performs static analysis to determine which operations can be executed without coordination,\n2. uses invariant-repair or violation-avoidance to resolve or avoid conflicts, and\n3. instruments user code with calls to middleware.\n\nThe system is called Indigo and is built on an existing causally consistent key-value store with various properties.\n\nExplicit Consistency. A database is a collection of objects replicated across data centers. Users issue reads and writes as part of transactions, and these transactions are asynchronously replicated between data centers. We denote by t(S) the database state achieved by applying transaction t to database state S. S_n is the database state achieved after the nth transaction. That is, $S_n = t_n(...(t_1(t_init))...)$. $T(S_n) = \\{t_1, ..., t_n\\}$ is the set of transactions used to create S_n. We say a transaction t_a happens before a transaction $t_b$, denoted $t_a \\rightarrow t_b$, if t_a is in $T(S_b)$. $O = (T,\\rightarrow)$ is a partial order. $O' = (T, <)$ is a serialization of $O$ if $<$ is total and respects $\\rightarrow$.\n\nGiven an invariant $I$, we say $S$ is $I$-valid if $I(S) = true$. $(T, <)$ is an I-valid serialization if I holds on all prefixes of the serialization. If a system ensures that all serializations are I-valid, it provides explicit consistency. In other words, an explicitly consistent database ensures invariants always hold. This builds off of Bailis et al.'s notion of invariant-confluence.\n\nDetermining $I$-offender Sets. An invariant is a universally quantified first order logic formula in prenex normal form. The invariant can include uninterpreted functions like Player($P$) and enrolled($P, T$).\n\nA postcondition states how operations affect the truth values of the uninterpreted functions in invariants. Every operation is annotated with postconditions. A predicate clause directly alters the truth assignments of a predicate (e.g. not Player(P)). A function clause relates old and new database states (e.g. nrPlayers(T) = nrPlayers(T) + 1).\n\nThis language is rather expressive, as evidenced by multiple examples in the paper.\n\nA set of transactions is an I-offender if it is not invariant-confluent. First, pairs of operations are checked to see if a contradictory truth assignment is formed (e.g. Player(P) and not Player(P)). Then, every pair of transactions is considered. Given the weakest liberal precondition of the transactions, we substitute the effects of the transactions into the invariant to get a formula. We then check for the validity of the formula using Z3. If the formula is valid, the transactions are invariant-confluent.\n\nHandling I-offender Sets. There are two ways to handle I-offenders: invariant-repair and violation-avoidance. Invariant-repair involves CRDTs; the bulk of this paper focuses on violation-avoidance which leverages existing reservation and escrow transaction techniques.\n\n - UID generation. Unique identifiers can easily be generated without coordination. For example, a node can concatenate an incrementing counter with its MAC address.\n - Multi-level lock reservation. Locking is the most general form of reservation. Locks come in three flavors: (i) shared forbid, (ii) shared allow, and (iii) exclusive allow. Transactions acquire locks to avoid invariant violation. For example, an enrollTournament could acquire a sharedForbid lock on removing players, and removePlayer could acquire a sharedAllow lock. Exclusive allow are used for self-conflicting operations.\n - Multi-level mask reservation. If our invariant is a disjunction P1 or ... or Pn, then to preserve the invariant, we only need to guarantee that at least one of the disjuncts remains true. A mask reservation is a vector of locks where an operation can falsify one of the disjuncts only after acquiring a lock on another true disjunct preventing it from being falsified.\n - Escrow reservation. Imagine our invariant is x >= k and x has initial value x0. Escrow transactions allocate x0 - k rights. A transaction can decrement x only after acquiring and spending a right. When x is incremented, a right is generated. This gets tricker for invariants like |A| >= k where concurrent additions could generate too many rights leading to an invariant violation. Here, we use escrow transactions for conditions, where a primary is allocated for each reservation. Rights are not immediately generated; instead, the primary is responsible for generating rights.\n - Partition lock reservation. Partition locks allow operations to lock a small part of an object. For example, an operation could lock part of a timeline to ensure there are no overlapping timespans.\nThere are many ways to use reservations to avoid invariant violations. Indigo uses heuristics and estimated operation frequencies to try and minimize reservation acquisitions.\n\nImplementation. Indigo can run over any key-value store that offers (i) causally consistency, (ii) snapshot transactions with CRDTs, and (iii) linearizability within a data center. Currently, it uses Swiftcloud. Its fault tolerance leverages the underlying fault tolerance of the key-value store. Each reservation is stored as an object in the key-value store, where operations are structured as transfers to avoid some concurrency oddities.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2741948.2741972"
    },
    "1277": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigmod/ArmbrustXLHLBMK15",
        "transcript": "Data processing frameworks like MapReduce and Spark can do things that relational databases can't do very easily. For example, they can operate over semi-structured or unstructured data, and they can perform advanced analytics. On the other hand, Spark's API allows user to run arbitrary code (e.g. rdd.map(some_arbitrary_function)) which prevents Spark from performing certain optimizations. Spark SQL marries imperative Spark-like data processing with declarative SQL-like data processing into a single unified interface.\n\nSpark's main abstraction was an RDD. Spark SQL's main abstraction is a DataFrame: the Spark analog of a table which supports a nested data model of standard SQL types as well as structs, arrays, maps, unions, and user defined types. DataFrames can be manipulated as if they were RDDs of row objects (e.g. dataframe.map(row_func)), but they also support a set of standard relational operators which take ASTs, built using a DSL, as arguments. For example, the code users.where(users(\"age\") < 40) constructs an AST from users(\"age\") < 40 as an argument to filter the users DataFrame. By passing in ASTs as arguments rather than arbitrary user code, Spark is able to perform optimizations it previously could not do. DataFrames can also be queries using SQL.\n\nNotably, integrating queries into an existing programming language (e.g. Scala) makes writing queries much easier. Intermediate subqueries can be reused, queries can be constructed using standard control flow, etc. Moreover, Spark eagerly typechecks queries even though their execution is lazy. Furthermore, Spark SQL allows users to create DataFrames of language objects (e.g. Scala objects), and UDFs are just normal Scala functions.\n\nDataFrame queries are optimized and manipulated by a new extensible query optimizer called Catalyst. The query optimizer manipulates ASTs written in Scala using rules, which are just functions from trees to trees that typically use pattern matching. Queries are optimized in four phases:\n\n1. Analysis. First, relations and columns are resolved, queries are typechecked, etc.\n2. Logical optimization. Typical logical optimizations like constant folding, filter pushdown, boolean expression simplification, etc are performed.\n3. Physical planning. Cost based optimization is performed.\n4. Code generation. Scala quasiquoting is used for code generation.\n\nCatalyst also makes it easy for people to add new data sources and user defined types.\n\nSpark SQL also supports schema inference, ML integration, and query federation: useful features for big data.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2723372.2742797"
    },
    "1278": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigmod/KulkarniBFKKMPR15",
        "transcript": "Storm was Twitter's first stream processing system. Unfortunately, it wasn't good enough for a number of reasons. Heron is a Storm API compliant rewrite of Storm.\n\nMotivation. In Storm, computation is expressed as a directed graph, or topology, where vertexes are computations and edges transport tuples. Storm topologies are run on a cluster of workers overseen by a central Nimbus. Each worker runs multiple worker processes which run a JVM which run multiple executors which run multiple tasks. The executors run multiple threads, and each worker also has a multi-threaded supervisor.\n\nThis worker architecture was far too complex. Multiple components were making scheduling decisions (e.g. OS schedules processes, JVM schedules executors; executors schedule tasks) which made it hard to predict when certain tasks would be run. Moreover, putting different types of tasks on the same executor complicated logs, exception handling, garbage collection, etc. The Storm scheduler was also not good at scheduling tasks with different resource requirements. The fact that workers were very multi-threaded meant that messages were traversing a lot of thread boundaries.\n\nThe Nimbus was a complicated piece of code that did too much stuff. It also often became a bottleneck and was a single point of failure. It's scheduling was so poor, that Twitter used to reserve nodes to exclusively run a single topology. The Nimbus also communicated with workers through ZooKeeper which became a bottleneck.\n\nStorm also did not implement backpressure; when bolts became overloaded; packets were just dropped.\n\nDesign Alternatives. Twitter considered extending and modifying Storm to fix its problems, but its flaws were deeply entrenched in its design, so a rewrite would be difficult. They considered using other existing stream processing systems, but didn't want to break the Storm API and have to rewrite a bunch of applications. In the end, they felt like a rewrite was the best bet.\n\nData Model and API. Heron follows the exact same API as Storm. Computation is expressed as a directed graph where vertexes are spouts (sources of tuples) or bolts (tuple processors) and edges transfer tuples between vertexes. Users provide logical plans which are expanded to physical plans in order to exploit data parallelism. Heron provides at least once and at most once semantics.\n\nArchitecture Overview. Users submit Heron topologies to Aurora, though Heron is able to run on top of Mesos, YARN, ECS, etc. Each topology is run as a set of containers. One container runs the Topology Master (TM). The rest run a Stream Manager (SM), a Metrics Manager (MM), and multiple Heron Instances (HI). Topology state is kept in ZooKeeper, and the TM can have a standby. All communication is done via protobufs.\n\nTopology Master. The TM is responsible for overseeing the execution of a topology and reporting its status. The TM holds an ephemeral node in ZooKeeper to ensure there is only ever one TM and so that other things can discover it.\n\nStream Manager. Stream Managers are responsible for routing tuples. There are k Stream Managers that form a clique. Though, O(k^2) connections is a lot, the number of Heron Instances can scale independently of k. Stream Managers communicate via TCP, short-circuiting if delivering within a container.\n\nHeron, unlike Storm, implements backpressure. Here are three kinds of backpressure implementations:\n\n - TCP Backpressure. Heron Instances communicate with Stream Managers via TCP. This provides a natural form of backpressure. If a TCP consumer is too slow, the TCP producer will slow down. This form of backpressure is easy to implement. However, multiple logical edges are mapped over a single SM to SM TCP connection which means that sometimes nodes are inadvertently slowed down when they shouldn't be.\n - Spout Backpressure. When a bolt is overloaded, the SM in charge of the bolt can tell the spout that is feeding into it to slow down. This is somewhat inefficient in that slowing an intermediate node may be sufficient.\n - Stage-by-Stage Backpressure. Backpressure can be propagated backwards from vertex to vertex.\n\nHeron implements TCP and Spout Backpressure. Each socket is associated with a queue whose size has a high and low watermark. If the size exceeds the high watermark, backpressure is applied until it drops below the low watermark.\n\nHeron Instances. Each Heron instance runs a single JVM which runs a single task which makes debugging significantly easier. Heron instances cannot be single-threaded because slow user code could prevent things like metrics from being reported in a timely manner. So, Heron implements Heron Instances with two threads: a Gateway Thread and a Task Execution Thread. The Gateway Thread communicates with the Task Execution Thread and also communicates with the SM and MM. The Task Execution Thread runs user code and gathers metrics. The Gateway Thread communicates with the Task Execution Thread using a set of one-directional queues. The sizes of these queues can be adjusted to avoid bad GC.\n\nMetrics Manager. The metrics manager, well, manages metrics. It reports metrics to the TM and to a Twitter monitoring system.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2723372.2742788"
    },
    "1279": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/queue/BurnsGOBW16",
        "transcript": "Google has spent the last decade developing three container management systems. Borg is Google's main cluster management system that manages long running production services and non-production batch jobs on the same set of machines to maximize cluster utilization. Omega is a clean-slate rewrite of Borg using more principled architecture. In Omega, all system state lives in a consistent Paxos-based storage system that is accessed by a multitude of components which act as peers. Kubernetes is the latest open source container manager that draws on lessons from both previous systems.\n\nAll three systems use containers for security and performance isolation. Container technology has evolved greatly since the inception of Borg from chroot to jails to cgroups. Of course containers cannot prevent all forms of performance isolation. Today, containers also contain program images.\n\nContainers allow the cloud to shift from a machine-oriented design to an application oriented-design and tout a number of advantages.\n\n- The gap between where an application is developed and where it is deployed is shrunk.\n - Application writes don't have to worry about the details of the operating system on which the application will run.\n - Infrastructure operators can upgrade hardware without worrying about breaking a lot of applications.\n - Telemetry is tied to applications rather than machines which improves introspection and debugging.\n\nContainer management systems typically also provide a host of other niceties including:\n\n - naming services,\n - autoscaling,\n - load balancing,\n - rollout tools, and\n - monitoring tools.\n\nIn borg, these features were integrated over time in ad-hoc ways. Kubernetes organizes these features under a unified and flexible API.\n\nGoogle's experience has led a number of things to avoid:\n\n - Container management systems shouldn't manage ports. Kubernetes gives each job a unique IP address allowing it to use any port it wants.\n - Containers should have labels, not just numbers. Borg gave each task an index within its job. Kubernetes allows jobs to be labeled with key-value pairs and be grouped based on these labels.\n - In Borg, every task belongs to a single job. Kubernetes makes task management more flexible by allowing a task to belong to multiple groups.\n - Omega exposed the raw state of the system to its components. Kubernetes avoids this by arbitrating access to state through an API.\n\nDespite the decade of experience, there are still open problems yet to be solved:\n\n - Configuration. Configuration languages begin simple but slowly evolve into complicated and poorly designed Turing complete programming languages. It's ideal to have configuration files be simple data files and let real programming languages manipulate them.\n - Dependency management. Programs have lots of dependencies but don't manually state them. This makes automated dependency management very tough.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2898442.2898444"
    },
    "1280": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/popl/GotsmanYFNS16",
        "transcript": "Strong consistency increases latency; weak consistency is hard to reason about. Compromising between the two, a number of databases allow each operation to run with either strong or weak consistency. Ideally, users choose the minimal set of strongly consistent operations needed to enforce some application specific invariant. However, deciding which operations to run with strong consistency and which to run with weak consistency can be very challenging. This paper\n\n - introduces a formal hybrid consistency model which subsumes many existing consistency models,\n - introduces a modular proof rule which can determine whether a given consistency model enforces an invariant, and\n - implements a prototype using a standard SMT solver.\n\nConsistency Model, Informally. Consider\n\n - a set of states $s, s_{init} \\in$ State,\n - a set of operations Op = {o, ...},\n - a set of values $\\bot$ in Val, and\n - a set of replicas $r_1, r_2,$ ....\n\nThe denotation of an operation is denoted F_o where\n\n - F_o: State -> (Val x (State -> State)),\n - F_o^val(s) = F_o(s)[0] (the value returned by the operation), and\n - F_o^eff(s) = F_o(s)[1] (the effect of the operation).\n\nFor example, a banking operation may let states range over natural numbers where\n\n - F_deposit_a(s) = (\\bot, fun s' -> s' + a)\n - F_interest(s) = (\\bot, fun s' -> s' + 0.05 * s)\n - F_query(s) = (s, fun s' -> s')\n\nIf all operations commute (i.e. forall o1, o2, s1, s2. F_o1(s1) o F_o2(s2) = F_o2(s2) o F_o1(s1)), then all replicas are guaranteed to converge. However, convergence does not guarantee all application invariants are maintained. For example, an invariant I = {s | s >= 0} could be violated by merging concurrent withdrawals. To enforce invariants, we introduce a token system which can be used to order certain operations.\n\nA token system TS = (Token, #) is a set of tokens Token and a symmetric relation # over Token. We say two sets of tokens T1 # T2 if exists t1 in T1, t2 in T2. t1 # t2. We also update our definition of operations to acquire tokens:\n\n - F_o: State -> (Val x (State -> State) x P(Token)),\n - F_o^val(s) = F_o(s)[0] (the value returned by the operation),\n - F_o^eff(s) = F_o(s)[1] (the effect of the operation), and\n - F_o^tok(s) = F_o(s)[2] (the tokens acquired by the operation).\n\nOur consistency model will ensure that two operations that acquire conflicting operations will be ordered.\n\nFormal Semantics. Recall a strict partial order is a partial order that is transitive and irreflexive (e.g. sets ordered by strict subset). Given a partial order R, we say (a, b) \\in R or a -R-> b. Consider a countably infinite set Event of events ranged over by e, f, g. If operations are like transactions, an event is like applying a transaction at a replica.\n\n- Definition 1. Given token system TS = (Token, #), an execution is a tuple X = (E, oper, rval, tok, hb) where\n   - E \\subset Event is a finite subset of events,\n   - oper: E -> Op designates the operation of each event,\n   - rval: E -> Val designates the return value of each event,\n   - tok: E -> P(Token) designates the tokens acquired by each event, and\n   - hb \\subset Event x Event is a happens before strict partial order where forall e, f. tok(e) # tok(f) => (e-hb->f or f-hb->e).\n\nAn execution formalizes operations executing at various replicas concurrently, and the happens before relation captures how these operations are propagated between replicas. The transitivity of the happens before relation ensures at least causal consistency.\n\nLet\n\n - Exec(TS) be the set of all executions over token system TS, and\n - ctxt(e, X) = (E, X.oper|E, X.rval|E, X.tok|E, X.hb|E) be the context of e where E = X.hb^-1(e). Intuitively, ctxt(e, X) is the subexection of X that only includes operations causally preceding e.\n\nExecutions are directed graphs of operations, but without a semantics, they are rather meaningless. Here, we define a relation evald_F \\subset Exec(TS) x P(State) where evald_F(Y) is the set of all final states Y can be in after all operations are propagated to all replicas. We'll see shortly that if all non-token-conflicting operations commute, then evald_F is a function.\n\n - evald_F(Y) = {} if Y.E = {}, and\n - evald_F(Y) = {F_e^eff(s)(s') | e \\in max(Y), s \\in evald_F(ctxt(e, Y)), s' \\in evalfd_F(Y|Y.E - {e})} otherwise.\n\nNow\n\n - Definition 2. An execution X \\in Exec(TS) is consistent with TS and F denoted X |= TS, F if forall e \\in X.e. exists s \\in evald_F(ctxt(e, x)). X.val(e) = F_X.oper(e)^val(s) and X.tok(e = F_X.oper(e)^tok(s)).\n\nWe let Exec(TS, F) = {X | X |= TS, F}. Consistent operations are closed under context. Furthermore, evald_F is a function when restricted to consistent executions where non-token-conflicting operations commute. We call this function eval_F.\n\nThis model can model a number of consistency models:\n\n - Causal consistency. Let Token = {}.\n - Sequential consistency. Let Token = {t}, t # t, and F_o^tok(s) = {t} for all o.\n - RedBlue Consistency. Let Token = {t}, t # t, and F_o^tok(s) = {t} for all red o and F_o^tok(s) = {} for all blue o.\n\nState Based Proof Rule. We want to construct a proof rule to establish the fact that Exec(TS, F) \\subset eval_F^-1(I). That is, every execution results in a state that satisfies the invariant. Since executions are closed under context, this also means that all operations execute on a state that satisfies the invariant.\n\nOur proof rule involves a guarantee relation G(t) over states which describes all possible state changes that can occur while holding token t. Similarly, G_0 describes the state transitions that can occur without holding any tokens.\n\nHere is the proof rule.\n\n - S1: s_init \\in I.\n - S2: G_0(I) \\subset I and forall t. G(t)(I) \\subset I.\n - S3: forall o, s, s'.\n    - s \\in I and\n    - (s, s') \\in (G_0 \\cup G(F_o^tok(s)^\\bot))* =>\n    - (s', F_o^eff(s)(s')) \\in G_0 \\cup G(F_o^tok(s)).\n\n\nIn English,\n\n - S1: s_init satisfies the invariant.\n - S2: G and G_0 preserve the invariant.\n - S3: If we start in any state s that satisfies the invariant and can transition in any finite number of steps to any state s' without acquiring any tokens conflicting with o, then we can transition from s' to F_o^eff(s)(s') in a single step using the tokens acquired by o.\n\nEvent Based Proof Rule and Soundness. Instead of looking at states, we can instead look at executions. That is, if we let invariants I \\subset Exec(TS), then we want to write a proof rule to ensure Exec(TS, F) \\subset I. That is, all consistent executions satisfy the invariant. Again, we use a guarantee G \\subset Exec(TS) x Exec(TS).\n\n - E1: X_init \\in I.\n - E2: G(I) \\in I.\n - E3: forall X, X', X''. forall e in X''.E.\n    - X'' |= TS, F and\n    - X' = X''|X''.E - {e} and\n    - e \\in max(X'') and\n    - X = ctxt(e, X'') and\n    - X \\in I and\n    - (X, X') \\in G* =>\n    - (X', X'') \\in G.\n\nThis proof rule is proven sound. The event based rule and its soundness is derived from this.\n\nExamples and Automation. The authors have built a banking, auction, and courseware application in this style. They have also built a prototype that you give TS, F, and I and it determines if Exec(T, f) \\subset eval_F^-1(I). Their prototype modifies the state-based proof rule eliminating the transitive closure and introducing intermediate predicates.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2837614.2837625"
    },
    "1281": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/pvldb/MaddoxGEMPD16",
        "transcript": "Decibel is like git for relational data.\n\nOften, teams need to simultaneously query, analyze, clean, or curate a collection of data without clobbering each other's work. Currently, the best solution available involves each team member making copies of the entire database. This is undesirable for a number of reasons:\n\n - Data is stored very redundantly, wasting a huge amount of storage.\n - It is difficult to share or merge changes made to local snapshots of the data.\n - There is no systematic way to say which version of the data was used for a given experiment.\n\nVersion control solves these problems, but existing version control systems (e.g. git) have a couple of shortcomings when applied naively to large datasets:\n\n - Using distributed version control systems like git, clients clone an entire copy of a repository's contents. This is infeasible when working with large datasets.\n - Version control systems like git operate on arbitrary data and do not understand the structure of relational data. This makes certain operations like generic diffing a bad fit.\n - Systems like git do not support high-level data management or query APIs.\n\nDecibel manages datasets which are collections of relations, and each relation's schema includes an immutable primary key which is used to track the version of each row. Beginning with an initial snapshot of a dataset, users can check out, branch, and commit changes to the data set in a style very similar to git. When data is merged, non-conflicting changes to separate columns of the same row are both applied. If conflicting changes to the same column of the same row occur, one branch's changes take priority. Diffing two tables across two branches results in a table of insertions and deletions. Finally, data can be queried across versions using VQuel: a query language which is notably not SQL.\n\nThis paper describes three physical realizations of Decibel's logical data model.\n\n1. In a tuple-first representation, tables contain every version of every row, and each row is annotated with a bitmap indicating the set of versions in which the row is live. In a tuple-oriented approach, each of N tuples comes with a B-bit bitmap for B branches. In a branch-oriented approach, there are B N-bit bitmaps.\n2. In a version-first representation, all the changes made to a table in a branch are stored together in the same file, and these branched files contain pointers to their ancestors forming a directed acyclic graph.\n3. In a hybrid representation, data is stored similarly to the version-first approach, but each of the branched files (called segments) includes a segment index: a bitmap, like in the tuple-first representation, that tracks the liveness of each row for all descendent branches. Moreover, there is a single branch-segment bitmap which for each branch, records the set of segments with a tuple live in that branch.\n\nThe tuple-first representation is good for multi-version queries, the version-first representation is good for single-version queries, and the hybrid approach is good at both.\n\nThis paper also presents a versioned database benchmarking framework in the form of a set of branching strategies and characteristic queries to analyze Decibel and any future versioned databases.\n\nCommentary. A git-like versioned relational database seems like a great idea! This paper focuses on how to implement and analyze such a database efficiently; it focuses less on the higher-level semantics of data versioning. Notably, two unanswered questions stuck out to me that seem like interesting areas for future research.\n\n1. The current merging strategy seems inconvenient, if not inconsistent. Imagine a table R(key, a, b) with the invariant that a + b < 10. Imagine branch b1 updates a tuple (0, 0, 0) to (0, 9, 0) and branch b2 updates it to (0, 0, 9). If these tuples are merged, the tuple becomes (0, 9, 9) violating the invariant. In systems like git, users can manually inspect and verify the correctness of the merge, but if a large dataset is being merged, this becomes infeasible. I think more research is needed to determine if this strategy is good enough and won't cause problems in practice. Or if it is insufficient, what merging strategies can be applied on large datasets. Perhaps ideas could be borrowed from CRDT literature; if all columns were semilattices, the columns could be merged in a sane way.\n2. How useful is diffing when dealing with large datasets? When working with git, even a modestly sized diff can become confusing. Would a non-trivial diff of a large dataset be incomprehensible?\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.vldb.org/pvldb/vol9/p624-maddox.pdf"
    },
    "1282": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1145/2987550.2987559",
        "transcript": "In the face of latency, partitions, and failures, application sometimes turn to weak consistency for improved performance. However, strong consistency cannot always be avoided, so some data stores (e.g. Cassandra, Riak) allow applications to operate on some data with weak consistency and on other data with strong consistency. However, it is difficult to ensure that data read with weak consistency does not leak into the strongly consistent data.\n\nFor example, imagine a ticketing application that displays the number of remaining tickets for an upcoming movie. To improve performance, an application may choose to read the data using weak consistency; it's okay if the displayed number is slightly erroneous. However, now imagine that we want to increase the price of the last 10 tickets. If we used the same weakly consistent value to determine the price of a ticket, we may make less money that we expect. Weak and strong consistency cannot be carelessly mixed.\n\nThe Inconsistent, Performance-bound, Approximate (IPA) storage system uses a type system of consistency types to ensure consistency safety: the guarantee that weakly consistent data does not leak into strongly consistent data. They also introduce error-bounded consistency which allows a runtime to dynamically choose the strongest consistency that satisfies certain constraints (e.g. latency or accuracy).\n\nProgramming Model. The IPA programming model involves three components: abstract data types, consistency policies, and consistency types. Users annotate ADTs with consistency policies which determines the consistency types returned by the ADT's methods.\n\n1. ADTs. IPA is bundled with a set of common ADTs (e.g. sets, maps, lists) that can be implemented with any number of backing stores (e.g. Cassandra, Riak, Redis). The collection of ADTs is also extensible; users can add their own ADTs.\n2. Consistency Policies. ADT instances (or their methods) are annotated with consistency policies which designate the level of consistency the user desires. Static policies, like strong consistency, are fixed throughout the lifetime of the ADT. Dynamic policies allow the runtime to dynamically choose the consistency of the system to meet some constraint. For example, a user can set a policy LatencyBound(x), and the system will choose the strongest consistency that can be served and still satisfy the latency bound. Or, a user can set a ErrorTolerance(x) allowing the system to return approximate results with weaker consistency.\n3. Consistency Types. Consistency types are parameterized on a type T and are partially ordered\n\n```\n          Consistent[T]\n          /     |     \\\nInterval[T] Rushed[T] ...\n          \\     |     /\n         Inconsistent[T]\n```\n\nUsers can explicitly endorse, or upcast, weak types to stronger types. Rushed types are produced by ADTs with LatencyBound policies and are a sum of other consistency types. Interval types are a numeric interval which are guaranteed to contain the correct value.\n\nEnforcing Consistency Policies. Static consistency policies are easy to enforce. The ADT operations simply set flags that are used by the underlying store. Dynamic policies are trickier to implement.\n\n- Latency bounds. To service a read with a latency bound of x milliseconds, for example, a read is issued at every single consistency level. The strongest to return in less that x milliseconds is used. This naive approach can put unwanted load on a system, so IPA can also monitor and predict the strongest consistency model for a given latency bound in order to issue only a couple of reads.\n\n- Error bounds. Error bounds are enforced using a reservation scheme similar to the one used by bounded CRDTs. For example, imagine a counter with value 10 should be concurrently decremented but never drop below 0. We can allocate 10 decrement tokens and distribute them to nodes. A node can decrement so long as it has enough tokens. IPA involves reservation servers which are assigned tokens. A node can return an approximate read by knowing the number of outstanding tokens. For example, given a value of 100 and knowing there are 10 outstanding increment tokens, the true value is somewhere in the range [100, 110]. The more outstanding tokens there are, the weaker the error bounds. To avoid unnecessarily granting tokens, IPA uses an allocation table which maps reservation servers to tokens. Tokens are only allocated when necessary.\n\nImplementaton. IPA is implemented in Scala on top of Cassandra with some middleware for the reservation servers. Type checking is done by leveraging Scala's type system.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1145/2987550.2987559"
    },
    "1283": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigmod/HalevyKNOPRW16",
        "transcript": "In fear of fettering development and innovation, companies often allow engineers free reign to generate and analyze datasets at will. This often leads to unorganized data lakes: a ragtag collection of datasets from a diverse set of sources. Google Dataset Search (Goods) is a system which uses unobstructive post-hoc metadata extraction and inference to organize Google's unorganized datasets and present curated dataset information, such as metadata and provenance, to engineers.\n\nBuilding a system like Goods at Google scale presents many challenges.\n\n- Scale. There are 26 billion datasets. 26 billion (with a b)!\n - Variety. Data comes from a diverse set of sources (e.g. BigTable, Spanner, logs).\n - Churn. Roughly 5% of the datasets are deleted everyday, and datasets are created roughly as quickly as they are deleted.\n - Uncertainty. Some metadata inference is approximate and speculative.\n - Ranking. To facilitate useful dataset search, datasets have to be ranked by importance: a difficult heuristic-driven process.\n - Semantics. Extracting the semantic content of a dataset is useful but challenging. For example consider a file of protos that doesn't reference the type of proto being stored.\n\nThe Goods catalog is a BigTable keyed by dataset name where each row contains metadata including\n\n- basic metatdata like timestamp, owners, and access permissions;\n - provenance showing the lineage of each dataset;\n - schema;\n - data summaries extracted from source code; and\n - user provided annotations.\n\nMoreover, similar datasets or multiple versions of the same logical dataset are grouped together to form clusters. Metadata for one element of a cluster can be used as metadata for other elements of the cluster, greatly reducing the amount of metadata that needs to be computed. Data is clustered by timestamp, data center, machine, version, and UID, all of which is extracted from dataset paths (e.g. /foo/bar/montana/August01/foo.txt).\n\nIn addition to storing dataset metadata, each row also stores status metadata: information about the completion status of various jobs which operate on the catalog. The numerous concurrently executing batch jobs use status metadata as a weak form of synchronization and dependency resolution, potentially deferring the processing of a row until another job has processed it.\n\nThe fault tolerance of these jobs is provided by a mix of job retries, BigTable's idempotent update semantics, and a watchdog that terminates divergent programs.\n\nFinally, a two-phase garbage collector tombstones rows that satisfy a garbage collection predicate and removes them one day later if they still match the predicate. Batch jobs do not process tombstoned rows.\n\nThe Goods frontend includes dataset profile pages, dataset search driven by a handful of heuristics to rank datasets by importance, and teams dashboard.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2882903.2903730"
    },
    "1284": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigmod/ChenWIJLSWWWY16",
        "transcript": "There's an enormous number of stream (a.k.a. real-time, interactive) processing systems in the wild: Twitter Storm, Twitter Heron, Google Millwheel, LinkedIn Samza, Spark Streaming, Apache Flink, etc. While all similar, the stream processing systems differ in their ease of use, performance, fault-tolerance, scalability, correctness, etc. In this paper, Facebook discusses the design decisions that go into developing a stream processing system and discusses the decisions they made with three of their real-time processing systems: Puma, Swift, and Stylus.\n\nSystems Overview. - Scribe. Scribe is a persistent messaging system for log data. Data is organized into categories, and Scribe buckets are the basic unit of stream processing. The data pushed into scribe is persisted in HDFS. - Puma. Puma is a real-time data processing system in which applications are written in a SQL-like language with user defined functions written in Java. The system is designed for compiled, rather than ad-hoc, queries. It used to compute aggregates and to filter Scribe streams. - Swift. Swift is used to checkpoint Scribe streams. Checkpoints are made every N strings or every B bytes. Swift is used for low-throughput applications. - Stylus. Stylus is a low-level general purpose stream processing system written in C++ and resembles Storm, Millwheel, etc. Stream processors are organized into DAGs and the system provides estimated low watermarks. - Laser. Laser is a high throughput, low latency key-value store built on RocksDB. - Scuba. Scuba supports ad-hoc queries for debugging. - Hive. Hive is a huge data warehouse which support SQL queries.\n\nExample Application. Imagine a stream of events, where each event belongs to a single topic. Consider a streaming application which computes the top k events for each topic over 5 minute windows composed of four stages:\n\n1. Filterer. The filterer filter events and shards events based on their dimension id.\n2. Joiner. The joiner looks up dimension data by dimension id, infers the topic of the event, and shards output by (event, topic).\n3. Scorer. The scorer maintains a recent history of event counts per topic as well as some long-term counts. It assigns a score for each event and shards output by topic.\n4. Ranker. The ranker computes the top k events per topic.\n\nThe filterer and joiner are stateless; the scorer and ranker are stateful. The filterer and ranker can be implemented in Puma. All can be implemented in Stylus.\n\nLanguage Paradigm. The choice of the language in which users write applications can greatly impact a system's ease of use:\n\n - Declarative. SQL is declarative, expressive, and everyone knows it. However, not all computations can be expressed in SQL.\n - Functional. Frameworks like Dryad and Spark provide users with a set of built-in operators which they chain together. This is more flexible that SQL.\n - Procedural. Systems like Storm, Heron, and Samza allow users to form DAGs of arbitrary processing units.\n\nPuma uses SQL, Swift uses Python, and Stylus uses C++.\n\nData Transfer. Data must be transferred between nodes in a DAG:\n\n- Direct message transfer. Data can be transferred directly with something like RPCs or ZeroMQ. Millwheel, Flink, and Spark Streaming do this.\n - Broker based message transfer. Instead of direct communication, a message broker can be placed between nodes. This allows an output to be multiplexed to multiple outputs. Moreover, brokers can implement back pressure. Heron does this.\n - Persistent message based transfer. Storing messages to a persistent messaging layer allows data to be multiplexed, allows for different reader and writer speeds, allows data to be read again, and makes failures independent. Samza Puma, Swift, and Stylus do this.\n\nFacebook connects its systems with Scribe for the following benefits:\n\n- Fault Tolerance: If the producer of a stream fails, the consumer is not affected. This failure independence becomes increasingly useful at scale.\n- Fault Tolerance: Recovery can be faster because only nodes need to be replaced.\n- Fault Tolerance: Multiple identical downstream nodes can be run to improve fault-tolerance.\n- Performance: Different nodes can have different read and write latencies. The system doesn't propagate back pressure to slow down the system.\n- Ease of Use: The ability to replay messages makes debugging easier.\n- Ease of Use: Storing messages in Scribe makes monitoring and alerting easier.\n- Ease of Use: Having Scribe as a common substrate lets different frameworks communicate with one another.\n- Scalability: Changing the number of Scribe buckets makes it easy to change the number of partitions.\n\nProcessing Semantics. Stream processors:\n\n1. Proccess inputs,\n2. Generate output, and\n3. checkpoint state, stream offsets, and outputs for recovery.\n\nEach node has\n\n - state semantics: can each input affect state at least once, at most once, or exactly once.\n - output semantics: can each output be produced at least once, at most once, or exactly once.\n\nFor state semantics, we can achieve\n\n - at least once by saving state before saving stream offsets,\n - at most once by saving stream offsets before saving state, and\n - exactly once by saving both state and stream offsets atomically.\n\nFor output semantics, we can achieve\n\n - at least once by saving output before offset/state,\n - at most once by saving offset/state before output,\n - exactly once by saving output and offset/state atomically.\n\nAt-least-once semantics is useful when low latency is more important than duplicate records. At most once is useful when loss is preferred over duplication. Puma guarantees at least once state and output semantics, and Stylus supports a whole bunch of combinations.\n\nState-saving Mechanisms. Node state can be saved in one of many ways:\n\n - Replication. Running multiple copies of a node provides fault tolerance.\n - Local DB. Nodes can save their state to a local database like LevelDB or RocksDB.\n - Remote DB. Nodes can save their state to remote databases. Millwheel does this.\n - Upstream backup. Output messages can be buffered upstream in case downstream nodes fail.\n - Global consistent snapshot. Flink maintains globally consistent snapshots.\n\nStylus can save to a local RocksDB instance with data asynchronously backed up to HDFS. Alternatively, it can store to a remote database. If a processing unit forms a monoid (identity element with associate operator), then input data can be processed and later merged into the remote DB.\n\nBackfill Processing. Being able to re-run old jobs or run new jobs on old data is useful for a number of reasons:\n\n - Running new jobs on old data is great for debugging.\n - Sometimes, we need to run a new metric on old data to generate historical metrics.\n - If a node has a bug, we'd like to re-run the node on the data.\n\nTo re-run processing on old data, we have three choices:\n\n - Stream only.\n - Separate batch and streaming systems. This can be very annoying and hard to manage.\n - Combined batch and streaming system. This is what Spark streaming, Flink, and Facebook does.\n\nPuma and Stylus code can be run as either streaming or batch applications.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2882903.2904441"
    },
    "1285": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigmod/CrooksPEGAC16",
        "transcript": "Overview. Strong consistency is expensive. The alternative, weak consistency, is hard to program against. The lack of distributed synchronization or consensus in a weakly consistent system means that replica state can diverge. Existing systems try to hide this divergence with causal consistency to deal with read-write conflicts and per-object eventual convergence to deal with write-write conflicts, but neither is sufficient to deal with complex multi-object write-write conflicts.\n\nAs a motivating example, imagine a Wikipedia article for Donald Trump with three parts: some text, an image, and some references. In one partition, Hillary modifies the text to oppose Trump and subsequently, Tim changes the picture to a nastier image of Trump. In another picture, Trump modifies to the text to support Trump and subsequently, Mike changes the references to link to pro-Trump websites. Later, the partitions need to be merged. The write-write conflict on the text needs to be reconciled. Moreover, the modifications to the image and references do not produce conflicts but still need to be updated to match the text. Existing systems solve this in one of two ways:\n\n1. syntactic conflict resolution: Some policy like last-writer-wins is chosen.\n2. lack of cross-object semantics: Users are forced to merge individual objects.\n\nTARDiS (Transactional Asynchronously Replicated Divergent Store) is a distributed weakly-consistent transactional key-value store that supports branching computation as a core abstraction. When working off a single branch, applications are shielded from diverging computations. Conversely, applications can merge branches and reconcile conflicts. By allowing applications to merge entire branches rather than single objects, users have the ability to perform semantically rich multi-object merges.\n\nTARDiS employs three techniques:\n\n1. branch-on-conflict\n2. inter-branch isolation\n3. application-driven cross-object merge\n\nand has the following properties:\n\n1. TARDiS knows history: TARDiS maintains a DAG of branching execution and uses DAG compression to minimize memory overhead.\n2. TARDiS merges branches, not objects: TARDiS allows applications to merge branches rather than single-objects.\n3. TARDiS is expressive: TARDiS supports various isolation levels.\n4. TARDiS improves performance of the local site: Branching on conflict and deferring merge until later can improve performance in a local setting as well as in a distributed setting.\n\nArchitecture. TARDiS is a distributed multi-master key value store with asynchronous replication. There are four main layers to TARDiS:\n\n1. Storage layer: this layer implements a disk-backed multiversion B-tree.\n2. Consistency layer: this layer maintains the DAG of execution branches where each vertex is a logical state.\n3. Garbage collection layer: this layer performs DAG compression and record pruning.\n4. Replicator service layer: this layer propagates transactions.\n\nInterface. Applications use TARDiS in either\n\n1. single-mode, where transaction execute on a single branch, or\n2. multi-mode, where a transaction can read from multiple branches and create a new merged branch.\n\nTARDiS provides an API to find the set of conflicting writes for a set of branches, the find the fork points for a set of branches, and to get the versioned values of objects. It also supports varying levels of begin and end constraints including serializability, snapshot isolation, read committed, etc.\n\nHere's an example TARDiS application that implements a counter.\n\n```\nfunc increment(counter)\n    Tx t = begin(AncestorConstraint)\n    int value = t.get(counter)\n    t.put(counter, value + 1)\n    t.commit(SerializabilityConstraint)\n\nfunc decrement(counter)\n    Tx t = begin(AncestorConstraint)\n    int value = t.get(counter)\n    t.put(counter, value - 1)\n    t.commit(SerializabilityConstraint)\n\nfunc merge()\n    Tx t = beginMerge(AnyConstraint)\n    forkPoint forkPt = t.findForkPoints(t.parents).first\n    int forkVal = t.getForId(counter, forkPt)\n    list<int> currentVals = t.getForId(counter, t.parents)\n    int result = forkVal\n    foreach c in currentVals\n        result += (c - forkVal)\n    t.put(counter, result)\n    t.commit(SerializabilityConstraint)\n```\n\nDesign and Implementation. When a transaction begins, TARDiS performs a BFS from the leaves of the state DAG to find the first state satisfying the begin constraint. When a transaction is committed, TARDiS walks down the state DAG as far as possible until a state is reached that doesn't satisfy the end constraint, branching if necessary. Branches are represented as a set of (b, i) pairs to indicate the bth child of the ith node. Keys are mapped to a topologically sorted list of versions used during reading.\n\nThe garbage collector performs DAG compression, eliminating unneeded states in the state DAG. It also prunes record unneeded record versions after DAG compression.\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2882903.2882951"
    },
    "1286": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigmod/LinC0OTW16",
        "transcript": "For a traditional single-node database, the data that a transaction reads and writes is all on a single machine. For a distributed OLTP database, there are two types of transactions:\n\n1. Local transactions are transactions that read and write data on a single machine, much like traditional transactions. A distributed database can process a local transaction as efficiently as a traditional single-node database can.\n2. Distributed transactions are transactions that read and write data that is spread across multiple machines. Typically, distributed databases use two-phase commit (2PC) to commit or abort distributed transactions. Because 2PC requires multiple rounds of communications, distributed databases process distributed transactions less efficiently than local transactions.\n\nThis paper presents an alternative to 2PC, dubbed Localizing Executions via Aggressive Placement of data (LEAP), which tries to avoid the communication overheads of 2PC by aggressively moving all the data a distributed transaction reads and writes onto a single machine, effectively turning the distributed transaction into a local transaction.\n\n#### LEAP\n\nLEAP is based on the following assumptions and observations:\n\n - Transactions in an OLTP workload don't read or write many tuples.\n - Tuples in an OLTP database are typically very small.\n - Multiple transactions issued one after another may access the same data again and again.\n - As more advanced network technology becomes available (e.g. RDMA), the cost of moving data becomes smaller and smaller.\n\nWith LEAP, tuples are horizontally partitioned across a set of nodes, and each tuple is stored exactly once. Each node has two data structures:\n\n - a data table which stores tuples, and\n - a horizontally partitioned owner table key-value store which stores ownership information.\n\nConsider a tuple $d = (k, v)$ with primary key $k$ and value $v$. The owner table contains an entry $(k, o)$ indicating that node o owns the tuple with key $k$. The node $o$ contains a $(k, v)$ entry in its data table. The owner table key-value store is partitioned across nodes using any arbitrary partitioning scheme (e.g. hash-based, range-based).\n\nWhen a node initiates a transaction, it requests ownership of every tuple it reads and writes. This migrates the tuples to the initiating node and updates the ownership information to reflect the ownership transfer. Here's how the ownership transfer protocol works. For a given tuple $d = (k, v)$, the requester is the node requesting ownership of $d$, the partitioner is the node with ownership information $(k, o)$, and the owner is the node that stores $d$.\n\n - First, the requester sends an owner request with key k to the partitioner.\n - Then, the partitioner looks up the owner of the tuple with k in its owner table and sends a transfer request to the owner.\n - The owner retrieves the value of the tuple and sends it in a transfer response back to the requester. It also deletes its copy of the tuple.\n - Finally, the requester sends an inform message to the partitioner informing it that the ownership transfer was complete. The partitioner updates its owner table to reflect the new owner.\n\nAlso note that\n\n - if the requester, partitioner, and owner are all different nodes, then this scheme requires 4 messages,\n - if the partitioner and owner are the same, then this scheme requires 3 messages, and\n - if the requester and partitioner are the same, then this scheme requires 2 messages.\n\nIf the transfer request is dropped and the owner deletes the tuple, data is lost. See the appendix for information on how to make this ownership transfer fault tolerant. Also see the paper for a theoretical comparison of 2PC and LEAP.\n\n#### LEAP-Based OLTP Engine\n\nL-Store is a distributed OLTP database based on H-Store which uses LEAP to manage transactions. Transactions acquire read/write locks on individual tuples and use strict two-phase locking. Transactions are assigned globally unique identifiers, and deadlock prevention is implemented with a wait-die scheme where lower timestamped transactions have higher priority. That is, higher priority threads wait on lower priority threads, but lower priority threads abort rather than wait on higher priority threads.\n\nConcurrent local transactions are processed as usual; what's interesting is concurrent transfer requests. Imagine a transaction is requesting ownership of a tuple on another node.\n\n - First, the requester creates a request lock locally indicating that it is currently trying to request ownership of the tuple. It then sends an owner request to the partitioner.\n - The partitioner may receive multiple concurrent owner requests. It processes them serially using the wait-die scheme. As an optimization, it processes requests in decreasing timestamp order to avoid aborts whenever possible. It then forwards a transfer request to the owner.\n - If the owner is currently accessing the tuple being requested, it again uses a wait-die scheme to access the tuple before sending it back to the owner.\n - Finally, the owner changes the request lock into a normal data lock and continues processing.\n\nIf a transaction cannot successfully get ownership of a tuple, it aborts. L-Store also uses logging and checkpointing for fault tolerance (see paper for details).\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/2882903.2882923"
    },
    "1287": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tocs/McKusickJLF84",
        "transcript": "The Fast Filesystem (FFS) improved the read and write throughput of the original Unix file system by 10x by\n\n1. increasing the block size,\n2. dividing blocks into fragments, and\n3. performing smarter allocation.\n\nThe original Unix file system, dubbed \"the old file system\", divided disk drives into partitions and loaded a file system on to each partition. The filesystem included a superblock containing metadata, a linked list of free data blocks known as the free list, and an inode for every file. Notably, the file system was composed of 512 byte blocks; no more than 512 bytes could be transfered from the disk at once. Moreover, the file system had poor data locality. Files were often sprayed across the disk requiring lots of random disk accesses.\n\nThe \"new file system\" improved performance by increasing the block size to any power of two at least as big as 4096 bytes. In order to handle small files efficiently and avoid high internal fragmentation and wasted space, blocks were further divided into fragments at least as large as the disk sector size.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/989.990"
    },
    "1288": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=Chamberlin1981-ck",
        "transcript": "Ed Codd proposed the relational model in 1970. As opposed to the navigational data models that came before it, the relational model boasted data independence: the ability for data storage and access methods to change independently of applications. Some worried that data independence necessitated poor performance. System R was one of the first relation databases and proved that the relational model could be implemented efficiently.\n\nSystem R development proceeded in three phases. Phase 0 (1974-1975) was a single-user PL/I interpreter prototype that processed a subset of SQL (e.g. no joins) using the XRM access method. The Phase 0 prototype was always intended to be thrown away. Instead, the focus was on tuning the user interface SQL. User studies and interviews were performed to increase the usability and understandability of SQL. Every tuple in the database was labelled with a TID which contained a page number. Each tuple contained pointers into separate domains, and inversions existed to map domain values to TIDs. The Phase 0 query optimizer aimed to minimize the number of fetched tuples and would perform tricks like TID intersection to evaluate conjunctions. The prototype also introduced the design that the system catalog should be stored as relations. Phase 0 brought about the following ideas:\n\n1. The optimizer should consider more than the cost of fetching tuples. It should also take into account the costs of TID manipulation, data fetching, etc.\n2. Number of I/Os would have been a better metric than the number of tuples fetched. This would have also exposed the deficiency of the XRM access method.\n3. The Phase 0 optimizer was CPU bound! This encouraged the later optimizer to be a weighted cost of CPU and I/O.\n4. SQL joins are very important.\n5. The query optimizer was complicated; more care should be given towards simpler and more common queries.\n\nPhase 1 ranged from 1976 to 1977 and included the implementation of a full blown multi-user relational database. Phase 1 was divided into two pieces:\n\n1. The Relational Data System (RDS) was an optimizing SQL processor responsible for query optimization.\n2. The Research Storage System (RSS) was the access method that replaced XRM and was responsible for things like locking and logging.\n\nUsers could query System R using interactive queries or by embedding SQL queries in PL/I or Cobol. A preprocessor would compile the embedded SQL queries into an access module using a repository of hand-compiled fragments. Of course, the compiled query plan could be invalidated over time. For example, the query plan could use an index which is later dropped. Thus, each query's dependencies were put in the system catalog and queries were recompiled when their dependencies were invalidated.\n\nUnlike the XRM, the RSS stored data directly in the tuples. This meant that certain column values were stored redundantly, but an entire row could be read in a single I/O. RSS also supported B+ tree indexes, tuple links, index scans, full table scans, link scans, tuple nested loop joins, index nested loop joins, and sort merge joins.\n\nThe query optimizer minimized a weighted sum of RSS calls and I/Os using a dynamic programming approach. It avoided using some of the TID list intersection tricks that the Phase 0 optimizer used.\n\nViews were stored as parse trees and merged back into the SQL queries used to query them. Updates were only allowed on single-table views. Views were the atomic unit of authorization using a grant/revoke mechanism.\n\nSystem R used a combination of logging and shadow pages to implement recovery. During recovery, pages were restored to their old shadow pages, and the log was processed backwards.\n\nSince Phase 1 was a multi-user database, it introduced multiple granularity locking in the form of intension locks. Originally, it had predicate locking, but this was abandoned because it was (1) difficult to check for predicate disjointness, (2) predicates were sometimes falsely marked as overlapping, and (3) predicate locking broke the abstraction barrier of the RSS.\n\nPhase 2 was a two-year period in which System R was evaluated. Users generally enjoyed the uniformity of SQL, and their recommendations led to the introduction of EXISTS, LIKE, prepared statements, and outer joins. The query optimizer was evaluated assuming that data was uniformly distributed and that all columns were independent. Shadow pages led to poor locality, extra bookkeeping, and semi-expensive page swapping. System R provided read uncommitted, read committed, and full serializable transactions. Read uncommitted wasn't implemented as fast as it should have been. Read committed had more overhead than expected. Serializable transactions ended up being the most commonly used.\n\n#### Commentary\n\nSystem R introduced a bevy of influential and perennial ideas in the field of databases. Unix introduced a bevy of influential and perennial ideas in the field of operating systems. It's no coincidence that there are a striking number of system design principles that System R and Unix---as presented in The Unix Time-Sharing System---share:\n\n1. Unified Abstractions. Unix unified the file and I/O device abstraction into a single interface. System R unified the table and catalog/metadata API into a single interface (i.e. everything is a relation). System R also unifed SQL as the query language used for ad-hoc queries, program-embeded queries, and view definitions. System R's decision to use relations to represent the catalog can also be seen as a form of dogfooding.\n2. Simple is Better. Unix started as Ken Thompson's pet project as an effort to make development simpler and more enjoyable. Unix's simplicity stuck and was one of its selling points. Similarly, System R spent a considerable amount of effort simplifying the SQL interface to make it as easy to use as possible. If a system's interface is too complicated, nobody will use it.\n3. Performance isn't Everything. Thompson and Ritchie implemented Unix in C instead of assembly despite the fact that the kernel size increased by one third because C greatly improved the readability and maintainability of the system. Similarly, the System R paper comments that the relational model may never exceed the performance of a hand-optimized navigational database, but the abstraction it provides is worth the cost. Somewhat comically, today's compilers and query optimizers are so good, compiled C is likely smaller than hand-written assembly and optimized queries are likely faster than hand-optimized ones. This is an example of another systems principle of favoring higher-level declarative APIs which leave room for optimization.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1289": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tods/LehmanY81",
        "transcript": "This paper introduces the B-link tree: a variant of a B+ tree (the paper says B* tree, but they mean B+ tree) which allows for concurrent searches, insertions, and deletions. Operations on a B-link tree lock at most three nodes at any given time, and searches are completely lock free.\n\n#### Storage Model\n\nWe assume that every node of a B+ tree or B-link tree is stored on disk. Threads read nodes from disk into memory, modify them in memory, and then write them back to disk. Threads can also lock a specific node which will block other threads trying to acquire a lock on the same node. However, we'll also see that threads performing a search will not acquire locks at all and may read a node which is locked by another thread.\n\n#### Concurrent B+ Trees\n\nLet's see what goes wrong when we concurrently search and insert into a B+ tree without any form of concurrency control. Consider a fragment of a B+ tree shown below with two nodes x and y. Imagine a thread is searching for the value 2 and reads the pointer to y from x.\n\n```\n            +-------+\n          x | 5 |   |\n            +-------+\n           /    |\n  +-------+    ...\ny | 1 | 2 |\n  +-------+\n```\n\nThen, another thread inserts the value 3 which reorganizes the tree like this:\n\n```\n            +-------+\n          x | 2 | 5 |\n            +-------+\n           /    |    \\\n  +-------+ +-------+ ...\ny | 1 |   | | 2 | 3 |\n  +-------+ +-------+\n```\n\nNext, the searching thread reads from y but cannot find the value 2!\n\nClearly, concurrently searching and inserting into a B+ tree requires some sort of locking. There are already a number of locking protocols:\n\n - The simplest protocol requires searches and insertions to lock every node along their path from root to leaf. This protocol is correct, but limits concurrency.\n - Smarter protocols have insertions place write intention locks along a path and upgrade those locks to exclusive locks when performing a write. Searches can read nodes with write intention locks on them but not with exclusive locks on them.\n - Even smarter protocols lock a subsection of the tree and bubble this subsection upwards through the tree. B-link trees will do something similar but will guarantee that at most three nodes are locked at any given point in time.\n\n#### B-link Trees\n\n - Typically, an internal node in a B+ tree with $n$ keys has $n + 1$ pointers. For example, if an internal node has keys $(5, 10, 15)$, then it has four pointers for values in the range $[-\\infty, 5)$, $[5, 10)$, $[10, 15)$, and $[15, \\infty)$. Internal nodes in a B-link tree with $n$ keys have $n$ pointers where the last key is known as the <strong>high key</strong>. For example, if an internal node has keys $(5, 10, 15)$ then it has three pointers for values in the range $[-\\infty, 5)$, $[5, 10)$, and $[10, 15)$.\n - In a B+ tree, leaves are linked together, but internal nodes are not. In a B-link tree, all sibling nodes (internal nodes and leaves) are linked together left to right.\n\n#### Search Algorithm\n\nThe search algorithm for a B-link tree is very similar to the search algorithm for a B+ tree. To search for a key $k$, we traverse the tree from root to leaf. At every internal node, we compare $k$ against the internal node's keys to determine which child to visit next.\n\nHowever, unlike with a B+ tree, we might have to walk rightward along the B-link tree to find the correct child pointer. For example, imagine we are searching for the key $20$ at an internal node $(5, 10, 15)$. Because $20 \\gt 15$, we have to walk rightward to the next internal node which might have keys $(22, 27, 35)$. We do something similar at leaves as well to find the correct value.\n\nNote that searching does not acquire any locks.\n\n#### Insertion Algorithm\n\nTo insert a key $k$ into a B-link tree, we begin by traversing the tree from root to leaf in exactly the same way as we did for the search algorithm. We walk downwards and rightwards and do not acquire locks. One difference is that we maintain a stack of the rightmost visited node in each level of the tree. Later, we'll use the stack to walk backwards up the tree.\n\nOne we reach a leaf node, we acquire a lock on it and crab rightward until we reach the correct leaf node $a$. If $a$ is not full, then we insert $k$ and unlock $a$. If $a$ is full, then we split it into $a'$ (previously $a$) and $b'$ (freshly allocated). We flush $b'$ to disk and then flush $a'$ to disk. Next, we have to adjust the parent of $a'$ (formerly $a'$). We acquire a lock on the parent node and then crab rightward until we reach the correct parent node. At this point, we repeat our procedure upwards through the tree.\n\nAt worst, we hold three locks at a time.\n\n#### Correctness Proof\n\nTo prove that the B-link tree works as we intend, we have to prove three things:\n\n - First, we have to prove that multiple threads operating on a B-link tree cannot deadlock. This is straightforward. If we totally order nodes bottom-to-top and left-to-right, then threads always acquire locks according to this total order.\n - Second, we have to prove that whenever a node modifies a B-link tree, the B-link tree still appears like a valid tree to all nodes except the modifying thread. Again, this is straightforward. The insertion procedure only makes three writes to disk.\n   - Writing to a node that is not full clearly does not invalidate the tree.\n   - Writing a newly allocated $b'$ node does not affect the tree because there are not any pointers to it.\n   - Writing $a'$ atomically transitions the tree from one legal state to another.\n - Finally, we have to ensure that concurrently executing operations do not interfere with one another. See paper for proof (it's complicated).\n\n#### Deletion\nB-link trees punt on deletion. They simply let leaf nodes get underfull and periodically lock the whole tree and reorganize it if things get too sparse.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/319628.319663"
    },
    "1290": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/cacm/LampsonR80",
        "transcript": "In 1980, synchronization primitives like semaphores, monitors, and condition variables had been well studied in the literature, but there weren't any large systems implementing them. Mesa was a programming language that was being developed to write the Pilot operating system at Xerox. Due to the inherent concurrency of an operating system, Mesa was designed to ease the development of concurrent programs. The Mesa designers considered implementing a message passing interface, but deemed it too complex. They considered semaphores, but found them too undisciplined. They considered cooperative multi-threading but came upon a number of serious disadvantages:\n\n - Cooperative multithreading cannot take advantage of multiple cores.\n - Preemption is already required to service time-sensitive I/O devices.\n - Cooperation is at odds with modularity. Critical sections have no principled way of knowing if they are calling a function which yields control.\n\nEventually, Mesa settled on implementing monitors and condition variables and exposed a number of previously undiscussed issues:\n\n - What is the interface for dynamically spawning threads and waiting for them to terminate?\n - What is the interface for dynamically constructing monitors?\n - How are threads scheduled when waiting and notifying each other?\n - What are the semantics of wait when one monitor calls into another monitor which also calls wait?\n - How are exceptions and I/O devices handled?\n\nMesa allowed an arbitrary function call to be forked and run by a separate thread and eventually joined:\n\n\nhttps://i.imgur.com/McaDktY.png\n\nMoreover, if a forked thread was not intended to be joined, it could instead be detached via detach[p]. This fork/join style process management had a number of advantages---(i) processes were first class values, (ii) thread forking was type checked, and (iii) any procedure could be forked---but also introduced lots of opportunities for dangling references.\n\nMonitors are objects that only allow a single thread to be executing one of its functions at any given time. They unify data, synchronization of the data, and access of the data into one lexical bundle. Mesa monitors included public entry preocedures and private internal procedures that operated with the monitor locked as well as public external procedures that operated without locking the monitor. Monitors, in conjunction with condition variables, were used to maintain an invariant about an object that was true upon entering and exiting any of the monitor's methods. Monitors also lead to potential deadlocks:\n\n - Two monitor methods could wait for one another.\n - Two different monitors could enter each other.\n - A monitor M could enter a monitor N, then wait on a condition that could only be enabled by another thread entering M through N.\n\n\nSpecial care also had to be taken to avoid priority inversion.\n\nMesa also introduced Mesa semantics, best explained with this code snippet:\n\n```\n\"\"\"\nCondition variables typically obey one of two semantics:\n\n1. Under *Hoare semantics* [1], when a thread calls `notify` on a condition\n   variable, the execution of one of the threads waiting on the condition\n   variable is immediately resumed. Thus, a thread that calls `wait` can assume\n   very strong invariants are held when it is awoken.\n2. Under *Mesa semantics* [2], a call to `notify` is nothing more than a hint.\n   Threads calling `wait` can be woken up at any time for any reason.\n\nUnderstanding the distinction between Hoare and Mesa semantics can be\nsolidified by way of example. This program implements a concurrent queue (aka\npipe) to which data can be written and from which data can be read. It spawns a\nsingle thread which iteratively writes data into the pipe, and it spawns\n`NUM_CONSUMERS` threads that read from the pipe. The producer produces the same\nnumber of items that the consumers consume, so if all goes well, the program\nwill run and terminate successfully.\n\nRun the program; not all goes well:\n\n    Exception in thread Thread-3:\n    Traceback (most recent call last):\n      File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n        self.run()\n      File \"/usr/lib/python2.7/threading.py\", line 763, in run\n        self.__target(*self.__args, **self.__kwargs)\n      File \"hoare_mesa.py\", line 66, in consume\n        pipe.pop()\n      File \"hoare_mesa.py\", line 52, in pop\n        return self.xs.pop(0)\n    IndexError: pop from empty list\n\nWhy? The pipe is implemented assuming Python condition variables obey Hoare\nsemantics. They do not. Modify the pipe's implementation assuming Mesa\nsemantics and re-run the program. Everything should run smoothly!\n\n[1]: https://scholar.google.com/scholar?cluster=16665458100449755173&hl=en&as_sdt=0,5\n[2]: https://scholar.google.com/scholar?cluster=492255216248422903&hl=en&as_sdt=0,5\n\"\"\"\n\nimport threading\n\n# The number of objects read from and written to the pipe.\nNUM_OBJECTS = 10000\n\n# The number of threads consuming from the pipe.\nNUM_CONSUMERS = 2\n\n# An asynchronous queue (a.k.a. pipe) that assumes (erroneously) that Python\n# condition variables follow Hoare semantics.\nclass HoarePipe(object):\n    def __init__(self):\n        self.xs = []\n        self.lock = threading.Lock()\n        self.data_available = threading.Condition(self.lock)\n\n    # Pop the first element from the pipe, blocking if necessary until data is\n    # available.\n    def pop(self):\n        with self.lock:\n            # This code is incorrect beacuse Python condition variables follows\n            # Mesa, not Hoare, semantics. To correct the code, simply replace\n            # the `if` with a `while`.\n            if len(self.xs) == 0:\n                self.data_available.wait()\n            return self.xs.pop(0)\n\n    # Push a value to the pipe.\n    def push(self, x):\n        with self.lock:\n            self.xs.append(x)\n            self.data_available.notify()\n\ndef produce(pipe):\n    for i in range(NUM_OBJECTS):\n        pipe.push(i)\n\ndef consume(pipe):\n    assert NUM_OBJECTS % NUM_CONSUMERS == 0\n    for i in range(NUM_OBJECTS / NUM_CONSUMERS):\n        pipe.pop()\n\ndef main():\n    pipe = HoarePipe()\n    producer = threading.Thread(target=produce, args=(pipe,))\n    consumers = [threading.Thread(target=consume, args=(pipe,))\n                 for i in range(NUM_CONSUMERS)]\n\n    producer.start()\n    for consumer in consumers:\n        consumer.start()\n\n    producer.join()\n    for consumer in consumers:\n        consumer.join()\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThreads waiting on a condition variable could also be awoken by a timeout, an abort, or a broadcast (e.g. notify_all).\n\nMesa's implementation was divided between the processor, a runtime, and the compiler. The processor was responsible for process management and scheduling. Each process was on a ready queue, monitor lock queue, condition variable queue, or fault queue. The runtime was responsible for providing the fork/join interface. The compiler performed code generation and a few static sanity checks.\n\nMesa was evaluated by Pilot (an OS), Violet (a distributed calendar), and Gateway (a router).\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/358818.358824"
    },
    "1291": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/sigops/LauerN79",
        "transcript": "Lauer and Needham explain the duality in expressiveness and performance between\n\n - message-oriented concurrency models in which there are a small number of fixed tasks that communicate explicitly, and\n - process-oriented concurrency models in which there are a larger number of dynamic processes that share memory.\n\nMessage-oriented systems can be characterized by the following hallmarks, consequences, and provided facilities.\n\n| Hallmark                                                                                 | Consequences                                                                                            | Facilities                                                                                              |\n|------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|\n| Long standing communication channels are typically created during program initialization | Synchronization is implicitly performed in the queues connecting processes                              | Messages and message ids                                                                                |\n| There are a fewer number of long lasting processes                                       | Shared structures are passed by reference; only processes with a reference to a structure can act on it | Message channels and ports that provide the ability to Send, WaitForReply, WaitForMessage, or SendReply |\n| Processes don't share memory                                                             | Peripheral devices are treated like processes and communicated with                                     | Process creation (but no deletion)                                                                      |\n| Processes read a small number of messages at a time                                      |                                                                                                         |                                                                                                         |\n\nProcess-oriented systems can be similarly characterized:\n\n| Hallmark                                                 | Consequences                                                         | Facilities                     |\n|----------------------------------------------------------|----------------------------------------------------------------------|--------------------------------|\n| Global data can be protected and accessed via interfaces | Synchronization is performed in locks                                | Procedures                     |\n| Process creation and deletion is a lightweight task      | Data is shared directly, with small portions being locked            | fork/join procedure invocation |\n| Processes typically have a single job                    | Peripheral interaction typically involves locking and sharing memory | Modules and monitors           |\n| Module instantiation                                     |                                                                      |                                |\n| Condition variables                                      |                                                                      |                                |\n\nThere is a duality between the two concurrency models. Any program in one has a corresponding program written in the other. Lauer and Needham demonstrate the duality not by simulating model's primitives using the other, but by drawing similarities between the two's components:\n\n| Message-oriented                        | Process-oriented                  |\n|-----------------------------------------|-----------------------------------|\n| Processes, CreateProcess                | Monitors, NEW/START               |\n| Message Channels                        | External Procedure identifiers    |\n| Message Ports                           | ENTRY procedure identifiers       |\n| SendMessage; AwaitReply                 | simple procedure call             |\n| SendMessage; ... AwaitReply             | FORK; ... JOIN                    |\n| SendReply                               | RETURN                            |\n| WaitForMessage loop with case statement | monitor lock, ENTRY attribute     |\n| arms of case statement                  | ENTRY procedure declarations      |\n| selective waiting for messages          | condition variables, WAIT, SIGNAL |\n\nThis correspondence can be used to directly rewrite a canonical program between the two models. In fact, the differences between the two models becomes simply a matter of keyword choice. Moreover, if both models are implemented with identical blocking and scheduling mechanism, then the two models lead to identical performance as well. Since the choice of model does not affect the user or implementor, the decision of which model to use should be based on the architecture of the underlying hardware.\n\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/850657.850658"
    },
    "1292": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/sigmod/SelingerACLP79",
        "transcript": "SQL queries declaratively describe some subset of the tuples in a database, but they do not specify the method by which these tuples should be retrieved. The same SQL query can be implemented in many different ways. For example, a single-relation query can be implemented with a full table scan, with an index scan, with an index-only scan, etc. Each of these different methods of accessing tuples is called an access path, and it's the job of the query optimizer to select the most efficient access path. This paper explores how System R's query optimizer selects access paths.\n\n#### Processing of a SQL Statement\n\nA SQL query goes through four phases: parsing, optimization, code generation, and execution. First, the query is parsed where it is decomposed into a set of SELECT-FROM-WHERE query blocks. Then, it is optimized. The optimizer first typechecks the query against type information in the catalog. It then chooses an order to evaluate the blocks and for each block chooses an access path. Each access path is expressed in the Access Specification Language (ASL). Then, the ASL plans are compiled to machine code by a table-driven code generator that maps specific forms of joins to precompiled machine code. Subqueries are treated as subroutines. Finally, the query is ready to be executed.\n\n#### Research Storage System\n\nThe Research Storage System (RSS) is System R's storage subsystem that is responsible for managing the physical layout and physical access of relations. Tuples are stored in 4KB pages, and pages are logically organized into segments. Tuples from different relations can share the same pages (each tuple is annotated with the id of its relation), but each relation is sequestered to a single segment. Tuples are accessed through the tuple-oriented Relation Storage Interface (RSI) which supports an OPEN/NEXT/CLOSE scan interface. The RSI supports full segment scans as well as B-tree backed index scans (including range scans). Moreover, the RSI accepts a set of search arguments or (SARGS)\u2014a collection of predicates of the form column op value in disjunctive normal form\u2014to filter the returned tuples. If a predicate is in the form column op value, we say it's a sargable predicate.\n\n#### Costs for Single Relation Access Paths\n\nSystem R's query optimizer tries to select an access path which minimizes cost as defined by the following formula:\n\n```\nCOST = PAGE_FETCHES + w*RSI_CALLS\n```\n\nPAGE_FETCHES (the number of page fetches) is a measure of the amount of I/O a query has to perform, and RSI_CALLS (the number of calls to the RSI) is a measure of the amount of CPU a query has to perform. RSI_CALLS also approximates the number of tuples a query returns.\n\nThe System R catalog maintains the following statistics which are used by the query optimizer. They are updated periodically.\n\n - NCARD(T): the cardinality of relation T.\n - TCARD(T): the number of pages that hold tuples from relation T.\n - P(T): TCARD(T) divided by the number of pages in T's segment.\n - ICARD(I): The number of distinct keys in index I.\n - NINDX(I): the number of pages in index I.\n\nThe WHERE clause of a query is considered in conjunctive normal form, and each conjunct is called a boolean factor. The query optimizer estimates a selectivity factor F for each boolean factor with the following rules.\n\n| column = value                   | F = 1 / ICARD(column index)                                                              | If there exists an index.         |\n|----------------------------------|------------------------------------------------------------------------------------------|-----------------------------------|\n| column = value                   | F = 1 / 10                                                                               | If there does not exist an index. |\n| column1 = column2                | F = 1 / MAX(ICARD(columnn1 index), ICARD(columnn2 index))                                | If there exists two indexes.      |\n| column1 = column2                | F = 1 / ICARD(columnni index)                                                            | If there exists one index.        |\n| column1 = column2                | F = 1 / 10                                                                               | If there does not exist an index. |\n| column > value                   | F = (high key - value) / (high key - low key)                                            | If column is arithmetic.          |\n| column > value                   | F = 1/3                                                                                  | If column is not arithmetic.      |\n| column BETWEEN value1 AND value2 | F = (value2 - value1) / (high key - low key)                                             | If column is not arithmetic.      |\n| column BETWEEN value1 AND value2 | F = 1/4                                                                                  | If column is not arithmetic.      |\n| column IN (list of values)       | F = (number of items in list) * (F for column=value)                                     | Capped at 1/2.                    |\n| column IN subquery               | F = (expected cardinality of subquery result) / (product of subquery FROM cardinalities) |                                   |\n| a OR b                           | F = F(a) + F(b) - F(a)*F(b)                                                              |                                   |\n| a AND b                          | F = F(a)*F(b)                                                                            |                                   |\n| NOT a                            | F = 1 - F(a)                                                                             |                                   |\n\nThe cardinality of query (QCARD) is the product of the sizes of the relations in the FROM clause multiplied by the selectivity factor of every boolean factor in the WHERE clause. The number of RSI calls (RSICARD) is the product of the sizes of the relations in the FROM clause multiplied by the selectivity of the sargable boolean factors.\n\nSome access paths produce tuples in a particular order. For example, an index scan produces tuples in the order of the index key. If this order is consistent with the order of a GROUP BY or ORDER BY clause, we say it is an interesting order. The query optimizer computes the minimum cost unordered plan and the minimum cost plan for every interesting order. After taking into account the (potential) additional overhead of sorting unordered tuples for a GROUP BY or ORDER BY, the least cost plan is selected.\n\nThe following costs include the number of index pages fetched, then the number of data pages fetched, and then the number of RSI calls weighted by W.\n\n| Unique index matching an equal predicate.                   | 1 + 1 + W                               |\n|-------------------------------------------------------------|-----------------------------------------|\n| Clustered index I matching one or more boolean factors.     | F(preds)*(NINDX(I) + TCARD) + W*RSICARD |\n| Non-clustered index I matching one or more boolean factors. | F(preds)*(NINDX(I) + NCARD) + W*RSICARD |\n| Clustered index I not matching any boolean factors          | NINDX(I) + TCARD + W*RSICARD            |\n| Non-clustered index I not matching any boolean factors      | NINDX(I) + NCARD + W*RSICARD            |\n| Segment scan.                                               | TCARD/P + W*RSICARD                     |\n\n#### Access Path Selection for Joins\n\nThe System R query optimizer considers access plans with (pipelined) tuple-nested loop joins and sort-merge joins. The most critical part of choosing an access plan is choosing a join order. There are n! left-deep access plans for n relations (that's a lot). To avoid enumerating all of them, the query optimizer uses dynamic programming.\n\nFirst, it determines the cheapest single-relation access path for each relation and for each interesting order. Note that interesting orders now include ordering by a GROUP BY or ORDER BY clause and any joining predicates which could take advantage of the order with a sort-merge join. Then, it determines the cheapest 2-way join with each single-relation access path as the outer relation. Then, it determines the cheapest 3-way join with the 2-way joins as the outer relation. And so on.\n\nThe query optimizer performs a couple of tricks to speed up this algorithm. First, it does not consider a cross-join if there are other more selective joins possible. Second, it computes interesting order equivalence classes to avoid computing redundant interesting orders. For example, if there are predicates E.DNO = D.DNO and D.DNO = F.DNO, then all three columns belong to the same equivalence class.\n\nThis algorithm computes at worst (2n times the number of interesting orders) intermediate access paths.\n\n#### Nested Queries\n\nNon-correlated subqueries are evaluated once before their parent query. Correlated subqueries are evaluated every time the parent query is evaluated. As an optimization, we can sort the parent tuples by the correlated column and compute the subquery once for every unique value of the correlated column.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/582095.582099"
    },
    "1293": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/ds/GrayLPT76",
        "transcript": "Locks are needed in a database system to ensure that transactions are isolated from one another. But what exactly should be locked?\n\nAt one extreme, we could lock the entire database using a single lock. This coarse-grained approach has incredibly low locking overhead; only one lock is ever acquired. However, it limits the amount of concurrency in the system. Even if two transactions operate on disjoint data, they cannot run concurrently using a single global lock.\n\nAt the other extreme, we could lock individual fields inside of records. This fine-grained approach has incredibly high concurrency. Two transactions could execute concurrently on the same record, so long as they access disjoint fields! However, this approach has very high locking overhead. If the transaction needs to read a lot of fields from a lot of records, it will spend a lot of time acquiring a lot of locks.\n\nA compromise between these to extremes is to use multiple granularity locking, where a transaction can choose the granularity of its locks. For example, one transaction may lock a table, another may lock a page, and another may lock a record. Note, however, that unlike with single granularity locking, care must be taken to ensure that locks at different granularities do not conflict. For example, imagine one transaction has an exclusive lock on a page; another transaction must be prohibited from acquiring an exclusive lock on the table that the page belongs to.\n\nIn this paper, Gray et al. present an implementation of multiple granularity locking that exploits the hierarchical nature of databases. Imagine a database's resources are organized into a hierarchy. For example, a database has tables, each table has pages, and each page has records. A transaction can acquire a lock on any node in this hierarchy of one of the following types:\n\n - IS: An intention shared lock on a node indicates that a transaction plans on acquiring a shared lock on one of the descendants of the node.\n - IX: An intention exclusive lock on a node indicates that a transaction plans on acquiring an exclusive lock on one of the descendants of the node.\n - S: A shared lock on a node implicitly grants the transaction shared read access to the subtree rooted at the node.\n - SIX: A SIX lock on a node implicitly grants the transaction shared read access to the subtree rooted at the node and simultaneously indicates that the same transaction may acquire an exclusive lock on one of the descendants of the node.\n - X: An exclusive lock on a node implicitly grants the transaction exclusive read and write access to the subtree rooted at the node.\n\nTransactions acquire locks starting at the root and obey the following compatibility matrix:\n\nhttps://i.imgur.com/ECnVvXB.png\n\nMore specifically, these are the rules for acquiring locks:\n\n1. If a transaction wants an S or IS lock on a node, it must acquire an IX or IS lock on its parent.\n2. If a transaction wants an X, SIX, or IX lock on a node, it must acquire a SIX, or IX lock on its parent.\n3. Locks are either released in any order all at once after the transaction or released from leaf to root.\n\nThis locking protocol can easily be extended to directed acyclic graphs (DAGs) as well. Now, a node is implicitly shared locked if one of its parents is implicitly or explicitly shared locked. A node is implicitly exclusive locked if all of its parents are implicitly or exclusive exclusive locked. Thus when a shared lock is acquired on a node, it implicitly locks all nodes reachable from it. When an exclusive lock is acquired on a node, it implicitly locks all nodes dominated by it.\n\nThe paper proves that if two lock graphs are compatible, then the implicit locks on the leaves are compatible. Intuitively this means that the locking protocol is equivalent to the naive scheme of explicitly locking the leaves, but it does so without the locking overhead.\n\nThe protocol can again be extended to dyamic lock graphs where the set of resources changes over time. For example, we can introduce index interval locks that lock an interval of the index. To migrate a node between parents, we simply acquire X locks on the old and new location.\n\n#### Degrees of Consistency\n\nEnsuring serializability is expensive, and some applications can get away with weaker consistency models. In this paper, Grey et al. present three definitions of four degrees of consistency.\n\nFirst, we can informal define what it means for a transaction to observe degree i consistency.\n\n - Degree 0: no dirty writes.\n - Degree 1: Degree 0 plus no writes are committed until the end of the transaction.\n - Degree 2: Degree 1 plus no dirty reads.\n - Degree 3: Degree 2 plus repeatable reads.\n\nSecond, we can provide definitions based on locking protocols.\n\n - Degree 0: Short X locks.\n - Degree 1: Long X locks\n - Degree 2: Long X locks and short read locks.\n - Degree 3: Long X locks and long read locks.\n\nFinally, we can define what it means for schedule to have degree i consistency. A transaction is a sequence of begin, end, S, X, unlock, read, and write actions beginning with a begin and ending with an end. A schedule is a shuffling of multiple transactions. A schedule is serial if every transaction is run one after another. A schedule is legal if obeys a locking protocol. A schedule is degree i consistent if every transaction observes degree i consistency according to the first definition.\n\n - Assertion 1. Definition 2 implies definition 3. That is, using the locking protocol for degree i ensures degree i consistent schedules.\n - Assertion 2. Transactions can pick their consistency. \n\nhttps://i.imgur.com/PFmcMJv.png\n\n\n",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1294": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/cacm/RitchieT74",
        "transcript": "Unix was an operating system developed by Dennis Ritchie, Ken Thompson, and others at Bell Labs. It was the successor to Multics and is probably the single most influential piece of software ever written.\n\nEarlier versions of Unix were written in assembly, but the project was later ported to C: probably the single most influential programming language ever developed. This resulted in a 1/3 increase in size, but the code was much more readable and the system included new features, so it was deemed worth it.\n\nThe most important feature of Unix was its file system. Ordinary files were simple arrays of bytes physically stored as 512-byte blocks: a rather simple design. Each file was given an inumber: an index into an ilist of inodes. Each inode contained metadata about the file and pointers to the actual data of the file in the form of direct and indirect blocks. This representation made it easy to support (hard) linking. Each file was protected with 9 bits: the same protection model Linux uses today. Directories were themselves files which stored mappings from filenames to inumbers. Devices were modeled simply as files in the /dev directory. This unifying abstraction allowed devices to be accessed with the same API. File systems could be mounted using the mount command. Notably, Unix didn't support user level locking, as it was neither necessary nor sufficient.\n\nProcesses in Unix could be created using a fork followed by an exec, and processes could communicate with one another using pipes. The shell was nothing more than an ordinary process. Unix included file redirection, pipes, and the ability to run programs in the background. All this was implemented using fork, exec, wait, and pipes.\n\nUnix also supported signals.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/361011.361061"
    },
    "1295": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jacm/LiuL73",
        "transcript": "Consider a hard-real-time environment in which tasks must finish within some time after they are requested. We make the following assumptions.\n\n - (A1) Tasks are periodic with fixed periods.\n - (A2) Tasks must finish before they are next requested.\n - (A3) Tasks are independent.\n - (A4) Tasks have constant runtime.\n - (A5) Non-periodic tasks are not realtime.\n\nThus, we can model each task $t_i$ as a period $T_i$ and runtime $C_i$.  A scheduling algorithm that immediately preempts tasks to guarantee that the task with the highest priority is running is called a preemptive priority scheduling algorithm. We consider three preemptive priority scheduling algorithms: a static/fixed priority scheduler (in which priorities are assigned ahead of time), a dynamic priority scheduler (in which priorities are assigned at runtime), and a mixed scheduling algorithm.\n\n#### Fixed Priority Scheduling Algorithm\nFirst, a few definitions:\n\n - The deadline of a task is the time at which the next request is issued.\n - An overflow occurs at time $t$ if $t$ is the deadline for an unfulfilled task.\n - A schedule is feasible if there is no overflow.\n - The response time of a task is the time between the task's request and the task's finish time.\n - A critical instant for task  $t$ is the instant where  $t$ has the highest response time.\n\nIt can be shown that the critical instant for any task occurs when the task is requested simultaneously with all higher priority tasks. This result lets us easily determine if a feasible fixed priority schedule exists by pessimistically assuming all tasks are scheduled at their critical instant.\n\nIt also suggests that given two tasks with periodicities $T_1$ and $T_2$ where $T_1 &lt; T_2$, we should give higher priority to the shorter task with period $T_1$. This leads to the rate-monotonic priority scheduling algorithm where we assign higher priorities to shorter tasks. A feasible static schedule exists if and only if a feasible rate-monotonic scheduling algorithm exists.\n\nDefine processor utilization to be the fraction of time the processor spends running tasks. We say a set of tasks fully utilize the processor if there exists a feasible schedule for them, but increasing the running time of any of the tasks implies there is no feasible schedule. The least upper bound on processor utilization is the minimum processor utilization for tasks that fully utilize the processor. For $m$ tasks, the least upper bound is $m(2^{1/m} - 1)$ which approaches $\\ln(2)$ for large $m$.\n\n#### Deadline Driven Scheduling Algorithm\nThe deadline driven scheduling algorithm (or earliest deadline first scheduling algorithm) dynamically assigns the highest priority to the task with the most imminent deadline. This scheduling algorithm has a least upper bound of 100% processor utilization. Moreover, if any feasible schedule exists for a set of tasks, a feasible deadline driven schedule exists.\n\n#### Mixed Scheduling Algorithm.\nScheduling hardware (at the time) resembled a fixed priority scheduler, but a dynamic scheduler could be implemented for less frequent tasks. A hybrid scheduling algorithm scheduled the \n$k$ most frequent tasks using the rate-monotonic scheduling algorithm and scheduled the rest using the deadline driven algorithm.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/321738.321743"
    },
    "1296": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=Codd70cacm",
        "transcript": "In this paper, Ed Codd introduces the relational data model. Codd begins by motivating the importance of data independence: the independence of the way data is queried and the way data is stored. He argues that existing database systems at the time lacked data independence; namely, the ordering of relations, the indexes on relations, and the way the data was accessed was all made explicit when the data was queried. This made it impossible for the database to evolve the way data was stored without breaking existing programs which queried the data. The relational model, on the other hand, allowed for a much greater degree of data independence. After Codd introduces the relational model, he provides an algorithm to convert a relation (which may contain other relations) into first normal form (i.e. relations cannot contain other relations). He then describes basic relational operators, data redundancy, and methods to check for database consistency.\n\nCommentary\n1. Codd's advocacy for data independence and a declarative query language have stood the test of time. I particularly enjoy one excerpt from the paper where Codd says, \"The universality of the data sublanguage lies in its descriptive ability (not its computing ability)\".\n2. Database systems at the time generally had two types of data: collections and links between those collections. The relational model represented both as relations. Today, this seems rather mundane, but I can imagine this being counterintuitive at the time. This is also yet another example of a unifying interface which is demonstrated in both the Unix and System R papers.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1297": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeZRS15",
        "transcript": "Sources:\n- https://arxiv.org/pdf/1512.03385.pdf\n- http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf\n\nSummary:\n- Took the first place in Imagenet 5 main tracks\n- Revolution of depth: GoogLeNet was 22 layers with 6.7 top-5 error, \nResnet is 152 layers with 3.57 top-5 error\n- Light on complexity: the 34 layer baseline is 18% of the FLOPs(multiply-adds) of VGG.\n    - Resnet 152 has lower time complexity than VGG-16/19\n- Extends well to detection and segmentation tasks\n- Just stacking more layers gives worse performance. Why? In theory:\n    > A deeper model should not have\n    higher training error\n    \u2022 A solution by construction:\n    \u2022 original layers: copied from a\n    learned shallower model\n    \u2022 extra layers: set as identity\n    \u2022 at least the same training error\n    \u2022 Optimization difficulties: solvers\n    cannot find the solution when going\n    deeper\u2026\n- Why do the residual connections help? it's easier to learn a residual mapping w.r.t. identity. \n    - If identity were optimal, easy to set weights as 0\n    - >If the optimal function is closer to an identity\nmapping than to a zero mapping, it should be easier for the\nsolver to find the perturbations with reference to an identity\nmapping, than to learn the function as a new one. We show\nby experiments (Fig. 7) that the learned residual functions in\ngeneral have small responses, suggesting that identity mappings\nprovide reasonable preconditioning.\n- Basic design (VGG-style)\n    - all 3x3 conv (almost)\n    - spatial size /2 => # filters x2\n    - Simple design; just deep!\n    - Other remarks:\n        - no max pooling (almost)\n        - no hidden fc\n        - no dropout\n- Training\n    - All plain/residual nets are trained from scratch\n    - All plain/residual nets use Batch Normalization\n    - Standard hyper-parameters & augmentation\n- The learned features are well transferable to other tasks\n    - Works well with Faster RCNN\n    - Works well with semantic instance segmentation\n \nAlso skimmed:\n- Deep Residual Networks with 1K Layers\nhttps://github.com/KaimingHe/resnet-1k-layers \nhttps://arxiv.org/pdf/1603.05027.pdf",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.03385"
    },
    "1298": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1612.01925",
        "transcript": "- Implementations:    \n    - https://hub.docker.com/r/mklinov/caffe-flownet2/\n    - https://github.com/lmb-freiburg/flownet2-docker\n    - https://github.com/lmb-freiburg/flownet2\n- Explanations:\n    - A Brief Review of FlowNet - not a clear explanation\n    https://medium.com/towards-data-science/a-brief-review-of-flownet-dca6bd574de0\n    - https://www.youtube.com/watch?v=JSzUdVBmQP4\nSupplementary material: \nhttp://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ilg_FlowNet_2.0_Evolution_2017_CVPR_supplemental.pdf",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1612.01925"
    },
    "1299": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1711.07618",
        "transcript": "It's like mask rcnn but for salient instances. \ncode will be available at https://github.com/RuochenFan/S4Net.\n\nThey invented a layer \"mask pooling\" that they claim is better than ROI pooling and ROI align.\n\n>As can be seen, our proposed\nbinary RoIMasking and ternary RoIMasking both outperform\nRoIPool and RoIAlign in mAP0.7\n. Specifically, our\nternary RoIMasking result improves the RoIAlign result by\naround 2.5 points. This reflects that considering more context\ninformation outside the proposals does help for salient\ninstance segmentation\n\n\nImportant benchmark attached: \nhttps://i.imgur.com/wOF2Ovz.png",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1711.07618"
    },
    "1300": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1705.07426",
        "transcript": "# Metadata\n* **Title**: The Do\u2019s and Don\u2019ts for CNN-based Face Verification\n* **Authors**: Ankan Bansal Carlos Castillo Rajeev Ranjan Rama Chellappa\nUMIACS - \nUniversity of Maryland, College Park\n* **Link**: https://arxiv.org/abs/1705.07426\n\n# Abstract\n>Convolutional neural networks (CNN) have become the most sought after tools for addressing object recognition problems. Specifically, they have produced state-of-the art results for unconstrained face recognition and verification tasks. While the research community appears to have developed a consensus on the methods of acquiring annotated data, design and training of CNNs, many questions still remain to be answered. In this paper, we explore the following questions that are critical to face recognition research: (i) Can we train on still images and expect the systems to work on videos? (ii) Are deeper datasets better than wider datasets? (iii) Does adding label noise lead to improvement in performance of deep networks? (iv) Is alignment needed for face recognition? We address these questions by training CNNs using CASIA-WebFace, UMDFaces, and a new video dataset and testing on YouTubeFaces, IJBA and a disjoint portion of UMDFaces datasets. Our new data set, which will be made publicly available, has 22,075 videos and 3,735,476 human annotated frames extracted from them.\n\n# Introduction\n>We make the following main contributions in this paper:\n\u2022 We introduce a large dataset of videos of over\n3,000 subjects along with 3,735,476 human annotated\nbounding boxes in frames extracted from these videos.\n\u2022 We conduct a large scale systematic study about the\neffects of making certain apparently routine decisions\nabout the training procedure. Our experiments clearly\nshow that data variety, number of individuals in the\ndataset, quality of the dataset, and good alignment are\nkeys to obtaining good performance.\n\u2022 We suggest the best practices that could lead to an improvement\nin the performance of deep face recognition\nnetworks. These practices will also guide future data\ncollection efforts.\n\n# How they made the dataset\n- collect youtube videos\n- automated filtering with yolo and landmark detection projects\n- crowd source final filtering (AMT - give 50 face images to turks and ask which don't belong)\n- quality control through sentinels: give turks the same test but with 5 known correct answers, \nand rank the turks according to how they perform on this ground truth test. \nIf they're good, trust their answers on the real tests.\n- result: \n    > we have 3,735,476 annotated frames in 22,075 videos. We will\n    publicly release this massive dataset\n\n# Questions and experiments\n## Do deep recognition networks trained on stills perform well on videos?\n> We study the effects of this difference between\nstill images and frames extracted from videos in section\n3.1 using our new dataset. We found that mixing both\nstill images and the large number of video frames during\ntraining performs better than using just still images or video\nframes for testing on any of the test datasets\n\n## What is better: deeper or wider datasets?\n>In section 3.2 we investigate the impact of using a deep\ndataset against using a wider dataset. For two datasets with\nthe same number of images, we call one deeper than the\nother if on average it has more images per subject (and\nhence fewer subjects) than the other. We show that it is\nimportant to have a wider dataset than a deeper dataset with\nthe same number of images.\n\n## Does some amount of label noise help improve the performance of deep recognition networks?\n>When training any supervised face classification system,\neach image is first associated with a label. Label noise is the\nphenomenon of assigning an incorrect label to some images.\nLabel noise is an inherent part of the data collection process.\nSome authors intentionally leave in some label noise [25, 6,\n7] in the dataset in hopes of making the deep networks more\nrobust. In section 3.3 we examine the effect of this label\nnoise on the performance of deep networks for verification\ntrained on these datasets and demonstrate that clean datasets\nalmost always lead to significantly better performance than\nnoisy datasets.\n\n## Does thumbnail creation method affect performance?\n>... This leads to generation of different types\nof bounding boxes for faces. Verification accuracy can\nbe affected by the type of bounding box used. In addition,\nmost recent face recognition and verification methods\n[35, 31, 33, 5, 9, 34] use some kind of 2D or 3D alignment\nprocedure [41, 14, 28, 8]. ... In section 3.4 we study the consequences\nof using different thumbnail generation methods\non verification performance of deep networks. We show\nthat using a good keypoint detection method and aligning\nfaces both during training and testing leads to the best performance.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1705.07426"
    },
    "1301": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/pami/SivicZ09",
        "transcript": "This paper presents an efficient object retrieval approach that employs methods from statistical text retrieval. High level features (visual analogy of words) are are provided by vector quantizing low level features (region descriptors). The use of high level features and techniques of text retrieval significantly reduce the matching cost.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/TPAMI.2008.111"
    },
    "1302": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/GongL11",
        "transcript": "Similarity-preserving binary code obtained from quantization can efficiently accelerate the retrieval of large-scale image collections. This paper presents iterative quantization (ITQ) that iteratively minimizes quantization error by rotating the data before quantization. ITQ can be couple with any projection of data onto an orthogonal basis. Experiment results show outstanding performance on the retrieval of large-scale image collections especially when the length of binary code is short.\nTechnical details and results\n\nIdea of ITQ\n\nFigure 1 illustrate the idea of rotating data before quantization to reduce quantization error.\n\n![](http://2.bp.blogspot.com/-riBFQ71aWKk/VRI4Aprwx5I/AAAAAAAAAxw/ggFrB14UGlw/s1600/toy.png)\n\nFigure 1. Toy illustration of the proposed ITQ method\nThe data points (blue points) are quantized to the closest vertex of the binary cube. By rotating the data points to that shown in Figure 1 (c), the quantization error is reduced and the partitioning respects the structure of the cluster.\n\nITQ in unsupervised code learning\n\nGiven a set of n data points and let each data point be d dimension, the data matrix is denoted by\n\n$X \\in \\mathbb{R}^{n \\times d}$\n\nThe binary code matrix can be computed by \n\n$B = sgn(XW)$ \n\n$W \\in \\mathbb{R}^{d \\times c}$\n\nwhere W denotes the projection matrix computed by PCA in this case.\n\nLet R denote any c x c orthogonal matrix. The quantization error after projection and rotation of the data matrix is denotes by\n\n$Q(B,R) = ||B - VR||^{2}_F$\n\n$V = XW$\n\nThe ITQ method minimizes the quantization error by seeking optimal R. Because of the quantization operator, the quantization error is not a smooth function and direct minimization of the quantization error is impractical. This paper propose optimizing R and B alternately like k-mean algorithm.  When R is fixed, B is computed by\n\n\nWhen B is fixed, R is computed by\n\n$B = sgn(VR)$\n\nwhere S and \"S hat\" are the left-singular vector and right-singular vector of the matrix \n         \n$B^TV$\n\nFigure 2 shows the quantization error for learning a 32-bit ITQ code on the CIFAR dataset.\n\n![](http://4.bp.blogspot.com/-F6wZGT5fl3E/VRJW5U2VvFI/AAAAAAAAAzg/aEt4DIDpV6E/s1600/quantization%2Berror.png)\n\nResults\n\n\"PCA-ITQ\" in the legend of the figures denote the proposed method.\n\n\n![](http://3.bp.blogspot.com/-WbVup__p9CQ/VRJTBh7NKpI/AAAAAAAAAx8/DCUvfo5Ijuw/s1600/result1.png)\n\n![](http://2.bp.blogspot.com/-dj6F3pd5WiM/VRJTBvF6G5I/AAAAAAAAAyA/WyW7He4TJOw/s1600/result2.png)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2011.5995432"
    },
    "1303": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=lleRoweis",
        "transcript": "This paper presents locally linear embedding (LLE) for nonlinear dimensionality reduction. LLE can learn the structure of the underlying low-dimensional manifold of the sampled data in high dimensional space. Therefore, LLE can preserve the distance in the manifold space much better than PCA. Unlike PCA which projects high dimensional space to low dimensional space with a global linear matrix, LLE seeks locally linear projections for locally linear patches formed by neighboring data points. Using many locally linear projections instead of a global linear projection is the key to nonlinear dimensionality reduction.\nTechnical details \n\nFig. 1 shows the problem if nonlinear dimensionality reduction.\n\n![](http://1.bp.blogspot.com/-Vpkju74n9w8/VSX3RGAjKtI/AAAAAAAAA08/NqJA7v1MLuQ/s1600/dimreduct.png)\n\n\nFig. 2 summarizes the LLE algorithm. The neighbors of each data point can be computed by K-nearest neighbor or by collecting the data points within a radius. The weights in step 2 reflect intrinsic geometric properties of the data that are invariant to locally linear projections. The third step finds new data points projected by locally linear projections in low-dimensional space.\n\n![](http://3.bp.blogspot.com/-kVz0nYKKSQ0/VSX3_Aio4lI/AAAAAAAAA1E/hrltFFQ7njY/s1600/LLEgraph.png)\n\n![](http://3.bp.blogspot.com/-bfgIDOavP5s/VSX23Jko4tI/AAAAAAAAA0s/IsE4Th0SN1Y/s1600/LLE.png)\n\n\n$\\varepsilon (W) = \\displaystyle\\sum\\_i | \\vec{X} = \\sum\\_j W\\_{ij} \\vec{X}\\_j|^2$\n\n$\\Phi (Y) = \\displaystyle\\sum\\_i | \\vec{Y} = \\sum\\_j W\\_{ij} \\vec{T}\\_j|^2$\n\n\nResults\n\nFigure 4 shows the results of dimensional reduction of images of lips using CFA and LLE.\n\n![](http://3.bp.blogspot.com/-STDK5O9TBX4/VSX4TyurcvI/AAAAAAAAA1M/XUoZ-CmnObQ/s1600/lips.png)\n",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1304": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/ToliasAJ13",
        "transcript": "Descriptors and matching kernel are key components in an image search system. This paper present a framework for matching kernels including non-aggregated kernel such as Hamming Embedding (HE) and aggregated kernel such as Bag-of-Words (BoW) and vector or locally aggregated descriptors (VLAD). To evaluate the effectiveness of aggregation, this paper introduces selective match kernel (SMK) (non-aggregated) and aggregated selective match kernel (ASMK) based on the framework. Experimental results show that ASMK outperforms SMK amd state-of-the-art methods because ASMK can deal with burstiness better than SMK.\n\nTechnical details \n\nThe frame work of matching kernel is described by the following general form.\n\n$$K(\\mathcal{X},\\mathcal{Y}) = \\gamma(\\mathcal{X})\\gamma(\\mathcal{Y})\n\\displaystyle\\sum\\_{c \\in C} w\\_c M (\\mathcal{X}\\_c,\\mathcal{Y}\\_c)$$\n\nwhere X and Y are the descriptors of two images, Xc and Yc are a subset of the descriptors that are assigned to a particular visual word, M denotes similarity function, wc is a scalar and gamma denotes normalization factor.\n\nThe proposed selective match kernel (SMK) is denoted by\n\n$$M\\_N(\\mathcal{X}\\_c,\\mathcal{Y}\\_c) = \n\\displaystyle\\sum\\_{x \\in \\mathcal{X}\\_c} \n\\displaystyle\\sum\\_{y \\in \\mathcal{Y}\\_c} \n\\sigma (\\phi(x)^T\\phi(y))$$\n\nNote that #$\\mathcal{X}\\_c$ times #$\\mathcal{Y}\\_c$ (# = number of )  matches (dot product) are needed for each visual word.\n\n\nThe proposed aggregated selective match kernel (ASMK) is denoted by\n\n![](http://1.bp.blogspot.com/-ocG18fK0TMM/VS8_MsfOeSI/AAAAAAAAA2k/BrXcXyF0lfM/s1600/ASMK.png)\n\nNote that only one match (dot product) is needed for each visual word.\n\nResults\n\nAs shown in Figure 5, ASMK outperform SMK and SMK-BURST. BURST refer to burstiness normalization.\n\n![](http://1.bp.blogspot.com/-PfG69JuQnxk/VS9B6BvrAfI/AAAAAAAAA20/_2eKsEiAdew/s1600/fig5.png)\n\nTable 4 shows that ASMK outperforms state-of-the-art methods.\n\n![](http://2.bp.blogspot.com/-3GtM-bH6C2c/VS9CRKqN3TI/AAAAAAAAA28/Grzn5QT_GhM/s1600/table%2B4.png)\n\nNote that all the results above are from the initial result set. Re-ranking approaches are not included.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ICCV.2013.177"
    },
    "1305": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=Hofmann99probabilisticlatent",
        "transcript": "Probabilistic latent semantic indexing (PLSI) is an approach for document retrieval by modeling the joint probability model of words and documents as a mixture of independent multinomial distribution conditioned by latent semantic classes. The model is based on two independence assumption. First, the observed words and documents are assumed to be generated independently. Second, conditioned on the latent class, words are generated independently of the specific document identity. Given that the number of classes is smaller than the number of documents, each class acts as a bottleneck variable in predicting the distribution of words conditioned on documents.\n\nTechnical details\n\nGiven a word w and a document d, their joint probability distribution is model as follows.\n\n$$P(d,w) = P(d)P(w|d), where$$\n\n$$P(w|d) = \\displaystyle\\sum\\_{z\\in Z} P(w|z)P(z|d)$$\n\nwhere $z$ denotes a latent class. \n\nFollowing the likelihood principle, one determines the distributions in (1) and (2) by maximization of the log-likelihood function\n\n$$\\mathcal{L} = \\displaystyle\\sum\\_{d \\in D} \\displaystyle\\sum\\_{w \\in W} n(d,w) log P(d,w)$$\n\nThe maximization is done by the Expectation Maximization (EM) algorithm.\n\nResults\n\n![](http://1.bp.blogspot.com/-eSKjS0950ac/VUHFJ2Vv7fI/AAAAAAAAA4U/bMGoVNh5_O0/s1600/result.png)",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1306": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/GirshickDDM14",
        "transcript": "This paper presents a object detection algorithm that improves mAP on PASCAL VOC dataset by over 20% to previous state-of-the-art. Unlike image classification which take an image or the center part of an image as input, object detection task requires an algorithm to detect bounding boxes of objects in an image. To use the high capacity CNN features in object detection, the proposed algorithm first generates region proposals. CNN features are extracted from those region proposals and are feed to a set of class-specific linear SVMs which tell whether objects are detected in those regions.\nTechnical details\n\nThe figure below show the object detection system in this paper.\n![](http://3.bp.blogspot.com/-O6e43qcpcYA/VWapFWyXt5I/AAAAAAAAA8c/rcjlQJAQ35s/s320/system.png)\n\nBecause the PASCAL VOC dataset is not large enough for training high capacity CNN features, this paper use supervised pre-training on a large auxiliary dataset (ILSVRC 2012). The CNN is then fine-tuned with a portion of the PASCAL VOC dataset.\n\nResults\n\nThe following table shows the detection mAP on VOC 2007 test.\n\n![](http://1.bp.blogspot.com/-AmEd1cI6iWs/VWaqnD1YYwI/AAAAAAAAA8o/07pfZpdvwck/s400/table%2B2.png)\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/CVPR.2014.81"
    },
    "1307": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZhangL15e",
        "transcript": "This paper apply temporal convolutional neural network on character input to learn abstract text concepts. Depending on application, the model can output the category of text or review sentiment. The model is trained from character level and do not require knowledge of syntax or semantic structure. Therefore, the model can work for various language including English and Chinese with little prior knowledge of languages.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.03719"
    },
    "1308": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=krizhevsky2012imagenet",
        "transcript": "Deep convolutional neural networks (DCNN) has been a popular model for image classification over the last few years. This paper proposes a DCNN structure, also known as AlexNet, for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC). To train AlexNet, which has 60 million parameters, this paper uses Rectified Linear Units (ReLU) and multiple GPU to accelerate training. This paper also report that using local response normalization and overlapping pooling can reduce error rate. To prevent over fitting, they suggest data augmentation and apply dropout in the fully connected layer. \nTechnical details\n\nThe following figure shows the architecture of AlexNet. It contains five convolutional and three fully connected layers. Response-normalization layers follow the first and second convolutional layers. Max-pooling layers follow the first and second response-normalization layers and the fifth convolutional layer.\n\n![](http://i.imgur.com/2iqwCq1.png)",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1309": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1308.3541",
        "transcript": "This paper is a direct extension of the work of \\cite{journals/corr/1305.2532}. Prior to this work, \\cite{journals/corr/1305.2532} proposed an algorithm for learning to predict fixed size lists using either a single greedy policy (SCP) or a list of sequentially applied policies (CONSEQOPT). Given a sub-modular reward function, \\cite{journals/corr/1305.2532} provided theoretical guarantees on the regret of the learned policies via a reduction to cost sensitive classification. In practice one can then approximate the cost sensitive classification loss with convex surrogate losses that can be efficiently learned.  In this work, the authors extend the framework of \\cite{journals/corr/1305.2532} to knapsack problems, wherein each element in the list has an associated length and the sum total lengths may not exceed a fixed budget. (Note that I intentionally use the word \"length\" to differentiate from the \"cost\" of cost-sensitive classification in the reduction, which gets a bit confusing in the paper as the authors overload the word \"cost\" frequently.) They extend the algorithm of \\cite{journals/corr/1305.2532} to be sensitive to the length of items added to the list, and extend the analysis of \\cite{journals/corr/1305.2532} to provide regret guarantees in this setting. The authors then apply their algorithm to the problem of multi-document summarization, where items correspond to sentences and the budget constraints the total length of the summary. They show an improvement over prior work with their new approach.  \n\nThe paper is reasonably well written and straightforward to follow, with the exception of containing lots of notation and terminology that is sometimes difficult to keep straight (e.g. the definitions of the words \"cost\" and \"weight\", and the use of \\ell as both cost sensitive losses and the length (cost? weight?) of each item for the budget.) The work itself is a useful extension to prior work that appears to be experimentally sound.  \n\nPros:  \n\n+ Novel extension of prior work to incorporate budgetary constraints \n+ Works well experimentally  \n\nCons:  \n- Overloading of symbols and words leads to confusion  \n\nReferences:  \n\\cite{journals/corr/1305.2532} Ross, Stephane, Zhou, Jiaji, Yue, Yisong, Dey, Debadeepta, and Bagnell, .A. Learning policies for contextual submodular prediction. In ICML, 2013.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1308.3541"
    },
    "1310": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/RefaatCD13",
        "transcript": "EDML is a recent algorithm for learning the parameters of a Bayesian Network (BN). EDML works by 'removing' some edges from a Bayesian or Markov Network in order to produce a simplified network in which exact inference is feasible. EDML removes edges in a manner tailored to parameter learning, by recognizing repeated structures in the so-called \"meta\" network.  While originally proposed for learning BN parameters, in this paper the authors show that EDML can be interpreted more generally  as a coordinate ascent algorithm. After recasting EDML in this light, they then show how it can be applied to learning in Markov Networks (MNs). Finally, they provide some preliminary experimental results on learning the parameters of small grids. In doing so, they compare EDML to an off-the-shelf conjugate gradient method.  \n\n**Assessment: **\nThere does not appear to be much novelty here. Parameter learning in BNs and MNs inherently involves optimization of a real valued function and coordinate ascent approaches (e.g. iterative proportional fitting) have been studied previously. While casting EDML in this manner is interesting, the benefit of doing so is not made clear - why is this perspective useful? If this perspective provided insight on how to optimize a particular loss function (e.g. 0/1 loss) then it would be very helpful. Finally, in the case of incomplete data, the authors claim that EDML is both different and more efficient than EM. I don't immediately see why this is true - variational forms of EM exist and this appears to be a specific form. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4963-edml-for-learning-parameters-in-directed-and-undirected-graphical-models"
    },
    "1311": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/ieicet/OmotoET14",
        "transcript": "This paper proposes two parallel inference algorithms for the Hierarchical Dirichlet Process (HDP) in a distributed/cluster setting. The proposed algorithms use a two-level approach to parallelization where the top level involves distributing the data to individual processors/machines across a symmetric multiprocessing (SMT) cluster and the second level utilizes existing algorithms earlier developed for parallel inference in HDP based models on each machine. The first algorithm uses the approximate distributed HDP (AD-HDP) algorithm of Newman et al (2009) whereas the second algorithm uses the Parallel HDP algorithm of Asuncion et al (2008). The proposed algorithms are compared against a full MPI implementation based on Parallel-HDP and are shown to scale better w.r.t. increasing the number of cores.  \n\nThe paper is pretty much a simple extension of the existing algorithms for distributed HDP by simply reusing then on each machine and synchronizing. There is no discussion whether the existing algorithms (AD-HDP and Parallel HDP) could further benefits from the the SMT architecture, discussion about possible communication strategies among processors, or how certain issues such as merging of topics are dealt with (vis-a-vis the AD-HDP). Some discussion would be nice. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://search.ieice.org/bin/summary.php?id=e97-d_4_815"
    },
    "1312": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/GensD13",
        "transcript": "This papers proposes a technique for learning the structure of SPNs by recursively splitting the data set along instances and dimensions. The data set is split by dimensions if the set of variables can be partitioned into independent subsets. Otherwise, it is split by instances. This paper improves upon previous work of Dennis and Ventura (NIPS 2012) by splitting on both instances and variables in a manner that fits in elegantly with the simplified recursive definition of SPNs used in the paper. The algorithm guarantees a locally optimal structure, if an independence oracle is available. Results show that the model is comparable to other graphical model learning approaches in terms of log probs but massively outperforms them wins in terms of time.\n\nThe proposed algorithm is a novel application of using simple clustering and mutual independence finding methods to SPN structure learning. The paper is well written and explains SPNs and the structure learning algorithm clearly.\n\nPros: \n- The authors do a thorough evaluation on a large number of data sets. \n- Significant gains in conditional log probs (though, as pointed out in the paper, this might be in part due to exact inference in SPN compared to lower-bounds in other models). \n- Much faster inference at the cost of slightly worse log probs.\n\nCons: \n- The algorithm makes hard decisions to split the data recursively. This makes portions of the data set completely inaccessible to each other after each recursion step. This might make the model very sensitive to any sub-optimal splits made early on. \n- Weak or higher-order relationships among variables that are not captured by the independence checker may be lost. \n- The algorithm can only learn SPNs where product nodes have disjoint scopes.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v28/gens13.html"
    },
    "1313": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1210-4885",
        "transcript": "This paper is about a complexity estimation of using distributed algorithm for branch-and-bound over graphical models. The paper proposes a distributed/parallel Branch-and-Bound algorithm and evaluates its efficiency for load balancing. The ability to search multiple sub problem in parallel for the same problem would speed up the search significantly. However, balancing the parallel search load is important for efficiency. This paper does a complete case study of learning how to parallelize And-Or graph using Branch and Bound search. They use a set of features collected from the graph (static) or from the search problem (dynamic) for the problem and evaluate each of the three learning cases: per problem instances, per problem class, and across problem class using linear and non-linear regression methods.T hey extensively evaluate all the possible combinations and show the pros and cons of using each type of learning. \n\nPros: The paper is well written, and covers all the possibilities for learning and has a good discussion that educates the reader. The evaluations are extensive and conclusive. The literature review is complete. \n\nCons: From a practicality point of view, the bottle neck problem is not discussed, when would parallelizing AOGBB be bad? Also the motivation for choosing the features that they used, are they standard set of features? (the good point is that they showed that some of the features were more important than other based on their experiments).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1210.4885"
    },
    "1314": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/aistats/LowdR13",
        "transcript": "The paper presents a learning algorithm for learning the structure of Markov Networks by   exploring the connection between Arithmetic Circuits and MNs. This allows them to employ their   previous work on learning ACs to learn a sub-set of MNs.\n\nThe main theme in the paper is to use \u201cinference complexity as a learning bias.\u201d Typically, in Markov network structure learning, one measures inference complexity using the number of edges added to the graph. In the present paper, the authors propose a more fine-grained measure: the number of edges added to an arithmetic circuit. As the authors show, this fine-grained measure yields an order of magnitude improvement in accuracy on several interesting benchmarks (I like the empirical evaluation).",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v31/lowd13a.html"
    },
    "1315": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ModiT13",
        "transcript": "This paper investigates a model which aims at predicting the order of events; each event is an english sentence. While previous methods relied on a graph representation to infer the right order, the proposed model is made of two stages. The first stage use a continuous representation of a verb frame, where the predicate and its arguments are represented by their word embeddings. A neural network is used to derive this continuous representation in order to capture the compositionality within the verb frame. The second stage uses a large margin extension of PRank. The learning scheme is very interesting: the error made by the ranker is used to update the ranker parameters, but is also back-propagated to update the NN parameters. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1312.5198"
    },
    "1316": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/EigenRS13",
        "transcript": "This paper extends the mixture-of-experts (MoE) model by stacking several blocks of the MoEs to form a deep MoE. In this model, each mixture weight is implemented with a gating network. The mixtures at each block is different. The whole deep MoE is trained jointly using the stochastic gradient descent algorithm. The motivation of the work is to reduce the decoding time by exploiting the structure imposed in the MoE model. The model was evaluated on the MNIST and speech monophone classification tasks.  ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1312.4314"
    },
    "1317": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SimonyanVZ13",
        "transcript": "This paper presents methods for visualizing the behaviour of an object recognition convolutional neural network. The first method generates a \"canonical image\" for a given class that the network can recognize. The second generates a saliency map for a given input image and specified class, that illustrates the part of the image (pixels) that influence the most the given class's output probability. This can be used to seed a graphcut segmentation and localize objects of that class in the input image. Finally, a connection between the saliency map method and the work of Zeiler and Fergus on using deconvolutions to visualize deep networks is established.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1312.6034"
    },
    "1318": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ContardoDAG13",
        "transcript": "The authors present a model that learns representations of sequential inputs on random trajectories through the state space, then feed those into a reinforcement learner, to deal with partially observable environments. They apply this to a POMDP mountain car problem, where the velocity of the car is not visible but has to be inferred from successive observations.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1312.6042"
    },
    "1319": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1301-3592",
        "transcript": "this paper uses the common 2-step procedure to first eliminate most of unlikely detection windows (high recall), then use a network with higher capacity for better discrimination (high precision). Deep learning (in the unsupervised sense) helps having features optimized for each of these 2 different tasks, adapt them for different situations (different robotics grippers) and beat hand-designed features for detection of graspable areas, using a mixture of inputs (depth + rgb + xyz). ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1301.3592"
    },
    "1320": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1301-5348",
        "transcript": "This paper presents a theoretical analysis and empirical validation of a novel view of feature extraction systems based on the idea of Nystrom sampling for kernel methods. The main idea is to analyze the kernel matrix for a feature space defined by an off-the-shelf feature extraction system. In such a system, a bound is identified for the error in representing the \"full\" dictionary composed of all data points by a Nystrom approximated version (i.e., represented by subsampling the data points randomly). The bound is then extended to show that the approximate kernel matrix obtained using the Nystrom-sampled dictionary is close to the true kernel matrix, and it is argued that the quality of the approximation is a reasonable proxy for the classification error we can expect after training. It is shown that this approximation model qualitatively predicts the monotonic rise in accuracy of feature extraction with larger dictionaries and saturation of performance in experiments.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1301.5348"
    },
    "1321": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1301-3568",
        "transcript": "The authors aim to introduce a new method for training deep Boltzmann machines. Inspired by inference procedure they turn the model into two hidden layers autoencoder with recurrent connections. Instead of reconstructing all pixels from all (perhaps corrupted) pixels they reconstruct one subset of pixels from the other (the complement).\n\nDBM are usually \"pre-trained\" in a layer-wise manner using RBMs, a conceivably suboptimal procedure. Here the authors propose to use a deterministic criterion that basically turns the DBM into a RNN. This RNN is trained with a loss that resembles that one of denoising auto-encoders (some inputs at random are missing and the task is to predict their values from the observed ones).\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1301.3568"
    },
    "1322": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/WestonWY14",
        "transcript": "This paper extends supervised embedding models by combining them multiplicatively, \n\ni.e. $f'(x,y) = G(x,y) f(x,y). $\n\nIt considers two types of model, dot product in the *embedding* space and kernel density in the *embedding* space, where the kernel in the embedding space is restricted to \n\n$k((x,y),(x','y)) = k(x-x')k(y-y'). $\n\nIt proposes an iterative algorithm which alternates $f$ and $G$ parameter updates.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/proceedings/papers/v32/weston14.html"
    },
    "1323": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1301-3583",
        "transcript": "This papers show the effects of under-fitting in a neural network as the size of a single neural network layer increases. The overall model is composed of SIFT extraction, k-mean, and this single hidden layer neural network. The paper suggest that this under-fitting problem is due to optimization problems with stochastic gradient descent.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1301.3583"
    },
    "1324": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SocherGMN13",
        "transcript": "The paper presents a framework to learn to classify images that can come either from known or unknown classes. This is done by first mapping both images and classes into a joint embedding space. Furthermore, the probability of an image being of an unknown class is estimated using a mixture of Gaussians. Experiments on CIFAR-10 show how performance vary depending on the threshold use to determine if an image is of a known class or not.\n\nThe model first tries to detect whether an image contains an object from a so-far unseen category. If not, the model relies on a regular, state-of-the art supervised classifier to assign the image to known classes. Otherwise, it attempts to identify what this object is, based on a comparison between the image and each unseen class, in a learned joint image/class representation space. The method relies on pre-trained word representations, extracted from unlabelled text, to represent the classes. Experiments evaluate the compromise between classification accuracy on the seen classes and the unseen classes, as a threshold for identifying an unseen class is varied. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5027-zero-shot-learning-through-cross-modal-transfer"
    },
    "1325": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/DosovitskiySB13",
        "transcript": "The paper presents an approach for learning the filters of a convolutional NN, for an image classification task, without making use of target labels. The algorithm proceeds in two steps: learning a transformation of the original image and then learning a classifier using this new representation. For the first step, patches are sampled from an image collection, each patch will then correspond to a surrogate class and a classifier will be trained to associate transformed versions of the patches to the corresponding class labels using a convolutional net. In a second step, this net is replicated on whole images leading to a transformed representation of the original image. A linear classifier is then trained using this representation as input and the target labels relative to the image collection. Experiments are performed on different image collections and a comparison with several baselines is provided.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1312.5242"
    },
    "1326": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/CohenL016",
        "transcript": "The paper introduces a heuristic which aims to revive \"dead\" units in neural networks with the ReLU activation. In such networks, units that are less useful may be abandoned during training because they no longer receive any gradient. This wastes capacity. The proposed heuristic is to detect when this happens and to reinitialize the units in question, so they get another shot at learning something useful.\n\nThe paper proposes an approach to re-set convolutional filters that are apparently not being trained well (and randomly reinitialize them) It proposes a criterion that is based on the gradients propagated to these filters. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.05931"
    },
    "1327": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/AlmeidaS15",
        "transcript": "A common setting in deep networks is to design the network first, \"freeze\" the network architecture, and then train the parameters. The paper pointed out a potential dilemma of that, in the sense that complex networks may have better representation power but may be hard to train. To address this issue the paper proposed to train the network in a hybrid fashion where simpler components and more complex components are combined via a weight average, and the weight is updated over the training procedure to introduce the more complex components, while utilizing the fast training capability of simpler ones.\n\nThe authors propose to blend any two architectural components as the time of optimisation progresses. As the time progresses, the initial approach, e.g. employed rectifier, is gradually switched off in place of another rectifier. The authors claim that this strategy is good for a fast convergence and they present some experimental results.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.06827"
    },
    "1328": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GyselMG16",
        "transcript": "The authors present a framework that can quantize Caffe models into 8-bit and lower fixed-point precision models, which is useful for lowering memory and energy consumption on embedded devices. The compression is an iterative algorithm that determines data statistics to figure out activation and parameter ranges that can be compressed, and conditionally optimizes convolutional weights, fully connected weights and activations given the compression of the other parts.\n\nThis work focuses on processing models already trained with high numerical precision (32 bits float) and compress them, as opposed to other work that tries to train directly with quantized operations.\n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.03168"
    },
    "1329": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ChoiBS15",
        "transcript": "This paper presents an applications of RNNs to predict \"clinical events\", such as disease diagnosis and medication prescription and their timing.\n\nThe paper proposes/suggests:\n1. Applying an RNN to disease diagnosis, medication prescription and timing prediction.\n\n2. \"Initializing\" the neural net with skipgrams instead of one-hot vectors. However, it seems from the description that the authors are not \"initializing\", rather just feeding a different feature vector into the RNN.\n\n3. Initializing a model that is to be trained on a small corpus from a model trained on a large corpus works. Concludes: information can be transferred between models (read across hospitals).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.05942"
    },
    "1330": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/TargAL16",
        "transcript": "The authors propose a new way to initialize the weights of a deep feedfoward network based on inspiration from residual networks, then apply it for initialization of layers in a residual network with improved results on CIFAR-10/100.\n\nThe abstract is inaccurate with respect to the experiments actually performed in the paper. An architecture with the ability to 'forget' is only mentioned without detail towards the end of the paper with a single experiment. \n\nThe authors propose an initialization scheme based on some comparisons to the ResNet architecture. They also replace CONV blocks with the proposed ResNetInit CONV blocks to obtained a Resnet in Resnet (RiR). These experiments are needed, the connections made between the models in the paper are interesting.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.08029"
    },
    "1331": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/KoppSU16",
        "transcript": "The paper suggests using a differentiable function which can smoothly interpolate between multiplicative and additive gates in neural networks.  It is an intriguing idea and the paper is well written.  The mathematical ideas introduced are perhaps not novel (a cursory search seems to indicate that Abel's functional equation with f=exp is called the tetration equation and its solution called the iterated logarithm), but their use in machine learning seem to be.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1604.03736"
    },
    "1332": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SzegedyIV16",
        "transcript": "This paper presents a combination of the inception architecture\nwith residual networks. This is done by adding a shortcut connection\nto each inception module. This can alternatively be seen as a resnet where\nthe 2 conv layers are replaced by a (slightly modified) inception module.\nThe paper (claims to) provide results against the hypothesis that adding residual\nconnections improves training, rather increasing the model size is what makes the difference.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.07261"
    },
    "1333": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/BojanowskiJM15",
        "transcript": "This paper introduces two model extensions to improve character level recurrent neural network language models. The authors evaluate their approaches on a multilingual language modeling benchmark along with the standard Penn Tree Bank Corpus. Evaluation uses only entropy rather than including the language model in a downstream task but that's okay for a paper of this scope. The paper is clearly written and definitely a sufficient contribution for the workshop track it would be really nice to see how well these methods can improve and more sophisticated recurrent architecture like gru or lstm units. On the PTB Corpus it would be nice to include a state-of-the-art or standard n-gram model to use as a reference point for the reported results.\n\nThe conditioning on words model is an interesting approach. It's unfortunate that such a small word level vocabulary is used with this model. It seems like the small vocabulary restriction is due to the fact that the word level model is jointly trained along with the character models. An alternative approach might be to use as input features the hidden representations from a word level recurrent model already trained when building the Character level language model. I don't have a good sense for how much joint training of both models matters. \n\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.06303"
    },
    "1334": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/XuO16",
        "transcript": "The authors present a new method to perform maximum likelihood training for Helmholtz machines. This paper follows up on recent work that jointly train a directed generative model $p(h)p(x|h)$ and an approximate inference model $q(h|x)$. The authors provide a concise summary of previous work and their mutual differences (e.g. Table 1). \n\nTheir new method maintains a (persistent) MCMC chain of latent configurations per training datapoint and it uses $q(h|x)$ as a proposal distribution in a Metropolis Hastings style sampling algorithm. The proposed algorithm looks promising although the authors do not provide any in-depth analysis that highlights the potential strengths and weaknesses of the algorithm. For example: It seems plausible that the persistent Markov chain could deal with more complex posterior distributions $p(h|x)$ than RWS or NVIL because these have to find high probability configurations $p(h|x)$ by drawing only a few samples from (a typically factorial) $q(h|x)$. It would therefore be interesting to measure the distance between the intractable $p(h|x)$ and the approximate inference distribution $q(h|x) $by estimating $KL(q|p)$ or by estimating the effective sampling size for samples $h$ ~ $q(h|x) $ or by showing the final testset NLL estimates over the number of samples h from q (compared to other methods). It would also be interesting to see how this method compares to the others when deeper models are trained.\n\nIn summary: I think the paper presents an interesting method and provides sufficient experimental results for a workshop contribution. For a full conference or journal publication it would need to be extended. ",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1603.06170"
    },
    "1335": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/WhitneyCKT16",
        "transcript": "In recent years, many generative models have been proposed to learn distributed representations automatically from data. One criticism of these models are that they produce representations that are \"entangled\": no single component of the representation vector has meaning on its own. This paper proposes a novel neural architecture and associated learning algorithm for learning disentangled representations. The paper demonstrates the network learning visual concepts on pairs of frames from Atari Games and rendered faces.\n\nThe proposed architecture uses a gating mechanism to select an index to hidden elements that store the \"unpredictable\" parts of the frame into a single component. The architecture bears some similarity to other \"gated\" architectures, e.g. relational autoencoders, three-way RBMs, etc. in that it models input-output pairs and encodes transformations. However, these other architectures do not use an explicit mechanism to make the network model \"differences\". This is novel. The paper claims that the objective function is novel: \"given the previous frame $x_{t-1}$ of a video and the current frame x_t, reconstruct the current frame $x_t$. This is essentially the same objective as relational autoencoders (Memisevic) and similar to gated and conditional RBMs which have been used to model pairs of frames. Therefore I would recommend de-emphasizingthe novelty of the objective.\n\nSignificance - This paper opens up many possibilities for explicit mechanisms of \"relative\" encodings to produce symbolic representations. There isn't much detail in the results (it's an extended abstract!) but I think the work is exciting and I'm looking forward to reading a follow up paper.\n\nPros \n- Attacks a major problem of current generative models (entanglement) \n- Proposes a simple yet novel solution \n- Results show visually that the technique seems to work on two non-trivial datasets\n\nCons \n- Experiments are really preliminary - no quantitative results \n- Doesn't mention mechanisms like dropout which attempt to prevent co-adaptation of features\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1602.06822"
    },
    "1336": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LinTA15",
        "transcript": "This paper proposes a layers wise adaptive depth quantization of DCNs, giving an better tradeoff of error rate/ memory requirement than the fixed bit width across layers.\n\nThe authors describe an optimization problem for determining the bit-width for different layers of DCNs for reducing model size and required computation.\n\nThis paper builds further upon the line of research that tries to represent neural network weights and outputs with lower bit-depths. This way, NN weights will take less memory/space and can speed up implementations of NNs (on GPUs or more specialized hardware).\n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1511.06393"
    },
    "1337": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1706.02515",
        "transcript": "_Objective:_ Design Feed-Forward Neural Network (fully connected) that can be trained even with very deep architectures.\n\n*   _Dataset:_ [MNIST](yann.lecun.com/exdb/mnist/), [CIFAR10](https://www.cs.toronto.edu/%7Ekriz/cifar.html), [Tox21](https://tripod.nih.gov/tox21/challenge/) and [UCI tasks](https://archive.ics.uci.edu/ml/datasets/optical+recognition+of+handwritten+digits).\n*   _Code:_ [here](https://github.com/bioinf-jku/SNNs)\n\n## Inner-workings:\n\nThey introduce a new activation functio the Scaled Exponential Linear Unit (SELU) which has the nice property of making neuron activations converge to a fixed point with zero-mean and unit-variance.  \nThey also demonstrate that upper and lower bounds and the variance and mean for very mild conditions which basically means that there will be no exploding or vanishing gradients.\n\nThe activation function is:  \n[![screen shot 2017-06-14 at 11 38 27 am](https://user-images.githubusercontent.com/17261080/27125901-1a4f7276-50f6-11e7-857d-ebad1ac94789.png)](https://user-images.githubusercontent.com/17261080/27125901-1a4f7276-50f6-11e7-857d-ebad1ac94789.png)  \nWith specific parameters for alpha and lambda to ensure the previous properties. The tensorflow impementation is:\n\n    def selu(x):\n        alpha = 1.6732632423543772848170429916717\n        scale = 1.0507009873554804934193349852946\n        return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha)\n    \n\nThey also introduce a new dropout (alpha-dropout) to compensate for the fact that [![screen shot 2017-06-14 at 11 44 42 am](https://user-images.githubusercontent.com/17261080/27126174-e67d212c-50f6-11e7-8952-acad98b850be.png)](https://user-images.githubusercontent.com/17261080/27126174-e67d212c-50f6-11e7-8952-acad98b850be.png)\n\n## Results:\n\nBatch norm becomes obsolete and they are also able to train deeper architectures. This becomes a good choice to replace shallow architectures where random forest or SVM used to be the best results. They outperform most other techniques on small datasets.  \n[![screen shot 2017-06-14 at 11 36 30 am](https://user-images.githubusercontent.com/17261080/27125798-bd04c256-50f5-11e7-8a74-b3b6a3fe82ee.png)](https://user-images.githubusercontent.com/17261080/27125798-bd04c256-50f5-11e7-8a74-b3b6a3fe82ee.png)\n\nMight become a new standard for fully-connected activations in the future.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1706.02515"
    },
    "1338": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MatzenBS17",
        "transcript": "_Objective:_ Analyze large scale dataset of fashion images to discover visually consistent style clusters.\n\n*   _Dataset:_ StreetStye-27K.\n*   _Code:_ demo [here](http://streetstyle.cs.cornell.edu/)\n\n## New dataset: StreetStye-27K\n\n1.  **Photos (100 million)**: from Instagram using the [API](https://www.instagram.com/developer/) to retrieve images with the correct location and time.\n2.  **People (14.5 million)**: they run two algorithms to normalize the body position in the image:\n    *   [Face++](http://www.faceplusplus.com/) to detect and localize faces.\n    *   [Deformable Part Model](http://people.cs.uchicago.edu/%7Erbg/latent-release5/) to estimate the visibility of the rest of the body.\n3.  **Clothing annotations (27K)**: Amazon Mechanical Turk with quality control. 4000$ for the whole dataset.\n\n## Architecture:\n\nUsual GoogLeNet but they use [Isotonice Regression](http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/) to correct the bias.\n\n## Unsupervised clustering:\n\nThey proceed as follow:\n\n1.  Compute the features embedding for a subset of the overall dataset selected to represent location and time.\n2.  Apply L2 normalization.\n3.  Use PCA to find the vector representing 90% of the variance (165 here).\n4.  Cluster them using a [GMM](https://en.wikipedia.org/wiki/Mixture_model#Multivariate_Gaussian_mixture_model) with 400 mixtures which represent the clusters.\n\nThey compute fashion clusters for city or bigger entities:  \n[![screen shot 2017-06-15 at 12 04 06 pm](https://user-images.githubusercontent.com/17261080/27176447-d33fc2dc-51c2-11e7-9191-dbf972ee96a1.png)](https://user-images.githubusercontent.com/17261080/27176447-d33fc2dc-51c2-11e7-9191-dbf972ee96a1.png)\n\n## Results:\n\nPretty standard techniques but all patched together to produce interesting visualizations.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1706.01869"
    },
    "1339": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tog/LiaoYYHK17",
        "transcript": "_Objective:_ Transfer visual attribute (color, tone, texture, and style, etc) between two semantically-meaningful images such as a picture and a sketch.\n\n## Inner workings:\n\n### Image analogy\n\nAn image analogy A:A\u2032::B:B\u2032 is a relation where:\n\n*   B\u2032 relates to B in the same way as A\u2032 relates to A\n*   A and A\u2032 are in pixel-wise correspondences\n*   B and B\u2032 are in pixel-wise correspondences\n\nIn this paper only a source image A and an example image B\u2032 are given, and both A\u2032 and B represent latent images to be estimated.\n\n[![screen shot 2017-05-18 at 10 43 48 am](https://cloud.githubusercontent.com/assets/17261080/26193907/f080e212-3bb6-11e7-9441-7b255e4219f5.png)](https://cloud.githubusercontent.com/assets/17261080/26193907/f080e212-3bb6-11e7-9441-7b255e4219f5.png)\n\n### Dense correspondence\n\nIn order to find dense correspondences between two images they use features from previously trained CNN (VGG-19) and retrieve all the ReLU layers.\n\nThe mapping is divided in two sub-mappings that are easier to compute, first a visual attribute transformation and then a space transformation.\n\n[![screen shot 2017-05-18 at 11 04 58 am](https://cloud.githubusercontent.com/assets/17261080/26194835/03ccd94a-3bba-11e7-93ca-9420d4d96162.png)](https://cloud.githubusercontent.com/assets/17261080/26194835/03ccd94a-3bba-11e7-93ca-9420d4d96162.png)\n\n## Architecture:\n\nThe algorithm proceeds as follow:\n\n1.  Compute features at each layer for the input image using a pre-trained CNN and initialize feature maps of latent images with coarsest layer.\n2.  For said layer compute a forward and reverse nearest-neighbor field (NNF, basically an offset field).\n3.  Use this NNF with the feature of the input current layer to compute the features of the latent images.\n4.  Upsample the NNF and use it as the initialization for the NNF of the next layer.\n\n[![screen shot 2017-05-18 at 11 14 33 am](https://cloud.githubusercontent.com/assets/17261080/26195178/35277e0e-3bbb-11e7-82ce-037466314640.png)](https://cloud.githubusercontent.com/assets/17261080/26195178/35277e0e-3bbb-11e7-82ce-037466314640.png)\n\n## Results:\n\nImpressive quality on all type of visual transfer but veryyyyy slow! (~3min on GPUs for one image).\n\n[![screen shot 2017-05-18 at 11 36 47 am](https://cloud.githubusercontent.com/assets/17261080/26196151/54ef423c-3bbe-11e7-9433-b29be5091fae.png)](https://cloud.githubusercontent.com/assets/17261080/26196151/54ef423c-3bbe-11e7-9433-b29be5091fae.png)",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/3072959.3073683"
    },
    "1340": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Beltramelli17",
        "transcript": "Generate code from a UI screenshot.\n\n_Code:_ [Demo](https://youtu.be/pqKeXkhFA3I) and [code](https://github.com/tonybeltramelli/pix2code) to come.\n\n## Inner-workings:\n\nDecomposed the problem in three steps:\n\n1.  a computer vision problem of understanding the given scene and inferring the objects present, their identities, positions, and poses.\n2.  a language modeling problem of understanding computer code and generating syntactically and semantically correct samples.\n3.  use the solutions to both previous sub-problems by exploiting the latent variables inferred from scene understanding to generate corresponding textual descriptions of the objects represented by these variables.\n\nThey also introduce a Domain Specific Languages (DSL) for modeling purposes.\n\n## Architecture:\n\n*   Vision model: usual AlexNet-like architecture\n*   Language model: use onehot encoding for the words in the DSL vocabulary which is then fed into a LSTM\n*   Combined model: LSTM too.\n\n[![screen shot 2017-06-16 at 11 34 28 am](https://user-images.githubusercontent.com/17261080/27221124-c9cadcc6-5287-11e7-9d38-c4234af92912.png)](https://user-images.githubusercontent.com/17261080/27221124-c9cadcc6-5287-11e7-9d38-c4234af92912.png)\n\n## Results:\n\nClearly not ready for any serious use but promising results!  \n[![screen shot 2017-06-16 at 11 57 45 am](https://user-images.githubusercontent.com/17261080/27222031-0bf8e7de-528b-11e7-896f-cdb410f928c3.png)](https://user-images.githubusercontent.com/17261080/27222031-0bf8e7de-528b-11e7-896f-cdb410f928c3.png)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.07962"
    },
    "1341": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/OlsonSCTVFHM17",
        "transcript": "_Objective:_ Develop a platform to make AI accessible\n\n_Website:_ [here](http://pennai.org/)\n\n## Inner-workings:\n\nPlatform for AI with deep learning and genetic programming. More focused on biology.\n\n## Architecture:\n\n[![screen shot 2017-06-26 at 11 00 07 am](https://user-images.githubusercontent.com/17261080/27690782-8b71f8c8-5ce2-11e7-9d84-77a4dd519e18.jpg)](https://user-images.githubusercontent.com/17261080/27690782-8b71f8c8-5ce2-11e7-9d84-77a4dd519e18.jpg)\n\n## Results:\n\nJust announced, keep an eye on it.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.00594"
    },
    "1342": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/LongCWJ17",
        "transcript": "_Objective:_ Perform domain-adaptation by adapting several layers using a randomized representation and not just the final layer thus performing alignment of the joint distribution and not just the marginals.\n\n_Dataset:_ [Office](https://cs.stanford.edu/%7Ejhoffman/domainadapt/) and [ImageCLEF-DA1](http://imageclef.org/2014/adaptation).\n\n## Inner-workings:\n\nBasically an improvement on [RevGrad](https://arxiv.org/pdf/1505.07818.pdf) where instead of using the last embedding layer for the discriminator, a bunch of them is used.  \nTo avoid dimension explosion when using the tensor product of all layers they instead use a randomized multi-linear representation:  \n[![screen shot 2017-06-01 at 5 35 46 pm](https://cloud.githubusercontent.com/assets/17261080/26687736/cff20446-46f0-11e7-918e-b60baa10aa67.png)](https://cloud.githubusercontent.com/assets/17261080/26687736/cff20446-46f0-11e7-918e-b60baa10aa67.png)  \nWhere:\n\n*   d is the dimension of the embedding (they use 1024)\n*   R is random matrix for which each element as a null average and variance of 1 (Bernoulli, Gaussian and Uniform are tried)\n*   z^l is the l-th layer\n*   \u2299 represents the Hadamard product  \n    In practice they don't use all layers but just the 3-4 last layers for ResNet and AlexNet.\n\n## Architecture:\n\n[![screen shot 2017-06-01 at 5 34 44 pm](https://cloud.githubusercontent.com/assets/17261080/26687686/acce0d98-46f0-11e7-89d1-15452cbb527e.png)](https://cloud.githubusercontent.com/assets/17261080/26687686/acce0d98-46f0-11e7-89d1-15452cbb527e.png)\n\nThey use the usual losses for domain adaptation with: - F minimizing the cross-entropy loss for classification and trying to reduce the gap between the distributions (indicated by D). - D maximizing the gap between the distributions.\n\n[![screen shot 2017-06-01 at 5 40 53 pm](https://cloud.githubusercontent.com/assets/17261080/26687936/8575ff70-46f1-11e7-917d-05129ab190b0.png)](https://cloud.githubusercontent.com/assets/17261080/26687936/8575ff70-46f1-11e7-917d-05129ab190b0.png)\n\n## Results:\n\nImprovement on state-of-the-art results for most tasks in the dataset, very easy to implement with any pre-trained network out of the box.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1705.10667"
    },
    "1343": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1704.06191",
        "transcript": "_Objective:_ Replace the usual GAN loss with a softmax croos-entropy loss to stabilize GAN training.\n\n_Dataset:_ [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)\n## Inner working:\n\nLinked to recent work such as WGAN or Loss-Sensitive GAN that focus on objective functions with non-vanishing gradients to avoid the situation where the discriminator `D` becomes too good and the gradient vanishes.\n\nThus they first introduce two targets for the discriminator `D` and the generator `G`:  \n[![screen shot 2017-04-24 at 6 18 11 pm](https://cloud.githubusercontent.com/assets/17261080/25347232/767049bc-291a-11e7-906e-c19a92bb7431.png)](https://cloud.githubusercontent.com/assets/17261080/25347232/767049bc-291a-11e7-906e-c19a92bb7431.png)  \n[![screen shot 2017-04-24 at 6 18 24 pm](https://cloud.githubusercontent.com/assets/17261080/25347233/7670ff60-291a-11e7-974f-83eb9269d238.png)](https://cloud.githubusercontent.com/assets/17261080/25347233/7670ff60-291a-11e7-974f-83eb9269d238.png)\n\nAnd then the two new losses:  \n[![screen shot 2017-04-24 at 6 19 50 pm](https://cloud.githubusercontent.com/assets/17261080/25347275/a303aa0a-291a-11e7-86b4-abd42c83d4a8.png)](https://cloud.githubusercontent.com/assets/17261080/25347275/a303aa0a-291a-11e7-86b4-abd42c83d4a8.png)  \n[![screen shot 2017-04-24 at 6 19 55 pm](https://cloud.githubusercontent.com/assets/17261080/25347276/a307bc6c-291a-11e7-98b3-cbd7182090cd.png)](https://cloud.githubusercontent.com/assets/17261080/25347276/a307bc6c-291a-11e7-98b3-cbd7182090cd.png)\n\n## Architecture:\n\nThey use the DCGAN architecture and simply change the loss and remove the batch normalization and other empirical techniques used to stabilize training.  \nThey show that the soft-max GAN is still robust to training.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1704.06191"
    },
    "1344": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/Sankaranarayanan17a",
        "transcript": " _Objective:_ Use a GAN to learn an embedding invariant from domain shift.\n_Dataset:_ [MNIST](yann.lecun.com/exdb/mnist/), [SVHN](http://ufldl.stanford.edu/housenumbers/), USPS, [OFFICE](https://cs.stanford.edu/%7Ejhoffman/domainadapt/) and [CFP](http://mukh.com/).\n\n\n## Architecture:\n\nThe total network is composed of several sub-networks:\n\n1.  `F`, the Feature embedding network that takes as input an image from either the source or target dataset and generate a feature vector.\n2.  `C`, the Classifier network when the image come from the source dataset.\n3.  `G`, the Generative network that learns to generate an image similar to the source dataset using an image embedding from `F` and a random noise vector.\n4.  `D`, the Discriminator network that tries to guess if an image is either from the source or the generative network.\n\n`G` and `D` play a minimax game where `D` tries to classify the generated samples as fake and `G` tries to fool `D` by producing examples that are as realistic as possible.\n\nThe scheme for training the network is the following:\n\n[![screen shot 2017-04-14 at 5 50 22 pm](https://cloud.githubusercontent.com/assets/17261080/25048122/f2a648b6-213a-11e7-93bd-954981bd3838.png)](https://cloud.githubusercontent.com/assets/17261080/25048122/f2a648b6-213a-11e7-93bd-954981bd3838.png)\n\n## Results:\n\nVery interesting, the generated image is just a side-product but the overall approach seems to be the state-of-the-art at the time of writing (the paper was published one week ago).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1704.01705"
    },
    "1345": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/PritzelUSBVHWB17",
        "transcript": " _Objective:_ Reduce learning time for [DQN](https://deepmind.com/research/dqn/)-type architectures.\n\nThey introduce a new network element, called DND (Differentiable Neural Dictionary) which is basically a dictionary that uses any key (especially embeddings) and computes the value by using kernel between keys. Plus it's differentiable.\n\n## Architecture:\n\nThey use basically a network in two steps:\n\n1.  A classical CNN network that computes and embedding for every image.\n2.  A DND for all possible actions (controller input) that stores the embedding as key and estimated reward as value.\n\nAlso they use a buffer to store all tuples (previous image, action, reward, next image) and for training basic technique is used.\n\n[![screen shot 2017-04-12 at 11 23 32 am](https://cloud.githubusercontent.com/assets/17261080/24951103/92930022-1f73-11e7-97d2-628e2f4b5a33.png)](https://cloud.githubusercontent.com/assets/17261080/24951103/92930022-1f73-11e7-97d2-628e2f4b5a33.png)\n\n## Results:\n\nClearly improves learning speed but in the end other techniques catchup and it gets outperformed.",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v70/pritzel17a.html"
    },
    "1346": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeGDG17",
        "transcript": "_Objective:_ Image segmentation and pose estimation with an extension of Faster R-CNN.\n\n_Dataset:_ [COCO](http://mscoco.org/) and [Cityscapes](https://www.cityscapes-dataset.com/).\n\n\n## Inner workings:\n\nThe core operator of Faster R-CNN is the _RoIPool_ which performs coarse spatial quantization for feature extraction but introduce misalignment for pixel-pixel comparison which is what segmentation is. The paper introduce a new layer _RoIAlign_ that faithfully preserves exact spatial location.\n\nOne important point is that mask and class prediction are decoupled, the segmentation is proposed for each class without competing and the class predictor finally elects the winner.\n\n## Architecture:\n\nBased on Faster R-CNN but with an added mask subnetwork that computes a segmentation mask for each class.\n\nDifferent feature extractors and proposers are tried, see two examples below:  \n[![screen shot 2017-05-22 at 7 25 04 pm](https://cloud.githubusercontent.com/assets/17261080/26320765/659bfd6e-3f24-11e7-9184-393e83e9108d.png)](https://cloud.githubusercontent.com/assets/17261080/26320765/659bfd6e-3f24-11e7-9184-393e83e9108d.png)\n\n## Results:\n\nRuns at about 200ms per frame on a GPU for segmentation (2 days training on a single 8-GPU) and 5 fps for pose estimation.  \nVery impressive segmentation and pose estimation:\n\n[![screen shot 2017-05-22 at 7 26 57 pm 1](https://cloud.githubusercontent.com/assets/17261080/26320824/a9a0909c-3f24-11e7-8e06-b2f132aad2d7.png)](https://cloud.githubusercontent.com/assets/17261080/26320824/a9a0909c-3f24-11e7-8e06-b2f132aad2d7.png)\n\n[![screen shot 2017-05-22 at 7 29 26 pm](https://cloud.githubusercontent.com/assets/17261080/26320929/08b71c4a-3f25-11e7-8eb5-959ceb7b6112.png)](https://cloud.githubusercontent.com/assets/17261080/26320929/08b71c4a-3f25-11e7-8eb5-959ceb7b6112.png)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.06870"
    },
    "1347": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1703.10717",
        "transcript": "_Objective:_ Improve GANs convergence to more diverse and visually pleasing images at higher resolution using a novel equilibrium method between the discriminator and the generator that also simplifies training procedures.\n\n_Dataset:_ [LFW](http://vis-www.cs.umass.edu/lfw/)\n\n## Inner workings:\n\nThey try to match the distribution of the errors (assumed to be normally distributed) instead of matching the distribution of the samples directly. In order to do this they compute the Wasserstein distance between a pixel-wise autoencoder loss distributions of real and generated samples defined as follow:\n\n1.  Autoencoder loss:\n\n[![screen shot 2017-04-24 at 3 46 32 pm](https://cloud.githubusercontent.com/assets/17261080/25340190/429f9788-2905-11e7-88dc-b44567b9cd34.png)](https://cloud.githubusercontent.com/assets/17261080/25340190/429f9788-2905-11e7-88dc-b44567b9cd34.png)\n\n2.  Wasserstein distance for two normal distributions \u03bc1 = N(m1, C1) and \u03bc2 = N(m2, C2)\n\n[![screen shot 2017-04-24 at 3 46 44 pm](https://cloud.githubusercontent.com/assets/17261080/25340191/42b23474-2905-11e7-9810-58d5326bf886.png)](https://cloud.githubusercontent.com/assets/17261080/25340191/42b23474-2905-11e7-9810-58d5326bf886.png)\n\nThey also introduce an equilibrium concept to account for the situation when `G` and `D` are not well balanced and the discriminator `D` wins easily. This is controlled by what they call the diversity ratio that balances between auto-encoding real images and discriminating real from generated images. It is defined as follow:  \n[![screen shot 2017-04-24 at 3 56 29 pm](https://cloud.githubusercontent.com/assets/17261080/25340609/992c2188-2906-11e7-8c51-498bbd293119.png)](https://cloud.githubusercontent.com/assets/17261080/25340609/992c2188-2906-11e7-8c51-498bbd293119.png)\n\nTo maintain this balance they use a standard SGD but they introduce a variable `kt` initially 0 to control how much emphasis is put on the generator `G`. This removes the need to do `x` steps on `D` followed by `y` steps on `G` or to pretrained one of the two.  \n[![screen shot 2017-04-24 at 3 59 57 pm](https://cloud.githubusercontent.com/assets/17261080/25340859/4ee06476-2907-11e7-971f-90421449cb51.png)](https://cloud.githubusercontent.com/assets/17261080/25340859/4ee06476-2907-11e7-971f-90421449cb51.png)\n\nFinally they derive a global convergence measure by using the equilibrium concept that can be used to determine when the network has reached its final state or if the model has collapsed:  \n[![screen shot 2017-04-24 at 4 04 12 pm](https://cloud.githubusercontent.com/assets/17261080/25340998/b8bf6ad6-2907-11e7-8afa-294cae32c6af.png)](https://cloud.githubusercontent.com/assets/17261080/25340998/b8bf6ad6-2907-11e7-8afa-294cae32c6af.png)\n\n## Architecture:\n\nThey tried to keep the architecture simple to really study the impact of their new equilibrium principle and loss. They don't use batch normalization, dropout, transpose convolutions or exponential growth for convolution filters.\n\n[![screen shot 2017-04-24 at 4 09 29 pm](https://cloud.githubusercontent.com/assets/17261080/25341219/6fb7be28-2908-11e7-8774-287c1b7d7684.png)](https://cloud.githubusercontent.com/assets/17261080/25341219/6fb7be28-2908-11e7-8774-287c1b7d7684.png)\n\n## Results:\n\nThey trained on images from 32x32 to 256x256, but at higher resolution images tend to lose sharpness. Nevertheless images are very very good!  \n[![screen shot 2017-04-24 at 4 20 30 pm](https://cloud.githubusercontent.com/assets/17261080/25341699/f99b0770-2909-11e7-84a0-3ac0436771e5.png)](https://cloud.githubusercontent.com/assets/17261080/25341699/f99b0770-2909-11e7-84a0-3ac0436771e5.png)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1703.10717"
    },
    "1348": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ZhuPIE17",
        "transcript": "_Objective:_ Image-to-image translation to perform visual attribute transfer using unpaired images.\n\n_Dataset:_ [Cityscapes](https://www.cityscapes-dataset.com/), [CMP Facade](http://cmp.felk.cvut.cz/%7Etylecr1/facade/), [UT Zappos50k](http://vision.cs.utexas.edu/projects/finegrained/utzap50k/) and [ImageNet](http://www.image-net.org/).\n\n_Code:_ [CycleGAN](https://github.com/junyanz/CycleGAN)\n\n## Inner-workings:\n\nBasically two GANs for each domain with their respective Generator and Discriminator plus two additional losses (called consistency losses) to make sure that translating to the other domain then back yields an image that is still realistic.  \n[![screen shot 2017-06-02 at 10 24 45 am](https://cloud.githubusercontent.com/assets/17261080/26717449/bcd8a9cc-477d-11e7-9137-fd277a0ec04f.png)](https://cloud.githubusercontent.com/assets/17261080/26717449/bcd8a9cc-477d-11e7-9137-fd277a0ec04f.png)\n\nFor the consistency los they use a pixel-wise L1 norm:  \n[![screen shot 2017-06-02 at 10 31 22 am](https://cloud.githubusercontent.com/assets/17261080/26717733/bc088cdc-477e-11e7-96af-2defa06a1660.png)](https://cloud.githubusercontent.com/assets/17261080/26717733/bc088cdc-477e-11e7-96af-2defa06a1660.png)\n\n## Architecture:\n\nBased on [Perceptual losses for real-time style transfer and super-resolution](https://arxiv.org/pdf/1603.08155.pdf), code available [here](https://github.com/jcjohnson/fast-neural-style).  \nTraining seems to employ several tricks and then even use a batch of 1.\n\n## Results:\n\nVery impressive and the really key point is that you don't need paired images which makes this trainable on any domain with the same representation behind.  \n[![screen shot 2017-06-02 at 10 26 29 am](https://cloud.githubusercontent.com/assets/17261080/26717502/f6d1fb7e-477d-11e7-8174-7bdd621cf1b6.png)](https://cloud.githubusercontent.com/assets/17261080/26717502/f6d1fb7e-477d-11e7-8174-7bdd621cf1b6.png)",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1703.10593"
    },
    "1349": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.03530",
        "transcript": "_Objective:_ Theoretical study of Deep Neural Network, their expressivity and regularizations.\n\n## Results:\n\nThe key findings of the article are:\n\n### A. Deep neural networks easily fit random labels.\n\nBoth when randomizing labels, replacing images with raw noise or all situations in-between.\n\n1.  The effective capacity of neural networks is sufficient for memorizing the entire data set.\n2.  Even optimization on random labels remains easy. In fact, training time increases only by a small constant factor compared with training on the true labels.\n3.  Randomizing labels is solely a data transformation, leaving all other properties of the learning problem unchanged.\n\n### B. Explicit regularization may improve generalization performance, but is neither necessary nor by itself sufficient for controlling generalization error.\n\nBy explicit regularization they mean batch normalisation, weight decay, dropout, data augmentation, etc.\n\n### C. Generically large neural networks can express any labeling of the training data.\n\nMore formally, a very simple two-layer ReLU network with `p = 2n + d` parameters can express any labeling of any sample of size `n` in `d` dimensions.\n\n### D. The optimization algorithm itself is implicitly regularizing the solution.\n\nSGD acts as an implicit regularizer and properties are inherited by models that were trained using SGD.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.03530"
    },
    "1350": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/TzengHSD17",
        "transcript": "_Objective:_ Define a framework for Adversarial Domain Adaptation and propose a new architecture as state-of-the-art.\n\n _Dataset:_ MNIST, USPS, SVHN and NYUD.   \n\n## Inner workings:\n\nSubsumes previous work in a generalized framework where designing a new method is now simplified to the space of making three design choices:\n\n*   whether to use a generative or discriminative base model.\n*   whether to tie or untie the weights.\n*   which adversarial learning objective to use.\n\n[![screen shot 2017-04-18 at 5 10 01 pm](https://cloud.githubusercontent.com/assets/17261080/25138167/15d5e644-245a-11e7-9fb8-636ce4111036.png)](https://cloud.githubusercontent.com/assets/17261080/25138167/15d5e644-245a-11e7-9fb8-636ce4111036.png)\n\n## Architecture:\n\n[![screen shot 2017-04-18 at 5 14 44 pm](https://cloud.githubusercontent.com/assets/17261080/25138526/07848bd0-245b-11e7-94c9-f6ae7ccea76f.png)](https://cloud.githubusercontent.com/assets/17261080/25138526/07848bd0-245b-11e7-94c9-f6ae7ccea76f.png)\n\n## Results:\n\nInteresting as the theoretical framework seem to converge with other papers and their architecture improves on previous papers performance even if it's not a huge improvement.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2017.316"
    },
    "1351": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/tcsv/WangZLZL17",
        "transcript": "_Objective:_ Specifically adapt Active Learning to Image Classification with deep learning\n\n_Dataset:_ [CARC](https://bcsiriuschen.github.io/CARC/) and [Caltech-256](http://authors.library.caltech.edu/7694/)\n\n## Inner-workings:\n\nThey labels from two sources:\n\n*   The most informative/uncertain samples are manually labeled using Least confidence, margin sampling and entropy, see [Active Learning Literature Survey](https://github.com/Deepomatic/papers/issues/192).\n*   The other kind is the samples with high prediction confidence that are automatically labelled. They represent the majority of samples.\n\n## Architecture:\n\n[![screen shot 2017-06-29 at 3 57 43 pm](https://user-images.githubusercontent.com/17261080/27691277-d4547196-5ce3-11e7-849c-aadd30d71d68.png)](https://user-images.githubusercontent.com/17261080/27691277-d4547196-5ce3-11e7-849c-aadd30d71d68.png)\n\nThey proceed with the following steps:\n\n1.  Initialization: they manually annotate a given number of images for each class in order to pre-trained the network.\n2.  Complementary sample selection: they fix the network, identity the most uncertain label for manual annotation and automatically annotate the most certain one if their entropy is higher than a given threshold.\n3.  CNN fine-tuning: they train the network using the whole pool of already labeled data and pseudo-labeled. Then they put all the automatically labeled images back into the unlabelled pool.\n4.  Threshold updating: as the network gets more and more confident the threshold for auto-labelling is linearly reducing. The idea is that the network gets a more reliable representation and its trustability increases.\n\n## Results:\n\nRoughly divide by 2 the number of annotation needed.  \n\u26a0\ufe0f I don't feel like this paper can be trusted 100% \u26a0\ufe0f",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.1109/TCSVT.2016.2589879"
    },
    "1352": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1701.07875",
        "transcript": "_Objective:_ Robust unsupervised learning of a probability distribution using a new module called the `critic` and the `Earth-mover distance`.\n\n _Dataset:_ [LSUN-Bedrooms](http://lsun.cs.princeton.edu/2016/)\n\n## Inner working:\n\nBasically train a `critic` until convergence to retrieve the Wasserstein-1 distance, see pseudo-algorithm below:\n\n[![screen shot 2017-05-03 at 5 05 09 pm](https://cloud.githubusercontent.com/assets/17261080/25667162/003c9330-3023-11e7-9081-c181011f4e6f.png)](https://cloud.githubusercontent.com/assets/17261080/25667162/003c9330-3023-11e7-9081-c181011f4e6f.png)\n\n## Results:\n\n*   Easier training: no need for batch normalization and no need to fine-tune generator/discriminator balance.\n*   Less sensitivity to network architecture.\n*   Very good proxy that correlates very well with sample quality.\n*   Non-vanishing gradients.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1701.07875"
    },
    "1353": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/VeitACKGB17",
        "transcript": "_Objective:_ Predict labels using a very large dataset with noisy labels and a much smaller (3 orders of magnitude) dataset with human-verified annotations.\n\n_Dataset:_ [Open image](https://research.googleblog.com/2016/09/introducing-open-images-dataset.html)\n\n\n## Architecture:\n\nContrary to other approaches they use the clean labels, the noisy labels but also image features. They basically train 3 networks:\n\n1.  A feature extractor for the image.\n2.  A label Cleaning Network that predicts to learn verified labels from noisy labels + image feature.\n3.  An image classifier that predicts using just the image.\n\n[![screen shot 2017-04-12 at 11 10 56 am](https://cloud.githubusercontent.com/assets/17261080/24950258/c4764106-1f70-11e7-82e4-c1111ffc089e.png)](https://cloud.githubusercontent.com/assets/17261080/24950258/c4764106-1f70-11e7-82e4-c1111ffc089e.png)\n\n## Results:\n\nOverall better performance but not breath-taking improvement: from `AP 83.832 / MAP 61.82` for a NN trained only on labels to `AP 87.67 / MAP 62.38` with their approach.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2017.696"
    },
    "1354": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1612.00005",
        "transcript": "_Objective:_ Find a generative model that avoids usual shortcomings: (i) high-resolution, (ii) variety of images and (iii) matching the dataset diversity.\n\n_Dataset:_ [ImageNet](https://www.image-net.org/)\n\n## Inner-workings:\n\nThe idea is to find an image that maximizes the probability for a given label by using a variant of a Markov Chain Monte Carlo (MCMC) sampler.  \n[![screen shot 2017-06-01 at 12 31 14 pm](https://cloud.githubusercontent.com/assets/17261080/26675978/3c9e6d94-46c6-11e7-9f67-477c4036a891.png)](https://cloud.githubusercontent.com/assets/17261080/26675978/3c9e6d94-46c6-11e7-9f67-477c4036a891.png)  \nWhere the first term ensures that we stay in the image manifold that we're trying to find and don't just produce adversarial examples and the second term makes sure that find an image corresponding to the label we're looking for.\n\nBasically we start with a random image and iteratively find a better image to match the label we're trying to generate.\n\n### MALA-approx:\n\nMALA-approx is the MCMC sampler based on the Metropolis-Adjusted Langevin Algorithm that they use in the paper, it is defined iteratively as follow:  \n[![screen shot 2017-06-01 at 12 25 45 pm](https://cloud.githubusercontent.com/assets/17261080/26675866/bf15cc28-46c5-11e7-9620-659d26f84bf8.png)](https://cloud.githubusercontent.com/assets/17261080/26675866/bf15cc28-46c5-11e7-9620-659d26f84bf8.png)  \nwhere:\n\n*   epsilon1 makes the image more generic.\n*   epsilon2 increases confidence in the chosen class.\n*   epsilon3 adds noise to encourage diversity.\n\n### Image prior:\n\nThey try several priors for the images:\n\n1.  PPGN-x: p(x) is modeled with a Denoising Auto-Encoder (DAE).\n\n[![screen shot 2017-06-01 at 1 48 33 pm](https://cloud.githubusercontent.com/assets/17261080/26678501/1737c64e-46d1-11e7-82a4-7ee0aa8bfe2f.png)](https://cloud.githubusercontent.com/assets/17261080/26678501/1737c64e-46d1-11e7-82a4-7ee0aa8bfe2f.png)\n\n2.  DGN-AM: use a latent space to model x with h using a GAN.\n\n[![screen shot 2017-06-01 at 1 49 41 pm](https://cloud.githubusercontent.com/assets/17261080/26678517/2e743194-46d1-11e7-95dc-9bb638128242.png)](https://cloud.githubusercontent.com/assets/17261080/26678517/2e743194-46d1-11e7-95dc-9bb638128242.png)\n\n3.  PPGN-h: incorporates a prior for p(h) using a DAE.\n\n[![screen shot 2017-06-01 at 1 51 14 pm](https://cloud.githubusercontent.com/assets/17261080/26678579/6bd8cb58-46d1-11e7-895d-f9432b7e5e1f.png)](https://cloud.githubusercontent.com/assets/17261080/26678579/6bd8cb58-46d1-11e7-895d-f9432b7e5e1f.png)\n\n4.  Joint PPGN-h: to increases expressivity of G, model h by first modeling x in the DAE.\n\n[![screen shot 2017-06-01 at 1 51 23 pm](https://cloud.githubusercontent.com/assets/17261080/26678622/a7bf2f68-46d1-11e7-9209-98f97e0a218d.png)](https://cloud.githubusercontent.com/assets/17261080/26678622/a7bf2f68-46d1-11e7-9209-98f97e0a218d.png)\n\n5.  Noiseless joint PPGN-h: same as previous but without noise.\n\n[![screen shot 2017-06-01 at 1 54 11 pm](https://cloud.githubusercontent.com/assets/17261080/26678655/d5499220-46d1-11e7-93d0-d48a6b6fa1a8.png)](https://cloud.githubusercontent.com/assets/17261080/26678655/d5499220-46d1-11e7-93d0-d48a6b6fa1a8.png)\n\n### Conditioning:\n\nIn the paper they mostly use conditioning on label but captions or pretty much anything can also be used.  \n[![screen shot 2017-06-01 at 2 26 53 pm](https://cloud.githubusercontent.com/assets/17261080/26679654/6297ab86-46d6-11e7-86fa-f763face01ca.png)](https://cloud.githubusercontent.com/assets/17261080/26679654/6297ab86-46d6-11e7-86fa-f763face01ca.png)\n\n## Architecture:\n\nThe final architecture using a pretrained classifier network is below. Note that only G and D are trained.  \n[![screen shot 2017-06-01 at 2 29 49 pm](https://cloud.githubusercontent.com/assets/17261080/26679785/db143520-46d6-11e7-9668-72864f1a8eb1.png)](https://cloud.githubusercontent.com/assets/17261080/26679785/db143520-46d6-11e7-9668-72864f1a8eb1.png)\n\n## Results:\n\nPretty much any base network can be used with minimal training of G and D. It produces very realistic image with a great diversity, see below for examples of 227x227 images with ImageNet.  \n[![screen shot 2017-06-01 at 2 32 38 pm](https://cloud.githubusercontent.com/assets/17261080/26679884/4494002a-46d7-11e7-882e-c69aff2ddd17.png)](https://cloud.githubusercontent.com/assets/17261080/26679884/4494002a-46d7-11e7-882e-c69aff2ddd17.png)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1612.00005"
    },
    "1355": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1612.08242",
        "transcript": "_Objective:_ Train on both classification and detection image to make a better faster and stronger detector.\n\n_Dataset:_ [ImageNet](http://www.image-net.org/), [COCO](http://mscoco.org/) and [WordNet](https://wordnet.princeton.edu/).\n\n\n## Architecture:\n\nApart from amelioration such as batch norm or other general tweaking the real improvements come from:\n\n1.  Using both a classification dataset and a detection dataset at the same time.\n2.  Replacing the usual final soft-max layer (which assumes that all labels are mutually exclusive) with a WordTree label hierarchy base on WordNet which enables the network to predict `dog` even if it doesn't know if it's a `Fox Terrier`.\n\n[![screen shot 2017-04-12 at 7 24 28 pm](https://cloud.githubusercontent.com/assets/17261080/24970727/b7abaf02-1fb5-11e7-8b78-2a430a861cbd.png)](https://cloud.githubusercontent.com/assets/17261080/24970727/b7abaf02-1fb5-11e7-8b78-2a430a861cbd.png)\n\n## Results:\n\nState of the art results at full resolution and possibility to lower performance to gain in computation time.\n\n[![screen shot 2017-04-12 at 7 31 26 pm](https://cloud.githubusercontent.com/assets/17261080/24971010/a51556f8-1fb6-11e7-9289-fc277b182686.png)](https://cloud.githubusercontent.com/assets/17261080/24971010/a51556f8-1fb6-11e7-9289-fc277b182686.png)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1612.08242"
    },
    "1356": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SchoenholzGGS16",
        "transcript": "_Objective:_ Fondamental analysis of random networks using mean-field theory. Introduces two scales controlling network behavior.\n\n## Results:\n\nGuide to choose hyper-parameters for random networks to be nearly critical (in between order and chaos). This in turn implies that information can propagate forward and backward and thus the network is trainable (not vanishing or exploding gradient).\n\nBasically for any given number of layers and initialization covariances for weights and biases, tells you if the network will be trainable or not, kind of an architecture validation tool.\n\n**To be noted:** any amount of dropout removes the critical point and therefore imply an upper bound on trainable network depth.\n\n## Caveats:\n\n*   Consider only bounded activation units: no relu, etc.\n*   Applies directly only to fully connected feed-forward networks: no convnet, etc.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1611.01232"
    },
    "1357": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/HuangRSZKFFWSG017",
        "transcript": "_Objective:_ Compare several meta-architectures and hyper-parameters in the same framework for easy comparison.\n\n## Architectures:\n\nFour meta architectures:\n\n1.  R-CNN\n2.  Faster R-CNN\n3.  SSD\n4.  YOLO Architecture (not evaluated in the paper)\n\n[![screen shot 2017-05-05 at 3 12 57 pm](https://cloud.githubusercontent.com/assets/17261080/25746807/5a294360-31a5-11e7-808e-d48497a16cd5.png)](https://cloud.githubusercontent.com/assets/17261080/25746807/5a294360-31a5-11e7-808e-d48497a16cd5.png)\n\n## Results:\n\nVery interesting to know which framework to implement or not at first glance.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2017.351"
    },
    "1358": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1611.01578",
        "transcript": "_Objective:_ Design a network that will itself find the best architecture for a given task.\n\n_Dataset:_ [CIFAR10](https://www.cs.toronto.edu/%7Ekriz/cifar.html) and [PTB](https://catalog.ldc.upenn.edu/ldc99t42).\n\n\n## Inner-workings:\n\nThe meta-network (a RNN) generates a string specifying the child network parameters. Such a child network is then trained for 35-50 epochs and its accuracy is used as the reward to train the meta-network with Reinforcement Learning.  \nThe RNN first generates networks with few layers (6) then this number is increased as training progresses.\n\n## Architecture:\n\nThey develop one architecture for CNN where they predict each layers characteristic plus it's possible skip-connection:  \n[![screen shot 2017-05-24 at 8 13 01 am](https://cloud.githubusercontent.com/assets/17261080/26389176/d807de42-4058-11e7-942a-8a129558e126.png)](https://cloud.githubusercontent.com/assets/17261080/26389176/d807de42-4058-11e7-942a-8a129558e126.png)\n\nAnd one specific for LTSM-style:  \n[![screen shot 2017-05-24 at 8 13 26 am](https://cloud.githubusercontent.com/assets/17261080/26389190/e2bfd506-4058-11e7-9168-62abd040156e.png)](https://cloud.githubusercontent.com/assets/17261080/26389190/e2bfd506-4058-11e7-9168-62abd040156e.png)\n\n## Distributed setting:\n\nBellow is the distributed setting that they use with parameter servers connected to replicas (GPUs) that trained child networks.  \n[![screen shot 2017-05-24 at 8 09 05 am](https://cloud.githubusercontent.com/assets/17261080/26389084/5e354456-4058-11e7-83a9-089cb2c115b7.png)](https://cloud.githubusercontent.com/assets/17261080/26389084/5e354456-4058-11e7-83a9-089cb2c115b7.png)\n\n## Results:\n\nOverall they trained 12800 networks on 800 GPUs but they achieve state of the art results which not human intervention except the vocabulary selection (activation type, type of cells, etc). Next step, transfer learning from one task to another for the meta-network?",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1611.01578"
    },
    "1359": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/GhifaryKZBL16",
        "transcript": "_Objective:_ Build a network easily trainable by back-propagation to perform unsupervised domain adaptation while at the same time learning a good embedding for both source and target domains.\n\n_Dataset:_ [SVHN](ufldl.stanford.edu/housenumbers/), [MNIST](yann.lecun.com/exdb/mnist/), [USPS](https://www.otexts.org/1577), [CIFAR](https://www.cs.toronto.edu/%7Ekriz/cifar.html) and [STL](https://cs.stanford.edu/%7Eacoates/stl10/).\n\n\n## Architecture:\n\nVery similar to RevGrad but with some differences.\n\nBasically a shared encoder and then a classifier and a reconstructor.  \n[![screen shot 2017-05-22 at 6 11 22 pm](https://cloud.githubusercontent.com/assets/17261080/26318076/21361592-3f1a-11e7-9213-9cc07cfe2f2a.png)](https://cloud.githubusercontent.com/assets/17261080/26318076/21361592-3f1a-11e7-9213-9cc07cfe2f2a.png)\n\nThe two losses are:\n\n*   the usual cross-entropy with softmax for the classifier\n*   the pixel-wise squared loss for reconstruction\n\nWhich are then combined using a trade-off hyper-parameter between classification and reconstruction.\n\nThey also use data augmentation to generate additional training data during the supervised training using only geometrical deformation: translation, rotation, skewing, and scaling\n\nPlus denoising to reconstruct clean inputs given their noisy counterparts (zero-masked noise and Gaussian noise).\n\n## Results:\n\nOutperforms state of the art on most tasks at the time, now outperformed itself by Generate To Adapt on most tasks.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1607.03516"
    },
    "1360": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/jmlr/GaninUAGLLML16",
        "transcript": "_Objective:_ Find a feature representation that cannot discriminate between the training (source) and test (target) domains using a discriminator trained directly on this embedding.\n\n_Dataset:_ MNIST, SYN Numbers, SVHN, SYN Signs, OFFICE, PRID, VIPeR and CUHK.\n\n## Architecture:\n\nThe basic idea behind this paper is to use a standard classifier network and chose one layer that will be the feature representation. The network before this layer is called the `Feature Extractor` and after the `Label Predictor`. Then a new network called a `Domain Classifier` is introduced that takes as input the extracted feature, its objective is to tell if a computed feature embedding came from an image from the source or target dataset.\n\nAt training the aim is to minimize the loss of the `Label Predictor` while maximizing the loss of the `Domain Classifier`. In theory we should end up with a feature embedding where the discriminator can't tell if the image came from the source or target domain, thus the domain shift should have been eliminated.\n\nTo maximize the domain loss, a new layer is introduced, the `Gradient Reversal Layer` which is equal to the identity during the forward-pass but reverse the gradient in the back-propagation phase. This enables the network to be trained using simple gradient descent algorithms.\n\nWhat is interesting with this approach is that any initial network can be used by simply adding a few new set of layers for the domain classifiers. Below is a generic architecture.\n\n[![screen shot 2017-04-18 at 1 59 53 pm](https://cloud.githubusercontent.com/assets/17261080/25129680/590f57ee-243f-11e7-8927-91124303b584.png)](https://cloud.githubusercontent.com/assets/17261080/25129680/590f57ee-243f-11e7-8927-91124303b584.png)\n\n## Results:\n\nTheir approach is working but for some domain adaptation it completely fails and overall its performance are not great. Since then the state-of-the-art has changed, see DANN combined with GAN or ADDA.",
        "sourceType": "blog",
        "linkToPaper": "http://jmlr.org/papers/v17/15-239.html"
    },
    "1361": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/RenHGS15",
        "transcript": "_Objective:_ Improve on Fast R-CNN and [SPPnet](https://arxiv.org/abs/1406.4729) by incorporating the region proposal network directly.\n\n_Dataset:_ [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) and [COCO](http://mscoco.org/).\n\n\nBoth Fast R-CNN and SPPnet takes as input an image and several possibles objects (corresponding to regions of interest) and score each of them. They are thus two different entities:\n\n1.  A region proposal network.\n2.  A classification/detection network (Fast R-CNN/SSPnet).\n\n## Architecture:\n\nFirst image features are extracted using a state of the art ConvNet, then they are used for both Region proposal and actual detection/classification on those regions.\n\n[![screen shot 2017-04-14 at 2 59 28 pm](https://cloud.githubusercontent.com/assets/17261080/25043807/01a287b6-2123-11e7-944c-01493371df29.png)](https://cloud.githubusercontent.com/assets/17261080/25043807/01a287b6-2123-11e7-944c-01493371df29.png)\n\n## Results:\n\nBy incorporating the region proposal network right after the feature ConvNet its computation cost becomes basically free which leads to an elegant solution (only one network) but more importantly greatly improve speed at test time.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks"
    },
    "1362": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/HeZRS15",
        "transcript": "_Objective:_ Solve the degradation problem where adding layers induces a higher training error.\n\n_Dataset:_ [CIFAR10](https://www.cs.toronto.edu/%7Ekriz/cifar.html), [PASCAL](http://host.robots.ox.ac.uk/pascal/VOC/) and [COCO](http://mscoco.org/).\n\n## Inner-workings:\n\nThey argue that it is easier to learn the difference to the identity (the residual) than the actual mapping. Basically they start with the identity and learn the residual mapping.  \nThis allows for easier training and thus deeper network.\n\n## Architecture:\n\nThey introduce two new building block for Residual Networks, depending on the input dimensionality:  \n[![screen shot 2017-05-31 at 3 49 59 pm](https://cloud.githubusercontent.com/assets/17261080/26635061/d489dbe2-4618-11e7-911e-68772265ee9f.png)](https://cloud.githubusercontent.com/assets/17261080/26635061/d489dbe2-4618-11e7-911e-68772265ee9f.png)  \n[![screen shot 2017-05-31 at 3 57 47 pm](https://cloud.githubusercontent.com/assets/17261080/26635420/f6f22af8-4619-11e7-9639-ed651f8b18bb.png)](https://cloud.githubusercontent.com/assets/17261080/26635420/f6f22af8-4619-11e7-9639-ed651f8b18bb.png)\n\nThat can then be chained to produce network such as:  \n[![screen shot 2017-05-31 at 3 54 16 pm](https://cloud.githubusercontent.com/assets/17261080/26635258/7b64530c-4619-11e7-81c8-5d6be547da77.png)](https://cloud.githubusercontent.com/assets/17261080/26635258/7b64530c-4619-11e7-81c8-5d6be547da77.png)\n\n## Results:\n\nWon most 1st places, very impressive and adding layers do increase accuracy.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1512.03385"
    },
    "1363": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1511.06434",
        "transcript": "_Objective:_ Propose a more stable set of architectures for training GAN and show that they learn good representations of images for supervised learning and generative modeling.\n\n_Dataset:_ [LSUN](http://www.yf.io/p/lsun) and [ImageNet 1k](www.image-net.org/).\n\n## Architecture:\n\nBelow are the guidelines for making DCGANs.  \n[![screen shot 2017-04-24 at 10 58 17 am](https://cloud.githubusercontent.com/assets/17261080/25329644/f3885f7c-28dc-11e7-8895-051124c8ff6c.png)](https://cloud.githubusercontent.com/assets/17261080/25329644/f3885f7c-28dc-11e7-8895-051124c8ff6c.png)\n\nAnd here is a sample network:  \n[![screen shot 2017-04-24 at 10 57 54 am](https://cloud.githubusercontent.com/assets/17261080/25329634/e9c14abc-28dc-11e7-8bed-068f7f7bc78d.png)](https://cloud.githubusercontent.com/assets/17261080/25329634/e9c14abc-28dc-11e7-8bed-068f7f7bc78d.png)\n\nA tensorflow implementation can be found [here](https://github.com/carpedm20/DCGAN-tensorflow) along with an [online demo](https://carpedm20.github.io/faces/).\n\n## Results:\n\nQuite interesting especially concerning the structure learned in the Z-space and how this can be used for interpolation or object removal, see the example that is shown everywhere:  \n[![screen shot 2017-04-24 at 11 20 03 am](https://cloud.githubusercontent.com/assets/17261080/25330458/080b6b4e-28e0-11e7-9ab6-ce58ef5b5562.png)](https://cloud.githubusercontent.com/assets/17261080/25330458/080b6b4e-28e0-11e7-9ab6-ce58ef5b5562.png)\n\nNonetheless the network is still generating small images (32x32).",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1511.06434"
    },
    "1364": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/aaai/HsuL15",
        "transcript": "Automatically learn which Active Learning strategy to use.\n\n_Code:_ [here](https://github.com/ntucllab/libact)\n\n## Inner-workings:\n\nThey use the multi-armed bandit framework where each arm is an Active Learning strategy.\n\nThe core RL algorithm used is [EXP4.P](https://arxiv.org/abs/1002.4058) which is itself based on EXP4 (**Exp**onential weighting for **Exp**loration and **Exp**lotation with **Exp**erts). They make only slight adjustments to the reward function.\n\n## Algorithm:\n\n[![screen shot 2017-06-14 at 7 33 46 pm](https://user-images.githubusercontent.com/17261080/27146101-6d8392b4-5138-11e7-8e12-5617b258ddfa.png)](https://user-images.githubusercontent.com/17261080/27146101-6d8392b4-5138-11e7-8e12-5617b258ddfa.png)\n\n## Results:\n\nBeats all other techniques most of the time and make sure that in the long run we use the best strategy.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9636"
    },
    "1365": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/iccv/Girshick15",
        "transcript": "Improve on [R-CNN](https://arxiv.org/abs/1311.2524) and [SPPnet](https://arxiv.org/abs/1406.4729) with easier and faster training.\n\nRegion-based Convolutional Neural Network (R-CNN), basically takes as input and image and several possibles objects (corresponding to Region of Interest) and score each of them.\n\n## Architecture:\n\nThe feature map is computed for the whole image and then for each region of interest a new fixed-length feature vector is computed using max-pooling. From it two predictions are made for classification and bounding-box offsets.\n\n[![screen shot 2017-04-14 at 12 46 38 pm](https://cloud.githubusercontent.com/assets/17261080/25041460/6e7cba40-2110-11e7-8650-faae2a6b0a92.png)](https://cloud.githubusercontent.com/assets/17261080/25041460/6e7cba40-2110-11e7-8650-faae2a6b0a92.png)\n\n## Results:\n\nBy sharing computation for RoIs of the same image and allowing simple SGD training it really improves performance training although at testing it's still not as fast as YOLO9000.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1109/ICCV.2015.169"
    },
    "1366": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1502.03167",
        "transcript": "Network training is very sensitive to learning rate and initialization factors. Each layer output distribution is different than its input distribution (called covariate shift) which implies that layers have to permanently adapt to new input distribution. In this paper the author introduce batch normalization, a new layer to reduce covariate shift.\n\n_Dataset:_ [MNIST](http://yann.lecun.com/exdb/mnist/), [ImageNet](www.image-net.org/).\n\n#### Inner workings:\n\nBatch normalization fixes the means and variances of layer inputs for a training batch by computing the following normalization on each batch.  \n[![screen shot 2017-04-13 at 10 21 39 am](https://cloud.githubusercontent.com/assets/17261080/24996464/4027fbba-2033-11e7-966a-2db3c0f1389d.png)](https://cloud.githubusercontent.com/assets/17261080/24996464/4027fbba-2033-11e7-966a-2db3c0f1389d.png)  \nThe parameters Gamma and Beta are then learned with a gradient descent.  \nDuring inference the statistics are computed using unbiased estimators of the whole dataset (and not just the batch).\n\n#### Results:\n\nBatch normalization provides several advantages:\n\n1.  Use of a higher learning rate without risk of divergence by stabilizing the gradient scale.\n2.  Regularizes the model.\n3.  Reduces the need for dropout.\n4.  Avoid the network to get stuck when using saturating nonlinearities.\n\n#### What to do?\n\n1.  Add batch norm layer before activation layers.\n2.  Increase the learning rate.\n3.  Remove dropout.\n4.  Reduce L2 weight regularization.\n5.  Accelerate learning rate decay.\n6.  Reduce picture distorsion for data augmentation.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1502.03167"
    },
    "1367": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/ReedLASER14",
        "transcript": "_Objective:_ Design a loss to make deep network robust to label noise.\n\n_Dataset:_ [MNIST](yann.lecun.com/exdb/mnist/), Toroto Faces Database, [ILSVRC2014](http://www.image-net.org/challenges/LSVRC/2014/).\n\n\n#### Inner-workings:\n\nThree types of losses are presented:\n\n*   reconstruciton loss:\n\n[![screen shot 2017-06-26 at 11 00 07 am](https://user-images.githubusercontent.com/17261080/27532200-bb42b8a6-5a5f-11e7-8c14-673958216bfc.png)](https://user-images.githubusercontent.com/17261080/27532200-bb42b8a6-5a5f-11e7-8c14-673958216bfc.png)\n\n*   soft bootstrapping which uses the predicted labels by the network `qk` and the user-provided labels `tk`:\n\n[![screen shot 2017-06-26 at 11 10 43 am](https://user-images.githubusercontent.com/17261080/27532296-1e01a420-5a60-11e7-9273-d1affb0d7c2e.png)](https://user-images.githubusercontent.com/17261080/27532296-1e01a420-5a60-11e7-9273-d1affb0d7c2e.png)\n\n*   hard bootstrapping replaces the soft predicted labels by their binary version:\n\n[![screen shot 2017-06-26 at 11 12 58 am](https://user-images.githubusercontent.com/17261080/27532439-a3f9dbd8-5a60-11e7-91a7-327efc748eae.png)](https://user-images.githubusercontent.com/17261080/27532439-a3f9dbd8-5a60-11e7-91a7-327efc748eae.png)\n\n[![screen shot 2017-06-26 at 11 13 05 am](https://user-images.githubusercontent.com/17261080/27532463-b52f4ab4-5a60-11e7-9aed-615109b61bd8.png)](https://user-images.githubusercontent.com/17261080/27532463-b52f4ab4-5a60-11e7-9aed-615109b61bd8.png)\n\n#### Architecture:\n\n\nThey test with Feed Forward Neural Networks only.\n\n#### Results:\n\nThey use only permutation noise with a very high probability compared with what we might encounter in real-life.\n\n[![screen shot 2017-06-26 at 11 29 05 am](https://user-images.githubusercontent.com/17261080/27533105-b051d366-5a62-11e7-95f3-168d0d2d7841.png)](https://user-images.githubusercontent.com/17261080/27533105-b051d366-5a62-11e7-95f3-168d0d2d7841.png)\n\nThe improvement for small noise probability (<10%) might not be that interesting.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1412.6596"
    },
    "1368": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1409.7495",
        "transcript": "_Objective:_ Build a network easily trainable by back-propagation to perform unsupervised domain adaptation while at the same time learning a good embedding for both source and target domains.\n\n _Dataset:_ [SVHN](ufldl.stanford.edu/housenumbers/), [MNIST](yann.lecun.com/exdb/mnist/), [USPS](https://www.otexts.org/1577), [CIFAR](https://www.cs.toronto.edu/%7Ekriz/cifar.html) and [STL](https://cs.stanford.edu/%7Eacoates/stl10/).\n\n#### Architecture:\n\nVery similar to RevGrad but with some differences.\n\nBasically a shared encoder and then a classifier and a reconstructor.  \n[![screen shot 2017-05-22 at 6 11 22 pm](https://cloud.githubusercontent.com/assets/17261080/26318076/21361592-3f1a-11e7-9213-9cc07cfe2f2a.png)](https://cloud.githubusercontent.com/assets/17261080/26318076/21361592-3f1a-11e7-9213-9cc07cfe2f2a.png)\n\nThe two losses are:\n\n*   the usual cross-entropy with softmax for the classifier\n*   the pixel-wise squared loss for reconstruction\n\nWhich are then combined using a trade-off hyper-parameter between classification and reconstruction.\n\nThey also use data augmentation to generate additional training data during the supervised training using only geometrical deformation: translation, rotation, skewing, and scaling\n\nPlus denoising to reconstruct clean inputs given their noisy counterparts (zero-masked noise and Gaussian noise).\n\n#### Results:\n\nOutperforms state of the art on most tasks at the time, now outperformed itself by Generate To Adapt on most tasks.",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1409.7495"
    },
    "1369": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/MirzaO14",
        "transcript": "_Objective:_ In an unconditional GAN it's not possible to control the mode of the data being generated which is what this paper tries to accomplish using the label data (but it can be generalized to any kind of conditional data).\n\n_Dataset:_ [MNIST](yann.lecun.com/exdb/mnist/) and [MIRFLICKR](http://press.liacs.nl/mirflickr/).\n\n\n#### Inner workings:\n\nChanges the loss to the conditional loss:  \n[![screen shot 2017-04-24 at 10 07 25 am](https://cloud.githubusercontent.com/assets/17261080/25327832/e86f53fe-28d5-11e7-8694-6df8f2e1ef18.png)](https://cloud.githubusercontent.com/assets/17261080/25327832/e86f53fe-28d5-11e7-8694-6df8f2e1ef18.png)\n\nFor implementation the only thing needed is to feed the label data to both the discriminator and generator:  \n[![screen shot 2017-04-24 at 10 07 18 am](https://cloud.githubusercontent.com/assets/17261080/25327826/e53ab4a8-28d5-11e7-8056-1518602d50c9.png)](https://cloud.githubusercontent.com/assets/17261080/25327826/e53ab4a8-28d5-11e7-8056-1518602d50c9.png)\n\n#### Results:\n\nInteresting at the time but not surprising now. There's not much more to the paper than what is in the summary.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1411.1784"
    },
    "1370": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/OquabBLS14",
        "transcript": "_Objective:_ Transfer feature learned from large-scale dataset to small-scale dataset\n\n _Dataset:_ [ImageNet](www.image-net.org), [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n\n\n#### Inner-workings:\n\nBasically they train the network on the large dataset, then replace the last layers, sometimes adding a new one and train this on the new dataset. Pretty standard transfer learning nowadays.  \n[![screen shot 2017-06-14 at 3 06 37 pm](https://user-images.githubusercontent.com/17261080/27133634-2d4c0fde-5113-11e7-848a-719514b1a12c.png)](https://user-images.githubusercontent.com/17261080/27133634-2d4c0fde-5113-11e7-848a-719514b1a12c.png)\n\nWhat's a bit more interesting is how they deal with background being overrepresented by using the bounding box that they have.  \n[![screen shot 2017-06-14 at 3 06 43 pm](https://user-images.githubusercontent.com/17261080/27133641-34d4ee7e-5113-11e7-8307-f1ff708bd5c7.png)](https://user-images.githubusercontent.com/17261080/27133641-34d4ee7e-5113-11e7-8307-f1ff708bd5c7.png)\n\n#### Results:\n\nA bit dated, not really applicable but the part on specifically tackling the domain shift (such as background) is interesting.  \nPlus they use the bounding-box information to refine the dataset.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/CVPR.2014.222"
    },
    "1371": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icann/MasciMCS11",
        "transcript": "_Objective:_ Introduces the Convolutional Auto-Encoder, a hierarchical unsupervised feature extractor.\n\n_Dataset:_ [MNIST](yann.lecun.com/exdb/mnist/) and [SVHN](ufldl.stanford.edu/housenumbers/).\n\n#### Architecture:\n\nUses convolutions to generate an encoding of the image and then decodes it and do a pixel-wise comparison.  \nUsed to initializes CNN.\n\n#### Results:\n\nOld article, not really relevant nowadays. They don't speak about the deconvolution part.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-642-21735-7_7"
    },
    "1372": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/cvpr/ZeilerKTF10",
        "transcript": "_Objective:_ Define a new deconvolution layer.\n\n#### Results:\n\nNot really interesting except from the fact that it first introduces **deconvolution layers** which are very ill-name as they are not actual deconvolution but instead a **transposed convolution** or also called a **fractionally strided convolutions**.\n\n[![Deconvolutional layer](https://cloud.githubusercontent.com/assets/17261080/25344392/44693b48-2912-11e7-8dda-2b64d99292a9.gif)](https://cloud.githubusercontent.com/assets/17261080/25344392/44693b48-2912-11e7-8dda-2b64d99292a9.gif)\n\nVisualization for other operations can be seen [here](https://github.com/vdumoulin/conv_arithmetic) corresponding to [A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285.pdf).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://ieeexplore.ieee.org/document/5539957/"
    },
    "1373": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=settles09",
        "transcript": "Very good introduction to active learning.\n\n#### Scenarios\n\nThere are three mains scenari:\n\n*   Pool-based: a large amount of unlabeled data is available and we need to chose which one to annotate next.\n*   Stream-based: same as above except example come one after the other.\n*   Membership query synthesis: we can generate the point to label.\n\n#### Query Strategy Frameworks\n\n2.1. Uncertainty Sampling\n\nBasically how to evaluate the informativeness of unlabeled instances and then select the most informative.\n\n2.1.1. Least Confident\n\nQuery the instances about which the algorithm is least certain how to label.  \n[![screen shot 2017-06-14 at 5 08 37 pm](https://user-images.githubusercontent.com/17261080/27139765-281f1374-5124-11e7-9418-fb458be0bfc3.png)](https://user-images.githubusercontent.com/17261080/27139765-281f1374-5124-11e7-9418-fb458be0bfc3.png)  \n[![screen shot 2017-06-14 at 5 09 36 pm](https://user-images.githubusercontent.com/17261080/27139841-5636458e-5124-11e7-95c4-ea586deb853a.png)](https://user-images.githubusercontent.com/17261080/27139841-5636458e-5124-11e7-95c4-ea586deb853a.png)  \nMost used by discard information on all other labels.\n\n2.1.2. Margin Sampling\n\nUse the first two labels and chose the instance for which the different between the two is the smallest.  \n[![screen shot 2017-06-14 at 5 12 29 pm](https://user-images.githubusercontent.com/17261080/27139968-aabebe6a-5124-11e7-879b-f518e2279eba.png)](https://user-images.githubusercontent.com/17261080/27139968-aabebe6a-5124-11e7-879b-f518e2279eba.png)\n\n2.1.3. Entropy\n\nInstead of using the two first labels, why not use all of them?  \n[![screen shot 2017-06-14 at 5 13 44 pm](https://user-images.githubusercontent.com/17261080/27140049-e33ea25a-5124-11e7-84ea-adab87d29174.png)](https://user-images.githubusercontent.com/17261080/27140049-e33ea25a-5124-11e7-84ea-adab87d29174.png)\n\n#### Query-By-Committee\n\nA committee of different models is trained. They then vote on which instance to label and the one for which they most disagree is chosen.\n\nTo measure the level of disagreement, one can either use:\n\n*   Vote entropy:\n\n[![screen shot 2017-06-14 at 5 20 26 pm](https://user-images.githubusercontent.com/17261080/27140436-d12d330a-5125-11e7-8f40-7be3bbc83987.png)](https://user-images.githubusercontent.com/17261080/27140436-d12d330a-5125-11e7-8f40-7be3bbc83987.png)\n\n*   Kullback-Leibler divergence:\n\n[![screen shot 2017-06-14 at 5 21 32 pm](https://user-images.githubusercontent.com/17261080/27140492-f45be722-5125-11e7-9b42-204aaf4bdd92.png)](https://user-images.githubusercontent.com/17261080/27140492-f45be722-5125-11e7-9b42-204aaf4bdd92.png)\n\n[![screen shot 2017-06-14 at 5 22 29 pm](https://user-images.githubusercontent.com/17261080/27140537-12289cd2-5126-11e7-8e1d-62158576cd95.png)](https://user-images.githubusercontent.com/17261080/27140537-12289cd2-5126-11e7-8e1d-62158576cd95.png)\n\n#### Expected Model Change\n\nSelects the instance that would impart the greatest change to the current model if we knew its label.\n\n*   Expected Gradient Length: compute the gradient for all instances and find the one with the largest magnitude on average for all labels.\n\n[![screen shot 2017-06-14 at 5 25 20 pm](https://user-images.githubusercontent.com/17261080/27140694-79cc6e4a-5126-11e7-9314-e837a1e0eba2.png)](https://user-images.githubusercontent.com/17261080/27140694-79cc6e4a-5126-11e7-9314-e837a1e0eba2.png)\n\n#### Expected Error Reduction\n\nMeasure not how much the model is likely to change, but how much its generalization error is likely to be reduced. Either by measuring:\n\n*   Expected 0/1 loss: to reduce the expected total number of incorrect predictions. A new model needs to be trained for every label and instance, very greedy.\n\n[![screen shot 2017-06-14 at 5 28 42 pm](https://user-images.githubusercontent.com/17261080/27140912-08d7410a-5127-11e7-9d53-33f2044692a2.png)](https://user-images.githubusercontent.com/17261080/27140912-08d7410a-5127-11e7-9d53-33f2044692a2.png)\n\n*   Expected Log-Loss: maximizing the expected information gain of the query. Still very greedy in computation! Not really usable except if the model can be analytically resolved instead of re-trained.\n\n[![screen shot 2017-06-14 at 5 30 42 pm](https://user-images.githubusercontent.com/17261080/27140970-3e117516-5127-11e7-9936-671fea5d94dd.png)](https://user-images.githubusercontent.com/17261080/27140970-3e117516-5127-11e7-9936-671fea5d94dd.png)\n\n#### Variance Reduction\n\nReduce generalization error indirectly by minimizing the output variance.  \n[![screen shot 2017-06-14 at 5 38 17 pm](https://user-images.githubusercontent.com/17261080/27141417-6507b71a-5128-11e7-81ca-ab227836098f.png)](https://user-images.githubusercontent.com/17261080/27141417-6507b71a-5128-11e7-81ca-ab227836098f.png)\n\n#### Density-Weighted Methods\n\n[![screen shot 2017-06-14 at 5 40 53 pm](https://user-images.githubusercontent.com/17261080/27141501-a920bd34-5128-11e7-8e9d-0870da365633.png)](https://user-images.githubusercontent.com/17261080/27141501-a920bd34-5128-11e7-8e9d-0870da365633.png)\n\nWith the left function is the informativeness of x and the right function represents average similarity to all other instances in the input distribution",
        "sourceType": "blog",
        "linkToPaper": "http://axon.cs.byu.edu/~martinez/classes/778/Papers/settles.activelearning.pdf"
    },
    "1374": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icml/KohL17",
        "transcript": "**Goal**: identifying training points most responsible for a given prediction.\n\nGiven training points $z_1, \\dots, z_n$, let loss function be $\\frac{1}{n}\\sum_{i=1}^nL(z_i, \\theta)$ \n\nA function called influence function let us compute the parameter change if $z$ were upweighted by some small $\\epsilon$. \n$$\\hat{\\theta}_{\\epsilon, z} := \\arg \\min_{\\theta \\in \\Theta} \\frac{1}{n}\\sum_{i=1}^n L(z_i, \\theta) + \\epsilon L(z, \\theta)$$\n\n$$\\mathcal{I}_{\\text{up, params}}(z) := \\frac{d\\hat{\\theta}_{\\epsilon, z}}{d\\epsilon} = -H_{\\hat{\\theta}}^{-1} \\nabla_\\theta L(z, \\hat{\\theta})$$\n\n$\\mathcal{I}_{\\text{up, params}}(z)$ shows how uplifting one point $z$ affect the estimate of the parameters $\\theta$. \n\nFurthermore, we could determine how uplifting $z$ affect the loss estimate of a test point through chain rule. \n$$\\mathcal{I}_{\\text{up, loss}}(z, z_{\\text{test}}) = \\nabla_\\theta L(z_{\\text{test}}, \\hat{\\theta})^\\top \\mathcal{I}_{\\text{up, params}}(z)$$ \n\nApart from lifting one training point, change of the parameters with the change of a training point could also be estimated. \n$$\\frac{d\\hat{\\theta}_{\\epsilon, z_\\delta, -z}}{d\\epsilon} = \\mathcal{I}_{\\text{up, params}}(z_\\delta) - \\mathcal{I}_{\\text{up, params}}(z)$$\nThis measures how purturbation $\\delta$ to training point $z$ affect the parameter estimation $\\theta$.\n\nSection 3 describes some practicals about efficient implementing.\n\nThis set of tool could be used for some interpretable machine learning tasks.",
        "sourceType": "blog",
        "linkToPaper": "http://proceedings.mlr.press/v70/koh17a.html"
    },
    "1375": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1806-00451",
        "transcript": "TL;DR although all researchers are overfitting the visual dataset, the relative rank of those classfiers are stable. In some sense, what happens about overfitting dataset in computer vision research is not too wrong.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1806.00451"
    },
    "1376": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1086/502691",
        "transcript": "Stuart and Ding's paper is a sociological look at the predictors of entrepreneurship among academic scientists. The paper is framed using the basic tension between academic science and commerce (discussed around patents in particular by Fiona Murry in The Oncomouse that roared: Resistance and accommodation to patenting in academic science). The authors essentially present an event history that models the conditions prompting university employed scientists to become entrepreneurs which they define as either founding as a biotechnology company or joining the scientific advisory board of a new biotechnology firm. The authors find that the most prominent scientists joined first with the very most prestigious being among the first but that, over time, the \"bar\" has been lowered and a wider number and variety of scientists are now engaging in entrepreneurship.\n\nThe paper's basic framing is around the idea that entrepreneurship and science used to be seen as incompatible or even counter-productive and bad. In the biosciences, this seem to no longer be the case. The paper aims to look at the changes over time and explain how this transition happened.\n\nThe paper looks at four determinants: (1) socialization in graduate school (2) peer influence exerted across a faculties social network (3) the presents of pro-entrepreneurship colleagues in a scientists workplace and (4) differential access to social resources that facilitate entrepreneurship.\n\nUsing an impressive dataset built from a from a variety of sources, the findings show taht commercial science began first, and diffused across, the most elite scientists (i.e., most prestigious institutions, most citations. most co-authors, etc) and then, over time, diffused to more.\n\nThe authors use an event history to test eight formal hypotheses (each included verbatim below:\n\n1. Scientists are more likely to transition to the entrepreneurial role when they are affiliated with institutions that employ other scientists who have participated in commercial science. (Supported)\n2. The effect of prior local adopters on scientists\u2019 rate of transition to entrepreneurship will have been weaker in medical schools than it was in university science departments. (Supported)\n3. The effect of prior local adopters on scientists\u2019 rate of transition to commercial science will decline as academic entrepreneurship gains acceptance in the scientific community. (Supported)\n4. As faculty members in arts and sciences departments come to accept entrepreneurship as a legitimate professional activity, the difference in the rates of transition to academic entrepreneurship between scientists in medical schools and those in departments of arts and sciences will decline. (Supported)\n5. Scientists are more likely to transition to the entrepreneurial role when they are affiliated with universities that employ high-status scientists who have previously made the transition. (Supported)\n6. Life scientists who were trained in universities with pro-entrepreneurship faculty members are more likely to transition to commercial science later in their careers. (Not supported)\n7. Scientists who have previously coauthored research with academic entrepreneurs are more likely to transition to commercial science. (Supported)\n8. Co-authorship ties with scientists who have high centrality in the commercial sector will have a particularly large effect on the transition rate. (Supported)\n\nThere are important remaining questions about why the high-status individuals are first to make the move to entrepreneurship. The authors suggest that is probably a factor of opportunity. Alternatively, there may be a status story in that the high-prestige scientists felt that their positions was less threatened.\n\n#### Theoretical and practical relevance:\n\nThe paper has been cited more than 100 times in the six year since its publication. These have primarily have been in the entrepreneurship literature.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1086/502691"
    },
    "1377": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1177/016224399301800306",
        "transcript": "Published in Science, Technology, and Human Values, Langdon Winner's article is adapted from a presidential speech given at the conference for the Society for Philosophy and Technology. The speech is an extended critique of social constructivism in technology from the perspective of a philosophy in general and morals in particular. The article is prompted by the groups of sociologists studying science and technology and the growth of STS more generally (he cite Bruno Latour and Steven Woolgar in particular as authors he responding to).\n\nWinner praises the sociological study of science for bringing empirical rigor to the study of science and the means through which it is created. Although he argues that these social constructivists have paid less attention to technology, he argues that provided a useful service in calling into questions the highly arbitrary divisions between the social sphere and the technical sphere. But Winner also argues that social constructivists are essentially fighting for primacy against more traditional considerations of technology like those by Marx, Lewis Mumford, and Heidegger that are more closely aligned with the philosophy of technology. Winner cautions that we should, \"notice what one gives up as well as what one gains in choosing this intellectual path to the study of technology and human affairs.\"\n\n* Winner argues that the social consequences of technical choice are almost left out of view completely by the more empirically minding approaches which calls things constructive, provides the evidence, and goes home.\n* He argues that with its focus on \"relevant social actors\" it ends up discounting the experience or values of \"irrelevant\" groups who are indeed affected by technology noting that unpacking black boxes will end up concealing as much as it reveals.\n* It's focus on social structure ignores other important factors that a focus on the technology itself or on other factors might leave out.\n* It leaves out moral questions that make it impossible to evaluate technological choices. In Winner's terms, \"the methodological bracketing of questions about interests and interpretations amounts to a political stance that regards the status quo and its ills an injustices with precision equanimity.\"\n\nThe paper ends with a response to Steven Woolgar who made an exemplary argument against Winner's description of the primacy of political interpretation of Robert Moses's bridges in Do artifacts have politics? and a memorable quote from Winner:\n\n*Although the social constructivists have opened the black box and shown a colorful array of social actors, processes, and images therein, the box they reveal is still a remarkably hollow one.*\n\n#### Theoretical and practical relevance:\n\nWinner's paper has been cited over 300 times since it's publication nearly 20 years ago.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1177/016224399301800306"
    },
    "1378": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1016/0048-7333(94)01002-1",
        "transcript": "Partha Dasgupta and Paul David open their article suggesting that economics literature has, \"lacked an overarching conceptual framework to guide empirical studies and public policy discussions\" about science. They attempt to unpack the reputation-based reward systems in science to help understand what the economic drivers are of scientific work and how public changes might influence those incentives and change science.\n\nThe basic argument is framed by three features of science:\n\n* Borrowing from agency theory, scientific production and progress is very costly for outsiders to monitor.\n* There are significant aspects of indivisibility, attendant fixed costs, and economies of scale inherent in the underlying processes of knowledge production.\n* Knowledge created can be kept from the public if researcher choose.\n\nThe goals of the authors is to introduce an \"economics of science\" which:\n\n* Exposes the logic of scientific institutions.\n* Examines implications of different types of institutions on the efficiency of resources allocation within science.\n\nThe authors argue that difference between science and technology basically comes down to a different set of socio-political reward systems affecting the allocation of resources. In technology, work is kept secret and owned, in science, it is put into the public domain. They argue that because markets are reasonable bad at the second type of production (i.e., the production of a public good), either (1) governments can engage in science directly, (2) society can grant monopoly rights to scientific production, or (3) scientific production can be done through public subsidies but without exclusive rights being granted to the creators. The authors collapse (1) and (3) together and argue that are really two core economic means of encouraging scientific production.\n\nMuch of the core of the article then goes into depth on the priority system which is the system whereby the first person that publishes something gets all the fruits (usually the credit) for a particular discovery. It talks about the combination methods that most science works under where scientists get both rewards and a set salary, as a way of balancing the agency concerns. It goes into some high-level game theory to talk about the possible inefficiencies that stem from a priority based system.\n\nThe authors argue that because there are repeated games a strong norm-based system in science, many of the potential problems with inefficiency are addressed. That said, they point out there problems with sciences emphasis on who makes discoveries (which society does not care about) and timing effects related to coordination which the scientific incentive system makes hard and that central funding organizations may have trouble manipulating.\n\nThe paper also discusses the role that science plays in training individuals for the work force and in technology and industry and a series of other issues related to science and the economy before its closing section that discusses implications for policy.\n\nThe general policy implications are one of skepticism. The authors warn that the incentive system in science works well but that it is a delicate balance and that there is some evidence that minor changes (e.g., reducing the number of PhD students being produced or promoting transferability from universities to industry) may have unintended bad effects.\n\n#### Theoretical and practical relevance:\n\nThe article was initially published in Research Policy but was subsequently re-published in the book Science bought and sold. The paper has been cited more than 1,400 times in the literature on science and innovation and innovation more broadly.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/0048-7333(94)01002-1"
    },
    "1379": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1016/0048-7333(94)01001-3",
        "transcript": "Brooks' article is a high-level review that attempts to lay out the complicated relationship between science and technology. Although almost impossible broad in scope, the article does a surprisingly good job that conveys both the depth necessary to treat the subject well and effective use of examples that go into enough specifics and examples to convey his points.\n\nHe argues that science contributed to technology in six ways:\n\n1. Direct source of new technological ideas where archetypal ideas might be the atomic bomb or X-Rays.\n2. A source of engineering design tools and techniques in ways that might be more common in more engineering-focused scientific investigations.\n3. Instrumentation, laboratory techniques, and analytical methods which includes techniques and other innovation created in the process of doing science and where scientists act as sorts of lead users creating new technologists in order to investigate questions that were otherwise not possible.\n4. Development of human skills through training students in technologies and scientific techniques and methods.\n5. Technology assessment that might look at the side effects of technologies like chemical waste and measurement of side effects.\n6. Source of development strategy that might help scientists avoid blind alleys.\n\nAdditionally, he argues that technology contributes to science in two ways:\n\n1. Source of new challenges as has been the case in material science which are driven by technological research.\n2. Instrumentation and measurement techniques where technologists create tools that end up being useful to science more generally and so that scientists don't have to create all their own tools or focus on the parts of tool creation that they are less good at.\n\nHarvey Brooks was the dean of the Harvard Division of Engineering and Applied Sciences for nearly 20 years (1957-1976) before founding the center for Science, Technology and Public Policy at the Kennedy School in 1976. This paper was published more than 10 years after his retirement.\n\n#### Theoretical and practical relevance:\n\nThe paper is a \"semi-famous\" paper and is more of a review article than an empirical piece but plays an important role in framing questions around science policy and has been cited by others exploring the relationship or making policy claims about the promotion of science for public policy reasons.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/0048-7333(94)01001-3"
    },
    "1380": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000133",
        "transcript": "A biofilm is a surface-associated population of microorganisms embedded in a matrix of extracellular polymeric substances. Biofilms are a major natural growth form of microorganisms and the cause of pervasive device-associated infection. This report focuses on the biofilm matrix of Candida albicans, the major fungal pathogen of humans. We report here that the C. albicans zinc-response transcription factor Zap1 is a negative regulator of a major matrix component, soluble \u00ce\u00b2-1,3 glucan, in both in vitro and in vivo biofilm models. To understand the mechanistic relationship between Zap1 and matrix, we identified Zap1 target genes through expression profiling and full genome chromatin immunoprecipitation. On the basis of these results, we designed additional experiments showing that two glucoamylases, Gca1 and Gca2, have positive roles in matrix production and may function through hydrolysis of insoluble \u00ce\u00b2-1,3 glucan chains. We also show that a group of alcohol dehydrogenases Adh5, Csh1, and Ifd6 have roles in matrix production: Adh5 acts positively, and Csh1 and Ifd6, negatively. We propose that these alcohol dehydrogenases generate quorum-sensing aryl and acyl alcohols that in turn govern multiple events in biofilm maturation. Our findings define a novel regulatory circuit and its mechanism of control of a process central to infection.\n\nA biofilm is a surface-associated population of microbes that is embedded in a cement of extracellular compounds. This cement is known as matrix. The two main functions of matrix are to protect cells from their surrounding environment, preventing drugs and other stresses from penetrating the biofilm, and to maintain the architectural stability of the biofilm, acting as a glue to hold the cells together. The presence of matrix is a contributing factor to the high degree of resistance to antimicrobial drugs observed in biofilms. Because biofilms have a major impact on human health, and because matrix is such a pivotal component of biofilms, it is important to understand how the production of matrix is regulated. We have begun to address this question in the major human fungal pathogen Candida albicans. We found that the zinc-responsive regulatory protein Zap1 controls the expression of several genes important for matrix formation in C. albicans. These target genes encode glucoamylases and alcohol dehydrogenases, enzymes that probably govern the synthesis of distinct matrix constituents. The findings here offer insight into the metabolic processes that contribute to biofilm formation and indicate that Zap1 functions broadly as a negative regulator of biofilm maturation.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000133"
    },
    "1381": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000140",
        "transcript": "A key question in the analysis of hippocampal memory relates to how attention modulates the encoding and long-term retrieval of spatial and nonspatial representations in this region. To address this question, we recorded from single cells over a period of 5 days in the CA1 region of the dorsal hippocampus while mice acquired one of two goal-oriented tasks. These tasks required the animals to find a hidden food reward by attending to either the visuospatial environment or a particular odor presented in shifting spatial locations. Attention to the visuospatial environment increased the stability of visuospatial representations and phase locking to gamma oscillations\u2014a form of neuronal synchronization thought to underlie the attentional mechanism necessary for processing task-relevant information. Attention to a spatially shifting olfactory cue compromised the stability of place fields and increased the stability of reward-associated odor representations, which were most consistently retrieved during periods of sniffing and digging when animals were restricted to the cup locations. Together, these results suggest that attention selectively modulates the encoding and retrieval of hippocampal representations by enhancing physiological responses to task-relevant information.\n\nAttention modulates the encoding and retrieval of memories, but the physiological basis of this interaction has largely been unexplored. The formation of memories which depend on the hippocampus involves the conscious recall of events that occur in specific spatial contexts, a form of memory known as episodic. To investigate the physiological consequences of the interaction between attention and memory in the hippocampus, we recorded single-cell activity and local field potentials \u2014 the local rhythmic oscillatory activity of neurons \u2014 from the same cells over several days while animals learned one of two goal-oriented tasks. In the visuospatial version of the task, mice had to associate a specific spatial location with a reward, independent of an odor cue. In the nonspatial, olfactory version, mice had to associate a specific odor with the food reward, independent of spatial location. We found that, during periods of navigation, only neurons in the visuospatially trained animals displayed long-term stable representations of space, and neuronal synchronization to so-called gamma oscillations, a mechanism of signal amplification that has been proposed to underlie attentional processes. Conversely, when animals were sniffing the odors in fixed spatial locations, only neurons in the olfactory-trained group displayed a stable increase in firing rate in response to the reward-associated odor. Our data suggest that attention modulates what is encoded and retrieved by hippocampal cells and that neuronal synchronization to gamma oscillations may underlie the mechanism whereby attention leads to stable spatial memory retrieval during navigation.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000140"
    },
    "1382": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000139",
        "transcript": "Pathogen perception by the plant innate immune system is of central importance to plant survival and productivity. The Arabidopsis protein RIN4 is a negative regulator of plant immunity. In order to identify additional proteins involved in RIN4-mediated immune signal transduction, we purified components of the RIN4 protein complex. We identified six novel proteins that had not previously been implicated in RIN4 signaling, including the plasma membrane (PM) H+-ATPases AHA1 and/or AHA2. RIN4 interacts with AHA1 and AHA2 both in vitro and in vivo. RIN4 overexpression and knockout lines exhibit differential PM H+-ATPase activity. PM H+-ATPase activation induces stomatal opening, enabling bacteria to gain entry into the plant leaf; inactivation induces stomatal closure thus restricting bacterial invasion. The rin4 knockout line exhibited reduced PM H+-ATPase activity and, importantly, its stomata could not be re-opened by virulent Pseudomonas syringae. We also demonstrate that RIN4 is expressed in guard cells, highlighting the importance of this cell type in innate immunity. These results indicate that the Arabidopsis protein RIN4 functions with the PM H+-ATPase to regulate stomatal apertures, inhibiting the entry of bacterial pathogens into the plant leaf during infection. Author Summary Top Plants are continuously exposed to microorganisms. In order to resist infection, plants rely on their innate immune system to inhibit both pathogen entry and multiplication. We investigated the function of the Arabidopsis protein RIN4, which acts as a negative regulator of plant innate immunity. We biochemically identified six novel RIN4-associated proteins and characterized the association between RIN4 and the plasma membrane H+-ATPase pump. Our results indicate that RIN4 functions in concert with this pump to regulate leaf stomata during the innate immune response, when stomata close to block the entry of bacterial pathogens into the leaf interior.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000139"
    },
    "1383": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000124",
        "transcript": "Apomixis, or asexual clonal reproduction through seeds, is of immense interest due to its potential application in agriculture. One key element of apomixis is apomeiosis, a deregulation of meiosis that results in a mitotic-like division. We isolated and characterised a novel gene that is directly involved in controlling entry into the second meiotic division. By combining a mutation in this gene with two others that affect key meiotic processes, we created a genotype called MiMe in which meiosis is totally replaced by mitosis. The obtained plants produce functional diploid gametes that are genetically identical to their mother. The creation of the MiMe genotype and apomeiosis phenotype is an important step towards understanding and engineering apomixis.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000124"
    },
    "1384": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000129",
        "transcript": "The mechanism by which a complex auditory scene is parsed into coherent objects depends on poorly understood interactions between task-driven and stimulus-driven attentional processes. We illuminate these interactions in a simultaneous behavioral-neurophysiological study in which we manipulate participants' attention to different features of an auditory scene (with a regular target embedded in an irregular background). Our experimental results reveal that attention to the target, rather than to the background, correlates with a sustained (steady-state) increase in the measured neural target representation over the entire stimulus sequence, beyond auditory attention's well-known transient effects on onset responses. This enhancement, in both power and phase coherence, occurs exclusively at the frequency of the target rhythm, and is only revealed when contrasting two attentional states that direct participants' focus to different features of the acoustic stimulus. The enhancement originates in auditory cortex and covaries with both behavioral task and the bottom-up saliency of the target. Furthermore, the target's perceptual detectability improves over time, correlating strongly, within participants, with the target representation's neural buildup. These results have substantial implications for models of foreground/background organization, supporting a role of neuronal temporal synchrony in mediating auditory object formation.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000129"
    },
    "1385": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000144",
        "transcript": "Reports of rapid growth in nature-based tourism and recreation add significant weight to the economic case for biodiversity conservation but seem to contradict widely voiced concerns that people are becoming increasingly isolated from nature. This apparent paradox has been highlighted by a recent study showing that on a per capita basis, visits to natural areas in the United States and Japan have declined over the last two decades. These results have been cited as evidence of \"a fundamental and pervasive shift away from nature-based recreation\" - but how widespread is this phenomenon? We address this question by looking at temporal trends in visitor numbers at 280 protected areas (PAs) from 20 countries. This more geographically representative dataset shows that while PA visitation (whether measured as total or per capita visit numbers) is indeed declining in the United States and Japan, it is generally increasing elsewhere. Total visit numbers are growing in 15 of the 20 countries for which we could get data, with the median national rate of change unrelated to the national rate of population growth but negatively associated with wealth. Reasons for this reversal of growth in the richest countries are difficult to pin down with existing data, but the pattern is mirrored by trends in international tourist arrivals as a whole and so may not necessarily be caused by disaffection with nature. Irrespective of the explanation, it is clear that despite important downturns in some countries, nature-related tourism is far from declining everywhere, and may still have considerable potential both to generate funds for conservation and to shape people's attitudes to the environment.\n\nNature-based tourism is frequently described as one of the fastest growing sectors of the world's largest industry, and a very important justification for conservation. However, a recent, high profile report has interpreted declining visit rates to US and Japanese national parks as evidence of a pervasive shift away from nature tourism. Here we use the largest database so far compiled on trends in visits to Protected Areas around the world to resolve this apparent paradox. We find that, while visit rates\u2014measured in two different ways\u2014are indeed declining in some wealthy countries, in roughly three-quarters of the nations where data are available, visits to Protected Areas are increasing. Internationally, rates of growth in the number of visits to such areas show a clear negative association with per capita income, which interestingly is matched by trends in foreign arrivals as a whole. Our results therefore suggest that, despite worrying local downturns, nature-related tourism is far from declining everywhere, and may still have considerable potential to generate funds for conservation and engage people with the environment.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000144"
    },
    "1386": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pmed.1000047",
        "transcript": "#### Background\nPain, although unpleasant, is essential for survival. Whenever the body is damaged, nerve cells detecting the injury send an electrical message via the spinal cord to the brain and, as a result, action is taken to prevent further damage. Usually pain is short-lived, but sometimes it continues for weeks, months, or years. Long-lasting (chronic) pain can be caused by an ongoing, often inflammatory condition (for example, arthritis) or by damage to the nervous system itself\u2014experts call this \u201cneuropathic\u201d pain. Damage to the brain or spinal cord causes central neuropathic pain; damage to the nerves that convey information from distant parts of the body to the spinal cord causes peripheral neuropathic pain. One example of peripheral neuropathic pain is \u201cradicular\u201d low back pain (also called sciatica). This is pain that radiates from the back into the legs. By contrast, axial back pain (the most common type of low back pain) is confined to the lower back and is non-neuropathic.\n\n#### Why Was This Study Done?\nChronic pain is very common\u2014nearly 10% of American adults have frequent back pain, for example\u2014and there are many treatments for it, including rest, regulated exercise (physical therapy), pain-killing drugs (analgesics), and surgery. However, the best treatment for any individual depends on the exact nature of their pain, so it is important to assess their pain carefully before starting treatment. This is usually done by scoring overall pain intensity, but this assessment does not reflect the characteristics of the pain (for example, whether it occurs spontaneously or in response to external stimuli) or the complex biological processes involved in pain generation. An assessment designed to take such factors into account might improve treatment outcomes and could be useful in the development of new therapies. In this study, the researchers develop and test a new, standardized tool for the assessment of chronic pain that, by examining many symptoms and signs, aims to distinguish between pain subtypes.\n\n#### What Did the Researchers Do and Find?\nOne hundred thirty patients with several types of peripheral neuropathic pain and 57 patients with non-neuropathic (axial) low back pain completed a structured interview of 16 questions and a standardized bedside examination of 23 tests. Patients were asked, for example, to choose words that described their pain from a list provided by the researchers and to grade the intensity of particular aspects of their pain from zero (no pain) to ten (the maximum imaginable pain). Bedside tests included measurements of responses to light touch, pinprick, and vibration\u2014chronic pain often alters responses to harmless stimuli. Using \u201chierarchical cluster analysis,\u201d the researchers identified six subgroups of patients with neuropathic pain and two subgroups of patients with non-neuropathic pain based on the patterns of symptoms and signs revealed by the interviews and physical tests. They then used \u201cclassification tree analysis\u201d to identify the six questions and ten physical tests that discriminated best between pain subtypes and combined these items into a tool for a Standardized Evaluation of Pain (StEP). Finally, the researchers asked whether StEP, which took 10\u201315 minutes, could identify patients with radicular back pain and discriminate them from those with axial back pain in an independent group of 137 patients with chronic low back pain. StEP, they report, accurately diagnosed these two conditions and was well accepted by the patients.\n\n#### What Do These Findings Mean?\nThese findings indicate that a standardized assessment of pain-related signs and symptoms can provide a simple, quick diagnostic procedure that distinguishes between radicular (neuropathic) and axial (non-neuropathic) low back pain. This distinction is crucial because these types of back pain are best treated in different ways. In addition, the findings suggest that it might be possible to identify additional pain subtypes using StEP. Because these subtypes may represent conditions in which different pain mechanisms are acting, classifying patients in this way might eventually enable physicians to tailor treatments for chronic pain to the specific needs of individual patients rather than, as at present, largely guessing which of the available treatments is likely to work best.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pmed.1000047"
    },
    "1387": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/ploscb/MasFH10",
        "transcript": "Modern societies are characterized by a large degree of pluralism in social, political and cultural opinions. In addition, there is evidence that humans tend to form distinct subgroups (clusters), characterized by opinion consensus within the clusters and differences between them. So far, however, formal theories of social influence have difficulty explaining this coexistence of global diversity and opinion clustering. This paper identifies a missing ingredient that helps to fill this gap: the striving for uniqueness. Besides being influenced by their social environment, individuals also show a desire to hold a unique opinion. Thus, when too many other members of the population hold a similar opinion, individuals tend to adopt an opinion that distinguishes them from others. This notion is rooted in classical sociological theory and is supported by recent empirical research. Authors develop a computational model of opinion dynamics in human populations and demonstrate that the new model can explain opinion clustering. Authors conduct simulation experiments to study the conditions of clustering. Based on our results, we discuss preconditions for the persistence of pluralistic societies in a globalizing world.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pcbi.1000959"
    },
    "1388": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1016/j.pestbp.2009.06.009",
        "transcript": "Genomic tools such as the availability of the Drosophila genome sequence, the relative ease of stable transformation, and DNA microarrays have made the fruit fly a powerful model in insecticide toxicology research. We have used transgenic promoter-GFP constructs to document the detailed pattern of induced Cyp6a2 gene expression in larval and adult Drosophila tissues. We also compared various insecticides and xenobiotics for their ability to induce this cytochrome P450 gene, and show that the pattern of Cyp6a2 inducibility is comparable to that of vertebrate CYP2B genes, and different from that of vertebrate CYP1A genes, suggesting a degree of evolutionary conservation for the \u201cphenobarbital-type\u201d induction mechanism. Our results are compared to the increasingly diverse reports on P450 induction that can be gleaned from whole genome or from \u201cdetox\u201d microarray experiments in Drosophila. These suggest that only a third of the genomic repertoire of CYP genes is inducible by xenobiotics, and that there are distinct subsets of inducers/induced genes, suggesting multiple xenobiotic transduction mechanisms. A relationship between induction and resistance is not supported by expression data from the literature. The relative abundance of expression data now available is in contrast to the paucity of studies on functional expression of P450 enzymes, and this remains a challenge for our understanding of the toxicokinetic aspects of insecticide action.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/j.pestbp.2009.06.009"
    },
    "1389": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000137",
        "transcript": "The Escherichia coli chemotaxis network is a model system for biological signal processing. In E. coli, transmembrane receptors responsible for signal transduction assemble into large clusters containing several thousand proteins. These sensory clusters have been observed at cell poles and future division sites. Despite extensive study, it remains unclear how chemotaxis clusters form, what controls cluster size and density, and how the cellular location of clusters is robustly maintained in growing and dividing cells. Here, we use photoactivated localization microscopy (PALM) to map the cellular locations of three proteins central to bacterial chemotaxis (the Tar receptor, CheY, and CheW) with a precision of 15 nm. We find that cluster sizes are approximately exponentially distributed, with no characteristic cluster size. One-third of Tar receptors are part of smaller lateral clusters and not of the large polar clusters. Analysis of the relative cellular locations of 1.1 million individual proteins (from 326 cells) suggests that clusters form via stochastic self-assembly. The super-resolution PALM maps of E. coli receptors support the notion that stochastic self-assembly can create and maintain approximately periodic structures in biological membranes, without direct cytoskeletal involvement or active transport.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000137"
    },
    "1390": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1056/nejmp068256",
        "transcript": "Published in the New England Journal of Medicine Murray's article can be seen as more of a case study and a high-level overview of her longer-form and much more detailed work in The Oncomouse that roared: Resistance and accommodation to patenting in academic science. Her article discusses the issuing of a patent that covers most lines of embryonic stem cells by James Thompson at the University of Michigan and the problems around licensing of stem-cells that followed and ultimately resulted in a successful challenge of the patent by a consumer watchdog organization.\n\nLike in her paper on the onco-mouse, Murray argues that there are two ideologies or major institutional models at conflict between open science and the mode of commercialization. In this article, however, Murray takes much more a perspective stance and argues that, \"it ought to be possible to create a stem-cell market that provides both rapid, unconditional access to the academic researchers and more circumscribed access to commercial scientists, along with higher prices and profit sharing.\"\n\nTheoretical and practical relevance: Murray's prescriptions seems to parallel the \"two economies\" model argued for by Lessig in a blog post and in his his book Remix: Making art and commerce thrive in the hybrid economy. Unlike Lessig who is geared more toward issues of culture and who is pursuing Creative Commons as the means toward this production, Murray is less clear about what a final arrangement might look like and is speaking toward a more scientific community.\n\nInteresting, Science Commons seems to have done little pursue the strategy that Murray suggests focusing much more strongly on a firm position of completely open science open to commercialization. This later option seems more likely to gain the benefits to commerce and the economy of open science detailed by Rosenberg and Brooks (for example).",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1056/nejmp068256"
    },
    "1391": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1126/science.1115813",
        "transcript": "Walsh, Cho, and Cohen offer a very short two page report in Science on a survey of scientists they ran that aimed to measure or detect an anticommons effect as a way of providing an empirical test of the theory suggested by Heller and Eisenberg in Can patents deter innovation? The anticommons in biomedical research. In their survey, they ask about material transfers, if they are refused, and why.\n\nThe authors survey 414 biomedical researchers in universities, governments, and nonprofits with a 40% response rate. The survey shows that authors have been instructed by their institutions to pay more attention to patents but that very few do. They conclude that, \"patents on knowledge inputs rarely impose a significant burden on biomedical research.\"\n\nThat said, they see reasonably frequent non-compliance with requests for shared material or knowledge. They probe this with two logistic regressions. Although they find that drugs and competitiveness are associated with reduced risks of sharing, they find no effect for patents. People who refuse most often tend to have a more commercial orientation, be more competitive, have a higher burden in term so the number of requests, and have published more.\n\nThey also discuss a case study of 93 academics that are working in a very patent-intensive sub-area and, again, find very little evidence for a negative effect of patents on the research.\n\nTheoretical and practical relevance: The paper has been cited more than 80 times in the last five years. It provides and important citation in research on the effects of patents on scientific innovation.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1126/science.1115813"
    },
    "1392": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000121",
        "transcript": "Recent evidence suggests that many malignancies, including breast cancer, are driven by a cellular subcomponent that displays stem cell-like properties. The protein phosphatase and tensin homolog (PTEN) is inactivated in a wide range of human cancers, an alteration that is associated with a poor prognosis. Because PTEN has been reported to play a role in the maintenance of embryonic and tissue-specific stem cells, we investigated the role of the PTEN/Akt pathway in the regulation of normal and malignant mammary stem/progenitor cell populations. We demonstrate that activation of this pathway, via PTEN knockdown, enriches for normal and malignant human mammary stem/progenitor cells in vitro and in vivo. Knockdown of PTEN in normal human mammary epithelial cells enriches for the stem/progenitor cell compartment, generating atypical hyperplastic lesions in humanized NOD/SCID mice. Akt-driven stem/progenitor cell enrichment is mediated by activation of the Wnt/\u00ce\u00b2-catenin pathway through the phosphorylation of GSK3-\u00ce\u00b2. In contrast to chemotherapy, the Akt inhibitor perifosine is able to target the tumorigenic cell population in breast tumor xenografts. These studies demonstrate an important role for the PTEN/PI3-K/Akt/\u00ce\u00b2-catenin pathway in the regulation of normal and malignant stem/progenitor cell populations and suggest that agents that inhibit this pathway are able to effectively target tumorigenic breast cancer cells.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000121"
    },
    "1393": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1038/341397a0",
        "transcript": "This paper reviews the known physical origins of hearing and equilibrium in vertebrates, focusing on the results of studies in the 1970s and 80s particularly on the role of hair bundles in converting sound into electrical potential in the nervous system. The contemporary understanding of structural details of the ear are summarized, including the structure of hair cells and mechanoreceptive hair bundles, transduction channels, adaptation to a range of frequencies, and the possibilities for direct mechanoelectrical transduction, driven directly by hair motion without secondary messengers.\n\nParticular attention is paid to mechanisms for transduction and frequency tuning, areas of active research and study at the time. Both positive and negative discoveries are covered, noting areas where further research is needed. Some new micrographs and figures from the author's work are included to tie the review together. Over 100 related papers are cited and synthesized into the review, most by other authors.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1038/341397a0"
    },
    "1394": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000135",
        "transcript": "During the development of neural circuitry, neurons of different kinds establish specific synaptic connections by selecting appropriate targets from large numbers of alternatives. The range of alternative targets is reduced by well organised patterns of growth, termination, and branching that deliver the terminals of appropriate pre- and postsynaptic partners to restricted volumes of the developing nervous system. We use the axons of embryonic Drosophila sensory neurons as a model system in which to study the way in which growing neurons are guided to terminate in specific volumes of the developing nervous system. The mediolateral positions of sensory arbors are controlled by the response of Robo receptors to a Slit gradient. Here we make a genetic analysis of factors regulating position in the dorso-ventral axis. We find that dorso-ventral layers of neuropile contain different levels and combinations of Semaphorins. We demonstrate the existence of a central to dorsal and central to ventral gradient of Sema 2a, perpendicular to the Slit gradient. We show that a combination of Plexin A (Plex A) and Plexin B (Plex B) receptors specifies the ventral projection of sensory neurons by responding to high concentrations of Semaphorin 1a (Sema 1a) and Semaphorin 2a (Sema 2a). Together our findings support the idea that axons are delivered to particular regions of the neuropile by their responses to systems of positional cues in each dimension.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000135"
    },
    "1395": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000147",
        "transcript": "For all animals, the taste sense is crucial to detect and avoid ingesting toxic molecules. Many toxins are synthesized by plants as a defense mechanism against insect predation. One example of such a natural toxic molecule is L-canavanine, a nonprotein amino acid found in the seeds of many legumes. Whether and how insects are informed that some plants contain L-canavanine remains to be elucidated. In insects, the taste sense relies on gustatory receptors forming the gustatory receptor (Gr) family. Gr proteins display highly divergent sequences, suggesting that they could cover the entire range of tastants. However, one cannot exclude the possibility of evolutionarily independent taste receptors. Here, we show that L-canavanine is not only toxic, but is also a repellent for Drosophila. Using a pharmacogenetic approach, we find that flies sense food containing this poison by the DmX receptor. DmXR is an insect orphan G-protein-coupled receptor that has partially diverged in its ligand binding pocket from the metabotropic glutamate receptor family. Blockade of DmXR function with an antagonist lowers the repulsive effect of L-canavanine. In addition, disruption of the DmXR encoding gene, called mangetout (mtt), suppresses the L-canavanine repellent effect. To avoid the ingestion of L-canavanine, DmXR expression is required in bitter-sensitive gustatory receptor neurons, where it triggers the premature retraction of the proboscis, thus leading to the end of food searching. These findings show that the DmX receptor, which does not belong to the Gr family, fulfills a gustatory function necessary to avoid eating a natural toxin.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000147"
    },
    "1396": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1111/j.1365-2656.2009.01567.x",
        "transcript": "In the Neotropics, most plants depend on animals for pollination. Solitary bees are the most important vectors, and among them members of the tribe Centridini depend on oil from flowers (mainly Malpighiaceae) to feed their larvae. This specialized relationship within 'the smallest of all worlds' (a whole pollination network) could result in a 'tiny world' different from the whole system. This 'tiny world' would have higher nestedness, shorter path lengths, lower modularity and higher resilience if compared with the whole pollination network. In the present study, we contrasted a network of oil-flowers and their visitors from a Brazilian steppe ('caatinga') to whole pollination networks from all over the world. A network approach was used to measure network structure and, finally, to test fragility. The oil-flower network studied was more nested (NODF = 0\u00b784, N = 0\u00b796) than all of the whole pollination networks studied. Average path lengths in the two-mode network were shorter (one node, both for bee and plant one-mode network projections) and modularity was lower (M = 0\u00b722 and four modules) than in all of the whole pollination networks. Extinctions had no or small effects on the network structure, with an average change in nestedness smaller than 2% in most of the cases studied; and only two species caused coextinctions. The higher the degree of the removed species, the stronger the effect and the higher the probability of a decrease in nestedness. We conclude that the oil-flower subweb is more cohesive and resilient than whole pollination networks. Therefore, the Malpighiaceae have a robust pollination service in the Neotropics. Our findings reinforce the hypothesis that each ecological service is in fact a mosaic of different subservices with a hierarchical structure ('webs within webs').\n\nTheoretical and practical relevance: This paper goes one step further in the hypothesis of mutualistic modules, evidencing that ecosystem services may be a mosaic of subservices with different properties. Furthermore, this finding has important implications for service-oriented conservation programs, as their planning should take into account this hierarchical structure.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1111/j.1365-2656.2009.01567.x"
    },
    "1397": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000131",
        "transcript": "Ongoing declines in production of the world's fisheries may have serious ecological and socioeconomic consequences. As a result, a number of international efforts have sought to improve management and prevent overexploitation, while helping to maintain biodiversity and a sustainable food supply. Although these initiatives have received broad acceptance, the extent to which corrective measures have been implemented and are effective remains largely unknown. We used a survey approach, validated with empirical data, and enquiries to over 13,000 fisheries experts (of which 1,188 responded) to assess the current effectiveness of fisheries management regimes worldwide; for each of those regimes, we also calculated the probable sustainability of reported catches to determine how management affects fisheries sustainability. Our survey shows that 7% of all coastal states undergo rigorous scientific assessment for the generation of management policies, 1.4% also have a participatory and transparent processes to convert scientific recommendations into policy, and 0.95% also provide for robust mechanisms to ensure the compliance with regulations; none is also free of the effects of excess fishing capacity, subsidies, or access to foreign fishing. A comparison of fisheries management attributes with the sustainability of reported fisheries catches indicated that the conversion of scientific advice into policy, through a participatory and transparent process, is at the core of achieving fisheries sustainability, regardless of other attributes of the fisheries. Our results illustrate the great vulnerability of the world's fisheries and the urgent need to meet well-identified guidelines for sustainable management; they also provide a baseline against which future changes can be quantified. Author Summary Top Global fisheries are in crisis: marine fisheries provide 15% of the animal protein consumed by humans, yet 80% of the world's fish stocks are either fully exploited, overexploited or have collapsed. Several international initiatives have sought to improve the management of marine fisheries, hoping to reduce the deleterious ecological and socioeconomic consequence of the crisis. Unfortunately, the extent to which countries are improving their management and whether such intervention ensures the sustainability of the fisheries remain unknown. Here, we surveyed 1,188 fisheries experts from every coastal country in the world for information about the effectiveness with which fisheries are being managed, and related those results to an index of the probable sustainability of reported catches. We show that the management of fisheries worldwide is lagging far behind international guidelines recommended to minimize the effects of overexploitation. Only a handful of countries have a robust scientific basis for management recommendations, and transparent and participatory processes to convert those recommendations into policy while also ensuring compliance with regulations. Our study also shows that the conversion of scientific advice into policy, through a participatory and transparent process, is at the core of achieving fisheries sustainability, regardless of other attributes of the fisheries. These results illustrate the benefits of participatory, transparent, and science-based management while highlighting the great vulnerability of the world's fisheries services. The data for each country can be viewed at http://as01.ucis.dal.ca/ramweb/surveys/fishery_assessment .",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000131"
    },
    "1398": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=MinerbiETAL:09",
        "transcript": "Synaptic plasticity is widely believed to constitute a key mechanism for modifying functional properties of neuronal networks. This belief implicitly implies, however, that synapses, when not driven to change their characteristics by physiologically relevant stimuli, will maintain these characteristics over time. How tenacious are synapses over behaviorally relevant time scales? To begin to address this question, we developed a system for continuously imaging the structural dynamics of individual synapses over many days, while recording network activity in the same preparations. We found that in spontaneously active networks, distributions of synaptic sizes were generally stable over days. Following individual synapses revealed, however, that the apparently static distributions were actually steady states of synapses exhibiting continual and extensive remodeling. In active networks, large synapses tended to grow smaller, whereas small synapses tended to grow larger, mainly during periods of particularly synchronous activity. Suppression of network activity only mildly affected the magnitude of synaptic remodeling, but dependence on synaptic size was lost, leading to the broadening of synaptic size distributions and increases in mean synaptic size. From the perspective of individual neurons, activity drove changes in the relative sizes of their excitatory inputs, but such changes continued, albeit at lower rates, even when network activity was blocked. Our findings show that activity strongly drives synaptic remodeling, but they also show that significant remodeling occurs spontaneously. Whereas such spontaneous remodeling provides an explanation for \"synaptic homeostasis\" like processes, it also raises significant questions concerning the reliability of individual synapses as sites for persistently modifying network function. Author Summary Top Neurons communicate via synapses, and it is believed that activity-dependent modifications to synaptic connections\u2014synaptic plasticity\u2014is a fundamental mechanism for stably altering the function of neuronal networks. This belief implies that synapses, when not driven to change their properties by physiologically relevant stimuli, should preserve their individual properties over time. Otherwise, physiologically relevant modifications to network function would be gradually lost or become inseparable from stochastically occurring changes in the network. So do synapses actually preserve their properties over behaviorally relevant time scales? To begin to address this question, we examined the structural dynamics of individual postsynaptic densities for several days, while recording and manipulating network activity levels in the same networks. We found that as expected in highly active networks, individual synapses undergo continual and extensive remodeling over time scales of many hours to days. However, we also observed, that synaptic remodeling continues at very significant rates even when network activity is completely blocked. Our findings thus indicate that the capacity of synapses to preserve their specific properties might be more limited than previously thought, raising intriguing questions about the long-term reliability of individual synapses.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1399": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/ploscb/Hogeweg11",
        "transcript": "From the late 1980s onward, the term \u201cbioinformatics\u201d mostly has been used to refer to computational methods for comparative analysis of genome data. However, the term was originally more widely defined as the study of informatic processes in biotic systems. In this essay, author traces this early history (from a personal point of view) and argues that the original meaning of the term is re-emerging.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pcbi.1002021"
    },
    "1400": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1038/nature10073",
        "transcript": "As hair bundles move, viscous friction between stereocilia and the surrounding liquid poses a physical challenge to the ear\u2019s high sensitivity and sharp frequency selectivity. This letter proposes that some of that energy is used for frequency-selective sound amplification, through fluid\u2013structure interaction between the liquid within the hair bundle and the stereocilia.\n\nA dynamic model is proposed to simulate hair bundles in a viscous environment, to see what large and small scale insights could be gained. Finite-element analysis, a submodel of hydrodynamic forces, stochastic simulation, and models of interferometric measurement all aimed to simulate both a hair bundle in natural conditions and what might be observed in an experiment involving it.\n\nForces between stereocilia are estimated, and the results suggest that the closeness of stereocilia reduces drag between them, supporting a sliding but not a squeezing mode. Tip links may couple mechanotransduction to this low-friction sliding mode, with motion between neighboring stereocilia of less than 1nm when the hair bundle moves the larger distance [O(10nm)]needed to stimulate its channels.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1038/nature10073"
    },
    "1401": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000134",
        "transcript": "The extent by which different cellular components generate phenotypic diversity is an ongoing debate in evolutionary biology that is yet to be addressed by quantitative comparative studies. We conducted an in vivo mass-spectrometry study of the phosphoproteomes of three yeast species (Saccharomyces cerevisiae, Candida albicans, and Schizosaccharomyces pombe) in order to quantify the evolutionary rate of change of phosphorylation. We estimate that kinase-substrate interactions change, at most, two orders of magnitude more slowly than transcription factor (TF)-promoter interactions. Our computational analysis linking kinases to putative substrates recapitulates known phosphoregulation events and provides putative evolutionary histories for the kinase regulation of protein complexes across 11 yeast species. To validate these trends, we used the E-MAP approach to analyze over 2,000 quantitative genetic interactions in S. cerevisiae and Sc. pombe, which demonstrated that protein kinases, and to a greater extent TFs, show lower than average conservation of genetic interactions. We propose therefore that protein kinases are an important source of phenotypic diversity.\n\nNatural selection at a population level requires phenotypic diversity, which at the molecular level arises by mutation of the genome of each individual. What kinds of changes at the level of the DNA are most important for the generation of phenotypic differences remains a fundamental question in evolutionary biology. One well-studied source of phenotypic diversity is mutation in gene regulatory regions that results in changes in gene expression, but what proportion of phenotypic diversity is due to such mutations is not entirely clear. We investigated the relative contribution to phenotypic diversity of mutations in protein-coding regions compared to mutations in gene regulatory sequences. Given the important regulatory role played by phosphorylation across biological systems, we focused on mutations in protein-coding regions that alter protein-protein interactions involved in the binding of kinases to their substrate proteins. We studied the evolution of this \"phosphoregulation\" by analyzing the in vivo complement of phosphorylated proteins (the \"phosphoproteome\") in three highly diverged yeast species\u2014the budding yeast Saccharomyces cerevisiae, the pathogenic yeast Candida albicans, and the fission yeast Schizosaccharomyces pombe\u2014and integrating those data with existing data on thousands of known genetic interactions from S. cerevisiae and Sc. pombe. We show that kinase-substrate interactions are altered at a rate that is at most two orders of magnitude slower than the alteration of transcription factor (TF)-promoter interactions, whereas TFs and kinases both show a faster than average rate of functional divergence estimated by the cross-species analysis of genetic interactions. Our data provide a quantitative estimate of the relative frequencies of different kinds of functionally relevant mutations and demonstrate that, like mutations in gene regulatory regions, mutations that result in changes in kinase-substrate interactions are an important source of phenotypic diversity.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000134"
    },
    "1402": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000138",
        "transcript": "The regulation of filopodia plays a crucial role during neuronal development and synaptogenesis. Axonal filopodia, which are known to originate presynaptic specializations, are regulated in response to neurotrophic factors. The structural components of filopodia are actin filaments, whose dynamics and organization are controlled by ensembles of actin-binding proteins. How neurotrophic factors regulate these latter proteins remains, however, poorly defined. Here, using a combination of mouse genetic, biochemical, and cell biological assays, we show that genetic removal of Eps8, an actin-binding and regulatory protein enriched in the growth cones and developing processes of neurons, significantly augments the number and density of vasodilator-stimulated phosphoprotein (VASP)-dependent axonal filopodia. The reintroduction of Eps8 wild type (WT), but not an Eps8 capping-defective mutant, into primary hippocampal neurons restored axonal filopodia to WT levels. We further show that the actin barbed-end capping activity of Eps8 is inhibited by brain-derived neurotrophic factor (BDNF) treatment through MAPK-dependent phosphorylation of Eps8 residues S624 and T628. Additionally, an Eps8 mutant, impaired in the MAPK target sites (S624A/T628A), displays increased association to actin-rich structures, is resistant to BDNF-mediated release from microfilaments, and inhibits BDNF-induced filopodia. The opposite is observed for a phosphomimetic Eps8 (S624E/T628E) mutant. Thus, collectively, our data identify Eps8 as a critical capping protein in the regulation of axonal filopodia and delineate a molecular pathway by which BDNF, through MAPK-dependent phosphorylation of Eps8, stimulates axonal filopodia formation, a process with crucial impacts on neuronal development and synapse formation.\n\nNeurons communicate with each other via specialized cell-cell junctions called synapses. The proper formation of synapses (\"synaptogenesis\") is crucial to the development of the nervous system, but the molecular pathways that regulate this process are not fully understood. External cues, such as brain-derived neurotrophic factor (BDNF), trigger synaptogenesis by promoting the formation of axonal filopodia, thin extensions projecting outward from a growing axon. Filopodia are formed by elongation of actin filaments, a process that is regulated by a complex set of actin-binding proteins. Here, we reveal a novel molecular circuit underlying BDNF-stimulated filopodia formation through the regulated inhibition of actin-capping factor activity. We show that the actin-capping protein Eps8 down-regulates axonal filopodia formation in neurons in the absence of neurotrophic factors. In contrast, in the presence of BDNF, the kinase MAPK becomes activated and phosphorylates Eps8, leading to inhibition of its actin-capping function and stimulation of filopodia formation. Our study, therefore, identifies actin-capping factor inhibition as a critical step in axonal filopodia formation and likely in new synapse formation.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000138"
    },
    "1403": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000020",
        "transcript": "Following one of the basic principles in evolutionary biology that complex life forms derive from more primitive ancestors, it has long been believed that the higher animals, the Bilateria, arose from simpler (diploblastic) organisms such as the cnidarians (corals, polyps, and jellyfishes). A large number of studies, using different datasets and different methods, have tried to determine the most ancestral animal group as well as the ancestor of the higher animals. Here, we use \u201ctotal evidence\u201d analysis, which incorporates all available data (including morphology, genome, and gene expression data) and come to a surprising conclusion. The Bilateria and Cnidaria (together with the other diploblastic animals) are in fact sister groups: that is, they evolved in parallel from a very simple common ancestor. We conclude that the higher animals (Bilateria) and lower animals (diploblasts), probably separated very early, at the very beginning of metazoan animal evolution and independently evolved their complex body plans, including body axes, nervous system, sensory organs, and other characteristics. The striking similarities in several complex characters (such as the eyes) resulted from both lineages using the same basic genetic tool kit, which was already present in the common ancestor. The study identifies Placozoa as the most basal diploblast group and thus a living fossil genome that nicely demonstrates, not only that complex genetic tool kits arise before morphological complexity, but also that these kits may form similar morphological structures in parallel.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000020"
    },
    "1404": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000130",
        "transcript": "Aquaporins are transmembrane proteins that facilitate the flow of water through cellular membranes. An unusual characteristic of yeast aquaporins is that they frequently contain an extended N terminus of unknown function. Here we present the X-ray structure of the yeast aquaporin Aqy1 from Pichia pastoris at 1.15 resolution. Our crystal structure reveals that the water channel is closed by the N terminus, which arranges as a tightly wound helical bundle, with Tyr31 forming H-bond interactions to a water molecule within the pore and thereby occluding the channel entrance. Nevertheless, functional assays show that Aqy1 has appreciable water transport activity that aids survival during rapid freezing of P. pastoris. These findings establish that Aqy1 is a gated water channel. Mutational studies in combination with molecular dynamics simulations imply that gating may be regulated by a combination of phosphorylation and mechanosensitivity.\n\nAll living organisms must regulate precisely the flow of water into and out of cells in order to maintain cell shape and integrity. Proteins of one family, the aquaporins, are found in virtually every living organism and play a major role in maintaining water homeostasis by acting as regulated water channels. Here we describe the first crystal structure of a yeast aquaporin, Aqy1, at 1.15 resolution, which represents the highest resolution structural data obtained to date for a membrane protein. Using this structural information, we address an outstanding biological question surrounding yeast aquaporins: what is the functional role of the amino-terminal extension that is characteristic of yeast aquaporins? Our structural data show that the amino terminus of Aqy1 fulfills a novel gate-like function by folding to form a cytoplasmic helical bundle with a tyrosine residue entering the water channel and occluding the cytoplasmic entrance. Molecular dynamics simulations and functional studies in combination with site-directed mutagenesis suggest that water flow is regulated through a combination of mechanosensitive gating and post-translational modifications such as phosphorylation. Our study therefore provides insight into a unique mechanism for the regulation of water flux in yeast.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000130"
    },
    "1405": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1126/science.280.5364.698",
        "transcript": "Published in Science in 1998, Heller and Eisenberg frame their argument explicitly in terms of Hardin's classic piece of The tragedy of the commons and applied to biomedical research although it has been used and cited as relevant more broadly. They argue that just as too much open access to an expendable public resource can create a tragedy of the commons, too much ownership -- especially an intellectual domain -- can create thickets that limit the progress of science more broadly. They argue that, \"privatization can solve one tragedy but cause another.\"\n\nHeller and Eisenberg are reacting, in large part, to the growth of patenting within in biomedical science (see Murray (2006) for more detail on case study of this in the area of mouse-research). Their core argument is that the anticommons emerges when the rights necessary to practice research are split up among a large number, and a large variety, of different researchers. This essentially introduces a set of complex collective action problems beyond those introduced by patent licensing which they suggest may create an important barrier to scientific progress. They explain quite clearly that, \"the tragedy of the anticommons refers to the more complex obstacles that arise when a user needs access to multiple patented inputs to create a single useful output.\"\n\nThey use examples of patents on concurrent fragments which they suggest may be creating thickets and reach-through licensing agreements to make this point. They end by describing why different types of organizations (i.e., non-profits and for-profits) create heterogeneous interests among rights holders, transaction costs around bundling, and cognitive biases where scientists think too highly off their own work might prevent institutional solutions to the anticommons that might reduce costs (e.g., ASCAP in the area the copyright).\n\nTheoretical and practical relevance: Heller and Eisenberg's article has been cited more than 1,300 times in the last 12 years and has become a major article in the literature critical of patents in science. The metaphor of the anticommons has become a frequently cited in the areas of open innovation, arguments in favor of open science, and critiques of the patent system more generally.\n\nThat said, the article seems to be somewhat missued by a number of \"downstream\" academics citing it. The article is often treated as argument against particular patents. In fact, it's argument is carefully crouched in terms of the problems of patents in aggregate. In that sense, Murray and Stern's article econometric article testing the hypothesis is a somewhat rough match for the theory offered. The article was also tested by Walsh et al. (2005) who found no evidence of an anticommons effect.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1126/science.280.5364.698"
    },
    "1406": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.1000132",
        "transcript": "We used allometric scaling to explain why the regular replacement of the primary flight feathers requires disproportionately more time for large birds. Primary growth rate scales to mass (M) as M0.171, whereas the summed length of the primaries scales almost twice as fast (M0.316). The ratio of length (mm) to rate (mm/day), which would be the time needed to replace all the primaries one by one, increases as the 0.14 power of mass (M0.316/M0.171 = M0.145), illustrating why the time required to replace the primaries is so important to life history evolution in large birds. Smaller birds generally replace all their flight feathers annually, but larger birds that fly while renewing their primaries often extend the primary molt over two or more years. Most flying birds exhibit one of three fundamentally different modes of primary replacement, and the size distributions of birds associated with these replacement modes suggest that birds that replace their primaries in a single wave of molt cannot approach the size of the largest flying birds without first transitioning to a more complex mode of primary replacement. Finally, we propose two models that could account for the 1/6 power allometry between feather growth rate and body mass, both based on a length-to-surface relationship that transforms the linear, cylindrical growing region responsible for producing feather tissue into an essentially two-dimensional structure. These allometric relationships offer a general explanation for flight feather replacement requiring disproportionately more time for large birds.\n\nThe pace of life varies with body size and is generally slower among larger organisms. Larger size creates opportunities but also establishes constraints on time-dependent processes. Flying birds depend on large wing feathers that deteriorate over time and must be replaced through molting. The lengths of flight feathers increase as the 1/3 power of body mass, as one expects for a length-to-volume ratio. However, feather growth rate increases as only the 1/6 power of body mass, possibly because a two-dimensional feather is produced by a one-dimensional growing region. The longer time required to grow a longer feather constrains the way in which birds molt, because partially grown feathers reduce flight efficiency. Small birds quickly replace their flight feathers, often growing several feathers at a time in each wing. Larger species either prolong molt over two or more years, adopt complex patterns of multiple feather replacement to minimize gaps in the flight surface, or, among species that do not rely on flight for feeding, simultaneously molt all their flight feathers. We speculate that the extinct 70-kg raptor, Argentavis magnificens, must have undergone such a simultaneous molt, living off fat reserves for the duration.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.1000132"
    },
    "1407": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/siamcomp/Opatrny79",
        "transcript": "Proves the NP-completeness of the total ordering problem: given finite sets $S$ and $R$, where $R$ is a subset of $S \\times S  \\times  S$, does there exist a total ordering of the elements of S such that for all (x, y, z) in R, either $x < y < z$ or $z < y < x$? The reduction is from the hypergraph 2-colorability problem with edges of size at most 3.\n\nThis problem is in \"Computers and Intractibility\" by Garey and Johnson as problem MS1, the betweenness problem \\cite{garey1979computers}.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1137/0208008"
    },
    "1408": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/nle/SporlederL08",
        "transcript": "This paper takes up the question of whether rhetorical relations can be automatically derived and classified. It focuses, in particular, on discourse markers. These may be ambigious (e.g 'since', 'yet' have multiple uses and are sometimes, but not always, discourse markers); and these discourse markers may also be missing altogether.\n\nThe authors comment that: \"what is needed is a model which can classify rhetorical relations in the absence of an explicit discourse marker.\" (p4). Previous work (e.g. Marcu & Echihabi 2002) has suggested creating training data for a classifier by labelling examples which contain an unambiguous lexically marked rhetorical relation, then removing the markers. The main purpose of this paper is to empirically test this.\n\nIt also provides an interesting theoretical observation: Two conditions are needed for training on marked examples to work well:\n\n\"First, there has to be a certain amount of redundancy between the discourse marker and the general linguistic context, i.e. removing the discourse marker should still leave enough residual information for the classifier to learn how to distinguish different relations.\"\n\nSecond, similarity between marked and unmarked examples is needed so that a classifier can make generalizations.\n\nThe paper suggests that texts with lexically marked and lexically unmarked rhetorical relations may be inherently different, in so far as removing discourse markers may change the meaning of a sentence, and classifiers built based on removing markers from classified sentences work little better than chance.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1017/S1351324906004451"
    },
    "1409": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=Horrocks08cacm",
        "transcript": "The simplicity of the Web has drawbacks, including the difficulty of integrating information from multiple sources.\n\nMashups may combine information from multiple sources in a custom integration, and user communities may collaborate to annotate images and videos. However it is desirable to integrate information by machine: this is the goal of the semantic web.\n\n\"A major difficulty in realizing this goal is that most Web content is primarily intended for presentation to and consumption by human users; HTML markup is primarily concerned with layout, size, color, and other presentation issues\"(59). Furthermore, \"This vision of a semantic Web is extremely ambitious and would require solving many long-standing research problems in knowledge representation and reasoning, databases, computational linguistics, computer vision, and agent systems\"(59).\n\nThe paper discusses the use RDF and RDFS and OWL, (using some examples from the world of the Harry Potter stories). A brief description of ontologies and of the context of Description Logics are also given. It contains a discussion of reasoning systems as well as a\n\nTheoretical and practical relevance: Compares databases and OWL ontologies and gives a current computer science perspective on the semantic web. Lists some ontology applications such as specific reasoning systems and ontologies.",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1410": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/cacm/HalpernP11",
        "transcript": "An opinion piece in the ACM Communications by two CS professors describing the current situation in CS publishing: unlike other academic disciplines that emphasize publishing in peer-reviewed journals, CS as a discipline emphasizes publication at conferences.\n\nThey theorize this is due to a number of factors:\n\n* Conferences give faster review and publication turnarounds than journals (implied but not stated: CS is a rapidly moving field where this is particularly vital).\n* Publicity. \"The best way to get your sub-discipline to know about your results is to publish them in the leading conference for that subdiscipline.\"\n\nAnd it is problematic for a few reasons:\n\n* Conference papers are usually limited to be shorter than journal ones, meaning that it's harder to explain results in reproducible detail.\n* Conference papers are often not reviewed as thoroughly as journal ones.\n* CS as a discipline has splintered into so many subfields and their corresponding conferences that presenting at a conference doesn't actually disseminate work to everyone who should see it.\n\nThe authors go on to suggest that the CS community shift their focus to journal publication for more thoughtful certification of quality work, and give a number of things that could support such a shift.\n\n* Use centralized web archives to store papers publicly online\n* Speed up journal review cycles\n* Make everyone who submits a paper \"pay\" to have that paper reviewed by reviewing papers themselves\n* Allow multiple certifications per paper -- that is, make it ok for a paper to get reviewed and approved by two or more publications.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/1978542.1978555"
    },
    "1411": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/fc/AndroulakiB10",
        "transcript": "Authors provide a privacy-preserving targeted ad system (PPOAd) via a User Ad Proxy which facilitates the anonymous expression of ad preferences and uses a blacklistable unlinkable credential system for registration credentials and an accountable ecash scheme for ad clicks. User information is only revealed if user clicks on an ad too many times, or attempts to double-spend an ad click allotment.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-642-14992-4_12"
    },
    "1412": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/air/BentaharMB10",
        "transcript": "This paper proposes a taxonomy of argumentation models, distinguishing three main types of models, and comparing models in each of these categories:\n\n1. monological models - micro structure\n2. dialogical models - macro structure\n3. rhetorical models - audience's perception\n\n### Monological models\nMonological models view arguments as a tentative proof, and focus on the internal structure of the chain of inference rules relating premises to conclusions\n\n### Dialogical models\nDialogical models emphasize the relationship between arguments. An argument can be seen as a dialogue game, where parties defend their viewpoint. In this view, argumentation is `defeasible' reasoning.\n\n### Rhetorical models\nRhetorical models study how arguments are used as a means of persuasion; they consider the audience's perception, and may relate to evaluative judgements (rather than truth).\n\n### Distinguishing between the models\nMonological models are generally about the interal structure; Dialogical models are generally about the external structure. Rhetorical models are external to the argument, considering the communication aspects;\n\n### Joint models\nBoth rhetorical and dialogical (Bench-Capon 2003; Bentahar et al. 2007b)\nBoth monological and dialogical (Bench-Capon 1989; Farley and Freeman 1995; and Atkinson et al. model 2006)\nFigure 1 summarizes the taxonomy, indicating the structure, foundation, and linkage of each type of model. The paper also presents an extensive description of various models, explaining the advantages and limits of each argumentation scheme considered.\n\n#### Theoretical and practical relevance:\n\nArgumentation is an everyday human activity and computational argumentation is also widespread; this paper works towards developing a \"global view of existing argumentation models and methods\".\n\nThis is a seminal paper in argumentation which references and describes a large body of work, making sense of it with the taxonomy described.\n\nThe three types of models complement each other and should be combined.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/s10462-010-9154-1"
    },
    "1413": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/jurix/PalauM07",
        "transcript": "This builds on the work of Automatic detection of arguments in legal texts; whereas that paper used argumentative texts from multiple domains (including newspapers and social media, despite the title), this work is restricted to the legal domain. Besides detecting argumentative and non-argumentative sentences, premises and conclusions are also detected. Additional features are added to analyze the importance of relations between sentences.\n\n#### Procedure\n29 admissibility reports and 25 legal cases randomly selected by European Court of Human Rights August 2006 & December 2006. These contain facts, complaints, the law, and final conclusions from judges, expressed in long and complex sentences.\n\nThese were manually analyzed by two lawyers to indicate whether they contained arguments. There were 12,904 sentences (10,133 non-argumentative and 2,771 argumentative), which included 2,355 premises and 416 conclusions.\n\nAverage accuracy of the maximum entropy model is 82%, using only the information from the current analyzed sentence. (Previous experiments used a naive Bayes model; the increased amount of information in this case meant they could not satisfy the independence assumptions of the naive Bayes classifier). They also experimented with using information in adjacent sentences.\n\nIn future work they plan to look at the clause level, instead of the sentence level.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.booksonline.iospress.nl/Content/View.aspx?piid=7846"
    },
    "1414": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/acl/WeiG10",
        "transcript": "This paper points out that product reviews contain domain-specific knowledge. To capture the hierarchical relationships between product attributes, they introduce a new approach: \"hierarchical learning with sentiment ontology tree\" (HL-SOT) in order to:\n\n1. identify attributes\n2. identify which attributes have sentiment attached to them\n\nThis would enable searching for particular attributes in reviews.\n\nTheir algorithm is based on H-RLS from Incremental algorithms for hierarchical classification. Evaluations are conducted against a human-labeled data set.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aclweb.org/anthology/P10-1042"
    },
    "1415": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/mpc/BackhouseF08",
        "transcript": "This paper shows the derivation of an algorithm that enables the positive rationals to be enumerated in two different ways. One way is known, and is called Calkin-Wilf-Newman enumeration; the second is new and corresponds to a flattening of the Stern-Brocot tree of rationals. We show that both enumerations stem from the same simple algorithm. In this way, we construct a Stern-Brocot enumeration algorithm with the same time and space complexity as Calkin-Wilf-Newman enumeration.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1007/978-3-540-70594-9_6"
    },
    "1416": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/annals/HaighPR14",
        "transcript": "The first in a three-part series in IEEE Annals, this article gives a historical explanation of the endemic confusion surrounding the stored-program concept. The authors suggest the adoption of more precisely defined alternatives to capture specific aspects of the new approach to computing associated with the 1945 work of von Neumann and his collaborators. The second article, \"Engineering--The Miracle of the ENIAC: Implementing the Modern Code Paradigm,\"' examines the conversion of ENIAC to use the modern code paradigm identified in this article. The third, \"Los Alamos Bets on ENIAC: Nuclear Monte Carlo Simulations, 1947-1948,\"' examines in detail the first program written in the new paradigm to be executed.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/MAHC.2013.56"
    },
    "1417": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/aaai/YoungMAOGG11",
        "transcript": "This paper presents a microtext corpus derived from hostage negotiation transcripts. This source was chosen for its availability and its density of persuasion: Traditional microtext sources (Twitter, SMS, chat rooms) showed \"limited occurrences of directly persuasive attempts\". Even the negotiation transcripts showed fewer than 12% persuasive utterances.\n\nThey definie persuasion as \"the ability of one party to convince another party to act or believe in some desired way\". Cialdini's persuasion model was used, focusing on:\n\n1. Reciprocity\n2. Commitment and Consistency\n3. Scarcity\n4. Liking\n5. Authority\n6. Social Proof",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aaai.org/ocs/index.php/WS/AAAIW11/paper/view/3896"
    },
    "1418": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/chi/ViegasWD04",
        "transcript": "Fernanda Viegas, Marin Wattenberg, and Kushal Dave describe a visualization system they have built called history flow that they use to visualize changes made to Wikipedia articles. The authors suggest that their papers makes three distinct contributions:\n\n* History flow itself which is able to reveal editing patterns in Wikipedia and provide context for editors.\n* Several examples of collaboration patterns that become visible using the visualization tool and contribute to the literature on Wikipedia.\n* Implications of these patterns for design and governance of online social spaces.\n\nThe paper is largely an examination of Wikipedia and the early parts of the paper give background into the sites. It uses shortcomings in the design of the Wikipedia to motivate the history flow visualization which essentially depicts articles, over time, with colors representing authors who contributed text in question. Examples can be seen online at the IBM History Flow website. The interface is particularly good at representing major deletions and insertions.\n\nThe authors use a lightweight statistically analysis to reveal patterns of editing on Wikipedia (which at the time, were not widely studied). In particular, they show vandalism including mass-deletion, the creation of phony redirects, and addition of idiosyncratic copy and show that it rarely stays on the site for more than few minutes before being removed.\n\nThey also show a zig-zag patten that represents negotiation of content, often in the form of edit wars. They also attempt to provide some basic data on the stability of Wikipedia and the growth of articles on average. They suggest something that is now taken for granted by researchers of wikis: that studying Wikipedia may have important implications for other types of work.\n\n#### Theoretical and practical relevance:\n\nThe paper is important more for its path-breaking work on Wikipedia -- now with its track at CHI, than for the history flow visualization which has not, for the most part, been widely deployed outside Wikipedia but which seems to hold promise in a variety of other contexts. The paper has been cited more than 400 times, mostly in the academic literature on Wikipedia.\n\nThis paper is a finalist for the Wikimedia France Research Award.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/985692.985765"
    },
    "1419": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/software/Jorgensen14",
        "transcript": "Cost and effort overruns tend to be about 30% and haven't changed much from 1980s to now. Estimation methods haven't changed, expert estimation still dominates. But we know more; author notes 7 lessons supported by research:\n\n1. There Is No \u201cBest\u201d Effort Estimation Model or Method (important variable depends on context, also explains overfitting of advanced statistical estimation methods)\n2. Clients\u2019 Focus on Low Price Is a Major Reason for Effort Overruns\n3. Minimum and Maximum Effort Intervals Are Too Narrow (estimates do not adequately reflect uncertainty)\n4. It\u2019s Easy to Mislead Estimation Work and Hard to Recover from Being Misled (strongest when estimators aware of constraints such as budget, resulting in \"estimation anchor\" bias even if unintentional)\n5. Relevant Historical Data and Checklists Improve Estimation Accuracy\n6. Combining Independent Estimates Improves Estimation Accuracy (groupthink leading to more risk not found in software estimation research)\n7. Estimates Can Be Harmful (too low: low quality, too high: work expands; consider whether estimate is really needed)\n\n3 estimation challenges research has no solution for:\n\n* How to Accurately Estimate the Effort of Mega - large, Complicated Software Projects (less relevant experience and data available, cf #5 above; large projects also involve complex interactions with stakeholders and organizational changes)\n* How to Measure Software Size and Complexity for Accurate Estimation\n* How to Measure and Predict Productivity (large difference among developers and team only discernible through trial; we don't even know if there are economies or diseconomies to scale for software production!)\n\nPractices likely to improve estimation, quote:\n\n* Develop and use simple estimation models tailored to local contexts in combination with expert estimation.\n* Use historical estimation error to set minimum - maximum effort intervals.\n* Avoid exposure to misleading and irrelevant estimation information.\n* Use checklists tailored to own organization.\n* Use structured, group - based estimation processes where independence of estimates are assured.\n* Avoid early estimates based on highly incomplete information.\n",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.ieeecomputersociety.org/10.1109/MS.2014.49"
    },
    "1420": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/chi/Thom-SantelliCG09",
        "transcript": "Thom-Santelli et al. present a qualitative study of 15 authors use of the {{maintained}} template. The authors searched though a full Wikipedia dump to find the approximate 1,100 pages that used the template on article talk pages to explain to other users that the article is maintained. They the contacted a subsample of 15 editors (5 women and 10 men) and engaged them in approximate 1 hours unstructured interviews to help understand their use of the template and the degree of \"territoriality\" (if any) that the authors felt over the articles in questions.\n\nTheir basic finding is that there is indeed territoriality on Wikipedia which the authors attempt to connect to a sense of ownership. They argue that this territoriality can be valuable (and retaining expertise), but suggest that it might also have the negative effect of deterring new member participation.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/1518701.1518925"
    },
    "1421": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/chi/KitturCS08",
        "transcript": "This paper gives advice for using micro-task markets for user studies, to get quick (and yet reliable) feedback from users. The way a task is defined makes a significant difference in the results, and good design can reduce the number of users \"gaming the system\". They conclude that micro-task markets may be useful for user studies that combine objective and subjective information gathering, and provide specific advice (below).\n\nThis paper defines a \"micro-task market\" -- where short tasks (which take minutes or seconds) are entered into a shared system where users select them and complete them for some reward (generally money or reputation). The advantages of micro-task markets for user studies are that they are global and diverse, with very quick turnaround times (responses within 24-48 hours) at inexpensive rates (e.g. 5 cents per rating). The disadvantages are the lack of demographic inforation, lack of verifiable credentials, and limited experimenter contact.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/1357054.1357127"
    },
    "1422": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/ws/Rahwan08",
        "transcript": "The goal of this paper is to apply this background, particularly to \"identify fallacies made by dialogue participants\" on the WWW. The Semantic Web is seen as a way to do this.\n\nThis paper begins by presenting a definition of argumentation, reviewing Walton's and Toulmin's analyses of arguments, and showing basic argument diagrams. It also discusses the importance, for computer modelling, of classifications of arguments, citing Walton's 1996 book, Argumentation Schemes for Presumptive Reasoning, as the most influential of these. Then, it presents the notion of critical questions and criteria for argument acceptability, which form part of the core basis for this work. Finally, dialogue games are mentioned in passing.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/j.websem.2007.11.007"
    },
    "1423": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1037/e660812010-001",
        "transcript": "The core question of this Master's thesis, as the author puts it, is: \u201cCan we learn to identify persuasion as characterized by Cialdini\u2019s model using traditional machine learning techniques?\u201d The authors give a qualified \"yes\"; improvement is needed for real-world results, but the methods function. The corpus used was developed in his colleague's Master's thesis, Persuasion detection in conversation.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1037/e660812010-001"
    },
    "1424": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/naacl/RitterCD10",
        "transcript": "This paper models dialog acts in Twitter conversations and presents a corpus of 1.3 million conversations. They provide a status diagram showing the likelihood of transitions between dialogue acts.\n\n![](http://i.imgur.com/eTTVcXO.png)\n\n### Methodology\nUnsupervised LDA modelling of Twitter conversations, evaluated by held-out test conversations. Uses a conversation+topic model (segmenting post words into those that involve the topic of conversation, the dialogue act, or something else). Trained on 10,000 randomly sampled conversations (conversation length 3-6) from the corpus.\n\n### Corpus\n1.3 million conversations with each conversation containing between 2 and 243 posts. In summer 2009, they selected a random sample of Twitter users by gathering 20 randomly selected posts per minute, then queried to get all their posts. Followed any replies to collect conversations. Removed non-English conversations and non-reply posts.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aclweb.org/anthology/N10-1020"
    },
    "1425": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/icwsm/OConnorBRS10",
        "transcript": "This study finds that poll data on consumer confidence and presidential job approvals can be approximated with straightforward sentiment analysis of Twitter data.\n\nThe idea of aggregate sentiment is particularly interesting--where the errors are treated as noise which is expected to cancel itself out in aggregate. They point to Hopkins and King (2010) to show that standard text analysis techniques are inappropriate for assessing aggregate populations. Further, they provide some evidence from their own experiment: they mention that filtering out \"will\" (which is treated as positive sentiment despite being a verb sense, since they don't do POS tagging). However, they mention one caution: errors could potentially correlate with information of interest, such as if certain demographic groups might tweet in ways that are harder to analyze.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://www.aaai.org/ocs/index.php/ICWSM/ICWSM10/paper/view/1536"
    },
    "1426": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/chi/HillHWM92",
        "transcript": "Hill et al. introduce the notion of computation wear in this paper from the awareness literature in computer-supported cooperative work. The authors present an example of a project which shows wear in terms of both editing and reading in a modified version of an Emacs-like text editor and suggest that the concept may be broadly relevant in a variety of other other contexts and show designs for menus and others systems that display ware.\n\nThe authors maps wear of documents into scrollbars of the Zmacs text editor (essential an Emacs clone for the Symbolics lisp machine) with what they call attribution mapped scrollbars. These scrollbars essentially emphasize different parts of the documents with a sort of histogram of edits based on how often that particular portion of the document has been edited (in the edit wear example) or read (in the read wear example). This provides an easy mode of showing \"hot spots\" in ways that parallel the way one can find the dog-eared or yellowed pages in a book or the stained recipe card very easily which correspond to physical, rather than computation, wear.\n\nTheoretically, the paper frames the design in terms of Schoen's concepts of reflective work and argues that wear provides a means of assisting in problem setting by providing attention to core areas that have received previous and attention. Before concluding, the authors generalize their suggestion from the specific example of edit and read wear to menus and spreadsheet and suggest that wear is a generally useful concept in a variety of cooperative environments.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/142750.142751"
    },
    "1427": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/hopl/HudakHJW07",
        "transcript": "Summary:\n\nAuthors describe context of Haskell's creation (many lazy purely functional research languages, desire for common language in genre), key branching factors (e.g., decision of Miranda developers to not allow their language to be base of common language; adoption of still new features of typeclasses and monads), and a number of the design decisions made, and tools, implementations, and applications now available.\n\nTheoretical and practical relevance:\n\nHaskell seems to have had inauspicious beginnings for a widely used general purpose programming language -- design by committee of academics, but through some combination of purity (authors argue decision to be lazy made it easier to stay purely functional), openness (of the design process, specification, libraries, and language implementations), and luck, the language seems to have remained interesting for researchers and become practical for industry, and has also influenced feature development in many other languages.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://doi.acm.org/10.1145/1238844.1238856"
    },
    "1428": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MathieuZZRSL16",
        "transcript": "The authors presented a new generative model that learns to disentangle the factors of variations of the data. The authors claim that the proposed model is pretty robust to supervision. This is achieved by combining two of the most successful generative models: VAE and GAN. The model is able to resolve the analogies in a consistent way on several datasets with minimal parameter/architecture tunning.\n\nThis paper presents a way to learn latent codes for data, that captures both the information relevant for a given classification task, as well as the remaining irrelevant factors of variation (rather than discarding the latter as a classification model would). This is done by combining a VAE-style generative model, and adversarial training. This model proves capable of disentangling style and content in images (without explicit supervision for style information), and proves useful for analogy resolution.\n\nThis paper introduces a generative model for learning to disentangle hidden factors of variation. The disentangling separates the code into two, where one is claimed to be the code that descries factors relevant to solving a specific task, and the other describing the remaining factors. Experimental results show that the proposed method is promising.\n\nThe authors combine state of the art methods VAE and GAN to generate images with two complementary codes: one relevant and one irrelevant. They major contribution of the paper is the development of a training procedure that exploits triplets of images (two sharing the relevant code, one note sharing) to regularize the encoder-decoder architecture and avoid trivial solutions. The results are qualitatively good and comparable to previous article using more sources of supervision.\n\nPaper seeks to explore the variations amongst samples which separate multiple classes using auto encoders and decoders. Specifically, the authors propose combining generative adversarial networks and variational auto encoders. The idea mimics the game play between two opponents, where one attempts to fool the other into believing a synthetic sample is in fact a natural sample. The paper proposes an iterative training procedure where a generative model was first trained on a number of samples while keeping the weights of the adversary constant and later the adversary is trained while keeping the generative model weights constant. The paper performs experiments on generation of instances between classes, retrieval of instances belonging to a given class, and interpolation of instances between two classes. The experiments were performed on MNIST, a set of 2D character animation sprites, and 2D NORB toy image dataset.\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6051-disentangling-factors-of-variation-in-deep-representation-using-adversarial-training"
    },
    "1429": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/SalimansGZCRC16",
        "transcript": "The Authors provide a bag of tricks for training GAN's in the image domain. Using these, they achieve very strong semi-supervised results on SHVN, MNIST, and CIFAR.\n\nThe authors then train the improved model on several images datasets, evaluate it on different tasks: semi-supervised learning, and generative capabilities, and achieve state-of-the-art results.\n\nThis paper investigates several techniques to stabilize GAN training and encourage convergence. Although lack of theoretical justification, the proposed heuristic techniques give better-looking samples. In addition to human judgement, the paper proposes a new metric called Inception score by applying pre-trained deep classification network on the generated samples. By introducing free labels with the generated samples as new category, the paper proposes the experiment using GAN under semi-supervised learning setting, which achieve SOTA semi-supervised performance on several benchmark datasets (MNIST, CIFAR-10, and SVHN).",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1606.03498"
    },
    "1430": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/JaitlyLVSSB16",
        "transcript": "The paper proposes a \"neural transducer\" model for sequence-to-sequence tasks that operates in a left-to-right and on-line fashion. In other words, the model produces output as the input is received instead of waiting until the full input is received like most sequence-to-sequence models do. Key ideas used to make the model work include a recurrent attention mechanism, the use of an end-of-block symbol in the output alphabet to indicate when the transducer should move to the next input block, and approximate algorithms based on dynamic programming and beam search for training and inference with the transducer model. Experiments on the TIMIT speech task show that the model works well and explore some of the design parameters of the model.\n\nLike similar models of this type, the input is processed by an encoder and a decoder produces an output sequence using the information provided by the encoder and conditioned on its own previous predictions. The method is evaluated on a toy problem and the TIMIT phoneme recognition task. The authors also propose some smaller ideas like two different attention mechanism variations.\n\nThe map from block input to output is governed by a standard sequence-to-sequence model with additional state carried over from the previous block. Alignment of the two sequences is approximated by a dynamic program using a greedy local search heuristic. Experimental results are presented for phone recognition on TIMIT.\n\nThe encoder is a multi-layer LSTM RNN. The decoder is an RNN model conditioned on weighted sums of the last layer of the encoder and it's previous output. The weighting schemes (attention) varies and can be conditioned on the hidden states or also previous attention vectors. The decoder model produces a sequence of symbols, until it outputs a special end character \"e\" and is moved to the next block (other mechanisms where explored as well (no end-of-block-symbol and separately predicting the end of a block given the attention vector). It is then fed the weighted sum of the next block of encoder states. The resulting sequence of symbols determines an alignment of the target symbols over the blocks of inputs, where each block may be assigned a variable number of characters. The system is trained by fixing an alignment, that approximately resembles the best alignment. Finding this approximately best alignment is akin to a beam-search with a beam size of M (line 169), but a restricted set of symbols conditional on the last symbol in a particular hypothesis (since the target sequence is known). Alignments are computed less frequently than model updates (typically every 100 to 300 sequences). For inference, an unconstrained beam-search procedure is performed with a threshold on sequence length and beam size.\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6594-an-online-sequence-to-sequence-model-using-partial-conditioning"
    },
    "1431": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1610.09038",
        "transcript": "Authors present a method similar to teacher forcing that uses generative adversarial networks to guide training on sequential tasks.\n\nThis work describes a novel algorithm to ensure the dynamics of an LSTM during inference follows that during training. The motivating example is sampling for a long number of steps at test time while only training on shorter sequences at training time. Experimental results are shown on PTB language modelling, MNIST, handwriting generation and music synthesis.\n\nThe paper is similar to Generative Adversarial Networks (GAN): in addition to a normal sequence model loss function, the parameters try to \u201cfool\u201d a classifier. That classifier is trying to distinguish generated sequences from the sequence model, from real data. A few Objectives are proposed in section 2.2. The key difference to GAN is the B in equations 1-4. B is a function outputs some statistics of the model, such as the hidden state of the RNN, whereas GAN tries rather to discriminate the actual output sequences.\n\n\nThis paper proposes a method for training recurrent neural networks (RNN) in the framework of adversarial training. Since RNNs can be used to generate sequential data, the goal is to optimize the network parameters in such a way that the generated samples are hard to distinguish from real data. This is particularly interesting for RNNs as the classical training criterion only involves the prediction of the next symbol in the sequence. Given a sequence of symbols $x_1, ..., x_t$, the model is trained so as to output $y_t$ as close to $x_{t+1}$ as possible. Training that way does not provide models that are robust during generation, as a mistake at time t potentially makes the prediction at time $t+k$ totally unreliable. This idea is somewhat similar to the idea of computing a sentence-wide loss in the context of encode-decoder translation models. The loss can only be computed after a complete sequence has been generated.\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1610.09038"
    },
    "1432": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/KaiserB16",
        "transcript": "The authors propose to replace the notion of 'attention' in neural architectures with the notion of 'active memory' where rather than focusing on a single part of the memory one would operate on the whole of it in parallel.\n\nThis paper introduces an extension to neural GPUs for machine translation. I found the experimental analysis section lacking in both comparisons to state of the art MT techniques as well as thoroughly evaluating the proposed method.\n\nThis paper proposes active memory, which is a memory mechanism that operates all the part in parallel. The active memory was compared to attention mechanism and it is shown that the active memory is more effective for long sentence translation than the attention mechanism in English-French translation.\n\nThis paper proposes two new models for modeling sequential data in the sequence-to-sequence framework. The first is called the Markovian Neural GPU and the second is called the Extended Neural GPU. Both models are extensions of the Neural GPU model (Kaiser and Sutskever, 2016), but unlike the Neural GPU, the proposed models do not model the outputs independently but instead connect the output token distributions recursively. The paper provides empirical evidence on a machine translation task showing that the two proposed models perform better than the Neural GPU model and that the Extended Neural GPU performs on par with a GRU-based encoder-decoder model with attention.\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6295-can-active-memory-replace-attention"
    },
    "1433": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/WuZZBS16",
        "transcript": "This paper has a simple premise: that the, say, LSTM cell works better with multiplicative updates (equation 2) rather than additive ones (equation 1). This additive update is used in various places in lieu of additive ones, in various places in the LSTM recurrence equations (the exact formulation is in the supplementary material). A slightly hand wavy argument is made in favour of the multiplicative update, on the grounds of superior gradient flow (section 2.2). Mainly however, the authors make a rather thorough empirical investigation which shows remarkably good performance of their new architectures, on a range of real problems. Figure 1(a) is nice, showing an apparent greater information flow (as defined by a particular gradient) through time for the new scheme, as well as faster convergence and less saturated hidden unit activations. Overall, the experimental results appear thorough and convincing, although I am not a specialist in this area.\n\nThis model presents a multiplicative alternative (with an additive component) to the additive update which happens at the core of various RNNs (Simple RNNs, GRUs, LSTMs). The multiplicative component, without introducing a significant change in the number of parameters, yields better gradient passing properties which enable the learning of better models, as shown in experiments.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6215-on-multiplicative-integration-with-recurrent-neural-networks"
    },
    "1434": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhangWCLMSB16",
        "transcript": "This paper proposes several definitions of measures of complexity of a recurrent neural network. They measure 1) recurrent depth (degree of multi-layeredness as a function of time of recursive connections) 2) feedforward depth (degree of multi-layeredness as a function of input -> output connections) 3) recurrent skip coefficient (degree of directness, like the inverse of multilayeredness, of connections) In addition to the actual definitions, there are two main contributions: - The authors show that the measures (which are limits as the number of time steps -> infinity) are well defined. - The authors correlate the measures with empirical performance in various ways, showing that all measure of depth can lead to improved performance.\n\nThis paper provides 3 measures of complexity for RNNs. They then show experimentally that these complexity measures are meaningful, in the sense that increasingly complexity seems to correlated with better performance.\n\nThe authors first present a rigorous graph-theoretic framework that describes the connecting architectures of RNNs in general, with which the authors easily explain how we can unfold an RNN. The authors then go on and propose tree architecture complexity measures of RNNs, namely the recurrent depth, the feedforward depth and the recurrent skip coefficient. Experiments on various tasks show the importance of certain measures on certain tasks, which indicates that those three complexity measures might be good guidelines when designing a recurrent neural network for certain tasks.\n\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6303-architectural-complexity-measures-of-recurrent-neural-networks"
    },
    "1435": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/NorouziBCJSWS16",
        "transcript": "The proposed approach consists in corrupting the training targets with a noise derived from the task reward while doing maximum likelihood training. This simple but specific smoothing of the target distribution allows to significantly boost the performance of neural structured output prediction as showcased on TIMIT phone and translation tasks. The link between this approach and RL-based expected reward maximization is also made clear by the paper,\n\nPrior work has chosen either maximum likelihood learning, which is relatively tractable but assumes a log likelihood loss, or reinforcement learning, which can be performed for a task-specific loss function but requires sampling many predictions to estimate gradients. The proposed objective bridges the gap with \"reward-augmented maximum likelihood,\" which is similar to maximum likelihood but estimates the expected loss with samples that are drawn in proportion to their distance from the ground truth. Empirical results show good improvements with LSTM-based predictors on speech recognition and machine translation benchmarks relative to maximum likelihood training.\n\n\nThis work is inspired by recent advancement in reinforcement learning and likelihood learning. The authors suggest to learn parameters so as to minimize the KL divergence between CRFs and a probability model that is proportional to the reward function (which the authors call payoff distribution, see Equation 4). The authors suggest an optimization algorithm for the KL-divergence minimization that depends on sampling from the payoff distribution.\n\nCurrent methods to learn a model for structured prediction include max margin optimisation and reinforcement learning. However, the max margin approach only optimises a bound on the true reward, and requires loss augmented inference to obtain gradients, which can be expensive. On the other hand, reinforcement learning does not make use of available supervision, and can therefore struggle when the reward is sparse, and furthermore the gradients can have high variance. The paper proposes a novel approach to learning for problems that involve structured prediction. They relate their approach to simple maximum likelihood (ML) learning and reinforcement learning (RL): ML optimises the KL divergence of a delta distribution relative to the model distribution, and RL optimises the KL divergence of the model distribution relative to the exponentiated reward distribution. They propose reward-augmented maximum likelihood learning, which optimises the KL divergence of the exponentiated reward distribution relative to the model distribution. Compared to RL, the arguments of the KL divergence are swapped. Compared to ML, the delta distribution is generalised to the exponentiated reward distribution. Training is cheap in RML learning. It is only necessary to sample from the output set according to the exponentiated reward distribution. All experiments are performed in speech recognition and machine translation, where the structure over the output set is defined by the edit distance. An improvement is demonstrated over simple ML.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6547-reward-augmented-maximum-likelihood-for-neural-structured-prediction"
    },
    "1436": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1605.06465",
        "transcript": "Swapout is a method that stochastically selects forward propagation in a neural network from a palette of choices: drop, identity, feedforward, residual. Achieves best results on CIFAR-10,100 that I'm aware of.\n\nThis paper examines a stochastic training method for deep architectures that is formulated in such a way that the method generalizes dropout and stochastic depth techniques. The paper studies a stochastic formulation for layer outputs which could be formulated as $Y =\\Theta_1  \\odot X+ \\Theta_2  \\odot F(X)$ where $\\Theta_1$ and $\\Theta_2$ are tensors of i.i.d. Bernoulli random variables. This allows layers to either: be dropped $(Y=0)$, act a feedforward layer $Y=F(X)$, be skipped $Y=X$, or behave like a residual network $Y=X+F(X)$. The paper provides some well reasoned conjectures as to why \"both dropout and swapout networks interact poorly with batch normalization if one uses deterministic inference\", while also providing some nice experiments on the importance of the choice of the form of stochastic training schedules and the number of samples required to obtain estimates that make sampling useful. The approach is able to yield performance improvement over comparable models if the key and critical details of the stochastic training schedule and a sufficient number of samples are used.\n\nThis paper proposes a generalization of some stochastic regularization techniques for effectively training deep networks with skip connections (i.e. dropout, stochastic depth, ResNets.) Like stochastic depth, swapout allows for connections that randomly skip layers, which has been shown to give improved performance--perhaps due to shorter paths to the loss layer and the resulting implicit ensemble over architectures with differing depth. However, like dropout, swapout is independently applied to each unit in a layer allowing for a richer space of sampled architectures. Since accurate expectation approximations are not easily attainable due to the skip connections, the authors propose stochastic inference (in which multiple forward passes are averaged during inference) instead of deterministic inference. To evaluate its effectiveness, the authors evaluate swapout on the CIFAR dataset, showing improvements over various baselines.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/1605.06465"
    },
    "1437": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/Yang0LX16",
        "transcript": "The paper addresses the problem of compressive sensing MRI (CS-MRI) by proposing a \"deep unfolding\" approach (cf. http://arxiv.org/abs/1409.2574) with a sparsity-based data prior and inference via ADMM. All layers of the proposed ADMM-Net are based on a generalization of ADMM inference steps and are discriminatively trained to minimize a reconstruction error. In contrast to other methods for CS-MRI, the proposed approach offers both high reconstruction quality and fast run-time.\n\nThe basic idea is to convert the convention optimization based CS reconstruction algorithm into a fixed neural network learned with back-propagation algorithm. Specifically, the ADMM-based CS reconstruction is approximated with a deep neural network. Experimental results show that the approximated neural network outperforms several existing CS-MRI algorithms with less computational time.\n\nThe ADMM algorithm has proven to be useful for solving problems with differentiable and non-differentiable terms, and therefore has a clear link with compressed sensing. Experiments prove some gain in performance with respect to the state of the art, specially in terms of computational cost at test time.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6406-deep-admm-net-for-compressive-sensing-mri"
    },
    "1438": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HeSMR16",
        "transcript": "A study of how scan orders influence Mixing time in Gibbs sampling.\n\nThis paper is interested in comparing the mixing rates of Gibbs sampling using either systematic scan or random updates. The basic contributions are two: First, in Section 2, a set of cases where 1) systematic scan is polynomially faster than random updates. Together with a previously known case where it can be slower this contradicts a conjecture that the speeds of systematic and random updates are similar. Secondly, (In Theorem 1) a set of mild conditions under which the mixing times of systematic scan and random updates are not \"too\" different (roughly within squares of each other).\n\nFirst, following from a recent paper by Roberts and Rosenthal, the authors construct several examples which do not satisfy the commonly held belief that systematic scan is never more than a constant factor slower and a log factor faster than random scan. The authors then provide a result Theorem 1 which provides weaker bounds, which however they verify at least under some conditions. In fact the Theorem compares random scan to a lazy version of the systematic scan and shows that and obtains bounds in terms of various other quantities, like the minimum probability, or the minimum holding probability.\n\nMCMC is at the heart of many applications of modern machine learning and statistics. It is thus important to understand the computational and theoretical performance under various conditions. The present paper focused on examining systematic Gibbs sampling in comparison to random scan Gibbs. They do so first though the construction of several examples which challenge the dominant intuitions about mixing times, and develop theoretical bounds which are much wider than previously conjectured.\n\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6589-scan-order-in-gibbs-sampling-models-in-which-it-matters-and-bounds-on-how-much"
    },
    "1439": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/RenKZ15",
        "transcript": "This paper addresses the task of image-based Q&A on 2 axes: comparison of different models on 2 datasets and creation of a new dataset based on existing captions.\n\nThe paper is addressing an important and interesting new topic which has seen recent surge of interest (Malinowski2014, Malinowski2015, Antol2015, Gao2015, etc.). The paper is technically sound, well-written, and well-organized. They achieve good results on both datasets and the baselines are useful to understand important ablations. The new dataset is also much larger than previous work, allowing training of stronger models, esp. deep NN ones.\n\nHowever, there are several weaknesses: their main model is not very different from existing work on image-Q&A (Malinowski2015, who also had a VIS+LSTM style model (but they were also jointly training the CNN and RNN, and also decoding with RNNs to produce longer answers) and achieves similar performance (except that adding bidirectionality and 2-way image input helps). Also, as the authors themselves discuss, the dataset in its current form, synthetically created from captions, is a good start but is quite conservative and limited, being single-word answers, and the transformation rules only designed for certain simple syntactic cases.\n\nIt is exploration work and will benefit a lot from a bit more progress in terms of new models and a slightly more broad dataset (at least with answers up to 2-3 words).\n\nRegarding new models, e.g., attention-based models are very relevant and intuitive here (and the paper would be much more complete with this), since these models should learn to focus on the right area of the image to answer the given question and it would be very interesting to analyze the results of whether this focusing happens correctly.\n\nBefore attention models, since 2-way image input helped (actually, it would be good to ablate 2-way versus bidirectionality in the 2-VIS+BLSTM model), it would be good to also show the model version that feeds the image vector at every time step of the question.\n\nAlso, it would be useful to have a nearest neighbor baseline as in Devlin et al., 2015, given their discussion of COCO's properties. Here too, one could imagine copying answers of training questions, for cases where the captions are very similar.\n\nRegarding a broader-scope dataset, the issue with the current approach is that it is too similar to the captioning approach or task, which has the drawback that a major motivation to move to image-Q&A is to move away from single, vague (non-specific), generic, one-event-focused captions to a more complex and detailed understanding of and reasoning over the image; which doesn't happen with this paper's current dataset creation approach, and so this will also not encourage thinking of very different models to handle image-Q&A, since the best captioning models will continue to work well here. Also, having 2-3 word answers will capture more realistic and more diverse scenarios; and though it is true that evaluation is harder, one can start with existing metrics like BLEU, METEOR, CIDEr, and human eval. And since these will not be full sentences but just 2-3 word phrases, such existing metrics will be much more robust and stable already.\n\nOriginality:\n\nThe task of image-Q&A is very recent with only a couple of prior and concurrent work, and the dataset creation procedure, despite its limitations (discussed above) is novel. The models are mostly not novel, being very similar to Malinowski2015, but the authors add bidirectionality and 2-way image input (but then Malinowski2015 was jointly training the CNN and RNN, and also decoding with RNNs to produce longer answers).\n\nSignificance:\n\nAs discussed above, the paper show useful results and ablations on the important, recent task of image-Q&A, based on 2 datasets -- an existing small dataset and a new large dataset; however, the second, new dataset is synthetically created by rule-transforming captions and only to single-word answers, thus keeping the impact of the dataset limited, because it keeps the task too similar to the generic captioning task and because there is no generation of answers or prediction of multi-word answers.\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5640-exploring-models-and-data-for-image-question-answering"
    },
    "1440": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MakhzaniF15",
        "transcript": "The paper proposes a novel way to train a sparse autoencoder where the hidden unit sparsity is governed by a winner-take-all kind of selection scheme. This is a convincing way to achieve a sparse autoencoder, while the paper could have included some more details about their training strategy and the complexity of the algorithm.\n\nThe authors present a fully connected auto-encoder with a new sparsity constraint called the lifetime sparsity. For each hidden unit across the mini-batch, they rank the activation values, keeping only the top-k% for reconstruction. The approach is appealing because they don't need to find a hard threshold and it makes sure every hidden unit/filter is updated (no dead filters because their activation was below the threshold).\n\nTheir encoder is a deep stack of ReLu and the decoder is shallow and linear (note that usually non-symmetric auto-encoders lead to worse results). They also show how to apply to RBM. The effect of sparsity is very effective and noticeable on the images depicting the filters.\n\nThey extend this auto-encoder in a convolutional/deconvolutional framework, making it possible to train on larger images than MNIST or TFD. They add a spatial sparsity, keeping the top activation per feature map for the reconstruction and combine it with the lifetime sparsity presented before.\n\nThe proposed approach exploits on a mechanism close to the one of k-sparse autoencoders proposed by Makkhzani et al [14]. The authors extend the idea from [14] to build winner-take-all encoders (and RBMs), that enforce both spatial and lifetime regularization by keeping only a percentage (the biggest) of activations. The lifetime sparsity allows overcoming problems that could arise with k-sparse autoencoders. The authors next propose to embed their modeling framework in convolutional neural nets to deal with larger images than e.g. those of mnist.\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5783-winner-take-all-autoencoders"
    },
    "1441": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SukhbaatarSWF15",
        "transcript": "This paper presents an end-to-end version of memory networks (Weston et al., 2015) such that the model doesn't train on the intermediate 'supporting facts' strong supervision of which input sentences are the best memory accesses, making it much more realistic. They also have multiple hops (computational steps) per output symbol. The tasks are Q&A and language modeling, and achieves strong results.\n\nThe paper is a useful extension of memNN because it removes the strong, unrealistic supervision requirement and still performs pretty competitively. The architecture is defined pretty cleanly and simply. The related work section is quite well-written, detailing the various similarities and differences with multiple streams of related work. The discussion about the model's connection to RNNs is also useful.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5846-end-to-end-memory-networks"
    },
    "1442": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HarikandehAVSKS15",
        "transcript": "This paper extends the stochastic optimization algorithm SVRG proposed in recent years. These modifications mainly includes: the convergence analysis of SVRG with corrupted full gradient; Mix the iteration of SGD and SVRG; the strategy of mini-batch; Using support vectors etc. For each modification, the author makes clear proofs and achieves linear convergence under smooth and strong convex assumptions. However, this paper's novelty is not big enough. The improvement of convergence rate is not obvious and the proof outline is very similar to the original SVRG. The key problem such as the support for non-strongly convex loss is still unsolved. \n\nThis paper starts with a key proposition showing that SVRG does not require a very accurate approximation of the total gradient of the objective function needed by SVRG algorithm. The authors use this proposition to derive a batching SVRG algorithm with the same convergence rate as that of original SVRG. Then, the authors propose a mixed stochastic gradient/SVRG approach and give a convergence proof for such a scheme. As a different approach of speeding up, the authors proposed a speed-up technique for Huberized hinge-loss support vector machine.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5711-stopwasting-my-gradients-practical-svrg"
    },
    "1443": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/JaderbergSZK15",
        "transcript": "This paper presents a novel layer that can be used in convolutional neural networks. A spatial transformer layer computes re-sampling points of the signal based on another neural network. The suggested transformations include scaling, cropping, rotations and non-rigid deformation whose paramerters are trained end-to-end with the rest of the model. The resulting re-sampling grid is then used to create a new representation of the underlying signal through bi-linear or nearest neighbor interpolation. This has interesting implications: the network can learn to co-locate objects in a set of images that all contain the same object, the transformation parameter localize the attention area explicitly, fine data resolution is restricted to areas important for the task. Furthermore, the model improves over previous state-of-the-art on a number of tasks.\n\nThe layer has one mini neural network that regresses on the parameters of a parametric transformation, e.g. affine), then there is a module that applies the transformation to a regular grid and a third more or less \"reads off\" the values in the transformed positions and maps them to a regular grid, hence under-forming the image or previous layer. Gradients for back-propagation in a few cases are derived. The results are mostly of the classic deep learning variety, including mnist and svhn, but there is also the fine-grained birds dataset. The networks with spatial transformers seem to lead to improved results in all cases.\n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5854-spatial-transformer-networks"
    },
    "1444": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/NguyenLJ15",
        "transcript": "This paper addresses the problem of inverse reinforcement learning when the agent can change it's objective during the recording of trajectories. This results in a transition between several reward functions that explain only locally the trajectory of the observed agent. Transition probabilities between reward functions are unknown. The author propose a cascade of an EM and Viterbi algorithms to discover the reward functions and the segments on which they are valid.\n\nTheir algorithm consists in maximizing the log-likelihood of the expert's demonstrated trajectories depending on some parameters which are the original distributions of states and rewards, the local rewards and the transition function between rewards. To do so, they use the expectation-maximisation (EM) method. Then, via the Viterbi algorithm, they are able to partition the trajectories into segments with local consistent rewards.\n\nStrengths of the paper:\n\n1. The authors leverage existing and classical methods from the machine learning and optimization fields such as EM, Viterbi, Value iteration and gradient ascent in order to build their algorithm. This will allow the community to easily reproduce their results. 2. The experiments are conducted on synthetic and real-world data. They compare their method to MLIRL which does not use locally consistent rewards and which is the canonical choice to compare to as their algorithm is a generalization of MLIRL. The results presented show the superiority of their method over MLIRL. 3. The idea presented by the authors is original as far as I know.\n\nWeaknesses of the paper:\n\n1. The paper is very dense ( the figures are incorporated in the text) which makes the reading difficult. \n2. The algorithm proposed needs the knowledge of the dynamics and the number of rewards. The authors, as future works, plan to extend their algorithm to unknown number of rewards, however they do not mention to get rid off the knowledge of the dynamics. Could the authors comment on that as some IRL algorithms do not need a perfect knowledge of the dynamics?\n\n3. The method needs to solve iteratively MDPs when learning the reward functions. For each theta in the gradient ascent a MDP needs to be solved. Is this prohibitive for huge MDPs? Is there a way to avoid that step? The action-value function Q is defined via a softmax operator in order to have a derivable policy, does it allow to solve more efficiently the MDP? \n4. The authors are using gradient ascent in the EM method, could they comment on the concavity of their criteria? \n5. In the experiments (gridworlds), the number of features for the states is very small and thus it is understandable that a reward which is linear on the features will perform badly. Do the authors consider comparing their method to an IRL method where the number of features defining the states is greater? This is the main problem that I have with the experiments, the features used are not expressive enough to consider using a classical IRL method and this can explain why MLIRL performs badly and that its performance does not improve when the number of expert trajectories grows. \n6. The performance is measured by the average log-likelihood of the expert's demonstrated trajectories which is the criterion maximized by the algorithm. I think that a more pertinent measure would be the value function of the policy produced by the optimization of the reward obtained by the algorithm. Could the authors comment on that and explain why their performance metric is more appropriate?\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5882-inverse-reinforcement-learning-with-locally-consistent-reward-functions"
    },
    "1445": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HermannKGEKSB15",
        "transcript": "This paper deals with the formal question of machine reading. It proposes a novel methodology for automatic dataset building for machine reading model evaluation. To do so, the authors leverage on news resources that are equipped with a summary to generate a large number of questions about articles by replacing the named entities of it. Furthermore a attention enhanced LSTM inspired reading model is proposed and evaluated. The paper is well-written and clear, the originality seems to lie on two aspects. First, an original methodology of question answering dataset creation, where context-query-answer triples are automatically extracted from news feeds. Such proposition can be considered as important because it opens the way for large model learning and evaluation. The second contribution is the addition of an attention mechanism to an LSTM reading model. the empirical results seem to show relevant improvement with respect to an up-to-date list of machine reading models.\n\nGiven the lack of an appropriate dataset, the author provides a new dataset which scraped CNN and Daily Mail, using both the full text and abstract summaries/bullet points. The dataset was then anonymised (i.e. entity names removed). Next the author presents a two novel Deep long-short term memory models which perform well on the Cloze query task.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend"
    },
    "1446": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/BareinboimFP15",
        "transcript": "The paper \"Bandits with unobs. confounders: a causal approach\" addresses the problem of bandit learning. It is assumed that in the observational setting, the player's decision is influenced by some unobserved context. If we randomize the player's decision, however, this intention is lost. The key idea is now that, using the available data from both scenarios, one can infer whether one should overrule the player's intention. Ultimately, this leads to the following strategy: observe the player's intention and then decide whether he should act accordingly or pull the other arm.\n\nThe author showed that the current MAB algorithms actually attempt to maximize rewards according to the experimental distribution, which is not optimal in the confounding case, and proposed to make use of the effect of the treatment on the treated (ETT), i.e., by comparing the average payouts obtained by players for going in favor of or against their intuition. To me, the paper is interesting because it addresses the confounding issue in MAB and proposed a way to estimate some properties of the confounder (related to the casino's payout strategy in the given example) based on ETT.\n\nAt first glance, one might think that the blinking light on the slot machines (B) and the drunkenness of the patron (D) could be either modified or observed in lines 153-159, where we read about a hypothetical attempt to optimize reward using traditional Thompson sampling. If those factors were observable or subject to intervention -- and I'd think they would be, in reality -- then it would be straightforward to do better than the 30% reward rate that's given. The paper eventually makes it clear that both of these variables are unobserved and unalterable. It would help if this were explicit early in the example, or if the cover story were modified to make this aspect more intuitive.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5692-bandits-with-unobserved-confounders-a-causal-approach"
    },
    "1447": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SwerskySA13",
        "transcript": "This paper presents a multi-task Bayesian optimization approach to hyper-parameter setting in machine learning models. In particular, it leverages previous work on multi-task GP learning with decomposable covariance functions and Bayesian optimization of expensive cost functions. Previous work has shown that decomposable covariance functions can be useful in multi-task regression problems (e.g. \\cite{conf/nips/BonillaCW07}) and that Bayesian optimization based on response-surfaces can also be useful for hyper-parameter tuning of machine learning algorithms \\cite{conf/nips/SnoekLA12} \\cite{conf/icml/BergstraYC13}. \n\nThe paper combines the decomposable covariance assumption \\cite{conf/nips/BonillaCW07} and Bayesian optimization based on expected improvement \\cite{journals/jgo/Jones01} and entropy search \\cite{conf/icml/BergstraYC13} to show empirically that it is possible to : \n1. Transfer optimization knowledge across related problems, addressing e.g. the cold-start problem \n2. Optimize an aggregate of different objective functions with applications to speeding-up cross validation \n3. Use information from a smaller problem to help optimize a bigger problem faster \n\nPositive experimental results are shown on synthetic data (Branin-Hoo function), optimizing logistic regression hyper-parameters and optimizing hyper-parameters of online LDA on real data. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5086-multi-task-bayesian-optimization"
    },
    "1448": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DenilSDRF13",
        "transcript": "Motivated by recent attempts to learn very large networks this work proposes an approach for reducing the number of free parameters in neural-network type architectures. The method is based on the intuition that there is typically strong redundancy in the learned parameters (for instance, the first layer filters of of NNs applied to images are smooth): The authors suggest to learn only a subset of the parameter values and to then predicted the remaining ones through some form of interpolation. The proposed approach is evaluated for several architectures (MLP, convolutional NN, reconstruction-ICA) and different vision datasets (MNIST, CIFAR, STL-10). The results suggest that in general it is sufficient to learn fewer than 50% of the parameters without any loss in performance (significantly fewer parameters seem sufficient for MNIST). \n\nThe method is relatively simple: The authors assume a low-rank decomposition of the weight matrix and then further fix one of the two matrices using prior knowledge about the data (e.g., in the vision case, exploiting the fact that nearby pixels - and weights - tend to be correlated). This can be interpreted as predicting the \"unobserved\" parameters from the subset of learned filter weights via kernel ridge regression, where the kernel captures prior knowledge about the topology / \"smoothness\" of the weights. For the situation when such prior knowledge is not available the authors describe a way to learn a suitable kernel from data. \n\nThe idea of reducing the number of parameters in NN-like architectures through connectivity constraints in itself is of course not novel, and the authors provide a pretty good discussion of related work in section 5. Their method is very closely related to the idea of factorizing weight matrices as is, for instance, commonly done for 3-way RBMs (e.g. ref [22] in the paper), but also occasionally for standard RBMs (e.g. [R1], missing in the paper). The present papers differs from these in that the authors propose to exploit prior knowledge to constrain one of the matrices. As also discussed by the authors, the approach can further be interpreted as a particular type of pooling -- a strategy commonly employed in convolutional neural networks. Another view of the proposed approach is that the filters are represented as a linear combination of basis functions (in the paper, the particular form of the basis functions is determined by the choice of kernel). Such representations have been explored in various forms and to various ends in the computer vision and signal processing literature (see e.g. [R2,R3,R4,R5]). [R4,R5], for instance, represent filters in terms of a linear combination of basis functions that reduce the computational complexity of the filtering process). ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning"
    },
    "1449": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MitliagkasC013",
        "transcript": "This paper proposes an approach to one-pass SVD based on a blocked variant of the power method, which variance is reduced within each block of streaming data, and compares to exact batch SVD. \nFigure 1d is offered as an example where the proposed Algo 1 can scale to data for which the authors claim to be so large that \"traditional batch methods\" could not be run and reported. Yet there are many existing well-known SVD methods which are routinely used for even larger data sets than the largest here (sparse 8.2M vectors for 120k dimensions). These include the EMPCA (Roweiss 1998) and fast randomized SVD (Haiko et at 2011), both of which the author's cite. Why were these methods (both very simple to implement efficiently even in Matlab, etc.) not reported for this data? Especially necessary to compare against is the randomized SVD, since it too can be done in one-pass (see Haiko et al); although that cited paper discusses the tradeoffs in doing multiple passes -- something this paper does not even discuss. The authors say it took \"a few hours\" for Algo 1 to extract the top 7 components. Methods like the randomized SVD family of Haiko et al scale linearly in those parameters (n=8.2M and d=120k and k=7 and the number of non-zeros of the sparse data) and typically run in less than 1 hour for even larger data sets. So, demonstrating both the speed and accuracy of the proposed Algo 1 compared to the randomized algorithms seems necessary at this point, to establish the practical significance of this proposed approach.\n\nThis paper identifies and resolves a basic gap in the design of streaming PCA algorithms. It is shown that a block stochastic streaming version of the power method recovers the dominant rank-k PCA subspace with optimal memory requirements and sample complexity not too worse than batch PCA (which maintains the covariance matrix explicitly), assuming that streaming data is drawn from a natural probabilistic generative model. The paper is excellently written and provides intuitions for the analysis, starting with exact rank 1 and exact rank k case to the general rank k approximation problem. Some empirical analysis is also provided illustrating the approach for PCA on large document-term matrices.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5035-memory-limited-streaming-pca"
    },
    "1450": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/WuLC13",
        "transcript": "The authors introduce a functional called the \"harmonic loss\" and show that (a) it characterizes smoothness in the sense that functions with small harmonic loss change little across large cuts (to be precise, the cut has to be a level set separator) (b) several algorithms for learning on graphs implicitly try to find functions that minimize the harmonic loss, subject to some constraints. \n\nThe \"harmonic loss\" they define is essentially the (signed) divergence $\\nabla f$ of the function across the cut, so it's not surprising that it should be closely related to smoothness. In classical vector calculus one would take the inner product of this divergence with itself and use the identity \n\n< $\\nabla f, \\nabla f $> = < $f, \\nabla^2 f $> \n\nto argue that functions with small variation, i.e., small $| \\nabla f |^2$ almost everywhere can be found by solving the Laplace equation. On graphs, modulo some tweaking with edge weights, essentially the same holds, leading to minimizing the quadratic form $ f^\\top L f$, which is at the heart of all spectral methods. So in this sense, I am not surprised. \n\nAlternatively, one can minimize the integral of $| \\nabla f |$, which is the total variation, and leads to a different type of regularization ($l1$ rather than $l2$ is one way to put it). The \"harmonic loss\" introduced in this paper is essentially this total variation, except there is no absolute value sign. Among all this fairly standard stuff, the interesting thing about the paper is that for the purpose of analyzing algorithms one can get away with only considering this divergence across cuts that separate level sets of $f$, and in that case all the gradients point in the same direction so one can drop the absolute value sign. This is nice because the \"harmonic loss\" becomes linear and a bunch of things about it are very easy to prove. At least this is my interpretation of what the paper is about. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5157-analyzing-the-harmonic-structure-in-graph-based-learning"
    },
    "1451": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=mikolov2013distributed",
        "transcript": "The paper discusses a number of extensions to the Skip-gram model previously proposed by Mikolov et al (citation [7] in the paper): which learns linear word embeddings that are particularly useful for analogical reasoning type tasks. The extensions proposed (namely, negative sampling and sub-sampling of high frequency words) enable extremely fast training of the model on large scale datasets. This also results in significantly improved performance as compared to previously proposed techniques based on neural networks. The authors also provide a method for training phrase level embeddings by slightly tweaking the original training algorithm. \n\nThis paper proposes 3 improvements for the skip-gram model which allows for learning embeddings for words. The first improvement is subsampling frequent word, the second is the use of a simplified version of noise constrastive estimation (NCE) and finally they propose a method to learn idiomatic phrase embeddings. In all three cases the improvements are somewhat ad-hoc. In practice, both the subsampling and negative samples help to improve generalization substantially on an analogical reasoning task. The paper reviews related work and furthers the interesting topic of additive compositionality in embeddings. \n\nThe article does not propose any explanation as to why the negative sampling produces better results than NCE which it is suppose to loosely approximate. In fact it doesn't explain why besides the obvious generalization gain the negative sampling scheme should be preferred to NCE since they achieve similar speeds. ",
        "sourceType": "blog",
        "linkToPaper": null
    },
    "1452": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/BalsubramaniDF13",
        "transcript": "This paper proves fast convergence rates for Oja's well-known incremental algorithm for PCA. The proof uses a novel technique to describe the progress of the algorithm, by breaking it into several \"epochs\"; this is necessary because the PCA problem is not convex, and has saddle points. The proof also uses some ideas from the study of stochastic gradient descent algorithms for strongly convex functions. The theoretical bounds give some insight into the practical performance of Oja's algorithm, and its sensitivity to different parameter settings. \n\nThey prove the $\\tilde{O}(1/n)$ finite sample rate of convergence for estimating the leading eigenvector of the covaraince matrix. Their results suggest the best learning rate for incremental PCA. Also, their analysis provide insights for relationship with SGD on strongly convex functions.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5132-the-fast-convergence-of-incremental-pca"
    },
    "1453": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SlawskiHL13",
        "transcript": "This paper discusses a new approach to binary matrix factorization \nthat is motivated by recent developments in non-negative matrix \nfactorization. The goal of \nthe paper is to present an algorithm for finding a factorization of a \nmatrix in the form $D = T A$ where the entries of $T$ are in \n$\\{0,1\\}$. Such a model has wide applicability and is of interest to \nthe ML community. The algorithm has provable recovery guarantees in \nthe case of noiseless observations. A modified algorithm is applied \nto the noisy setting; however, the authors do not establish recovery \nguarantees. \n\nThe paper presents an algorithm for low-rank matrix factorization with constraints on one of the factors should be binary. The paper has several novel contributions for this problem. The algorithm guarantees the exact solution with the time complexity of $O(mr2^r+mnr)$, where previous approach (E. Meeds et al., NIPS 2007) uses MCMC algorithm so that it cannot guarantee a global convergence. Under additional assumptions on the binary factor matrix $T$, the uniqueness of $T$ is proved which means that each data point has a unique representation with the columns of $T$. Using Littlewood-Offord lemma, the paper computes a theoretical speed-up factor for their heuristic of the candidate binary vector set reduction step. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4860-matrix-factorization-with-binary-components"
    },
    "1454": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=heess_learning_2013",
        "transcript": "This paper proposes to learning expectation propagation (EP) message update operators from data that would enable fast and efficient approximate inference in situations where computing these operators is otherwise intractable. \n\nThis paper attacks the problem of computing the intractable low dimensional statistics in EP message passing by training a neural network. Training data is obtained using importance sampling and assuming that we know the forward model. The paper appears technically correct, honest about shortcomings, provides an original approach to a known challenge within EP and nicely illustrates the developed method in a number of well-chosen examples.\n\nThe authors propose a method for learning a mapping from input messages to the output message in the context of expectation propagation. The method can be thought of as a sort of \"compilation\" step, where there is a one-time cost of closely approximating the true output messages using important sampling, after which a neural network is trained to reproduce the output messages in the context of future inference queries. ",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://media.nips.cc/nipsbooks/nipspapers/paper_files/nips26/1493.pdf"
    },
    "1455": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SongD13",
        "transcript": "The authors present a robust low rank kernel embedding related to higher order tensors and \nlatent variable models. In general the work is interesting and promising. It provides synergies between machine learning, kernel methods, tensors and latent variable models. \n\nThe RKHS embedding of a joint probability distribution between two variables involves the notion of covariance operators. For joint distributions over multiple variables, a tensor operator is needed. The paper defines these objects together with appropriate inner product, norms and reshaping operations on them. The paper then notes that in the presence of latent variables where the conditional dependence structure is a tree, these operators are low-rank when reshaped along the edges connecting latent variables. A low-rank decomposition of the embedding is then proposed that can be implemented on Gram matrices. Empirical results on density estimation tasks are impressive. \n\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5080-robust-low-rank-kernel-embeddings-of-multivariate-distributions"
    },
    "1456": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/VossRB13",
        "transcript": "This paper presents a fast ICA algorithm that works best under Gaussian noise. This is demonstrated with components simulated from different univariate distributions and variable Gaussian noise. \n\nThe writing is clear. The paper is incremental in the sense that it builds on ideas from (Belkin et. al, 2013) but focuses on speeding up and improving their cumulant-based approach. \nThis is achieved via \n1. a Hessian expansion of the cumulant-tensor-based quasi-orthogonalization. \n2. gradient-based iterations that preserve quasi-orthogonalization of the latent factors (noised case) as well as whitening in the noiseless case. \n\nThis paper proposes a cumulant based independent component analysis (ICA) algorithm for source separation in the presence of additive Gaussian noise. The algorithm is somewhat incremental building upon Refs [2] and [3], but appear technically correct with experimental results confirming the claims made. The algorithms used for benchmarking assume no additive noise but is like InfoMax often quite robust to addition of noise. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5134-fast-algorithms-for-gaussian-noise-invariant-independent-component-analysis"
    },
    "1457": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/GoodfellowMCB13",
        "transcript": "The paper presents a method for learning layers of representation and for completing missing queries both in input and labels in single procedure unlike some other methods like deep boltzmann machines (DBM). It is a recurrent net following the same operations as DBM with the goal of predicting a subset of inputs from its complement. Parts of paper are badly written, especially model explanation and multi-inference section, nevertheless the paper should be published and I hope the authors will rewrite them. \n\nDeep Boltzmann Machines (DBNs) are usually initialized by greedily training a stack of RBMs, and then fine-tuning the overall model using persistent contrastive divergence (PCD). To perform classification, one typically provides the mean-field features to a separate classifier (e.g. a MLP) which is trained discriminatively. Therefore the overall process is somewhat ad-hoc, consisting of L + 2 models (where L is the number of hidden layers) each with its own objective. This paper presents a holistic training procedure for DBNs which has a single training stage (where both input and output variables are predicted) producing models which can classify directly as well as efficiently performing other tasks such as imputing missing inputs. The main technical contribution is the mechanism by which training is performed; a way of training DBNs which uses the mean field equations for the DBN to induce recurrent nets that are trained to solve different inference tasks (essentially predicting different subsets of observed variables). ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5024-multi-prediction-deep-boltzmann-machines"
    },
    "1458": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DauphinB13",
        "transcript": "The paper uses a subsampling-based method to speed up ratio matching training of \nRBMs on high-dimensional sparse binary data. The proposed approach is a simple \nadaptation of the method proposed by Dauphin et al. (2011) for denoising \nautoencoders. \n\nThis paper develops an algorithm that can successfully train RBMs on very high dimensional but sparse input data, such as often arises in NLP problems. The algorithm adapts a previous method developed for denoising autoencoders for use with RBMs. The authors present extensive experimental results verifying that their method learns a good generative model; provides unbiased gradient estimates; attains a two order of magnitude speed up on large sparse problems relative to the standard implementation; and yields state of the art performance on a number of NLP tasks. They also document the curious result that using a biased version of their estimator in fact leads to better performance on the classification tasks they tested. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5022-stochastic-ratio-matching-of-rbms-for-sparse-high-dimensional-inputs"
    },
    "1459": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SyrgkanisALS15",
        "transcript": "The authors perform theoretical analysis about faster convergence with multi-player normal-form games by generalizing techniques for two-player zero-sum games. They also perform empirical evaluation by using the 4-bidder simultaneous auction game.\n\nThe paper is concerned with two problems:\n\n1. How does the social welfare of players using regret minimization algorithms compare to the optimal welfare. \n2. Can one obtain better regret bounds when all players use a regret minimization algorithm\n\nThe paper deals with bounds on regret minimization algorithms in games. The usual regret bounds on these algorithms is in $O(\\sqrt{T})$. However, this assumes that the learner faces a completely adversarial opponent. However, it is natural to assume that on a game everyone will play a regret minimization algorithm and the question is whether or not one can obtain better rates in this scenario. The authors show that regret in $O(T^{1/4})$ is achievable for general games.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5763-fast-convergence-of-regularized-learning-in-games"
    },
    "1460": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/OrlitskyS15",
        "transcript": "The paper gives justification for the widespread use of the Good-Turing estimator for discrete distribution estimation through minimax regret analysis with two comparator classes. The paper obtains competitive regret bounds that lead to a more accurate characterization of the performance of the the Good-Turing estimators and in some cases is much better than the best known risk bounds. The comparator classes considered are estimators with knowledge of the distribution up to permutation, and estimators with full knowledge of the distribution, but with the constraint that the must assign the same probability mass to symbols appearing with the same frequencies.\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5762-competitive-distribution-estimation-why-is-good-turing-good"
    },
    "1461": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MaddisonTM14",
        "transcript": "This paper introduces a new approach to sampling from continuous probability distributions. The method extends prior work on using a combination of Gumbel perturbations and optimization to the continuous case. This is technically challenging, and they devise several interesting ideas to deal with continuous spaces, e.g. to produce an exponentially large or even infinite number of random variables (one per point of the continuous/discrete space) with the right distribution in an implicit way. Finally, they highlight an interesting connection with adaptive rejection sampling. Some experimental results are provided and show the promise of the approach. \n\nThis paper introduces a sampling algorithm based on the Gumbel-max trick and A* search for continuous spaces. The Gumbel-Max trick adds perturbations to an energy function and after applying argmax, results in exact samples from the Gibbs distribution. While this applies to discrete spaces, this paper extends this idea to continuous spaces using the upper bounds on the infinitely many perturbation values. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5449-a-sampling"
    },
    "1462": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/Shrivastava014",
        "transcript": "This paper generalizes the LSH method to account for the (bounded) lengths of the data base vectors, so that the LSH tricks for fast approximate nearest neighbor search can exploit the well-known relation between Euclidian distance and dot product similarity (e.g. as in equation 2) and support MIPS search as well. They give 3 motivating examples where solving MIPS vs kNN per se is more appropriate and needed. Their algorithm is essentially equation 9 (using equation 7 compute vector reformulations $Q(q)$ and $P(x)$ of the query a database element respectively). This is based on apparently novel observation (equation 8) that the distance from the query converges to the dot product plus a constant, when a parameter m which exponentiated the $P(x)$ vector elements is sufficiently large (e.g. just 3 is claimed to suffice, leading to vectors $Q(q)$ and $P(x)$ which are just that m times larger than the original input dimensionality.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips"
    },
    "1463": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DuSGZ13",
        "transcript": "The paper addresses how to estimate and maximize influence in large networks, where influence of node (or set of nodes) A is the expected number of nodes that will eventually adopt a certain idea following the initial adoption by A. The authors develop an algorithm for estimating influence within a given time frame, then use it as the basis of a greedy algorithm to find a given number of nodes to (approximately) maximize influence within the given time frame. They present theoretical bounds and an experimental evaluation of the algorithm. \n\nThe authors build on an extensive list of existing work, which is appropriately cited. The most relevant is the work by Gomez-Rodriguez & Scholkopf (2012) \\cite{conf/icml/Gomez-RodriguezS12}, which provides an exact analytical solution to the identical formulation of the influence estimation problem. The main innovation in the present paper is a fast randomized algorithm for estimating influence, which is based on the algorithm for estimating neighborhood size by Cohen (1997) \\cite{journals/jcss/Cohen97}. This approximation allows more flexibility in modeling the flows through the edges, is substantially faster than the analytical solution, and scales well with network size. Overall, this is a solid paper on an important topic of practical relevance. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4857-scalable-influence-estimation-in-continuous-time-diffusion-networks"
    },
    "1464": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/IyerB13",
        "transcript": "The authors introduce two new submodular optimization problems and \ninvestigate approximation algorithms for them. The problems are \nnatural generalizations of many previous problems: there is a covering \nproblem ($min\\\\{ f(X) : g(X) \\ge c\\\\}$) and a packing or knapsack problem \n($max\\\\{ g(X) : f(X) \\le b\\\\}$), where both f and g are submodular. These \ngeneralize well-known previously studied versions of the problems \nusually assume that f is modular. They show that there is an intimate \nrelationship between the two problems: any polynomial-time \nbi-criterion algorithm for one problem implies one for the other \nproblem (with similar approximation factors) using a simple reduction. \nThey then present a general iterative framework for solving the two \nproblems by replacing either f or g by tight upper or lower bounds \n(often modular) at each iteration. These tend to reduce the problem \nat each iteration to a simpler subproblem for which there are existing \nalgorithms with approximation guarantees. In many cases, they are able \nto translate these into approximation guarantees for the more general \nproblem. Their approximation bounds are curvature-dependent and \nhighlight the importance of this quantity on the difficulty of the \nproblem. The authors also present a hardness result that matches their \nbest approximation guarantees up to log factors, show that a number of \nexisting approximation algorithms (e.g. greedy ones) for the simpler \nproblem variants can be recovered from their framework by using \nspecific modular bounds, and show experimentally that the simpler \nalgorithm variants may perform as well as the ones with better \napproximation guarantees in practice. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4911-submodular-optimization-with-submodular-cover-and-submodular-knapsack-constraints"
    },
    "1465": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LahiriG13",
        "transcript": "The paper studies the problem of memory storage with discrete (digital) synapses. Previous work established that memory capacity can be increased by adding a cascade of (latent) states but the optimal state transition dynamics was unknown and the actual dynamics was usually hand-picked using some heuristic rules. In this paper the authors aim to derive the optimal transition dynamics for synaptic cascades. They first derive an upper bound on achievable memory capacity and show that simple models with linear chain structures can approach (achieve) this bound. \n\nThe paper applies the theory if ergodic Markov chains in continuous time to the analysis of the memory properties of online learning in synapses with intrinsic states extending earlier work of Abbott, Fusi and their co-workers. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4872-a-memory-frontier-for-complex-synapses"
    },
    "1466": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/WangSL13",
        "transcript": "Finding the objective functions that regions of the nervous system are optimized for is a central question in neuroscience, providing a central computational principle behind neural representation in a given region. One common objective is to maximize the Shannon Information the neural response encodes about the input (infomax). This is supported by some experimental. Another is to minimize the decoding error when the neural population is decoded for a particular variable or variable. This has also been found to have some experimental evidence. These two different objectives are similar in some circumstances, giving similar predictions, in other cases they differ more. \n\nStudies finding model optimal distributions of neural population tuning that minimizes decoding error (L2-min) have mostly considered 1-dimensional stimuli. In this paper the authors extend substantially on this, by developing analytical methods for finding the optimal distributions of neural tuning for higher dimensional stimuli. Their methods apply under certain limited conditions , such as when there is an equal number of neurons as stimulus dimensions (diffeomorphic). The authors compare their results to the infomax solution (in most detail for the 2D case), and find fairly similar results in some respects, but with two key differences. That the L2-min basis functions are more orthogonal than the infomax, and that the L2-min has discrete solutions rather than the continuum found for infomax. A consequence of these differences is that L2-min representations encode more correlated signals. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4994-optimal-neural-population-codes-for-high-dimensional-stimulus-variables"
    },
    "1467": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SavinDL13",
        "transcript": "The paper investigates how correlation among synaptic weights, not correlation among neural activity, influences the retrieval performance of auto-associative memory. Authors studied two types of well-known learning rules, additive learning rule (e.g., Hebbian learning) and palimpsest learning rule (e.g., cascade learning), and showed that synaptic correlations are induced in most of the cases. They also investigated optimal retrieval dynamics and showed that there exists a local version of dynamics that can be implemented in neural networks (except for an XOR cascade model). ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4871-correlations-strike-back-again-the-case-of-associative-memory-retrieval"
    },
    "1468": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/TitsiasL13",
        "transcript": "In a GP regression model, the process outputs can be integrated over analytically, but this is not so for (a) inputs and (b) kernel hyperparameters. Titsias etal 2010 showed a very clever way to do (a) with a particular variational technique (the goal was to do density estimation). In this paper, (b) is tackled, which requires some nontrivial extensions of Titsias etal. In particular, they show how to decouple the GP prior from the kernel hyperparameters. This is a simple trick, but very effective for what they want to do. They also treat the large number of kernel hyperparameters with an additional level of ARD and show how the ARD hyperparameters can be solved for analytically, which is nice. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5088-variational-inference-for-mahalanobis-distance-metrics-in-gaussian-process-regression"
    },
    "1469": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DickerF13",
        "transcript": "This paper studies a linear latent factor model, where one observes \"examples\" consisting of high-dimensional vectors $x_1, x_2, ..\\in R^d$, and one wants to predict \"labels\" consisting of scalars $y_1, y_2, ... \\in R$. Crucially, one is working in the \"one-shot learning\" regime, where the number of training examples n is small (say, $n=2$ or $n=10$), while the dimension d is large (say, $d \\rightarrow \\infty$). This paper considers a well-known method, principal component regression (PCR), and proves some somewhat surprising theoretical results: PCR is inconsistent, but a modified PCR estimator is weakly consistent; the modified estimator is obtained by \"expanding\" the PCR estimator, which is different from the usual \"shrinkage\" methods for high-dimensional data. \n\nThis paper aims to provide an analysis for principle component \nregression in the setting where the feature vectors $x$. The authors \nlet $x = v + e$ where $e$ is some corruption of the nominal feature \nvector $v$; and $v = a u$ where $a \\sim N(0,\\eta^2 \\gamma^2 d)$ while \nthe observations $y = \\theta/(\\gamma \\sqrt{d}) \\langle v,u \\rangle + \\xi$. This \nformulation is slightly different than the standard one because our \ndesign vectors are noisy, which can pose challenges in identifying the \nlinear relationship between $x$ and $y$. Thus, using the top principle \ncomponents of $x$ is a standard method used in order to help \nregularize the estimation. The paper is relevant to the ML \ncommunity. The key message of using a bias-corrected estimate of $y$ \nis interesting, but not necessarily new. Handling bias in regularized \nmethods is a common problem (cf. Regularization and variable selection \nvia the Elastic Net, Zou and Hastie, 2005). The authors present \ntheoretical analysis to justify their results. I find the paper \ninteresting; however I am not sure if the number of new results and \nlevel of insights warrants acceptance. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5137-one-shot-learning-and-big-data-with-n2"
    },
    "1470": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/FidanerC13",
        "transcript": "The authors propose novel approaches for summarizing the posterior of partitions in infinite mixture models. Often in applications, the posterior of the partition is quite diffuse; thus, the default MAP estimate is unsatisfactory. The proposed approach is based on the cumulative block sizes, which counts the number of clusters of size $\\ge k$, for $k=1, \u2026,n$. They also examine the projected cumulative block sizes, when the partition is projected onto a subset of $\\\\{1,...,n\\\\}$. These quantities are summarized by the cumulative occurrence distribution, the per element information of a set, the entropy, the projected entropy, and the subset occurrence. Finally, they propose using an agglomerative clustering algorithm where the projection entropy is used to measure distances between sets. In illustrations, the posterior of the partition is summarized by the dendrogram produced from the entropy agglomerative algorithm, along with existing summaries such as the posterior histogram of the number of clusters and the pairwise occurrences. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5093-summary-statistics-for-partitionings-and-feature-allocations"
    },
    "1471": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LAG13",
        "transcript": "The paper addresses the problem of finding a policy with a high expected return and a bounded variance. The paper considers both the discounted and the average reward cases. The authors propose formulate this problem as a constrained optimization problem, where the gradient of the Lagrangian dual function is estimated form samples. This gradient is composed of the gradient of the expected return and the gradient of the expected squared return. Both gradients need to be estimated in every state. The authors use a linear function approximation to generalize the gradient estimates to states that were not encountered in the samples. The authors use stochastic perturbation to evaluate the gradients in particular states by sampling two trajectories, one with policy parameters theta and another with policy parameters theta+beta, where beta is a perturbation random variable. The policy parameters are updated in an actor-critic scheme. The authors prove that the proposed optimization method converges to a local optimum. Numerical experiments on a traffic lights control problem show that the proposed technique finds a policy with a slightly higher risque than the optimal solution, but with a significantly lower variance. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4917-actor-critic-algorithms-for-risk-sensitive-mdps"
    },
    "1472": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DaiEL13",
        "transcript": "This paper presents a generative model for natural image patches which takes into account occlusions and the translation invariance of features. The model consists of a set of masks and a set of features which can be translated throughout the patch. Given a set of translations for the masks and features the patch is then generated by sampling (conditionally) independent Gaussian noise. An inference framework for the parameters is proposed and is demonstrated on synthetic data with convincing results. Additionally, experiments are run on natural image patches and the method learns a set of masks and features for natural images. When combined together the resulting receptive fields look mostly like Gabors, but some of them have a globular structures. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5195-what-are-the-invariant-occlusive-components-of-image-patches-a-probabilistic-generative-approach"
    },
    "1473": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ShottonSKNWC13",
        "transcript": "This paper revisits the idea of decision DAGs for classification. Unlike a decision tree, a decision DAG is able to merge nodes at each layer, preventing the tree from growing exponentially with depth. This represents an alternative to decision-trees utilizing pruning methods as a means of controlling model size and preventing overfitting. The paper casts learning with this model as an empirical risk minimization problem, where the idea is to learn both the DAG structure along with the split parameters of each node. Two algorithms are presented to learn the structure and parameters in a greedy layer-wise manner using an information-gain based objective. Compared to several baseline approaches using ensembles of fixed-size decision trees, ensembles of decision DAGs seem to provide improved generalization performance for a given model size (as measured by the total number of nodes in the ensemble). ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5199-decision-jungles-compact-and-rich-models-for-classification"
    },
    "1474": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LuxburgA13",
        "transcript": "A method of estimating a density (up to constants) from an unweighted, directed k nearest neighbor graph is described. It is assumed (more or less) that the density is continuously differentiable, supported on a compact and connected subset of $R^d$ with non-empty interior and a smooth boundary, and is upper- and lower-bounded on its support. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5112-density-estimation-from-unweighted-k-nearest-neighbor-graphs-a-roadmap"
    },
    "1475": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LevineK13",
        "transcript": "The paper introduces a new approach of how classical policy search can be combined and improved with trajectory optimization methods serving as exploration strategy. An optimization criteria with the goal of finding optimal policy parameters is decomposed with a variational approach. The variational distribution is approximated as Gaussian distribution which allows a solution with the iterative LQR algorithm. The overall algorithm uses expectation maximization to iterate between minimizing the KL divergence of the variational decomposition and maximizing the lower bound with respect to the policy parameters. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5178-variational-policy-search-via-trajectory-optimization"
    },
    "1476": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MillerH13",
        "transcript": "This paper addresses one simple but potentially very important point: That Dirichlet process mixture models can be inconsistent in the number of mixture components that they infer. This is important because DPs are nowadays widely used in various types of statistical modeling, for example when building clustering type algorithms. This can have real-world implications, for example when clustering breast cancer data with the aim of identifying distinct disease subtypes. Such subtypes are used in clinical practice to inform treatment, so identifying the correct number of clusters (and hence subtypes) has a very important real-world impact. \n\nThe paper focuses on proofs concerning two specific cases where the DP turns out to be inconsistent. Both consider the case of the \"standard normal DPM\", where the likelihood is a univariate normal distribution with unit variance, the mean of which is subject to a normal prior with unit variance. The first proof shows that, if the data are drawn i.i.d. from a zero-mean, unit-variance normal (hence matching the assumed DPM model), $P(T=1 | \\text{data})$ does not converge to 1. The second proof takes this further, demonstrating that in fact$ P(T=1 | \\text{data}) -> 0 $",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4880-a-simple-example-of-dirichlet-process-mixture-inconsistency-for-the-number-of-components"
    },
    "1477": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HermansS13",
        "transcript": "The authors propose a new deep architecture, which combines the hierarchy of deep learning with time-series modeling known from HMMs or recurrent neural networks. The proposed training algorithm builds the network layer-by-layer using supervised (pre-)training a next-letter prediction objective. The experiments demonstrate that after training very large networks for about 10 days, the network performance on a Wikipedia dataset published by Hinton et al. improves over previous work. The authors then proceed to analyze and discuss details of how the network approaches its task. For example, long-term dependencies are modeled in higher layers, correspondence between opening and closing parenthesis are modeled as a \u201cpseudo-stable attractor-like state\u201d. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5166-training-and-analysing-deep-recurrent-neural-networks"
    },
    "1478": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/WangCSX13",
        "transcript": "The authors propose to accelerate the stochastic gradient optimization algorithm by reducing the variance of the noisy gradient estimate by using the 'control variate' trick (a standard variance reduction technique for Monte Carlo simulations, explained in [3] for example). The control variate is a vector which hopefully has high correlation with the noisy gradient but for which the expectation is easier to compute. Standard convergence rates for stochastic gradient optimization depend on the variance of the gradient estimates, and thus a variance reduction technique should yield an acceleration of convergence. The authors give examples of control variates by using Taylor approximations of the gradient estimate for the optimization problem arising in regularized logistic regression as well as for MAP estimation for the latent Dirichlet Allocation (LDA) model. They compare constant step-size SGD with and without variance reduction for logistic regression on the covtype dataset, claiming that the variance reduction allows to use bigger step-sizes without having the problem of high variance and thus yields faster empirical convergence. For LDA, they compare the adaptive step-size version of the stochastic optimization method of [10] with and without variance reduction, showing a faster convergence on the held-out test log-likelihood on three large corpora. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5034-variance-reduction-for-stochastic-gradient-optimization"
    },
    "1479": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/Shi13",
        "transcript": "This paper presents a model inspired by the SAGE (Sparse Additive GEnerative) model of Eisenstein et al. The authors use a different approach for modeling the \"background\" component of the model. SAGE uses the same background model for all; the authors allow different backgrounds for different topics/classification labels/etc., but try to keep the background matrix low rank. To make inference faster when using this low rank constraint, they use a bound on the likelihood function that avoids the log-sum-exp calculations from SAGE. Experimental results are positive for a few different tasks.\n\nSparse additive models represent sets of distributions over large vocabularies as log-linear combinations of a dense, shared background vector and a sparse, distribution-specific vector. The paper presents a modification that allows distributions to have distinct background vectors, but requires that the matrix of background vectors be low-rank. This method leads to better predictive performance in a labeled classification task and in a mixed-membership LDA-like setting. \n\nPrevious work on SAGE introduced a new model for text. It built a lexical distribution by adding deviation components to a fixed background. The model presented in this paper SAM-LRB, builds on SAGE and claims to improve it by two additions. First, providing a unique background for each class/topic. Second, providing an approximation of log-likelihood so as to provide a faster learning and inference algorithm in comparison to SAGE. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5139-sparse-additive-text-models-with-low-rank-background"
    },
    "1480": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SimonyanVZ13",
        "transcript": "The paper proposes a new image representation for recognition based on a stacking of two layers of Fisher vector encoders, with the first layer capturing semi-local information and the second performing sum-pooling aggregation over the entire picture. The approach is inspired by the recent success of deep convolutional networks (CNN). The key-difference is that the architecture proposed in this paper is predominantly hand-designed with relatively few parameters learned compared to CNNs. This is both the strength and the weakness of the approach as it leads to much faster training but also slighter lower accuracy compared to fully learned deep networks. \n\nThis paper uses Fisher Vectors as inner building blocks in a recognition architecture. The basic Fisher vector module had previously demonstrated superior performance in recognition application. Here, it is augmented with discriminative linear projection for dimensionality reduction, and multiscale local pooling, to make it suitable for stacking. Inputs of all layers are jointly used for classification.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4926-deep-fisher-networks-for-large-scale-image-classification"
    },
    "1481": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/PetersJS13",
        "transcript": "This paper considers a class of structural equation models for times series data.  The models allow nonlinear instantaneous effects and lagged effects. On the other hand, Granger-causality based methods do not allow instantaneous effects and a linear non-Gaussian method TS-LiNGAM (Hyvarinen et al., ICML2008, JMLR2010) assumes linear effects. \n\nThis paper introduces a model and procedure for learning instantaneous and lagged causal relationships among variables in a time series when each causal relationship is either identifiable in the sense of the additive noise model (Hoyer et al. 2009) or exhibits a time structure. The learning procedure finds a causal order by iteratively fitting VAR or GAM models where each variable is a function of all other variables and making the variable with the least dependence the lowest variable in the order. Excess parents are then pruned to produce the summary causal graph (where x->y indicates either an instantaneous or lagged cause up to the order of the VAR or GAM model that is fit). Experiments show that the method outperforms competing methods and returns no results in cases where the model can be identified (rather than wrong results). ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5063-causal-inference-on-time-series-using-restricted-structural-equation-models"
    },
    "1482": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DanielyLS13",
        "transcript": "This paper provides one of the most natural examples of a learning problem for which the problem becomes computationally tractable when given a sufficient amount of data, but is computationally intractable (though still information theoretically tractable) when given a smaller quantity of data. This computational intractability is based on a complexity-theoretic assumption about the hardness of distinguishing satisfiable 3SAT formulas from random ones at a given clause density (more specifically, the 3MAJ variant of the conjecture). \n\nThe specific problem considered by the authors is learning halfspaces over 3-sparse vectors. The authors complement their negative results with nearly matching positive results (if one believes a significantly stronger complexity theoretic conjecture-- that hardness persists even for random formulae whose density is $n^\\mu$ over the satistfiability threshold). Sadly, the algorithmic results are described in the Appendix, and are not discussed. It seems like they are essentially modifications of Hazan et al.'s 2012, though it would be greatly appreciated if the authors included a high-level discussion of the algorithm. Even if no formal proofs of correctness will fit in the body, a description of the algorithm would be helpful. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4905-more-data-speeds-up-training-time-in-learning-halfspaces-over-sparse-vectors"
    },
    "1483": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/BareinboimP14",
        "transcript": "Previously it has been shown that do-calculus is a sound inferential machinery for estimating a causal effect from a causal diagram and a set of observations and interventions. This paper further proves that it is not only sound, but also complete, meaning that every valid equality between probabilities defined on a semi-Markovian graph can be obtained through finite applications of the three rules of do-calculus. Moreover, the paper studies mz-transportability, which unifies those previously studied special cases of meta-identifiability. The authors proposed a complete algorithm to determine if a causal effect is mz-transportable, and if it is, outputs a transport formula for estimating the causal effect. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5536-transportability-from-multiple-environments-with-limited-experiments-completeness-results"
    },
    "1484": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/FioriSVMS13",
        "transcript": "This paper examines the problem of approximate graph matching (isomorphism). Given graphs G, H with p nodes, represented by respective adjacency matrices A, B, Find a permutation matrix P that best \"matches\" AP and PB. \n\nThis paper poses the multimodal graph matching problem as a convex optimization problem, and solves it using augmented Langrangian techniques (viz., ADMM). This is an important problem with application in several fields. Experimental results on synthetic and multiple real world datasets demonstrate effectiveness of the proposed approach.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4925-robust-multimodal-graph-matching-sparse-coding-meets-graph-matching"
    },
    "1485": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/YuHSZ13",
        "transcript": "This paper proposes an image-based model for visual clutter perception (\"a crowded, disorderly state\"). For a given image, the model begins by applying an existing superpixel clustering then computing the intensity, colour and orientation histograms of pixels within each superpixel. Boundaries between adjacent superpixels are then retained or merged to create \"proto-objects\". The novel merging algorithm acts on the Earth Movers Distance (EMD), a measure of the similarity between two histograms. The distribution of histogram distances in each image for each image feature is modeled as a mixture of two Weibull distributions. The crossover point between the two distributions (or a fixed cumulative percentile if a single distribution is preferred by model selection) is used as the threshold point for merging: an edge is labelled ``similar'', and the superpixels merged, if the pair of superpixels exceed the threshold point for all three features. The clutter value for each image is the ratio of the final number of proto-objects to the initial number of superpixels (i.e. 0 = no proto-objects, not cluttered; 1 = all superpixels are proto-objects). \nThe model is validated by comparing to human clutter rankings of a subset of an existing image database. Human observers rank images from least to most cluttered, then the median ranking for each image is used as the ground truth for clutter perception. The new model correlates more highly with human rankings of clutter than a number of previous clutter perception and image segmentation models (including human object segmentation from a previous study). ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5201-modeling-clutter-perception-using-parametric-proto-object-partitioning"
    },
    "1486": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/TolstikhinS13",
        "transcript": "This paper derives a new empirical PAC Bayesian bound by combining an existing (non-empirical) PAC Bayesian Berstein bound (i.e., involving the true variance of the loss values) with a PAC Bayesian analysis of the concentration of the empirical variance around its true value. This new bound has the advantage of being tighter when the empirical variance is small compared to the empirical loss. Experiments on real and empirical data with simple models compare the new bound with the usual empirical PAC Bayesian bound confirming the advantage. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4903-pac-bayes-empirical-bernstein-inequality"
    },
    "1487": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MacDermedI13",
        "transcript": "This paper proposes a new method for Dec-POMDP planning that is built out of several components. The first is a new way of solving cooperative Bayesian games using an integer linear program. The second is the transformation of the Dec-POMDP to a belief POMDP in which a \"centralized mediator\" must select at each timestep the best action for each agent-belief pair. The third is to automate the discovery of optimal belief compression by dividing each timestep into two parts, the first corresponding to the original Dec-POMDP and the second giving each agent a chance to select how its beliefs in that timestep are mapped to a bounded set and thus compressed. The fourth assembles these components together into a point-based value iteration method that solves the resulting belief POMDP using a varient of PERSEUS in which the CBG solver is used to compute maximizations. \n\nThree contributions are made: \n\n* An approach to convert DEC-POMDPs to bounded belief DEC-POMDPs \n* An approach to convert bounded belief DEC-POMDPs to POMDPs with exponentially many actions \n* An integer linear program to optimize one-step look-ahead policies in POMDPs with exponentially many actions ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5144-point-based-value-iteration-with-optimal-belief-compression-for-dec-pomdps"
    },
    "1488": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/Yu13",
        "transcript": "The paper deals with an interesting theoretical question concerning the proximity operator. It investigates when the proximity of the sum of two convex functions decomposes into the composition of the corresponding proximity operators. The problem is interesting since in the applications there is a growing interest in building complex regularizers by adding several simple terms. \nThey pursues a quite complete study. After proving a simple sufficient condition (Theorem 1), they gives the main result of the paper (Theorem 4): it is a complete characterization of the property (for a function) of being radial versus the property of being \"well-coupled\" with positively homogeneous functions (where well-coupled means that the prox of the sum of the couple decomposes into the composition of the two individual prox map). They also consider the case of polyhedral gauge functions, deriving a sufficient condition which is expressed by means of a cone invariance property. Examples are provided which show several prox-decomposition results, recovering known facts (in a simpler way) but also proving new ones. \n\nThe value of the paper is mainly on the theoretical side. It sheds light on the mechanism of composing proximity operators and unifies several particular results that were spread in the literature. The article is well written and technically sound. The only fault I see is that perhaps some times is not completely rigorous as I explain in the following. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4863-on-decomposing-the-proximal-map"
    },
    "1489": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhangYS13",
        "transcript": "The authors build their work on top of the generalized conditional gradient (GCG) method for sparse optimization. In particular, GCG methods require computation of the polar operator for the sparse regularization function (an example is the dual norm if the regularization function is an atomic norm). In this work, the authors identify a class of regularization functions, which are based on an underlying subset cost function. The key idea is a to 'lift' the regularizer into a higher dimensional space together with some constraints in the higher-dimensional space, where it has the property of 'marginalized modularity' allowing it to be reformulated as a linear program. Finally, the approach is generalized to general proximal objectives. The results demonstrate that the method is able to achieve better objective values in much less CPU time when compared with another polar operator method and accelerated proximal gradient (APG) on group Lasso and path coding problems. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4935-polar-operators-for-structured-sparse-estimation"
    },
    "1490": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SoufianiDLP13",
        "transcript": "This paper is related with the problem of demand estimation in multi-heterogeneous agents, specifically, to classify agents and estimate preferences of each agent type using agents\u2019 ranking data of different alternatives. The problem is important since it has great practical value in studying underlying preference distributions of multiple agents. To tackle the problem, the authors introduce generalized random utility models (GRUM), provide RJMCMC algorithms for parameter estimation in GRUM and theoretically establish conditions for identifiability for the model. Experimental results on both synthetic and real dataset show the model\u2019s effectiveness. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4998-generalized-random-utility-models-with-multiple-types"
    },
    "1491": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/WangXL13",
        "transcript": "This paper proposes a new subspace clustering algorithm called Low Rank Sparse Subspace Clustering (LRSSC) and aims to study the conditions under which it is guaranteed to produce a correct clustering. The correctness is defined in terms of two properties. The self-expressiveness property (SEP) captures whether a data point is expressed as a linear combination of other points in the same subspace. The graph connectivity property (GCP) captures whether the points in one subspace form a connected component of the graph formed by all the data points. The LRSSC algorithm builds on two existing subspace clustering algorithms, SSC and LRR, which have complementary properties. The solution of LRR is guaranteed to satisfy the SEP under the strong assumption of independent subspaces and the GCP under weak assumptions (shown in this paper). On the other hand, the solution of SSC is guaranteed to satisfy the SEP under milder conditions, even with noisy data or data corrupted with outliers, but the solution of SSC need not satisfy the GCP. This paper combines the objective functions of both methods with the hope of obtaining a method that satisfies both SEP and GEP for some range of values of the relative weight between the two objective functions. Theorem 1 derives conditions under which LRSSC satisfies SEP in the deterministic case. These conditions are natural generalizations of existing conditions for SSC. But they are actually weaker than existing conditions. Theorem 2 derives conditions under which LRSSC satisfies SEP in the random case (data drawn at random from randomly drawn subspaces). Overall, it is shown that when the weight of the SSC term is large enough and the ratio of the data dimension to the subspace dimension grows with the log of the number of points, then LRSSC is guaranteed to satisfy SEP with high probability. I say high, because it does not tend to 1. Finally, Proposition 1 and Lemma 4 show that LRR satisfies GCP (presumably almost surely). Experiments support that for a range of the SSC weight, LRSCC works. Additional experiments on model selection show the usefulness of the analysis. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4865-provable-subspace-clustering-when-lrr-meets-ssc"
    },
    "1492": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/BorjiI13",
        "transcript": "The authors explore different optimization strategies for 1-D continuous functions and their relationship to how people optimize the functions. They used a wide variety of continuous functions (with one exception): polynomial, exponential, trigonometric, and the Dirac function. They also explore how people interpolate and extrapolate noisy samples from a latent function (which has a long tradition in psychology under the name of function learning) and how people select an additional sample to observe under the task of interpolating or extrapolating. Over all, they found that Gaussian processes do a better job at describing human performance than any of the approx. 20 other tested optimization methods. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4952-bayesian-optimization-explains-human-active-search"
    },
    "1493": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/RohrbachES13",
        "transcript": "This paper describes how to attack the zero-, one-, or few-shot recognition problem, where we have a fair amount of training data for some classes, but none or very few for some other classes. It does this using three different techniques, all combined in a single framework: using semantically-meaningful mid-layer knowledge (attributes), building a graph on new classes to exploit the manifold structure, and finally by using an attribute-based representation for building the graph structure (rather than low-level features), which improves performance. The method is evaluated on 3 different datasets (Animals with Attributes, ImageNet, and MPII Cooking composites), and shows improved performance on all compared to the state-of-the-art (slightly).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5209-transfer-learning-in-a-transductive-setting"
    },
    "1494": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MevissenRY13",
        "transcript": "The authors considered robust optimization for polynomial optimization problems where the uncertainty set is a set of possible distributions of the parameter. In specific, this set is a ball around a density function estimated from data samples. The authors showed that this distributionally robust optimization formulation can be reduced to a polynomial optimization problem, hence computationally the robust counterpart is of the same hardness as the nominal (non-robust) problem, and can be solved using a tower of SDP known in literature. The authors also provide finite-sample guarantees for estimating the uncertainty set from data. Finally, they applied their methods to a water network problem. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/4943-data-driven-distributionally-robust-polynomial-optimization"
    },
    "1495": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhouLVM13",
        "transcript": "This work proposes an extension to the maximum margin clustering (MMC) method that introduces latent variables. The motivation for adding latent variables is that they can model additional data semantics, resulting in better final clusters. The authors introduce a latent MMC (LMMC) objective, state how to optimize it, and then apply it to the task of video clustering. For this task, the latent variables are tag words, and the affinity of a video for a tag is given by a pre-trained binary tag detector. Experiments show that LMMC consistently, and sometimes substantially, beats several reasonable baselines. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5078-latent-maximum-margin-clustering"
    },
    "1496": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhangW13",
        "transcript": "The authors show by approximate analysis of two identical continuous attractor networks (Zhang 1996), reciprocally coupled by Gaussian weights, that such a network can approximately implement the Bayesian posterior solution for queue integration. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5055-reciprocally-coupled-local-estimators-implement-bayesian-information-integration-distributively"
    },
    "1497": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/PerinaJBT13",
        "transcript": "This paper describes a creative alternative for topic modeling:  mixed-membership on a \"counting grid.\" The advantage of this approach  seems to be that you can move smoothly across the grid, achieving a \nhigh effective number of topics while the spatial smoothing prevents  overfitting. The disadvantage seems to be that there are more  parameters (grid dimension and size, and window size). A variational  inference procedure that is somewhat to LDA is possible, although no speed/complexity comparisons are provided. The spatial nature of the  approach has potential advantages for visualization as well.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5140-documents-as-multiple-overlapping-windows-into-grids-of-counts"
    },
    "1498": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/Lopez-PazHS13",
        "transcript": "The authors propose a non-linear measure of dependence between two random variables. This turns out to be the canonical correlation between random, nonlinear projections of the variables after a copula transformation which renders the marginals of the r.vs invariant to linear transformations. \n\nThe paper introduces a new method called RDC to measure the statistical dependence between random variables. It combines a copula transform to a variant of kernel CCA using random projections, resulting in a $O(n log n)$ complexity. Results on synthetic and real benchmark data show promising results for feature selection. \n\nThe RDC is a non-linear dependency estimator that satisfies Renyi's criteria and exploits the very recent FastFood speedup trick (ICML13) \\cite{journals/corr/LeSS14}. This is a straightforward recipe: 1) copularize the data, effectively preserving the dependency structure while ignoring the marginals, 2) sample k non-linear features of each datum (inspired from Bochner's theorem) and 3) solve the regular CCA eigenvalue problem on the resulting paired datasets. Ultimately, RDC feels like a copularised variation of kCCA (misleading as this may sound). Its efficiency is illustrated successfully on a set of classical non-linear bivariate dependency scenarios and 12 real datasets via a forward feature selection procedure. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5138-the-randomized-dependence-coefficient"
    },
    "1499": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/GardnerMGWBC15",
        "transcript": "The authors introduce a new method for actively selecting the model that best fits a dataset. Contrary to active learning, where the next learning point is chosen to get a better estimate of the model hyperparameters, this methods selects the next point to better distinguish between a set of models. Similar active model selection techniques exist, but they need to retrain each model for each new data point to evaluate. The strength of the author's method is that is only requires to evaluate the predictive distributions of models, without retraining.\n\nThey propose to apply this method to detect noise-induced hearing loss. The traditional way of screening for NIHL involves testing a wide range of intensities and frequencies, which is time consuming. The authors show that with their method, the number of tests to be run could be drastically decreased, reducing the cost of large-scale screenings for NIHL.\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5871-bayesian-active-model-selection-with-an-application-to-automated-audiometry"
    },
    "1500": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SrivastavaGS15",
        "transcript": "Machine learning researchers frequently find that they get better results by adding more and more layers to their neural networks, but the difficulties of initialization and decaying/exploding gradients have been severely limiting. Indeed, the difficulties of getting information to flow through deep neural networks arguably kept them out of widespread use for 30 years. This paper addresses this problem head on and demonstrates one method for training 100 layer nets.\n\nThe paper describes an affective method to train very deep neural networks by means of 'information highways', or building direct connections to upper network layers. Although a generalization of prior techniques, such as cross-layer connections, the authors have shown this method to be effective by experimentation. The contributions are quite novel and well supported by experimental evidence.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5850-training-very-deep-networks"
    },
    "1501": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/TripuraneniGGG15",
        "transcript": "The paper proposes a sampler for iHMMs, which the authors show has improved mixing properties and performs better in posterior inference problems when compared to the existing state-of-the-art sampling methods. An existing Gibbs sampler is turned into a particle Gibbs sampler by using a conditional SMC step to sample the latent sequence of states. The paper uses conjugacy to derive optimal SMC proposals and ancestor sampling to improve the performance of the conditional SMC step. The result is more efficient sampling of the latent states, making the sampler robust to spurious states and yielding faster convergence.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5968-particle-gibbs-for-infinite-hidden-markov-models"
    },
    "1502": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/KhalvatiR15",
        "transcript": "The authors' model confidence data from two experiments (conducted by others and previously published in the scientific literature) using a POMDP. In both experiments, subjects saw a random-dot kinematogram on each trial and made a binary choice about the dominant motion direction. The first experiment used monkeys as subjects and stimuli had a fixed duration. The second experiment used people as subjects and stimuli continued until a subject made a response. The paper reports that the POMDP model does a good job of fitting the experimental data, both the accuracy data and the confidence data.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5659-a-bayesian-framework-for-modeling-confidence-in-perceptual-decision-making"
    },
    "1503": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/NeyshaburSS15",
        "transcript": "Deep rectified neural networks are over-parameterized in the sense that scaling of the weights in one layer, can be compensated for exactly in the subsequent layer. This paper introduces Path-SGD, a simple modification to the SGD update rule, whose update is invariant to such rescaling. The method is derived from the proximal form of gradient descent, whereby a constraint term is added which preserves the norm of the \"product weight\" formed along each path in the network (from input to output node). Path-SGD is thus principled and shown to yield faster convergence for a standard 2 layer rectifier network, across a variety of dataset (MNIST, CIFAR-10, CIFAR-100, SVHN). As the method implicitly regularizes the neural weights, this also translates to better generalization performance on half of the datasets.\n\nAt its core, Path-SGD belongs to the family of learning algorithms which aim to be invariant to model reparametrizations. This is the central tenet of Amari's natural gradient (NG) \\cite{amari_natural_1998}, whose importance has resurfaced in the area of deep learning. Path-SGD can thus be cast an approximation to NG, which focuses on a particular type of rescaling between neighboring layers. The paper would greatly benefit from such a discussion in my opinion. I also believe NG to be a much more direct way to motivate Path-SGD, than the heuristics of max-norm regularization.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5797-path-sgd-path-normalized-optimization-in-deep-neural-networks"
    },
    "1504": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/FromeCSBDRM13",
        "transcript": "This computer vision paper uses an unsupervised, neural net based semantic embedding of a Wikipaedia text corpus trained using skip-gram coding to enhance the performance of the Krizhevsky et al deep network \\cite{krizhevsky2012imagenet} that won the 2012 ImageNet large scale visual recognition challenge, particularly for zero-shot learning problems (i.e. previously unseen classes with some similarity to previously seen ones). The two networks are trained separately, then the output layer of \\cite{krizhevsky2012imagenet} is replaced with a linear mapping to the semantic text representation and re-trained on ImageNet 1k using a dot product loss reminiscent of a structured output SVM one. The text representation is not currently re-trained. The model is tested on ImageNet 1k and 21k. With the semantic embedding output it does not quite manage to reproduce the ImageNet 1k flat-class hit rates of the original softmax-output model, but it does better than the original on hierarchical-class hit rates and on previously unseen classes from ImageNet 21k. For unseen classes, the improvements are modest in absolute terms (albeit somewhat larger in relative ones). \n\nIt consists of the following steps: \n1. Learn an embedding of a large number of words in a Euclidean space. \n2. Learn a deep architecture which takes images as input and predicts one of 1,000 object categories. \nThe 1,000 categories are a subset of the 'large number of words' of step (1). \n3. Remove the last layer of the visual model -- leaving what is referred to as the 'core' visual model. \nReplace it by the word embeddings and add a layer to map the core visual model output to the word embeddings. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model"
    },
    "1505": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/BengioYAV13",
        "transcript": "This paper continues a recent line of theoretical work that seeks to explain what autoencoders learn about the data-generating distribution. Of practical importance from this work have been ways to sample from autoencoders. Specifically, this paper picks up where \\cite{journals/jmlr/AlainB14} left off. That paper was able to show that autoencoders (under a number of conditions) estimate the score (derivative of the log-density) of the data-generating distribution in a way that was proportional to the difference between reconstruction and input. However, it was these conditions that limited this work: it only considered Gaussian corruption, it only applied to continuous inputs, it was proven for only squared error, and was valid only in the limit of small corruption. The current paper connects the autoencoder training procedure to the implicit estimation of the data-generating distribution for arbitrary corruption, arbitrary reconstruction loss, and can handle both discrete and continuous variables for non-infinitesimal corruption noise. Moreover, the paper presents a new training algorithm called \"walkback\" which estimates the same distribution as the \"vanilla\" denoising algorithm, but, as experimental evidence suggests, may do so in a more efficient way.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models"
    },
    "1506": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/XuRLJ14",
        "transcript": "This paper presents a method for nonblind deconvolution of blurry images, that also can also fix artifacts (e.g. compression, clipping) in the input, and is robust to deviations from the input generation model. A convolutional network is used both to deblur and fix artifacts; deblurring is performed using a sequence of horizontal and vertical conv kernels, taking advantage of a high degree of separability in the pseudoinverse blur kernel, and are initialized with a decomposition of the pseudoinverse. A standard compact-kernel convnet is stacked on top, allowing further fixing of artifacts and noise, and traned end-to-end with pairs of blurry and ground truth images.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5485-deep-convolutional-neural-network-for-image-deconvolution"
    },
    "1507": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/DentonZBLF14",
        "transcript": "The paper addresses the problem of speeding up the evaluation of pre-trained image classification ConvNets. To this end, a number of techniques are proposed, which are based on the tensor representation of the conv. layer weight matrix. Namely, the following techniques are considered (Sect. 3.2-3.5):\n\n1. SVD decomposition of the tensor\n2. outer product decomposition of the tensor\n3. monochromatic approximation of the first conv. layer - projecting RGB colors to a 1-D space, followed by clustering\n4. biclustering tensor approximation - clustering input and output features to split the tensor into a number of sub-tensors, each of which is then separately approximated\n5. fine-tuning of approximate models to (partially) recover the lost accuracy",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation"
    },
    "1508": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SimonyanZ14",
        "transcript": "This paper proposes a model for solving discriminative tasks with video inputs. The\nmodel consists of two convolutional nets. The input to one net is an appearance\nframe. The input to the second net is a stack of densely computed optical flow\nfeatures. Each pathway is trained separately to classify its input. The\nprediction for a video is obtained by taking a (weighted) average of the\npredictions made by each net.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos"
    },
    "1509": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/LiASY14",
        "transcript": "This paper presents improvements on a system for large-scale learning known as \"parameter server\". The parameter server is designed to perform reliable distributed machine learning in large-scale industrial systems (1000's of nodes). The architecture is based on a bipartite graph composed by \"servers\" and \"workers\". Workers compute gradients based on subsets of the training instances, while servers aggregate the workers' gradients, update the shared parameter vector and redistribute it to the workers for the next iteration. The architecture is based on asynchronous communication and allows trading-off convergence speed and accuracy through a flexible consistency model. The optimization problem is solved with a modified proximal gradient method, in which only blocks of coordinates are updated at a time. Results are shown in an ad-click prediction dataset with $O(10^{11})$ instances as well as features. Results are presented both in terms of convergence time of the algorithm and average time spent per worker. Both are roughly half of the values for the previous version of the parameter server (version called \"B\" in the paper). Roughly 1h convergence time using 1000 machines each with 16 cores and 192Gb RAM, 10Gb Ethernet connection (800 workers and 200 servers). Other jobs were concurrently run in the cluster. The authors claim it was not possible to compare against other algorithms since at the scale they are operating there is no other open-source solution. In the supplementary material they do compare their system with shotgun \\cite{conf/icml/BradleyKBG11} and obtain faster convergence (4x) and similar value of the objective function at convergence. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5597-communication-efficient-distributed-machine-learning-with-the-parameter-server"
    },
    "1510": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhangS14",
        "transcript": "This paper proposes a way to speed up Hamiltonian Monte Carlo (HMC) \\cite{Duane1987216} sampling for hierarchical models. It is similar in spirit to RMHMC, in which the mass matrix varies according to local topology, except that here the mass matrices for each parameter type (parameter or hyperparameter) only depend on their counterpart, which allows an explicit leapfrog integrator to be used to simulate dynamics rather than an implicit integrator requiring fixed-point iteration to convergence for each step. The authors point out that their method goes beyond straightforward Gibbs sampling with HMC within each Gibbs step since their method leaves the counterpart parameter's momentum intact.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5591-semi-separable-hamiltonian-monte-carlo-for-inference-in-bayesian-hierarchical-models"
    },
    "1511": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MuandetSS14",
        "transcript": "The paper presents a family of kernel mean shrinkage estimators. These estimators generalize the ones proposed in \\cite{journals/jmlr/FukumizuSG13} and can incoporate useful domain knowledge through spetral filters. Here is a summary of interesting contributions:\n1. Theorem 1 that shows the consistency and admissibility of kmse presented in \\cite{journals/jmlr/FukumizuSG13}.\n2. The idea of spectral kmse (its use in this unsupervised setting) and similarity of final form with the supervised setting.\n3. Theorem 5 that shows consistency of the proposed spectral kmse.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5239-kernel-mean-estimation-via-spectral-filtering"
    },
    "1512": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/JoulinM15",
        "transcript": "Endowing memory to recurrent neural networks is clearly one of the most important topics of deep learning and crucial to do real reasoning. The proposed stack-augmented recurrent nets outperform simple RNN and LSTM \\cite{journals/neco/HochreiterS97} on a series of synthetic problems (learning simple algorithmic patterns). The complexity of problems is clearly defined and the behavior of resulting stack RNN could be well understood and easily analyzed. However, the conclusions merely depending on those synthetic data set may take a risk. The importance of the problems to real sequence modeling task could be uncertain and the failures of other models could be greatly improved by more and dense hyper-parameter searching. Like in \\cite{journals/corr/LeJH15}, by a very simple trick a RNN works very well on a toy task (a adding problem) which seems to need to model long term dependencies. ",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets"
    },
    "1513": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MahsereciH15",
        "transcript": "The authors propose a probabilistic version of the \"line search\" procedure that is commonly used as a subroutine in many deterministic optimization algorithms. The new technique can be applied when the evaluations of the objective function and its gradients are corrupted by noise. Therefore, the proposed method can be successfully used in stochastic optimization problems, eliminating the requirement of having to specify a learning rate parameter in this type of problems. The proposed method uses a Gaussian process surrogate model for the objective and its gradients. This allows us to obtain a probabilistic version of the conditions commonly used to terminate line searches in the deterministic scenario. The result is a soft version of those conditions that is used to stop the probabilistic line search process. At each iteration within such process, the next evaluation location is collected by using Bayesian optimization methods. A series of experiments with neural networks on the MNIST and CIFAR10 datasets validate the usefulness of the proposed technique.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5753-probabilistic-line-searches-for-stochastic-optimization"
    },
    "1514": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MaystreG15",
        "transcript": "This paper propose a new inference mechanism for the Plackett-Luce model based on the preliminary observation that the ML estimate can be seen as the stationary distribution of a certain Markov chain. In fact, two inferences mechanisms are proposed, one is approximate and consistent, the other converges to the ML estimate but is slower. The authors then debate on the application settings (pairwise preferences, partial rankings). Finally, the authors exhibit three sets of experiments. The first one compares the proposed algorithm to other approximate inference mechanisms for the PL model in terms of statistical efficiency. Then on real-world datasets, one experiment compares the empirical performance of the approximate methods and a second the speed of exact methods to reach a certain level of optimality.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5681-fast-and-accurate-inference-of-plackettluce-models"
    },
    "1515": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/Chakrabarti15",
        "transcript": "The algorithm presented here is simple and interesting. Pixel luminance, chrominance, and illumination chrominance are all histogrammed, and then evaluation is simply each pixel's luminance voting on each pixel's true chrominance for each of the \"memorized\" illuminations. The model can be trained generative by simply counting pixels in the training set, or can be trained end-to-end for a slight performance boost. This algorithm's simplicity and speed are appealing, and additionally it seems like it may be a useful building block for a more sophisticated spatially-varying illumination model.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5864-color-constancy-by-learning-to-predict-chromaticity-from-luminance"
    },
    "1516": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ParkJQSBS15",
        "transcript": "The paper introduces a model which is probabilistic for non linear manifold discovery. It is based on a generative model with missing variables and required a variational EM implementation which is standard but nevertheless technical to derive in this specific context.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5973-bayesian-manifold-learning-the-locally-linear-latent-variable-model-ll-lvm"
    },
    "1517": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ParkBM15",
        "transcript": "This paper describes using an additional time scale over trials to model (slow) non-stationarities. It adds to the successful PLDS model, another gain vector matching the latent dimensions that is constant during each trial. Many neuroscientific datasets indeed show such slow drifts, which could very well be captured by such modeling effort.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5790-unlocking-neural-population-non-stationarities-using-hierarchical-dynamics-models"
    },
    "1518": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/MorgensternR15",
        "transcript": "This paper addresses the problem of learning reserve prices that approximately maximize revenue, using sample draws from an unknown distribution over bidder valuations. The authors introduce t-level auctions, in which (roughly speaking) each bidder's bid space is effectively discretized into levels, and the bidder whose bid falls on the highest level wins and pays the lowest value that falls on its lowest level required to win.\n\nThe authors bound the number of samples needed to find an approximately revenue-maximizing auction from all auctions in a set C (e.g., from the set of 10-level auctions). They bound the difference in revenue between the revenue-maximizing t-level auction and the optimal auction. Results are presented for single-item auctions but are generalized to matroid settings and single-parameter settings.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5766-on-the-pseudo-dimension-of-nearly-optimal-auctions"
    },
    "1519": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/WuYLFT15",
        "transcript": "The authors introduce a novel approach for inferring hidden physical properties of objects (mass and friction), which also allows the system to make subsequent predictions that depend on these properties. They use a black-box generative model (a physics simulator), to perform sampling-based inference, and leverage a tracking algorithm to transform the data into more suitable latent variables (and reduce its dimensionality) as well as a deep model to improve the sampler. The authors assume priors over the hidden physical properties, and make point estimates of the geometry and velocities of objects using a tracking algorithm, which comprise a full specification of the scene that can be input to a physics engine to generate simulated velocities. These simulated velocities then support inference of the hidden properties within an MCMC sampler: the properties' values are proposed and their consequent simulated velocities are generated, which are then scored against the estimated velocities, similar to ABC. A deep network can be trained as a recognition model, from the inferences of the generative model, and also from the Physics 101 dataset directly. Its predictions of the mass and friction can be used to initialize the MCMC sampler.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5780-galileo-perceiving-physical-object-properties-by-integrating-a-physics-engine-with-deep-learning"
    },
    "1520": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/HeY15",
        "transcript": "This paper considers a generalization of the Interactive Submodular Set Cover (ISSC) problem \\cite{conf/icml/GuilloryB10}. In ISSC, the goal is to interactively collect elements until the value of the set of elements, represented by an unknown submodular function, reaches some threshold. In the original ISSC there is a single correct submodular function, which can be revealed using responses to each selected element, and a single desired threshold. This paper proposes to simultaneously require reaching some threshold for all the possible submodular functions. The threshold value is determined as a convex function of a submodular agreement measure between the given function and the responses to all elements. Each element has a cost, and so the goal is to efficiently decide which elements to collect to satisfy the goal at a small cost.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6002-smooth-interactive-submodular-set-cover"
    },
    "1521": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ZhengL15",
        "transcript": "The paper presents results on recovery of low-rank semidefinite matrices from linear measurements, using nonconvex optimization. The approach is inspired by recent work on phase retrieval, and combines spectral initialization with gradient descent. The connection to phase retrieval comes because measurements which are linear in the semidefinite matrix $X = Z Z'$ are quadratic in the factors $Z$. The paper proves recovery results which imply that correct recovery occurs when the number of measurements m is essentially proportional to n $r^2$, where n is the dimensionality and r is the rank. The convergence analysis is based on a form of restricted strong convexity (restricted because there is an $r(r-1)/2$-dimensional set of equivalent solutions along which the objective is flat). This condition also implies linear convergence of the proposed algorithm.\n\nThe implementation seems awful. When compared to recent implementations, e.g. http://arxiv.org/abs/1408.2467 the performance seems orders of magnitude away from the state of the art -- and being an order of magnitude faster than general-purpose SDP solver on the nuclear norm does not make it any better. The authors should acknowledge that and compare the results with other codes on some established benchmark (e.g. Lenna), so as to show that the price in terms of run-time brings about much better performance in terms of objective function values (SNR, RMSE) -- which is plausible, but far from certain.\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5830-a-convergent-gradient-descent-algorithm-for-rank-minimization-and-semidefinite-programming-from-random-linear-measurements"
    },
    "1522": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/SunWKM15",
        "transcript": "The paper presents a data visualisation method based on the concept of space-time. The space-time representation is capable of showing a broader family of proximities than an Euclidean space with the same dimensionality. Based on the KL measure, the authors argue that the lower dimensional representation of the high dimensional data using the space-time local embedding method can keep more information than Euclidean embeddings. I am quite convinced, but there is one question about interpretability of the visualised data in space-time.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5971-space-time-local-embeddings"
    },
    "1523": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/PanPORRJ15",
        "transcript": "This work addresses an important special case of the correlation clustering problem: Given as input a graph with edges labeled -1 (disagreement) or +1 (agreement), the goal is to decompose the graph so as to maximize agreement within components. Building on recent work \\cite{conf/kdd/BonchiGL14} \\cite{conf/kdd/ChierichettiDK14}, this paper contributes two concurrent algorithms, a proof of their approximation ratio, a run-time analysis as well as a set of experiments which demonstrate convincingly the advantage of the proposed algorithms over the state of the art.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5814-parallel-correlation-clustering-on-big-graphs"
    },
    "1524": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ParkK15",
        "transcript": "The paper attacks the problem of describing a sequence of images from blog-posts with a sequence of consistent sentences. For this the paper proposes to first retrieve the K=5 most similar images and associated sentences from the training set for each query image. The main contribution of the paper lies in defining a way to select the most relevant sentences for the query image sequence, providing a coherent description. For this sentences are first embedded in a vector and then the sequence of sentences is modeled with a bidirectional LSTM. The output of the bi-directional LSTM is first fed through a relu \\cite{conf/icml/NairH10} and fully connected layer and then scored with a compatibility score between image and sentence. Additionally a local coherence \u001amodel \\cite{journals/coling/BarzilayL08} is included to enforce the compatibility between sentences.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences"
    },
    "1525": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/YarkonyF15",
        "transcript": "The paper presents a method to obtain a hierarchical clustering of a planar graph by posing the problem as that of approximating a set of edge weights using an ultrametric. This is accomplished by minimizing the $\\ell_2$ norm between the given edge weights and the learnt ultrametric. Learning the ultrametric amounts to estimating a collection of multicuts that satisfies a hierarchical partitioning constraint. An efficient algorithm is presented that solves an approximation based on a finding a linear combination of a subset of possible two-way cuts of the graph.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5802-planar-ultrametrics-for-image-segmentation"
    },
    "1526": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ChoromanskaL15",
        "transcript": "This paper proposes a novel online algorithm for constructing a multiclass classifier that enjoys a time complexity logarithmic in the number of classes k. This is done by constructing online a decision tree which locally maximizes an appropriate novel objective function, which measures the quality of a tree according to a combined \"balancedness\" and \"purity\" score. A theoretical analysis (of a probably intractable algorithm) is provided via a boosting argument (assuming weak learnability), essentially extending the work of Kearns and Mansour (1996) \\cite{conf/stoc/KearnsM96} to the multiclass setup. A concrete algorithm is given to a relaxed problem (but see below) without any guarantees, but quite simple, natural and interesting.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5937-logarithmic-time-online-multiclass-prediction"
    },
    "1527": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/QiuHLC15",
        "transcript": "The authors derive an estimator of a \"proxy\" of the covariance matrix of a stationary stochastic process (in their case asset returns) which is robust to data outliers and does not make assumptions on the tails of the distribution. They show that for elliptical distributions, which includes Gaussians, this proxy is consistent with true covariance matrix up to a scaling factor; and that their proposed estimator of the proxy has bounded error.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5714-robust-portfolio-optimization"
    },
    "1528": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ShangZLS15",
        "transcript": "This paper presents a new method (the \"covariance-controlled adaptive Langevin thermostat\") for MCMC posterior sampling for Bayesian inference. Along the lines of previous work in scalable MCMC, this is a stochastic gradient sampling method. The presented method aims to decrease parameter-dependent noise (in order to speed-up convergence to the given invariant distribution of the Markov chain, and generate beneficial samples more efficiently), while maintaining the desired invariant distribution of the Markov chain. Similar to existing stochastic gradient MCMC methods, this method aims to find use in large-scale machine learning settings (i.e. Bayesian inference with large numbers of observations). Experiments on three models (a normal-gamma model, Bayesian logistic regression, and a discriminative restricted Boltzmann machine) aim to show that the presented method performs better than Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) \\cite{10.1016/0370-2693(87)91197-X} and Stochastic Gradient Nose-Hoover Thermostat (SGNHT), two similar existing methods.\n",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5978-covariance-controlled-adaptive-langevin-thermostat-for-large-scale-bayesian-sampling"
    },
    "1529": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/TsiligkaridisF15",
        "transcript": "This paper introduces ASUGS (adaptive sequential updating and greedy search), building on the previous work on SUGS by Wang & Dunson 2011 \\cite{10.1198/jcgs.2010.07081}, which is a sequential (ie online) MAP inference method for DPMMs.\n\nThe main contribution of the paper is to provide online updating for the concentration parameter, $\\alpha$.\n\nThe paper shows that the posterior distribution on $\\alpha$ can be expected to behave has a gamma distribution (that depends on the current number of clusters and on n) in the large-scale limit, assuming an exponential prior on $\\alpha$.\n\nASUGS uses the mean of this gamma distribution as the $\\alpha$ for updating cluster assignments, the remainder of the algorithm proceeding as in SUGS (ie using conjugacy to update model parameters in an online fashion, with hard assignments of data to clusters.)\n\nThe paper also shows that this choice of \\alpha is bounded by $\\log^\\epsilon n$ for an arbitrarily small $\\epsilon$, so that we may expect this process to converge, or at the very least be stable even in large settings.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6035-adaptive-low-complexity-sequential-inference-for-dirichlet-process-mixture-models"
    },
    "1530": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/Alabdulmohsin15",
        "transcript": "The paper seeks to establish a connection between algorithmic stability and generalization performance. Notions of algorithmic stability have been proposed before and linked to the generalization performance of learning algorithms \\cite{conf/uai/KutinN02} \\cite{journals/neco/KearnsR99} and have also been shown to be crucial for learnability \\cite{journals/jmlr/Shalev-ShwartzSSS10}.\n\n\\cite{PoggioETAL:04} proved that for bounded loss functions, the generalization of ERM is equivalent to the probabilistic leave-one-out stability of the learning algorithm. \\cite{journals/jmlr/Shalev-ShwartzSSS10} then showed that a problem is learnable in Vapnik's general setting of learning iff there exists an asymptotically stability ERM procedure.\n\nThis paper first establishes that for Vapnik's general setting of learning, a probabilistic notion of stability, is necessary and sufficient for the training losses to converge to test losses uniformly for all distributions. The paper then presents some discussions on how this notion of stability can be interpreted to give results in terms of the capacity of the function class or the size of the population.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/6019-algorithmic-stability-and-uniform-generalization"
    },
    "1531": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/RooyenMW15",
        "transcript": "The paper presents a solution to binary classification with symmetric label noise (SLN). They show that, in order to obtain consistency (w.r.t. to the 0-1 loss in the \"noiseless\" case) while using a convex surrogate, one must use the loss $\\ell(v,y) = 1 - vy$ -- the \"unhinged loss\" -- , which is shown to enjoy some useful properties, including robustness to SLN. In a more restricted sense of robustness, it is the only such loss, but in any case it overcomes the limitations of other convex losses for the same problem.\n\nDifferent implications of using the unhinged loss are discussed; the problem of classification with SLN with the unhinged loss and \"linear\" classifiers is investigated and solved analytically. The authors also present an empirical evaluation to motivate that their theoretical considerations have practical impact.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5941-learning-with-symmetric-label-noise-the-importance-of-being-unhinged"
    },
    "1532": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/ShahZ15",
        "transcript": "The paper proposes a payment rule for crowdsourced tasks. This rule is intended to incentivize workers to accurately report their confidence (e.g. by skipping a task when they have low confidence), and to pay little to spammers. Payment is based on the product of the evaluations of a worker's responses to a set of gold-standard tasks; if the worker gets a single gold standard task wrong and asserts high confidence, the overall payment is zero.",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5677-double-or-nothing-multiplicative-incentive-mechanisms-for-crowdsourcing"
    },
    "1533": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=conf/nips/RenHGS15",
        "transcript": "This work proposes a two stage object detection algorithm based on convolutional neural network (CNN). The first stage is region proposal, which is based on the traditional sliding window method but working on the top layer feature map of CNN (RPN). In the second stage, a fast R-CNN is applied to the proposed regions. Since the convolution layers are shared between RPN and R-CNN, and the calculation is speeded up using GPU, the algorithm can achieve near real-time (5fps).",
        "sourceType": "blog",
        "linkToPaper": "http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks"
    },
    "1534": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1016/j.biopsych.2008.06.026",
        "transcript": "Animal model testing the effect of maternal separation on later stress responsiveness and gut microbiome",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1016/j.biopsych.2008.06.026"
    },
    "1535": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1038/nrgastro.2012.44",
        "transcript": "A comprehensive and accessible recent review detailing the methodological advances in the assessment of the gut microbiota",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1038/nrgastro.2012.44"
    },
    "1536": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1371/journal.pbio.0060280",
        "transcript": "Early study showing effect of an antibiotic on gut microbiome and the majority of community recovered within 4 wks",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1371/journal.pbio.0060280"
    },
    "1537": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1097/01.aoa.0000443385.07422.6a",
        "transcript": "An important review exploring the relationship mode of delivery and development of the immune system",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1097/01.aoa.0000443385.07422.6a"
    },
    "1538": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1002/bies.201400071",
        "transcript": "Good review on how microbiome alter host eating and other behaviors",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1002/bies.201400071"
    },
    "1539": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2110.15149",
        "transcript": "Model combination\\ensembling:\nAverage ensembling is practical - but naive.\nCombine considering each network's strengths, much better!\nMoreover, let's make the networks diverse so they will have different strengths.\n\nWenjuan Han & Hwee Tou Ng (no twitters?)\n#enough2skim #NLProc\n\nThe basic idea is quite simple:\nGiven some models, why would we want the average? We want to rely on each one(or group) when it is more likely to be the correct one.\nThis was actually introduced in our previous work (as admitted by the authors) in\naclanthology.org/W19-4414.pdf\nThe paper's addition:\n1. Given a set of black-box models we may train at least one of them to be different from the rest with RL.\n2. we can use more sophisticated NNs to combine the outputs\n3. we can ignore domain knowledge for the combination (I am not sure this is a bonus) \nResults are very strong. Especially nice is that they show that the diversity training indeed helps\n\nMy criticism:\nThe comparisons are always to SoTA, this is meaningless. The authors propose different parts (the diversity, the combination and the combined models).\nIt is unclear whether ensembling after the diversity would be preferable over their's or not. \nSimilarly, they compare to Kantor et al., but Kantor provided a combination method, why not compare on the same models, or combine with Kantor's method the models after the diversity training? \nTo conclude, I really like the direction, and ensembling is a very practical tool that for some reason was not improved in a long time. \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2110.15149"
    },
    "1540": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2110.10668",
        "transcript": "Evaluation of ecaluation for style transfer in multiple languages.\n@ebriakou, @swetaagrawal20, @Tetreault_NLP, @MarineCarpuat\narxiv.org/pdf/2110.10668\u2026\nThey end up with the following best practices:\nCapture formality - XLM-R with regression not classification\nPreservation - with chrf not BLEU\nFluency - XLM-R but there is room for improvement\nSystem Ranking - XLM-R and chrf\nCrosslingual Transfer - rely on zero shot not machine translation \nWhy chrf and not chrf++ I wonder? \n",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2110.10668"
    },
    "1541": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2106.01044",
        "transcript": "Are all language orders as hard?\nSupposedly, for RNNs yes, for Transformers no\n\n@JenniferCWhite @ryandcotterell\naclanthology.org/2021.acl-long.\u2026\n\ngithub.com/rycolab/artifi\u2026 (currently empty)\n#NLProc\nReally cool, with a caveat\nAdapted from: https://twitter.com/LChoshen/status/1450809889106931716?s=20\n\nGitHub - rycolab/artificial-languages\nContribute to rycolab/artificial-languages development by creating an account on GitHub.\nhttps://github.com/rycolab/artificial-languages\nThe paper creates synthetic languages (using a PCFG) with various ordering rules, being able to compare each order.\nImage\nThey also add agreement, and a vocabulary to introduce more of real language important features (e.g. long-distance dependencies) \nLast, the words are taken from words that could have been in English (e.g. daxing) \nTheir results are the Transformers do have inductive biases, but those are not towards the most probable language orders (e.g. VOS is quite easy for Transformers)\nImage\nAnother interesting thing is that RNNs do not show that. Despite all that we know about their strong inductive bias regarding forgetting (hard to relate far away words\\information). \nLast, it seems that a lot of what makes things easy and hard is related to the agreement, at least from looking at which rules matter together.\nImage\nI really like this direction (which they seem to discuss a lot, seems like a reviewer was quite annoying) and the results are interesting.\n\nHowever, honestly, I also have a lot of criticism about this work. \nA lot of the details are missing. For example, how were the words tokenized? If BPE, then morphology is split to tokens perfectly? If none, then their sophisticated way of choosing words doesn't matter, because they anyway convert the words into one hot vectors? \nHow large were the vocabularies? What were the parameters of the networks train? There are works saying that depth changes inductive biases and not only arch... \nBecause of the large number of networks (~128 * 10 reps), they train on really small amounts of data. Hard to say to which amount did that matter, and whether with a small vocab it is reasonable. Still, hard to generalize from the results because of it. \nLast, and that is especially peculiar:\n\"Weights given to each production were chosen manually through experimentation. Some principles for choosing weights for a grammar in this manner are described by Eisner and Smith (2008)\" \nSo, we can't know what type of sentences they create (balanced trees, long sentences, really low weights on something that makes a phenomenon\\switch less interesting). And they don't share the method or even the chosen weights. \nTo sum, this is a very interesting approach, with a lot of potential, that could benefit from more analysis of the results (e.g. per switch), and analysing myself is also problematic as a lot is left out of the paper (nor there is an appendix with the info.)",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2106.01044"
    },
    "1542": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/2108.10763",
        "transcript": "Huge \ud835\ude58\ud835\ude64\ud835\ude62\ud835\ude62\ud835\ude5e\ud835\ude69 \ud835\ude68\ud835\ude6a\ud835\ude62\ud835\ude62\ud835\ude56\ud835\ude67\ud835\ude5e\ud835\ude6f\ud835\ude56\ud835\ude69\ud835\ude5e\ud835\ude64\ud835\ude63 dataset\nThe dataset cleans tons of open source projects to have only ones with high quality committing habits \n\n(e.g. large active projects with commits that are of significant length etc.)\nWe present some ways to evaluate that the meaning was kept while summarizing, so you can go beyond ROUGE\nWe provide a strict split that keeps some (thousand+-) repositories totally out of the training, so you can check in domain and out of domain or just be sure results are clean.\n\nIf you ever want an even larger dataset, follow the same procedure and use more repositories (we took only ones active in 2020, pick ones that are active no longer or wasn't active until now)\n\nDataset in https://figshare.com/articles/dataset/CumSum_data_set/14711370\nCode is found in https://github.com/evidencebp/comsum\nPaper in https://arxiv.org/pdf/2108.10763.pdf",
        "sourceType": "blog",
        "linkToPaper": "http://www.arxiv-sanity.com/2108.10763"
    },
    "1543": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/abs-1708-00630",
        "transcript": "*Note: This is a review of both Self Governing Neural Networks and ProjectionNet.*\n\n# [Self Governing Neural Networks (SGNN): the Projection Layer](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer)\n\n> A SGNN's word projections preprocessing pipeline in scikit-learn\n\nIn this notebook, we'll use T=80 random hashing projection functions, each of dimensionnality d=14, for a total of 1120 features per projected word in the projection function P. \n\nNext, we'll need feedforward neural network (dense) layers on top of that (as in the paper) to re-encode the projection into something better. This is not done in the current notebook and is left to you to implement in your own neural network to train the dense layers jointly with a learning objective. The SGNN projection created hereby is therefore only a preprocessing on the text to project words into the hashing space, which becomes spase 1120-dimensional word features created dynamically hereby. Only the CountVectorizer needs to be fitted, as it is a char n-gram term frequency prior to the hasher. This one could be computed dynamically too without any fit, as it would be possible to use the [power set](https://en.wikipedia.org/wiki/Power_set) of the possible n-grams as sparse indices computed on the fly as (indices, count_value) tuples, too.\n\n\n```python\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom collections import Counter\nfrom pprint import pprint\n```\n\n## Preparing dummy data for demonstration:\n\n\n```python\nclass SentenceTokenizer(BaseEstimator, TransformerMixin):\n    # char lengths:\n    MINIMUM_SENTENCE_LENGTH = 10\n    MAXIMUM_SENTENCE_LENGTH = 200\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return self._split(X)\n    \n    def _split(self, string_):\n        splitted_string = []\n        \n        sep = chr(29)  # special separator character to split sentences or phrases.\n        string_ = string_.strip().replace(\".\", \".\" + sep).replace(\"?\", \"?\" + sep).replace(\"!\", \"!\" + sep).replace(\";\", \";\" + sep).replace(\"\\n\", \"\\n\" + sep)\n        for phrase in string_.split(sep):\n            phrase = phrase.strip()\n            \n            while len(phrase) > SentenceTokenizer.MAXIMUM_SENTENCE_LENGTH:\n                # clip too long sentences.\n                sub_phrase = phrase[:SentenceTokenizer.MAXIMUM_SENTENCE_LENGTH].lstrip()\n                splitted_string.append(sub_phrase)\n                phrase = phrase[SentenceTokenizer.MAXIMUM_SENTENCE_LENGTH:].rstrip()\n            \n            if len(phrase) >= SentenceTokenizer.MINIMUM_SENTENCE_LENGTH:\n                splitted_string.append(phrase)\n\n        return splitted_string\n\n\nwith open(\"./data/How-to-Grow-Neat-Software-Architecture-out-of-Jupyter-Notebooks.md\") as f:\n    raw_data = f.read()\n\ntest_str_tokenized = SentenceTokenizer().fit_transform(raw_data)\n\n# Print text example:\nprint(len(test_str_tokenized))\npprint(test_str_tokenized[3:9])\n```\n\n    168\n    [\"Have you ever been in the situation where you've got Jupyter notebooks \"\n     '(iPython notebooks) so huge that you were feeling stuck in your code?',\n     'Or even worse: have you ever found yourself duplicating your notebook to do '\n     'changes, and then ending up with lots of badly named notebooks?',\n     \"Well, we've all been here if using notebooks long enough.\",\n     'So how should we code with notebooks?',\n     \"First, let's see why we need to be careful with notebooks.\",\n     \"Then, let's see how to do TDD inside notebook cells and how to grow a neat \"\n     'software architecture out of your notebooks.']\n\n\n## Creating a SGNN preprocessing pipeline's classes\n\n\n```python\nclass WordTokenizer(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        begin_of_word = \"<\"\n        end_of_word = \">\"\n        out = [\n            [\n                begin_of_word + word + end_of_word\n                for word in sentence.replace(\"//\", \" /\").replace(\"/\", \" /\").replace(\"-\", \" -\").replace(\"  \", \" \").split(\" \")\n                if not len(word) == 0\n            ]\n            for sentence in X\n        ]\n        return out\n\n```\n\n\n```python\nchar_ngram_range = (1, 4)\n\nchar_term_frequency_params = {\n    'char_term_frequency__analyzer': 'char',\n    'char_term_frequency__lowercase': False,\n    'char_term_frequency__ngram_range': char_ngram_range,\n    'char_term_frequency__strip_accents': None,\n    'char_term_frequency__min_df': 2,\n    'char_term_frequency__max_df': 0.99,\n    'char_term_frequency__max_features': int(1e7),\n}\n\nclass CountVectorizer3D(CountVectorizer):\n\n    def fit(self, X, y=None):\n        X_flattened_2D = sum(X.copy(), [])\n        super(CountVectorizer3D, self).fit_transform(X_flattened_2D, y)  # can't simply call \"fit\"\n        return self\n\n    def transform(self, X):\n        return [\n            super(CountVectorizer3D, self).transform(x_2D)\n            for x_2D in X\n        ]\n    \n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X)\n\n```\n\n\n```python\nimport scipy.sparse as sp\n\nT = 80\nd = 14\n\nhashing_feature_union_params = {\n    # T=80 projections for each of dimension d=14: 80 * 14 = 1120-dimensionnal word projections.\n    **{'union__sparse_random_projection_hasher_{}__n_components'.format(t): d\n       for t in range(T)\n    },\n    **{'union__sparse_random_projection_hasher_{}__dense_output'.format(t): False  # only AFTER hashing.\n       for t in range(T)\n    }\n}\n\nclass FeatureUnion3D(FeatureUnion):\n    \n    def fit(self, X, y=None):\n        X_flattened_2D = sp.vstack(X, format='csr')\n        super(FeatureUnion3D, self).fit(X_flattened_2D, y)\n        return self\n    \n    def transform(self, X): \n        return [\n            super(FeatureUnion3D, self).transform(x_2D)\n            for x_2D in X\n        ]\n    \n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X)\n\n```\n\n## Fitting the pipeline \n\nNote: at fit time, the only thing done is to discard some unused char n-grams and to instanciate the random hash, the whole thing could be independent of the data, but here because of discarding the n-grams, we need to \"fit\" the data. Therefore, fitting could be avoided all along, but we fit here for simplicity of implementation using scikit-learn.\n\n\n```python\nparams = dict()\nparams.update(char_term_frequency_params)\nparams.update(hashing_feature_union_params)\n\npipeline = Pipeline([\n    (\"word_tokenizer\", WordTokenizer()),\n    (\"char_term_frequency\", CountVectorizer3D()),\n    ('union', FeatureUnion3D([\n        ('sparse_random_projection_hasher_{}'.format(t), SparseRandomProjection())\n        for t in range(T)\n    ]))\n])\npipeline.set_params(**params)\n\nresult = pipeline.fit_transform(test_str_tokenized)\n\nprint(len(result), len(test_str_tokenized))\nprint(result[0].shape)\n```\n\n    168 168\n    (12, 1120)\n\n\n## Let's see the output and its form. \n\n\n```python\nprint(result[0].toarray().shape)\nprint(result[0].toarray()[0].tolist())\nprint(\"\")\n\n# The whole thing is quite discrete:\nprint(set(result[0].toarray()[0].tolist()))\n\n# We see that we could optimize by using integers here instead of floats by counting the occurence of every entry.\nprint(Counter(result[0].toarray()[0].tolist()))\n```\n\n    (12, 1120)\n    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 2.005715251142432, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n    \n    {0.0, 2.005715251142432, -2.005715251142432}\n    Counter({0.0: 1069, -2.005715251142432: 27, 2.005715251142432: 24})\n\n\n## Checking that the cosine similarity before and after word projection is kept\n\nNote that this is a yet low-quality test, as the neural network layers above the projection are absent, so the similary is not yet semantic, it only looks at characters.\n\n\n```python\nword_pairs_to_check_against_each_other = [\n    # Similar:\n    [\"start\", \"started\"],\n    [\"prioritize\", \"priority\"],\n    [\"twitter\", \"tweet\"],\n    [\"Great\", \"great\"],\n    # Dissimilar:\n    [\"boat\", \"cow\"],\n    [\"orange\", \"chewbacca\"],\n    [\"twitter\", \"coffee\"],\n    [\"ab\", \"ae\"],\n]\n\nbefore = pipeline.named_steps[\"char_term_frequency\"].transform(word_pairs_to_check_against_each_other)\nafter = pipeline.named_steps[\"union\"].transform(before)\n\nfor i, word_pair in enumerate(word_pairs_to_check_against_each_other):\n    cos_sim_before = cosine_similarity(before[i][0], before[i][1])[0,0]\n    cos_sim_after  = cosine_similarity( after[i][0],  after[i][1])[0,0]\n    print(\"Word pair tested:\", word_pair)\n    print(\"\\t - similarity before:\", cos_sim_before, \n          \"\\t Are words similar?\", \"yes\" if cos_sim_before > 0.5 else \"no\")\n    print(\"\\t - similarity after :\", cos_sim_after , \n          \"\\t Are words similar?\", \"yes\" if cos_sim_after  > 0.5 else \"no\")\n    print(\"\")\n```\n\n    Word pair tested: ['start', 'started']\n    \t - similarity before: 0.8728715609439697 \t Are words similar? yes\n    \t - similarity after : 0.8542062410985866 \t Are words similar? yes\n    \n    Word pair tested: ['prioritize', 'priority']\n    \t - similarity before: 0.8458888522202895 \t Are words similar? yes\n    \t - similarity after : 0.8495862181305898 \t Are words similar? yes\n    \n    Word pair tested: ['twitter', 'tweet']\n    \t - similarity before: 0.5439282932204212 \t Are words similar? yes\n    \t - similarity after : 0.4826046482460216 \t Are words similar? no\n    \n    Word pair tested: ['Great', 'great']\n    \t - similarity before: 0.8006407690254358 \t Are words similar? yes\n    \t - similarity after : 0.8175049752615363 \t Are words similar? yes\n    \n    Word pair tested: ['boat', 'cow']\n    \t - similarity before: 0.1690308509457033 \t Are words similar? no\n    \t - similarity after : 0.10236537810666581 \t Are words similar? no\n    \n    Word pair tested: ['orange', 'chewbacca']\n    \t - similarity before: 0.14907119849998599 \t Are words similar? no\n    \t - similarity after : 0.2019908169580899 \t Are words similar? no\n    \n    Word pair tested: ['twitter', 'coffee']\n    \t - similarity before: 0.09513029883089882 \t Are words similar? no\n    \t - similarity after : 0.1016460166230715 \t Are words similar? no\n    \n    Word pair tested: ['ab', 'ae']\n    \t - similarity before: 0.408248290463863 \t Are words similar? no\n    \t - similarity after : 0.42850530886130067 \t Are words similar? no\n    \n\n\n## Next up\n\nSo we have created the sentence preprocessing pipeline and the sparse projection (random hashing) function. We now need a few feedforward layers on top of that. \n\nAlso, a few things could be optimized, such as using the power set of the possible n-gram values with a predefined character set instead of fitting it, and the Hashing's fit function could be avoided as well by passing the random seed earlier, because the Hasher doesn't even look at the data and it only needs to be created at some point. This would yield a truly embedding-free approach. Free to you to implement this. I wanted to have something that worked first, leaving optimization for later.\n\n\n## License\n\n\nBSD 3-Clause License\n\n\nCopyright (c) 2018, Guillaume Chevalier\n\nAll rights reserved.\n\n\n## Extra links\n\n### Connect with me\n\n- [LinkedIn](https://ca.linkedin.com/in/chevalierg)\n- [Twitter](https://twitter.com/guillaume_che)\n- [GitHub](https://github.com/guillaume-chevalier/)\n- [Quora](https://www.quora.com/profile/Guillaume-Chevalier-2)\n- [YouTube](https://www.youtube.com/c/GuillaumeChevalier)\n- [Dev/Consulting](http://www.neuraxio.com/en/)\n\n### Liked this piece of code? Did it help you? Leave a [star](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer/stargazers), [fork](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer/network/members) and share the love!\n\n# ProjectionNets\n\n**Notes are from [Issue 1](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer/issues/1)**:\n\nVery interesting. I've finally read the [previous supporting paper](https://arxiv.org/pdf/1708.00630.pdf), thanks for the shootout. Here are my thoughts after reading it. To sum up, I think that the projections are at word-level instead of at sentence level. This is for two reasons: \n1. they use a hidden layer size of only 256 to represent words neurally (whereas sentence representations would be quite bigger), and \n2. they seem to use an LSTM on top of the ProjectionNet (SGNN) to model short sentences in their benchmarks, which would mean the ProjectionNet doesn't encode at sentence-level but at least at a lower level (probably words). \n\nHere is my full review: \n\nOn 80\\*14 v.s. 1\\*1120 projections:\n- I thought the set of 80 projection functions was not for time performance, but rather to make the union of potentially different features. I think that either way, if one projection function of 1120 entries would take as much time to compute as 80 functions of 14 entries (80\\*14=1120) - please correct me if I'm wrong. \n\nOn the hidden layer size of 256:\n- I find peculiar that the size of their FullyConnected layers is only of 256. I'd expect 300 for word-level neural representations and rather 2000 for sentence-level neural representations. This leads me to think that the projection layer is at the word-level with char features and not at the sentence-level with char features. \n\nOn the benchmark against a nested RNN (see section \"4.3 Semantic Intent Classification\") in the previous supporting paper: \n- They say \"We use an RNN sequence model with multilayer LSTM architecture (2 layers, 100 dimensions) as the baseline and trainer network. The LSTM model and its ProjectionNet variant are also compared against other baseline systems [...]\". The fact they phrase their experiment as \"The LSTM model and its ProjectionNet\" leads me to think that they pre-tokenized texts on words and that the projection layer is applied at word-level from skip-gram char features. This would seem to go in the same direction of the fact they use a hidden layer (FullyConnected) size of only 256 rather than something over or equal to like 1000. \n\nOn [teacher-student model training](https://www.quora.com/What-is-a-teacher-student-model-in-a-Convolutional-neural-network/answer/Guillaume-Chevalier-2): \n- They seem to use a regular NN like a crutch to assist the projection layer's top-level layer to reshape information correctly. They even train the teacher at the same time that they train the student SGNN, which is something I hadn't yet seen compared to regular teacher-student setups. I'd find simpler to use a Matching Networks directly which would be quite simpler than setting up student-teacher learning. I'm not sure how their \"graph structured loss functions\" works - I yet still assume that they'd need train the whole thing like in word2vec with skip-gram or CBOW (but here with the new type of skip-gram training procedure instead of the char feature-extraction skip-gram). I wonder why they did things in a so complicated way. Matching Networks (a.k.a. cosine similarity loss, a.k.a. self-attention queries dotted with either attention keys or values before a softmax) directly with negative sampling seems so much simpler. \n",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1708.00630"
    },
    "1544": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/corr/1302.1886",
        "transcript": "People turn into a flat, hot gas in mosh pits.",
        "sourceType": "blog",
        "linkToPaper": "https://arxiv.org/abs/1302.1886v1"
    },
    "1545": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=10.1080/23337486.2017.1397865",
        "transcript": "Although women in both Iraq and Afghanistan have taken part in ground combat and although restrictions have been lifted officially, there remains a strong resistance to the policy that now allows women to work in officially designated combat-related jobs. This resistance is especially strong among online comments to military media coverage. This study examines narratives of masculinity and resistance to women\u2019s presence in combat through online commentary by mostly men who self-identify as military past and present. Using narrative analysis as technique we examine the hegemonic conditions of military culture that render women\u2019s participation problematic. We identify a melodrama, an emotional, simplified, persuasive, and morally laden narrative surrounding the battle between \u2018good\u2019 and \u2018evil\u2019,one that results in what we term masculinity under attack, complete with prototypical plots, characters, and moral evaluations, such as blameless victims, evil villains, and (blocked) heroes. Because narratives are strategic, persuasive, and deeply ideological, this work offers a unique examination that connects the local narratives of this online community to a broader cultural narrative, one that has material implications for women and all things feminine in the military.",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/http://dx.doi.org/10.1080/23337486.2017.1397865"
    },
    "1546": {
        "sourceUrl": "https://shortscience.org/paper?bibtexKey=journals/remotesensing/MeiYJZWD17",
        "transcript": "Someone understands the author's code implementation process? How does the pa_test.mat file get generated during the predict?",
        "sourceType": "blog",
        "linkToPaper": "http://sci-hub.st/https://doi.org/10.3390/rs9111139"
    }
}