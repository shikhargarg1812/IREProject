paper_links,paper_pdf_links,paper_titles,paper_authors,video_links,abstracts
http://papers.nips.cc/paper/by-source-2019-3824,https://proceedings.neurips.cc/paper/2019/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf,Updates of Equilibrium Prop Match Gradients of Backprop Through Time in an RNN with Static Input,Maxence Ernoult &middot; Julie Grollier &middot; Damien Querlioz &middot; Yoshua Bengio &middot; Benjamin Scellier,https://www.youtube.com/watch?v=Xb5sM0NRy_0&feature=youtu.be,"Equilibrium Propagation (EP) is a biologically inspired learning algorithm for
"
http://papers.nips.cc/paper/by-source-2019-6214,https://proceedings.neurips.cc/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf,Uniform convergence may be unable to explain generalization in deep learning,Vaishnavh Nagarajan &middot; J. Zico Kolter,https://www.youtube.com/watch?v=o3GfnEjTdIQ,"Aimed at explaining the surprisingly good generalization behavior of overparameterized deep networks, recent works have developed a variety of generalization bounds for deep learning,  all  based on the fundamental learning-theoretic technique of uniform convergence. While
"
http://papers.nips.cc/paper/by-source-2019-9024,https://proceedings.neurips.cc/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf,Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks,Aaron Voelker &middot; Ivana Kajić &middot; Chris Eliasmith,https://docs.google.com/document/d/1uarYP9YqcKx2Qh7sHJJhrY87C-mVZbNTAVqRKVPDXmI/edit,"We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit~(LMU) is mathematically derived to orthogonalize its continuous-time history -- doing so by solving $d$ coupled ordinary differential equations~(ODEs), whose phase space linearly maps onto sliding windows of time via the Legendre polynomials up to degree $d - 1$. Backpropagation across LMUs outperforms equivalently-sized LSTMs on a chaotic time-series prediction task, improves memory capacity by two orders of magnitude, and significantly reduces training and inference times. LMUs can efficiently handle temporal dependencies spanning $100\text{,}000$ time-steps, converge rapidly, and use few internal state-variables to learn complex functions spanning long windows of time -- exceeding state-of-the-art performance among RNNs on permuted sequential MNIST. These results are due to the network&#x27;s disposition to learn scale-invariant features independently of step size. Backpropagation through the ODE solver allows each layer to adapt its internal time-step, enabling the network to learn task-relevant time-scales. We demonstrate that LMU memory cells can be implemented using $m$ recurrently-connected Poisson spiking neurons, $\mathcal{O}( m )$ time and memory, with error scaling as $\mathcal{O}( d / \sqrt{m} )$. We discuss implementations of LMUs on analog and digital neuromorphic hardware."
http://papers.nips.cc/paper/by-source-2019-539,https://proceedings.neurips.cc/paper/2019/file/5737034557ef5b8c02c0e46513b98f90-Paper.pdf,Point-Voxel CNN for Efficient 3D Deep Learning,Zhijian Liu &middot; Haotian Tang &middot; Yujun Lin &middot; Song Han,https://pvcnn.mit.edu,"We present Point-Voxel CNN (PVCNN) for efficient, fast 3D deep learning. Previous work processes 3D data using either voxel-based or point-based NN models. However, both approaches are computationally inefficient. The computation cost and memory footprints of the voxel-based models grow cubically with the input resolution, making it memory-prohibitive to scale up the resolution. As for point-based networks, up to 80% of the time is wasted on dealing with the sparse data which have rather poor memory locality, not on the actual feature extraction. In this paper, we propose PVCNN that represents the 3D input data in points to reduce the memory consumption, while performing the convolutions in voxels to reduce the irregular, sparse data access and improve the locality. Our PVCNN model is both memory and computation efficient. Evaluated on semantic and part segmentation datasets, it achieves much higher accuracy than the voxel-based baseline with 10× GPU memory reduction; it also outperforms the state-of-the-art point-based models with 7× measured speedup on average. Remarkably, the narrower version of PVCNN achieves 2× speedup over PointNet (an extremely efficient model) on part and scene segmentation benchmarks with much higher accuracy. We validate the general effectiveness of PVCNN on 3D object detection: by replacing the primitives in Frustrum PointNet with PVConv, it outperforms Frustrum PointNet++ by 2.4% mAP on average with 1.5× measured speedup and GPU memory reduction."
http://papers.nips.cc/paper/by-source-2019-6138,https://proceedings.neurips.cc/paper/2019/file/b2ead76dfdc4ae56a2abd1896ec46291-Paper.pdf,Identification of Conditional Causal Effects under Markov Equivalence,Amin Jaber &middot; Jiji Zhang &middot; Elias Bareinboim,https://www.dropbox.com/s/qucyzi28s7ycka9/NeurIPS19-CID.m4v?dl=0,"Causal identification is the problem of deciding whether a post-interventional distribution is computable from a combination of qualitative knowledge about the data-generating process, which is encoded in a causal diagram, and an observational distribution. A generalization of this problem restricts the qualitative knowledge to a class of Markov equivalent causal diagrams, which, unlike a single, fully-specified causal diagram, can be inferred from the observational distribution.
"
http://papers.nips.cc/paper/by-source-2019-71,https://proceedings.neurips.cc/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf,"Adversarial Examples Are Not Bugs, They Are Features",Andrew Ilyas &middot; Shibani Santurkar &middot; Dimitris Tsipras &middot; Logan Engstrom &middot; Brandon Tran &middot; Aleksander Madry,https://youtu.be/-vhUWSHOqIM,"Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a {\em misalignment} between the (human-specified) notion of robustness and the inherent geometry of the data."
http://papers.nips.cc/paper/by-source-2019-2823,https://proceedings.neurips.cc/paper/2019/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf,Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness,Saeed Mahloujifar &middot; Xiao Zhang &middot; Mohammad Mahmoody &middot; David Evans,https://drive.google.com/open?id=1z-97tgTqB5dH3WzQcE6kBOVJHizo5z7A,"Many recent works have shown that adversarial examples that fool classifiers can be found by minimally perturbing a normal input. Recent theoretical results, starting with Gilmer et al. (2018b), show that if the inputs are drawn from a concentrated metric probability space, then adversarial examples with small perturbation are inevitable. A concentrated space has the property that any subset with Ω(1) (e.g.,1/100) measure, according to the imposed distribution, has small distance to almost all (e.g.,  99/100) of the points in the space. It is not clear,  however,  whether these theoretical results apply to actual distributions such as images. This paper presents a method for empirically measuring and bounding the concentration of a concrete dataset which is proven to converge to the actual concentration. We use it to empirically estimate the intrinsic robustness to and L_2 and L_infinity perturbations of several image classification benchmarks. Code for our experiments is available at https://github.com/xiaozhanguva/Measure-Concentration."
http://papers.nips.cc/paper/by-source-2019-5315,https://proceedings.neurips.cc/paper/2019/file/5ad742cd15633b26fdce1b80f7b39f7c-Paper.pdf,A Game Theoretic Approach to Class-wise Selective Rationalization,Shiyu Chang &middot; Yang Zhang &middot; Mo Yu &middot; Tommi Jaakkola,https://www.dropbox.com/s/tnr5emcp5a4wvig/video_final.mp4?dl=0,"Selection of input features such as relevant pieces of text has become a common technique of highlighting how complex neural predictors operate. The selection can be optimized post-hoc for trained models or incorporated directly into the method itself (self-explaining). However, an overall selection does not properly capture the multi-faceted nature of useful rationales such as pros and cons for decisions. To this end, we propose a new game theoretic approach to class-dependent rationalization, where the method is specifically trained to highlight evidence supporting alternative conclusions. Each class involves three players set up competitively to find evidence for factual and counterfactual scenarios. We show theoretically in a simplified scenario how the game drives the solution towards meaningful class-dependent rationales. We evaluate the method in single- and multi-aspect sentiment classification tasks and demonstrate that the proposed method is able to identify both factual (justifying the ground truth label) and counterfactual (countering the ground truth label) rationales consistent with human rationalization.  The code for our method is publicly available. "
http://papers.nips.cc/paper/by-source-2019-1853,https://proceedings.neurips.cc/paper/2019/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf,Adversarial training for free!,Ali Shafahi &middot; Mahyar Najibi &middot; Mohammad Amin Ghiasi &middot; Zheng Xu &middot; John Dickerson &middot; Christoph Studer &middot; Larry Davis &middot; Gavin Taylor &middot; Tom Goldstein,https://youtu.be/v8U9mM1Vwv0?t=285,"Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our &quot;free&quot; adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40% accuracy against PGD attacks."
http://papers.nips.cc/paper/by-source-2019-2823,https://proceedings.neurips.cc/paper/2019/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf,Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness,Saeed Mahloujifar &middot; Xiao Zhang &middot; Mohammad Mahmoody &middot; David Evans,https://drive.google.com/open?id=1z-97tgTqB5dH3WzQcE6kBOVJHizo5z7A,"Many recent works have shown that adversarial examples that fool classifiers can be found by minimally perturbing a normal input. Recent theoretical results, starting with Gilmer et al. (2018b), show that if the inputs are drawn from a concentrated metric probability space, then adversarial examples with small perturbation are inevitable. A concentrated space has the property that any subset with Ω(1) (e.g.,1/100) measure, according to the imposed distribution, has small distance to almost all (e.g.,  99/100) of the points in the space. It is not clear,  however,  whether these theoretical results apply to actual distributions such as images. This paper presents a method for empirically measuring and bounding the concentration of a concrete dataset which is proven to converge to the actual concentration. We use it to empirically estimate the intrinsic robustness to and L_2 and L_infinity perturbations of several image classification benchmarks. Code for our experiments is available at https://github.com/xiaozhanguva/Measure-Concentration."
http://papers.nips.cc/paper/by-source-2019-1034,https://proceedings.neurips.cc/paper/2019/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf,Multi-marginal Wasserstein GAN,Jiezhang Cao &middot; Langyuan Mo &middot; Yifan Zhang &middot; Kui Jia &middot; Chunhua Shen &middot; Mingkui Tan,http://caojiezhang.com/video/NeurIPS2019_MWGAN_video.mp4,"Multiple marginal matching problem aims at learning mappings to match a source domain to multiple target domains and it has attracted great attention in many applications, such as multi-domain image translation. However, addressing this problem has two critical challenges: (i) Measuring the multi-marginal distance among different domains is very intractable; (ii) It is very difficult to exploit cross-domain correlations to match the target domain distributions. In this paper, we propose a novel Multi-marginal Wasserstein GAN (MWGAN) to minimize Wasserstein distance among domains. Specifically, with the help of multi-marginal optimal transport theory, we develop a new adversarial objective function with inner- and inter-domain constraints to exploit cross-domain correlations. Moreover, we theoretically analyze the generalization performance of MWGAN, and empirically evaluate it on the balanced and imbalanced translation tasks. Extensive experiments on toy and real-world datasets demonstrate the effectiveness of MWGAN.  "
http://papers.nips.cc/paper/by-source-2019-4985,https://proceedings.neurips.cc/paper/2019/file/785ca71d2c85e3f3774baaf438c5c6eb-Paper.pdf,Correlation clustering with local objectives,Sanchit Kalhan &middot; Konstantin Makarychev &middot; Timothy Zhou,https://www.youtube.com/watch?v=0s1LF5qL2Do&feature=youtu.be,"Correlation Clustering is a powerful graph partitioning model that aims to cluster items based on the notion of similarity between items. An instance of the Correlation Clustering problem consists of a graph G (not necessarily complete) whose edges are labeled by a binary classifier as
"
http://papers.nips.cc/paper/by-source-2019-5511,https://proceedings.neurips.cc/paper/2019/file/3cc697419ea18cc98d525999665cb94a-Paper.pdf,On Fenchel Mini-Max Learning,Chenyang Tao &middot; Liqun Chen &middot; Shuyang Dai &middot; Junya Chen &middot; Ke Bai &middot; Dong Wang &middot; Jianfeng Feng &middot; Wenlian Lu &middot; Georgiy Bobashev &middot; Lawrence Carin,https://github.com/chenyang-tao/FML/tree/master/video,"Inference, estimation, sampling and likelihood evaluation are four primary goals of probabilistic modeling. Practical considerations often force modeling approaches to make compromises between these objectives. We present a novel probabilistic learning framework, called Fenchel Mini-Max Learning (FML), that accommodates all four desiderata in a flexible and scalable manner. Our derivation is rooted in classical maximum likelihood estimation, and it overcomes a longstanding challenge that prevents unbiased estimation of unnormalized statistical models. By reformulating MLE as a mini-max game, FML enjoys an unbiased training objective that (i) does not explicitly involve the intractable normalizing constant and (ii) is directly amendable to stochastic gradient descent optimization. To demonstrate the utility of the proposed approach, we consider learning unnormalized statistical models, nonparametric density estimation and training generative models, with encouraging empirical results presented. "
http://papers.nips.cc/paper/by-source-2019-4340,https://proceedings.neurips.cc/paper/2019/file/61f3a6dbc9120ea78ef75544826c814e-Paper.pdf,Solving Interpretable Kernel Dimensionality Reduction,Chieh Wu &middot; Jared Miller &middot; Yale Chang &middot; Mario Sznaier &middot; Jennifer Dy,https://youtu.be/q1ETeX3HvPY,"Kernel dimensionality reduction (KDR) algorithms find a low dimensional representation of the original data by optimizing kernel dependency measures that are capable of capturing nonlinear relationships. The standard strategy is to first map the data into a high dimensional feature space using kernels prior to a projection onto a low dimensional space. While KDR methods can be easily solved by keeping the most dominant eigenvectors of the kernel matrix, its features are no longer easy to interpret.  Alternatively, Interpretable KDR (IKDR) is different in that it projects onto a subspace \textit{before} the kernel feature mapping, therefore, the projection matrix can indicate how the original features linearly combine to form the new features. Unfortunately, the IKDR objective requires a non-convex manifold optimization that is difficult to solve and can no longer be solved by eigendecomposition. Recently, an efficient iterative spectral (eigendecomposition) method (ISM) has been proposed for this objective in the context of alternative clustering. However, ISM only provides theoretical guarantees for the Gaussian kernel. This greatly constrains ISM&#x27;s usage since any kernel method using ISM is now limited to a single kernel. This work extends the theoretical guarantees of ISM to an entire family of kernels, thereby empowering ISM to solve any kernel method of the same objective. In identifying this family, we prove that each kernel within the family has a surrogate $\Phi$ matrix and the optimal projection is formed by its most dominant eigenvectors. With this extension, we establish how a wide range of IKDR applications across different learning paradigms can be solved by ISM. To support reproducible results, the source code is made publicly available on  \url{https://github.com/ANONYMIZED}."
http://papers.nips.cc/paper/by-source-2019-2137,https://proceedings.neurips.cc/paper/2019/file/9bb6dee73b8b0ca97466ccb24fff3139-Paper.pdf,Exact Rate-Distortion in Autoencoders via Echo Noise,Rob Brekelmans &middot; Daniel Moyer &middot; Aram Galstyan &middot; Greg Ver Steeg,http://bit.ly/2Wdm6St,"Compression is at the heart of effective representation learning. However, lossy compression is typically achieved through simple parametric models like Gaussian noise to preserve analytic tractability, and the limitations this imposes on learning are largely unexplored.  Further, the Gaussian prior assumptions in models such as variational autoencoders (VAEs) provide only an upper bound on the compression rate in general.  We introduce a new noise channel, Echo noise, that admits a simple, exact expression for mutual information for arbitrary input distributions.  The noise is constructed in a data-driven fashion that does not require restrictive distributional assumptions.  With its complex encoding mechanism and exact rate regularization, Echo leads to improved bounds on log-likelihood and dominates beta-VAEs across the achievable range of rate-distortion trade-offs. Further, we show that Echo noise can outperform flow-based methods without the need to train additional distributional transformations."
http://papers.nips.cc/paper/by-source-2019-7205,https://proceedings.neurips.cc/paper/2019/file/75da5036f659fe64b53f3d9b39412967-Paper.pdf,Communication-efficient Distributed SGD with Sketching,Nikita Ivkin &middot; Daniel Rothchild &middot; Enayat Ullah &middot; Vladimir braverman &middot; Ion Stoica &middot; Raman Arora,http://enayatullah.github.io/sketchedsgd/video,"Large-scale distributed training of neural networks is often limited by network bandwidth, wherein the communication time overwhelms the local computation time. Motivated by the success of sketching methods in sub-linear/streaming algorithms, we introduce Sketched-SGD, an algorithm for carrying out distributed SGD by communicating sketches instead of full gradients. We show that \ssgd has favorable convergence rates on several classes of functions. When considering all communication -- both of gradients and of updated model weights -- Sketched-SGD reduces the amount of communication required compared to other gradient compression methods from $\mathcal{O}(d)$ or $\mathcal{O}(W)$ to $\mathcal{O}(\log d)$, where $d$ is the number of model parameters and $W$ is the number of workers participating in training. We run experiments on a transformer model, an LSTM, and a residual network, demonstrating up to a 40x reduction in total communication cost with no loss in final model performance. We also show experimentally that Sketched-SGD scales to at least 256 workers without increasing communication cost or degrading model performance."
http://papers.nips.cc/paper/by-source-2019-71,https://proceedings.neurips.cc/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf,"Adversarial Examples Are Not Bugs, They Are Features",Andrew Ilyas &middot; Shibani Santurkar &middot; Dimitris Tsipras &middot; Logan Engstrom &middot; Brandon Tran &middot; Aleksander Madry,https://youtu.be/-vhUWSHOqIM,"Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a {\em misalignment} between the (human-specified) notion of robustness and the inherent geometry of the data."
http://papers.nips.cc/paper/by-source-2019-1100,https://proceedings.neurips.cc/paper/2019/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf,Neural networks grown and self-organized by noise,Guruprasad Raghavan &middot; Matt Thomson,https://caltech.box.com/s/9tqfhrgo3cicoi5ue9z7qyhlsjfefdka,"Living neural networks emerge through a process of growth and self-organization that begins with a single cell and results in a brain, an organized and functional computational device. Artificial neural networks, however, rely on human-designed, hand-programmed architectures for their remarkable performance. Can we develop artificial computational devices that can grow and self-organize without human intervention? In this paper, we propose a biologically inspired developmental algorithm that can ‘grow’ a functional, layered neural network from a single initial cell. The algorithm organizes inter-layer connections to construct retinotopic pooling layers. Our approach is inspired by the mechanisms employed by the early visual system to wire the retina to the lateral geniculate nucleus (LGN), days before animals open their eyes.  The key ingredients for robust self-organization are an emergent spontaneous spatiotemporal activity wave in the first layer and a local learning rule in the second layer that ‘learns’ the underlying activity pattern in the first layer.  The algorithm is adaptable to a wide-range of input-layer geometries, robust to malfunctioning units in the first layer, and so can be used to successfully grow and self-organize pooling architectures of different pool-sizes and shapes. The algorithm provides a primitive procedure for constructing layered neural networks through growth and self-organization. We also demonstrate that networks grown from a single unit perform as well as hand-crafted networks on MNIST. Broadly, our work shows that biologically inspired developmental algorithms can be applied to autonomously grow functional `brains&#x27; in-silico."
http://papers.nips.cc/paper/by-source-2019-3824,https://proceedings.neurips.cc/paper/2019/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf,Updates of Equilibrium Prop Match Gradients of Backprop Through Time in an RNN with Static Input,Maxence Ernoult &middot; Julie Grollier &middot; Damien Querlioz &middot; Yoshua Bengio &middot; Benjamin Scellier,https://www.youtube.com/watch?v=Xb5sM0NRy_0&feature=youtu.be,"Equilibrium Propagation (EP) is a biologically inspired learning algorithm for
"
http://papers.nips.cc/paper/by-source-2019-539,https://proceedings.neurips.cc/paper/2019/file/5737034557ef5b8c02c0e46513b98f90-Paper.pdf,Point-Voxel CNN for Efficient 3D Deep Learning,Zhijian Liu &middot; Haotian Tang &middot; Yujun Lin &middot; Song Han,https://pvcnn.mit.edu,"We present Point-Voxel CNN (PVCNN) for efficient, fast 3D deep learning. Previous work processes 3D data using either voxel-based or point-based NN models. However, both approaches are computationally inefficient. The computation cost and memory footprints of the voxel-based models grow cubically with the input resolution, making it memory-prohibitive to scale up the resolution. As for point-based networks, up to 80% of the time is wasted on dealing with the sparse data which have rather poor memory locality, not on the actual feature extraction. In this paper, we propose PVCNN that represents the 3D input data in points to reduce the memory consumption, while performing the convolutions in voxels to reduce the irregular, sparse data access and improve the locality. Our PVCNN model is both memory and computation efficient. Evaluated on semantic and part segmentation datasets, it achieves a much higher accuracy than the voxel-based baseline with 10x GPU memory reduction; it also outperforms the state-of-the-art point-based models with 7x measured speedup on average. Remarkably, the narrower version of PVCNN achieves 2x speedup over PointNet (an extremely efficient model) on part and scene segmentation benchmarks with much higher accuracy. We validate the general effectiveness of PVCNN on 3D object detection: by replacing the primitives in Frustrum PointNet with PVConv, it outperforms Frustrum PointNet++ by up to 2.4% mAP with 1.8x measured speedup and 1.4x GPU memory reduction."
http://papers.nips.cc/paper/by-source-2019-760,https://proceedings.neurips.cc/paper/2019/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf,Model Compression with Adversarial Robustness: A Unified Optimization Framework,Shupeng Gui &middot; Haotao Wang &middot; Haichuan Yang &middot; Chen Yu &middot; Zhangyang Wang &middot; Ji Liu,https://github.com/shupenggui/ATMC/blob/master/materials/video.link,"Deep model compression has been extensively studied, and state-of-the-art methods can now achieve high compression ratios with minimal accuracy loss. This paper studies model compression through a different lens: could we compress models without hurting their robustness to adversarial attacks, in addition to maintaining accuracy? Previous literature suggested that the goals of robustness and compactness might sometimes contradict. We propose a novel Adversarially Trained Model Compression (ATMC) framework. ATMC constructs a unified constrained optimization formulation, where existing compression means (pruning, factorization, quantization) are all integrated into the constraints. An efficient algorithm is then developed. An extensive group of experiments are presented, demonstrating that ATMC obtains remarkably more favorable trade-off among model size, accuracy and robustness, over currently available alternatives in various settings. The codes are publicly available at: https://github.com/shupenggui/ATMC."
http://papers.nips.cc/paper/by-source-2019-2260,https://proceedings.neurips.cc/paper/2019/file/4cb811134b9d39fc3104bd06ce75abad-Paper.pdf,A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models,Maxim Kuznetsov &middot; Daniil Polykovskiy &middot; Dmitry Vetrov &middot; Alex Zhebrak,https://drive.google.com/file/d/1fDc9_BhTKT3gZAyCtuafL4tfuZivusia/view,"Generative models produce realistic objects in many domains, including text, image, video, and audio synthesis. Most popular models—Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)—usually employ a standard Gaussian distribution as a prior. Previous works show that the richer family of prior distributions may help to avoid the mode collapse problem in GANs and to improve the evidence lower bound in VAEs. We propose a new family of prior distributions—Tensor Ring Induced Prior (TRIP)—that packs an exponential number of Gaussians into a high-dimensional lattice with a relatively small number of parameters. We show that these priors improve Fréchet Inception Distance for GANs and Evidence Lower Bound for VAEs. We also study generative models with TRIP in the conditional generation setup with missing conditions. Altogether, we propose a novel plug-and-play framework for generative models that can be utilized in any GAN and VAE-like architectures.
"
http://papers.nips.cc/paper/by-source-2019-394,https://proceedings.neurips.cc/paper/2019/file/28f0b864598a1291557bed248a998d4e-Paper.pdf,Differentiable Cloth Simulation for Inverse Problems,Junbang Liang &middot; Ming Lin &middot; Vladlen Koltun,https://youtu.be/ipIM-c4lCJ0,"We propose a differentiable cloth simulator that can be embedded as a layer in deep neural networks.  This approach provides an effective, robust framework for modeling cloth dynamics, self-collisions, and contacts. Due to the high dimensionality of the dynamical system in modeling cloth, traditional gradient computation for collision response can become impractical. To address this problem, we propose to compute the gradient directly using QR decomposition of a much smaller matrix. Experimental results indicate that our method can speed up backpropagation by two orders of magnitude. We demonstrate the presented approach on a number of inverse problems, including parameter estimation and motion control for cloth."
http://papers.nips.cc/paper/by-source-2019-7203,https://proceedings.neurips.cc/paper/2019/file/118921efba23fc329e6560b27861f0c2-Paper.pdf,A Simple Baseline for Bayesian Uncertainty in Deep Learning,Wesley Maddox &middot; Pavel Izmailov &middot; Timur Garipov &middot; Dmitry Vetrov &middot; Andrew Gordon Wilson,https://izmailovpavel.github.io/swag_video,"We propose SWA-Gaussian (SWAG), a simple,  scalable,  and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration,  and transfer learning,  in comparison to many popular alternatives including variational inference, MC dropout, KFAC Laplace, and temperature scaling."
http://papers.nips.cc/paper/by-source-2019-9024,https://proceedings.neurips.cc/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf,Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks,Aaron Voelker &middot; Ivana Kajić &middot; Chris Eliasmith,https://docs.google.com/document/d/1uarYP9YqcKx2Qh7sHJJhrY87C-mVZbNTAVqRKVPDXmI/edit,"We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit~(LMU) is mathematically derived to orthogonalize its continuous-time history -- doing so by solving $d$ coupled ordinary differential equations~(ODEs), whose phase space linearly maps onto sliding windows of time via the Legendre polynomials up to degree $d - 1$. Backpropagation across LMUs outperforms equivalently-sized LSTMs on a chaotic time-series prediction task, improves memory capacity by two orders of magnitude, and significantly reduces training and inference times. LMUs can efficiently handle temporal dependencies spanning $100\text{,}000$ time-steps, converge rapidly, and use few internal state-variables to learn complex functions spanning long windows of time -- exceeding state-of-the-art performance among RNNs on permuted sequential MNIST. These results are due to the network&#x27;s disposition to learn scale-invariant features independently of step size. Backpropagation through the ODE solver allows each layer to adapt its internal time-step, enabling the network to learn task-relevant time-scales. We demonstrate that LMU memory cells can be implemented using $m$ recurrently-connected Poisson spiking neurons, $\mathcal{O}( m )$ time and memory, with error scaling as $\mathcal{O}( d / \sqrt{m} )$. We discuss implementations of LMUs on analog and digital neuromorphic hardware."
http://papers.nips.cc/paper/by-source-2019-9119,https://proceedings.neurips.cc/paper/2019/file/d921c3c762b1522c475ac8fc0811bb0f-Paper.pdf,Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics,Niru Maheswaranathan &middot; Alex Williams &middot; Matthew Golub &middot; Surya Ganguli &middot; David Sussillo,https://drive.google.com/open?id=1J-YIS9aSMNz1WX1ifBK4KlZaWhH32vXp,"Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks."
http://papers.nips.cc/paper/by-source-2019-7516,https://proceedings.neurips.cc/paper/2019/file/bb836c01cdc9120a9c984c525e4b1a4a-Paper.pdf,Explanations can be manipulated and geometry is to blame,Ann-Kathrin Dombrowski &middot; Maximillian Alber &middot; Christopher Anders &middot; Marcel Ackermann &middot; Klaus-Robert Müller &middot; Pan Kessel,https://www.youtube.com/watch?v=xTAPZbQlq3A,"Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network&#x27;s output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks. This allows us to derive an upper bound on the susceptibility of explanations to manipulations. Based on this result, we propose effective mechanisms to enhance the robustness of explanations."
http://papers.nips.cc/paper/by-source-2019-1687,https://proceedings.neurips.cc/paper/2019/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf,Fooling Neural Network Interpretations via Adversarial Model Manipulation,Juyeon Heo &middot; Sunghwan Joo &middot; Taesup Moon,https://github.com/rmrisforbidden/Fooling_Neural_Network-Interpretations/blob/master/Materials/Video.md,"We ask whether the neural network interpretation methods can be fooled via adversarial model manipulation, which is defined as a model fine-tuning step that aims to radically alter the explanations without hurting the accuracy of the original models, e.g., VGG19, ResNet50, and DenseNet121. By incorporating the interpretation results directly in the penalty term of the objective function for fine-tuning, we show that the state-of-the-art saliency map based interpreters, e.g., LRP, Grad-CAM, and SimpleGrad, can be easily fooled with our model manipulation. We propose two types of fooling, Passive and Active, and demonstrate such foolings generalize well to the entire validation set as well as transfer to other interpretation methods. Our results are validated by both visually showing the fooled explanations and reporting quantitative metrics that measure the deviations from the original explanations.  We claim that the stability of neural network interpretation method with respect to our adversarial model manipulation is an important criterion to check for developing robust and reliable neural network interpretation method."
http://papers.nips.cc/paper/by-source-2019-1058,https://proceedings.neurips.cc/paper/2019/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf,Visualizing the PHATE of Neural Networks,Scott Gigante &middot; Adam S Charles &middot; Smita Krishnaswamy &middot; Gal Mishne,http://mishne.ucsd.edu/m-phate-video.html,"Understanding why and how certain neural networks outperform others is key to guiding future development of network architectures and optimization methods. To this end, we introduce a novel visualization algorithm that reveals the internal geometry of such networks: Multislice PHATE (M-PHATE), the first method designed explicitly to visualize how a neural network&#x27;s hidden representations of data evolve throughout the course of training. We demonstrate that our visualization provides intuitive, detailed summaries of the learning dynamics beyond simple global measures (i.e., validation loss and accuracy), without the need to access validation data. Furthermore, M-PHATE better captures both the dynamics and community structure of the hidden units as compared to visualization based on standard dimensionality reduction methods (e.g., ISOMAP, t-SNE). We demonstrate M-PHATE with two vignettes: continual learning and generalization. In the former, the M-PHATE visualizations display the mechanism of &quot;catastrophic forgetting&quot; which is a major challenge for learning in task-switching contexts. In the latter, our visualizations reveal how increased heterogeneity among hidden units correlates with improved generalization performance. An implementation of M-PHATE, along with scripts to reproduce the figures in this paper, is available at https://github.com/scottgigante/M-PHATE."
http://papers.nips.cc/paper/by-source-2019-6138,https://proceedings.neurips.cc/paper/2019/file/b2ead76dfdc4ae56a2abd1896ec46291-Paper.pdf,Identification of Conditional Causal Effects under Markov Equivalence,Amin Jaber &middot; Jiji Zhang &middot; Elias Bareinboim,https://www.dropbox.com/s/qucyzi28s7ycka9/NeurIPS19-CID.m4v?dl=0,"Causal identification is the problem of deciding whether a post-interventional distribution is computable from a combination of qualitative knowledge about the data-generating process, which is encoded in a causal diagram, and an observational distribution. A generalization of this problem restricts the qualitative knowledge to a class of Markov equivalent causal diagrams, which, unlike a single, fully-specified causal diagram, can be inferred from the observational distribution.
"
http://papers.nips.cc/paper/by-source-2019-2543,https://proceedings.neurips.cc/paper/2019/file/1b113258af3968aaf3969ca67e744ff8-Paper.pdf,Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning,David Janz &middot; Jiri Hron &middot; Przemysław Mazur &middot; Katja Hofmann &middot; José Miguel Hernández-Lobato &middot; Sebastian Tschiatschek,djanz.org/successor_uncertainties/video,"Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing exploration and exploitation in reinforcement learning. Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL. However, we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective, and provably fail in sparse reward problems. Moreover, we find that propagation of uncertainty, a property of PSRL previously thought important for exploration, does not preclude this failure. We use these insights to design Successor Uncertainties (SU), a cheap and easy to implement RVF algorithm that retains key properties of PSRL. SU is highly effective on hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it surpasses human performance on 38 of 49 games tested (achieving a median human normalised score of 2.09), and outperforms its closest RVF competitor, Bootstrapped DQN, on 36 of those."
http://papers.nips.cc/paper/by-source-2019-4951,https://proceedings.neurips.cc/paper/2019/file/8cbe9ce23f42628c98f80fa0fac8b19a-Paper.pdf,Sampling Networks and Aggregate Simulation for Online POMDP Planning,Hao(Jackson) Cui &middot; Roni Khardon,https://youtu.be/IcC-O1AALXE,"The paper introduces a new algorithm for planning in partially observable Markov decision processes (POMDP) based on the idea of aggregate simulation. The algorithm uses product distributions to approximate the belief state and shows how to build a representation graph of an approximate action-value function over belief space. The graph captures the result of simulating the model in aggregate under independence assumptions, giving a symbolic representation of the value function. The algorithm supports large observation spaces using sampling networks, a representation of the process of sampling values of observations, which is integrated into the graph representation. Following previous work in MDPs this approach enables action selection in POMDPs through gradient optimization over the graph representation. This approach complements recent algorithms for POMDPs which are based on particle representations of belief states and an explicit search for action selection. Our approach enables scaling to large factored action spaces in addition to large state spaces and observation spaces. An experimental evaluation demonstrates that the algorithm provides excellent performance relative to state of the art in large POMDP problems. "
http://papers.nips.cc/paper/by-source-2019-175,https://proceedings.neurips.cc/paper/2019/file/82161242827b703e6acf9c726942a1e4-Paper.pdf,Chasing Ghosts: Instruction Following as Bayesian State Tracking,Peter Anderson &middot; Ayush Shrivastava &middot; Devi Parikh &middot; Dhruv Batra &middot; Stefan Lee,https://www.panderson.me/images/vln-chasing-ghosts.mp4,"A visually-grounded navigation instruction can be interpreted as a sequence of expected observations and actions an agent following the correct trajectory would encounter and perform. Based on this intuition, we formulate the problem of finding the goal location in Vision-and-Language Navigation (VLN) within the framework of Bayesian state tracking - learning observation and motion models conditioned on these expectable events. Together with a mapper that constructs a semantic spatial map on-the-fly during navigation, we formulate an end-to-end differentiable Bayes filter and train it to identify the goal by predicting the most likely trajectory through the map according to the instructions. The resulting navigation policy constitutes a new approach to instruction following that explicitly models a probability distribution over states, encoding strong geometric and algorithmic priors while enabling greater explainability. Our experiments show that our approach outperforms a strong LingUNet baseline when predicting the goal location on the map. On the full VLN task, i.e. navigating to the goal location, our approach achieves promising results with less reliance on navigation constraints."
http://papers.nips.cc/paper/by-source-2019-6214,https://proceedings.neurips.cc/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf,Uniform convergence may be unable to explain generalization in deep learning,Vaishnavh Nagarajan &middot; J. Zico Kolter,https://www.youtube.com/watch?v=o3GfnEjTdIQ,"Aimed at explaining the surprisingly good generalization behavior of overparameterized deep networks, recent works have developed a variety of generalization bounds for deep learning,  all based on the fundamental learning-theoretic technique of uniform convergence. While
"
http://papers.nips.cc/paper/by-source-2019-6250,https://proceedings.neurips.cc/paper/2019/file/947018640bf36a2bb609d3557a285329-Paper.pdf,Causal Confusion in Imitation Learning,Pim de Haan &middot; Dinesh Jayaraman &middot; Sergey Levine,https://sites.google.com/view/causal-confusion,"Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive &quot;causal misidentification&quot; phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions---either environment interaction or expert queries---to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations."
http://papers.nips.cc/paper/by-source-2019-1457,https://proceedings.neurips.cc/paper/2019/file/01d8bae291b1e4724443375634ccfa0e-Paper.pdf,Asymmetric Valleys: Beyond Sharp and Flat Local Minima,Haowei He &middot; Gao Huang &middot; Yang Yuan,https://youtu.be/zWwaJ3gbv_Q,"Despite the non-convex nature of their loss functions, deep neural networks are known to generalize well when optimized with stochastic gradient descent (SGD). Recent work conjectures that SGD with proper conﬁguration is able to ﬁnd wide and ﬂat local minima, which are correlated with good generalization performance. In this paper, we observe that local minima of modern deep networks are more than being ﬂat or sharp. Instead, at a local minimum there exist many asymmetric directions such that the loss increases abruptly along one side, and slowly along the opposite side – we formally deﬁne such minima as asymmetric valleys. Under mild assumptions, we ﬁrst prove that for asymmetric valleys, a solution biased towards the ﬂat side generalizes better than the exact empirical minimizer. Then, we show that performing weight averaging along the SGD trajectory implicitly induces such biased solutions. This provides theoretical explanations for a series of intriguing phenomena observed in recent work [25, 5, 51]. Finally, extensive empirical experiments on both modern deep networks and simple 2 layer networks are conducted to validate our assumptions and analyze the intriguing properties of asymmetric valleys."
http://papers.nips.cc/paper/by-source-2019-98,https://proceedings.neurips.cc/paper/2019/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement,Chao Yang &middot; Xiaojian Ma &middot; Wenbing Huang &middot; Fuchun Sun &middot; Huaping Liu &middot; Junzhou Huang &middot; Chuang Gan,https://sites.google.com/view/neurips19-iddm/home,"This paper studies Learning from Observations (LfO) for imitation learning with access to state-only demonstrations. In contrast to Learning from Demonstration (LfD) that involves both action and state supervisions, LfO is more practical in leveraging previously inapplicable resources (e.g., videos), yet more challenging due to the incomplete expert guidance. In this paper, we investigate LfO and its difference with LfD in both theoretical and practical perspectives. We first prove that the gap between LfD and LfO actually lies in the disagreement of inverse dynamics models between the imitator and expert, if following the modeling approach of GAIL. More importantly, the upper bound of this gap is revealed by a negative causal entropy which can be minimized in a model-free way. We term our method as Inverse-Dynamics-Disagreement-Minimization (IDDM) which enhances the conventional LfO method through further bridging the gap to LfD. Considerable empirical results on challenging benchmarks indicate that our method attains consistent improvements over other LfO counterparts."
http://papers.nips.cc/paper/by-source-2019-1955,https://proceedings.neurips.cc/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf,Implicit Generation and Modeling with Energy Based Models,Yilun Du &middot; Igor Mordatch,https://sites.google.com/view/igebm,"Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts."
http://papers.nips.cc/paper/by-source-2019-5254,https://proceedings.neurips.cc/paper/2019/file/5d0d5594d24f0f955548f0fc0ff83d10-Paper.pdf,Residual Flows for Invertible Generative Modeling,Tian Qi Chen &middot; Jens Behrmann &middot; David Duvenaud &middot; Joern-Henrik Jacobsen,http://www.cs.toronto.edu/~rtqichen/posters/residual_flows_poster.pdf,"Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network&#x27;s expressiveness. We give a tractable unbiased estimate of the log density, and reduce the memory required during training by a factor of ten. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid gradient saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling."
http://papers.nips.cc/paper/by-source-2019-5115,https://proceedings.neurips.cc/paper/2019/file/d324a0cc02881779dcda44a675fdcaaa-Paper.pdf,Guided Meta-Policy Search,Russell Mendonca &middot; Abhishek Gupta &middot; Rosen Kralev &middot; Pieter Abbeel &middot; Sergey Levine &middot; Chelsea Finn,https://drive.google.com/file/d/1hwnb9TaY4uPTvsLv1C6WULqcVWMm9AIc/view?usp=sharing,"Reinforcement learning (RL) algorithms have demonstrated promising results on complex tasks, yet often require impractical numbers of samples because they learn from scratch. Meta-RL aims to address this challenge by leveraging experience from previous tasks so as to more quickly solve new tasks. However, in practice, these algorithms generally also require large amounts of on-policy experience during the \emph{meta-training} process, making them impractical for use in many problems. To this end, we propose to learn a reinforcement learning procedure in a federated way, where individual off-policy learners can solve the individual meta-training tasks, and then consolidate these solutions into a single meta-learner. Since the central meta-learner learns by imitating the solutions to the individual tasks, it can accommodate either the standard meta-RL problem setting, or a hybrid setting where some or all tasks are provided with example demonstrations. The former results in an approach that can leverage policies learned for previous tasks without significant amounts of on-policy data during meta-training, whereas the latter is particularly useful in cases where demonstrations are easy for a person to provide. Across a number of continuous control meta-RL problems, we demonstrate significant improvements in meta-RL sample efficiency in comparison to prior work as well as the ability to scale to domains with visual observations. "
http://papers.nips.cc/paper/by-source-2019-4792,https://proceedings.neurips.cc/paper/2019/file/4ff3e350028d0cfcb92c3a87a57585b1-Paper.pdf,Parameter elimination in particle Gibbs sampling,Anna Wigren &middot; Riccardo Sven Risuleo &middot; Lawrence Murray &middot; Fredrik Lindsten,https://www.it.uu.se/katalog/annwi999/Publications/Neurips2019,"Bayesian inference in state-space models is challenging due to high-dimensional state trajectories. A viable approach is particle Markov chain Monte Carlo (PMCMC), combining MCMC and sequential Monte Carlo to form ``exact approximations&#x27;&#x27; to otherwise-intractable MCMC methods. The performance of the approximation is limited to that of the exact method. We focus on particle Gibbs (PG) and particle Gibbs with ancestor sampling (PGAS), improving their performance beyond that of the ideal Gibbs sampler (which they approximate) by marginalizing out one or more parameters. This is possible when the parameter(s) has a conjugate prior relationship with the complete data likelihood. Marginalization yields a non-Markov model for inference, but we show that, in contrast to the general case, the methods still scale linearly in time. While marginalization can be cumbersome to implement, recent advances in probabilistic programming have enabled its automation. We demonstrate how the marginalized methods are viable as efficient inference backends in probabilistic programming, and demonstrate with examples in ecology and epidemiology."
http://papers.nips.cc/paper/by-source-2019-7901,https://proceedings.neurips.cc/paper/2019/file/3e883840fee4384dd3d2afea5e822517-Paper.pdf,Learning Positive Functions with Pseudo Mirror Descent,Yingxiang Yang &middot; Haoxiang Wang &middot; Negar Kiyavash &middot; Niao He,https://www.dropbox.com/sh/veks5wyi9p1s4wd/AACfKZ-D-gjlorqfqT_bH_Gea?dl=0,"The nonparametric learning of positive-valued functions appears widely in machine learning, especially in the context of estimating intensity functions of point processes. Yet, existing approaches either require computing expensive projections or semidefinite relaxations, or lack convexity and theoretical guarantees after introducing nonlinear link functions. In this paper, we propose a novel algorithm, pseudo mirror descent, that performs efficient estimation of positive functions within a Hilbert space without expensive projections. The algorithm guarantees positivity by performing mirror descent with an appropriately selected Bregman divergence, and a pseudo-gradient is adopted to speed up the gradient evaluation procedure in practice. We analyze both asymptotic and nonasymptotic convergence of the algorithm. Through simulations, we show that pseudo mirror descent outperforms the state-of-the-art benchmarks for learning intensities of Poisson and multivariate Hawkes processes, in terms of both computational efficiency and accuracy."
http://papers.nips.cc/paper/by-source-2019-6771,https://proceedings.neurips.cc/paper/2019/file/195f15384c2a79cedf293e4a847ce85c-Paper.pdf,Hindsight Credit Assignment,Anna Harutyunyan &middot; Will Dabney &middot; Thomas Mesnard &middot; Mohammad Gheshlaghi Azar &middot; Bilal Piot &middot; Nicolas Heess &middot; Hado van Hasselt &middot; Gregory Wayne &middot; Satinder Singh &middot; Doina Precup &middot; Remi Munos,https://youtu.be/2EnckznIV2o,"We consider the problem of efficient credit assignment in reinforcement learning. In order to efficiently and meaningfully utilize new data, we propose to explicitly assign credit to past decisions based on the likelihood of them having led to the observed outcome. This approach uses new information in hindsight, rather than employing foresight. Somewhat surprisingly, we show that value functions can be rewritten through this lens, yielding a new family of algorithms. We study the properties of these algorithms, and empirically show that they successfully address important credit assignment challenges, through a set of illustrative tasks."
http://papers.nips.cc/paper/by-source-2019-8295,https://proceedings.neurips.cc/paper/2019/file/d202ed5bcfa858c15a9f383c3e386ab2-Paper.pdf,"Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations",Debraj Basu &middot; Deepesh Data &middot; Can Karakus &middot; Suhas Diggavi,https://youtu.be/UxXdAdkyAF0,"Communication bottleneck has been identified as a significant issue in distributed optimization of large-scale learning models. Recently, several approaches to mitigate this problem have been proposed, including different forms of gradient compression or computing local models and mixing them iteratively. In this paper we propose Qsparse-local-SGD algorithm, which combines aggressive sparsification with quantization and local computation along with error compensation, by keeping track of the difference between the true and compressed gradients. We propose both synchronous and asynchronous implementations of Qsparse-local-SGD. We analyze convergence for Qsparse-local-SGD in the distributed case, for smooth non-convex and convex objective functions. We demonstrate that Qsparse-local-SGD converges at the same rate as vanilla distributed SGD for many important classes of sparsifiers and quantizers. We use Qsparse-local-SGD to train ResNet-50 on ImageNet, and show that it results in significant savings over the state-of-the-art, in the number of bits transmitted to reach target accuracy."
http://papers.nips.cc/paper/by-source-2019-5115,https://proceedings.neurips.cc/paper/2019/file/d324a0cc02881779dcda44a675fdcaaa-Paper.pdf,Guided Meta-Policy Search,Russell Mendonca &middot; Abhishek Gupta &middot; Rosen Kralev &middot; Pieter Abbeel &middot; Sergey Levine &middot; Chelsea Finn,https://drive.google.com/file/d/1hwnb9TaY4uPTvsLv1C6WULqcVWMm9AIc/view?usp=sharing,"Reinforcement learning (RL) algorithms have demonstrated promising results on complex tasks, yet often require impractical numbers of samples because they learn from scratch. Meta-RL aims to address this challenge by leveraging experience from previous tasks so as to more quickly solve new tasks. However, in practice, these algorithms generally also require large amounts of on-policy experience during the \emph{meta-training} process, making them impractical for use in many problems. To this end, we propose to learn a reinforcement learning procedure in a federated way, where individual off-policy learners can solve the individual meta-training tasks, and then consolidate these solutions into a single meta-learner. Since the central meta-learner learns by imitating the solutions to the individual tasks, it can accommodate either the standard meta-RL problem setting, or a hybrid setting where some or all tasks are provided with example demonstrations. The former results in an approach that can leverage policies learned for previous tasks without significant amounts of on-policy data during meta-training, whereas the latter is particularly useful in cases where demonstrations are easy for a person to provide. Across a number of continuous control meta-RL problems, we demonstrate significant improvements in meta-RL sample efficiency in comparison to prior work as well as the ability to scale to domains with visual observations. "
http://papers.nips.cc/paper/by-source-2019-7901,https://proceedings.neurips.cc/paper/2019/file/3e883840fee4384dd3d2afea5e822517-Paper.pdf,Learning Positive Functions with Pseudo Mirror Descent,Yingxiang Yang &middot; Haoxiang Wang &middot; Negar Kiyavash &middot; Niao He,https://www.dropbox.com/sh/veks5wyi9p1s4wd/AACfKZ-D-gjlorqfqT_bH_Gea?dl=0,"The nonparametric learning of positive-valued functions appears widely in machine learning, especially in the context of estimating intensity functions of point processes. Yet, existing approaches either require computing expensive projections or semidefinite relaxations, or lack convexity and theoretical guarantees after introducing nonlinear link functions. In this paper, we propose a novel algorithm, pseudo mirror descent, that performs efficient estimation of positive functions within a Hilbert space without expensive projections. The algorithm guarantees positivity by performing mirror descent with an appropriately selected Bregman divergence, and a pseudo-gradient is adopted to speed up the gradient evaluation procedure in practice. We analyze both asymptotic and nonasymptotic convergence of the algorithm. Through simulations, we show that pseudo mirror descent outperforms the state-of-the-art benchmarks for learning intensities of Poisson and multivariate Hawkes processes, in terms of both computational efficiency and accuracy."
http://papers.nips.cc/paper/by-source-2019-2434,https://proceedings.neurips.cc/paper/2019/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf,On Adversarial Mixup Resynthesis,Christopher Beckham &middot; Sina Honari &middot; Alex Lamb &middot; Vikas Verma &middot; Farnoosh Ghadiri &middot; R Devon Hjelm &middot; Yoshua Bengio &middot; Chris Pal,https://www.youtube.com/watch?v=ezbC3_VZeNY,"In this paper, we explore new approaches to combining information encoded within the learned representations of auto-encoders. We explore models that are capable of combining the attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator for real versus synthesised data. Furthermore, we explore the use of such an architecture in the context of semi-supervised learning, where we learn a mixing function whose objective is to produce interpolations of hidden states, or masked combinations of latent representations that are consistent with a conditioned class label. We show quantitative and qualitative evidence that such a formulation is an interesting avenue of research."
http://papers.nips.cc/paper/by-source-2019-2585,https://proceedings.neurips.cc/paper/2019/file/36e729ec173b94133d8fa552e4029f8b-Paper.pdf,Symmetry-Based Disentangled Representation Learning requires Interaction with Environments,Hugo Caselles-Dupré &middot; Michael Garcia Ortiz &middot; David Filliat,https://youtu.be/jyN3ZQNH1GI,"Finding a generally accepted formal definition of a disentangled representation in the context of an agent behaving in an environment is an important challenge towards the construction of data-efficient autonomous agents. Higgins et al. recently proposed Symmetry-Based Disentangled Representation Learning, a definition based on a characterization of symmetries in the environment using group theory. We build on their work and make observations, theoretical and empirical, that lead us to argue that Symmetry-Based Disentangled Representation Learning cannot only be based on static observations: agents should interact with the environment to discover its symmetries. Our experiments can be reproduced in Colab and the code is available on GitHub."
http://papers.nips.cc/paper/by-source-2019-1955,https://proceedings.neurips.cc/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf,Implicit Generation and Modeling with Energy Based Models,Yilun Du &middot; Igor Mordatch,https://sites.google.com/view/igebm,"Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts."
http://papers.nips.cc/paper/by-source-2019-5254,https://proceedings.neurips.cc/paper/2019/file/5d0d5594d24f0f955548f0fc0ff83d10-Paper.pdf,Residual Flows for Invertible Generative Modeling,Tian Qi Chen &middot; Jens Behrmann &middot; David Duvenaud &middot; Joern-Henrik Jacobsen,http://www.cs.toronto.edu/~rtqichen/posters/residual_flows_poster.pdf,"Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network&#x27;s expressiveness. We give a tractable unbiased estimate of the log density, and reduce the memory required during training by a factor of ten. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid gradient saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling."
http://papers.nips.cc/paper/by-source-2019-2300,https://proceedings.neurips.cc/paper/2019/file/46a558d97954d0692411c861cf78ef79-Paper.pdf,Limitations of the empirical Fisher approximation for natural gradient descent,Frederik Kunstner &middot; Philipp Hennig &middot; Lukas Balles,https://www.youtube.com/watch?v=f0jeOi4pW_A,"Natural gradient descent, which preconditions a gradient descent update
"
http://papers.nips.cc/paper/by-source-2019-3487,https://proceedings.neurips.cc/paper/2019/file/944a5ae3483ed5c1e10bbccb7942a279-Paper.pdf,Maximum Mean Discrepancy Gradient Flow,Michael Arbel &middot; Anna Korba &middot; Adil Salim &middot; Arthur Gretton,https://michaelarbel.github.io/materials/MMD_flow.mp4,"  We construct a Wasserstein gradient flow of the maximum mean discrepancy (MMD) and study its convergence properties.
"
http://papers.nips.cc/paper/by-source-2019-1973,https://proceedings.neurips.cc/paper/2019/file/deb54ffb41e085fd7f69a75b6359c989-Paper.pdf,Tight Dimension Independent Lower Bound on the Expected Convergence Rate for Diminishing Step Sizes in SGD,Ha Nguyen &middot; Lam Nguyen &middot; Marten van Dijk,https://drive.google.com/file/d/18st_NJYsYWUimP3HhM1UW-LTALupQ-ms/view?usp=sharing,We study the convergence of Stochastic Gradient Descent (SGD) for strongly convex  objective functions. We prove for all $t$ a lower bound on the expected convergence rate after the $t$-th SGD iteration; the lower bound is over all possible sequences of diminishing step sizes. It implies that recently proposed sequences of step sizes at ICML 2018 and ICML 2019 are {\em universally} close to optimal in that the expected convergence rate after {\em each} iteration is within a factor $32$ of our lower bound. This factor is independent of dimension $d$. We offer a framework for comparing with lower bounds in state-of-the-art literature and when applied to SGD for strongly convex objective functions our lower bound is a significant factor $775\cdot d$ larger compared to existing work.
http://papers.nips.cc/paper/by-source-2019-1457,https://proceedings.neurips.cc/paper/2019/file/01d8bae291b1e4724443375634ccfa0e-Paper.pdf,Asymmetric Valleys: Beyond Sharp and Flat Local Minima,Haowei He &middot; Gao Huang &middot; Yang Yuan,https://youtu.be/zWwaJ3gbv_Q,"Despite the non-convex nature of their loss functions, deep neural networks are known to generalize well when optimized with stochastic gradient descent (SGD). Recent work conjectures that SGD with proper conﬁguration is able to ﬁnd wide and ﬂat local minima, which are correlated with good generalization performance. In this paper, we observe that local minima of modern deep networks are more than being ﬂat or sharp. Instead, at a local minimum there exist many asymmetric directions such that the loss increases abruptly along one side, and slowly along the opposite side – we formally deﬁne such minima as asymmetric valleys. Under mild assumptions, we ﬁrst prove that for asymmetric valleys, a solution biased towards the ﬂat side generalizes better than the exact empirical minimizer. Then, we show that performing weight averaging along the SGD trajectory implicitly induces such biased solutions. This provides theoretical explanations for a series of intriguing phenomena observed in recent work [25, 5, 51]. Finally, extensive empirical experiments on both modern deep networks and simple 2 layer networks are conducted to validate our assumptions and analyze the intriguing properties of asymmetric valleys."
http://papers.nips.cc/paper/by-source-2019-2038,https://proceedings.neurips.cc/paper/2019/file/2557911c1bf75c2b643afb4ecbfc8ec2-Paper.pdf,"Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates",Sharan Vaswani &middot; Aaron Mishkin &middot; Issam Laradji &middot; Mark Schmidt &middot; Gauthier Gidel &middot; Simon Lacoste-Julien,https://github.com/IssamLaradji/sls/tree/master/Video,"Recent works have shown that stochastic gradient descent (SGD) achieves the fast convergence rates of full-batch gradient descent for over-parameterized models satisfying certain interpolation conditions. However, the step-size used in these works depends on unknown quantities and SGD&#x27;s practical performance heavily relies on the choice of this step-size. We propose to use line-search techniques to automatically set the step-size when training models that can interpolate the data. In the interpolation setting, we prove that SGD with a stochastic variant of the classic Armijo line-search attains the deterministic convergence rates for both convex and strongly-convex functions. Under additional assumptions, SGD with Armijo line-search is shown to achieve fast convergence for non-convex functions. Furthermore, we show that stochastic extra-gradient with a Lipschitz line-search attains linear convergence for an important class of non-convex functions and saddle-point problems satisfying interpolation. To improve the proposed methods&#x27; practical performance, we give heuristics to use larger step-sizes and acceleration. We compare the proposed algorithms against numerous optimization methods on standard classification tasks using both kernel methods and deep networks. The proposed methods result in competitive performance across all models and datasets, while being robust to the precise choices of hyper-parameters. For multi-class classification using deep networks, SGD with Armijo line-search results in both faster convergence and better generalization. "
http://papers.nips.cc/paper/by-source-2019-7995,https://proceedings.neurips.cc/paper/2019/file/2d44e06a7038f2dd98f0f54c4be35e22-Paper.pdf,Integrating Markov processes with structural causal modeling enables counterfactual inference in complex systems,Robert Ness &middot; Kaushal Paneri &middot; Olga Vitek,http://bit.ly/MarkovP2SCM,"This manuscript contributes a general and practical framework for casting a Markov process model of a system at equilibrium as a structural causal model, and carrying out counterfactual inference. Markov processes mathematically describe the mechanisms in the system, and predict the system’s equilibrium behavior upon intervention, but do not support counterfactual inference. In contrast, structural causal models support counterfactual inference, but do not identify the mechanisms. This manuscript leverages the benefits of both approaches. We define the structural causal models in terms of the parameters and the equilibrium dynamics of the Markov process models, and counterfactual inference flows from these settings. The proposed approach alleviates the identifiability drawback of the structural causal models, in that the counterfactual inference is consistent with the counterfactual trajectories simulated from the Markov process model. We showcase the benefits of this framework in case studies of complex biomolecular systems with nonlinear dynamics. We illustrate that, in presence of Markov process model misspecification, counterfactual inference leverages prior data, and therefore estimates the outcome of an intervention more accurately than a direct simulation."
http://papers.nips.cc/paper/by-source-2019-6256,https://proceedings.neurips.cc/paper/2019/file/a87c11b9100c608b7f8e98cfa316ff7b-Paper.pdf,The Case for Evaluating Causal Models Using Interventional Measures and Empirical Data,Amanda Gentzel &middot; Dan Garant &middot; David Jensen,https://agentzel.github.io/publications/neurips-2019-video.m4v,"Causal inference is central to many areas of artificial intelligence, including complex reasoning, planning, knowledge-base construction, robotics, explanation, and fairness. An active community of researchers develops and enhances algorithms that learn causal models from data, and this work has produced a series of impressive technical advances.  However, evaluation techniques for causal modeling algorithms have remained somewhat primitive, limiting what we can learn from experimental studies of algorithm performance, constraining the types of algorithms and model representations that researchers consider, and creating a gap between theory and practice.  We argue for more frequent use of evaluation techniques that examine interventional measures rather than structural or observational measures, and that evaluate those measures on empirical data rather than synthetic data.  We survey the current practice in evaluation and show that the techniques we recommend are rarely used in practice. We show that such techniques are feasible and that data sets are available to conduct such evaluations.  We also show that these techniques produce substantially different results than using structural measures and synthetic data."
http://papers.nips.cc/paper/by-source-2019-4792,https://proceedings.neurips.cc/paper/2019/file/4ff3e350028d0cfcb92c3a87a57585b1-Paper.pdf,Parameter elimination in particle Gibbs sampling,Anna Wigren &middot; Riccardo Sven Risuleo &middot; Lawrence Murray &middot; Fredrik Lindsten,https://www.it.uu.se/katalog/annwi999/Publications/Neurips2019,"Bayesian inference in state-space models is challenging due to high-dimensional state trajectories. A viable approach is particle Markov chain Monte Carlo (PMCMC), combining MCMC and sequential Monte Carlo to form ``exact approximations&#x27;&#x27; to otherwise-intractable MCMC methods. The performance of the approximation is limited to that of the exact method. We focus on particle Gibbs (PG) and particle Gibbs with ancestor sampling (PGAS), improving their performance beyond that of the ideal Gibbs sampler (which they approximate) by marginalizing out one or more parameters. This is possible when the parameter(s) has a conjugate prior relationship with the complete data likelihood. Marginalization yields a non-Markov model for inference, but we show that, in contrast to the general case, the methods still scale linearly in time. While marginalization can be cumbersome to implement, recent advances in probabilistic programming have enabled its automation. We demonstrate how the marginalized methods are viable as efficient inference backends in probabilistic programming, and demonstrate with examples in ecology and epidemiology."
http://papers.nips.cc/paper/by-source-2019-6250,https://proceedings.neurips.cc/paper/2019/file/947018640bf36a2bb609d3557a285329-Paper.pdf,Causal Confusion in Imitation Learning,Pim de Haan &middot; Dinesh Jayaraman &middot; Sergey Levine,https://sites.google.com/view/causal-confusion,"Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive &quot;causal misidentification&quot; phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions---either environment interaction or expert queries---to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations."
http://papers.nips.cc/paper/by-source-2019-6805,https://proceedings.neurips.cc/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf,When to Trust Your Model: Model-Based Policy Optimization,Michael Janner &middot; Justin Fu &middot; Marvin Zhang &middot; Sergey Levine,https://people.eecs.berkeley.edu/~janner/mbpo/,"Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely."
http://papers.nips.cc/paper/by-source-2019-2815,https://proceedings.neurips.cc/paper/2019/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf,On the Utility of Learning about Humans for Human-AI Coordination,Micah Carroll &middot; Rohin Shah &middot; Mark Ho &middot; Tom Griffiths &middot; Sanjit Seshia &middot; Pieter Abbeel &middot; Anca Dragan,https://drive.google.com/file/d/13XjCGQ-suZGQOmCZQNbSiL1rMfExnKGj/view?usp=sharing,"While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human&#x27;s gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked_ai."
http://papers.nips.cc/paper/by-source-2019-6771,https://proceedings.neurips.cc/paper/2019/file/195f15384c2a79cedf293e4a847ce85c-Paper.pdf,Hindsight Credit Assignment,Anna Harutyunyan &middot; Will Dabney &middot; Thomas Mesnard &middot; Mohammad Gheshlaghi Azar &middot; Bilal Piot &middot; Nicolas Heess &middot; Hado van Hasselt &middot; Gregory Wayne &middot; Satinder Singh &middot; Doina Precup &middot; Remi Munos,https://youtu.be/2EnckznIV2o,"We consider the problem of efficient credit assignment in reinforcement learning. In order to efficiently and meaningfully utilize new data, we propose to explicitly assign credit to past decisions based on the likelihood of them having led to the observed outcome. This approach uses new information in hindsight, rather than employing foresight. Somewhat surprisingly, we show that value functions can be rewritten through this lens, yielding a new family of algorithms. We study the properties of these algorithms, and empirically show that they successfully address important credit assignment challenges, through a set of illustrative tasks."
http://papers.nips.cc/paper/by-source-2019-98,https://proceedings.neurips.cc/paper/2019/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement,Chao Yang &middot; Xiaojian Ma &middot; Wenbing Huang &middot; Fuchun Sun &middot; Huaping Liu &middot; Junzhou Huang &middot; Chuang Gan,https://sites.google.com/view/neurips19-iddm/home,"This paper studies Learning from Observations (LfO) for imitation learning with access to state-only demonstrations. In contrast to Learning from Demonstration (LfD) that involves both action and state supervisions, LfO is more practical in leveraging previously inapplicable resources (e.g., videos), yet more challenging due to the incomplete expert guidance. In this paper, we investigate LfO and its difference with LfD in both theoretical and practical perspectives. We first prove that the gap between LfD and LfO actually lies in the disagreement of inverse dynamics models between the imitator and expert, if following the modeling approach of GAIL. More importantly, the upper bound of this gap is revealed by a negative causal entropy which can be minimized in a model-free way. We term our method as Inverse-Dynamics-Disagreement-Minimization (IDDM) which enhances the conventional LfO method through further bridging the gap to LfD. Considerable empirical results on challenging benchmarks indicate that our method attains consistent improvements over other LfO counterparts."
http://papers.nips.cc/paper/by-source-2019-4552,https://proceedings.neurips.cc/paper/2019/file/f8905bd3df64ace64a68e154ba72f24c-Paper.pdf,Multiclass Learning from Contradictions,Sauptik Dhar &middot; Vladimir Cherkassky &middot; Mohak Shah,https://youtu.be/6TayjC7qqPg,"We introduce the notion of learning from contradictions, a.k.a Universum learning, for multiclass problems and propose a novel formulation for multiclass universum SVM (MU-SVM). We show that learning from contradictions (using MU-SVM) incurs lower sample complexity compared to multiclass SVM (M-SVM) by deriving the Natarajan dimension for sample complexity for PAC-learnability of MU-SVM. We also propose an analytic span bound for MU-SVM and demonstrate its utility for model selection resulting in $\sim 2-4 \times$ faster computation times than standard resampling techniques. We empirically demonstrate the efficacy of MU- SVM on several real world datasets achieving $&gt;$ 20\% improvement in test accuracies compared to M-SVM. Insights into the underlying behavior of MU-SVM using a histograms-of-projections method are also provided."
http://papers.nips.cc/paper/by-source-2019-667,https://proceedings.neurips.cc/paper/2019/file/b5dc4e5d9b495d0196f61d45b26ef33e-Paper.pdf,Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations,Vincent Sitzmann &middot; Michael Zollhoefer &middot; Gordon Wetzstein,https://vsitzmann.github.io/srns/video/,"Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model."
http://papers.nips.cc/paper/by-source-2019-6628,https://proceedings.neurips.cc/paper/2019/file/1c336b8080f82bcc2cd2499b4c57261d-Paper.pdf,Calibration tests in multi-class classification: A unifying framework,David Widmann &middot; Fredrik Lindsten &middot; Dave Zachariah,https://vimeo.com/369295144,"In safety-critical applications a probabilistic model is usually required to be calibrated, i.e., to capture the uncertainty of its predictions accurately. In multi-class classification, calibration of the most confident predictions only is often not sufficient. We propose and study calibration measures for multi-class classification that generalize existing measures such as the expected calibration error, the maximum calibration error, and the maximum mean calibration error. We propose and evaluate empirically different consistent and unbiased estimators for a specific class of measures based on matrix-valued kernels. Importantly, these estimators can be interpreted as test statistics associated with well-defined bounds and approximations of the p-value under the null hypothesis that the model is calibrated, significantly improving the interpretability of calibration measures, which otherwise lack any meaningful unit or scale."
http://papers.nips.cc/paper/by-source-2019-9031,https://proceedings.neurips.cc/paper/2019/file/e2e14235335d2c0aa5f6855e339233d9-Paper.pdf,Fast structure learning with modular regularization,Greg Ver Steeg &middot; Hrayr Harutyunyan &middot; Daniel Moyer &middot; Aram Galstyan,http://bit.ly/2N9IJ66,"Estimating graphical model structure from high-dimensional and undersampled data is a fundamental problem in many scientific fields.
"
http://papers.nips.cc/paper/by-source-2019-9031,https://proceedings.neurips.cc/paper/2019/file/e2e14235335d2c0aa5f6855e339233d9-Paper.pdf,Fast structure learning with modular regularization,Greg Ver Steeg &middot; Hrayr Harutyunyan &middot; Daniel Moyer &middot; Aram Galstyan,http://bit.ly/2N9IJ66,"Estimating graphical model structure from high-dimensional and undersampled data is a fundamental problem in many scientific fields.
"
http://papers.nips.cc/paper/by-source-2019-6658,https://proceedings.neurips.cc/paper/2019/file/8ca01ea920679a0fe3728441494041b9-Paper.pdf,Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration,Meelis Kull &middot; Miquel Perello Nieto &middot; Markus Kängsepp &middot; Telmo Silva Filho &middot; Hao Song &middot; Peter Flach,https://dirichletcal.github.io/documents/neurips2019/video/,"Class probabilities predicted by most multiclass classifiers are uncalibrated, often tending towards over-confidence. With neural networks, calibration can be improved by temperature scaling, a method to learn a single corrective multiplicative factor for inputs to the last softmax layer. On non-neural models the existing methods apply binary calibration in a pairwise or one-vs-rest fashion. We propose a natively multiclass calibration method applicable to classifiers from any model class, derived from Dirichlet distributions and generalising the beta calibration method from binary classification. It is easily implemented with neural nets since it is equivalent to log-transforming the uncalibrated probabilities, followed by one linear layer and softmax. Experiments demonstrate improved probabilistic predictions according to multiple measures (confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of datasets and classifiers. Parameters of the learned Dirichlet calibration map
"
http://papers.nips.cc/paper/by-source-2019-6628,https://proceedings.neurips.cc/paper/2019/file/1c336b8080f82bcc2cd2499b4c57261d-Paper.pdf,Calibration tests in multi-class classification: A unifying framework,David Widmann &middot; Fredrik Lindsten &middot; Dave Zachariah,https://vimeo.com/369295144,"In safety-critical applications a probabilistic model is usually required to be calibrated, i.e., to capture the uncertainty of its predictions accurately. In multi-class classification, calibration of the most confident predictions only is often not sufficient. We propose and study calibration measures for multi-class classification that generalize existing measures such as the expected calibration error, the maximum calibration error, and the maximum mean calibration error. We propose and evaluate empirically different consistent and unbiased estimators for a specific class of measures based on matrix-valued kernels. Importantly, these estimators can be interpreted as test statistics associated with well-defined bounds and approximations of the p-value under the null hypothesis that the model is calibrated, significantly improving the interpretability of calibration measures, which otherwise lack any meaningful unit or scale."
http://papers.nips.cc/paper/by-source-2019-8301,https://proceedings.neurips.cc/paper/2019/file/1e79596878b2320cac26dd792a6c51c9-Paper.pdf,Likelihood Ratios for Out-of-Distribution Detection,Jie Ren &middot; Peter Liu &middot; Emily Fertig &middot; Jasper Snoek &middot; Ryan  Poplin &middot; Mark Depristo &middot; Joshua V Dillon &middot; Balaji Lakshminarayanan,https://youtu.be/-FduW9ZWAR4,"Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. Finally, we demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images."
http://papers.nips.cc/paper/by-source-2019-4757,https://proceedings.neurips.cc/paper/2019/file/d80126524c1e9641333502c664fc6ca1-Paper.pdf,Modelling heterogeneous distributions with an Uncountable Mixture of Asymmetric Laplacians,Axel Brando &middot; Jose A Rodriguez &middot; Jordi Vitria &middot; Alberto Rubio Muñoz,https://vimeo.com/369179175/a17d3de911,"In regression tasks, aleatoric uncertainty is commonly addressed by considering a parametric distribution of the output variable, which is based on strong assumptions such as symmetry, unimodality or by supposing a restricted shape. These assumptions are too limited in scenarios where complex shapes, strong skews or multiple modes are present. In this paper, we propose a generic deep learning framework that learns an Uncountable Mixture of Asymmetric Laplacians (UMAL), which will allow us to estimate heterogeneous distributions of the output variable and we show its connections to quantile regression. Despite having a fixed number of parameters, the model can be interpreted as an infinite mixture of components, which yields a flexible approximation for heterogeneous distributions. Apart from synthetic cases, we apply this model to room price forecasting and to predict financial operations in personal bank accounts. We demonstrate that UMAL produces proper distributions, which allows us to extract richer insights and to sharpen decision-making."
http://papers.nips.cc/paper/by-source-2019-6422,https://proceedings.neurips.cc/paper/2019/file/ee0c1616bbc82804b2f4b635d4a055fb-Paper.pdf,Prediction of Spatial Point Processes: Regularized Method with Out-of-Sample Guarantees,Muhammad Osama &middot; Dave Zachariah &middot; Peter Stoica,https://youtu.be/pg3Z3HEKHjc,"A spatial point process can be characterized by an intensity function which predicts the number of events that occur across space. In this paper, we develop a method to infer predictive intensity intervals by learning a spatial model using a regularized criterion. We prove that the proposed method exhibits out-of-sample prediction performance guarantees which, unlike standard estimators, are valid even when the spatial model is misspecified. The method is demonstrated using synthetic as well as real spatial data."
http://papers.nips.cc/paper/by-source-2019-159,https://proceedings.neurips.cc/paper/2019/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf,Joint-task Self-supervised Learning for Temporal Correspondence,Xueting Li &middot; Sifei Liu &middot; Shalini De Mello &middot; Xiaolong Wang &middot; Jan Kautz &middot; Ming-Hsuan Yang,https://www.youtube.com/watch?v=Rfy3dG3lRpM&feature=youtu.be,"This paper proposes to learn reliable dense correspondence from videos in a self-supervised manner. Our learning process integrates two highly related tasks: tracking large image regions and establishing fine-grained pixel-level associations between consecutive video frames. We exploit the synergy between both tasks through a shared inter-frame affinity matrix, which simultaneously models transitions between video frames at both the region- and pixel-levels. While region-level localization helps reduce ambiguities in fine-grained matching by narrowing down search regions; fine-grained matching provides bottom-up features to facilitate region-level localization. Our method outperforms the state-of-the-art self-supervised methods on a variety of visual correspondence tasks, including video-object and part-segmentation propagation, keypoint tracking, and object tracking. Our self-supervised method even surpasses the fully-supervised affinity feature representation obtained from a ResNet-18 pre-trained on the ImageNet."
http://papers.nips.cc/paper/by-source-2019-405,https://proceedings.neurips.cc/paper/2019/file/bbcbff5c1f1ded46c25d28119a85c6c2-Paper.pdf,Learning Conditional Deformable Templates with Convolutional Networks,Adrian Dalca &middot; Marianne Rakic &middot; John Guttag &middot; Mert Sabuncu,http://voxelmorph.mit.edu/atlas_creation/files/cr_video.mp4,"	We develop a learning framework for building deformable templates, which play a fundamental role in many image analysis and computational anatomy tasks. Conventional methods for template creation and image alignment to the template have undergone decades of rich technical development. In these frameworks, templates are constructed using an iterative process of template estimation and alignment, which is often computationally very expensive. Due in part to this shortcoming, most methods compute a single template for the entire population of images, or a few templates for  specific sub-groups of the data. In this work, we present a probabilistic model and efficient learning strategy that yields either universal or \textit{conditional} templates, jointly with a neural network that provides efficient alignment of the images to these templates. We demonstrate the usefulness of this method on a variety of domains, with a special focus on neuroimaging. This is particularly useful for clinical applications where a pre-existing template does not exist, or creating a new one with traditional methods can be prohibitively expensive. Our code and atlases are available online as part of the VoxelMorph library at http://voxelmorph.csail.mit.edu."
http://papers.nips.cc/paper/by-source-2019-667,https://proceedings.neurips.cc/paper/2019/file/b5dc4e5d9b495d0196f61d45b26ef33e-Paper.pdf,Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations,Vincent Sitzmann &middot; Michael Zollhoefer &middot; Gordon Wetzstein,https://vsitzmann.github.io/srns/video/,"Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model."
http://papers.nips.cc/paper/by-source-2019-4493,https://proceedings.neurips.cc/paper/2019/file/438124b4c06f3a5caffab2c07863b617-Paper.pdf,Learning-In-The-Loop Optimization: End-To-End Control And Co-Design Of Soft Robots Through Learned Deep Latent Representations,Andrew Spielberg &middot; Allan Zhao &middot; Yuanming Hu &middot; Tao Du &middot; Wojciech Matusik &middot; Daniela Rus,https://youtu.be/umWPhw9o968,"Soft robots have continuum solid bodies that can deform in an infinite number of ways. Controlling soft robots is very challenging as there are no closed form solutions.  We present a learning-in-the-loop co-optimization algorithm in which a latent state representation is learned as the robot figures out how to solve the task. Our solution marries hybrid particle-grid-based simulation with deep, variational convolutional autoencoder architectures that can capture salient features of robot dynamics with high efficacy. We demonstrate our dynamics-aware feature learning algorithm on both 2D and 3D soft robots, and show that it is more robust and faster converging than the dynamics-oblivious baseline.  We validate the behavior of our algorithm with visualizations of the learned representation."
http://papers.nips.cc/paper/by-source-2019-1488,https://proceedings.neurips.cc/paper/2019/file/8a146f1a3da4700cbf03cdc55e2daae6-Paper.pdf,Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller,Pratyusha Sharma &middot; Deepak Pathak &middot; Abhinav Gupta,https://youtu.be/eWBkDuNFEKA,"We study a generalized setup for learning from demonstration to build an agent that can manipulate novel objects in unseen scenarios by looking at only a single video of human demonstration from a third-person perspective. To accomplish this goal, our agent should not only learn to understand the intent of the demonstrated third-person video in its context but also perform the intended task in its environment configuration. Our central insight is to enforce this structure explicitly during learning by decoupling what to achieve (intended task) from how to perform it (controller). We propose a hierarchical setup where a high-level module learns to generate a series of first-person sub-goals conditioned on the third-person video demonstration, and a low-level controller predicts the actions to achieve those sub-goals. Our agent acts from raw image observations without any access to the full state information. We show results on a real robotic platform using Baxter for the manipulation tasks of pouring and placing objects in a box. Project video is available at https://pathak22.github.io/hierarchical-imitation/"
http://papers.nips.cc/paper/by-source-2019-1385,https://proceedings.neurips.cc/paper/2019/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf,Multiview Aggregation for Learning Category-Specific Shape Reconstruction,Srinath Sridhar &middot; Davis Rempe &middot; Julien Valentin &middot; Bouaziz Sofien &middot; Leonidas Guibas,https://geometry.stanford.edu/projects/xnocs,"We investigate the problem of learning category-specific 3D shape reconstruction from a variable number of RGB views of previously unobserved object instances. Most approaches for multiview shape reconstruction operate on sparse shape representations, or assume a fixed number of views. We present a method that can estimate dense 3D shape, and aggregate shape across multiple and varying number of input views. Given a single input view of an object instance, we propose a representation that encodes the dense shape of the visible object surface as well as the surface behind line of sight occluded by the visible surface. When multiple input views are available, the shape representation is designed to be aggregated into a single 3D shape using an inexpensive union operation. We train a 2D CNN to learn to predict this representation from a variable number of views (1 or more). We further aggregate multiview information by using permutation equivariant layers that promote order-agnostic view information exchange at the feature level. Experiments show that our approach is able to produce dense 3D reconstructions of objects that improve in quality as more views are added."
http://papers.nips.cc/paper/by-source-2019-9052,https://proceedings.neurips.cc/paper/2019/file/1fa6269f58898f0e809575c9a48747ef-Paper.pdf,TAB-VCR: Tags and Attributes based Visual Commonsense Reasoning Baselines,Jingxiang Lin &middot; Unnat Jain &middot; Alex Schwing,https://youtu.be/FS66skIlIYQ,"Reasoning is an important ability that we learn from a very early age. Yet, reasoning is extremely hard for algorithms. Despite impressive recent progress that has been reported on tasks that necessitate reasoning, such as visual question answering and visual dialog, models often exploit biases in datasets. To develop models with better reasoning abilities, recently, the new visual commonsense reasoning (VCR) task has been introduced. Not only do models have to answer questions, but also do they have to provide a reason for the given answer. The proposed baseline achieved compelling results, leveraging a meticulously designed model composed of LSTM modules and attention nets. Here we show that a much simpler model obtained by ablating and pruning the existing intricate baseline can perform better with half the number of trainable parameters. By associating visual features with attribute information and better text to image grounding, we obtain further improvements for our simpler &amp; effective baseline, TAB-VCR. We show that this approach results in a 5.3%, 4.4% and 6.5% absolute improvement over the previous state-of-the-art on question answering, answer justification and holistic VCR. Webpage: https://deanplayerljx.github.io/tabvcr/"
http://papers.nips.cc/paper/by-source-2019-8215,https://proceedings.neurips.cc/paper/2019/file/1e0feeaff84a19bf3936e693311fa66d-Paper.pdf,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces,Benyamin Allahgholizadeh Haghi &middot; Spencer Kellis &middot; Sahil Shah &middot; Maitreyi Ashok &middot; Luke Bashford &middot; Daniel Kramer &middot; Brian Lee &middot; Charles Liu &middot; Richard Andersen &middot; Azita Emami,https://www.youtube.com/watch?v=VbgM-hO_fQ0,"We present a new deep multi-state Dynamic Recurrent Neural Network (DRNN) architecture for Brain Machine Interface (BMI) applications. Our DRNN is used to predict Cartesian representation of a computer cursor movement kinematics from open-loop neural data recorded from the posterior parietal cortex (PPC) of a human subject in a BMI system. We design the algorithm to achieve a reasonable trade-off between performance and robustness, and we constrain memory usage in favor of future hardware implementation. We feed the predictions of the network back to the input to improve prediction performance and robustness. We apply a scheduled sampling approach to the model in order to solve a statistical distribution mismatch between the ground truth and predictions. Additionally, we configure a small DRNN to operate with a short history of input, reducing the required buffering of input data and number of memory accesses. This configuration lowers the expected power consumption in a neural network accelerator. Operating on wavelet-based neural features, we show that the average performance of DRNN surpasses other state-of-the-art methods in the literature on both single- and multi-day data recorded over 43 days. Results show that multi-state DRNN has the potential to model the nonlinear relationships between the neural data and kinematics for robust BMIs."
http://papers.nips.cc/paper/by-source-2019-4758,https://proceedings.neurips.cc/paper/2019/file/0bfce127947574733b19da0f30739fcd-Paper.pdf,Learning Macroscopic Brain Connectomes via Group-Sparse Factorization,Farzane Aminmansour &middot; Andrew Patterson &middot; Lei Le &middot; Yisu  Peng &middot; Daniel  Mitchell &middot; Franco Pestilli &middot; Cesar F Caiafa &middot; Russell Greiner &middot; Martha White,https://github.com/framinmansour/Learning-Macroscopic-Brain-Connectomes-via-Group-Sparse-Factorization/raw/master/video/3minute_presentation.mp4,"Mapping structural brain connectomes for living human brains typically requires expert analysis and rule-based models on diffusion-weighted magnetic resonance imaging. A data-driven approach, however, could overcome limitations in such rule-based approaches and improve precision mappings for individuals. In this work, we explore a framework that facilitates applying learning algorithms to automatically extract brain connectomes. Using a tensor encoding, we design an objective with a group-regularizer that prefers biologically plausible fascicle structure. We show that the objective is convex and has unique solutions, ensuring identifiable connectomes for an individual. We develop an efficient optimization strategy for this extremely high-dimensional sparse problem, by reducing the number of parameters using a greedy algorithm designed specifically for the problem. We show that this greedy algorithm significantly improves on a standard greedy algorithm, called Orthogonal Matching Pursuit. We conclude with an analysis of the solutions found by our method, showing we can accurately reconstruct the diffusion information while maintaining contiguous fascicles with smooth direction changes."
http://papers.nips.cc/paper/by-source-2019-1331,https://proceedings.neurips.cc/paper/2019/file/e077e1a544eec4f0307cf5c3c721d944-Paper.pdf,Disentangled behavioural representations,Amir Dezfouli &middot; Hassan Ashtiani &middot; Omar Ghattas &middot; Richard Nock &middot; Peter Dayan &middot; Cheng Soon Ong,https://vimeo.com/368702071,"Individual characteristics in human decision-making are often
"
http://papers.nips.cc/paper/by-source-2019-3508,https://proceedings.neurips.cc/paper/2019/file/7d2be41b1bde6ff8fe45150c37488ebb-Paper.pdf,From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI,Roman Beliy &middot; Guy Gaziv &middot; Assaf Hoogi &middot; Francesca Strappini &middot; Tal Golan &middot; Michal Irani,https://www.youtube.com/watch?v=h2JhDAdaa-Q&feature=youtu.be,"Reconstructing observed images from fMRI brain recordings is challenging. Unfortunately, acquiring sufficient &#x27;&#x27;labeled&#x27;&#x27; pairs of {Image, fMRI} (i.e., images with their corresponding fMRI responses) to span the huge space of natural images is prohibitive for many reasons. We present a novel approach which, in addition to the scarce labeled data (training pairs), allows to train fMRI-to-image reconstruction networks also on &quot;unlabeled&quot; data (i.e., images without fMRI recording, and fMRI recording without images). The proposed model utilizes both an Encoder network (image-to-fMRI) and a Decoder network (fMRI-to-image). Concatenating these two networks back-to-back (Encoder-Decoder &amp; Decoder-Encoder) allows augmenting the training data with both types of unlabeled data. Importantly, it allows training on the unlabeled test-fMRI data. This self-supervision adapts the reconstruction network to the new input test-data, despite its deviation from the statistics of the scarce training data."
http://papers.nips.cc/paper/by-source-2019-6220,https://proceedings.neurips.cc/paper/2019/file/90cc440b1b8caa520c562ac4e4bbcb51-Paper.pdf,Structured Graph Learning Via Laplacian Spectral Constraints,Sandeep Kumar &middot; Jiaxi Ying &middot; Jose Vinicius de Miranda Cardoso &middot; Daniel Palomar,https://youtu.be/klAqFvyQx7k,"Learning a graph with a specific structure is essential for interpretability and identification of the relationships among data. But structured graph learning from observed samples is an NP-hard combinatorial problem. In this paper, we first show, for a set of important graph families it is possible to convert the combinatorial constraints of structure into eigenvalue constraints of the graph Laplacian matrix. Then we introduce a unified graph learning framework lying at the integration of the spectral properties of the Laplacian matrix with Gaussian graphical modeling, which is capable of learning structures of a large class of graph families. The proposed algorithms are provably convergent and practically amenable for big-data specific tasks. Extensive numerical experiments with both synthetic and real datasets demonstrate the effectiveness of the proposed methods. An R package containing codes for all the experimental results is submitted as a supplementary file."
http://papers.nips.cc/paper/by-source-2019-5017,https://proceedings.neurips.cc/paper/2019/file/0af787945872196b42c9f73ead2565c8-Paper.pdf,Language as an Abstraction for Hierarchical Deep Reinforcement Learning,YiDing Jiang &middot; Shixiang (Shane) Gu &middot; Kevin Murphy &middot; Chelsea Finn,https://drive.google.com/open?id=1tvJ6CsEdskbRIA2BFLxSgDqjbm1BaaRt,"Solving complex, temporally-extended tasks is a long-standing problem in reinforcement learning (RL). We hypothesize that one critical element of solving such problems is the notion of compositionality. With the ability to learn sub-skills that can be composed to solve longer tasks, i.e. hierarchical RL, we can acquire temporally-extended behaviors. However, acquiring effective yet general abstractions for hierarchical RL is remarkably challenging. In this paper, we propose to use language as the abstraction, as it provides unique compositional structure, enabling fast learning and combinatorial generalization, while retaining tremendous flexibility, making it suitable for a variety of problems. Our approach learns an instruction-following low-level policy and a high-level policy that can reuse abstractions across tasks, in essence, permitting agents to reason using structured language. To study compositional task learning, we introduce an open-source object interaction environment built using the MuJoCo physics engine and the CLEVR engine. We find that, using our approach, agents can learn to solve to diverse, temporally-extended tasks such as object sorting and multi-object rearrangement, including from raw pixel observations. Our analysis find that the compositional nature of language is critical for learning and systematically generalizing sub-skills in comparison to non-compositional abstractions that use the same supervision."
http://papers.nips.cc/paper/by-source-2019-1504,https://proceedings.neurips.cc/paper/2019/file/49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf,Learning Robust Options by Conditional Value at Risk Optimization,Takuya Hiraoka &middot; Takahisa Imagawa &middot; Tatsuya Mori &middot; Takashi Onishi &middot; Yoshimasa Tsuruoka,https://drive.google.com/file/d/1DRmIaK5VomCey70rKD_5DgX2Jm_1rFlo/view,"Options are generally learned by using an inaccurate environment model (or simulator), which contains uncertain model parameters.
"
http://papers.nips.cc/paper/by-source-2019-4994,https://proceedings.neurips.cc/paper/2019/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf,Multiclass Performance Metric Elicitation,Gaurush Hiranandani &middot; Shant Boodaghians &middot; Ruta Mehta &middot; Sanmi Koyejo,https://drive.google.com/drive/folders/1AWZ6xvSUwNGQYeYzI2yf-MYPvgnba8YJ?usp=sharing,"Metric Elicitation is a principled framework for selecting the performance metric that best reflects implicit user preferences. However, available strategies have so far been limited to binary classification. In this paper, we propose novel strategies for eliciting multiclass classification performance metrics using only relative preference feedback. We also show that the strategies are robust to both finite sample and feedback noise. "
http://papers.nips.cc/paper/by-source-2019-5517,https://proceedings.neurips.cc/paper/2019/file/6c7cd904122e623ce625613d6af337c4-Paper.pdf,"Poincaré Recurrence, Cycles and Spurious Equilibria in Gradient-Descent-Ascent for Non-Convex Non-Concave Zero-Sum Games",Emmanouil-Vasileios Vlatakis-Gkaragkounis &middot; Lampros Flokas &middot; Georgios Piliouras,http://www.cs.columbia.edu/~emvlatakis/Videos/Poincare_Recurrence_Cycles_and_Spurious_Equilibria_in_Gradient-Descent-Ascent_for_Non-Convex_Non-Concave_Zero-Sum_Games.html,"We study a wide class of non-convex non-concave min-max games that generalizes over standard bilinear zero-sum games. In this class, players control the inputs of a smooth function whose output is being applied to a bilinear zero-sum game. This class of games is motivated by the  indirect nature of the competition in
"
http://papers.nips.cc/paper/by-source-2019-5823,https://proceedings.neurips.cc/paper/2019/file/658bbbdef9415ba5e2ff857f1146ba6e-Paper.pdf,McDiarmid-Type Inequalities for Graph-Dependent Variables and Stability Bounds,Rui (Ray) Zhang &middot; Xingwu Liu &middot; Yuyi Wang &middot; Liwei Wang,https://rui-ray-zhang.github.io/NIPS/NIPS.mp4,"A crucial assumption in most statistical learning theory is that samples are independently and identically distributed (i.i.d.). However, for many real applications, the i.i.d. assumption does not hold. We consider learning problems in which examples are dependent and their dependency relation is characterized by a graph. To establish algorithm-dependent generalization theory for learning with non-i.i.d. data, we first prove novel McDiarmid-type concentration inequalities for Lipschitz functions of graph-dependent random variables. We show that concentration relies on the forest complexity of the graph, which characterizes the strength of the dependency. We demonstrate that for many types of dependent data, the forest complexity is small and thus implies good concentration. Based on our new inequalities we are able to build stability bounds for learning from graph-dependent data."
http://papers.nips.cc/paper/by-source-2019-3960,https://proceedings.neurips.cc/paper/2019/file/ac52c626afc10d4075708ac4c778ddfc-Paper.pdf,Optimal Sparse Decision Trees,Xiyang Hu &middot; Cynthia Rudin &middot; Margo Seltzer,https://www.youtube.com/watch?v=UMjMQaH508M&feature=youtu.be,"Decision tree algorithms have been among the most popular algorithms for interpretable (transparent) machine learning since the early 1980&#x27;s. The problem that has plagued decision tree algorithms since their inception is their lack of optimality, or lack of guarantees of closeness to optimality: decision tree algorithms are often greedy or myopic, and sometimes produce unquestionably suboptimal models. Hardness of decision tree optimization is both a theoretical and practical obstacle, and even careful mathematical programming approaches have not been able to solve these problems efficiently. This work introduces the first practical algorithm for optimal decision trees for binary variables. The algorithm is a co-design of analytical bounds that reduce the search space and modern systems techniques, including data structures and a custom bit-vector library. We highlight possible steps to improving the scalability and speed of future generations of this algorithm based on insights from our theory and experiments."
http://papers.nips.cc/paper/by-source-2019-3960,https://proceedings.neurips.cc/paper/2019/file/ac52c626afc10d4075708ac4c778ddfc-Paper.pdf,Optimal Sparse Decision Trees,Xiyang Hu &middot; Cynthia Rudin &middot; Margo Seltzer,https://www.youtube.com/watch?v=UMjMQaH508M&feature=youtu.be,"Decision tree algorithms have been among the most popular algorithms for interpretable (transparent) machine learning since the early 1980&#x27;s. The problem that has plagued decision tree algorithms since their inception is their lack of optimality, or lack of guarantees of closeness to optimality: decision tree algorithms are often greedy or myopic, and sometimes produce unquestionably suboptimal models. Hardness of decision tree optimization is both a theoretical and practical obstacle, and even careful mathematical programming approaches have not been able to solve these problems efficiently. This work introduces the first practical algorithm for optimal decision trees for binary variables. The algorithm is a co-design of analytical bounds that reduce the search space and modern systems techniques, including data structures and a custom bit-vector library. Our experiments highlight advantages in scalability, speed, and proof of optimality."
http://papers.nips.cc/paper/by-source-2019-5350,https://proceedings.neurips.cc/paper/2019/file/fd0a5a5e367a0955d81278062ef37429-Paper.pdf,Unsupervised Meta-Learning for Few-Shot Image Classification,Siavash Khodadadeh &middot; Ladislau Boloni &middot; Mubarak Shah,https://github.com/siavash-khodadadeh/MetaLearning-TF2.0/raw/master/UMTRA_Paper/VideoPresentation.mp4,"Few-shot or one-shot learning of classifiers requires a significant inductive bias towards the type of task to be learned. One way to acquire this is by meta-learning on tasks similar to the target task. In this paper, we propose UMTRA, an algorithm that performs unsupervised, model-agnostic meta-learning for classification tasks.
"
http://papers.nips.cc/paper/by-source-2019-2016,https://proceedings.neurips.cc/paper/2019/file/95192c98732387165bf8e396c0f2dad2-Paper.pdf,MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies,Xue Bin Peng &middot; Michael Chang &middot; Grace Zhang &middot; Pieter Abbeel &middot; Sergey Levine,https://youtu.be/HLD6H2eJK1E,"Humans are able to perform a myriad of sophisticated tasks by drawing upon skills acquired through prior experience. For autonomous agents to have this capability, they must be able to extract reusable skills from past experience that can be recombined in new ways for subsequent tasks. Furthermore, when controlling complex high-dimensional morphologies, such as humanoid bodies, tasks often require coordination of multiple skills simultaneously. Learning discrete primitives for every combination of skills quickly becomes prohibitive. Composable primitives that can be recombined to create a large variety of behaviors can be more suitable for modeling this combinatorial explosion. In this work, we propose multiplicative compositional policies (MCP), a method for learning reusable motor skills that can be composed to produce a range of complex behaviors. Our method factorizes an agent&#x27;s skills into a collection of primitives, where multiple primitives can be activated simultaneously via multiplicative composition. This flexibility allows the primitives to be transferred and recombined to elicit new behaviors as necessary for novel tasks. We demonstrate that MCP is able to extract composable skills for highly complex simulated characters from pre-training tasks, such as motion imitation, and then reuse these skills to solve challenging continuous control tasks, such as dribbling a soccer ball to a goal, and picking up an object and transporting it to a target location."
http://papers.nips.cc/paper/by-source-2019-2447,https://proceedings.neurips.cc/paper/2019/file/2c3ddf4bf13852db711dd1901fb517fa-Paper.pdf,Uncertainty-based Continual Learning with Adaptive Regularization,Hongjoon Ahn &middot; Sungmin Cha &middot; Donggyu Lee &middot; Taesup Moon,https://sites.google.com/view/project-page-of-ucl,"We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference. We focus on two significant drawbacks of the recently proposed regularization-based methods: a) considerable additional memory cost for determining the per-weight regularization strengths and b) the absence of gracefully forgetting scheme, which can prevent performance degradation in learning new tasks. In this paper, we show UCL can solve these two problems by introducing a fresh interpretation on the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian mean-field approximation. Based on the interpretation, we propose the notion of node-wise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization. Moreover, we devise two additional regularization terms that enforce \emph{stability} by freezing important parameters for past tasks and allow \emph{plasticity} by controlling the actively learning parameters for a new task. Through extensive experiments, we show UCL convincingly outperforms most of recent state-of-the-art baselines not only on popular supervised learning benchmarks, but also on challenging lifelong reinforcement learning tasks. The source code of our algorithm is available at  https://github.com/csm9493/UCL."
http://papers.nips.cc/paper/by-source-2019-4192,https://proceedings.neurips.cc/paper/2019/file/cd474f6341aeffd65f93084d0dae3453-Paper.pdf,A Unifying Framework for Spectrum-Preserving Graph Sparsification and Coarsening,Gecia Bravo-Hermsdorff &middot; Lee Gunderson,https://www.youtube.com/watch?v=xTAPZbQlq3A,"How might one &quot;reduce&quot; a graph?
"
http://papers.nips.cc/paper/by-source-2019-8774,https://proceedings.neurips.cc/paper/2019/file/e9bf14a419d77534105016f5ec122d62-Paper.pdf,DeepWave: A Recurrent Neural-Network for Real-Time Acoustic Imaging,Matthieu SIMEONI &middot; Sepand Kashani &middot; Paul Hurley &middot; Martin Vetterli,https://www.youtube.com/watch?v=PwB3CS2rHdI,"We propose a recurrent neural-network for real-time reconstruction of acoustic camera spherical maps. The network, dubbed DeepWave, is both physically and algorithmically motivated: its recurrent architecture mimics iterative solvers from convex optimisation, and its parsimonious parametrisation is based on the natural structure of acoustic imaging problems.
"
http://papers.nips.cc/paper/by-source-2019-2765,https://proceedings.neurips.cc/paper/2019/file/370bfb31abd222b582245b977ea5f25a-Paper.pdf,Few-shot Video-to-Video Synthesis,Ting-Chun Wang &middot; Ming-Yu Liu &middot; Andrew Tao &middot; Guilin Liu &middot; Bryan Catanzaro &middot; Jan Kautz,https://www.youtube.com/watch?v=8AZBuyEuDqc&feature=youtu.be,"Video-to-video synthesis (vid2vid) aims at converting an input semantic video, such as videos of human poses or segmentation masks, to an output photorealistic video. While the state-of-the-art of vid2vid has advanced significantly, existing approaches share two major limitations. First, they are data-hungry. Numerous images of a target human subject or a scene are required for training. Second, a learned model has limited generalization capability. A pose-to-human vid2vid model can only synthesize poses of the single person in the training set. It does not generalize to other humans that are not in the training set. To address the limitations, we propose a few-shot vid2vid framework, which learns to synthesize videos of previously unseen subjects or scenes by leveraging few example images of the target at test time. Our model achieves this few-shot generalization capability via a novel network weight generation module utilizing an attention mechanism. We conduct extensive experimental validations with comparisons to strong baselines using several large-scale video datasets including human-dancing videos, talking-head videos, and street-scene videos. The experimental results verify the effectiveness of the proposed framework in addressing the two limitations of existing vid2vid approaches."
http://papers.nips.cc/paper/by-source-2019-753,https://proceedings.neurips.cc/paper/2019/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf,Image Synthesis with a Single (Robust) Classifier,Shibani Santurkar &middot; Andrew Ilyas &middot; Dimitris Tsipras &middot; Logan Engstrom &middot; Brandon Tran &middot; Aleksander Madry,https://youtu.be/l3SoeIIjxTM,"We show that the basic classification framework alone can be used to tackle some of the most challenging tasks in image synthesis. In contrast to other state-of-the-art approaches, the toolkit we develop is rather minimal: it uses a single, off-the-shelf classifier for all these tasks. The crux of our approach is that we train this classifier to be adversarially robust. It turns out that adversarial robustness is precisely what we need to directly manipulate salient features of the input. Overall, our findings demonstrate the utility of robustness in the broader machine learning context."
http://papers.nips.cc/paper/by-source-2019-2164,https://proceedings.neurips.cc/paper/2019/file/c3e4035af2a1cde9f21e1ae1951ac80b-Paper.pdf,Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose Reconstruction,Aleksis Pirinen &middot; Erik Gärtner &middot; Cristian Sminchisescu,https://lu.app.box.com/s/xd1nidkirdlg0htlkyomq33ipk01k6gk,"Existing state-of-the-art estimation systems can detect 2d poses of multiple people in images quite reliably. In contrast, 3d pose estimation from a single image is ill-posed due to occlusion and depth ambiguities. Assuming access to multiple cameras, or given an active system able to position itself to observe the scene from multiple viewpoints, reconstructing 3d pose from 2d measurements becomes well-posed within the framework of standard multi-view geometry. Less clear is what is an informative set of viewpoints for accurate 3d reconstruction, particularly in complex scenes, where people are occluded by others or by scene objects. In order to address the view selection problem in a principled way, we here introduce ACTOR, an active triangulation agent for 3d human pose reconstruction. Our fully trainable agent consists of a 2d pose estimation network (any of which would work) and a deep reinforcement learning-based policy for camera viewpoint selection. The policy predicts observation viewpoints, the number of which varies adaptively depending on scene content, and the associated images are fed to an underlying pose estimator. Importantly, training the view selection policy requires no annotations -- given a pre-trained 2d pose estimator, ACTOR is trained in a self-supervised manner. In extensive evaluations on complex multi-people scenes filmed in a Panoptic dome, under multiple viewpoints, we compare our active triangulation agent to strong multi-view baselines, and show that ACTOR produces significantly more accurate 3d pose reconstructions. We also provide a proof-of-concept experiment indicating the potential of connecting our view selection policy to a physical drone observer."
http://papers.nips.cc/paper/by-source-2019-365,https://proceedings.neurips.cc/paper/2019/file/9be40cee5b0eee1462c82c6964087ff9-Paper.pdf,Multiway clustering via tensor block models  ,Miaoyan Wang &middot; Yuchen Zeng,https://youtu.be/b3sOTLwxHMY,"We consider the problem of identifying multiway block structure from a large noisy tensor. Such problems arise frequently in applications such as genomics, recommendation system, topic modeling, and sensor network localization. We propose a tensor block model, develop a unified least-square estimation, and obtain the theoretical accuracy guarantees for multiway clustering. The statistical convergence of the estimator is established, and we show that the associated clustering procedure achieves partition consistency. A sparse regularization is further developed for identifying important blocks with elevated means. The proposal handles a broad range of data types, including binary, continuous, and hybrid observations. Through simulation and application to two real datasets, we demonstrate the outperformance of our approach over previous methods."
http://papers.nips.cc/paper/by-source-2019-2764,https://proceedings.neurips.cc/paper/2019/file/98d8a23fd60826a2a474c5b4f5811707-Paper.pdf,Visual Concept-Metaconcept Learning,Chi Han &middot; Jiayuan Mao &middot; Chuang Gan &middot; Josh Tenenbaum &middot; Jiajun Wu,https://drive.google.com/open?id=1yP5gYyd8-Tx8SKvd1bWbO2H7prZO1Ndn,"Humans reason with concepts and metaconcepts: we recognize red and blue from visual input; we also understand that they are colors, i.e., red is an instance of color. In this paper, we propose the visual concept-metaconcept learner (VCML) for joint learning of concepts and metaconcepts from images and associated question-answer pairs. The key is to exploit the bidirectional connection between visual concepts and metaconcepts. Visual representations provide grounding cues for predicting relations between unseen pairs of concepts. Knowing that red and blue are instances of color, we generalize to the fact that green is also an instance of color since they all categorize the hue of objects. Meanwhile, knowledge about metaconcepts empowers visual concept learning from limited, noisy, and even biased data. From just a few examples of purple cubes we can understand a new color purple, which resembles the hue of the cubes instead of the shape of them. Evaluation on both synthetic and real-world datasets validates our claims."
http://papers.nips.cc/paper/by-source-2019-6955,https://proceedings.neurips.cc/paper/2019/file/4fdaa19b1f22a4d926fce9bfc7c61fa5-Paper.pdf,Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm Evaluation,Ruibo Tu &middot; Kun Zhang &middot; Bo Bertilson &middot; Hedvig Kjellstrom &middot; Cheng Zhang,https://youtu.be/1UvVnIbjSX8,"Discovery of causal relations from observational data is essential for many disciplines of science and real-world applications. However, unlike other machine learning algorithms, whose development has been greatly fostered by a large amount of available benchmark datasets, causal discovery algorithms are notoriously difficult to be systematically evaluated because few datasets with known ground-truth causal relations are available. In this work, we handle the problem of evaluating causal discovery algorithms by building a flexible simulator in the medical setting. We develop a neuropathic pain diagnosis simulator, inspired by the fact that the biological processes of neuropathic pathophysiology are well studied with well-understood causal influences. Our simulator exploits the causal graph of the neuropathic pain pathology and its parameters in the generator are estimated from real-life patient cases. We show that the data generated from our simulator have similar statistics as real-world data. As a clear advantage, the simulator can produce infinite samples without jeopardizing the privacy of real-world patients. Our simulator provides a natural tool for evaluating various types of causal discovery algorithms, including those to deal with practical issues in causal discovery, such as unknown confounders, selection bias, and missing data. Using our simulator, we have evaluated extensively causal discovery algorithms under various settings. "
http://papers.nips.cc/paper/by-source-2019-8401,https://proceedings.neurips.cc/paper/2019/file/c055dcc749c2632fd4dd806301f05ba6-Paper.pdf,Icebreaker: Element-wise Efficient Information Acquisition with a Bayesian Deep Latent Gaussian Model,Wenbo Gong &middot; Sebastian Tschiatschek &middot; Sebastian Nowozin &middot; Richard Turner &middot; José Miguel Hernández-Lobato &middot; Cheng Zhang,https://www.youtube.com/watch?feature=%3Dyoutu.be&v=UST9Uhg8CkA,"In this paper, we address the ice-start problem, i.e., the challenge of deploying machine learning models when only a little or no training data is initially available, and acquiring each feature element of data is associated with costs. This setting is representative of the real-world machine learning applications. For instance, in the health care domain, obtaining every single measurement comes with a cost. We propose Icebreaker, a principled framework for elementwise training data acquisition. Icebreaker introduces a full Bayesian Deep Latent Gaussian Model (BELGAM) with a novel inference method, which combines recent advances in amortized inference and stochastic gradient MCMC to enable fast and accurate posterior inference. By utilizing BELGAM’s ability to fully quantify model uncertainty, we also propose two information acquisition functions for imputation and active prediction problems. We demonstrate that BELGAM performs significantly better than previous variational autoencoder (VAE) based models, when the data set size is small, using both machine learning benchmarks and real world recommender systems and health-care applications. Moreover, Icebreaker not only demonstrates improved performance compared to baselines, but it is also capable of achieving better test performance with less training data available."
http://papers.nips.cc/paper/by-source-2019-7407,https://proceedings.neurips.cc/paper/2019/file/99a401435dcb65c4008d3ad22c8cdad0-Paper.pdf,ODE2VAE: Deep generative second order ODEs with Bayesian neural networks,Cagatay Yildiz &middot; Markus Heinonen &middot; Harri Lahdesmaki,https://github.com/cagatayyildiz/ODE2VAE,"We present Ordinary Differential Equation Variational Auto-Encoder (ODE2VAE), a latent second order ODE model for high-dimensional sequential data. Leveraging the advances in deep generative models, ODE2VAE can simultaneously learn the embedding of high dimensional trajectories and infer arbitrarily complex continuous-time latent dynamics. Our model explicitly decomposes the latent space into momentum and position components and solves a second order ODE system, which is in contrast to recurrent neural network (RNN) based time series models and recently proposed black-box ODE techniques. In order to account for uncertainty, we propose probabilistic latent ODE dynamics parameterized by deep Bayesian neural networks. We demonstrate our approach on motion capture, image rotation, and bouncing balls datasets. We achieve state-of-the-art performance in long term motion prediction and imputation tasks."
http://papers.nips.cc/paper/by-source-2019-5181,https://proceedings.neurips.cc/paper/2019/file/3b7d09dac07c9ccd3aae2025cc195250-Paper.pdf,Convergence-Rate-Matching Discretization of Accelerated Optimization Flows Through Opportunistic State-Triggered Control,Miguel Vaquero &middot; Jorge Cortes,https://www.dropbox.com/sh/aildrjci7n55359/AAB1A2jEg66lgs2k8s2GWuWAa?dl=0,"A recent body of exciting work seeks to shed light on the behavior of accelerated methods in optimization via high-resolution differential equations. These differential equations are continuous counterparts of the discrete-time optimization algorithms, and their convergence properties can be characterized using the powerful tools provided by classical Lyapunov stability analysis. An outstanding question of pivotal importance is how to discretize these continuous flows while maintaining their convergence rates. This paper provides a novel approach through the idea of opportunistic state-triggered control. We take advantage of the Lyapunov functions employed to characterize the rate of convergence of high-resolution differential equations to design variable-stepsize forward-Euler discretizations that preserve the Lyapunov decay of the original dynamics. The philosophy of our approach is not limited to forward-Euler discretizations and may be combined with other integration schemes."
http://papers.nips.cc/paper/by-source-2019-5343,https://proceedings.neurips.cc/paper/2019/file/8dd291cbea8f231982db0fb1716dfc55-Paper.pdf,Decentralized sketching of low rank matrices,Rakshith Sharma Srinivasa &middot; Kiryung Lee &middot; Marius  Junge &middot; Justin Romberg,https://www.dropbox.com/sh/kpl064804i9ks1y/AADxPFiVivRDgPlku0lXX68na?dl=0,"We address a low-rank matrix recovery problem where each column of a rank-r matrix X of size (d1,d2) is compressed beyond the point of recovery to size L with L &lt;&lt; d1. Leveraging the joint structure between the columns, we propose a method to recover the matrix to within an epsilon relative error in the Frobenius norm from a total of O(r(d_1 + d_2)\log^6(d_1 + d_2)/\epsilon^2) observations. This guarantee holds uniformly for all incoherent matrices of rank r. In our method, we propose to use a novel matrix norm called the mixed-norm along with the maximum l2 norm of the columns to design a novel convex relaxation for low-rank recovery that is tailored to our observation model. We also show that our proposed mixed-norm, the standard nuclear norm, and the max-norm are particular instances of convex regularization of low-rankness via tensor norms. Finally, we provide a scalable ADMM algorithm for the mixed-norm based method and demonstrate its empirical performance via large-scale simulations. "
http://papers.nips.cc/paper/by-source-2019-6671,https://proceedings.neurips.cc/paper/2019/file/a1e865a9b1065392ed6035d8ccd072d9-Paper.pdf,Fast and Accurate Stochastic Gradient Estimation,Beidi Chen &middot; Yingchen Xu &middot; Anshumali Shrivastava,https://drive.google.com/open?id=1GJAErrv_7iAvzqhJEcfZd2vGVnQdaA_A,"Stochastic Gradient Descent or SGD is the most popular optimization algorithm for large-scale problems. SGD estimates the gradient by uniform sampling with sample size one. There have been several other works that suggest faster epoch-wise convergence by using weighted non-uniform sampling for better gradient estimates. Unfortunately, the per-iteration cost of maintaining this adaptive distribution for gradient estimation is more than calculating the full gradient itself, which we call the chicken-and-the-egg loop. As a result, the false impression of faster convergence in iterations, in reality, leads to slower convergence in time. In this paper, we break this barrier by providing the first demonstration of a scheme, Locality sensitive hashing (LSH) sampled Stochastic Gradient Descent (LGD), which leads to superior gradient estimation while keeping the sampling cost per iteration similar to that of the uniform sampling. Such an algorithm is possible due to the sampling view of LSH, which came to light recently. As a consequence of superior and fast estimation, we reduce the running time of all existing gradient descent algorithms, that relies on gradient estimates including Adam, Ada-grad, etc. We demonstrate the effectiveness of our proposal with experiments on linear models as well as the non-linear BERT, which is a recent popular deep learning based language representation model."
http://papers.nips.cc/paper/by-source-2019-7941,https://proceedings.neurips.cc/paper/2019/file/46c7cb50b373877fb2f8d5c4517bb969-Paper.pdf,"Fast, Provably convergent IRLS Algorithm for p-norm Linear Regression",Deeksha Adil &middot; Richard Peng &middot; Sushant Sachdeva,https://www.youtube.com/watch?v=lG3ycfrK88g&feature=youtu.be,"Linear regression in L_p-norm is a canonical optimization problem that arises in several applications, including sparse recovery, semi-supervised learning, and signal processing. Generic convex optimization algorithms for solving L_p-regression are slow in practice. Iteratively Reweighted Least Squares (IRLS) is an easy to implement family of algorithms for solving these problems that has been studied for over 50 years. However, these algorithms often diverge for p &gt; 3, and since the work of Osborne (1985), it has been an open problem whether there is an IRLS algorithm that converges for p &gt; 3. We propose p-IRLS, the first IRLS algorithm that provably converges geometrically for any p \in [2,\infty). Our algorithm is simple to implement and is guaranteed to find a high accuracy solution in a sub-linear number of iterations. Our experiments demonstrate that it performs even better than our theoretical bounds, beats the standard Matlab/CVX implementation for solving these problems by 10–50x, and is the fastest among available implementations in the high-accuracy regime."
http://papers.nips.cc/paper/by-source-2019-5233,https://proceedings.neurips.cc/paper/2019/file/26b58a41da329e0cbde0cbf956640a58-Paper.pdf,Stochastic Variance Reduced Primal Dual Algorithms for Empirical Composition Optimization,Adithya M Devraj &middot; Jianshu Chen,https://github.com/adidevraj/SVRPDA/wiki/Video,"We consider a generic empirical composition optimization problem, where there are empirical averages present both outside and inside nonlinear loss functions. Such a problem is of interest in various machine learning applications, and cannot be directly solved by standard methods such as stochastic gradient descent (SGD). We take a novel approach to solving this problem by reformulating the original minimization objective into an equivalent min-max objective, which brings out all the empirical averages that are originally inside the nonlinear loss functions. We exploit the rich structures of the reformulated problem and develop a stochastic primal-dual algorithms, SVRPDA-I, to solve the problem efficiently. We carry out extensive theoretical analysis of the proposed algorithm, obtaining the convergence rate, the total computation complexity and the storage complexity. In particular, the algorithm is shown to converge at a linear rate when the problem is strongly convex. Moreover, we also develop an approximate version of the algorithm, named SVRPDA-II, which further reduces the memory requirement. Finally, we evaluate the performance of our algorithms on several real-world benchmarks and experimental results show that they significantly outperform existing techniques."
http://papers.nips.cc/paper/by-source-2019-8540,https://proceedings.neurips.cc/paper/2019/file/39d929972619274cc9066307f707d002-Paper.pdf,Function-Space Distributions over Kernels,Gregory Benton &middot; Wesley Maddox &middot; Jayson Salkey &middot; Julio Albinati &middot; Andrew Gordon Wilson, class=,"	Gaussian processes are flexible function approximators, with inductive biases controlled by a covariance kernel. Learning the kernel is the key to representation learning and strong predictive performance. In this paper, we develop functional kernel learning (FKL) to directly infer functional posteriors over kernels. In particular, we place a transformed Gaussian process over a spectral density, to induce a non-parametric distribution over kernel functions. The resulting approach enables learning of rich representations, with support for any stationary kernel, uncertainty over the values of the kernel, and an interpretable specification of a prior directly over kernels, without requiring sophisticated initialization or manual intervention. We perform inference through elliptical slice sampling, which is especially well suited to marginalizing posteriors with the strongly correlated priors typical to function space modeling. We develop our approach for non-uniform, large-scale, multi-task, and multidimensional data, and show promising performance in a wide range of settings, including interpolation, extrapolation, and kernel recovery experiments."
http://papers.nips.cc/paper/by-source-2019-5517,https://proceedings.neurips.cc/paper/2019/file/6c7cd904122e623ce625613d6af337c4-Paper.pdf,"Poincaré Recurrence, Cycles and Spurious Equilibria in Gradient-Descent-Ascent for Non-Convex Non-Concave Zero-Sum Games",Emmanouil-Vasileios Vlatakis-Gkaragkounis &middot; Lampros Flokas &middot; Georgios Piliouras,http://www.cs.columbia.edu/~emvlatakis/Videos/Poincare_Recurrence_Cycles_and_Spurious_Equilibria_in_Gradient-Descent-Ascent_for_Non-Convex_Non-Concave_Zero-Sum_Games.html,"We study a wide class of non-convex non-concave min-max games that generalizes over standard bilinear zero-sum games. In this class, players control the inputs of a smooth function whose output is being applied to a bilinear zero-sum game. This class of games is motivated by the  indirect nature of the competition in
"
http://papers.nips.cc/paper/by-source-2019-5823,https://proceedings.neurips.cc/paper/2019/file/658bbbdef9415ba5e2ff857f1146ba6e-Paper.pdf,McDiarmid-Type Inequalities for Graph-Dependent Variables and Stability Bounds,Rui (Ray) Zhang &middot; Xingwu Liu &middot; Yuyi Wang &middot; Liwei Wang,https://rui-ray-zhang.github.io/NIPS/NIPS.mp4,"A crucial assumption in most statistical learning theory is that samples are independently and identically distributed (i.i.d.). However, for many real applications, the i.i.d. assumption does not hold. We consider learning problems in which examples are dependent and their dependency relation is characterized by a graph. To establish algorithm-dependent generalization theory for learning with non-i.i.d. data, we first prove novel McDiarmid-type concentration inequalities for Lipschitz functions of graph-dependent random variables. We show that concentration relies on the forest complexity of the graph, which characterizes the strength of the dependency. We demonstrate that for many types of dependent data, the forest complexity is small and thus implies good concentration. Based on our new inequalities we are able to build stability bounds for learning from graph-dependent data."
http://papers.nips.cc/paper/by-source-2019-6969,https://proceedings.neurips.cc/paper/2019/file/7813d1590d28a7dd372ad54b5d29d033-Paper.pdf,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs,Jonas Kubilius &middot; Martin Schrimpf &middot; Ha Hong &middot; Najib Majaj &middot; Rishi Rajalingham &middot; Elias Issa &middot; Kohitij Kar &middot; Pouya Bashivan &middot; Jonathan Prescott-Roy &middot; Kailyn Schmidt &middot; Aran Nayebi &middot; Daniel Bear &middot; Daniel Yamins &middot; James J DiCarlo,https://cbmm.mit.edu/video/brain-object-recognition-high-performing-shallow-recurrent-anns,"Deep convolutional artificial neural networks (ANNs) are the leading class of candidate models of the mechanisms of visual processing in the primate ventral stream. While initially inspired by brain anatomy, over the past years, these ANNs have evolved from a simple eight-layer architecture in AlexNet to extremely deep and branching architectures, demonstrating increasingly better object categorization performance, yet bringing into question how brain-like they still are. In particular, typical deep models from the machine learning community are often hard to map onto the brain&#x27;s anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. Here we demonstrate that better anatomical alignment to the brain and high performance on machine learning as well as neuroscience measures do not have to be in contradiction. We developed CORnet-S, a shallow ANN with four anatomically mapped areas and recurrent connectivity, guided by Brain-Score, a new large-scale composite of neural and behavioral benchmarks for quantifying the functional fidelity of models of the primate ventral visual stream. Despite being significantly shallower than most models, CORnet-S is the top model on Brain-Score and outperforms similarly compact models on ImageNet. Moreover, our extensive analyses of CORnet-S circuitry variants reveal that recurrence is the main predictive factor of both Brain-Score and ImageNet top-1 performance. Finally, we report that the temporal evolution of the CORnet-S &quot;IT&quot; neural population resembles the actual monkey IT population dynamics. Taken together, these results establish CORnet-S, a compact, recurrent ANN, as the current best model of the primate ventral visual stream."
http://papers.nips.cc/paper/by-source-2019-4795,https://proceedings.neurips.cc/paper/2019/file/adf7ee2dcf142b0e11888e72b43fcb75-Paper.pdf,This Looks Like That: Deep Learning for Interpretable Image Recognition,Chaofan Chen &middot; Oscar Li &middot; Daniel Tao &middot; Alina Barnett &middot; Cynthia Rudin &middot; Jonathan K Su,https://youtu.be/k3IQnRsl9U4,"When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models."
http://papers.nips.cc/paper/by-source-2019-3146,https://proceedings.neurips.cc/paper/2019/file/5d4ae76f053f8f2516ad12961ef7fe97-Paper.pdf,Adversarial Training and Robustness for Multiple Perturbations,Florian Tramer &middot; Dan Boneh,https://www.youtube.com/watch?v=aUDkfVd3t_8,"Defenses against adversarial examples, such as adversarial training, are typically tailored to a single perturbation type (e.g., small $\ell_\infty$-noise). For other perturbations, these defenses offer no guarantees and, at times, even increase the model&#x27;s vulnerability.
"
http://papers.nips.cc/paper/by-source-2019-8678,https://proceedings.neurips.cc/paper/2019/file/2201611d7a08ffda97e3e8c6b667a1bc-Paper.pdf,Multi-Criteria Dimensionality Reduction with Applications to Fairness,Uthaipon Tantipongpipat &middot; Samira Samadi &middot; Mohit Singh &middot; Jamie Morgenstern &middot; Santosh Vempala,https://youtu.be/aR64H5Jn0gY,"Dimensionality reduction is a classical technique widely used for data analysis. One foundational instantiation is Principal Component Analysis (PCA),
"
http://papers.nips.cc/paper/by-source-2019-1349,https://proceedings.neurips.cc/paper/2019/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf,Integrating Bayesian and Discriminative Sparse Kernel Machines for  Multi-class Active Learning,Weishi Shi &middot; Qi Yu,https://drive.google.com/open?id=1awBUI9liuh28vfJWHlc2beoYm8Zfybm9,"We propose a novel active learning (AL) model that integrates Bayesian and discriminative kernel machines for fast and accurate multi-class data sampling. By joining a sparse Bayesian model and a maximum margin machine under a unified kernel machine committee (KMC), the proposed model is able to identify a small number of data samples that best represent the overall data space while accurately capturing the decision boundaries. The integration is conducted using the  maximum entropy discrimination framework, resulting in a joint objective function that contains generalized entropy as a regularizer. Such a property allows the proposed AL model to choose data samples that more effectively handle non-separable classification problems. Parameter learning is achieved through a principled optimization framework that leverages  convex duality and sparse structure of KMC to efficiently optimize the joint objective function. Key model parameters are used to design a novel sampling function  to choose data samples that can simultaneously improve multiple decision boundaries, making it an effective sampler for problems with a large number of classes. Experiments conducted over both synthetic and real data and comparison with competitive AL methods demonstrate the effectiveness of the proposed model.
"
http://papers.nips.cc/paper/by-source-2019-5204,https://proceedings.neurips.cc/paper/2019/file/2e6d9c6052e99fcdfa61d9b9da273ca2-Paper.pdf,Practical Two-Step Lookahead Bayesian Optimization,Jian Wu &middot; Peter Frazier,https://github.com/wujian16/TwoStep-BayesOpt/tree/master/Presentation,"Expected improvement and other acquisition functions widely used in Bayesian optimization use a &quot;one-step&quot; assumption: they value objective function evaluations assuming no future evaluations will be performed. Because we usually evaluate over multiple steps, this assumption may leave substantial room for improvement. Existing theory gives acquisition functions looking multiple steps in the future but calculating them requires solving a high-dimensional continuous-state continuous-action Markov decision process (MDP). Fast exact solutions of this MDP remain out of reach of today&#x27;s methods. As a result, previous two- and multi-step lookahead Bayesian optimization algorithms are either too expensive to implement in most practical settings or resort to heuristics that may fail to fully realize the promise of two-step lookahead. This paper proposes a computationally efficient algorithm that provides an accurate solution to the two-step lookahead Bayesian optimization problem in seconds to at most several minutes of computation per batch of evaluations. The resulting acquisition function provides increased query efficiency and robustness compared with previous two- and multi-step lookahead methods in both single-threaded and batch experiments. This unlocks the value of two-step lookahead in practice. We demonstrate the value of our algorithm with extensive experiments on synthetic test functions and real-world problems."
http://papers.nips.cc/paper/by-source-2019-1775,https://proceedings.neurips.cc/paper/2019/file/36d7534290610d9b7e9abed244dd2f28-Paper.pdf,Thompson Sampling for Multinomial Logit Contextual Bandits,Min-hwan Oh &middot; Garud Iyengar,http://www.columbia.edu/~mo2499/ts-mnl.html,"We consider a dynamic assortment selection problem where the goal is to offer a sequence of assortments that maximizes the expected cumulative revenue, or alternatively, minimize the expected regret. The feedback here is the item that the user picks from the assortment. The distinguishing feature in this work is that this feedback has a multinomial logistic distribution. The utility of each item is a dynamic function of contextual information of both the item and the user.
"
http://papers.nips.cc/paper/by-source-2019-6371,https://proceedings.neurips.cc/paper/2019/file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf,Online Continual Learning with Maximal Interfered Retrieval,Rahaf Aljundi &middot; Eugene Belilovsky &middot; Tinne Tuytelaars &middot; Laurent Charlin &middot; Massimo Caccia &middot; Min Lin &middot; Lucas Page-Caccia,https://www.youtube.com/watch?v=wfb9UV_n8jg&feature=youtu.be,"Continual learning, the setting where a learning agent is faced with a never-ending stream of data, continues to be a great challenge for modern machine learning systems. In particular the online or &quot;single-pass through the data&quot; setting has gained attention recently as a natural setting that is difficult to tackle. Methods based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from a generative model, which is suboptimal. In this work, we consider a controlled sampling of memories for replay. We retrieve the samples which are most interfered, i.e. whose prediction will be most negatively impacted by the foreseen parameters update. We show a formulation for this sampling criterion in both the generative replay and the experience replay setting, producing consistent gains in performance and greatly reduced forgetting. We release an implementation of our method at https://github.com/optimass/Maximally_Interfered_Retrieval"
http://papers.nips.cc/paper/by-source-2019-3880,https://proceedings.neurips.cc/paper/2019/file/0dd1bc593a91620daecf7723d2235624-Paper.pdf,Unsupervised Emergence of Egocentric Spatial Structure from Sensorimotor Prediction,Alban Laflaquière &middot; Michael Garcia Ortiz,https://www.dropbox.com/sh/6jcy6mq1tp2u8th/AAD8aQiSvCybLKSL4kA-wdhia?dl=0,"Despite its omnipresence in robotics application, the nature of spatial knowledge and the mechanisms that underlie its emergence in autonomous agents are still poorly understood. Recent theoretical works suggest that the Euclidean structure of space induces invariants in an agent’s raw sensorimotor experience. We hypothesize that capturing these invariants is beneficial for sensorimotor prediction and that, under certain exploratory conditions, a motor representation capturing the structure of the external space should emerge as a byproduct of learning to predict future sensory experiences. We propose a simple sensorimotor predictive scheme, apply it to different agents and types of exploration, and evaluate the pertinence of these hypotheses. We show that a naive agent can capture the topology and metric regularity of its sensor’s position in an egocentric spatial frame without any a priori knowledge, nor extraneous supervision."
http://papers.nips.cc/paper/by-source-2019-8678,https://proceedings.neurips.cc/paper/2019/file/2201611d7a08ffda97e3e8c6b667a1bc-Paper.pdf,Multi-Criteria Dimensionality Reduction with Applications to Fairness,Uthaipon Tantipongpipat &middot; Samira Samadi &middot; Mohit Singh &middot; Jamie Morgenstern &middot; Santosh Vempala,https://youtu.be/aR64H5Jn0gY,"Dimensionality reduction is a classical technique widely used for data analysis. One foundational instantiation is Principal Component Analysis (PCA),
"
http://papers.nips.cc/paper/by-source-2019-4795,https://proceedings.neurips.cc/paper/2019/file/adf7ee2dcf142b0e11888e72b43fcb75-Paper.pdf,This Looks Like That: Deep Learning for Interpretable Image Recognition,Chaofan Chen &middot; Oscar Li &middot; Daniel Tao &middot; Alina Barnett &middot; Cynthia Rudin &middot; Jonathan K Su,https://youtu.be/k3IQnRsl9U4,"When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models."
http://papers.nips.cc/paper/by-source-2019-3146,https://proceedings.neurips.cc/paper/2019/file/5d4ae76f053f8f2516ad12961ef7fe97-Paper.pdf,Adversarial Training and Robustness for Multiple Perturbations,Florian Tramer &middot; Dan Boneh,https://www.youtube.com/watch?v=aUDkfVd3t_8,"Defenses against adversarial examples, such as adversarial training, are typically tailored to a single perturbation type (e.g., small $\ell_\infty$-noise). For other perturbations, these defenses offer no guarantees and, at times, even increase the model&#x27;s vulnerability.
"
http://papers.nips.cc/paper/by-source-2019-86,https://proceedings.neurips.cc/paper/2019/file/93db85ed909c13838ff95ccfa94cebd9-Paper.pdf,Multi-Resolution Weak Supervision for Sequential Data,Paroma Varma &middot; Frederic Sala &middot; Shiori Sagawa &middot; Jason A Fries &middot; Daniel Fu &middot; Saelig Khattar &middot; Ashwini Ramamoorthy &middot; Ke Xiao &middot; Kayvon  Fatahalian &middot; James Priest &middot; Christopher Ré,http://fredsala.github.io/MultiResWS.mov,"Since manually labeling training data is slow and expensive, recent industrial and scientific research efforts have turned to weaker or noisier forms of supervision sources. However, existing weak supervision approaches fail to model multi-resolution sources for sequential data, like video, that can assign labels to individual elements or collections of elements in a sequence. A key challenge in weak supervision is estimating the unknown accuracies and correlations of these sources without using labeled data. Multi-resolution sources exacerbate this challenge due to complex correlations and sample complexity that scales in the length of the sequence. We propose Dugong, the first framework to model multi-resolution weak supervision sources with complex correlations to assign probabilistic labels to training data. Theoretically, we prove that Dugong, under mild conditions, can uniquely recover the unobserved accuracy and correlation parameters and use parameter sharing to improve sample complexity. Our method assigns clinician-validated labels to population-scale biomedical video repositories, helping outperform traditional supervision by 36.8 F1 points and addressing a key use case where machine learning has been severely limited by the lack of expert labeled data. On average, Dugong improves over traditional supervision by 16.0 F1 points and existing weak supervision approaches by 24.2 F1 points across several video and sensor classification tasks. "
http://papers.nips.cc/paper/by-source-2019-192,https://proceedings.neurips.cc/paper/2019/file/58a2fc6ed39fd083f55d4182bf88826d-Paper.pdf,Reducing Noise in GAN Training with Variance Reduced Extragradient,Tatjana Chavdarova &middot; Gauthier Gidel &middot; François Fleuret &middot; Simon Lacoste-Julien,https://github.com/Chavdarova/SVRE/tree/master/video,"We study the effect of the stochastic gradient noise on the training of generative adversarial networks (GANs) and show that it can prevent the convergence of standard game optimization methods, while the batch version converges. We address this issue with a novel stochastic variance-reduced extragradient (SVRE) optimization algorithm, which for a large class of games improves upon the previous convergence rates proposed in the literature. We observe empirically that SVRE performs similarly to a batch method on MNIST while being computationally cheaper, and that SVRE yields more stable GAN training on standard datasets."
http://papers.nips.cc/paper/by-source-2019-93,https://proceedings.neurips.cc/paper/2019/file/98dce83da57b0395e163467c9dae521b-Paper.pdf,The Point Where Reality Meets Fantasy: Mixed Adversarial Generators for Image Splice Detection,Vladimir V. Kniaz &middot; Vladimir Knyaz &middot; Fabio Remondino,https://www.dropbox.com/s/xfxszql3dh6a7uk/MAG_Presentation.mp4?dl=0,"Modern photo editing tools allow creating realistic manipulated images easily. While fake images can be quickly generated, learning models for their detection is challenging due to the high variety of tampering artifacts and the lack of large labeled datasets of manipulated images. In this paper, we propose a new framework for training of discriminative segmentation model via an adversarial process. We simultaneously train four models: a generative retouching model G_R that translates manipulated image to the real image domain, a generative annotation model G_A that estimates the pixel-wise probability of image patch being either real or fake, and two discriminators D_R and D_A that qualify the output of G_R and G_A. The aim of model G_R is to maximize the probability of model G_A making a mistake. Our method extends the generative adversarial networks framework with two main contributions: (1) training of a generative model G_R against a deep semantic segmentation network G_A that learns rich scene semantics for manipulated region detection, (2) proposing per class semantic loss that facilitates semantically consistent image retouching by the G_R. We collected large-scale manipulated image dataset to train our model. The dataset includes 16k real and fake images with pixel-level annotations of manipulated areas. The dataset also provides ground truth pixel-level object annotations. We validate our approach on several modern manipulated image datasets, where quantitative results and ablations demonstrate that our method achieves and surpasses the state-of-the-art in manipulated image detection. We made our code and dataset publicly available."
http://papers.nips.cc/paper/by-source-2019-2770,https://proceedings.neurips.cc/paper/2019/file/a7a3d70c6d17a73140918996d03c014f-Paper.pdf,Neural Similarity Learning,Weiyang Liu &middot; Zhen Liu &middot; James Rehg &middot; Le Song,http://wyliu.com/,"Inner product-based convolution has been the founding stone of convolutional neural networks (CNNs), enabling end-to-end learning of visual representation. By generalizing inner product with a bilinear matrix, we propose the neural similarity which serves as a learnable parametric similarity measure for CNNs. Neural similarity naturally generalizes the convolution and enhances flexibility. Further, we consider the neural similarity learning (NSL) in order to learn the neural similarity adaptively from training data. Specifically, we propose two different ways of learning the neural similarity: static NSL and dynamic NSL. Interestingly, dynamic neural similarity makes the CNN  become a dynamic inference network. By regularizing the bilinear matrix, NSL can be viewed as learning the shape of kernel and the similarity measure simultaneously. We further justify the effectiveness of NSL with a theoretical viewpoint. Most importantly, NSL shows promising performance in visual recognition and few-shot learning, validating the superiority of NSL over the inner product-based convolution counterparts."
http://papers.nips.cc/paper/by-source-2019-4818,https://proceedings.neurips.cc/paper/2019/file/e88f243bf341ded9b4ced444795c3f17-Paper.pdf,Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations,Kevin Smith &middot; Lingjie Mei &middot; Shunyu Yao &middot; Jiajun Wu &middot; Elizabeth Spelke &middot; Josh Tenenbaum &middot; Tomer Ullman,https://www.youtube.com/watch?v=95HlF9nCca4&feature=youtu.be,"From infancy, humans have expectations about how objects will move and interact. Even young children expect objects not to move through one another, teleport, or disappear. They are surprised by mismatches between physical expectations and perceptual observations, even in unfamiliar scenes with completely novel objects. A model that exhibits human-like understanding of physics should be similarly surprised, and adjust its beliefs accordingly.  We propose ADEPT, a model that uses a coarse (approximate geometry) object-centric representation for dynamic 3D scene understanding. Inference integrates deep recognition networks, extended probabilistic physical simulation, and particle filtering for forming predictions and expectations across occlusion. We also present a new test set for measuring violations of physical expectations, using a range of scenarios derived from developmental psychology.  We systematically compare ADEPT, baseline models, and human expectations on this test set.  ADEPT outperforms standard network architectures in discriminating physically implausible scenes, and often performs this discrimination at the same level as people."
http://papers.nips.cc/paper/by-source-2019-4107,https://proceedings.neurips.cc/paper/2019/file/2297607a5db8576d5ad6bbd83696ff60-Paper.pdf,Nonlinear scaling of resource allocation in sensory bottlenecks,Laura R Edmondson &middot; Alejandro Jimenez Rodriguez &middot; Hannes P. Saal,https://www.youtube.com/watch?v=M6ohnt_6J9Q,"In many sensory systems, information transmission is constrained by a bottleneck,
"
http://papers.nips.cc/paper/by-source-2019-6969,https://proceedings.neurips.cc/paper/2019/file/7813d1590d28a7dd372ad54b5d29d033-Paper.pdf,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs,Jonas Kubilius &middot; Martin Schrimpf &middot; Kohitij Kar &middot; Rishi Rajalingham &middot; Ha Hong &middot; Najib Majaj &middot; Elias Issa &middot; Pouya Bashivan &middot; Jonathan Prescott-Roy &middot; Kailyn Schmidt &middot; Aran Nayebi &middot; Daniel Bear &middot; Daniel Yamins &middot; James J DiCarlo,https://cbmm.mit.edu/video/brain-object-recognition-high-performing-shallow-recurrent-anns,"Deep convolutional artificial neural networks (ANNs) are the leading class of candidate models of the mechanisms of visual processing in the primate ventral stream. While initially inspired by brain anatomy, over the past years, these ANNs have evolved from a simple eight-layer architecture in AlexNet to extremely deep and branching architectures, demonstrating increasingly better object categorization performance, yet bringing into question how brain-like they still are. In particular, typical deep models from the machine learning community are often hard to map onto the brain&#x27;s anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. Here we demonstrate that better anatomical alignment to the brain and high performance on machine learning as well as neuroscience measures do not have to be in contradiction. We developed CORnet-S, a shallow ANN with four anatomically mapped areas and recurrent connectivity, guided by Brain-Score, a new large-scale composite of neural and behavioral benchmarks for quantifying the functional fidelity of models of the primate ventral visual stream. Despite being significantly shallower than most models, CORnet-S is the top model on Brain-Score and outperforms similarly compact models on ImageNet. Moreover, our extensive analyses of CORnet-S circuitry variants reveal that recurrence is the main predictive factor of both Brain-Score and ImageNet top-1 performance. Finally, we report that the temporal evolution of the CORnet-S &quot;IT&quot; neural population resembles the actual monkey IT population dynamics. Taken together, these results establish CORnet-S, a compact, recurrent ANN, as the current best model of the primate ventral visual stream."
http://papers.nips.cc/paper/by-source-2019-5317,https://proceedings.neurips.cc/paper/2019/file/125b93c9b50703fe9dac43ec231f5f83-Paper.pdf,Efficiently avoiding saddle points with zero order methods: No gradients required ,Emmanouil-Vasileios Vlatakis-Gkaragkounis &middot; Lampros Flokas &middot; Georgios Piliouras,http://www.cs.columbia.edu/~emvlatakis/Videos/Poincare_Recurrence_Cycles_and_Spurious_Equilibria_in_Gradient-Descent-Ascent_for_Non-Convex_Non-Concave_Zero-Sum_Games.html,"We consider the case of derivative-free algorithms for non-convex optimization, also known as zero order algorithms, that use only function evaluations rather than gradients. For a wide variety of gradient approximators based on finite differences, we establish asymptotic convergence to second order stationary points using a carefully tailored application of the Stable Manifold Theorem.  Regarding efficiency, we introduce a noisy zero-order method that converges to second order stationary points, i.e avoids saddle points. Our algorithm uses only $\tilde{\mathcal{O}}(1 / \epsilon^2)$ approximate gradient calculations and, thus, it matches the converge rate guarantees of their exact gradient counterparts up to constants. In contrast to previous work, our convergence rate analysis avoids imposing additional dimension dependent slowdowns in the number of iterations required for non-convex zero order optimization."
http://papers.nips.cc/paper/by-source-2019-6819,https://proceedings.neurips.cc/paper/2019/file/b6f97e6f0fd175613910d613d574d0cb-Paper.pdf,Control What You Can: Intrinsically Motivated Task-Planning Agent,Sebastian Blaes &middot; Marin Vlastelica Pogančić &middot; Jiajie Zhu &middot; Georg Martius,https://owncloud.tuebingen.mpg.de/index.php/s/M3Ze4GStAn4aZdD,"We present a novel intrinsically motivated agent that learns how to control the
"
http://papers.nips.cc/paper/by-source-2019-7793,https://proceedings.neurips.cc/paper/2019/file/e2ccf95a7f2e1878fcafc8376649b6e8-Paper.pdf,Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck,Maximilian Igl &middot; Kamil Ciosek &middot; Yingzhen Li &middot; Sebastian Tschiatschek &middot; Cheng Zhang &middot; Sam Devlin &middot; Katja Hofmann,https://1drv.ms/v/s!Aj45y31uNubdlVXufoKxmwAqr0fP?e=TtGFfX,"The ability for policies to generalize to new environments is key to the broad application of RL agents. A promising approach to prevent an agent’s policy from overfitting to a limited set of training environments is to apply regularization techniques originally developed for supervised learning. However, there are stark differences between supervised learning and RL. We discuss those differences and propose modifications to existing regularization techniques in order to better adapt them to RL. In particular, we focus on regularization techniques relying on the injection of noise into the learned function, a family that includes some of the most widely used approaches such as Dropout and Batch Normalization. To adapt them to RL, we propose Selective Noise Injection (SNI), which maintains the regularizing effect the injected noise has, while mitigating the adverse effects it has on the gradient quality. Furthermore, we demonstrate that the Information Bottleneck (IB) is a particularly well suited regularization technique for RL as it is effective in the low-data regime encountered early on in training RL agents. Combining the IB with SNI, we significantly outperform current state of the art results, including on the recently proposed generalization benchmark Coinrun."
http://papers.nips.cc/paper/by-source-2019-7307,https://proceedings.neurips.cc/paper/2019/file/a1a527267c0d33a86382a03c4c721cd2-Paper.pdf,Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning,Mahmoud Assran &middot; Joshua Romoff &middot; Nicolas Ballas &middot; Joelle Pineau &middot; Mike Rabbat,https://www.dropbox.com/sh/1ommra478q2j0mx/AAD8DpJMrnX_jYcfPKrht4nWa?dl=0,"Multi-simulator training has contributed to the recent success of Deep Reinforcement Learning (Deep RL) by stabilizing learning and allowing for higher training throughputs. In this work, we propose Gossip-based Actor-Learner Architectures (GALA) where several actor-learners (such as A2C agents) are organized in a peer-to-peer communication topology, and exchange information through asynchronous gossip in order to take advantage of a large number of distributed simulators. We prove that GALA agents remain within an epsilon-ball of one-another during training when using loosely coupled asynchronous communication. By reducing the amount of synchronization between agents, GALA is more computationally efficient and scalable compared to A2C, its fully-synchronous counterpart. GALA also outperforms A2C, being more robust and sample efficient. We show that we can run several loosely coupled GALA agents in parallel on a single GPU and achieve significantly higher hardware utilization and frame-rates than vanilla A2C at comparable power draws."
http://papers.nips.cc/paper/by-source-2019-9215,https://proceedings.neurips.cc/paper/2019/file/5a44a53b7d26bb1e54c05222f186dcfb-Paper.pdf,Imitation-Projected Programmatic Reinforcement Learning,Abhinav Verma &middot; Hoang Le &middot; Yisong Yue &middot; Swarat Chaudhuri,https://sites.google.com/view/programmatic-rl,"We study the problem of programmatic reinforcement learning, in which policies are represented as short programs in a symbolic language. Programmatic policies can be more interpretable, generalizable, and amenable to formal verification than neural policies; however, designing rigorous learning approaches for such policies remains a challenge. Our approach to this challenge - a meta-algorithm called PROPEL - is based on three insights. First, we view our learning task as optimization in policy space, modulo the constraint that the desired policy has a programmatic representation, and solve this optimization problem using a form of mirror descent that takes a gradient step into the unconstrained policy space and then projects back onto the constrained space.  Second, we view the unconstrained policy space as mixing neural and programmatic representations, which enables employing state-of-the-art deep policy gradient approaches.  Third, we cast the projection step as program synthesis via imitation learning, and exploit contemporary combinatorial methods for this task. We present theoretical convergence results for PROPEL and empirically evaluate the approach in three continuous control domains. The experiments show that PROPEL can significantly outperform state-of-the-art approaches for learning programmatic policies."
http://papers.nips.cc/paper/by-source-2019-4541,https://proceedings.neurips.cc/paper/2019/file/2f3c6a4cd8af177f6456e7e51a916ff3-Paper.pdf,The spiked matrix model with generative priors,Benjamin Aubin &middot; Bruno Loureiro &middot; Antoine Maillard &middot; Florent Krzakala &middot; Lenka Zdeborová,https://youtu.be/0XEVjCL_9Dk,"Using a low-dimensional parametrization of signals is a generic and powerful way to enhance performance in signal processing and statistical inference. A very popular and widely explored type of dimensionality reduction is sparsity; another type is generative modelling of signal distributions. Generative models based on neural networks, such as GANs or variational auto-encoders, are particularly performant and are gaining on applicability. In this paper we study spiked matrix models, where a low-rank matrix is observed through a noisy channel. This problem with sparse structure of the spikes has attracted broad attention in the past literature. Here, we replace the sparsity assumption by generative modelling, and investigate the consequences on statistical and algorithmic properties. We analyze the Bayes-optimal performance under specific generative models for the spike. In contrast with the sparsity assumption, we do not observe regions of parameters where statistical performance is superior to the best known algorithmic performance. We show that in the analyzed cases the approximate message passing algorithm is able to reach optimal performance. We also design enhanced spectral algorithms and analyze their performance and thresholds using random matrix theory, showing their superiority to the classical principal component analysis. We complement our theoretical results by illustrating the performance of the spectral algorithms when the spikes come from real datasets."
http://papers.nips.cc/paper/by-source-2019-6476,https://proceedings.neurips.cc/paper/2019/file/ab49ef78e2877bfd2c2bfa738e459bf0-Paper.pdf,The Impact of Regularization on High-dimensional Logistic Regression,Fariborz Salehi &middot; Ehsan Abbasi &middot; Babak Hassibi,https://caltech.box.com/s/e8gcoyxd2bwasd6kvkrgxvc9i9osmmmv,"Logistic regression is commonly used for modeling dichotomous outcomes. In the classical setting, where the number of observations is much larger than the number of parameters, properties of the maximum likelihood estimator in logistic regression are well understood. Recently, Sur and Candes~\cite{sur2018modern} have studied logistic regression in the high-dimensional regime, where the number of observations and parameters are comparable, and show, among other things, that the maximum likelihood estimator is biased. In the high-dimensional regime the underlying parameter vector is often structured (sparse, block-sparse, finite-alphabet, etc.) and so in this paper we study regularized logistic regression (RLR), where a convex regularizer that encourages the desired structure is added to the negative of the log-likelihood function. An advantage of RLR is that it allows parameter recovery even for instances where the (unconstrained) maximum likelihood estimate does not exist. We provide a precise analysis of the performance of RLR via the solution of a system of six nonlinear equations, through which any performance metric of interest (mean, mean-squared error, probability of support recovery, etc.) can be explicitly computed. Our results generalize those of Sur and Candes and we provide a detailed study for the cases of $\ell_2^2$-RLR and sparse ($\ell_1$-regularized) logistic regression. In both cases, we obtain explicit expressions for various performance metrics and can find the values of the regularizer parameter that optimizes the desired performance. The theory is validated by extensive numerical simulations across a range of parameter values and problem instances."
http://papers.nips.cc/paper/by-source-2019-6320,https://proceedings.neurips.cc/paper/2019/file/faf02b2358de8933f480a146f4d2d98e-Paper.pdf,On the Downstream Performance of Compressed Word Embeddings,Avner May &middot; Jian Zhang &middot; Tri Dao &middot; Christopher Ré,https://avnermay.github.io/compressed_embeddings_neurips_2019_video.mp4,"Compressing word embeddings is important for deploying NLP models in memory-constrained settings. However, understanding what makes compressed embeddings perform well on downstream tasks is challenging---existing measures of compression quality often fail to distinguish between embeddings that perform well and those that do not. We thus propose the eigenspace overlap score as a new measure. We relate the eigenspace overlap score to downstream performance by developing generalization bounds for the compressed embeddings in terms of this score, in the context of linear and logistic regression. We then show that we can lower bound the eigenspace overlap score for a simple uniform quantization compression method, helping to explain the strong empirical performance of this method. Finally, we show that by using the eigenspace overlap score as a selection criterion between embeddings drawn from a representative set we compressed, we can efficiently identify the better performing embedding with up to 2x lower selection error rates than the next best measure of compression quality, and avoid the cost of training a separate model for each task of interest."
http://papers.nips.cc/paper/by-source-2019-39,https://proceedings.neurips.cc/paper/2019/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf,"Ask not what AI can do, but what AI should do: Towards a framework of task delegability",Brian Lubars &middot; Chenhao Tan,http://tiny.cc/delegationvideo,"While artificial intelligence (AI) holds promise for addressing societal challenges, issues of exactly which tasks to automate and to what extent to do so remain understudied. We approach this problem of task delegability from a human-centered perspective by developing a framework on human perception of task delegation to AI. We consider four high-level factors that can contribute to a delegation decision: motivation, difficulty, risk, and trust. To obtain an empirical understanding of human preferences in different tasks, we build a dataset of 100 tasks from academic papers, popular media portrayal of AI, and everyday life, and administer a survey based on our proposed framework. We find little preference for full AI control and a strong preference for machine-in-the-loop designs, in which humans play the leading role. Among the four factors, trust is the most correlated with human preferences of optimal human-machine delegation. This framework represents a first step towards characterizing human preferences of AI automation across tasks. We hope this work encourages future efforts towards understanding such individual attitudes; our goal is to inform the public and the AI research community rather than dictating any direction in technology development."
http://papers.nips.cc/paper/by-source-2019-5938,https://proceedings.neurips.cc/paper/2019/file/62e0973455fd26eb03e91d5741a4a3bb-Paper.pdf,Online Forecasting of Total-Variation-bounded Sequences,Dheeraj Baby &middot; Yu-Xiang Wang,https://slideslive.com/38917673/online-forecasting-of-totalvariationbounded-sequences,"We consider the problem of online forecasting of sequences of length $n$ with total-variation at most $C_n$ using observations contaminated by independent $\sigma$-subgaussian noise. We design an $O(n\log n)$-time algorithm that achieves a cumulative square error of $\tilde{O}(n^{1/3}C_n^{2/3}\sigma^{4/3} + C_n^2)$ with high probability. We also prove a lower bound that matches the upper bound in all parameters (up to a $\log(n)$ factor). To the best of our knowledge, this is the first **polynomial-time** algorithm that achieves the optimal $O(n^{1/3})$ rate in forecasting total variation bounded sequences and the first algorithm that **adapts to unknown** $C_n$.Our proof techniques leverage the special localized structure of Haar wavelet basis and the adaptivity to unknown smoothness parameters in the classical wavelet smoothing [Donoho et al., 1998]. We also compare our model to the rich literature of dynamic regret minimization and nonstationary stochastic optimization, where our problem can be treated as a special case. We show that the workhorse in those settings --- online gradient descent and its variants with a fixed restarting schedule --- are instances of a class of **linear forecasters** that require a suboptimal regret of $\tilde{\Omega}(\sqrt{n})$. This implies that the use of more adaptive algorithms is necessary to obtain the optimal rate. "
http://papers.nips.cc/paper/by-source-2019-8876,https://proceedings.neurips.cc/paper/2019/file/73bf6c41e241e28b89d0fb9e0c82f9ce-Paper.pdf,Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology,Nima Dehmamy &middot; Albert-Laszlo Barabasi &middot; Rose Yu,https://github.com/nimadehmamy/Understanding-GCN,"To deepen our understanding of graph neural networks, we investigate the representation power of Graph Convolutional Networks (GCN) through the looking glass of graph moments, a key property of  graph topology encoding path of various lengths.
"
http://papers.nips.cc/paper/by-source-2019-6710,https://proceedings.neurips.cc/paper/2019/file/dffbb6efd376d8dbb22cdf491e481edc-Paper.pdf,Universality in Learning from Linear Measurements,Ehsan Abbasi &middot; Fariborz Salehi &middot; Babak Hassibi,https://caltech.box.com/s/vwr8lrxy9edr5m2vbwqheeryoi7y3wq6,"We study the problem of recovering a structured signal from independently and identically drawn linear measurements. A convex penalty function $f(\cdot)$ is considered which penalizes deviations from the desired structure, and signal recovery is performed by minimizing $f(\cdot)$ subject to the linear measurement constraints. The main question of interest is to determine the minimum number of measurements that is necessary and sufficient for the perfect recovery of the unknown signal with high probability. Our main result states that, under some mild conditions on $f(\cdot)$ and on the distribution from which the linear measurements are drawn, the minimum number of measurements required for perfect recovery depends only on the first and second order statistics of the measurement vectors. As a result, the required of number of measurements can be determining by studying measurement vectors that are Gaussian (and have the same mean vector and covariance matrix) for which a rich literature and comprehensive theory exists. As an application, we show that the minimum number of random quadratic measurements (also known as rank-one projections) required to recover a low rank positive semi-definite matrix is $3nr$, where $n$ is the dimension of the matrix and $r$ is its rank. As a consequence, we settle the long standing open question of determining the minimum number of measurements required for perfect signal recovery in phase retrieval using the celebrated PhaseLift algorithm, and show it to be $3n$."
http://papers.nips.cc/paper/by-source-2019-7492,https://proceedings.neurips.cc/paper/2019/file/c82b013313066e0702d58dc70db033ca-Paper.pdf,"Search-Guided, Lightly-Supervised Training of Structured Prediction Energy Networks",Amirmohammad Rooshenas &middot; Dongxu Zhang &middot; Gopal Sharma &middot; Andrew McCallum,https://rooshenas.github.io/files/sg-spen.mp4," In structured output prediction tasks, labeling ground-truth training output is often expensive. However, for many tasks, even when the true output is unknown, we can evaluate predictions using a scalar reward function, which may be easily assembled from human knowledge or non-differentiable pipelines.  But searching through the entire output space to find the best output with respect to this reward function is typically intractable.  In this paper, we instead use efficient truncated randomized search in this reward function to train structured prediction energy networks (SPENs), which provide efficient test-time inference using gradient-based search on a smooth, learned representation of the score landscape, and have previously yielded state-of-the-art results in structured prediction.  In particular, this truncated randomized search in the reward function yields previously unknown local improvements, providing effective supervision to SPENs, avoiding their traditional need for labeled training data."
http://papers.nips.cc/paper/by-source-2019-8097,https://proceedings.neurips.cc/paper/2019/file/5a2afca61e35f45a7dd44ca46e0225f4-Paper.pdf,Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization,Miika Aittala &middot; Prafull Sharma &middot; Lukas Murmann &middot; Adam Yedidia &middot; Gregory Wornell &middot; Bill Freeman &middot; Fredo Durand,http://compmirrors.csail.mit.edu/videos/compmirrors_3minute.mp4,"We recover a video of the motion taking place in a hidden scene by observing changes in indirect illumination in a nearby uncalibrated visible region. We solve this problem by factoring the observed video into a matrix product between the unknown hidden scene video and an unknown light transport matrix. This task is extremely ill-posed, as any non-negative factorization will satisfy the data. Inspired by recent work on the Deep Image Prior, we parameterize the factor matrices using randomly initialized convolutional neural networks trained in a one-off manner, and show that this results in decompositions that reflect the true motion in the hidden scene."
http://papers.nips.cc/paper/by-source-2019-5098,https://proceedings.neurips.cc/paper/2019/file/f5ac21cd0ef1b88e9848571aeb53551a-Paper.pdf,Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer,Wenzheng Chen &middot; Huan Ling &middot; Jun Gao &middot; Edward Smith &middot; Jaakko Lehtinen &middot; Alec Jacobson &middot; Sanja Fidler,https://nv-tlabs.github.io/DIB-R/,"Many machine learning models operate on images, but ignore the fact that images are 2D projections formed by 3D geometry interacting with light, in a process called rendering. Enabling ML models to understand image formation might be key for generalization. However, due to an essential rasterization step involving discrete assignment operations, rendering pipelines are non-differentiable and thus largely inaccessible to gradient-based ML techniques. In this paper, we present DIB-Render, a novel rendering framework through which gradients can be analytically computed. Key to our approach is to view rasterization as a weighted interpolation, allowing image gradients to back-propagate through various standard vertex shaders within a single framework. Our approach supports optimizing over vertex positions, colors, normals, light directions and texture coordinates, and allows us to incorporate various well-known lighting models from graphics. We showcase our approach in two ML applications: single-image 3D object prediction, and 3D textured object generation, both trained using exclusively 2D supervision. "
http://papers.nips.cc/paper/by-source-2019-4875,https://proceedings.neurips.cc/paper/2019/file/cf040fc71060367913e81ac1eb050aea-Paper.pdf,STAR-Caps: Capsule Networks with Straight-Through Attentive Routing,Karim Ahmed &middot; Lorenzo Torresani,http://bit.ly/2PA6JlV," Capsule networks have been shown to be powerful models for image classification, thanks to their ability to represent and capture viewpoint variations of an object. However, the high computational complexity of capsule networks that stems from the recurrent dynamic routing poses a major drawback making their use for large-scale image classification challenging. In this work, we propose Star-Caps a capsule-based network that exploits a straight-through attentive routing to address the drawbacks of capsule networks. By utilizing attention modules augmented by differentiable binary routers, the proposed mechanism estimates the routing coefficients between capsules without recurrence, as opposed to prior related work. Subsequently, the routers utilize straight-through estimators to make binary decisions to either connect or disconnect the route between capsules, allowing stable and faster performance. The experiments conducted on several image classification datasets, including MNIST, SmallNorb, CIFAR-10, CIFAR-100, and ImageNet show that STAR-Caps outperforms the baseline capsule networks.
"
http://papers.nips.cc/paper/by-source-2019-39,https://proceedings.neurips.cc/paper/2019/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf,"Ask not what AI can do, but what AI should do: Towards a framework of task delegability",Brian Lubars &middot; Chenhao Tan,http://tiny.cc/delegationvideo,"While artificial intelligence (AI) holds promise for addressing societal challenges, issues of exactly which tasks to automate and to what extent to do so remain understudied. We approach this problem of task delegability from a human-centered perspective by developing a framework on human perception of task delegation to AI. We consider four high-level factors that can contribute to a delegation decision: motivation, difficulty, risk, and trust. To obtain an empirical understanding of human preferences in different tasks, we build a dataset of 100 tasks from academic papers, popular media portrayal of AI, and everyday life, and administer a survey based on our proposed framework. We find little preference for full AI control and a strong preference for machine-in-the-loop designs, in which humans play the leading role. Among the four factors, trust is the most correlated with human preferences of optimal human-machine delegation. This framework represents a first step towards characterizing human preferences of AI automation across tasks. We hope this work encourages future efforts towards understanding such individual attitudes; our goal is to inform the public and the AI research community rather than dictating any direction in technology development."
http://papers.nips.cc/paper/by-source-2019-6217,https://proceedings.neurips.cc/paper/2019/file/9d1827dc5f75b9d65d80e25eb862e676-Paper.pdf,A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions,Mejbah Alam &middot; Justin Gottschlich &middot; Nesime Tatbul &middot; Javier Turek &middot; Tim Mattson &middot; Abdullah Muzahid,https://youtu.be/FkT1aNoKbG4,"The field of machine programming (MP), the automation of the development of software, is making notable research advances. This is, in part, due to the emergence of a wide range of novel techniques in machine learning. In this paper, we apply MP to the automation of software performance regression testing. A performance regression is a software performance degradation caused by a code change. We present AutoPerf – a novel approach to automate regression testing that utilizes three core techniques: (i) zero-positive learning, (ii) autoencoders, and (iii) hardware telemetry. We demonstrate AutoPerf’s generality and efficacy against 3 types of performance regressions across 10 real performance bugs in 7 benchmark and open-source programs. On average, AutoPerf exhibits 4% profiling
"
http://papers.nips.cc/paper/by-source-2019-4567,https://proceedings.neurips.cc/paper/2019/file/6562c5c1f33db6e05a082a88cddab5ea-Paper.pdf,The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic,Arash Ardakani &middot; Zhengyun Ji &middot; Amir Ardakani &middot; Warren Gross,http://isip.ece.mcgill.ca/media/neurips_2019/video.mp4,"The emergence of XNOR networks seek to reduce the model size and computational cost of neural networks for their deployment on specialized hardware requiring real-time processes with limited hardware resources. In XNOR networks, both weights and activations are binary, bringing great benefits to specialized hardware by replacing expensive multiplications with simple XNOR operations. Although XNOR convolutional and fully-connected neural networks have been successfully developed during the past few years, there is no XNOR network implementing commonly-used variants of recurrent neural networks such as long short-term memories (LSTMs). The main computational core of LSTMs involves vector-matrix multiplications followed by a set of non-linear functions and element-wise multiplications to obtain the gate activations and state vectors, respectively. Several previous attempts on quantization of LSTMs only focused on quantization of the vector-matrix multiplications in LSTMs while retaining the element-wise multiplications in full precision. In this paper, we propose a method that converts all the multiplications in LSTMs to XNOR operations using stochastic computing. To this end, we introduce a weighted finite-state machine and its synthesis method to approximate the non-linear functions used in LSTMs on stochastic bit streams. Experimental results show that the proposed XNOR LSTMs reduce the computational complexity of their quantized counterparts by a factor of 86x without any sacrifice on latency while achieving a better accuracy across various temporal tasks."
http://papers.nips.cc/paper/by-source-2019-2401,https://proceedings.neurips.cc/paper/2019/file/959ef477884b6ac2241b19ee4fb776ae-Paper.pdf,Learning Deep Bilinear Transformation for Fine-grained Image Representation,Heliang Zheng &middot; Jianlong Fu &middot; Zheng-Jun Zha &middot; Jiebo Luo,https://drive.google.com/open?id=1IBfLuHMZ5cUUDugVgcpupwwT7e5OneAu,"Bilinear feature transformation has shown the state-of-the-art performance in learning fine-grained image representations. However, the computational cost to learn pairwise interactions between deep feature channels is prohibitively expensive, which restricts this powerful transformation to be used in deep neural networks. In this paper, we propose a deep bilinear transformation (DBT) block, which can be deeply stacked in convolutional neural networks to learn fine-grained image representations. The DBT block can uniformly divide input channels into several semantic groups. As bilinear transformation can be represented by calculating pairwise interactions within each group, the computational cost can be heavily relieved. The output of each block is further obtained by aggregating intra-group bilinear features, with residuals from the entire input features. We found that the proposed network achieves new state-of-the-art in several fine-grained image recognition benchmarks, including CUB-Bird, Stanford-Car, and FGVC-Aircraft."
http://papers.nips.cc/paper/by-source-2019-4927,https://proceedings.neurips.cc/paper/2019/file/50d2d2262762648589b1943078712aa6-Paper.pdf,"Write, Execute, Assess: Program Synthesis with a REPL",Kevin Ellis &middot; Maxwell Nye &middot; Yewen Pu &middot; Felix Sosa &middot; Josh Tenenbaum &middot; Armando Solar-Lezama,https://www.youtube.com/watch?v=B3T_mEb2NGA&feature=youtu.be,"  We present a neural program synthesis approach integrating components which write, execute, and assess code to navigate the search space of possible programs. We equip the search process with an interpreter or a read-eval-print-loop (REPL), which immediately executes partially written programs, exposing their semantics. The REPL addresses a basic challenge of program synthesis: tiny changes in syntax can lead to huge changes in semantics. We train a pair of models, a policy that proposes the new piece of code to write, and a value function that assesses the prospects of the code written so-far. At test time we can combine these models with a Sequential Monte Carlo algorithm. We apply our approach to two domains: synthesizing text editing programs and inferring 2D and 3D graphics programs."
http://papers.nips.cc/paper/by-source-2019-4701,https://proceedings.neurips.cc/paper/2019/file/a9ad5f2808f68eea468621a04c49efe1-Paper.pdf,Seeing the Wind: Visual Wind Speed Prediction with a Coupled Convolutional and Recurrent Neural Network,Jennifer Cardona &middot; Michael Howland &middot; John Dabiri,https://purl.stanford.edu/xx328hs4974,"Wind energy resource quantification, air pollution monitoring, and weather forecasting all rely on rapid, accurate measurement of local wind conditions. Visual observations of the effects of wind---the swaying of trees and flapping of flags, for example---encode information regarding local wind conditions that can potentially be leveraged for visual anemometry that is inexpensive and ubiquitous. Here, we demonstrate a coupled convolutional neural network and recurrent neural network architecture that extracts the wind speed encoded in visually recorded flow-structure interactions of a flag and tree in naturally occurring wind. Predictions for wind speeds ranging from 0.75-11 m/s showed agreement with measurements from a cup anemometer on site, with a root-mean-squared error approaching the natural wind speed variability due to atmospheric turbulence. Generalizability of the network was demonstrated by successful prediction of wind speed based on recordings of other flags in the field and in a controlled wind tunnel test. Furthermore, physics-based scaling of the flapping dynamics accurately predicts the dependence of the network performance on the video frame rate and duration."
http://papers.nips.cc/paper/by-source-2019-6993,https://proceedings.neurips.cc/paper/2019/file/383beaea4aa57dd8202dbff464fee3af-Paper.pdf,Recurrent Space-time Graph Neural Networks,Andrei Nicolicioiu &middot; Iulia Duta &middot; Marius Leordeanu,https://drive.google.com/file/d/1OY4J4kJV6XF6kH0ZWIthsfUYFK138SMd/view?usp=sharing," Learning in the space-time domain remains a very challenging problem in machine learning and computer vision. Current computational models for understanding spatio-temporal visual data are heavily rooted in the classical single-image based paradigm. It is not yet well understood how to integrate information in space and time into a single, general model. We propose a neural graph model, recurrent in space and time, suitable for capturing both the local appearance and the complex higher-level interactions of different entities and objects within the changing world scene. Nodes and edges in our graph have dedicated neural networks for processing information. Nodes operate over features extracted from local parts in space and time and over previous memory states. Edges process messages between connected nodes at different locations and spatial scales or between past and present time. Messages are passed iteratively in order to transmit information globally and establish long range interactions. Our model is general and could learn to recognize a variety of high level spatio-temporal concepts and be applied to different learning tasks. We demonstrate, through extensive experiments and ablation studies, that our model outperforms strong baselines and top published methods on recognizing complex activities in video. Moreover, we obtain state-of-the-art performance on the challenging Something-Something human-object interaction dataset."
http://papers.nips.cc/paper/by-source-2019-5037,https://proceedings.neurips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,Andrei Barbu &middot; David Mayo &middot; Julian Alverio &middot; William Luo &middot; Christopher Wang &middot; Dan Gutfreund &middot; Josh Tenenbaum &middot; Boris Katz,https://objectnet.dev/3-minute-video.avi,"We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet (objects are largely centered and unoccluded) and harder (due to the controls). Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance."
http://papers.nips.cc/paper/by-source-2019-1294,https://proceedings.neurips.cc/paper/2019/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf,GENO -- GENeric Optimization for Classical Machine Learning,Soeren Laue &middot; Matthias Mitterreiter &middot; Joachim Giesen,http://www.geno-project.org/videoNeurIPS.mp4,"Although optimization is the longstanding, algorithmic backbone of machine learning new models still require the time-consuming implementation of new solvers. As a result, there are thousands of implementations of optimization algorithms for machine learning problems. A natural question is, if it is always necessary to implement a new solver, or is there one algorithm that is sufficient for most models. Common belief suggests that such a one-algorithm-fits-all approach cannot work, because this algorithm cannot exploit model specific structure. At least, a generic algorithm cannot be efficient and robust on a wide variety of problems. Here, we challenge this common belief. We have designed and implemented the optimization framework GENO (GENeric Optimization) that combines a modeling language with a generic solver. GENO takes the declaration of an optimization problem and generates a solver for the specified problem class. The framework is flexible enough to encompass most of the classical machine learning problems. We show on a wide variety of classical but also some recently suggested problems that the automatically generated solvers are (1) as efficient as well engineered, specialized solvers, (2) more efficient by a decent margin than recent state-of-the-art solvers, and (3) orders of magnitude more efficient than classical modeling language plus solver approaches."
http://papers.nips.cc/paper/by-source-2019-6320,https://proceedings.neurips.cc/paper/2019/file/faf02b2358de8933f480a146f4d2d98e-Paper.pdf,On the Downstream Performance of Compressed Word Embeddings,Avner May &middot; Jian Zhang &middot; Tri Dao &middot; Christopher Ré,https://avnermay.github.io/compressed_embeddings_neurips_2019_video.mp4,"Compressing word embeddings is important for deploying NLP models in memory-constrained settings. However, understanding what makes compressed embeddings perform well on downstream tasks is challenging---existing measures of compression quality often fail to distinguish between embeddings that perform well and those that do not. We thus propose the eigenspace overlap score as a new measure. We relate the eigenspace overlap score to downstream performance by developing generalization bounds for the compressed embeddings in terms of this score, in the context of linear and logistic regression. We then show that we can lower bound the eigenspace overlap score for a simple uniform quantization compression method, helping to explain the strong empirical performance of this method. Finally, we show that by using the eigenspace overlap score as a selection criterion between embeddings drawn from a representative set we compressed, we can efficiently identify the better performing embedding with up to 2x lower selection error rates than the next best measure of compression quality, and avoid the cost of training a separate model for each task of interest."
http://papers.nips.cc/paper/by-source-2019-7101,https://proceedings.neurips.cc/paper/2019/file/b33128cb0089003ddfb5199e1b679652-Paper.pdf,Piecewise Strong Convexity of Neural Networks,Tristan Milne,https://www.youtube.com/watch?v=z89BTMQGVng,"We study the loss surface of a feed-forward neural network with ReLU non-linearities, regularized with weight decay. We show that the regularized loss function is piecewise strongly convex on an important open set which contains, under some conditions, all of its global minimizers. This is used to prove that local minima of the regularized loss function in this set are isolated, and that every differentiable critical point in this set is a local minimum, partially addressing an open problem given at the Conference on Learning Theory (COLT) 2015; our result is also applied to linear neural networks to show that with weight decay regularization, there are no non-zero critical points in a norm ball obtaining training error below a given threshold. We also include an experimental section where we validate our theoretical work and show that the regularized loss function is almost always piecewise strongly convex when restricted to stochastic gradient descent trajectories for three standard image classification problems."
http://papers.nips.cc/paper/by-source-2019-8002,https://proceedings.neurips.cc/paper/2019/file/d9fbed9da256e344c1fa46bb46c34c5f-Paper.pdf,PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization,Thijs Vogels &middot; Sai Praneeth Karimireddy &middot; Martin Jaggi,https://youtu.be/xVxSu7KGtHw,"We study gradient compression methods to alleviate the communication bottleneck in data-parallel distributed optimization. Despite the significant attention received, current compression schemes either do not scale well, or fail to achieve the target test accuracy. We propose a low-rank gradient compressor that can i) compress gradients rapidly, ii) efficiently aggregate the compressed gradients using all-reduce, and iii) achieve test performance on par with SGD. The proposed algorithm is the only method evaluated that achieves consistent wall-clock speedups when benchmarked against regular SGD with an optimized communication backend. We demonstrate reduced training times for convolutional networks as well as LSTMs on common datasets."
