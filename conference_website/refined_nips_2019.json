[
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Xb5sM0NRy_0",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf",
        "summary": "this is a brief overview of a paper to appear at Europe's 2019 equilibrium propagation or egg drop is a very cleans by the alternative to back prop for computing error gradients it is similar to the contrast if a beam learning algorithm used to Train Boltzmann machines however it's continuous time formulation in terms very long simulation times our work proposes the discrete inversion effect prop that in specific conditions is equivalent to backpack through time and enables to Train convolutional architectures first and foremost what is egg prop like prop is used to Train islands which are fed ways that you can put eggs and minimize the energy to steady state est\u00e3o in this context training consists in making estar to coincide the best with a given target way to Train such a network we would generally perform a first full-time phase then perform by propagation through time but egg prop proceeds differently instead of propagating the arrow backward for the first phase the error is included as an elastic force that matches the system to a second steady state s theta star during a second phase the learning will consequently reads like the difference between the two equilibria we call this setting the energy base sitting to compute error gradients mac prep through time goes backward in time while egg prop goes forward in time and the effort looks very different still they are intimately related our theorem is the following provided that the first phase is converged the temporal updates of the system the second phase of egg-drop are equal to the gradients provided by back birth through time each temporal updated egg drop on the right is matched in the same color with the corresponding gradient computed by backward through time on the Left more formally we can define the gradients of Bagdad through time and the temporal update effect prop to rewrite the theorem we can numerically check this property when the system perfectly fulfills the conditions of the theorem now that all theorem all in a broader setting where the dynamics can be simply defined in terms of a primitive function Phi now what is this useful for interestingly this property can also be useful when the system does not exactly meet the requirements of our theorem this is the case for the full connected and convolutional architectures we have studied which do not exactly have a primitive function it turns out that even in this approximate setting the prediction of the theorem is still very well observed this encourages training experiences as anticipated or training results are the same that those we obtain with back prep through time our simplified equations accelerate simulations by a factor five to eight finally or convolutional architecture achieve the best performance ever reported with egg-drop on a list our work sheds new light on egg-drop and enables to train new networks with simplified equations still it has yet to be scaled to deeper architectures this study is one more step towards energy efficient implementations of backrub out of device physics see your paper for more details"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=o3GfnEjTdIQ",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf",
        "summary": "you may have come across these two papers that were published a few years ago highlighting a particular aspect of deep learning that we had all been taking for granted until then why do deep networks with so many parameters that are made to fit the training data to zero error learn a nice function that generalizes so well to unseen data conventional wisdom suggests that such complex models should learn a bad function that simply memorizes the labels on the training data this is a question that has caught the attention of both theoreticians and practitioners alike and has since become a pretty active area of research so what do these papers say mathematically speaking conventional bounds of the generalization gap like the VC dimension cannot explain this generalization puzzle as these bounds estimate the representational complexity of the network by its parameter count hence yielding vacuous generalization paths to this end these two papers proposed that we should derive more refined bounds by taking into account the fact that SGD implicitly controls the representational capacity of the network and this suggestion resulted in an exciting line of work that found new ways of deriving generalization bounds in deep learning using many different learning theoretic tools while these tools may look pretty different externally in essence they are all the same learning theoretic tool called uniform convergence unfortunately all these existing uniform convergence bounds are still either parameter account abandoned or requires some kinds of explicit modification or regularization to the network learn bias Chile in addition to these issues in our paper we bring to light some more concerning problems troubling these paths first is our finding that even though the true generalization gap decreases with training set size as expected these bounds in contrast increase with the training set size hence parameter count dependence is not the only problem plaguing these bounds next and more importantly we present some example binary classification tasks tasks and deep learning where we show that even though as Chile generalizes well it learns a decision boundary that is complex in a certain way that all uniform convergence bounds are vacuous in these settings that is uniform convergence provably fails to explain generalization in these cases through these two findings we call into question the current approach of using uniform convergence to understand generalization and deep learning perhaps it's time to look for a new tool"
    },
    {
        "sourceUrl": "https://youtu.be/-vhUWSHOqIM",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf",
        "summary": "this is a video summary of our paper adversarial examples aren't bugs they're features to appear at nerps 2019 the focus of our paper is on adversarial perturbations specifically we'd like to make progress towards answering why do these perturbations even exist one way that adversarial examples clear eyes this of our classifiers are unreasonably sensitive to useless patterns in the input pictorially suppose we think of our inputs as a collection of features here we have some useful features that the classifier is supposed to learn to do well on the classification task we also have some useless features that through some error and learning such as overfitting or sensitivity to label noise the classifier puts an unreasonably large weight on an adversary can change the decision of the classifier on any given input by just changing these sensitive features that don't make any sense to humans so with this model in mind let's try an experiment we'll start with the training set of a standard image classification task we'll then take a pre trained classifier for this data set and make an adversarial example for every training image after we make these adversarial examples we'll label every image with its adversarial class note that at this point the resulting data set consists entirely of adversarial examples and thus looks completely mislabeled to a human finally we'll train the classifier on this relabeled data set and test on the original test set surprisingly we get non-trivial accuracy on the unmodified test set this result indicates a flaw in our conceptual model of adversarial examples under that model crafting adversarial examples doesn't really change anything meaningful about the input and so learning anything about the true class boundary from adversarial examples would be impossible our experiment prompts us to think about adversarial examples in a different way resulting in what we'll call the robust features model the new trainings that we made in our experiment clearly had to have carried some information about how to distinguish dogs from cats on the other hand this information isn't something that humans can perceive since to us the data set just looked mislabeled this leads us to predict the existence of non robust features these are features that are actually indicative of the true label but can be easily manipulated by an adversary this conceptual model is actually pretty predictive using a pre trained robust model we tried removing non robust features from standard data sets and managed to construct training sets where standard non robust training yields robust classifiers for the original test sets in general her finding suggests that adversarial examples can arise from non robust features in the data that actually helped generalization but hurt robustness note that because these features help generalization simply learning better models may not be sufficient for fixing the problem of adversarial examples thanks for watching our paper summary you can find links here and in the description for our paper blog post and the robustness library our open source framework for module early training and manipulating standard in the robust classifiers you can also find a link to the summary video for our other NURBS 2019 paper showing the applications of robust classifiers to image synthesis tasks"
    },
    {
        "sourceUrl": "https://youtu.be/v8U9mM1Vwv0",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf",
        "summary": "hi this is a summarization for the adversarial training for free paper in which we trained robust models efficiently in supervised machine learning we train models using labeled data let's say we want to build a classifier that can distinguish a panda from a pumpkin we start off with some images of pandas and also some images of pumpkins then usually we have a neural network and we use some optimization routine such as some variant of stochastic gradient descent to build this classifier this classifier works very well on natural images and also images which come from the same distribution as our training data however it is known that these classifiers work horribly on adversarial examples given a classifier F which Maps an image X to the label why we say another example X plus Delta is something which the classifier correctly classifies as something other than Y but we want our adversarial example X plus Delta to look like the clean example X and we can have different measures of this look like property and one more popular measure is having the perturbation the Delta which we're going to add two eggs be bounded by Epsilon in some penal their different LP norm perturbation bounce can result in different looking adversarial examples in this study we are going to focus on L infinity bounded adversarial examples note that other store examples can be realistic adversarial training is one of the few defenses which has not been broken and is known to make networks robust against adversarial perturbations a special training involves solving a min/max optimization problem where the Maximizer is the adversary which has access to these Delta perturbations which are going to be added to the clean image and the minimizer is the network which has access to the network parameters W the way that this is practically solved is by training on address show examples on the fly so we efficiently what we do is we make adversarial examples and we instead of training on natural examples we train on adversarial examples and this way we can become robust against that force early examples there are many ways that we can generate adversarial examples to train on and it is known that if we train on weak adversarial examples we will not be robust against a stronger adverse real examples so we usually do train on address or examples made in an iterative fashion such as the projected gradient descent method in KPG d adversarial training we take a mini batch and we have to produce a dresser examples for this mini batch so what we do is we forward pass this mini batch all the way to the classification layer and we backward pass that to compute the gradients with respect to the mini batch and that basically gives us one update on the adversary examples and we do this K times after K times we have a mini batch of adversarial examples which is here and what we do is we basically forward and backward pass this one last time and this time we do update the network weights and after this we go on to the next mini batch again we do K forward and backward passes without updating the network parameters to generate that versus aerial perturbations and then we train on these adversarial examples finally after these case steps so basically for every minimization update now we have an additional case forward and backward steps which we are taking without updating the network parameters this is why adversarial training has this K factor overhead where the K is the number of iterations used for generating the adversarial examples well adversarial training small data sets and small networks is feasible the question is whether it is practical to adversary train a large data set such as emission it note that to naturally sure in a resonant 50 we need four GPUs and two days so if you want to adversary train the same resonance 50 on image net data using a seven step PD attack now we need two weeks whether adversarial training is impractical for image net depends on how many GPUs one research lab hats so adversarial on imagenet was done before using hundreds of GPUs and even TP use we wanted to make a torso training doable for labs with small or limited number of computer resources so that's why we came up with add virtual training for free in adverse spell training for free we remove the additional K factor overhead from traditional adversarial training so that our training time is the same as natural training to do so every time that we do a backward pass on the network we use the gradients to update the network parameters so unlike adversarial training where we have this additional k4 and backward passes where we don't update the network parameters using them here we update the network parameters every time so we have this global perturbation which is initialized at zero and we add that to the first mini batch so this is the first step of training and we forward pass and then compute the gradients all the way to the input and we use all the gradients of the weights to update the weights and we use the gradient with respect to the input to update this global perturbation and then in the next minimization iteration we train on the same mini batch and we add this perturbation to the same mini batch and now again we do a forward and backward pass and we use the gradients of the weights to update the weights and we use the gradient of the perturbation to update the perturbation and we're going to replay the same mini batch 4m times the reason we do so is that if we move on to the next movie batch the gradient is computed for the previous mini batch or no longer that I created for this current mini batch after the M replay steps are over we basically add this perturbation computed on the last replace step of the previous mini batch to the next me batch note that this perturbation is acting as somewhat of a random perturbation and again using this next mini batch we do M replays and M for them backward passes and every time we simultaneously update the perturbation and also the network points so adversarial training for free is benefiting from the fact that we're updating both the perturbation under Network parameters in one pass simultaneously and it only has one hyper parameter this replay and this replay is somehow simulating the K steps of PGD adversarial training when we look at our results on the cypher 10 benchmark we can see that compared to the 7 step PD train model which is this last row we can train a lot faster and we have robustness which is comparable to that so every column here is referring to a different type of attack and every row here except for our last row is referring to one of our models with a different replay parameter note that for the replay plummer 8 we have more robustness compared to the adversary train model even though our training time is a lot less and it is comparable to a natural train model as Michelle training for free also has additional benefits which add virtual training hat for example our free method still maintains the interpretable gradients characteristics of robust models so for example if we look at this cat and we try to basically make this cat into something else for a robust model if we allow the perturbation to be large here we're allowing the perturbation to be 50 out of 255 then this cat would turn into this horse which is not true if we had a naturally trained model also our models have smooth and flat and lost surfaces if we look at the laws respect to the input we can see that compared to a natural trained model the robust models such as free our free end models have very smooth lost surfaces we use our outer sole training for free algorithm to train different models on imagenet similar to the case for C 410 we see that we can basically train robust models using different replay parameters and as we increase the replay parameter and our accuracy on natural examples drops but up to some point our robustness increases the good thing about that virtual training for free is that since it's a fast method for training robust models we can test a lot of hypotheses for example we can see that similar to the case which is well-known as we increase the networks capacity our robustness increases our original training for free code is available in both pi torch and tensorflow and can be found on our github repos thank you very much and we hope you liked this work"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=0s1LF5qL2Do",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/785ca71d2c85e3f3774baaf438c5c6eb-Paper.pdf",
        "summary": "this video presents the paper correlation clustering with local objectives which describes improved approximation algorithms for several variants of the correlation clustering problem in machine learning correlation clustering is a general framework for doing cluster analysis given some objects and information about which pairs are similar it is similar we would like to group the objects into clusters so that similar objects are together and the similar ones are apart this problem is distinct from other problems such as k-means clustering because we don't make any assumptions about how many clusters there is that we would like to all come to discover this information by itself more formally we are given a graph G with n vertices and edges labeled according to whether their end points are similar or not suppose we cluster the vertices in some way an edge disagrees with the clustering if it is a similar edge across two clusters or the similar edge inside one the classical correlation clustering problem asks us to find a clustering which minimizes the total number of disagreements unfortunately this problem is np-hard so we probably can't find the best clustering to get around this difficulty we can design approximation algorithms which return a clustering with the cost within some small factor of the optimal cost there has been a significant amount of research on finding such algorithms in some applications however total disagreement might not be the best objective to think about suppose that the vertices represent products in Amazon and we use clustering to recommend similar products buyers if most of the disagreements are instant to a few courtesies then a few products will solve very poorly compared to the others we would like to make sure that the clustering is fair and this says that there are only a few disagreements at each vertex these considerations let the further research more local objectives such as minimizing the maximum disagreement on any single vertex although this minimax objective is more fair it completely ignores how much disagreement there is over all we would like an objective function which looks at both the global and local pictures to this end we define the disagreement vector which lists how many edges disagree with our clustering at each vertex then we can think about minimizing the norm of this vector in L Q space a classical correlation clustering problem minimizes the l1 norm while the minimax version minimizes the elephant any norm by looking at norms in between such as the l2 norm we find clusters that give a more balanced perspective on our data in our paper we give new results for correlation clustering in the lq norm for any queue on complete graphs and bipartite graphs we achieve a 5 approximation which improves over the previous 7 approximation we also get the first approximation algorithms for correlation clustering on arbitrary graphs in general lq norms in particular in the l2 norm we get a fourth root of n approximation finally there is another version of this problem which looks the maximum total disagreement inside a single cluster instead of over the entire graph for this objective we find a two approximation which improves over their previous log of n approximation for a detail description of our algorithms please check out our paper thank you for watching"
    },
    {
        "sourceUrl": "https://youtu.be/q1ETeX3HvPY",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/61f3a6dbc9120ea78ef75544826c814e-Paper.pdf",
        "summary": "hi everybody my name is J and this is the video presentation for nips 20 19 on how to solve interpretable kernel dimension reduction o source codes are available on github the purpose of dimension reduction is to reduce the number of features by keeping only the most important information this reduces the size of the data and make it easier to handle principal component analysis or PCA is currently by far the most popular dimension reduction technique given some data X PCA finds the W so that W X retains the most important information and most importantly it is actually interpreted well as you can see that we can see how the new features relate directly to the previous one for example if here we have a W we can see and interpret how the new features related to the old features unfortunately PCA only captures linear relationship so in this example when the relationship are actually not linear if you perform PCA it has tendency of grouping everything together so that you can't tell the blue and the green apart anymore ideally we will want something like this where the blue and the green are separated in the original image we want them to also be separated now this requires us to also capture the nonlinear relationship from the kernel community we know that if you first project the data to some higher dimensional feature space then if we do PCA after that we can now capture nonlinear relationship this technique is called kernel pca or just k pca now kpc is very powerful but you can't really use the labels help you guide the dimensional reduction and to since k pca is just PCA in the feature space it's not obvious what they mean for example here is the Gaussian kernel feature map as you look at this it's not too obvious what running PCA on these features mean therefore interpret about Colonel dimension reduction attempts to solve both both problems to perform dimension reduction K PCA first projects the data to the feature space before projecting onto a subspace W this makes the projection difficult to interpret alternatively i ka our first project the data onto a subspace W before the feature space since this is a linear projection that captures nonlinear relationship the result becomes interpretable commonly we use asic to achieve this Itzik maximizes the dependence between two variables in the feature space so here H SEC measures the dependence between X W and the label Y by finding the W that maximizes the dependency in the feature space i KD R can be accomplished although this approach make the solution interpretable it is actually a very difficult problem to solve if we just look at a generic ikd a problem gamma here is a symmetric positive semi definite matrix and ka here is the kernel matrix the kernel matrix here significantly complicates the objective here if we use the Gaussian kernel we can quickly see why this objective is so complex graphically even in low dimensions this problem is a highly non convex manifold we are trying to find the optimal point on this manifold that simultaneously intersect with this sphere which is a hyper sphere there are currently many existing ways to solve this problem however there are have performance problems too slow to difficult to implement or Annette stuck in the saddle point alternatively we propose is m to solve this problem it actually solves many of the problems with other approaches for is M we discovered that for a family of kernels each kernel has an she ate scaled covariance matrix Phi and just like PCA the solution W is the most dominant eigen vector of Phi here are some examples of kernels that's in the family we simply calculate Phi and set its most dominant eigen vector as the solution however as you may have noticed for certain kernels the Phi itself is the function of W for those cases we approximate the kernel via Taylor expansion to remove the W dependency let me show you what that looks like here we see an approximative version of Phi notice that none of them are dependent on W therefore by initially approximate Phi we can then initialize a w 0 and use this W to compute the next Phi we can repeat this process until W converges and this is the is M algorithm it's pretty simple although the is M algorithm itself looks simple the analysis required to guarantee its effectiveness was not here are all the theoretical guarantees that explains why this algorithm work to get into the detail will take too long but you can look up the proofs there in the appendices section besides the rigorous theoretical analysis yes this does work in practice and he has impact across many different domains here it is useful supervised classification problem notice that I am consistently achieves high accuracy on a 10-fold cross-validation 99% 100% 97 95 but what is most impressive is the speed difference notice that why he took other approaches almost two days I am was able to finish on the one second this enormous speed difference is repeated with other kernels as well here with a polynomial kernel below I kdr is also used for clustering again if you pause the video you will see a significant execution time improvement while getting better results besides supervised and unsupervised problems ikd I can also be used for alternative clustering here in the top image if we ask the algorithm to separate the pixels into two groups as black and white colors it will first yield this pattern however if you ask the algorithm to use a different perspective to separate the pixels the algorithm would then generate this picture notice how they both separate the pixels into black and white groups but the patterns they generate are completely different alternatively if we have pictures of people's faces and we ask the algorithm to group them the first clustering will put the same person into the same group that makes sense again if we ask them to regroup with a different perspective the algorithm would then regroup the picture based on the pose the direction which the person is facing if you have any more questions regarding to this work please come visit us on Europe's 2019 I'll post the number is four three four zero my advisor is Jennifer D and here's a picture of me thank you for watching this video and I will see you at the conference"
    },
    {
        "sourceUrl": "https://youtu.be/-vhUWSHOqIM",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf",
        "summary": "this is a video summary of our paper adversarial examples aren't bugs they're features to appear at nerps 2019 the focus of our paper is on adversarial perturbations specifically we'd like to make progress towards answering why do these perturbations even exist one way that adversarial examples clear eyes this of our classifiers are unreasonably sensitive to useless patterns in the input pictorially suppose we think of our inputs as a collection of features here we have some useful features that the classifier is supposed to learn to do well on the classification task we also have some useless features that through some error and learning such as overfitting or sensitivity to label noise the classifier puts an unreasonably large weight on an adversary can change the decision of the classifier on any given input by just changing these sensitive features that don't make any sense to humans so with this model in mind let's try an experiment we'll start with the training set of a standard image classification task we'll then take a pre trained classifier for this data set and make an adversarial example for every training image after we make these adversarial examples we'll label every image with its adversarial class note that at this point the resulting data set consists entirely of adversarial examples and thus looks completely mislabeled to a human finally we'll train the classifier on this relabeled data set and test on the original test set surprisingly we get non-trivial accuracy on the unmodified test set this result indicates a flaw in our conceptual model of adversarial examples under that model crafting adversarial examples doesn't really change anything meaningful about the input and so learning anything about the true class boundary from adversarial examples would be impossible our experiment prompts us to think about adversarial examples in a different way resulting in what we'll call the robust features model the new trainings that we made in our experiment clearly had to have carried some information about how to distinguish dogs from cats on the other hand this information isn't something that humans can perceive since to us the data set just looked mislabeled this leads us to predict the existence of non robust features these are features that are actually indicative of the true label but can be easily manipulated by an adversary this conceptual model is actually pretty predictive using a pre trained robust model we tried removing non robust features from standard data sets and managed to construct training sets where standard non robust training yields robust classifiers for the original test sets in general her finding suggests that adversarial examples can arise from non robust features in the data that actually helped generalization but hurt robustness note that because these features help generalization simply learning better models may not be sufficient for fixing the problem of adversarial examples thanks for watching our paper summary you can find links here and in the description for our paper blog post and the robustness library our open source framework for module early training and manipulating standard in the robust classifiers you can also find a link to the summary video for our other NURBS 2019 paper showing the applications of robust classifiers to image synthesis tasks"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Xb5sM0NRy_0",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf",
        "summary": "this is a brief overview of a paper to appear at Europe's 2019 equilibrium propagation or egg drop is a very cleans by the alternative to back prop for computing error gradients it is similar to the contrast if a beam learning algorithm used to Train Boltzmann machines however it's continuous time formulation in terms very long simulation times our work proposes the discrete inversion effect prop that in specific conditions is equivalent to backpack through time and enables to Train convolutional architectures first and foremost what is egg prop like prop is used to Train islands which are fed ways that you can put eggs and minimize the energy to steady state est\u00e3o in this context training consists in making estar to coincide the best with a given target way to Train such a network we would generally perform a first full-time phase then perform by propagation through time but egg prop proceeds differently instead of propagating the arrow backward for the first phase the error is included as an elastic force that matches the system to a second steady state s theta star during a second phase the learning will consequently reads like the difference between the two equilibria we call this setting the energy base sitting to compute error gradients mac prep through time goes backward in time while egg prop goes forward in time and the effort looks very different still they are intimately related our theorem is the following provided that the first phase is converged the temporal updates of the system the second phase of egg-drop are equal to the gradients provided by back birth through time each temporal updated egg drop on the right is matched in the same color with the corresponding gradient computed by backward through time on the Left more formally we can define the gradients of Bagdad through time and the temporal update effect prop to rewrite the theorem we can numerically check this property when the system perfectly fulfills the conditions of the theorem now that all theorem all in a broader setting where the dynamics can be simply defined in terms of a primitive function Phi now what is this useful for interestingly this property can also be useful when the system does not exactly meet the requirements of our theorem this is the case for the full connected and convolutional architectures we have studied which do not exactly have a primitive function it turns out that even in this approximate setting the prediction of the theorem is still very well observed this encourages training experiences as anticipated or training results are the same that those we obtain with back prep through time our simplified equations accelerate simulations by a factor five to eight finally or convolutional architecture achieve the best performance ever reported with egg-drop on a list our work sheds new light on egg-drop and enables to train new networks with simplified equations still it has yet to be scaled to deeper architectures this study is one more step towards energy efficient implementations of backrub out of device physics see your paper for more details"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=xTAPZbQlq3A",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/bb836c01cdc9120a9c984c525e4b1a4a-Paper.pdf",
        "summary": "In this video, we describe an analytic unification of two actions frequently used in graph reduction: the deletion of edges, often used in graph sparsification algorithms, and the contraction of edges (that is, the merging of two adjacent nodes), which is often used in graph coarsening algorithms. Prior to this work, sparsification and coarsening were treated as separate algorithmic primitives, with different objective functions. What you are seeing now  is our graph reduction algorithm in action, which uses this analytic unification to simultaneously sparsify and coarsen a graph. The actions of edge deletion and edge contraction are, in fact, dual operations. One manifestation of this duality is seen by considering a planar graph, shown here in blue. The planar dual of this graph, shown here in red, is created by first placing its red vertices in the regions formed by the planar embedding of the original graph. Then, one adds a red edge between each of these red vertices that share a blue border. Notice that the planar dual always has the same number of edges as the original graph, such that each edge in one graph crosses exactly one edge in the other graph. Now, if one of the blue edges is deleted, its corresponding red edge is contracted, merging the two red nodes into one. Likewise, if a blue edge were to be contracted, it would amount to deleting its corresponding red edge. This duality generalizes to non planar graphs by considering its associated graphic matroid and its dual. Algebraically, this duality is reflected in the graph Laplacian, a matrix operator related to diffusion throughout a graph, and whose spectrum reveals information about the graph's global structure. Indeed, many sparsification and coarsening algorithms aim to preserve properties associated with the graph Laplacian. Consider the heat equation,  a prototypical diffusive process. With respect to the dynamics of this differential equation, edge deletion corresponds to the limit of zero edge weight, while edge contraction corresponds to  the limit of infinite edge weight. However, this edge contraction limit requires some entries in the graph Laplacian to become infinite, making analytic treatment difficult. Our primary insight came from the fact that the  inverse of the graph Laplacian remains finite in both of these limits. And in fact, many relevant problems on graphs involve solving the equation Lx = b for x, so the solution is given by applying the inverse to b. Moreover, while the lowest eigenvalues of the Laplacian are associated with the graph's global structure, by inverting the spectrum, these correspond to the highest eigenvalues of the inverse Laplacian. Motivated by these considerations,  we developed a probabilistic graph reduction algorithm that preserves the inverse Laplacian in expectation and aims to minimize the expected squared error for a given amount of reduction. Thus, by preserving the inverse Laplacian, our algorithm is able to perform both edge deletion and edge contraction while preferentially retaining the large-scale structure of a graph. If this video piqued your interest please stop by our poster Wednesday from 5 to 7 p.m. in East Exhibition Hall B+C. Thanks for watching!"
    },
    {
        "sourceUrl": "https://youtu.be/IcC-O1AALXE",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/8cbe9ce23f42628c98f80fa0fac8b19a-Paper.pdf",
        "summary": "this video gives a quick overview for paper presented at the Norris conference 2019 planning under uncertainty in mdps can be captured using a dynamic Bayesian network to chose the evolution of states over time from left to right conditioned on actions the goal is to find an assignment to action the orange nodes that maximizes the expected cumulative reward represented by the our node on the bottom right here it is assumed that the agent knows the state when choosing the next action in partially observable MDPs the agent does not know the state but instead has partial observations revealing some information as shown at the bottom in both images we emphasize that state's actions and observations are described by sets of variables so they all have exponentially many possible values planning in such factored BOM dps is computationally hard our main contribution is a new algorithm snap the tackles such problems it is an approximate solver suboptimal even when it has enough time but very effective on large problems where other algorithms are too slow snap uses the idea of aggregate simulation which was developed for MDP planning with a sub buffa system so gabbatha takes as input and symbolic representation of the domain model as in this example taken from a recent planning competition or the toy problem on the top left of this slide so Bopha translates the model to a computation graph is shown on the right where action variables the orange nodes are symbolic inputs and one node represents an approximation of the cumulative reward so buffer then optimizes action variables using automatic differentiation the forward computation was shown to be equivalent to belief propagation with no downstream evidence which means information only flows forward in the Cibola graph and no backward messages are used in bTW this fact is crucial for the Cibola construction unfortunately this breaks down for pom DPS because observation nodes shown at the bottom are downstream from actions and they serve as evidence our solution is to reorder the computations by enumerate in possible values for observations and conditioning on these values this works when there are only a few values in this example we have only two values and we generate two tracks in the graph the graph computes probabilities belief States and pom DP values the last portion computes the value of a belief state by using the Cibola construction on the nodes representing the belief stage in the graph when there are many observation values we sample a few the challenge is that we must generate a value at construction time which is sampled conditioned on the action which is only known during optimization time sampling networks achieve this and generate both the value and the node in the graph that calculates its probability to be used in other portions and this is embedded in the large construction optimization of action proceeds using automatic differentiation and gradient search experiments in the paper show that snap has excellent performance on the range of challenge problems with large state action and observation spaces please see the paper for more details on the algorithm and experiments"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=o3GfnEjTdIQ",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf",
        "summary": "you may have come across these two papers that were published a few years ago highlighting a particular aspect of deep learning that we had all been taking for granted until then why do deep networks with so many parameters that are made to fit the training data to zero error learn a nice function that generalizes so well to unseen data conventional wisdom suggests that such complex models should learn a bad function that simply memorizes the labels on the training data this is a question that has caught the attention of both theoreticians and practitioners alike and has since become a pretty active area of research so what do these papers say mathematically speaking conventional bounds of the generalization gap like the VC dimension cannot explain this generalization puzzle as these bounds estimate the representational complexity of the network by its parameter count hence yielding vacuous generalization paths to this end these two papers proposed that we should derive more refined bounds by taking into account the fact that SGD implicitly controls the representational capacity of the network and this suggestion resulted in an exciting line of work that found new ways of deriving generalization bounds in deep learning using many different learning theoretic tools while these tools may look pretty different externally in essence they are all the same learning theoretic tool called uniform convergence unfortunately all these existing uniform convergence bounds are still either parameter account abandoned or requires some kinds of explicit modification or regularization to the network learn bias Chile in addition to these issues in our paper we bring to light some more concerning problems troubling these paths first is our finding that even though the true generalization gap decreases with training set size as expected these bounds in contrast increase with the training set size hence parameter count dependence is not the only problem plaguing these bounds next and more importantly we present some example binary classification tasks tasks and deep learning where we show that even though as Chile generalizes well it learns a decision boundary that is complex in a certain way that all uniform convergence bounds are vacuous in these settings that is uniform convergence provably fails to explain generalization in these cases through these two findings we call into question the current approach of using uniform convergence to understand generalization and deep learning perhaps it's time to look for a new tool"
    },
    {
        "sourceUrl": "https://youtu.be/2EnckznIV2o",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/195f15384c2a79cedf293e4a847ce85c-Paper.pdf",
        "summary": "this is a video for the paper called hindsight credit assignment the question that credit assignment asks is how did my past actions influence the future outcomes the way that the reinforcement learning agent usually goes about this question is by relying on the MDP structure of the world and in particular by taking time as being the main proxy for credit relevance for example here we have credit assignment in RL or Carl who looks at the weather forecast in the morning and has to decide whether or not to take an umbrella to work Carl decides not to take the umbrella goes to work and it ends up raining now Carl has to figure out what it was in his day that made this happen because Carl is an aural algorithm he goes through his day in the backward chronological order until eventually after many trials and iterations he figures out that in fact it was the umbrella now what we would like to happen instead is for the credit to be moved directly from the rain to the umbrella so the key intuition that we proposed in his paper is to instead of relying on the standard MPP assumptions to explicitly learn the credit relevance structure in particular we consider the probability of a past action given the state that he was taken and some form of the future outcome which has measured as a function of the future trajectory that starts from that state and action in particular in this paper we take this function to be a future state or a future return and it turns out that this is not just a heuristic and that we can actually rewrite our usual value functions in these terms in particular now in the usual discounted sum we can additionally wait each reward by this probability ratio of the probability of the action whose value were estimating that is conditioned on reaching the state that is giving us the reward divided by the probability of just taking that action and this ratio explicitly how relevant was the past action in achieving the state and consequently the reward so in the family of HCA algorithms were proposed to learn an estimate for this probability P and then use it to sample this expectation and follow the policy gradients accordingly we devised a set of diagnostic tasks that illustrate the issues of when standard credit assignment in RL and show that HCA is able to tackle them for the complete details please read the paper"
    },
    {
        "sourceUrl": "https://youtu.be/UxXdAdkyAF0",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/d202ed5bcfa858c15a9f383c3e386ab2-Paper.pdf",
        "summary": "hi I'm Dave Raj and I'll be talking about our work on communication efficient distributed learning the goal is to perform empirical risk minimization using data which is distributed over worker nodes which communicate over bandwidth limited networks traditionally this is solved iteratively by communicating stochastic gradients from workers to a parameter server which returns the aggregated gradient each volt precision exchange uses 32 bits per dimension which could amount to gigabytes of data being transmitted in training language models as an example therefore to mitigate this cost techniques such as quantization specification of gradients and increased local computations have been previously proposed we first proposed a novel class of cue sparse operators which combines either general stochastic quantizers or deterministic sign based quantizers with either top K or random K specification together with our compensation we have a distributed algorithm called Q sparse SGD which mitigates the communication bottleneck to some extent furthermore we propose an algorithm called Q sparse local SGD which combines the benefits of RQ sparse operators with increased local iterations while incurring little penalty in convergence rates here the workers perform local iterations independently until the next synchronization step in which this parts of I and quantize the sum of the net local updates since the last synchronization together with the historical error this is sent to the parameter server which returns the aggregated updates we also analyze an asynchronous operation in which each worker has its own synchronization schedule therefore only a subset of workers chooses to communicate with the parameter server in each iteration the workers compensate for the compression error in future iterations by storing it in local memory as a result of error compensation we recover convergence at rates matching vanilla SGD for both our synchronous and asynchronous operations despite biased updates resulting from quantization specification together with in frequent communication we also characterize the asymptotic limits of local iterations as well as the minimum iterations for converging rates matching vanilla SGD both in non convex and strongly convex settings recovering previously known bounds for local SGD when the compression coefficient gamma is 1 as well as for centralized stop k SGD when the synchronization period H is equal to 1 our robot implementation of Q sparse local HDD was used to train resonate 50 on imagenet and they clearly see the gains on combining the three techniques by a factor of over 15 to 20 times as compared to quantization specification and local HDD being individually used without incurring significant penalty in convergence rates we also validate a scheme in a convex setting by training a soft max classifier with l2 regularization on the MS data set in which we also observe the superiority of q sparse local HDD over the other baselines such as top k HDD sign SGD local HDD all of which are also in fact specializations of our algorithm"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=ezbC3_VZeNY",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf",
        "summary": "Hello there, welcome to our video presentation on adversarial mixup resynthesis  This was recently accepted into NeurIPS 2019 as a conference poster  I hope you'll enjoy it Okay, to start off, adversarial mixup resynthesis, which we'll just call 'AMR' -- it's the abbreviation -- is basically a technique to improve representations learned by auto-encoders  Autoencoders being one of the most fundamental building blocks that we have in unsupervised learning So just to give an example: in an autoencoder we have an input x, it could be an image, it could be text whatever, and then we're going to run it through an encoding function 'f' to produce a representation here Now commonly this representation is going to be much smaller in dimensionality than x because here we want to capture the most salient features, the most important features to basically reconstruct x  And we reconstructed it by having a decoder function 'g' which puts it back into the original space so 'x tilde' here denotes the reconstruction of x and so we're basically training this  We're going to update its parameters so as to minimise the reconstruction loss which is the L2 norm between x and 'x tilde' which in this case would just be x minus the decoder of the encoder of x like so So in deep learning in general -- not just an unsupervised learning -- data augmentation is actually an extremely useful thing to do and in the case of images it's really easy to do right  So the reason why we're doing this is you can think about it as like you're trying to artificially augment your training set which is usually quite small in the grand scheme of things and so for instance we have a bunch of images here, these five images and we can perform various transformations like here maybe we can randomly rotate the image, we can blur it a bit, we can play with the contrast and vice versa and so and for instance if you were training and classifier to classify these images then you'd be making the classifier a lot more robust It would see this image in different lighting conditions or different angles so just in general it's a really good idea to do and it's really effective Another example is actually this thing called a denoising auto-encoder so here we're dealing with MNIST digits and we can see this two has been corrupted with noise and so the idea is that if it's being corrupted with this sort of noise we can run it through the encoder and it's actually going to produce the original image and so what we're doing is that we're training this auto-encoder to learn to separate signal from noise and so the objective that we'd be minimizing here would actually be this L2 norm between 'x' and 'g' of 'f' of 'x' plus epsilon where epsilon is this noise  So for instance this could be sampled from a Gaussian and so each pixel would have an epsilon sampled from it  So this is called the denoising auto-encoder and it's much more robust than the one that we explained in the previous slide  So here we're going to talk about another data augmentation algorithm called 'mixup' which is proposed by Zhang et al in 2017 Now our technique actually uses a form of this but just for the sake of this slide we're going to explain the original algorithm in the context of supervised learning  And so mixup is actually really simple, it's only a few lines of code really and basically what we're going to do is that we're going to construct random convex combinations between random pairs of examples and their labels in the data set  So for instance imagine we are looping through let's just say D1 and D2 where these are the same training set but they have different orderings, they're shuffled differently  We could say you know for (x_i, y_i) and and (x_j, y_j) in D1, D2 we're going to construct this x_mix to be alpha times x_i plus one minus alpha times x_j where alpha let's say is going to be sampled from say -- sorry -- a uniform distribution and y_mix it's the same thing it's going to be alpha times y_i plus 1 minus alpha times y_j and now we have this new pseudo example here (x_mix, y_mix) and we're going to train our classifier on that  So what does this do? Well, oops sorry -- the text is above the diagram -- but here's an example here so if we look at this diagram we had these two classes, we have orange as one class and green as the other class and blue, this blue shading here is the probability that y equals one so we can see that you know it's not so smooth at so it's a pretty sharp decision boundary but when we do mixup we get something that's a lot smoother in this region here  So here's an example here we're using some some random images from CIFAR10 and so if we just look at the top row for now  We have this this truck x1 and we have this deer x2 and as we're moving from from left to right where we're traversing that convex combination so for instance you know this second image here would be something like you know let's just say 0.9 of x_1 and 0.1 of x_2 and this would be the opposite, this would be like 0.1 of x1 and 0.9 of x_2 and in the middle this would be about 50/50  So we can see here what we can see here is this assumption that we're making that when we interpolate between these images in pixel space that were also going to interpolate their probabilities right so this would have a class but let's just say there are only two classes right this would be [1, 0] the one-hot vector and this would be [0, 1] and what we're saying is that this class here would would be [0.5, 0.5] so the classifier is going to say well you know I'm half confident that it's a truck and I'm half confident that it's a deer but there are some problems The main one is that these interpolations they don't really look like they're representative of the training set so you could easily have a case of underfitting But they're also not really semantically meaningful If we were interpolating between two cars for example we wouldn't expect to see some you know this novel combination that looks realistic, it would just be these cars that have been halfway faded so it's not 'semantic' mixing it's just mixing in pixel space But to address this issue recently we proposed a paper called 'manifold mixup' this is actually from Verma et al which was recently accepted in ICML 2019 and this basically does the mixing in the hidden space of a classifier so now you have these semantic mixes  AMR does something similar but in the case of unsupervised learning and so we'll get to that really So our technique -- which we'll get through quite shortly -- can be motivated in part at least by a statistical way of looking at the problem and so we have these faces here you know we can imagine there being this complicated function p(x) and so you know these x's are samples from p(x) and in unsupervised learning what we're really interested in is this inference of function p(z|x) right you know what are the latent variables which explain or generated this data x  So in the cases of faces let's just say we might have a lot of these latent variables z_1, z_2, to z_m, there may be a lot of these and they all you know come together and basically influence or generate x so for instance this could be something like you know hair colour, skin colour, age, eye colour and vice versa Now let's just say for example that your m was 32 well you know if these are all binary variables Well you know 2 to the power of m you'd be 2 to the power 32 which would be 4.2 billion which is a very large number, a very large configuration space of these variables and most likely we're not going to have 4.2 billion images and even so there's going to be a lot of redundancy so we can see that you there are a lot of possible configurations of these latent variables to generate this x  So just as an example, suppose we're training an auto-encoder and let's just say this lady here is x_1 and this guy is x_2 and we're gonna encode both of these images with the encoder function and you can imagine the auto-encoder having extracted -- having learned -- some of these these latent factors and let's just say for example it just so happens speed at and our training set we've got no guys with red hair but we have woman with red hair Well depending on how we combine these latents -- how we mix these latent variables we might end up getting the same guy with orange hair so we can produce configurations of latent variables as a form of data augmentation configurations of latent variables which may not be initially present in our training set and really that's the whole idea behind AMR Okay just to explain that again just in a better slide  We have these two images here x_1 and x_2 and we're training an autoencoder and so you know we're going to encode using this encoding function some representation here h_1 and we also have x_2 which we're going to encode into some latent representation h_2  Now because this is an autoencoder right we're going to -- you know -- still do the reconstruction so this would be x_1 tilde and we're minimising L2 norm here and same thing for x_2 and its reconstruction here right and we have this norm Okay so the mixing happens at h_1 h_2 right so we're going to feed both of these through some mixing function and it's going to produce some mix between h1 and h2 and it's going to output some h_mix and then we're going to decode this mix with 'g' and get some x_mix  Now if you do this with say a regular auto-encoder it's not guaranteed that this mix is going to look realistic it could be something that's completely gibberish But what we're doing in order for this algorithm to work we're gonna use generative adversarial networks  I won't go into it you know too much but basically what we do is -- just selecting a different colour -- we have this discriminator function here and basically it's going to This discriminator is trained to say that this is fake and that you know these two images here are real and the auto- encoder is going to try to fool this discriminator so basically it's going to try to it's going to update as parameters so as to producing mixes that could plausibly look like they come from the data distribution  Okay so what is this mixing function?  Well the first one that comes to mind would just be mixup I'm just putting it in quotes to say this is this is the original mixup you know using the original paper and used in manifold mixup and also in some related work  So let's just say here this is you know this is h_1 so this is basically an encoding of x_1, it's a bunch of feature maps, so we have four feature maps and we also have h_2 which is the encoding of x_2, also a bunch of feature maps  So what we can say is that well we could sample some scalar alpha from say some uniform distribution and just literally multiply, produce this convex combination so alpha times h_1 plus one minus alpha times h_2 and then we get some some combination here h_mix which we've then decode into some x_mix So another one actually we propose that specific to our work is this thing called 'Bernoulli mixup' in which we do discrete mixes so to explain this again we have h_1 and h_2 and just to present it a bit better I'm just flattening these out, these feature maps So what we're doing is that we're actually going to sample from a Bernoulli distribution So for each index -- because there are four feature maps -- we're gonna sample some let's just say some 'm' from some Bernoulli distribution parameterised by 'p' and this could be 0.5 for example and so maybe for the first one you know it samples 0 and then maybe for this one when we sample that m we're going to get 0 and maybe for this one we get 1 and maybe for this one we get 1  So what that means is that because these are 0 we're gonna retain these feature maps and because these are 1 and we're going to retain these feature maps and then we're gonna do this discrete swapping so we have the first two feature maps of this guy and the last two feature maps of this guy and again we're going to basically decode that into an x_mix, so that's discrete mixing In the previous two examples we were just mixing up two examples at a time but you know one question you might ask us we know can we mix between multiple examples, can we mix between three examples, four examples and vice-versa? And absolutely  So if we just go back to the case let's say we're k=2 to where we were mixing with two examples you know we have these two mixing coefficients alpha_1 and alpha_2 but really we only need one alpha right because alpha_1 is just alpha and alpha_2 is just 1 minus alpha but we can visualize this as like line segments right so here would be alpha_1 and you know this would be [1, 0] and here's alpha 2 with [0, 1] and you know we can sort of draw a line segment between them and so this halfway point would be you know [0.5, 0.5] And so we can imagine you know drawing these alphas from this line segment Now in the case of k=3 now you know we have alpha_1, alpha_2, alpha_3 and yeah we have this constraint that you know these alphas have to sum to 1 and again you know we can sort of draw let's say the corners of this vector, so this one here would be use [1, 0, 0] so alpha_1 equals 1, alpha_2 equals 0, alpha_3 equals 0 This is another corner [0, 1, 0] and this is another corner [0, 0, 1] right and so this actually forms a triangle and so this midpoint here would be [0.33, 0.33, 0.33] and so you know we're drawing these alphas from this triangle So by the way this is called a '1-simplex' and this is called a '2-simplex' right and you can generalize this through more dimensions so in k=4 you know it would be something like a tetrahedron Just to give some animations to further illustrate the point in this case we have mix-up with k=2 and so you can imagine this being like a three dimensional latent space and red these are the real points latent states for real examples and these blue dots are the interpolations and so we can see that you know we're taking random pairs of these real points and interpolating along their lines This is mixup with k=3 Same thing now but we're actually taking random triplets so here's a use of these three real examples and we're interpolating between them, interpolating in their triangle Here's Bernoulli mixup for k=2 This is a bit less intuitive visually but you can also see the type of interpolation that's imposed by doing this Bernoulli mixing It's also useful to talk about some related work There are actually two papers that are very similar in nature Adversarially constrained autoencoder interpolation which we'll call 'ACAI' which is a recently published in ICLR 2019 and 'generative adversarial interpolative auto-encoding' otherwise known as 'GAIA' So there are some differences between the works  In ACAI what's going on is that they're using mixup in the k=2 case but when the discriminator gets that x_mix the discriminator now is actually going to try to predict the mixing coefficient alpha right so it wants to predict you know to what extent to what extent let's just say h_1 was mixed and what extent h_2 was mixed and so the auto-encoder's objective is to try to make the discriminator think that that the mixing coefficient for x_mix is either 0 or 1 and so the output of this discriminator is tied to the mixing function that's used but in that case they only try mixup with k=2  In GAIA they use a mixing function which is based on a Gaussian so if we have h_1, h_2 we're actually gonna sample an h_mix from a Gaussian distribution which is just the the midpoint of h_1 and h_2 with some variance sigma squared so you know if h_1 was here and h_2 was here we're basically sampling is Gaussian which is at the midpoint so you're more likely to sample interpolations that are really in the middle of these two points but again it's in the k=2 case In particular what we're interested in is just mixing functions in general, just evaluating you know different types of mixing functions like Bernoulli mixup, mixup for k>2 and just really think about what these specific mixing functions are actually doing and what their implications are on the resulting representation So here I'm just showing some qualitative results finally  This is on the Zappos shoe dataset so you know we have one type of shoe x_1 we have another type of shoe here x_2 and this row is basically pixel space interpolation, this one is just a regular auto-encoder -- interpolating in the latent space of a regular auto-encoder sorry -- this is ACAI and this is a AMR so we can see that for these top two rows there's a lot of ghosting and these interpolations it doesn't look very realistic  Same thing for the auto-encoder but for ACAI and AMR things look a lot better especially in this case for our technique, there's not really much ghosting going on and again this is due to the fact that any interpolations produced by the auto-encoder have to look realistic, they have to fool a discriminator so it's going to want to get rid of this ghosting because that ghosting isn't present in the original data set  Also this is just for k=2 with just regular mixup Here's another example of a more high-res version This is actually the first figure in the paper, we have this x_1 here and we have this x_2 here So these are kinda like two completely different shoes, this is a wedge sandal and this is a boot and we're doing these interpolations but what you will notice in some of the figures is that there are sometimes it can seem like the jump -- it's a pretty big jump -- between these interpolations and this seems to be at least in part a consequence of doing this adversarial thing so just to elaborate let's just suppose that we have this space here and you know h_1 lies here and h2 utilize here we're gonna do an interpolation right in the middle and let's just say you know we decode it into some x_mix you know we might find it when we re-encode it -- gonna draw the same diagram again -- it might be sort of let's say more closer to one of the original h's so it's kind of you know analogous to a denoising autoencoder right like these shoes are completely different and maybe it can't seem to  produce something that's really in between the two shoes and so it pushes it in either direction so again the seems to be a natural consequence of just you know doing this adversarial game but in order to -- actually -- if you wanted to mitigate this you could do something which is called a 'consistency loss'  I won't go into it for this video but you can read a bit about it in the appendix of the paper So here it is an example of some interpretations with Bernoulli mixup up and so what we're doing is doing a discrete mixing between feature maps so for example well let's just say this is x_1 -- or its encoding is h1 -- and this guy's encoding is h2  Well let's just say for this one you know we want to keep most of the future maps from this guy and and we want a few feature maps from this guy so we might say I'm gonna sample a vector, a mask vector from Bernoulli and let's just make it maybe 0.9 right so this is actually a vector a binary vector you know m_1 m_2 up to let's just say we have 'p' feature maps and then the interpolation simply going to be m times h_1 plus 1 minus m times h_2 where m is this this binary mask for instance  Now for here for example maybe we want roughly half the feature maps from this guy and roughly half from this guy and so you know we might sample a Bernoulli mask from something like 0.5 and then we'll do the same thing m times h_1 plus 1 minus M times h_2 and vice versa  And this guy here will have most of his feature maps from this guy and a few from this guy So here's another interesting example, this is actually a mixing function which is an MLP that's trained in a supervised manner, so if we look here for example we're just considering there are three attributes male makeup and lipstick and so our guy here Kiefer Sutherland you know his attribute vector is [1, 0, 0] so yeah he's male but he doesn't have makeup and he doesn't have lipstick and this lady here, well her attribute vector is the opposite it's actually [0, 1, 1] so not male, she's female but has makeup and has lipstick and basically yeah we learn this mixing function which is an MLP which basically you condition on say a vector like this, so not male , makeup and lipstick, and it's going to try to figure out what feature maps does it have to take from this guy and this woman in order to produce a mix which satisfies this classification here right and so we do that using an 'ACGAN' -- an auxiliary classifier GAN -- which is just one supervised variant of a GAN and so not only does this this mix here have to look realistic it also has to satisfy this classification here  And so you can see in this row we're showing different versions of Kiefer Sutherland with these different attribute vectors which is pretty neat  So to actually evaluate these representations like how how 'good' they are we're gonna do sign it's similar to what they did in the ACAI paper and we're actually going to train a linear classifier on top of these and coatings So let's just say we you know we're training our AMR auto-encoder and you know we encode some x into some h and you know and there is also the reconstruction, this is the encoder, this is the decoder, we're gonna branch so during training we have a linear classifier p(y|x) which is branching off this encoding here and we're actually going to train it to predict the class from h But what's very important to note is that we're not backpropping the gradients from this back into the auto-encoder right so this is this is cut-off here So you know this auto-encoder is trained completely unsupervised -- it doesn't see the labels -- but this part is trained supervised  And it doesn't have to be a linear classifier per se it could also be a deep neural network but it just so happens to be that linear classifiers are fast to train and it really makes a difference when you're training / running hundreds of experiments and so you know we're going to train this and we're gonna evaluate its accuracy on both the validation and test sets  As well as that each experiment we run we we run it three times so we get two repeats and so for each of these trials we're gonna find basically the epoch corresponding to the point at which the validation performance was highest and then we're gonna evaluate at that point on the test set and so basically we're actually we're gonna get you know three test set numbers and then we're just going to take the mean and variance of that and that's what we report  In terms of hyperparameters there is 'lambda' the reconstruction loss so this also affects how much weighting you give to the GAN loss because essentially the order auto-encoder is trying to minimise let's just say you know the loss here would be your reconstruction plus the scan loss would be lambda times reconstruction plus this GAN loss where this is the loss that tries to fool the discriminator with the mixes and so you know the higher this lambda is relatively speaking the less weighting that the GAN loss gets so you really have to tune this  The mixing function, so we explore just mixup and Bernoulli  Unfortunately due to time and resource constraints we couldn't explore this as rigorously but really we do have a lot of experiments on mixup which is good  And for that there's also the k value so how many examples do we mix between at a time? Do we mix two, three four, six, eight, ..., and and vice versa  Okay so here are our first set of experiments that we ran we actually ran some experiments on MNIST, KMNIST, and SVHN and so this is the test set performance and we're also showing the values of lambda, the best performing lambdas for those experiments, and so if you look row by row the first one we have is auto-encoder plus GAN so this is just an auto-encoder with a GAN on the reconstruction so there's no mixing but the reason why we put this on the reconstruction is so that you get nicer reconstructions -- ones that aren't blurry -- this is a common problem if you just train a regular auto-encoder Now for AMR these are the experiments that we ran so there's mixup with k=2 Bernoulli with k=2 and mixup with k=3  For ACAI we we did our own implementation of ACAI as well and here just in this little this little box here we're showing we're showing the results from the ACAI paper quoting them directly So for MNIST it actually performs -- our implementation of ACAI -- performs the best, same thing for KMNIST but fortunately I guess for SVHN the slightly more complicated dataset we actually achieve a big win here, we've actually 47.34% compared to the baseline which is ~37% Our implementation ACAI which is ~34% and their's which is ~34% and we actually get this with a k=3 which is nice so for our second set of experiments we actually did an ablation so we're just looking at SVHN The encoding size is still 32 but what we're doing is that we're basically reducing the size of the training set so in this case for example we've taken a thousand -- randomly selected a thousand examples in the training set -- and decided to use that as the training set For here we randomly selected 5000 and decide to use that and vice versa, 10,000 and 20,000 And this is really just to see you know what the behaviour of these algorithms are and the sort of 'low data regime'  So if we look at 1k the lowest well okay ACAI here performs best For 5k you know we you know perform the best here but actually you know partly because of this low data regime were operating in this is a pretty high variance whereas ACAI here actually performs only slightly less but with a much lower variance  For 10,000 and we achieve the best result for mixup with k=3 with a reasonable variance And here for 20k we achieve ~37% although the variance here is a bit big as well But things in general do look pretty good for mixup k=3 So here are some more results again on SVHN but now we also have CIFAR1 Here the encoding size is actually 256 So corresponds to 16 feature maps now with basically a 4x4 spatial dimension so that's 256 and this is really just to see what results we get you know when we beef up the size of encoding [but also because ACAI did it] For 1024 we're actually using 64 feature maps of 4x4 so that would be 1024  And so you know here we actually explore what happens when you go beyond k=3 so you know we have k=4, k=6 and k=8 and so if we look at SVHN with the 256 size encoding we actually get a pretty impressive 75.71% for k=8 But actually the quoted ACAI result is 85.14% which is really impressive so I'm not really sure what the reason for that difference is [#reproducibility], it may be the case that we needed to run our experiments for longer, but you know, time and resource constraints! :-( For our CIFAR10 for 256 we achieve the best result here 54.94% with k=3 And for CIFAR10 with the big bottle bottleneck dimension 1024 we get 61.72% with k=4 As you can see it's not always the case that a higher k corresponds to a better accuracy, this is something that needs to be looked at in more detail Maybe there are some theoretical things you can say about the choice of k but either way there will be a brief discussion on this in the appendix if you want to know more about it  Lastly we evaluate our algorithm in the context of disentanglement  There's a data set called 'DSprite' it's basically just this one sprite which can take on various positions, rotations, shapes and it's basically used in the context of measuring disentanglement  So if we look at the ground truth latent factors of this data set you have six, or really it's five -- because the only color you have is white -- but you have these five latent variables: the shape of the sprite, the scale, the orientation, the x position, the y position, and they all take on various ranges of values and so if you take all possible latent configurations you get this dataset here and so this animations is showing you know all the possible configurations and so for instance you know we might want to try to recover these latent factors in an unsupervised manner so what we could do is say well you know if we could train this auto-encoder and maybe this auto- encoder -- well let's just say it's a five dimension -- we want each of these units to encode the latent factors like shape scale, orientation, x-position, y-position, but we also want it to encode these latent factors such that there's 'disentanglement' so now if you're only going to change x for example it shouldn't change anything else or if you're gonna change scale -- if you're gonna vary this dimension -- it shouldn't change anything else when you decode into the image  And so there are two metrics that were proposed recently to measure this this disentanglement and one is the 'Beta-VAE' paper but an improved version of that metric was proposed in 'Factor-VAE' and so we use this in our evaluation  So if we look at our table of results this accuracy is this disentanglement metric and we have a 'Beta-VAE' just a finely-tuned baseline, and we have our auto-encoder and we have AMR and so actually the finely-tuned VAE -- the Beta-VAE -- actually performs the best with a disentanglement accuracy of 68%  For us it turns out that Bernoulli with k=3 actually performed the second best followed by Bernoulli with k=2 which is interesting so maybe there is something interesting about using Bernoulli mixup in the context of this dataset Again this really requires further exploration It would be good to know better what some of the implications are of using say regular mixup versus Bernoulli and also what happens when you play with the value of k So in conclusion we perform an exploration mixup in the context of unsupervised learning and so one of the motivating reasons was this idea that we can imagine the encoding of our auto-encoder trying to learn these latent factors that represent the data and that we can generate novel combinations of these by performing mixing and in making these novel mixes look realistic through an adversarial GAN framework, and we looked at it in the context of the mixing function so we tried different functions like mixup, Bernoulli, supervised Bernoulli --so this is the example where we did attribute swapping for celebrity faces --  and also mixing with k greater than two Qualitatively compared to to our baselines we achieved more realistic interpolations quantitatively we found that a mixup k>2 generally performs really well, it was for the the tasks where we branched a linear classifier off the encoding and try to predict the class Unfortunately due to time and resource constraints we weren't able to explore Bernoulli mixup as rigorously but at the same time we found some evidence that it might be useful in the context of disentanglement, the results that we presented on the DSprite dataset said  For future work be nicer to have a better theoretical understanding of these mixing functions, a better 'theoretical framework' and also maybe looking at mixup from a more biological point of view It is interesting to note that well if we look at say Bernoulli mixup this could be seen as being somewhat analogous to biological crossover in which we're swapping over different segments of DNA and also what's interesting is that recently genetic algorithms have become sort of more widely used in deep reinforcement learning under this this idea called 'evolutionary strategies' as an alternate alternative way to train reinforcement learning algorithms so there might be something interesting to see from this point of view  Well thank you for watching this presentation! If you have any questions or comments or whatever can email me at this address or tweet me I'll put the link to the code and the paper below so you can look at them easily and again thank you very much for watching! FIN"
    },
    {
        "sourceUrl": "https://youtu.be/jyN3ZQNH1GI",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/36e729ec173b94133d8fa552e4029f8b-Paper.pdf",
        "summary": "i'ma go kacy Despres and i will present our paper symmetry based disentangle representation learning requires interaction with environments so we do first a quick recap on symmetry bases integral representation on e SPN DSB DRL then we'll present our contribution and then we'll talk about discussion in future work so motivation is disentangle representation learning where we try to iterate factors of valuation and data but the problem is that design the government needs a proper definition which is not the case at the moment so SB TRL tries to define what this entanglement is through the use of symmetries and group theory in their definition they use an underlying work state W and the use symmetries like translation or rotation and they try to build representation Z such that the effect of transformation on W will be the same as on Z so that's the definition of symmetry based organization and then they define disentitlement as the fact that the symmetries can be decomposed into different subgroups that do not affect each other such that a transformation will affect only serve groups subspaces of the representation in Z so this is the definition of disentangle presentations in SBT RL and in their paper they learn representation using only still observation and we try to make a bunch that SVD area requires transition and that still samples because for example the transformation can have a camera effect in the words that in different ways regarding what our physics in the world so we are able to prove that using 0 & 1 and for practical options we show that there are two options that you can do to learn as B it isn't a good representations so the first one would be Arabic models well while you would learn representation and then you would learn the power the transformation after distance for this presentation and then you can also do your end-to-end learning strategy where you would learn both at the same time we show that both approaches works in practice in a small toy example which is pretty much the same as the use in the killings paper and then we also test if the neural representation can be useful for learning those three tasks which is the case in our example so there are many open questions about SBT RL and one of them is how do you learn the subgroups of transformations and maybe we can associate these with actions that's one of the possibilities to go so in conclusion a formal definition of descent amendment is needed as the GRI tries to you know to find one and then we make the points that improve that trainer as B as B gr n needs transition is not and not still supposed to be learning practice so our current is a variable on : and on kita and we make sure you check out the paper on pocket thank you"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=f0jeOi4pW_A",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/46a558d97954d0692411c861cf78ef79-Paper.pdf",
        "summary": "gradient descent is a great algorithm for optimization but one of its shortcomings is that it can be slower when the function is ill-conditioned the curvature behaves very differently in different directions and the gradient is not 1 towards the minimum of the function one approach to this problem is the second order information to correct a direction an increasingly popular method is the natural gradient of Amaury it adapts to the geometry of the problem using the Fisher information matrix but many implementations of natural gradient methods in machine learning do not compute the Fisher instead they often use the outer product of gradients which has been called the empirical Fisher the twin pieces look similar but some authors have warned against this approximation our work looks at the relationship between those two matrices in more details we showed that even on very simple problems using the empirical Fisher can lead to very bad results the main difference between those two expression is that the Fisher uses the probability distribution of the model to sample outputs but the empirical vivir uses samples from the training data it loses the connection to Fisher information unless the two distributions are equal this is not possible when the parameters have not been fit to the data as is the case during optimization on different problems where gradient descent struggles but natural gradient converges quickly preconditioning with the empirical Fisher does not guarantee the same improvement looking at the angle between the empirical Fisher direction and the natural gradient median said that you can even be opposite the situation improves at the minimum but even there at the empirical Fisher need specific conditions to equal the Fisher the models needs to be correctly specified which is the case on this classification and regression problem but this is often understood just as being a good model correct specification is more subtle and small deviations that are difficult to know in practice lead to very different outcomes our paper provides an overview of the non connection between the Fisher information and other quantities used in second order optimization such as the Hessian and the journalist Goss Newton matrix and we provide a detailed criticism of existing argument strong connections between those quantities and the empirical failure"
    },
    {
        "sourceUrl": "https://youtu.be/2EnckznIV2o",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/195f15384c2a79cedf293e4a847ce85c-Paper.pdf",
        "summary": "this is a video for the paper called hindsight credit assignment the question that credit assignment asks is how did my past actions influence the future outcomes the way that the reinforcement learning agent usually goes about this question is by relying on the MDP structure of the world and in particular by taking time as being the main proxy for credit relevance for example here we have credit assignment in RL or Carl who looks at the weather forecast in the morning and has to decide whether or not to take an umbrella to work Carl decides not to take the umbrella goes to work and it ends up raining now Carl has to figure out what it was in his day that made this happen because Carl is an aural algorithm he goes through his day in the backward chronological order until eventually after many trials and iterations he figures out that in fact it was the umbrella now what we would like to happen instead is for the credit to be moved directly from the rain to the umbrella so the key intuition that we proposed in his paper is to instead of relying on the standard MPP assumptions to explicitly learn the credit relevance structure in particular we consider the probability of a past action given the state that he was taken and some form of the future outcome which has measured as a function of the future trajectory that starts from that state and action in particular in this paper we take this function to be a future state or a future return and it turns out that this is not just a heuristic and that we can actually rewrite our usual value functions in these terms in particular now in the usual discounted sum we can additionally wait each reward by this probability ratio of the probability of the action whose value were estimating that is conditioned on reaching the state that is giving us the reward divided by the probability of just taking that action and this ratio explicitly how relevant was the past action in achieving the state and consequently the reward so in the family of HCA algorithms were proposed to learn an estimate for this probability P and then use it to sample this expectation and follow the policy gradients accordingly we devised a set of diagnostic tasks that illustrate the issues of when standard credit assignment in RL and show that HCA is able to tackle them for the complete details please read the paper"
    },
    {
        "sourceUrl": "https://youtu.be/6TayjC7qqPg",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/f8905bd3df64ace64a68e154ba72f24c-Paper.pdf",
        "summary": "multi-class learning from contradictions we focus on the problem of learning with high dimensional data especially when access to labeled samples is limited most algorithms designed for such problems involve inductive settings one approach specifically designed for high dimensional problems uses large margin classifiers a popular example is the multi-class SVM or M SVM introduced by Cramer and Sigma unfortunately the performance of most such approaches is suboptimal when the label data is limited for high dimensional problems this motivates the need for advanced learning settings once a setting is learning from contradiction or Universal learning where the algorithm can exploit unlabeled samples from the same domain but belong to none of the classes being learned for example consider the task of gender classification from images here the labeled examples are male and female faces while the universal CentOS can be facing ages there are neither male nor female under the universal learning setting we then built a classifier which explains the training samples well and also maximizes the contradiction on universim samples the idea was originally introduced for binary classification problems and has been shown to be very effective in high dimensional settings this paper extends the universe of learning framework to multi class setting as presented in definition - we show that maximum contradiction for MSDN can be achieved when the universal samples like close to the decision boundary of each class we use this insight to formalize multi-class SVM for universal settings called nu SVM where we use large margin laws same as M SVM for training samples and Delta insensitive loss for universal samples we derive several useful properties for the musm formulation and show that mus am is solvable through any state of art M is lim solvers we also analyze the sample complexity for Peck learnability of mus I am using the most widely adopted capacity measure for multi class problems the Natarajan dimension we show that the sample complexity for MU SVM is smaller than M SVM this indicates that musm can achieve better test accuracies for problems with high dimensional limited label samples our empirical results confirm this by demonstrating 20% improvement over em svm the proposed HIV visualization method provides further insights into the performance finally we derive a new computationally efficient span bound for live on out error there also allows for efficient model selection a results show up to four times speed up against five fold CV and over 100 times speed up against naman art model selection in conclusion we formalized Universal learning for multi class learning provided several useful properties for the formulation analyzed a sample complexity for pac learnability and derived an efficient analytic bound there also enables model selection finally we provide exhaustive results in support of our methodology additional details are available in the paper accompanying codes are provided in the video description thank you"
    },
    {
        "sourceUrl": "https://youtu.be/-FduW9ZWAR4",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/1e79596878b2320cac26dd792a6c51c9-Paper.pdf",
        "summary": "hello here is a presentation of the paper likelihood ratios for out of distribution detection this is a trying to work with people in Google ai and deepmind bacteria net vacation is an important problem in medical diagnosis given a DNA sequence we would like to predict which bacteria is from this can be regarded as a classification problem we train a classifier with high accuracy on cross-validation but when we deploy the classifier to real data we found it performs poorly the reason is that a lot of real did have belong to unknown bacteria that is those bacteria are not in the training data distribution we call this out of distribution data OD surprisingly the classifier can assign high confident predictions to those all the inputs that say I don't know so we need an accurate OD detection master to ensure the safety ployment one popular strategy is to use a generative model to model the input distribution and evaluate the likelihood of new inputs however that's being observed that a generative model can assign even higher likelihood to all the inputs for example a generative model trained for feminists can say higher like equal to illness we observe a similar failure mode and genomic data so why is that here is an image from Amnesty when we interpret the image we humans in an easily ignore the background and focus primarily on semantics but the likelihood P of X calculates for all pixels in an image including both semantics and background though we want to just focus our semantics the likelihood can be dominated by the background so we propose a likely for the racial method we train a background model unperturbed includes the right amount reservation can corrupt the semantic structure in the data and capture only the background now way model the likelihood ratio between the full model and the background model and the background component is canceled out only likelihood for semantics remains so likelihood ratio is apt a quantitative score that captures the significance of the semantics compared waste of that word to call it eight him they evaluate the difference between the likelihood and likelihood ratio we plot their values for each pixel likelihood is dominated by the background pixels as we can see well the likelihood ratio focuses more on semantic pixels applying our method to the genomic data said we found likely for the ratio corrects for the background GC bias and it achieved the state of art to sum up we found the raw likelihood can be confirmed by the background we do I politely put the ratios that the corrects for the background and other forms the row like it would are all detection the new benchmark data set a new code is available and either sense for your attention"
    },
    {
        "sourceUrl": "https://youtu.be/pg3Z3HEKHjc",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/ee0c1616bbc82804b2f4b635d4a055fb-Paper.pdf",
        "summary": "so in this book we basically address the problem of prediction and special point browsers so suppose we observe events along the space as per some underlying intensity function and our goal is to predict counts why in a given region extent and we also want to produce intervals capital lambda of X in units of counts per unit area so that we can have validity as described in equation 1 but intervals produced by traditional parametric models are not valid if the assumed model class is wrong but in this work we want to produce intervals that will be valid even if the model is misses specified so in our approach we assume a space to be discretized into our regions which converts the event data into region and count pairs from some unknown distribution and we want to now learn a conditional distribution for counts given regions we assume the Poisson model class for the counts in every region where the mean is parametrized by theta we define the loss as the per sample KL divergence and the best model is the one that minimizes the smalls now suppose somehow from the data using some criteria we learn a model to eat ahead then you can use this model along with the conformal prediction frameworks to produce intensity intervals that are valid even under model misses pacification but to get intervals that are tight and informative you need a criteria that will give you a loss at theta hat which is close to the optimum loss so we propose the following criteria and this criteria leads to this provable out-of-sample accuracy guarantee so we can prove that the loss at theta hat learn using equation 3 would be close to the optimal loss according to equation 4 and this theorem basically leads to tired intervals so here we have a case where we generate count data using negative binomial distribution for three different intensity functions and the data in this region is missing so you can see that the average interval size for the proposed method in blue is four times less than the average intervals has produced by maximum likelihood moreover the empirical coverage is greater than the set 80 percent coverage probability for all the intensity functions here are some examples in one and two dimensions where we can see that the interval size also grows in Mason regions to reflect the inherent uncertainty so we basically proposed a method for producing valid intervals by using a Poisson model class which has an EZ which has a out-of-sample accuracy guarantee and board guarantees are valid even under model misses classification for more details you can look at our paper thank you"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=Rfy3dG3lRpM",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf",
        "summary": "in this work we learn real importance correspondence from videos in self supposed manner our learning process integrate two highly related tasks our children level matching which locates a local image region randomly cropped from a reference frame at another nearby frame and finally matching which establishes pixel level associations between these two widow friends although highly related in the previous work the object level organization and find when matching tasks are certainly served together due to the different optimization goals we on the other hand demonstrate that these two tasks benefit each other by sharing honorable into a friend affinity matrix that associates the content of the two images the rating level organization helps to reduce the ambiguities in fine-grained merging by narrowing down the search in regions where the fine grain merchant provides bottom-up features to facilitate reading level localization the two tasks is jointly trained and progressively benefit to each other specifically given two frames refers to randomly sample of hash from the first frame recorders up offering a sauce patch and a second frame as a jockey frame the seal and the learners image representations are features of pasta sauce pan and at a keyframe the affinity matrix is computed to represent the similarity between these two filaments but modified affinity matrix with the standard gray code in the matrix we are able to locate the source patch in a key frame as shown in the yellow boxes the localize the pair share common content with the sauce patch we call dispatch the target patch at final stage and then will emerge to walk the cat information from the source patch to the taki patch since the taki patch is a local region of the target frame a sub affinity matrix can be extracted accordingly we use this opportunity to work the color channels of the sauce patch well the differences between the predicted target patch and the Guangzhou's page serves as the spoliation signal with the shared affinity as the bridge the serum that's gradient from both tasks during the joint training process once the feature representation has involved the battle organization will be obtained where the color matching can be learned between a better aligned patches so the two stages improve each other and jointly benefit the sealants representation learning yourself civilized manner presentation will use to learn affinity matrix between consecutive frames to propagate various informations from the first frame to the rest of the video here we show the propagation of instrumentation masks on the dailies 17 - the set where the initial mask of the true object instances are provided at the first frame our method is able to preserve more details through propagation process sense to the learnt dance of vanity metrics and the joint training of region and pixel level matching the qualitative lead has to the user segmentation performance which shows that our model performs favorably against a stay of our messages more importantly our results surpass the feature which is super wisely learned on the large-scale internet data set beside segmentation masks we can also use the learned affinity matrix to propagate human hosts key points through the videos human opposed imitations in the first frame Republic eight the key points to the rest of the frames in traditional examples on human part publication but operating small possibly cross more fine queen details compared to populating the incense masks or model is still able to achieve decent result as shown in this video and here is another example"
    },
    {
        "sourceUrl": "https://youtu.be/umWPhw9o968",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/438124b4c06f3a5caffab2c07863b617-Paper.pdf",
        "summary": "we present a novel method for optimizing and controlling soft robots while simultaneously learning a compact representation of the robots state one of the largest challenges in modeling motion planning and control of soft robots is expressing their high dimensional state space with a lower dimensional representation that is tractable for control existing methods such as modal analysis can introduce modeling error and do not consider how a task will be completed our solution is to iteratively learn a latent space while simultaneously optimizing robot design parameters control parameters or both we build upon recent work in differentiable soft body physics engines which produce a fully differentiable simulations for optimization and differentially handle contact because these simulations are fully expressed on a grid we can apply deep convolutional neural networks to grid States and Lorna Li in space this control architecture is also fully differentiable meaning the entire system can be optimized via gradient based optimization techniques we demonstrate our algorithm our model robots this 2d biped must walk as far to the right as possible here is an optimized biped in both materials and control blue regions represent stiffer material you Center of the cross marked section on the the arm needs to reach the green circle this arm is relatively stiff compared to the amount of actuation that can receive and so it must optimize a controller that can swing back and forth to build up velocity to reach its target the center of the cross mark section on this 2d elephant needs to reach the green circle the elephant must optimize controller so that it can reach the target circle in this bunny task the robot must walk forward and its two upper arms or ears must reach the two circles this is a challenging task that cannot be solved with 100% accuracy this 2d rhinoceros robot was created directly from a dot PNG image to demonstrate that our algorithm is capable of handling unstructured inputs with curved sections in this extension to 3d this 3d quadruped it must run as far to the right as possible in the allotted time this variation preserves a curved analog of our boxier Khwaja pod and presents similar performance in forward locomotion this hexapod which also must run as far to the right as possible represents our most dynamically complex 3d example with 24 actuators thank you for your time"
    },
    {
        "sourceUrl": "https://youtu.be/FS66skIlIYQ",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/1fa6269f58898f0e809575c9a48747ef-Paper.pdf",
        "summary": "hi I'm Dean champ presenting our work on Tabrizi our tags and attributes based visual common-sense reasoning based on the shakaama sense reasoning consists of two related subtasks the first subtask is question answering note that for each image object detection czar provided by the test data set this deductions are directly referred to in the text missing tags targets are represented by the indices of the detected upon e boxes for example the question how did the lady and the man get here has two tags planning to corresponding person detections in image for this question the correct answer is the travelling a cart the second the results of the task is answer justification for this given question and the correct answer the model needs to choose a cracker rationale from four options here the crack formation is recorded to recite them is likely their mode of transportation we improved over previous methods on a ratio task using simple and effective baselines particularly our contributions are threefold first we use a simple base network with just half of the number of trainable parameters with this a patient will have work alone we improve over previous state of the art second we leverage attribute information about objects to augment the image features used by our network third we improve image tax funding by finding new tags and adding them to data set here is the architecture for simpler based Network we first take the detected objects and represent them using frasassi and features then we represent each word of a given sentence using protein-protein weddings next we concatenate each word inviting with their detection feature then the past base concatenated image text representations through an OST M we pulled out hood of ours TM together journey buddy for the image in the sentence the query and the options are represented using embeddings obtained via this approach each option is scored using a two layer MLP the internet work is Trina and to end using a cross entropy loss we further improve the models reasoning ability by augmenting image feature with corresponding object attribute information we achieve this by using a fuzzy and Virginia with an auxiliary task of predicting object attributes here are the models predicted attributes the model assign squared with hi is married to a woman and snelling to men this provides richer information for better reasoning also we found that the bcad Rosetta contains many words which are pointing to their corresponding detections in image for example the cart which is important for choosing the correct option was in fact not tacked to sub days we developed a simple algorithm for finding the additional tax here is the output of our algorithm on this example note that the word cart is not crack recorded which supports the reasoning process we quantitatively show that our simple base network also performs a model proposed by in the data we see our paper we also show that both are true edge words and new tags help improve performance on the BCR tasks we are operating with significantly fewer general parameters thank you"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=VbgM-hO_fQ0",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/1e0feeaff84a19bf3936e693311fa66d-Paper.pdf",
        "summary": "hello I'm Benjamin Hockey and I'm presenting or paper with titled deep multi-state dynamic recurrent neural networks operating on wavelet based neural features for robots brain machine interfaces brain machine interfaces have many different applications in its most basic form they translate the neural data to useful control signals if we want to design a tiny chip and put it inside the brain so that this chip can decode the neural data for example here a tetraplegic patient is moving a robotic limb on the screen to a target but as you see he has some bulky devices and long wires on his head by designing this tiny chip we can get rid of these devices or goal is to use high-performance and robust machine learning algorithms to design low power and low area chip to minimize the treatment cost but the challenges or variability of the neural data and very limited data that we can record from a human subject we have shown in our paper that or DRN and outperforms other algorithms the key is that you are passing its own predictions to the next level to do the next prediction so the inputs are neural data and the previous prediction since we can assume that the movement is a continuous trajectory and then it does another prediction but then by flipping a coin we decide whether we pass the ground truth or its previous prediction at the beginning that the network hasn't learned how to do well they mostly pass the ground truth but after a while we pass its own prediction the other difference of our algorithm and compared to El SEM and ordinal is that we added a derivative state that the derivative of s is related to s and all that also exists in an RSS lsdm an Oran so this is architecture of our system first we are doing feature extraction from a IP and behave five regions of the brain and then by doing some data cleaning feature selection by X abuse and PCA of your decoding neural data to movement kinematics we are extracting wavelet based Fourier based and sparks features here are the results of the single day analysis of different quarters on wavelet based features as we see or do renin is doing smooth predictions and it outperforms others here the are performed or D or Ihnen on different features and we see that the ordinary wavelet feature is still out performing other features and then we evaluated different decoders with mid frequency wavelet feature and again we see that or Ihnen has better performance and then other decoders so in summary we are designing high performance algorithms by using informative features to implement them on an icy and implanted inside the brain I want to thank our team and then our code and our paper and supplementary material available online thank you"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=h2JhDAdaa-Q",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/7d2be41b1bde6ff8fe45150c37488ebb-Paper.pdf",
        "summary": "hello and welcome to this video summary of our work from voxels to pixels and back self supervision in natural image reconstruction from fMRI to be published at nori peace 2019 i'm gigas eve beach the candidate from the Mahalo Roni lab at the Weizmann Institute of Science and I'm going to walk you through the main contributions and results from this paper in this work we demonstrate highly accurate reconstruction of images observed by a subject using nothing but his recorded brain activity captured via fMRI we achieved this for two very different fMRI data set using a novel self supervised method the available data for the task consists of about a thousand pairs of images and their corresponding fMRI scans traditionally this paired or labeled data is used to learn the mapping between the visual stimuli and their brain activity representation however since the data is limited and cannot spend the huge space of natural images and natural fMRI samples such decoders are prone to poor generalization to new held out data we propose to put back-to-back an encoder model which Maps images to their fMRI and a decoder model which captures the inverse mapping the following training configuration imposes that images returned to themselves under transformation IDI encoder decoder this enables to train on additional data of any unlabeled images those can be images for which we don't have an fMRI recording at all in our case we used additional 50,000 natural images from image net validation set this introduces adaptation to the statistics of natural images moreover we can also cast a de encoder and the decoder the other way around imposing that unlabeled fMRI samples are mapped to themselves under transformation decoder encoder de once again you have the liberty to choose here your unlabeled data to train on importantly our approach encourages to use here the unlabeled fMRI samples from the test data but without using any of their corresponding images since those images are never used in it is perfectly legal to train on those tests have memorized samples which are just the samples from the decoders input space importantly this training configuration adapts the decoder to the statistics of your test fMRI data this is the main performance gain factor of our method we conduct training in two phases in the first phase we train the encoder alone in a supervised way this enables the encoder to converge first and serve as strong guidance for the decoder which is trained next in the second phase the encoders weights are fixed and we train the decoder on three objectives simultaneously within each single batch these objectives include supervised training on label training data and supervised training on unlabeled natural images and unsupervised training on unlabeled tests fMRI data let's see the effects of each of these components on the test three constructions so these are five test images never used in training this is what we get when we employ the decoder supervised training alone one can observe that the reconstructions are quite blurry adding the training on unlabeled images indeed introduces someone over F natural image statistics however the main leap appears when adding the training on unlabeled tests fMRI this highlights the importance of training on the input test data here the unlabeled test fMRI adaptation to test data makes our method reconstruct impressively not only selected few test examples but many of them all of which are included in the paper in addition our method is also easily applied to other very different Fri datasets under the same configuration and hyper parameters we also favorably compete against gun based methods which oftentimes produce natural-looking images but not as faithful as ours to the underlying fMRI and its corresponding ground truth image log on to our project website for more information about the project I'm going as Eve and thank you for watching"
    },
    {
        "sourceUrl": "https://youtu.be/klAqFvyQx7k",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/90cc440b1b8caa520c562ac4e4bbcb51-Paper.pdf",
        "summary": "hello everyone you are watching a promotional video of our recent work accepted at new ribs 2019 structural graph learning viola plus in spectral constraints the existing state-of-the-art methods learn and directed Gaussian graphical models under the Gaussian Markov random field assumption by including laplacian constraints into the graph colossal framework however they fall short when it comes to include additional knowledge on the structure of the underlying graph which makes them unable to learn practical models such as key component grabs to an able learning of more complex structures we make use of the spectral constraints on the eigenvalues of the laplacian matrix however in this way the problem becomes intractable therefore we approximate the previous problem by adding a spectral regularization term to the objective function together with the constraints on the vector lambda the spectral regularization term can be thought of as a prior information on the graph structure unlike existing methods this formulation is able to learn structures that often appear in a supervised in machine learning tasks such as clustering here we see a few results on toi datasets we can verify that our algorithm is able to correctly cluster those e spatial datasets as for real datasets our algorithm achieves almost 100% accuracy in the Cancer Genome Atlas dataset which contains about 800 nodes each of which with 20,000 features the code for the paper is available online on github and Surin to see more come to our poster at no ribs 2019 thank you"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=UMjMQaH508M",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/ac52c626afc10d4075708ac4c778ddfc-Paper.pdf",
        "summary": "decision tree algorithms have been popular since the very beginning of machine learning the main problem that's always plagued decision tree algorithms is their lack of optimality because they've historically been greedy myopic algorithms like see four point five in cart and these algorithms construct trees from the top downward and then greedily prune them back afterward the problem is that if a greedy algorithm chooses the wrong split at the top of the tree there's no way to undo it so these greedy algorithms produce suboptimal trees but it's hard to improve over the greedy methods because decision tree optimization is hard both theoretically and practically right there's a combinatorial explosions in the number of possible trees we could consider and even careful modern approaches haven't been able to solve these problems efficiently our work provides the first practical algorithm for producing optimal sparse binary split decision trees we minimize the miss classification error regularized by the number of leaves in the tree we don't use greedy splitting and pruning instead we developed a specialized branch and bound method to solve the problem to optimality leveraging computational caching and when we solve it to optimality we get sparse accurate trees like this one on the Florida rear ass data our approach uses several important insights first we have a collection of analytical bounds that reduce the size of the search space and these bounds allow us to prove that certain partial trees can never be extended to form optimal full trees and these bounds tell us that the leaves of optimal trees must capture enough data and be accurate enough and if they're not we can eliminate that tree and its descendants and some of the bounds tell us that when the tree has too many leaves to be optimal then we can eliminate that tree and its descendants and there are several other bounds too we represent each tree only by its collection of leaves and this is a very convenient way to work with trees and restore balance and intermediate results within each leaf we also maintain a permutation map which lets us figure out whether we've already seen a different permutation of the leaves in a different tree that we've already explored and we can also detect when we create a leaf we've used before and avoid recomputing the bounds for that leaf again additionally we store a bit vector indicating which data points have features corresponding to the features described by the leaf and this lets us use fast bit vector operations to compute bounds and the bounds and bit vectors in each leaf also let us use incremental computation to evaluate the children of the leaf should we decide to split it these features the strong analytical bounds are representation the permutation map computational caching and incremental computation combined to make our implementation really really fast which lets us produce truly optimal and sparse decision trees thank you"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=UMjMQaH508M",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/ac52c626afc10d4075708ac4c778ddfc-Paper.pdf",
        "summary": "decision tree algorithms have been popular since the very beginning of machine learning the main problem that's always plagued decision tree algorithms is their lack of optimality because they've historically been greedy myopic algorithms like see four point five in cart and these algorithms construct trees from the top downward and then greedily prune them back afterward the problem is that if a greedy algorithm chooses the wrong split at the top of the tree there's no way to undo it so these greedy algorithms produce suboptimal trees but it's hard to improve over the greedy methods because decision tree optimization is hard both theoretically and practically right there's a combinatorial explosions in the number of possible trees we could consider and even careful modern approaches haven't been able to solve these problems efficiently our work provides the first practical algorithm for producing optimal sparse binary split decision trees we minimize the miss classification error regularized by the number of leaves in the tree we don't use greedy splitting and pruning instead we developed a specialized branch and bound method to solve the problem to optimality leveraging computational caching and when we solve it to optimality we get sparse accurate trees like this one on the Florida rear ass data our approach uses several important insights first we have a collection of analytical bounds that reduce the size of the search space and these bounds allow us to prove that certain partial trees can never be extended to form optimal full trees and these bounds tell us that the leaves of optimal trees must capture enough data and be accurate enough and if they're not we can eliminate that tree and its descendants and some of the bounds tell us that when the tree has too many leaves to be optimal then we can eliminate that tree and its descendants and there are several other bounds too we represent each tree only by its collection of leaves and this is a very convenient way to work with trees and restore balance and intermediate results within each leaf we also maintain a permutation map which lets us figure out whether we've already seen a different permutation of the leaves in a different tree that we've already explored and we can also detect when we create a leaf we've used before and avoid recomputing the bounds for that leaf again additionally we store a bit vector indicating which data points have features corresponding to the features described by the leaf and this lets us use fast bit vector operations to compute bounds and the bounds and bit vectors in each leaf also let us use incremental computation to evaluate the children of the leaf should we decide to split it these features the strong analytical bounds are representation the permutation map computational caching and incremental computation combined to make our implementation really really fast which lets us produce truly optimal and sparse decision trees thank you"
    },
    {
        "sourceUrl": "https://youtu.be/HLD6H2eJK1E",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/95192c98732387165bf8e396c0f2dad2-Paper.pdf",
        "summary": "in this work we introduced multiplicative compositional policies a method for learning reusable motor primitives that can be composed to produce a flexible range of skills standard hierarchical policies compose primitive skills by using a gating function which specifies the probability of activating each primitive in a given scenario one of the limitations of this model is that only one primitive can be activated at each time step which can restrict the range of behaviors that can be produced by the composite policy we propose combining primitives using a multiplicative composition scheme which enables multiple primitives to be activated simultaneously and contribute to the composite policies action distribution the weights from the gating function specify each permit of influence on the composite distribution with the higher weight corresponding to a larger influence given a state each primitive proposes in action distribution in response to that state the gating function receives both the state and a task specific goal as input then outputs the weights for each primitive the distributions are then composed according to the weights to produce the composite action distribution each primitives action distribution is modeled by a Gaussian varying the weights produces different interpolations of the primitives distributions primitives are learned through pre-training tasks that encourage the primitives to specialize in different skills when transferring the primitives to new tasks a new gating function is trained to compose the primitives for the new task the primitives are trained by imitating reference motions such as mocap clips recorded from human actors once trained the primitives can be transferred to challenging new tasks such as picking up an object and carrying it to a target location we can also train characters to dribble a soccer ball to a goal more details are available in the paper thanks for watching"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=xTAPZbQlq3A",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/cd474f6341aeffd65f93084d0dae3453-Paper.pdf",
        "summary": "In this video, we describe an analytic unification of two actions frequently used in graph reduction: the deletion of edges, often used in graph sparsification algorithms, and the contraction of edges (that is, the merging of two adjacent nodes), which is often used in graph coarsening algorithms. Prior to this work, sparsification and coarsening were treated as separate algorithmic primitives, with different objective functions. What you are seeing now  is our graph reduction algorithm in action, which uses this analytic unification to simultaneously sparsify and coarsen a graph. The actions of edge deletion and edge contraction are, in fact, dual operations. One manifestation of this duality is seen by considering a planar graph, shown here in blue. The planar dual of this graph, shown here in red, is created by first placing its red vertices in the regions formed by the planar embedding of the original graph. Then, one adds a red edge between each of these red vertices that share a blue border. Notice that the planar dual always has the same number of edges as the original graph, such that each edge in one graph crosses exactly one edge in the other graph. Now, if one of the blue edges is deleted, its corresponding red edge is contracted, merging the two red nodes into one. Likewise, if a blue edge were to be contracted, it would amount to deleting its corresponding red edge. This duality generalizes to non planar graphs by considering its associated graphic matroid and its dual. Algebraically, this duality is reflected in the graph Laplacian, a matrix operator related to diffusion throughout a graph, and whose spectrum reveals information about the graph's global structure. Indeed, many sparsification and coarsening algorithms aim to preserve properties associated with the graph Laplacian. Consider the heat equation,  a prototypical diffusive process. With respect to the dynamics of this differential equation, edge deletion corresponds to the limit of zero edge weight, while edge contraction corresponds to  the limit of infinite edge weight. However, this edge contraction limit requires some entries in the graph Laplacian to become infinite, making analytic treatment difficult. Our primary insight came from the fact that the  inverse of the graph Laplacian remains finite in both of these limits. And in fact, many relevant problems on graphs involve solving the equation Lx = b for x, so the solution is given by applying the inverse to b. Moreover, while the lowest eigenvalues of the Laplacian are associated with the graph's global structure, by inverting the spectrum, these correspond to the highest eigenvalues of the inverse Laplacian. Motivated by these considerations,  we developed a probabilistic graph reduction algorithm that preserves the inverse Laplacian in expectation and aims to minimize the expected squared error for a given amount of reduction. Thus, by preserving the inverse Laplacian, our algorithm is able to perform both edge deletion and edge contraction while preferentially retaining the large-scale structure of a graph. If this video piqued your interest please stop by our poster Wednesday from 5 to 7 p.m. in East Exhibition Hall B+C. Thanks for watching!"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=PwB3CS2rHdI",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/e9bf14a419d77534105016f5ec122d62-Paper.pdf",
        "summary": "he had your dark suit and greasy washed water all year he had dark wash water all year he had your dark suit and greasy washed water all year he had your dark suit and greasy washed water all year all year he had your dark suit and greasy wash"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=8AZBuyEuDqc",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/370bfb31abd222b582245b977ea5f25a-Paper.pdf",
        "summary": "we present a few short framework for video to video synthesis our network can translate high-level representation videos to photorealistic videos based on example image when we change the example images the same model can generate different looking outputs for example given an image of a person we can transfer the motion of other persons to the person in the example image here are the synthesized sequences known as the results realistically capture the moving motions for all different subjects while still maintaining the appearance of the original person this shows that our model can be generalized to unseen subjects not present in the training set here we show more examples of different dancing sequences moreover we can also transfer motions to sculpture images making steel object stands like a real human as well in addition to pop sequences often work can also be applied to faces here is surely example images the driving sequences which we steal the motion from and the synthesized results again the results faithfully reflects the inked emotions while preserving the person's identity in example images we can also transfer motion to paintings thus making still pictures talk in a lively way finally our model can also be applied to stray things we use images from different cities as the example images and synthesized sequences of different styles thank you please find our website encode here"
    },
    {
        "sourceUrl": "https://youtu.be/l3SoeIIjxTM",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf",
        "summary": "this is a video summary of our nukes 2019 paper image synthesis with a single robust classifier it is by now clear that deep neural networks have evolution s computer vision the world easily successfully as the most experts and have since been used to build a sophistic intuitive for higher vision tasks the keeper should explore theories are simply claspers enough to perform some of these more advanced tasks from an undisturbed we need to manipulate the input in a semantic way how do we do this using a classifier the most natural approach would be to maximize the score for chosen class as predicted by the classifier for example we can try to make this skype don't like by increasing the dog score let's try this out well it didn't really work the change the name put is very noticeable in fact this shouldn't come as a surprise we already know from the notorious and reserve exams that class effects are sensitive to small changes in the input in order to perform in manipulation we need to at least ensure that our classifiers are not too pretty in other words what seems to be missing here is robustness well if we instead cleaner models to also be invariant to small input changes using robust optimization this works glass maximization for busman's seems to actually add minifig dog features the diamond so starting from this primitive what kind of visual tasks can we do internal policy and also questioning thus create the center of the cache course with no priors or regular returns is enough for example we can do things like generation for large data sets like a madman super resolution in painting where the code is reconstruct a corrupted image image dimension station such as turning horses to zoom and vice versa or even turn crude steps into out taking this further McKellar to directly paying teachers on to Avengers beyond these applications the world has shown that robust class pairs can be used for imaginative and star transfer overall the broader message of our paper is that robustness can be useful beyond the security context in fact robust models can be versatile tools for domain specific applications you can read more her paper on purpose we also release her library robustness which can be used to replicate these experiments finally check out our other newspaper on understanding of material examples as a phenomenon"
    },
    {
        "sourceUrl": "https://youtu.be/1UvVnIbjSX8",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/4fdaa19b1f22a4d926fce9bfc7c61fa5-Paper.pdf",
        "summary": "hello everyone in this paper we propose a neuropathic pain diagnosis simulator for causal discovery evaluation a fundamental task in various disciplines of science is to find underlying causal relations and make use of them to understand and explain phenomena causal discovery aims for finding causal relationships by analyzing observational data assume that we have proposed a causal discovery method and we want to evaluate it we generally achieve this by testing our method on ground truth annotated real-world data a lemma there are very few real-world data sets available for constant discovery evaluation because it is difficult to get ground truth about causal relations moreover the challenges in real data analysis make the evaluation even harder to bypass real data related difficulties we use synthetic data for evaluations instead we know however that there is a performance gap between synthetic data and real data researchers today test their various methods using different synthetic datasets thus it is hard to compare ones method with others and improve upon them to solve such problems we can use simulators for evaluating causal discovery methods a simulator can mimic real-world scenarios and involve real data challenges our simulator is based on Europe attic pain diagnosis because the causal connections are known from exhaustive biomedical research for example due to lifting a heavy bag Alex hurt his disc alignment l5 to s1 this presses the nerve l5 and causes l5 radiculopathy consequently l5 radiculopathy causes the lower back and knee pain with the help of doctors we thoroughly summarized all the causal relations that we know of into a directed graph for simulating data from such a causal graph we need to know the conditional probability distribution for every node we use a real-world data set of 141 patients for learning the parameters we can then sample the synthetic data that we need from our simulator more details about our simulator can be found in this paper we show that our simulation data are indistinguishable from the real Diagnostics records even medical experts are unable to tell the difference we also evaluate causal discovery algorithms with our simulator in the presence of various practical challenges our simulator is fully open source online with data generation code that can be used with one single command line many examples for causal discovery evaluation are also valuable check them out use them for your research and cite us accordingly thank you"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?feature=%3Dyoutu.be",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/c055dcc749c2632fd4dd806301f05ba6-Paper.pdf",
        "summary": "hello everyone in this paper we propose a neuropathic pain diagnosis simulator for causal discovery evaluation a fundamental task in various disciplines of science is to find underlying causal relations and make use of them to understand and explain phenomena causal discovery aims for finding causal relationships by analyzing observational data assume that we have proposed a causal discovery method and we want to evaluate it we generally achieve this by testing our method on ground truth annotated real-world data a lemma there are very few real-world data sets available for constant discovery evaluation because it is difficult to get ground truth about causal relations moreover the challenges in real data analysis make the evaluation even harder to bypass real data related difficulties we use synthetic data for evaluations instead we know however that there is a performance gap between synthetic data and real data researchers today test their various methods using different synthetic datasets thus it is hard to compare ones method with others and improve upon them to solve such problems we can use simulators for evaluating causal discovery methods a simulator can mimic real-world scenarios and involve real data challenges our simulator is based on Europe attic pain diagnosis because the causal connections are known from exhaustive biomedical research for example due to lifting a heavy bag Alex hurt his disc alignment l5 to s1 this presses the nerve l5 and causes l5 radiculopathy consequently l5 radiculopathy causes the lower back and knee pain with the help of doctors we thoroughly summarized all the causal relations that we know of into a directed graph for simulating data from such a causal graph we need to know the conditional probability distribution for every node we use a real-world data set of 141 patients for learning the parameters we can then sample the synthetic data that we need from our simulator more details about our simulator can be found in this paper we show that our simulation data are indistinguishable from the real Diagnostics records even medical experts are unable to tell the difference we also evaluate causal discovery algorithms with our simulator in the presence of various practical challenges our simulator is fully open source online with data generation code that can be used with one single command line many examples for causal discovery evaluation are also valuable check them out use them for your research and cite us accordingly thank you"
    },
    {
        "sourceUrl": "https://youtu.be/k3IQnRsl9U4",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/adf7ee2dcf142b0e11888e72b43fcb75-Paper.pdf",
        "summary": "in this video we will present our paper this looks like that deep learning for interpret about image recognition suppose you are looking at a spurt and wondering what kind of burn it is you guess that the bird is a sparrow but how would you describe your thought process perhaps the birds had looks like that of a prototypical sparrow or the wing bars look like a sparrow swing bars when we classify images we might focus some parts of the image and compared them with prototypical aspects of a given class by saying this looks like that in this work we introduce a network architecture prototypical parts network or proto P net that defines a new form of interpretability in image recognition by explaining its classification decisions just like how we humans would do it in this way our model is interpret well in the sense that it has a transparent reasoning process while making predictions previous integratable models often explain classification decisions using attentions they point to either the entire object or the important parts of an object however it is often unclear for these models why the highlighter regions are recognized as important in contrast our model provides a retry explanation by not only highlighting the important parts but also justifying the highlighting by drawing comparison to prototypical aspects of each class more concretely our proto pianet introduced as a special prototype layer that can follow any feature extraction convolutional layers the product idea contains prototypes that can be understood as representations of typical parts in each class such as red wings for the class of red-winged blackbirds in the prototype layer the patches of convolutional features are compared to each other learned prototypes using l2 distances this generates a prototype activation map which tells us both the location of the most singular patch as well as the degree of such celerity as indicated by a similarity score for example the top would have here corresponds to the head of a clay color sparrow given the new input image on the left our model recognizes the upper right column edge of the input image which is the head of the bird to be very similar to this arrow head prototype which has the similarity score of 3.95 every blue have union in the prototype layer produces such as memory score and those linearity scores are weighted by a fully connected layer to produce the auto logits i the final source for all the classes our special prototype layer can be used on top of any deep convolutional feature extractors to enhance the models interpretability in our experiments we integrated the prototype layer with vgg resonant and dense net architectures the experimental results show that proto peanut can achieve comparable accuracy with its analogous non interpreting counterpart we can view decision-making of our model as evaluating a scoring sheet for each class here the final score for every class can be understood as a weighted sum of similarity scores with visualizable prototypes of that class the explanations generated by our network are actually used during classification and are not created post hoc to learn more about our work please check out our paper for more detail"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=aUDkfVd3t_8",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/5d4ae76f053f8f2516ad12961ef7fe97-Paper.pdf",
        "summary": "this video presents our paper on address hull training and robustness for multiple perturbations which appears at nurbs 2019 we tackle the problem of defending machine learning models against multiple types of address all examples we show both formally and experimentally the existence of our business trade-off were defending against multiple perturbation types lowers the model's robustness to each individual type we also introduce a fine attacks that further reduce the accuracy of robust models by interpolating between perturbations so what are addressed all examples they are minimally perturbed inputs that reliably cause classifiers to make mistakes these fail show that machine learning models learn very different features than our own visual system and our concern for safety or security critical deployments a natural defense against address all examples is adverse el training for some chosen sets of perturbations that we want to be robust to we continuously generate worst-case address L perturbations for our model and add these to the training set address all training does improve robustness for the type of perturbations that the model is trained against infinity noise in this case but the model remains vulnerable to other types of perceptually small perturbations such as sparse noise or small rotations we just ask whether we can extend the address on training so as to learn a model that is simultaneously robust to multiple types of perturbations that an adversary might choose we generalize address all training to this setting by training a model on worst-case address on examples that come from the union of all the chosen sets so does this work yes though some interesting caveats on safe are 10 for instance we show that the model trained to be robust to just two types of perturbations loses about five percent of robust accuracy compared to models that were individually trained against each perturbation type we actually prove that this type of robustness trade-off is inherent in some natural classification tasks surprisingly on mes the situation is even worse a model trained on multiple types of LP noise achieves only about 50% robust accuracy a drop of 20% compared to models individually trained on each type of noise what's happening here is that to achieve an and finicky robustness the model learns to threshold the input pixels and is partially zeroes out to models gradients as a side effect is breaks l1 and l2 attacks and the model fails to learn the robust representation our work shows that this issue commonly known as gradient asking also effects advice on training finally we consider a more general adversary' that combines perturbation types as an example we show that instead of Eva rotating an image or adding small noise to it an adversity that there's a little bit of both confer producer models robust accuracy by 10 percent to conclude while we can train models against different types of adverse health examples this comes at a cost in robustness to each individual type our work raises some open questions such as how to prevent gradient masking on a list or how to more efficiently scale multi perturbation adverse health training finally there remains a fundamental question of how to list all the types of perturbations that we want our models to be robust to in order to emulate our own visual system if you'd like to learn more we invite you to read our paper or to attend our spotlight talk and poster at Europe's"
    },
    {
        "sourceUrl": "https://youtu.be/aR64H5Jn0gY",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/2201611d7a08ffda97e3e8c6b667a1bc-Paper.pdf",
        "summary": "hello my name is Tao I want to present a three-minute video for a multi criteria dimensionality reduction with applications to famous paper except at two nerves 2019 let's start first with the motivation we perform the standard PCA on the real data of face image consisting of male and female group what we observe is that the reconstruction area for male group is consistently better than female group for many target dimensions here we see that the reconstruction error is about 10% better for male than female so the question is how can we make it more fair to both groups our first contribution is to formulate a problem as multi-criteria dimensionality reduction or MC dr as you can see here you can think of fi as the generalization of variance I give you protection P the group may define the own utility something more general than just to variance and social welfare here doesn't have to be the sum like total variance as in standard PCA but it can be other function so for example in a Social Welfare the FI utility is the same is just o variance the social welfare is taken to be the product instead of some another example is marginal loss this case the FI is not the variance but the change of variance between the best projection the group could have gotten compared to the given protection which may be different because of the group in the same dataset and G is taken to be the group that has the worst performance algorithmically we give a polynomial time algorithm for this problem the guarantee is that it has the optimal utility and small rank correlation s which is roughly square root of 2 times the number of groups by scaling we can also achieve no Rev violation but with approximation ratio 1 - s over D on utility in practice we develop another method called multiplicative by update which scales better we perform an experiment and here's an example we look at the marginal last objective compare between our algorithm STP round and normal PCA STP round is specified to maximize national sugar Farah and SF and minimize marginal loss and interestingly even though it's specified to maximize now social welfare in marginal our objective is to perform significantly better than no more PCA our one theoretical contribution is to prove that every extreme point of the STP relaxation has low rank and this is a connection we made between up enough optimization community and machine learning community we give a complexity result of this problem showing the NP hardness of general for general K and polynomial-time solvability for fixed K the code is available online on github we also have a web page that give more explanation and motivation for free PCA thank you for listening"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=wfb9UV_n8jg",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf",
        "summary": "hi everyone today I will present our inner EPS paper on online continual learning and this is some work I did with a half Lukas Eugene myself Minh lava and Tina so in continued learning we're currently focused on solving the catastrophic forgetting problem which is that when a model learns a new task it completely forgets how to perform the previous task it learned so there's a couple of way to alleviate the catastrophic forgetting problem but one popular family of methods is rear salt so simply put when you train on a new task you're going to reuse the previous ones there's two Popular's way to achieve this the simplest one is when you sequentially train on task you're going to save some data from the previous task and you're going to retrain on those data points later on we're gonna call this experience replay or a memory replay another way to achieve rehearsal is instead of storing old data as is we're gonna compress it with a generative model so you're gonna train a generative model culturally and when you want to rehearse on past tasks you're just gonna generate some data and train on those generated data points now one important point I emitted is that you're going to randomly rears on the previous task this sort of assumes that when you're learning a new task it's causing a equal amount of interference on the previous task you've learned but this might be a bad assumption for example if I'm learning a new language maybe this will cause more interference on my language skills than on my ability to ride my bike so in this paper we ask can we do better than random rehearsal so I will explain the method with a little cartoon and then we're gonna dive deeper to it so let's say you've trained a classifier to recognize the digit from 0 to 4 and now you would like it to recognize the digit 5 as well so on the left you have the current loss landscape for this new task in parameter space and on the right you have the last landscape but in input space or in task space this might not be super clear right now but it's gonna get clearer in a second so the first thing we're gonna do is when we're going to take a data point for the from the current task and we're going to take a virtual update step this will most likely decrease the loss for the current task but will also most likely increase the loss on the previous task next we're gonna find where that loss would increase the most in this case it's on the digit number three because the tree looks a little bit like a five and next we're gonna up meant our mini-batch with the digit number three here and this will and now we're gonna take a real gradient update step and hopefully this will not increase as much the loss on the digit number three okay so we can apply our method to experience replay or generative replay it's a little bit easier to explain with experience replay so I will start with this one so you're training a classifier continually you have a standard objective function you're going to store memories in a buffer and when a new data point comes in you're going to take a virtual update step and then you're going to look inside the buffer for the top values where the in fearing interference is maximized and here the interference is the loss is the difference between the loss of the virtual model and the current model so to summarize you look inside your buffer for samples that you're forgetting the most basically and then you augment the mini-batch with those samples and hopefully you don't forget as much okay so now I will explain our method applied to generative replay so now instead of storing old data in a buffer we're storing it in a generative model which is composed of an encoder and a decoder again and you did a point comes in we're going to take our virtual update step and now instead of looking inside the buffer for those samples where interference is maximized we're going to look inside the latent space of the generator and we're gonna do this by gradient descent on the latent codes to maximize interference on a generated samples so long story short we were looking inside the latent space of the generator for a sample that we're forgetting the most given that virtual update and we also have a real constraint to encourage diversity in the retrieve samples okay so we're now going to look into qualitative results for the split M&S data set and this data set it's we split and this into five tasks first we have some offline experiment results so more than one epic pre-task so now you can see that when the model is learning the last task which consists of binary classification between eights and nines when the model is learning the digit 8 it's fetching sometimes the digit 3 which is similar to a 8 and when it's running about nines its fetching the digit number 4 so everything seems to be working as we anticipated in the online setting the it's much harder so as you can see that vai baseline is generating fading in digits or that are not always recognizable but in our case again when we're learning about the digit 8 & 9 we're fetching again the number 4 and the number 3 which seems to be recognizable so qualitatively speaking it seems that our method can outperform the baseline I'm now going to quickly talk about the quantitative results so first under splittin is data set and the permitted and this data set both our methods can outperform their respective baseline both in terms of accuracy and in terms of forgetting and we can also achieve the best performance over all the baselines and for the split safe art and data set we only applied our experience replay method because generative replay doesn't work yet on harder data sets like so far and again here we cannot perform the baselines so other stuffs you can find in the paper is another approach with which is an ibrid between the experience replay and the genetic replay approach we have results on mini imagenet which is a really hard data set for consider learning we have an ablation study we have results on online control generative modeling and we also have a nice code base that can reproduce reproduce all of the results so thanks for listening"
    },
    {
        "sourceUrl": "https://youtu.be/aR64H5Jn0gY",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/2201611d7a08ffda97e3e8c6b667a1bc-Paper.pdf",
        "summary": "hello my name is Tao I want to present a three-minute video for a multi criteria dimensionality reduction with applications to famous paper except at two nerves 2019 let's start first with the motivation we perform the standard PCA on the real data of face image consisting of male and female group what we observe is that the reconstruction area for male group is consistently better than female group for many target dimensions here we see that the reconstruction error is about 10% better for male than female so the question is how can we make it more fair to both groups our first contribution is to formulate a problem as multi-criteria dimensionality reduction or MC dr as you can see here you can think of fi as the generalization of variance I give you protection P the group may define the own utility something more general than just to variance and social welfare here doesn't have to be the sum like total variance as in standard PCA but it can be other function so for example in a Social Welfare the FI utility is the same is just o variance the social welfare is taken to be the product instead of some another example is marginal loss this case the FI is not the variance but the change of variance between the best projection the group could have gotten compared to the given protection which may be different because of the group in the same dataset and G is taken to be the group that has the worst performance algorithmically we give a polynomial time algorithm for this problem the guarantee is that it has the optimal utility and small rank correlation s which is roughly square root of 2 times the number of groups by scaling we can also achieve no Rev violation but with approximation ratio 1 - s over D on utility in practice we develop another method called multiplicative by update which scales better we perform an experiment and here's an example we look at the marginal last objective compare between our algorithm STP round and normal PCA STP round is specified to maximize national sugar Farah and SF and minimize marginal loss and interestingly even though it's specified to maximize now social welfare in marginal our objective is to perform significantly better than no more PCA our one theoretical contribution is to prove that every extreme point of the STP relaxation has low rank and this is a connection we made between up enough optimization community and machine learning community we give a complexity result of this problem showing the NP hardness of general for general K and polynomial-time solvability for fixed K the code is available online on github we also have a web page that give more explanation and motivation for free PCA thank you for listening"
    },
    {
        "sourceUrl": "https://youtu.be/k3IQnRsl9U4",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/adf7ee2dcf142b0e11888e72b43fcb75-Paper.pdf",
        "summary": "in this video we will present our paper this looks like that deep learning for interpret about image recognition suppose you are looking at a spurt and wondering what kind of burn it is you guess that the bird is a sparrow but how would you describe your thought process perhaps the birds had looks like that of a prototypical sparrow or the wing bars look like a sparrow swing bars when we classify images we might focus some parts of the image and compared them with prototypical aspects of a given class by saying this looks like that in this work we introduce a network architecture prototypical parts network or proto P net that defines a new form of interpretability in image recognition by explaining its classification decisions just like how we humans would do it in this way our model is interpret well in the sense that it has a transparent reasoning process while making predictions previous integratable models often explain classification decisions using attentions they point to either the entire object or the important parts of an object however it is often unclear for these models why the highlighter regions are recognized as important in contrast our model provides a retry explanation by not only highlighting the important parts but also justifying the highlighting by drawing comparison to prototypical aspects of each class more concretely our proto pianet introduced as a special prototype layer that can follow any feature extraction convolutional layers the product idea contains prototypes that can be understood as representations of typical parts in each class such as red wings for the class of red-winged blackbirds in the prototype layer the patches of convolutional features are compared to each other learned prototypes using l2 distances this generates a prototype activation map which tells us both the location of the most singular patch as well as the degree of such celerity as indicated by a similarity score for example the top would have here corresponds to the head of a clay color sparrow given the new input image on the left our model recognizes the upper right column edge of the input image which is the head of the bird to be very similar to this arrow head prototype which has the similarity score of 3.95 every blue have union in the prototype layer produces such as memory score and those linearity scores are weighted by a fully connected layer to produce the auto logits i the final source for all the classes our special prototype layer can be used on top of any deep convolutional feature extractors to enhance the models interpretability in our experiments we integrated the prototype layer with vgg resonant and dense net architectures the experimental results show that proto peanut can achieve comparable accuracy with its analogous non interpreting counterpart we can view decision-making of our model as evaluating a scoring sheet for each class here the final score for every class can be understood as a weighted sum of similarity scores with visualizable prototypes of that class the explanations generated by our network are actually used during classification and are not created post hoc to learn more about our work please check out our paper for more detail"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=aUDkfVd3t_8",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/5d4ae76f053f8f2516ad12961ef7fe97-Paper.pdf",
        "summary": "this video presents our paper on address hull training and robustness for multiple perturbations which appears at nurbs 2019 we tackle the problem of defending machine learning models against multiple types of address all examples we show both formally and experimentally the existence of our business trade-off were defending against multiple perturbation types lowers the model's robustness to each individual type we also introduce a fine attacks that further reduce the accuracy of robust models by interpolating between perturbations so what are addressed all examples they are minimally perturbed inputs that reliably cause classifiers to make mistakes these fail show that machine learning models learn very different features than our own visual system and our concern for safety or security critical deployments a natural defense against address all examples is adverse el training for some chosen sets of perturbations that we want to be robust to we continuously generate worst-case address L perturbations for our model and add these to the training set address all training does improve robustness for the type of perturbations that the model is trained against infinity noise in this case but the model remains vulnerable to other types of perceptually small perturbations such as sparse noise or small rotations we just ask whether we can extend the address on training so as to learn a model that is simultaneously robust to multiple types of perturbations that an adversary might choose we generalize address all training to this setting by training a model on worst-case address on examples that come from the union of all the chosen sets so does this work yes though some interesting caveats on safe are 10 for instance we show that the model trained to be robust to just two types of perturbations loses about five percent of robust accuracy compared to models that were individually trained against each perturbation type we actually prove that this type of robustness trade-off is inherent in some natural classification tasks surprisingly on mes the situation is even worse a model trained on multiple types of LP noise achieves only about 50% robust accuracy a drop of 20% compared to models individually trained on each type of noise what's happening here is that to achieve an and finicky robustness the model learns to threshold the input pixels and is partially zeroes out to models gradients as a side effect is breaks l1 and l2 attacks and the model fails to learn the robust representation our work shows that this issue commonly known as gradient asking also effects advice on training finally we consider a more general adversary' that combines perturbation types as an example we show that instead of Eva rotating an image or adding small noise to it an adversity that there's a little bit of both confer producer models robust accuracy by 10 percent to conclude while we can train models against different types of adverse health examples this comes at a cost in robustness to each individual type our work raises some open questions such as how to prevent gradient masking on a list or how to more efficiently scale multi perturbation adverse health training finally there remains a fundamental question of how to list all the types of perturbations that we want our models to be robust to in order to emulate our own visual system if you'd like to learn more we invite you to read our paper or to attend our spotlight talk and poster at Europe's"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=95HlF9nCca4",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/e88f243bf341ded9b4ced444795c3f17-Paper.pdf",
        "summary": "people have an early possibly innate understanding of the world termed core knowledge developmental research suggests that even babies already expect objects to behave in certain ways for example objects shouldn't just disappear or move through one another consider this video of a solid screen rotating over a teddy bear hiding it from view then rotating right through it Wow infants look longer at such events compared to similar looking events that do not violate physics such surprise signals can in turn direct learning the goal of this project is to model the early understanding of physics to capture expectation violation models should take in events and output scalars indicating the level of surprise the surprise for a physical violation event should be higher than a matched norm violation event we created eight different types of violations based on developmental research probing the principles of permanence solidity and continuity previous models based on video prediction performed poorly due to a lack of physical understanding we propose a model based on approximate D rendering extended physics and tracking or adept adept has two parts a perception module and a reasoning module the perception module D renders objects by segmenting them in extracting course attributes from each segment the coarser presentation helps generalize to unseen objects the reasoning module uses a probabilistic physics engine to unfold the belief state over time when a new frame comes in a depth compares the observation from the perception module with its prediction compute surprise and updates the belief putting it all together considering the following violation of permanence video in which two objects enter but only one object leaves the object tracking window shows the physics modules particle filter estimation for where the objects are the yellow object is assumed to have stopped behind the screen but then when the screen comes down this event is registered as surprising we compared them to this models based on video prediction including encoder/decoder Gann analyst TM we use relative accuracy to measure how well the models discriminate violations from control ad that performs best or ties in seven of the eight scenarios in our stimuli set it is the only model to perform above chance in all scenarios adept also generalizes best novel shapes not in the training we asked people to judge how surprising they found our scenarios we then compared the relative accuracies of each model to those of humans well not a perfect match and that predictions are the closest to humans with other models deviating by at least twice the rude mean square error in summary we introduced a new benchmark based on developmental research to test core physics the adapt model that recognizes implausible events using approximate perception and probabilistic dynamics and we also carried out a quantitative model comparison to human judgments this approach highlights the importance of object centric representations for generalizable physical scene understanding"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=M6ohnt_6J9Q",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/2297607a5db8576d5ad6bbd83696ff60-Paper.pdf",
        "summary": "hello my name is Laura Edmondson and I will give a short overview of our work non-linear scaling of resource allocation in sensory bottlenecks in biological sensory systems information transmission is often constrained by a neuro bottleneck where the number of output neurons is vastly smaller than the number of input neurons this implies that some information will be lost in the bottleneck furthermore receptors are often not distributed uniformly across the sensory sheet for varying their density for example envision the density of cones in the retina is much greater at the favia than the periphery in touch the mechanoreceptors are more densely packed in the fingertips and they are in the palm so given these constraints how should the output neurons represent the input spaces in this example we have a higher density region with three times as many input receptors packed into the same space as the corresponding low density region if there was no bottleneck then we might expect a one-to-one mapping of each receptor to an output neuron leading to a proportional representation of the input region receptors what happens when we have fewer outputs and inputs that is in the case of a bottleneck and how does the representation change depending on the width of the bottleneck we investigated these questions using an efficient coding model that maximizes overall information transmission information maximization is achieved by removing correlations from the input signal nearby receptors on the sensory sheet tend to be correlated with the correlations dropping off as a function of the receptor distance crucially neighboring receptors in a high-density input space will be more correlated than those in a low density input space in this paper we show that this problem can be solved by calculating the assault in the eigenvalues of the input covariances for the two regions shown here in blue and orange respectively furthermore for certain forms of the covariance function this can be done analytically so what did we find when the bottleneck is narrow the vast majority of the neurons are allocated to the high-density input region shown in blue leading to an expansion in the bottleneck for intermediate-sized bottlenecks the allocation converges to a constant value leaving to the plateau you see here now the lower density region shown in orange tends to be over-represented finally for wide bottlenecks the allocation converges to a representation proportional to the input ratio when the density ratio is larger the representation of the low-density region can be seen at smaller bottle net widths the width at which the low-density region over-representation plateaus is also smaller what happens when we change how fast the covariance function decays for the low-density region and narrow a covariance leads to the high density region being over-represented for increasingly large bottlenecks in conclusion limiting the number of output neurons does not lead to proportional representation and sensory receptors in the two input regions the input region representation can contract and expand depending on the width of the bottleneck and finally the extent of the spatial correlations and different ratios of receptor densities also influence the allocation in the bottleneck thank you for listening and please check out our paper for more details"
    },
    {
        "sourceUrl": "https://youtu.be/0XEVjCL_9Dk",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/2f3c6a4cd8af177f6456e7e51a916ff3-Paper.pdf",
        "summary": "hi my name is Bruno I need this video introduces the main ideas and results of our recent work on the spiked matrix model of genetic priors we all know that the structure of data plays a very important role in machine learning tasks and knowing how to capitalize it can make a major difference in training a model in recent years generative models for data such as variational autoencoders and generative diversity networks gaining popularity this generative models exploit expressivity of neural networks to create low dimensional latent representations of data sets on general grounds would like to understand how can we use these generative networks to improve signal reconstruction for example say that we want to denoise a corrupted image of a face if I train again on a given data set of photos can I use it again as a prior to include reconstruction in our work we study these questions on an analytically tractable setting our playground to explore this question will be Rank 1 matrix factorization in this problem we are given a matrix Y generated from adding noise to a rank one matrix and the aim is to reconstruct the exact realization of this star knowing this context at the spike this model is what is a widely study as a proxy for principal component analysis and indeed PCA is an optimal algorithm when the spike is unstructured but what happens when V has a lower dimensional structure a popular way to model this is to take the spike to be sparse in this case it is not surprising the PCA is no longer optimal since it doesn't use any information about the sparsity many algorithms know under the umbrella of sparse PCA are able to improve over vanilla piecing however quite striking no non algorithm is able to reach the statistically optimal noise threshold in this case instead what if we take the low dimensional structure to be encoded by a deep generative Network can we do better spoiler our work show that yes we did Ivan rigorously proved a form of describing the performance of the Bayes optimal estimator for any distribution for the spike in particular when this price taking from the example of our unknown generative networks with iid weights our formulas values for any distribution of the latent variable and for any choice of activation functions we also derive an approximate message passing algorithm to estimate the spike from Y we show that the performance of our EMP algorithm can be tracked down exactly in the high dimensional limit in particular for the more kamaal textures with activation functions to be linear real or sign the MP performance coincide with the optimal performance of the bayesian estimator previously discussed in other words different from sparse PC there is no algorithmic to statistical gap for studied architectures we also proposed a simple and easy to implement spectral method derived from our MD algorithm ensure that it starts to correlate with the spike at a statistically optimal fresh wound in particular it always beat PCA in performance curiously our spectral method depends on the structure of the spike only through its covariance this suggests we can apply to arbitrary structured data by using the empirical variance instead we probe this experimentally by choosing spike from real data set fashion immunised surprisingly our spectral method beats PC even though the spike is not drawn from a generative model unfortunately time is short and this video is just a taster of the main ideas and results of our work if you got here is and wants to know more please do pop by our poster on Friday or send us an email to arrange a discussion see you in your ribs"
    },
    {
        "sourceUrl": "https://youtu.be/FkT1aNoKbG4",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/9d1827dc5f75b9d65d80e25eb862e676-Paper.pdf",
        "summary": "I'd like to talk to you about our paper at Europe's zero positive learning approach for diagnosing softer performance regressions so this work is part of our larger program around machine programming automating software development and it's described in something we call the three pillars of machine programming intention invention and adaptation this work here falls within that adaptation pillar hunt down this paper you won't be disappointed and take a look it's really great any rate regression testing you release software you modify programs and you don't know it but sometimes you might introduce a performance degradation and when you do that be kind of good if you caught it before you released it for example our good friends with the MySQL world in 5.5 they noticed a performance issue that all the threads were contending over a single lock they got really clever and came out a way to get around that by giving each thread their own lock and released the code and only later found out it was 67% slower it was slower because of false sharing they didn't have a system to catch this and do the regression testing they released slower code so it's very important to look around and find your regression problems your performance problems so auto proof is what we call our tool it uses zero positive learning auto-encoders and hardware telemetry to build an automatic regression testing system so here's an example you run your program you collect performance counters that's the hardware telemetric data you train an autoencoder and then you run the modified program you collect the same hardware performance counters and then you run it through the auto encode a auto encoder and see if you get back the the input and output match if they do then you say you don't have a performance bug if they don't within a threshold you set you soom you have a performance bug and take appropriate action so here's the results we we ran this with performance bugs we inserted into seven benchmark and open source programs and it's important to note we didn't get any false negatives so that's really cool so now we're comparing our auto perf at the blue line to a state-of-the-art automated performance regression system called UBL and then another threshold approach that we came up with it looks at the length of the input and output vectors around the auto encoder and what we're showing here is that for a number of threshold exact quality metric getting a very very high true positive rate without a high false positive rate so really good really successful we're really excited about it so it's a generalized software performance analysis system it's effective general scalable come hunt us down at nura and learn more about this it's really great thank you"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=B3T_mEb2NGA",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/50d2d2262762648589b1943078712aa6-Paper.pdf",
        "summary": "hello this is a three-minute presentation on write execute assess program synthesis with the repple program synthesis is a process where one turns a specification into a program which satisfies the specification in this work we consider two domains of program synthesis the first domain is the programmatic recovery of a pixel or voxel field the specification is a voxel field and once the program is recovered it can be rendered under different views in the second domain the spec is a set of text manipulation input/output examples which was the program is synthesized can then be run on different input program synthesis is useful in two ways as the last slide shows it can be directly useful in addition it is a good formulation of general structural induction tasks such as semantic parsing and even modeling human handwriting's program synthesis can be difficult in this work we focus on addressing two difficulties one there is a syntax semantics gap where a small change in syntax can caused a huge change in semantics to the issue of compounding errors in synthesizing long programs to address these two difficulties we use a rebel here's what program synthesis with the rebel looks like first a code writing policy writes a piece of code second a code executing repo execute the code last a code assessing value function looks at the execution and see if we're on the right track by using the repo we address the difficulty of the syntax semantics gap by exposing the semantics of the partially written code as a rebel state we use SMC to address the difficulty of synthesizing lung programs by using the value function on the report state we can better guide the search by reallocating search budgets toward publishing partial programs qualitatively we show that the repple and the value function are important components for successful synthesis where our approach outperforms just a beam search and policy rollouts which do not use the values and we also outperform the note rapport baseline here are the quantitative results showing our approach indeed or performs a baseline that do not leverage the rebel and a learn value function thank you"
    },
    {
        "sourceUrl": "https://www.youtube.com/watch?v=z89BTMQGVng",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/b33128cb0089003ddfb5199e1b679652-Paper.pdf",
        "summary": "hello my name is Tristan Milne and this is a short summary of my paper piecewise strong convexity of neural networks in this paper we consider a feed-forward neural network Y with parameters W a scalar output and r lu nonlinearities for a target function f we take the loss function as 1/2 the mean squared error over a data set if the network has more than one layer this function is non convex surprisingly stochastic gradient descent works well on it so how bad can it really be in this paper we obtain some structural results for this loss function regularized with weight decay we will use L lambda for the regularize loss function our first result is that L has no differentiable local Maxima this follows by showing that the laplacian of L is non-negative and then using the maximum principle for sub harmonic functions this means that the loss function can't look like the plot on the left which has a differentiable local maximum it can look like the function on the right however which has a non differentiable local maximum our second result is that L lambda is piecewise strongly convex on the set u lambda theta here C depends on the architecture and data set H is depth the star norm is the largest spectral norm of the weight matrices and L lambda has a positive definite Hessian on you lambda theta does you lambda theta matter at all though well with some conditions it contains all global minimizer's of L lambda so this result describes the structure of L lambda around its global minimizer's how similar are piecewise strongly convex functions and strongly convex functions we show that they share some important properties from an optimization perspective first every differentiable critical point of a piecewise strongly convex function is a local minimum second local minima are isolated from each other in other words they are locally unique whereas for a strongly convex function minimizer's are simply unique we also include an experimental sex where we validate our theoretical work by showing the gradient descent does enter you lambda theta for a toy problem we also analyze three classic image classification problems and show that the loss function on SGD trajectories is almost always piecewise strongly convex to give context for our work will mention some related papers these three papers include strong results for mostly linear networks these two papers include similar results to ours for nonlinear networks with restricted architectures usually with a single hidden layer our contribution is to the understanding of L lambda for nonlinear networks with any architecture and any data set I'll be it on a subset of the weights thanks for watching"
    },
    {
        "sourceUrl": "https://youtu.be/xVxSu7KGtHw",
        "sourceType": "Conference",
        "linkToPaper": "https://proceedings.neurips.cc/paper/2019/file/d9fbed9da256e344c1fa46bb46c34c5f-Paper.pdf",
        "summary": "in data parallel optimization of deep learning models several workers compute gradients on their own batches of data after computing them they need to share what they found with each other but because the gradients they compute are typically hundreds of megabytes large this communication limits the scalability of distributed training one way people have dealt with this issue is to apply lossy compression to the gradients before sending them around a popular scheme is to only send a sparse set of the gradient coordinates and another approach is to quantize the coordinates values barosky D compresses gradients differently we see the gradients as a matrix its columns correspond to the layers input features and the rows correspond to output features we compress this matrix using a low-rank approximation you could compute this using a singular value decomposition but that is very expensive so instead power SGD computes a cheap approximation to this factorization using just one step of power iteration ok let's take a look at communication in distributed gradient descent a normal uncompressed SGD workers can quickly settle on their average gradient by computing it in a hierarchical fashion using all reduce unfortunately you usually can't use all reduce for compressed gradients if you have to one bit gradients for example then you cannot average them into a new one bit gradient without additional approximation so for that reason compressed algorithms resort to less scalable all-to-all communication or use a parameter server power std remarkably combines the benefits of compression with all reduce let me show you why to compute the low-rank approximation we apply matrix multiplications with the average gradient across workers but we can actually avoid the heavy communication required to compute this average gradient because of linearity each worker can equivalently do the computation on their own in gradient and we average they're much smaller outputs instead because this is a normal averaging operation per se D enjoys all the benefits of all reduce now even though a sloppy Rank 2 approximation to gray of science thousand five thousand is quite inaccurate porosity still converges that is due to error feedback with error feedback every time workers compress a gradient they memorize the errors they made locally and the next time a grain is compressed they add this error to the gradient before compression this means that all of their gradient will be used eventually to sum up power STV can achieve very high compression ratios and we find that these reductions in communication can result in significant end to end training speed ups to achieve the same test accuracy as normal SGD the best part is simple you can start using power HTV by plugging it into your existing optimizer you shouldn't have to modify the current type of parameters with a high enough rank you should be good to go if you're interested have a look at our paper or the code on github"
    }
]