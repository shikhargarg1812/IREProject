[
    {
        "video_id": "c_bVBYxX5EU",
        "video_title": "Building an Image Captioner with Neural Networks",
        "position_in_playlist": 0,
        "description": "Let's build an Image Captioner using only neural networks. In this video, we're going to get you thinking like an AI researcher. Instead of a \"Here is the state of the art. Here is how it works\", we'll go for a \"Let think about this from scratch. Oh! We just derived the state of the art!\"\n\nCode for this video: https://github.com/yunjey/show-attend-and-tell/blob/master/core/model.py\nMain Evaluation Notebook: https://github.com/yunjey/show-attend-and-tell/blob/master/evaluate_model.ipynb\n\n#deeplearning\n#machinelearning\n\n\nREFERENCES\n\nShow & Tell: https://arxiv.org/pdf/1411.4555.pdf\nShow, Attend & Tell: https://arxiv.org/pdf/1502.03044.pdf\nCompeting with Show & Tell: https://arxiv.org/pdf/1411.4555.pdf",
        "transcript": "image captioning given an image we want to summarize the image in a phrase or in a sentence what I usually do in my videos on such concepts is we would take some state of the art paper and try to dissect their approach but I want to do things a little differently this time let's try to devise our own approach to image captioning and compare it to the state of the art the goal of doing this kind of exercise is to get you thinking like an AI researcher so that you can come up with similar architectures for any problem and in the process understand that those researchers aren't really super human so we have a problem of image captioning and we want to determine how to use neural networks to solve it and we're doing this from scratch using our understanding of neural nets when you hear the term neural network what do you think perhaps an interconnection of neurons that takes raw data as input performed some hocus pocus in the middle and spits out some probability and classification problems or some real values in case of regression problems this notion of neural nets isn't incorrect but with that understanding can you really say what these layers represent what exactly are these layers well to understand this it's better to think of neural nets from a more mathematical perspective there are basically mathematical functions that transform one kind of variable to a variable of another kind it could be vectors to vectors as we would see in classification problems or vectors to scalars as we would see in regression networks we treat each of these interconnections seen in every layer as a transformation on the input so each layer is simply the vector representation of the same input this is going to be important in some time so keep this in mind let's define the structure of our problem now identifying the inputs and the outputs the input to an image captioner is some kind of image like a matrix or a tensor the output is a sentence basically a sequence a sequence is a set of variables that has some defined ordering to it sentences or sequences because one word has to come after another and in that order to have some meaning I said before that neural nets are mathematical functions that map one kind of variable to a variable of another kind now if one of those variables is a sequence then we get a recurrent neural network or at least that's the first thing we would think of in such problems so because of output sequences of the image captioner you may think of recurrent neural networks coming into play like any other network it helps to think of recurrent Nets as mathematical functions that map sequences to vectors vectors to sequences and sequences to other sequences image captioning would fall under a vector to sequence representation sure the input image isn't exactly a natural vector but the output sentence is most certainly a sequence cool so we have the last part of our architecture a recurrent neural network let's now take a look at our input it's an image to feed this to our neural network we need it in some form of vector format first thing that comes to mind is simply flattening the input image so the matrix or tensor becomes a one dimensional vector now this works but this image representation is pretty sparse a better way to represent an image is through convolutional neural networks consider the basic convolution neural network architecture this is the Linette five architecture with the basic convolution activation and pooling layers followed by fully connected layers if you want intuition on each of these layers you can check out my series on the convolutional neural Nets but you really don't need to know all that to understand one about to do here here instead of thinking of these layers as some complex transformation let's do the same thing we did before with the recurrent neural nets and the vanilla neural networks and think of each of these transformations as some mathematical function at each layer we are just chaining functions performing transformations on the same input so this layer is the condensed matrix or tensor representation of the image and what is this layer it's basically the dense vector representation of the input image this holds true for any network where we have a sequential flow of information that is where all information passes through the layers so we can pass an image into a CNN to get a dense vector representation of the image then we can pass this dense vector to an RNN to generate a sequence the sentence or phrase that describes the image so nice our architecture is now taking some form but there are some tweaks we can still do for example we can take into account the meanings or the semantics of the captions instead of just treating them as raw numbers so how do we do this our recurrent neural Nets are typically trained using a mechanism called teacher forcing that is the correct labels are used to train the recurrent Nets for the next state this is done to ensure the back propagation through time algorithm doesn't become super expensive in case you were wondering the truncated be PTT algorithm is a method of training recurrent neural networks the output for our case is the words of the caption each word can be represented by a one hot encoded vector in teacher forcing we would feed this one hot vector of the previous word in the caption directly to the next iteration but let's be smarter about this instead of simply feeding the word in the next iteration we can learn a set of embeddings w e now w e is a set of word embedding vectors that incorporates the meaning of a word and the closeness to other words in terms of meaning and semantics if you are using this on specific types of data it is good to learn these embeddings during the training of your LS TM Network simultaneously instead of feeding some word s T in the teeth iteration we would feed a vector W est that incorporates the meaning of the word in this way the image captioner has knowledge of language while generating captions so that's awesome we now have an architecture that converts an image to a caption using neural networks and to end but you may be thinking this is great at all but what is a state of the art image captioner what is the forefront of AI research and my answer to this is well you're looking at it this architecture that we talked about just now is the state of the art and is the basis for the paper show-and-tell for image captioning we show an image to the CNN part and then the RN part of the architecture generates captions to tell us what the image is about hence show-and-tell if you understand everything I just said you've understood show-and-tell so Congrats but can we go beyond this let's throw attention in the mix just because we can attention involves focusing on certain parts of an image while generating different words of the caption this can help create more detailed sentences of an image so how do we do this let's come up with the architecture for attention intuitively in our previous architecture we took the dense vector representation of the image from the FC layer of for CNN but attention involves looking at different spatial regions of an image so it makes sense to get a tensor representation of an image to preserve spatial features to do this we can use any of the convolution output layers remember convolution activation and pooling are just mathematical functions applied to an input so the output of any of these operations is a tensor representation of the input itself this tensor has L regions where each region of an image is represented by the vector AI there are two types of attention that we can perform soft attention and hard attention soft attention involves constructing a word considering multiple parts of an image to a different degree ZT is the context vector to generate the teeth word for an image think of it as the parts of the image to concentrate on while generating a specific word alpha TI is a strength or a probability value that ranges in 0 to 1 its magnitude is basically the amount the image captioner should focus on the region I to generate the teeth word the other type of attention is hard attention instead of a value that determines how much of the image part to consider each part of the image is either completely considered or completely disregarded while generating a specific word STI takes on a binary value of 0 or 1 if 1 it means the i3 j'en is considered while constructing the teeth word otherwise it isn't now let's get back to our architecture we changed the Show and Tell architecture by taking a convolution output instead of the FC output but we need a vector input to our RNN we extract a context vector Z using our attention mechanism in this way we can generate words considering different parts of an image this architecture that we have here is now the show attend Intel Architecture and this is considered the forefront of recent research on visual attention we show the image to the CNN focus our attention to specific regions of a name and then tell the caption using the RNN show attend intel it's as simple as that let's take a look at some code this here is the show part where we show our image to the convolution neural network consisting of a set of convolution activation and pooling layers the attend and tell part is a part of our caption generator class so it is here that we build the LS TM model this is done while considering the word embedding as the input and for every word getting the set of alphas and context vector this method bill sampler will allow us to generate the caption itself the results are pretty slick the image captioner is able to generate meaningful captions for the input images note that these results are for soft attention but it can be modified easily for heart attention if you want to know more about soft and heart attention I've made a video on visual attention from a different perspective so check that out I hope this video gave you an intuition on how to think about neural networks allowing you to create them from scratch AI researchers are humans too they just happen to get ideas before anyone else thinking about neural networks mathematically really does help and I hope you can see why thanks for watching and if you liked the video hit that like button hit that subscribe button share the video with friends family acquaintances your next-door neighbor perhaps and I look forward to your support",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "GcvGxXePI2g",
        "video_title": "DropBlock - A BETTER DROPOUT for Neural Networks",
        "position_in_playlist": 1,
        "description": "Dropout is a common method of regularization in neural networks. However, it doesn\u2019t work too well in Convolution Neural network Architectures. We are going to understand why this is this case, and offer an alternative approach to regularization: DropBlock.\n\nHit that SUBSCRIBE, ring that BELL, share this content, like and comment down below your video suggestions. I don\u2019t get many so I\u2019ll read them all.\n\nFOLLOW ME\nQuora: https://www.quora.com/profile/Ajay-Halthor\n\nREFERENCES\n\n[1] DropBlock (Main Paper): https://arxiv.org/pdf/1810.12890v1.pdf\n[2] Neural Network Playground: https://playground.tensorflow.org \n[3] AlexNet: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n[4] Pytorch Code for DropBlock: https://github.com/miguelvr/dropblock/blob/master/dropblock/dropblock.py",
        "transcript": "this here is a good old feed-forward neural network we've thrown an input from the left the network then learned some representation in the middle and it spits out some output values on the right some prediction fundamentally at every neuron we just take the weighted sums of all the incoming neurons from the previous layer we can throw in activation functions in every neuron - this allows the network to learn more complex patterns in the input and in this way the entire input image or audio or text propagates throughout the network but in 2012 Alice crew SEFs key came up with a technique to improve the performance of four neural networks originally when you input an image the information reaches every neuron so every single wave edge in the network is influenced by every single input image in the training set but is this a good thing you could argue that more samples that contribute to the weight calculation the better but we end up with a network where every neuron is trying to learn a representation of everything so in the end you end up with neurons that learn something that its neighbors already know and some representations that may be unique are very weak because of the influence of hundreds of thousands of other samples khrushchev ski was able to get the highest performance on image classification with convolution neural nets using a simple technique drop out give every neuron a probability of being fired some bro then flip a biased coin with that probability if heads the neuron is left alone if tails we turn the neuron off we flip a coin for every neuron while inputting a sample for training in math it's equivalent to sampling from a Bernoulli distribution with a parameter Rho so the image only affects selected active neurons every time the result is that every edge weight is now influenced by different sets of inputs hence they can learn their own representation this same network with the same said if neurons can now find a better pattern in data without as much overlap from the other neurons so yeah drop out simple enough right but even though drop out was introduced originally in convolution neural networks we see them more in traditional deep learning architectures why is this the case well in convolution neural networks the spatial orientation of the input is preserved take the convolution operation itself convolution is the sum of element-wise products between a sliding window and a filter the result of convolution operation for a pixel value depends on the value of its neighbors spatially let's try applying dropout for an input image say we turn some neurons off initially the result of the convolution operation may not have the entire representation that's fine but say we perform a pooling operation or another convolution operation after this there's a good chance that all the input image information is still transferred to this layer so dropout isn't even serving its purpose and the model still tends to overfit to prevent this researchers at Google brain came up with a way to regularize convolution neural networks such that pixel information doesn't propagate and this technique is called drop block instead of randomly turning off individual neurons in a layer we randomly turn off a cluster of neighboring neurons in that layer this would mean that even with the convolution operation there are some parts of the input that don't propagate in the network this is the effect we want because now every neuron learns different regions of every sample so the model can generalize better makes sense right for those curious more about the mathematical intuition let's dive deeper into the algorithm say we want to perform some convolution operation the input would be a set of activations from the previous layer for now let's say it's a 10 cross 10 grid if we want to incorporate drop block the type of parameter we need to decide is the size of the region to turn off this number is less than the size of the grid obviously let's at this number to be 5 in other words if we want to turn on or off a neuron we turn on or off a 5 cross 5 patch with that neuron as a center we are now going to construct a binary mask a binary mask is basically a grid equal to the dimensions of the input from the previous layer but the values can only take on a 0 or a 1 in this mask these six cross six cells can be the center of a 5 cross 5 region iterate over every one of these squares and flip a biased coin if heads leave it alone if tails things get a little interesting we don't just turn the single neuron off but we turn off the 5 cross 5 region with that neuron as a center this way we can create a mask for neurons activated after applying the mask to the neurons by element-wise multiplication we have spatial regions turned off and this prevents information flow clearly draw block is parameterised by two variables the first is the block size which is the width and height of the region to turn off we set it to 5 in our example and the gamma value which is the parameter of the Bernoulli distribution the bias of the coin flip I was talking about we can then set gamma with this formula typically it's a value between 0.75 and 0.95 the results when applied to different architectures are significant we see up to a two percent improvement in various convolution neural network architecture is like ResNet and amoeba net let's take a look at some PI torch code to implement this this isn't my own code but it's a pretty good example to get your hands dirty this cross drop block has two arguments in its constructor these are the same ones that I mentioned a drop prob is basically gamma the variable of our Bernoulli distribution and block size is the size of the block to region to turn off it was 5 in our example the fork method takes an input it computes gamma using that awesome formula and it computes the mask if we consider the 10 cross 10 grid with a 5 cross 5 block we're going to sample from a Bernoulli distribution 6 times 6 which is 36 times per input image to generate the mask we then apply the mask to switch ON or switch off the neurons the same principles applied to 3d images as well drop block is a very interesting tweak in neural network architectures that actually makes sense intuitively and should be used more with models where input data exhibits some spatial correlations I have a few videos on convolutional neural networks explaining how they work and some training applications in computer vision so be sure to check them all out there is always something interesting coming up and I'm hoping to cover as much as I can thanks for stopping by today hit that like smash that subscribe bring that they'll share the video whatever with whomsoever and I'll see you in the next one",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "xPCCyiw8M2U",
        "video_title": "Curiosity in AI",
        "position_in_playlist": 2,
        "description": "Reinforcement learning generally uses a carrot-and-stick approach. Good actions are rewarded and bad actions are punished. But what are the drawbacks of this simple approach? How can we use curiosity to overcome it? Let's find out in this video! \n\n\nREFERENCES\n[1]  Main Paper (Episodic Curiosity through Reachability) : https://arxiv.org/pdf/1810.02274.pdf\n[2] Blog: https://ai.googleblog.com/2018/10/curiosity-and-procrastination-in.html\n[3] AI learns to walk: https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/",
        "transcript": "reinforcement learning a method of learning where an agent is given positive rewards for correct moves and penalized for wrong moves this simple concept forms the basis for modern AI breakthroughs that we've seen over the past years alphago 0 for example learn the ancient game of go from scratch by continuously playing against itself and within a matter of 40 days it surpassed every human and computer to become the best go player in the world deep queue networks incorporate cue learning and neural networks to enable agents to learn and complete tasks in a variety of environments however there are some disadvantages of this carrot-and-stick approach an approach where every action is either rewarded or penalized what if you are nowhere near your objective then an action doesn't have a properly defined reward or punishment a sequence of such actions can cause an agent to just go around in circles for example consider the case of an agent in a supermarket and say it's trying to look for some cheese let's also assume that the agent has no knowledge of the environment and it starts in a lane containing meat an action by the agent could be to go to the next aisle say the agent now sees fish in the next action the agent might try to go to the next aisle again but there's also nothing stopping it from going back to the previous meat aisle if not in the first iteration the Asian might eventually get stuck checking the same aisles over and over again so what happened here when the aichi moved to the fish aisle it didn't actually seem closer to the cheese so there was no defined positive reward but we know intuitively it makes sense for an agent to continue exploration instead of going back to what it is previously seen on October 24th Google dropped a blog and paper on how we can make an agent Explorer using curiosity the idea is to have an additional reward type besides the reward for getting closer to an objective the agent should be rewarded for discovering new parts of the world so how does this help when an agent starts at the meat section and sees no cheese it moves to the fish section however it still sees no cheese at this point the agent has two options it can either move back to the meat section or it can move to the next aisle this time the agent would most certainly move to the new aisle why because it gets a reward for seeing a new part of the supermarket the pots and pans section moving back to the meat section the agent gets no reward because it's already seen it this reward for seeing new parts of the world is equivalent to rewarding curiosity in human beings question how does the agent determine whether it has seen the same place before the agent keeps in its memory snapshots of the states that it has seen however comparing the memory directly to the observations doesn't help much you may be looking at the same room in different angles instead we can train a neural network that takes as input the current observation and the previous states in the agent memory the model can then estimate the number of steps it needs to observe the same state again or the output can be a binary classifier that states whether the observation is reachable in some case Tepes or not reachable in some case Tepes the results are pretty astounding we now have an agent that explores the world and no longer loops in a corner so how is this better than past attempts in past models surprise would merit reward instead of just curiosity every time and model took action it would predict the resulting state if incorrect in its prediction then it would be rewarded this would intuitively entice a model to explore the parts of the world that it predicted incorrectly however this surprise based reward system has its own drawback it would get stuck when encountering a TV and why would this be the case say a model goes from state a and some aisle to a state be facing a TV seeing a TV for the first time the model would have never thought it would encounter such a state so the surprise reward brings the agent to the state B now the agent has several options it can either move forward move back or stay in the same position on staying in the same position the TV flips channels randomly this is once again not something the agent would have predicted and hence it's surprised and rewarded for staying in the same place this cycle repeats because the agent would never be able to correctly predict the state of the TV which is also random so it gets stuck in front of the TV and no longer progresses in the original objective of finding the cheese this is similar to procrastination in human beings the episodic memory based curiosity we discussed before however can avoid this consider the agent is in front of the TV every time the channel flips the agent observes a new state and stores it in memory however since the number of channels is finite once all channels have been flipped the agent is lo longer rewarded for seeing the same channels and it will move forward looking at parts that were unexplored this research introduces a new fundamental concept of environment interaction behaviors that can be useful in reinforcement learning research looking forward to read more about that in the future check out the paper on the blog post and the links down below when I got the Twitter notification for this paper I just had to make a video on it hopefully now the paper becomes more accessible now hit that like button hit that subscribe button ring that bell for notifications share the video do what needs to be done and I'll see you in the next one bye bye",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "NyAosnNQv_U",
        "video_title": "Unpaired Image-Image Translation using CycleGANs",
        "position_in_playlist": 3,
        "description": "We talk about cycle consistent adversarial networks for unpaired image-image translation. Some image-image translation problems include:\n- Season Transfer\n- Object Transfiguration\n- Style transfer\n- Photo Enhancement\n\nIf you like the video, hit that like button. Wanna see content like this AI, Machine learning, Deep Learning Data Sciences, hit that SUBSCRIBE button. For instant notifications when I upload, RING that BELL.\n\nOTHER COOL VIDEOS\n- Generative Adversarial Networks: https://www.youtube.com/watch?v=O8LAi6ksC80\n- CNN Architectures: https://www.youtube.com/watch?v=m8pOnJxOcqY\n\nSOCIAL LINKS\nFollow me on Quora: https://www.quora.com/profile/Ajay-Halthor\nEmail: ask.ajhalthor@gmail.com\n\nREFERENCES\n\n[1] Main Paper: https://arxiv.org/pdf/1703.10593.pdf\n[2] Blog for architecture (Code too): https://hardikbansal.github.io/CycleGANBlog/\n[3] Architecture borrowed from here: https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf\n[5] ConvNets are PatchGANs: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39\n[6] Network Architecture is heavily based on Conditional Adversarial Nets: https://arxiv.org/pdf/1611.07004.pdf\n[7] pytorch implementation of CycleGAN: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix",
        "transcript": "take a look at this famous painting by Monet of the bank of Seine near audience way France even without knowing what it is we can all agree it is a painting of someplace if photography had been invented in 1873 that is when this painting was painted what do you think the scene would have looked like perhaps something like this this is an example of style transfer where we synthesize a photo style image from a Monet styled painting style transfer works the other way to here's a photograph of the little cassis harbor and France clearly this was taken after Monet's time but if we were alive in the 20th century how do you think Monet's rendition of the scene would look like if you've seen any of his previous works then you may think it looks something like this now consider this picture of a horse just galloping in the field how common is it to see I don't know a zebra gallop in the field not as common right oh look we just made it happen by replacing the horse with the zebra this is an example of object Transfiguration now take a look at this gorgeous summer landscape how do you think the same scene would look in the onset of winter perhaps something like this an example of season transfer in all of these examples we saw an image and imagine how it would look in different circumstances and in this video we're gonna take a look at exactly how we can implement this using a cycle consistent adversarial Network I'm Ajay how Thor and you're watching so we saw some cool examples of what exactly we want to do however to solve this problem we need to actually look at a much broader perspective we need to somehow map the input image coordinates x-two domain coordinates Y and this problem is image to image translation if you've been in computer vision for even a little while you'll know this problem isn't really a new one here's a dozen papers that have been the problem to death but every single one of these uses paired image data their models are trained on both the original image and the corresponding acquired image after translation but creating such a data set is a pain and existing data sets are usually too small to be of any use hence we are looking for an algorithm that works on unpaired image data where we have a set of photo style images X and we have another set of monet style paintings Y but we don't have access to Monet paintings for every single input sample image such data is much easier to gather we assume there exists a mapping between images X to its corresponding image and Y our goal is thus to train a model to learn this mapping G a typical objective we use to train the scan or rather learn the mapping G is an adversarial loss this forces the generated images to be indistinguishable from the real images y so let's map this out an image Y hat is sampled from the generator G parameterised by theta G the distribution of real images in Y is represented as say by P of Y the goal of minimizing an adversarial loss or the goal of optimizing any Gann is to model the generator G such that the image sample from it is indistinguishable from the actual distribution but matching distributions in this case isn't enough remember we don't have access to pair data there are many parameters data G that could potentially minimise the difference in distributions so the chance of learning a mapping G that makes meaningless pairings between the input images domain X and the output domain y is very high this leads to completely meaningless results in order to reduce the number of possible mappings G that can be learned we introduced a second type of loss this is called cycle consistency loss here's the idea if we translate for example a sentence from English to French and then translate it back from French to English we should arrive back at the original sentence inner image to image translation problem we introduce another mapping F which is the inverse of G that is it maps an image in Y to some image in the X domain so we not only need a mapping of G that generates similar distribution but we also need one that is cyclo consistent with respect to its inverse mapping F this significantly reduces the number of such possible mappings G can take now that you have a high level intuition of these two types of losses let's derive them mathematically but before doing so I'm gonna introduce some notation since we have two mappings to learn G and F we have two gans to train where each has a discriminator and a generator the generators actually generate images for a given domain G will generate images in the Y domain and F will generate images in the X domain discriminators distinguish between the real images and the generated images let dy be the discriminator that distinguishes between the images in the Y demesne and the ones that were generated by the generator G of X let DX be the discriminator that distinguishes between the images in the X domain which are real images and the ones that were generated by the generator f so you can say that Gann one for the X 2y mapping is the G dy pair and again two for the y 2x mapping is the F DX pair now that we have some notation let's start deriving the adversarial losses we have two gains so two adversarial loss is to compute first consider the G dy pair for the discriminator each input sample has to be classified as either real or generated will model the parameters up again theta G that maximizes its performance using maximum likelihood estimation each sample comes from either the original output space Y in which case the corresponding label would be real or it may come from the generated space G in which case the corresponding label would be fake each sample is assumed to be independently and identically distributed that is iid so we can write it as a product of probabilities we can further break this down into K classifications TN is a one Haughton code factor that corresponds to the true label of the input X n now consider the log-likelihood denoted by the little L and expand the inner Sigma over K remember this is a binary classification where K can take two values zero for generated data and one for real data for any sample xn only one of these terms is nonzero so why is that it's because TN is one hot encoded hence we can separate real data samples in Y from generated data samples in G making a substitution for the discriminator notation we get the following form we can approximate the value taking the expectation over both terms this is the likelihood that the discriminator dy seeks to maximize and the generator GX seeks to minimize remember that theta G represents the parameters of Gann one so that's the parameters of both the generator G and the discriminator dy let's put that in there so that you don't get confused we can derive a similar likelihood expression for the second gun FD X and determine its parameters let's do this real quick so that you get the hang of the math we are determining the adversarial loss of the second gun with the generator discriminator pair FD y the likelihood estimation is initially the same as before before moving on I want to point out the x and y using this part of the likelihood derivations are these sample inputs to our network so X is the input image and Y is the output label which is either real or fake but in other parts of this video I use X and y to represent the input and output image domains I'm sticking to this notation because that's what you would see in most other papers as well just want to point this out so that there's no confusion once again we assume that the input samples from the image domain X and the generated images from B generator F are iid so we can express them as a product we break this down into K class classification using TN as a one hot encoded vector to signify the true values like we did before we then take the log-likelihood to make the expression easily to compute because sum of sons is easier to compute than product of products this is a binary classification where images are either real 1 or generated 0 we can now separate real data from the set X from the generated data that is from F of Y making the substitution for the discriminator notation we get this following form and we can approximate the values taking the expectation over both terms theta F is a set of parameters of the second gun that needs to be computed by maximizing this likelihood since it is a set of parameters that the discriminator DX needs to maximize and the generator F needs to minimize let's write this in the form of a minim ax objective combining the objectives for these two gains we get the overall adversarial objective the first term is computed when the X domain is the input and Y domain is the output while the reverse is true for the second term let me just include this to distinguish between the two this is the adversarial objective and the adversarial loss is just the negative of this value that is the negative log likelihood hope this derivation clears things up let's talk about the second type of loss that I mentioned before cycle consistency loss like I said for adversarial losses since we have two gains to Train we have to cycle consistency losses and we'll call them the forward cycle consistency and backward cycle consistency for word cycle consistency is established when the source image in X matches its transformation after applying G followed by its inverse F similarly backward consistency is established when an image in the output space Y is retained when F and its inverse G are applied in succession we can define both losses as a measure of the l1 distance the overall loss is a linear combination of both the adversarial loss and the cycle consistency loss lambda will control the relative importance of the adversarial losses now solving these together we find them two mapping functions G and F so now we know exactly how to compute the losses but what exactly is the generator and the discriminator like what are its components the generator follows an encoder decoder architecture with three main parts the encoder transformer and decoder the encoder is a set of three convolution layers so it takes an image input and outputs a feature volume the transformer takes the feature volume and passes it through six residual blocks its residual block is a set of two convolution layers with a bypass like I mentioned in the resident architecture in my video on various CN n architectures this bypass allows a transformation of earlier layers to be retained throughout the network hence we can build deeper networks effectively you can think of the transformer as 12 convolution blocks with bypasses now the decoder is the exact opposite of the encoder it takes a transformer input which is another feature volume and outputs a generated image this is done with two layers of deconvolution or transpose convolution to rebuild from the low-level extracted features then a final convolution layer is applied to get the final generated image the discriminator is a simple architecture it takes an image input and outputs probability of whether it is a part of the real data set or the fake generated image data set this architecture is a patch gun it involves chopping an image input into 70 cross 70 overlapping patches running a regular discriminator over each patch and averaging the results that is determining overall whether it's either real or fake but we can implement it as a confident more specifically a fully convolution network where the final convolution layer outputs a single value training this against the loss function that we discussed the cycle gans produced remarkable results on various translation problems let's first compare this to pics depicts which was trained with a conditional again that used a fully paired data set not only is it able to create the sketch of photo translation like pics depicts it does a decent job in generating sketches from the image we can perform style transfer transforming a picture into works of art in any artists style like Monet or van Gogh we can also perform object Transfiguration in these images we have replaced all zebras with horses and all horses with zebras we can perform seasonal transformation here the images of Yosemite and summer have been translated into winter images and vice versa photo enhancement we map iPhone camera pictures to DLSR images so we can observe a depth of field effect for absolutely stunning images so what did we learn cycle consistent adversarial Nets are a type of gun that can be used to solve image to image translation problems without paired dataset we defined and derive the gans objective the loss is divided into two parts adversarial losses and cycle consistency losses the architecture of a cycle gang consists of two generator networks to generate new images and to discriminator networks to distinguish between the real and generated images the generator network consists of three parts an encoder which is three conv layers a transformer which is six procedural blocks and a decoder which is 2d conv lares followed by a con flare the discriminator networks are patch gans which essentially can be implemented as fully convolutional networks the cycle consistent adversarial Nets can solve image to image translation problems like object Transfiguration photo enhancement style transformation and seasonal transformation and that's all I have for you now if you liked the video hit that like button if you like content like this in AI deep learning machine learning and data Sciences then hit that subscribe button for immediate notifications when I upload ring that little bell links to the main paper and other sources are down in the description below so check them out still haven't had your daily dose of AI click or tap one of the videos right there it'll take you to an awesome video and I will see you in the next one bye",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "BeYbQkbKox8",
        "video_title": "AI creates Image Classifiers\u2026by DRAWING?",
        "position_in_playlist": 4,
        "description": "In this video, we talk about \"Sketch-a-Classifier\"  released by researchers at the university of London.\n\nKEYWORDS\n1. Zero Shot Learning\n2. Model Regression Networks (MRN)\n3. Parametric Model\n4. Multilayer Perceptron (MLP)\n5. Fully Convolutional Network (FCN)\n6. Regression Loss\n7. Performance Loss\n\nREFERENCES\n1. Sketch-a-Classifier (main paper): https://arxiv.org/abs/1804.11182\n2. Sketchy: https://www.cc.gatech.edu/~hays/tmp/sketchy-database.pdf\n3. Imagenet: https://www-cs.stanford.edu/groups/vision/pdf/ImageNet_CVPR2009.pdf\n\nFollow me: https://www.quora.com/profile/Ajay-Halthor\n\nHit that SUBSCRIBE button and ring the bell for instant access to my videos on Machine Learning, Deep Learning, Data Sciences and Artificial Intelligence!",
        "transcript": "ever since the first imagenet competition ILS VRC in 2012 deep learning computer vision have come a long way we have new architectures with better performance all to solve the problem of image classification despite the changes in architecture the core process remains the same it take a bunch of labeled images of different categories you show it to your classifier for training and eventually test the classifier on unseen images although it seems that we are on the brink of solving image classification there's still one fundamental flaw with all of these architectures and this flaw is actually to do with the process of image classification itself so what if we don't really have all images of every single category in the example of classifying animals what if we don't have enough labeled samples of squids or giraffes what do we do then well our image classifier certainly won't know I'm AJ Hathor and in this video we're gonna take a look at exactly how we can build or rather synthesize an image classifier without having every category sample of an object so let's get to it let's restate our problem here we need to create an image classifier without having some samples of some object categories so like in the example of creating an animal classifier we want to create an animal classifier without having as many giraffe samples or probably no giraffe samples so one way we could possibly do this is zero shot learning that is learning from zero examples instead of training the classifier with an image we can input a description of that object this description can be text word vectors or any other input type and such types are called modalities so we are now able to train our classifier by describing the objects for which we don't have a labelled image category for so does that solve our problem well not quite how do we know which features to define and how do we know what to call them there could be name ambiguity when many people are contributing to the descriptions so instead of actually describing an object category what if we were to draw it this would remove the problem with a naming ambiguity and they do say that a picture describes a thousand words the paper sketch a classifier released by researchers at the University of London does exactly that we're going to discuss three types of models that leverage drawings or sketches to synthesize image classifiers the first model converts a sketch classifier to a photo classifier the second model uses a sketch or some sketches to synthesize a photo classifier and third model uses some sketches and a photo classifier to synthesize another more fine-grain photo classifier these models that do the magic are called model regression networks or mr ends notice the goal of an MRN is to generate an image classifier let's take a look at the three ways of doing this using sketches with some math let the mrn be a parametric model f parameterised by big theta we also let the input sketch classifier be a parametric model little F parameterised by theta s and the output image classifier is a parametric model little G parameterised by theta P they all have their own parameters that need to be learned parametric models make life easier because the problem of determining a model is reduced to the problem of determining their parameters we train a sketch classifier to get the parameters theta s this is input to our M RN and trained to get the parameters of the photo classifier theta P since the output of the M RN is an approximation I put a cap on it since we have theta P and the photo classifier is parametric we have essentially synthesized a photo classifier now instead of an entire sketch classifier what if we the input of the M RN is just a sketch or a few sketches let's represent this sketch by Sigma parameterize by the feature extractor fine we feed case of sketches to train the M RN in order to generate a photo classifier now the third type of M RN takes a trained photo classifier H parameterize by theta pH and a sketch Sigma to synthesize a more fine grained classifier you're probably thinking why are you using a photo classifier to generate another photo classifier well the input classifier is more generic it could be a bird classifier for example but when input to the M RN with the sketch of say a swan then we can make it a swan classifier that's pretty neat right I talked about the mrn and it's three types but what exactly is the model regression network that is what is the mrn to synthesize a binary classifier the mrn is a multi-layer perceptron and to synthesize a multi class classifier the mrn is a six layer fully convolution network let's now determine the objective function remember the output of an M RN is an image classifier but that is defined by the parameters if theta P is the original classifier and theta P hat are the parameters predicted by the M RN then the loss is a simple else.you norm of the difference between them but there is a problem with this intuitively the difference between the parameters doesn't necessarily signify the distance between the models themselves a small difference in weights could lead to drastic changes in the results so to solve this problem instead of just comparing the parameters of the synthesized model and the actual photo model let's actually compare their results or their performance and we can model this or measure this using a performance loss let Y be the ground truth and Y hat be the output predicted by the mrn synthesized image classifier for multi-class classification Y is a one hot encoded vector and Y hat is a vector of probabilities the loss is computed with good old cross-entropy the overall loss is thus a weighted sum of these two losses the regression loss and the performance loss and we need to find the parameters of theta that minimize this result this paper uses an alpha that's equal to 0.01 in beta equal to 1 and the loss is minimize using the atom optimizer it uses 75,000 sketches from the sketchy data set along with 12,500 photos over 125 categories they also use 56,000 photos in imagenet that match the categories in sketchy training the mrn with sketch models synthesizes photo classifiers with a 78 to 80 percent performance on multi-class classification interestingly simple standalone sketches when fed to the mr ends yield better photo classifiers with the performance of about 83 percent now that is cool so what have we learned today do photo classifiers perform better when you have all the data you need well yes yes they do but it's usually the case where we don't simply have the abundance of annotated data and this research shows us a method to overcome that drawback we design a model regression network and mrn that are used to synthesize photo classifiers M RNs are classified into three types depending on their inputs we have an input sketch classifier or an input sketch or sketches or just input photo classifier and a sketch to create a more fine-tuned photo classifier and that's all I have for you now so if you liked what you saw hit that like button if you liked content like this like AI machine learning deep learning and data Sciences then hit that subscribe button bring that Bell icon for instant notifications when I upload links of the main paper and other resources are down in description below so check them out still have about your daily dose of AI yet then click or tap one of the videos right here for an awesome video and I will see you in the next one see ya",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "gVehTbi6Ipc",
        "video_title": "Neural Voice Cloning",
        "position_in_playlist": 5,
        "description": "In this video, we take a look at a paper released by Baidu on Neural Voice Cloning with a few samples. The idea is to \u201cclone\u201d an unseen speaker\u2019s voice with only a few sound clips.\n\nIf you like the video, hit that like button. Ring the bell to stay notified of my videos on Machine Learning, Deep Learning, Data Sciences and AI.\n\nmain paper: https://arxiv.org/abs/1802.06006\nCheck out the audio demos: https://audiodemos.github.io/\n\nMY EQUIPMENT (on a $350 budget)\nCamera (GoPro Hero 5 Black + 32 GB Memory + Kit): https://goo.gl/V4542j\nMicrophone: https://goo.gl/BxBRcW\nPop filter: https://goo.gl/oQTQ8W\n\nFOLLOW ME\nhttps://www.quora.com/profile/Ajay-Halthor",
        "transcript": "wouldn't it be cool to have an AI that listens to you speak for a few seconds and then is able to say different things in your voice well it exists and before getting into details I'll show you exactly what it can do consider the original audio clip this is how the speaker sounds the regional newspapers have outperformed the national titles listening to a few seconds of audio like this from a speaker the AI will be able to take some text input and can generate new audio saying that text like this the large items are put into containers for disposal so you can make an AI say anything you want and in your voice that's pretty slick right I'm AJ Hathor and in this video we're gonna take a look at how exactly such a neural voice cloning system works stay notified about my videos by clicking that subscribe button and hitting that Bell icon now let's get to it last month researchers at Baidu Silicon Valley AI lab developed a neural voice cloning system this system requires only a few samples of single speaker audio to generate speech in the speaker's voice I stress on the term using a few samples because until now neural networks required copious amounts of data in order to train themselves to actually perform any type of task they need such large amounts of training data because of the thousands and even millions of training parameters that they require to estimate the paper neural voice cloning with a few samples proposes two different methods to perform voice cloning the first is speaker adaptation this involves just tweaking or fine-tuning a pre trained model and hence making it adapt to or cater to the current speaker the second method is speaker encoding there are no pre trained models here we train two models the generator model and the speaker encoder model simultaneously before getting into the details of these processes I'm gonna have to explain a few concepts so that we're all on the same page let's start with something easy voice cloning voice cloning involves reproducing the voice of an unseen speaker it's a beautiful day isn't it I'm going to conquer the world whether you like it or not okay [Music] we perform voice cloning with only a few samples and this is considered few shot generative modeling of speech in other words we only require a few samples of such speech in order to clone that speech future generative modeling is challenging because it requires to learn speaker characteristics with just limited amounts of data let's now take a look at another term generative models these are distributions that can be sampled from and such samples correspond to real data for example consider a generative model that models animal images then sampling from this I should be able to get the image of say a dog next time I sample from it I may get the image of a cat for the current problem of voice cloning the generative model models speech so sampling from this model gives some speech audio by some speaker and every time I sample from this model I can get different speaker audios now that you know that generator models generate data I think you could guess what needs to be done to create a voice cloning AI we need to train the generator model so that it can sample speech audio from it this model is trained on multiple speakers with multiple accents if you want to get into details this paper uses the libery speech data set which consists of about 2500 speakers and 820 hours of data to train our model you assume to use text audio pairs however speakers with different accents say the same sentence in different ways hence multiple spectrograms will be mapped to the same text this leads to a less accurate generative model however when the model is additionally given information about the speaker such as dialect accent or gender then it is able to model the differences between the speech and hence improve performance this information about the speaker is called speaker embedding and so to train a generative model instead of just text audio pairs we require Triplets of text audio and speaker embeddings for every sample formally define speaker embeddings are low dimension continuous representations of speaker characteristics now that we got the basic terms out of the way let's talk about designing an objective function like I mentioned before we train our generative model to generate audio we call our generative model some F in such parametric models training refers to learning parameters of the model let's call these parameters theta remember we also want to learn an embedding to distinguish between user characteristics like pitch accent and dialect learning is equivalent to estimating a set of parameters let's call the speaker embedding parameters for speaker si as a subscript si what exactly do we give the train model to produce an output we provide two things the first is the text to say here t IJ is the j DH words spoken by speaker i and the second is the identity of the speaker that's si this helps us to model the parameters Ford speaker embedding sampling from this F we get some cloned audio of speaker si saying the word tij for training we have a data set for every speaker as I consisting of some text T and the corresponding actual audio of the speaker AI J saying the word the idea is thus to minimize the divergence between the cloned audio samples from F and the actual audio in the data set for the same speaker this is the loss for just one sample from a single speaker and so we take the expected value of this loss over all speakers s and overall samples in the data set TSI this is done to learn the model parameters theta and the speaker embedding E and this is the general objective function with a written explanation of each so here's a question why do we take the expected value of loss instead of computing the loss directly this is because we don't know how tractable or easy to compute the loss function is it can be and usually is a complex function and hence becomes more feasible to determine the approximate value of the loss and in math this approximation is given by the expected value let us now take a look at the two methods for actually computing this loss and hence performing voice cloning we start with speaker adaptation here is the idea we have a pre trained audio generator we just need to fine-tune it to produce the voice of some unseen speaker given some text even in speaker adaptation there are two approaches of fine-tuning the first is embedding only adaptation and the second is whole model adaptation in the embedding only adaptation approach the only thing we need to do is further train the embedding to cater to a new speaker we don't need to touch the speech generative model so the new loss function can be obtained from the general one we derived since the generative model is pre trained there is no theta estimation we only need some text and the corresponding audio sample spoken by the current speaker sk note that sk is an unseen speaker that the speech generative model f hasn't seen before i put a cap on theta to indicate it's fixed here since the embedding doesn't have nearly as many parameters as the speech model we don't require the new speaker to talk too much as we don't need that much data to model his or her voice let's take a look at some results of this approach first here's the original sample voice we also need a small plastic snake and a big toy frog for the kids now using embedding only adaptation here's the synthesized voice learn about setting up wireless network configuration you can tell the voice is similar to the original speaker let's try something similar but with a male voice this time so here's the original speech some have accepted it as a miracle without physical explanation and here is the synthesized voice using the embedding only adaptation feedback must be timely and accurate throughout the project not bad right the voices are nearly the same so now let's take a look at another speaker adaptive that I mentioned that's used for voice cloning - and that's whole model adaptation we have a pre train model but not only do we fine-tune the speaker embedding as in the case of embedding only approach but we also fine-tune the generative model F itself I'm certain you can imagine the cost function to minimize if you can't tell well that's why I'm here the cost for the embedding only approach is given by this equation but now F is also being tweaked so get rid of that hat over the theta as it is no longer fixed we are predicting both the embedding and theta in the process and that's it let's take a look at this in action here's the original voice Oscar to bring these things with her from the store and here is a synthesized voice when using whole model adaptation both users have opened a massive investigation into allegations of fixing games and illegal betting they sound pretty similar right now we do the same for the male voice here's the original the Greeks used to imagine that it was a sign from the gods to foretell war or heavy rain and here's the synthesized voice instead of fixing it they gave it a nickname comparing the two methods we see that the whole model adaptation has more degrees of freedom and hence more flexibility however it can easily overfit when applied to very less speaker data so there's always a trade-off until now we have just looked at the voice cloning phase using speaker adaptation this is actually just one phase of the entire process now let us look at how we actually cloned the voice from the first step to the last the training part Maps the speaker identity to some embedding the text audio embedding triplet is then fed to the model for training initially both the model parameters theta and the speaker embeddings e are initialized randomly with supervised training samples these parameters are gradually learned after this phase we have trained the multi speaker model and we have also trained the multi speaker speech embedding in Phase two we have cloning which we discussed with the speaker adaptation this involves fine-tuning either only the embedding or both the embedding and the model using the cloning samples these cloning samples are collected by sampling the speaker's voice after Phase two we have trained the multi speaker model well if the whole model adaptation was used then we've also catered it to a specific speaker or it's just the same as the output of the first phase if we just use embedding only and the second is well speaker embedding is now catered to the current speaker the third phase is audio generation given an input piece of text the generative model is able to synthesize speech in the voice of the specific speaker with the help of the embedding of course on to the next approach speaker encoding now this method doesn't really involve fine-tuning any model or embedding per se the speaker encoding function G takes in a specific speaker's speech as input that's a subscript SK and it outputs the corresponding speaker embedding ease of script SK here a subscript SK is the set of audio samples taken from the current speaker that is the voice to be cloned this is represented as cloned audios in the figure let us now try to determine the loss function for the speaker encoding approach we have the original loss function but now we have a speaker encoder G to generate the speech embeddings so just substitute that in place of e we are thus able to train the generative model and the speech encoder simultaneously however in practice there are problems in training these generative models from scratch the first is the missing modes problem or mode collapse without enough training data when sampling generative models we may not be able to sample all classes very well to give a concrete example say you trained a generative model to output animals by showing it images of dogs cats and drafts the we don't have enough of giraffe images so every time we sample the generator you only end up with a dog or cat images and we cannot sample giraffe images one way to solve this problem is to get more training data but if we do that then what would be the point of this paper we are trying to perform neural voice cloning with only a few samples right that was the objective so the idea is to use a pre trained generative model hence we have the model parameters including the speech embeddings learned for the multi speaker model the speech encoder is trained from scratch to make sure that we have a custom voice to train this we first sample some speech from our pre trained multi speaker model F this will generate an audio sample of speaker si this audio cone sample is then used as an input to the speaker encoder labeled as cloned audios since we have the speaker embeddings for the speaker from the pre trained multi-speaker model we keep it fixed and hence indicated in blue we compare this embedding to that generated by the encoder and modify the encoders parameters theta subscript encoder in other words the speech encoder is trained so what is a good objective to minimize this encoder training cost a simple l1 law seems to work best once again the hats indicate the fixed values a subscript si hat indicates the speaker embedding created for the speaker by the pre trained multi-speaker model and G is the speaker embedding predicted by the current speaker encoder eventually the speaker encoder can generate appropriate speaker embeddings more cater to the individual now how do we synthesize the speech first text and some cloning audio samples is input to the model then the speaker encoder creates a speech embedding this embedding along with the text is passed to the generator model the audio corresponding to these inputs is sampled and we get the required audio now that we talked about the speaker encoder model for voice cloning it's on to the next topic what exactly is the speaker encoder like what does it consist of audio A's are converted into Mel spectrograms these are passed into a pre net which consists of FC layers with an AVO activation this is just for feature transformation next the transform features are passed through convolution blocks to extract temporal features these Conflux have residual connections allowing deeper networks global average pooling summarises the utterance if you want to know exactly how residual connections work and various other convolution neural network architectures check out the eye on the sky or the description down below different audio samples have different amounts of information some of them are valuable others are less so a self attention mechanism is used to determine the weights of audio samples and get aggregate embeddings this is kind of like soft attention where we focus on the important parts for more information on attention mechanisms and its types I have a video for that too the output is the predicted speaker embedding for the audio let's take a look at some results here here's the original speakers voice they had four children together now after training a generator model and voice cloning using speaker encoding here is a generated sample in the same voice churches should not encourage it or make it look harmless let's listen to similar results for a male voice versus the original speakers voice it was even worse than at home and here's the corn voice using speaker encoding saying something else different telescope designs perform differently and have different strengths and weaknesses that's pretty cool if you ask me in this video we took a look at a paper released by Baidu on neural voice cloning with a few samples the idea is to clone an unseen speakers voice with only a few sound clips the entire speech synthesis process involves three steps first is training the multi speaker generative model and speaker embedding the second is vocal cloning and the third is synthesizing voice given text in phase 2 that is vocal cloning it is carried out using two approaches the first that we discussed is speaker adaptation where we just either fine-tune the embedding only or fine-tune the generative model and the embedding to cater to the speaker the second approach was speaker encoding where we trained a speaker encoder to accurately model speaker embeddings I encourage you to read the paper yourself to understand extra details the link to it is down in the description below the video I understand that many people are deterred because of the complex math in these papers however I hope that my video helps make the paper more accessible and it bridges the gap between complex math and concept there are fascinating works published every week on this topic and I'm here to make it more accessible if you like this content hit that like button if you want to watch similar content hit that subscribe button and hit the bell icon too I'm trying out this new setup with the camera and the microphone so just let me know how you like it in the comments down below and the links to it will also be in the description down below so if you want to get yourself your own camera or your own microphone it's all there still not satisfied click our talk one of the videos right there and it'll take you to another awesome video and I will see you in next one by",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "GNza2ncnMfA",
        "video_title": "Sound play with Convolution Neural Networks",
        "position_in_playlist": 6,
        "description": "In this video, I talk about processing audio using a convolutional Neural Network and discriminate environmental sounds.\n\nCode: https://github.com/ajhalthor/audio-classifier-convNet\nConvolution Neural Networks: https://www.youtube.com/watch?v=m8pOnJxOcqY\nGenerative Adversarial Networks: https://www.youtube.com/watch?v=O8LAi6ksC80\nAudio dataset: https://serv.cusp.nyu.edu/projects/urbansounddataset/download-urbansound8k.html\n\n \nREFERENCES\n\nReference Research Paper: https://arxiv.org/pdf/1608.04363.pdf\nGenerate waveforms from WAV files: http://convert.ing-now.com/",
        "transcript": "in this video we're gonna create a convolutional neural network to distinguish between different sounds sounds like air conditioners and automobiles etc we can do this in about 50 lines of Kaos code it isn't too difficult but before thinking about modeling we need to understand our data sounds that we are going to distinguish are a part of the urban sound 8k dataset this consists of over 8,000 sounds of ten categories children playing dogs barking jackhammer running engine air conditioners street music gunshots siren drilling and a car horn in order to visualize an audio signal we're going to convert them into spectrograms these are 2d visual representations of frequency varying over time once we have a grid like topology for our data it can be processed by a convolutional neural network so here's a question why CN NS for this well convolutional neural networks can discriminate spectral temporal patterns in other words they are able to capture patterns across time and frequency for given input spectrograms this is important for distinguishing between noise like sounds like the sounds in our data set furthermore they have an edge over traditional mell frequency substrate coefficients M FCC's as these convolutional neural networks are still able to make distinctions even when the sound is masked in time and frequency by other noise however there hasn't been much advancement in the fields of audio distinction using CN NS because of its one main drawback the sheer amount of data required to build the model we don't have enough labeled sound samples but now we have a solution to this problem data augmentation data augmentation involves generating new sounds by introducing slight distortions in the original data the amount of distortion should still ensure the label is valid this allows a model to become invariant to small changes in input and hence allow for better generalization which is exactly what deep all networks try to achieve let's take a look at the code and I'll explain the details along the way we start with the following library imports we have Karos which is our deep learning framework built on tensorflow back-end to create and train our convolutional neural network we have lebra which allows us to read write and play around with audio files numpy is a math library to express inputs feature maps and outputs in the form of matrices pandas reads and manipulates data stored in tables using data frames we use this for reading metadata and we have random which is used to shuffle our data set we begin by reading the metadata of the urban sound 8k data set each tuple has information about the audio files like the name the label and the start and end times most of these clips are about 3 to 4 seconds long for uniform input to the model I don't want stray sound clips which are too short to derive any useful information so I get rid of those by computing the duration I want to convert files to a log scale mel spectrogram using libero so consider the first three seconds of the clip to keep it uniform size here's an example of a mel spectrogram of a siren clip note the color indicates the loudness brighter the color louder is the sound and here's the spectrogram for an air conditioner here's one of children playing outside and here's a spectrogram for a drilling machine now we iterate over all valid data samples which we conveniently have in a list in the metadata and create mel spectrograms for each this is done by dividing the frequency range into 128 components ranging from zero to twenty two point zero five kilohertz this frequency is a range of audible sound well technically it's 20 Hertz onwards but you get the idea and for time consider the three seconds of input divided into frames of twenty three milliseconds this leads to 128 components across time as well so the overall input is a 128 crossed 128 matrix of real numbers the input sounds may be longer than three seconds but we consider three continuous random seconds in the sound wave to constitute the ML spectrogram we end up with seven thousand four hundred and sixty seven valid samples which is fed into our convolutional neural network next shuffle the data set split up test and trainsets reshape the inputs to 128 cross 128 cross one for CN n input and encode the class labels using a one hot encoding for ten classes I'll describe the model that I reconstructed from a recent research paper and I'll link the paper down in the description below this video okay so now let's start the model like I said before we have 128 cross 128 log scaled Mel spectra grounds as input we first introduced a convolution layer where we perform convolution with 24 5 cross 5 kernels with the stride of one using these small 5 cross 5 kernels during convolution we can discover time frequency signatures in the input these are distinct for different sounds and hence help us distinguish these sounds despite the noise interference the output of these convolution is a 124 across 124 feature map but since we use 24 such filters the output is 124 cross 124 cross 24 to the South put we apply a max pooling where each filter is 4 by 2 the output on one layer of the input volume is 31 cross 62 but we apply it to 24 feature maps of the convolved input leading to an output volume of 31 cross 62 cross 24 now apply the rate of activation which doesn't change the dimensionality once this is done apply another sequence of convolution activation and pooling for the next convolution layer the input is convolve with 48 5 cross 5 filters the convolution with each filter without padding leads to a 27 cross 58 feature map since we have 48 such filters the output is a 3d volume of shape 27 cross 58 cross 48 once again we down sample the features by applying max pooling with 4 cross 2 filters with strides of 4 along the height and 2 along the width like in the last pooling layer this leads to a 6 cross 29 output for a single layer since the fooling layer is applied to every one of the 48 layers the output is a 3d volume of the same depth 48 to this 3d volume we apply another reloj activation next just apply another round of convolution activation with no pooling the convolution is similar to before where we convolve the input volume with 48 5 cross 5 filters without padding this leads to a 3d volume of shape 2 across 25 cross 48 the following rate of activation doesn't change its shape we are now at the final part of our model the fully connected layers the 2 across 25 cross 48 volume is flattened to form a 2400 dimensional vector this is in an admissible form for the fully connected layers we then apply a hidden layer of 64 neurons with activation then apply dropout the final layer is a softmax layer with ten neurons as we have ten classes of output sounds and so we have defined our convolution neural network if you want to know exactly what each layer does and how these dimensions are computed then check out my video on convolutional neural networks I explain everything there and I'll link it down in the description below the code you see here is whatever I just explained just plain and simple and chaos now we compile the model using the atom optimizer and measuring loss using cross entropy loss because it's a classification problem I display the performance of the system using simple accuracy the model is then trained with 7000 training samples over 12 epochs and a batch size of 128 this just means that the model learns every 128 samples we then evaluate the performance of our model with simple accuracy and print it on screen the current convolutional neural network is able to get us a 70 to 75 percent accuracy not bad but let's see if we can improve this value using data augmentation like I said before data augmentation involves the distortion of training data to an extent that the original label still holds for the distorted data to augment means to add so we are basically adding this new data to our data set to create a larger data set let's take a look at two methods of distorting data obese with respect to audio the first is varying the speed of the audio signal the clip can either be slowed down or sped up and the second is varying the pitch of the input without changing the duration of the clip for speed we take zero point eight one times the speed of the clip which is slower and the another distortion that we take is one point zero seven times to speed up the clip for pitch each sample is pitch shifted by two semitones and 2.5 semitones we can try out other transformations but it generates a lot of data my computer just doesn't have enough space yeah the alone is six gigabytes for every type of distortion I'm creating a new set of samples I'll use for distortions only but that still caps out to about 30 gigs of space so just clear some space up on your computer before you do this you can try out other distortions on audio data as well as long as it doesn't change the label Augmented data set has 35,000 training samples as opposed to the previous 7000 we passed this augmented data set through our model and see that we can distinguish these noises with an overall accuracy of around 82% this is better than the 75% we obtain but whether it's worth actually performing data augmentation is subjective we have an increased accuracy but the training time was also much longer there is also the time taken to generate these distorted samples in any case data augmentation is still a decent method for getting your hands on more data the link to this notebook is down in the description below perhaps an even more fun method and probably a more valid method would be to use gans generative adversarial networks to generate completely new audio data in this way we get more variety than simple distortion provides but that's a topic for another video thanks for stopping by today and if you liked the video click that like button and subscribe for more awesome content and I will see you in the next one bye you",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "Y2Tna77k1aI",
        "video_title": "The Evolution of Convolution Neural Networks",
        "position_in_playlist": 7,
        "description": "From the one that started it all \"LeNet\" (1998) to the deeper networks we see today like Xception (2017), here are some important CNN architectures you should know. If you like the video, show your support with a like, and SUBSCRIBE for more awesome content on Machine Learning, deep Learning, Data Science and AI\n\n\nMY EQUIPMENT (on a $350 budget)\nCamera (GoPro Hero 5 Black + 32 GB Memory + Kit): https://goo.gl/V4542j\nMicrophone: https://goo.gl/BxBRcW\nPop filter: https://goo.gl/oQTQ8W\n\nFOLLOW ME\nhttps://www.quora.com/profile/Ajay-Halthor\n\n\nREFERENCES\n[1] LeNet-5 (the start of it all): http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n[2] Nice Blog post: https://towardsdatascience.com/neural-network-architectures-156e5bad51ba\n[3] CNN Architectures: http://slazebni.cs.illinois.edu/spring17/lec01_cnn_architectures.pdf\n[4] ImageNet - The data that transformed AI research: https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/\n[5]Imagenet (main paper): https://www.researchgate.net/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database\n[6] AlexNet: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n[7] Difference between saturating & non-saturating nonlinearities: https://stats.stackexchange.com/questions/174295/what-does-the-term-saturating-nonlinearities-mean\n[8] Top-1 accuracy Vs Top-5 Accuracy. What do they mean? https://stats.stackexchange.com/questions/156471/imagenet-what-is-top-1-and-top-5-error-rate\n[9] OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks: https://arxiv.org/abs/1312.6229\n[10] Network in Network (NiN) Architecture: https://arxiv.org/abs/1312.4400\n[11] GoogleNet: https://arxiv.org/pdf/1409.4842.pdf\n[12] R-CNN: https://arxiv.org/abs/1311.2524\n[13] ResNet: https://arxiv.org/abs/1512.03385\n[14] An Overview of ResNet and its Variants: https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035\n[15] Xception: Deep Learning with Depthwise Separable Convolutions: https://arxiv.org/abs/1610.02357",
        "transcript": "hello everyone AJ here so as you can see there's something different can you guess what it is did did you guess you can see my face that's right that's right so a few things before I start this video I'm a little microphone in front of me so that's great most of my audio you know audio right here so the entire room there's gonna be a lot of echo I haven't really adjusted this setup yet but it'll get there got my little camera yeah so in the video I'll be interchanging between my face and well the screen we'll see how that goes I'm gonna try to change my setup every single video so that I get the best feel and eventually hopefully I'll settle down on like well one location for now it's like in my little living room that's why there's a lot of echo the floors are wooden so yeah then I'll probably go back to the room which is where I usually record because of the carpeting and it's just a lot better for ambient noise and yeah we'll just see how it goes so that's all I have to say enjoy the video you know the typical convolutional neural network architectures that you see in those getting started with PI torch tutorials are getting started with any other deep learning library you know the one where we have like a stack of layers of convolution pooling activation followed by some fully connected layers and a soft Max for classification yeah I think you know what I mean this is just one of the basic neural network architectures based on the Linette family in this video we're going to talk about different such neural network architectures emphasizing exactly why they were introduced and what new concepts they bring to the fields that make convolution neural network research what it is today we'll start with the beginning of convolution neural networks in Yamla Koons pioneering paper in 1998 in this paper he introduces a class of neural network architectures the Linette family well in that five being one of the most common forms that we see to this day Lynnette five is a seven layer neural network architecture excluding inputs that consists of two alternate convolution and pooling layers followed by three fully connected layers at the end it used convolution to preserve spatial orientation of features average pooling for down sampling of the feature space and it makes use of the tench and sigmoid activation between layers Lynnette five obtained a 99% test accuracy on the Emnes data set understand that this was a time before GPUs so computers were not able to process such large amounts of data within a reasonable amount of time furthermore we didn't have such large stores to begin with and this is the reason why neural networks didn't spark until 2010 so what exactly happened well back in the day it was considered that a better algorithm would always yield better results regardless of data but we now know today that this theory is flawed then professor at UIUC Feifei lee agreed claiming the best algorithm wouldn't work well if the data learned from it didn't reflect the real world she then said we decided we wanted to do something that was completely historically unprecedented we're going to map out the entire world of objects the resulting data set imagenet from 2010 onwards an annual imagenet competition that is imagenet large-scale visual recognition challenge ILS VRC is held between researchers to determine which algorithm can yield the best performance many consider this the dawn of the AI boom I bring imagenet into this discussion because the winners every year always provided something new to contribute to the field that would influence architectures that succeeded it the 2012 winner of this competition was Alex net the architecture consists of eight learned layers it is five convolution layers followed by three fully connected layers some of the features in this architecture include well first is the introduction of the Ray Liu non-linearity rectified linear unit is a non saturating non-linearity a non saturating function f has the property that it tends to a positive or negative infinity as its parameters tend to positive or negative infinity a function f is saturating if it is not non saturating example of such a saturating function would be tench which ranges from negative 1 to positive 1 or sigmoid which ranges from 0 to 1 deep convolution networks with Ray Liu trained several times faster than their 10 CH activation counterparts as shown in this graph the solid lines show performance of Ray Liu and the dashed lines show that of 10 CH on the C for 10 dataset the former was about six times faster than its saturated counterpart this allows us to train huge neural networks as we see today in reasonable amounts of time it's very interesting right right right now the second feature that we see in Aleks net is that it can be trained on multiple GPUs Aleks net specifically is trained on two GPUs their parallelization scheme puts half of the on each GPU with one additional trick the GPUs communicate only in certain layers for example the neurons of layer 3 take input from feature maps in layer 2 however neurons in layer 4 take input only from those feature Maps in layer 3 which reside on the same GPU now a third feature of alex net is normalization Raley's have the desire property that they don't require input normalization to prevent them from saturating if at least some training examples produces a positive input to a rail uu learning will happen in that neuron however a vocal normalization scheme still helps generalization like the one shown on screen using this to train a four layer CNN on the c-4 10 dataset they achieved a 13% test error rate without normalization and 11% test error rate with normalization so that's an improvement the fourth and final feature of Alex net is overlapping pooling pooling is used to down sample the features face and Alex not performed pooling with a window of 3 cross 3 but stride of 2 hence there would be an overlap between subsequent positions of the kernel with this overlap pooling scheme the top one accuracy and the top 5 accuracy reduced by 0.4 percent and 0.3 percent respectively now let's take a look at the overall architecture i'll explain is very briefly like I said before it is an 8 layer architecture with 5 convolution layers and 3 fully connected layers the last layers pass through softmax activation for 1000 classes here the top layers are in the first GPU and the bottom set of layers run on the other second GPU the kernels of the 2nd 4th and 5th convolution layers are connected only to those kernel maps in the previous layer which reside on the same GPU the kernels of the third convolution layer are connected to all kernel maps in the second layer the and they fully connected layers are connected to all neurons in the previous layer response normalization layers follow the first and second convolution layers max pulling layers follow both response normalization layers as well as the fifth convolution layer the Rayleigh non-linearity is applied to the output of every convolutional and fully connected layer hope you understood Alex Ned I'm emphasizing a bit more on this than on the other architectures because this has a large influence on larger neural networks that we will see in the future up next we have the 2013 ALS VRC winner buys a l'heure and furgus clarify it's a refinement of Alex Ned with an 8 layer architecture each layer performs several operations in the first layer the input image to 24 cross through 24 is convolve with ninety six seven cross seven filters this is followed by reloj activation which doesn't change the output volume shape next overlapping max polling is applied with a 3 cross 3 sliding window and astride of 2 and finally we apply contrast normalization this set of operations yields an output feature volume for layer 1 a stack of the same operations is performed in layers 2 through 5 layer 6 and 7 are fully connected layers and layer 8 is the output softmax layer to perform image classification next up we introduce the runner-up for the 2014 image net challenge the visual geometry group from the University of Oxford one of their major changes to the network in their vgg net architecture is a smaller kernel size linette's use 5 cross 5 kernels Alex net use 11 cross 11 kernels for convolution and clarified used 7 cross 7 kernels in vgg nets the images pass through a stack of convolution layers where the filters have a very small receptive field of 3 cross 3 this is the smallest size required to capture the notion of left-right up-down and center and this leads a significant parameter decrease spatial polling is carried out by five max pooling layers which follows some of the conv layers I say some and not all Cohn flares as not all of them are followed by Max pulling max polling is performed over a 2 cross 2 pixel window with a stride of 2 a typical configuration we see today all hidden layers are followed by Rayleigh non-linearity falling from alex net in this figure we see the architectures of different configurations in the vgg family labeled a through e a has the smallest number of weighted layers 11 with 8 conv layers and 3 FC layers and architecture e has the largest number of weighted layers 19 with 16 convolution layers and 3 FC layers the architecture also plays with one cross one convolutions as we see in type C this increases the non-linearity of decision functions without affecting the receptive fields and so it can effectively model more complex problems without affecting the number of learned parameters we can also see the number of parameters is not as much as its predecessor network architectures that demonstrated revolutionary performance in image localization in ILS VRC 2013 this is attributed to the decreased kernel sizes for convolution and the use of one cross one convolutions the next CNN architecture is the network in network architecture it makes use of one cross one convolutions with multi-layer perceptron layers in normal convolution we apply an inner product of a number of kernels with the input feature volume to get an output feature volume this is typically followed by a nonlinear activation typically such convolution layers are stacked in order to learn more complex features however stacking convolution layers can lead to really deep networks that exponentially increase a number of parameters to learn so what if we replace these stack convolutions with a micro network well network and network does exactly that where they use multi-layer perceptron MLP this is the choice because it is a network trainable by backprop so it meshes in with the existing CNN components and it significantly reduces the number of parameters to learn without stacking a number of convolution layers like what we saw in the vgg Nets it is called network in network because there are many of these micro networks in the entire network another feature of Ni n is that it uses global average pooling this when used with micro Nets creates more interpretable classification results relative to traditional CN NS furthermore fully connected layers are more prone to overfitting and hence rely on dropout as a means of regularization on the other hand global average pooling is a structural regularizer and so it inherently avoids overfitting in its structure this network in network architecture inspired a seminal work in the field known as Google net let us talk about the ILS VRC 2014 winner Google net their 22 layer deep neural network architecture used twelve times fewer parameters in Alex net yet yields much higher performance the leading approach for object detection at the time was regions with convolutional neural networks are CNN's used for object detection as well Google net attributes its success to deeper networks and the RCN and algorithm a fundamental unit of this network is the inception module where it heavily uses the network networks architectures one cross one convolutions these one cross one convolutions are typically used for dimensionality reduction to avoid patch or kernel misalignment the inception module makes use of all kernels of size 1 Cross 1 3 cross 3 & 5 cross 5 the outputs of which are concatenated to form the inputs to the next stage these inception units are stacked one on top of the other throughout the network however there is a problem the deeper we go the convolution neural network extracts lower and more complex features so each of these 3 cross 3 and 5 cross 5 filters correspond to smaller and smaller regions as we go deeper into the network we thus need more parameters and estimating them can lead to a computation blow up within just a few layers to address this inception performs reductions through one cross one convolutions before carrying out each of the expensive 3 cross 3 and 5 cross 5 convolutions these one cross one convolutions additionally provide a rectified linear activation allowing them to model more complex operations so we don't need to overload on these inception modules as far as performance is concerned Google Net obtains a top 5 error of 6.67 percent on both the validation and testing data ranking the first among other participants it was about 40 percent better than clarify which was the previous year's best approach that's quite the improvement we now move on to the 2015 ILS VRC winner ResNet just when you thought they couldn't get any deeper they still do to train on the imagenet data set the residual network had 152 layers this is 8 times more than that of the vgg nets yet it has a lower complexity now let's step back a bit and just ask ourselves why are we trying to build deeper networks what is the point of this this goes back to the universal approximation theorem from wikipedia which is clearly the most verifiable source in mathematical theory of artificial neural networks the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons that is an MLP can approximate continuous functions under mild assumptions of an activation function in other words we can potentially model any kind of problem using a single layered neural network however in the process we may run into a problem of memorizing training data which is popularly known as overfitting and so a common trend in deep learning research in order to enhance generalization is to well go deeper building deeper networks but we cannot just simply stack layers in order to actually just go deeper because this leads to the vanishing gradient problem basically in typical back propagation algorithms the neural network will learn through gradient updates in very deep networks that gradient will eventually reduce to zero and when a gradient is reduced to zero the weights themselves will not be updated and so there's no learning taking place so performance saturates and even degrades after some point to combat this problem folks at Microsoft introduced shortcut connections that bypass two layers they called this an identity shortcut connection the intuition here is that the stacking of such layers won't harm the performance as the layers can be bypassed hence a ResNet will perform at least as good as it's shallower counterpart so deeper the network is the better we can observe results on the 1,000 class classification of imagenet by comparing performance of plain networks with the residual network counterpart which is just a plain network with the bypass in their paper we observe an overall better performance of resonant by about 2.8 percent furthermore the shallower 18 layer plane net performs better than the deeper 34 layer Network attributed to the vanishing gradient problem on the other hand the deeper ResNet has the better performance showing that ResNet was able to address this gradient problem the next and final architecture we'll talk about is exception it's based off inception and depth Y separable convolutions in standard convolution cross channel correlations and spatial correlations are determined together simultaneously exception on the other hand takes advantage of the fact that these two processes can be completely decoupled in effect we can perform convolution in two phases the first is depth wise convolution which determines spatial correlations this is the relationship of pixels with respect to its neighbors and then we have point wise convolution which determines the cross channel correlations the relationship between points in the feature volume along the different channels these are the fundamental phases of depth-wise separable convolution I made a video on this so check it out in the info card at the top but for now just understand that depth I separable convolutions breakup the convolution operation into two phases which reduces the number of parameters and hence computation time the typical inception module first looks at the cross channel correlations the reset of one cross one convolutions mapping that input data into three or four separate spaces that are smaller than the original input space it then looks at all the spatial correlations in these smaller 3d spaces through regular 3 cross 3 or 5 cross 5 convolutions now consider a simplified inception module without the polling layers and only 3 cross 3 convolutions as an extreme version of the inception module we first use a 1 cross one convolution to map cross channel correlations and then separately map the spatial correlations of every output channel this extreme version of Inception is called exception the main differences between exception and depth wise convolution is well first the order of operations in depth I separable convolution the depth wise convolution is performed before the point wise convolution whereas in exception the operation orders are reversed second the depth wise and the point wise convolutions in exception are followed by Rayleigh nonlinear activations whereas there is no non-linearity that occurs in depth why separable convolutions from exceptions architecture we see it makes use of these depth-wise separable convolutions reloj activations and the shortcut connections from ResNet exception was also able to achieve state-of-the-art performance on a 350 million image dataset jft for a classification of 17,000 categories so what have we learned the history of convolution neural net architectures started with the Linette family this involves stacking a number of convolution units for feature extraction and max pooling units for spatial subsampling in 2012's Aleks net convolution operations were repeated between the subsequent max pulling to extract richer features in 2013 we saw sailors and fergus's architecture clarify and the vgg architecture in 2014 in the vgg architecture we saw an increase in the performance with decrease in the number of learn about parameters by using smaller kernel sizes and one cross one convolutions we saw the introduction of network and network architecture that introduced micro networks between subsequent convolution layers this allowed the network to prevent stacking of numerous simple convolution layers to decrease the number of parameters furthermore it created more interpretable classification results relative to traditional cnn's the ILS VRC 2014 winner google net was a 22 layer deep neural network architecture that introduced the inception module it borrowed heavily from the networking Network and our cnn's these modules allowed modeling of complex operations so a small stack of these inception modules could solve problems in computer vision leading to less computation and in the end of 2015 we had the introduction of residual networks that mitigated the problem of vanishing gradients through short cut connection this allowed the training of very deep networks to enhance generalization in April 2017 we had exception or extreme inception it's based heavily off of the inception module and makes use of depth-wise separable convolutions it also uses short connections of ResNet to construct deeper networks with high performance and that's all I have for you now if you liked the video hit that like button if you're interested in videos like this like videos on machine learning data Sciences artificial intelligence in general then hit that subscribe button for notifications when I upload hit that Bell icon all the links to the papers are down in the description below so check them out interested in other hot topics in the field click one of the videos right here and I will see you in the next one",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "vpc35rBs_Bc",
        "video_title": "One Neural network learns EVERYTHING ?!",
        "position_in_playlist": 8,
        "description": "We explore a neural network architecture that can solve multiple tasks: multimodal Neural Network. We discuss important components and concepts along the way.\n\nIf you like this video, hit that like button. If you really like this video, hit that SUBSCRIBE button. And if you just love me hit that BELL next to the subscribe button. \n\nRELATED VIDEOS\n[1] Convolution Neural Networks: https://www.youtube.com/watch?v=m8pOnJxOcqY\n[2] Depthwise Separable Convolution (A Faster Convolution): https://www.youtube.com/watch?v=T7o3xvJLuHk&t=248s\n[3] Attention in Neural Networks: https://www.youtube.com/watch?v=W2rWgXJBZhU\n[4] Sound Play with Convolution Neural Networks: https://www.youtube.com/watch?v=GNza2ncnMfA\n\nREFERENCES\n[1]Main paper \"One Model to Learn them all\": https://arxiv.org/pdf/1706.05137v1.pdf\n[2] Separable Convolutions (and other types): https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n[3] Outrageously large neural networks: https://techburst.io/outrageously-large-neural-network-gated-mixture-of-experts-billions-of-parameter-same-d3e901f2fe05\n[4] Mobile Nets, Depthwise Separable Convolution: https://arxiv.org/pdf/1704.04861.pdf\n[5] Blog post on Depthwise Separable Convolution: https://arxiv.org/pdf/1610.02357.pdf\n[6] Attention Gan (Microsoft's AttnGAN): https://arxiv.org/abs/1711.10485\n[7] Show, attend and tell: https://arxiv.org/pdf/1502.03044.pdf \n\n\nMusic at : https://www.bensound.com/royalty-free-music/track/tenderness",
        "transcript": "when we train a neural network we adjust weight parameters so it can usually do just one task and do it very well this can be classifying images or to perform object detection or determining the type of audio sampled it could be any of these but only any one of these tasks we're going to be cool to have one network to do everything the same network that takes speech input and converts it into text the same network that takes an image input and is able to recognize objects in the image or the same network that takes English text and is able to translate it to French or German and so much more in this video we're going to take a look at the crux of such a multitask network and see what results we can get out of it we'll follow the mechanism used by Google in their 2017 paper 1 model to learn them all from the highest level we are going to train the network on 8 separate tasks the first is speech Texas this can be done with the WSJ speech corpus second is we'll perform visual object detection that is determining an object in an image using the image net data set then we'll construct captions from the corresponding images by training it with the cocoa data set parsing image from a text input so as to get the part of speech constituents and word tokens this can be done with Wall Street Journal's parsing data set next we can use WM T's English German translation corpus for hint-hint English to German translation then we can perform reverse German to English translation which uses the same corpus and we do the same for French so that's English to French translation using the WMT English French translation corpus and the final task would be French to English translation using the same corpus now let's talk about multi model architecture the multitasking model consists of four main components the first is modality networks this converts the input to a universal input representation then we have an ink which processes the input we have an i/o mixer which encodes the input with the previous outputs then we have an autoregressive decoder which processes the input and mixture to generate some output I'll explain these components in detail starting with modality nets different tasks in a network require inputs to have different types and also different input sizes to accommodate for this difference multitask networks incorporate modality nets and convert the original features to a universal feature representation this is done differently for different modalities modalities like text speech and image inputs such multitask networks also require the modality net to transform the output of the network in Universal representation to the original form these modality nets are designed to be computationally minimal and hence it performs the transformation and leaves the rest of the processing to the crux of the multitasking network modality Nets are designed for every domain and not every task so instead of dealing with different modality nets for say image captioning and an image classification problem since they have the same type of input domain that is they're dealing with input images they can be passed through the same modality net this enhances generalization and allows new tasks to be added on the fly without much change to the overall network modality Nets are designed to create different sized Universal representations for different domains having a fixed sized representation can hinder performance anyways I'll talk about the types of modality nets in a bit but first let me explain the other parts of this network architecture now we have the encoder the mixer and the decoder and these are made up of basically three major fundamental components depth I separable convolution units attention mechanisms and sparsely gated mixture of experts each of these topics worn their own video but I'll keep it brief for this simple explanation let's start with depth wise separable convolutions they were introduced in an architecture for neural networks called exception basically it's a faster method of performing convolution with significantly less multiplication operations this becomes more important while dealing with large networks with billions of parameters essentially depth I separable convolution is performed in two phases a depth wise convolution phase followed by a point wise convolution Fey's death wise convolution applies different filters to each individual input channel very different from standard convolution which applies the convolution through all channels then we have point wise convolution which is a 1 cross one convolution to create a linear combination of outputs of depth wise convolution the ratio of the number of multiplication operations between the standard convolution and this two step depth Y separable convolution becomes significant for larger networks now for more details on this little derivation on screen and also on useful places where this is used check out my video on depth y separable convolution now let's move on to attention mechanisms attention mechanisms found in neural networks is somewhat similar to that found in humans they focus in high resolution on certain parts of an input while the rest of the input is in low resolution or blurred they find their applications in neural machine translation to translate one language to another in attention generative adversarial networks Microsoft's attention again generates images from sample.txt for example they can also be used with recurrent neural networks to easily generate answers to questions in a book if you want to know more about attention in the context of neural networks check out my video in the info card at the top next we have sparsely gated mixture of experts a big problem with neural networks in general is the amount of training data required to get decent results companies like Google and Facebook have this kind of data however there is an another problem while training neural networks after every sample or every batch of samples the entire neural network is updated despite the fact that neurons may not have been affected furthermore larger the neural network more is the processing power required for training this was until Google's paper on the mixture of experts layer and mo e layer consists of a number of experts and each of these are neural networks specialized in handling specific aspects of a given task the gating layer in an MOA layer takes the input X and determines which set of experts to consult weighing in on the importance through G X sub I for the IPX pert in this figure the gating Network takes the input X and consults primarily experts to and expert n minus 1 the output is the weighted arithmetic mean of the output of these experts in the MOA block of our current step we have a pool of 240 experts trained on the eight tasks jointly and they use sixty experts while training each problem separately like I mentioned before the encoder and decoder are constructed from convolution units attention mechanisms and sparsely gated mo EES the encoder has six convolution blocks and a mixture of experts layer the i/o mixer contains two attention and convolution blocks and the decoder contains for attention and convolution blocks now let's jump back to modality nets and discuss its architecture to solve that eight tasks we require four modality nets one for each type of input language that is text input audio and just categorical data with language modality nets we convert text to a universal internal notation input text is tokenized into constituent sub words considering an 8000 sub word dictionary the sub word vector is then encoded on the output end the modality net takes the decoded output from the multi-model neural network and performs a softmax operation to output the probability distribution that determines the most likely sub words spoken by the way sub words can be like mono phones try phones or syllables now with image modality nets we convert images to a universal internal representation specifically we increase the input depth using residual convolution blocks more specifically to the input we apply F 3 cross 3 filters to the input image X and get the convolve c1 we repeat the same convolution step and then apply Mac's bowling with a 3 cross 3 window and a stride of two this pooled output is added to the one cross one convolution on the input the two three cross three convolutions max pooling and one cross one convolution constitute the residual convolution block conv rez as we see here you can see it in action as we convert the image X into an internal Universal representation image modality in the difference is that well the representation is very deep D here is taken to be 1,024 first we have a convolution on 32 3 cross 3 filters followed by 64 convolutions on 3 cross 3 filters then a set of 2 3 cross 3 convolutions max poling and one cross one conclusions is performed thrice as I just explained in the residual convolution block this gives rise to a feature column depth D in the case of an image we only need to convert the input into a universal internal representation the output is usually not an image but more of a category and hence we don't require an image modality out unit this brings us to the next type of modality net categorical modality this is used when the input is an image or audio and the output is a category much like the case of determining an input image is a dog or a cat or the context of audio input it could involve determining the type of urban noise the audio clip corresponds children playing or the sound of an engine or the sound of a jackhammer like so this involves performing 2 steps of convolution H and h2 performing pooling with a 3 cross 3 window with stripe 2 then we add the result to the convolution of 3 cross 3 filter on input X with a stride of - with this new result h3 apply 1536 3 cross 3 kernels to get the output volume h4 similarly applied 2048 3 cross 3 filters on h4 to get h5 then apply reloj activation which doesn't change the shape of h5 we then downsample the features with global average pooling to get h6 and then we finally apply point wise convolution on the kernel corresponding to class weights to get an output now let's talk about audio modality Nets the one-dimensional raw audio can be converted into a two-dimensional spectrogram this is basically a 2d plot of time on the x-axis and frequency on the y-axis the spectrogram can be treated as an image input and pass through the image modality net like I talked about before so we got the entire network architecture thing out of the way that's good now let's try to answer a few questions about performance question one how far is the multi-model trained on eight tasks simultaneously from the state-of-the-art results here is a table comparing the multi model implementation in tensorflow with the state-of-the-art for three tasks sure they are slightly lower but for multitasking model the results are not too shabby question - how does training on eight tasks simultaneously compare it to training on each task separately the multi model performs on par for certain tasks but performs even better on tasks where less data is available as is in the case of parsing question three how do the different computational blocks discussed before influence the different tasks to answer this question we compare the performance of multi model neural network the network without the mixture of experts and the network without the attention mechanism since mo e and attention help in neural machine translation let's check out the performance of English to French translation and see how it's affected we'll also include the comparison with image net interestingly note the removal of these blocks didn't affect the performance of these tasks specifically so why is that result interesting this shows that inclusion of even useless components in a network for a specific problem does not decrease the performance for that specific problem so we can add components to this multi model to improve the performance of task a without hurting the performance on another task B so here's a few points to remember from this video it is possible to design a multi model neural network capable of performing different tasks the multi model architecture has four basic components modality nets to convert different input types to a universal internal representation and encoder to process inputs a mixer to encode inputs with previous outputs and a decoder to generate outputs the encoder mixer and decoder are constructed based on the problems to be solved for this specific eight task problem they are made up of three basic components depth Y separable convolution blocks attention mechanisms and sparsely gated mixture of experts and finally the inclusion of components to the small team model architecture does not adversely affect the performance of tasks that don't use those components and that's all I have for you now the paper one model to learn them all shows it is certainly possible to construct a single neural network to solve problems in different domains it will be interesting to see how this pans out in the future we should be seeing some better results pretty soon I've linked the main paper along with useful resources in the description down below thanks for stopping by today and if you liked this video hit that like button if you really like this video hit that subscribe button and if you just loved me just hit that Bell right next to the subscribe button and I'll see you in the next one",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "T7o3xvJLuHk",
        "video_title": "Depthwise Separable Convolution - A FASTER CONVOLUTION!",
        "position_in_playlist": 9,
        "description": "In this video, I talk about depthwise Separable Convolution - A faster method of convolution with less computation power & parameters. We mathematically prove how it is faster, and discuss applications where it is used in modern research. \n\nIf you liked that video, hit that like button. If you wanna stick around, hit that subscribe button. If you really wanna stick around, hit that bell icon next to the subscribe button to be notified of my uploads immediately. \n\nConvolution Neural Networks: https://www.youtube.com/watch?v=m8pOnJxOcqY\n\nREFERENCES\n\nXception (main paper): https://arxiv.org/pdf/1610.02357.pdf\nMobile Nets (Efficient CNN for mobile vision applications) : https://arxiv.org/pdf/1704.04861.pdf\nOne model Learns all: https://arxiv.org/pdf/1706.05137v1.pdf\n\n\nMusic at : https://www.bensound.com/royalty-free-music/track/tenderness",
        "transcript": "convolution is a measure of overlap between two functions as one slides over the other mathematically it's a sum of products the standard convolution operation is slow to perform however we can speed this up with an alternative method that is the topic of this video depth wise separable convolution let's first very quickly go over the basics of convolution on an input volume consider an input volume f.o shape d f cross d f cross m where DF is the width and height of the input volume and M is the number of input channels if a color image was an input then M would be equal to 3/4 the RG and B channels we apply convolution on a kernel K of shape DK cross DK cross M this will give us an output of shape D G cross DG cross 1 if we apply n such kernels on the input then we get an output volume G of shape DG cross DG cross n the convolution operation takes the sum of products of the input and the kernel to return a scalar this operation is continued by sliding the kernel over the input I've explained this concept in detail on my video on convolution neural networks check that out for a clear understanding I'm more concerned now with the cost of this convolution operation so let's take a look at that we can measure the computation required for convolution by taking a look at the number of multiplications required so why is that it's because multiplication is an expensive operation relative to addition so let's determine the number of multiplications for one convolution operation the number of multiplications is the number of elements in that kernel so that would be D K times D K times M multiplications but we slide this kernel over the input we perform DG convolutions along the width and DG convolutions along the height and hence D G cross DG convolutions over all so the number of multiplications in the convolution of one kernel over the entire input f is DG square times D K square times M now this is for just one kernel but if we have n such kernels which makes the absolute total number of multiplications become n times D G square times D K square times M multiplications let's now take a look at depth wise separable convolutions in standard convolution the application of filters across all input channels and the combination of these values are done in a single step def y separable convolution on the other hand breaks us down into two parts the first is depth wise convolution that is it performs the filtering stage and then point wise convolution which performs the combining stage let's get into some details here depth wise convolution applies convolution to a single input channel at a time this is different from the standard convolution that applies convolution to all channels let us take the same input volume F to understand this process F has a shape D F cross D F cross M where D F is the width and height of the input volume and M is the number of input channels like I mentioned before for depth wise convolution we use filters or kernels K of shape DK cross DK cross one here DK is the width and height of the square kernel and it has a depth of 1 because this convolution is only applied to a channel unlike standard convolution which is applied throughout the entire day and since we apply one kernel to a single input channel we require M such DK cross DK cross one kernels over the entire input volume F for each of these M convolutions we end up with an output DG cross DG cross one in shape now stacking these outputs together we have an output volume of G which is of shape DG cross DG cross M this is the end of the first phase that is the end of depth wise convolution now this is succeeded by point wise convolution point wise convolution involves performing the linear combination of each of these layers here the input is the volume of shape DG cross DG cross M the filter K PC has a shape one cross one cross M this is basically a 1 Cross 1 convolution operation over all M layers the output will thus have the same input width and height as the input D G cross DG for each filter assuming that we want to use some n such filters the output volume becomes D G cross DG cross n so that's great we got this down now let's take a look at the complexity of this convolution we can split this into two parts as we have two phases first we compute the number of multiplications in depth wise convolution so here the kernels have a shape DK cross D K cross 1 so the number of multiplications on one convolution operation is all DK times DK DK square when applied over the entire input channel this convolution is performed DG x DG number of times so the number of multiplications for the kernel over the input channel becomes DG square times DK square now such multiplications are applied over all em input channels for each channel we have a different kernel and hence the total number of multiplications in the first phase that is depth wise convolution is M times D G square times D K square next we compute the number of multiplications in the second phase that is point wise convolution here the kernels have a shape one cross one cross M where m is the depth of the input volume and hence the number of multiplications for one instance of convolution is M this is applied to the entire output of the first phase which has a width and height of D G so the total number of multiplications for this kernel is d G times D G times M so for some n kernels will have n times D G times D G times M such multiplications and thus the total number of multiplications is the sum of multiplications in the depth wise convolution stage plus the number of multiplications in the point-wise convolution stage we can take M times D G squared common now we compare the standard convolution with depth wise convolution we get the ratio as the sum of reciprocal of the depth of output volume that is n and the reciprocal of the squared dimensions of the kernel DK to put this into perspective of how effective depth wise convolution is let us take an example so consider the output feature volume n of 1024 and a kernel of size 3 that's DK is equal to 3 plugging these values into the relation we get zero point 1 1 2 in other words standard convolution has 9 times more the number of multiplications as that of depth Y separable convolution this is a lot of computing power we can also quickly compare the number of parameters in both convolutions in standard convolution each kernel has k times D K times M learn about parameters since there are n such kernels there are n times M times D K squared parameters in depth by separable convolutions will split this once again into two parts in the depth wise convolution phase we use M kernels of shape DK cross DK in point wise convolution we use n kernels of shape 1 Cross 1 cross M so the total is M times DK square plus M times n or we can just take M common taking the ratio we get the same ratio as we did for computational power required so we understood exactly what depth wise convolution is and also its computation power with respect to the traditional standard convolution but where exactly has this been used well there are some very interesting papers here the first is on multi model neural networks these are networks designed to solve multiple problems using a single network a multi model network has four parts the first is modality Nets to convert different input types to a universal internal representation then we have an encoder to process inputs we have a mixer to encode inputs with previous outputs and we have a decoder to generate outputs a fundamental component of each of these parts is depth wise separable convolution it works effectively in such large networks next up we have exception a convolution neural network architecture based entirely on depth wise separable convolution layers it has shown the state-of-the-art performance on large datasets like Google's jft image data set it's a repository of 350 million images with 17,000 class labels to put this into perspective the popular image net took 3 days to Train however to Train even a subset of this jft data set it took a month and it didn't even converge in fact it would have approximately taken about three months to converge how'd they let it run to its full length so that's useful this paper is pushing convolution neural networks to use depth Y separable convolution as the de facto up third we have mobile Nets a neural network architecture that strives to minimize latency of smaller scale networks so that computer vision applications run well on mobile devices mobile nets used F Y separable convolutions in its 28 layer architecture this paper compares the performance of mobile nets with fully connected layers versus depth wise separable convolution layers it turns out the accuracy on image net only drops a 1% while using significantly less number of parameters from twenty nine point three million the number of parameters it's down to just 4.2 million we can see the mulch as the number of multiplications and additions which is a direct measure of computation has also significantly decreased for depth by separable convolution mobile Nets so here are some things to remember in this video depth Y separable convolution decreases the computation and number of parameters when compared to standard convolution second is that depth Y separable convolution is a combination of depth wise convolution followed by a point wise convolution depth wise convolution is the filtering step and point wise convolution can be thought of as the combination step finally they have been successfully implemented in neural network architectures like multi model networks exception and mobile nets and that's all I have for you now thank you all for stopping by today if you liked the video hit that like button if you want to stick around hit that subscribe button if you really want to stick around hit that Bell icon next to the subscribe button so as to be notified of my uploads immediately links to important papers are down below so check them out have a good day and I'll see you in the next one bye",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "W2rWgXJBZhU",
        "video_title": "Attention in Neural Networks",
        "position_in_playlist": 10,
        "description": "In this video, we discuss Attention in neural networks. We go through Soft and hard attention, discuss the architecture with examples. SUBSCRIBE to the channel for more awesome content! \n\nMy video on Generative Adversarial Networks: https://www.youtube.com/watch?v=O8LAi6ksC80\nMy video on Convolution Neural Networks: https://www.youtube.com/watch?v=m8pOnJxOcqY\n\nREFERENCES\nShow attend and tell (Image Captioning): https://arxiv.org/pdf/1502.03044.pdf\nWhat is attention: https://blog.heuritech.com/2016/01/20/attention-mechanism/\nAttention is all you need: https://arxiv.org/pdf/1706.03762v5.pdf\nNice blog on Attention: http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\nFeed Forward + Attention can solve problems: https://arxiv.org/pdf/1512.08756.pdf\nTeaching Machines to Read and Comprehend: https://arxiv.org/pdf/1506.03340.pdf",
        "transcript": "[Music] say I give you the deep learning book along with the question how is convolution equivalent with respect to translation what would you do to answer this question well one way you can do this is to read the entire book and assuming you remember everything you've read try to answer the question but there's a better way since it's a question on convolution I flip to the chapter on convolution neural networks then I find equivalence as one of the properties and read out that page or at least that part of the page which do you think is a faster method if we read the entire text like in the first method answering the question may take us a few weeks but in the second method the same can be done within a few minutes that's a very big difference furthermore our answer while reading the entire book may be more vague as it's based on too much information what did we do differently here in the former case we didn't focus on any part of the book specifically whereas in the latter case we focused our attention to the chapter on convolution neural networks and then further focused our attention to the part where the concept of equal variance is explained this second approach would be the exact thought process many of us humans would take it's quite intuitive given this example scenario we can now better define atencion atencion mechanisms found in neural networks is somewhat similar to that found in humans they focus in high resolution on certain parts of the input while the rest of the input is in low resolution or blurred in this video I'm going to talk about the attention mechanism applied on image inputs let's take a look at visual attention at a higher level consider the problem of determining appropriate captions for an input image based on the papers show tell and attend this normally consists of two steps first is to encode the image in an internal vector representation H using a convolution neural network and then we decode H into word vectors signifying the captions using a recurrent neural network the problem with this method is when generating a single word of the caption the LST M looks at the entire image representation H every time this is not very efficient as usually we generate different words of a caption looking at different and specific parts of an image to solve this problem we create n different non-overlapping sub regions hence H I would be the internal feature representation used to generate the eighth word it is not necessarily the representation of the I 3 gene of the original image I'll explain this in a bit for now the figure on screen is a high-level diagram of attention when the decoder decides on a caption for every word it only looks at specific regions of the image leading to a more accurate description now that's good but how does it exactly decide the region or regions to consider this is the crux of the attention mechanism an attention unit considers all sub regions and contexts as its input and it outputs the weighted arithmetic mean of these regions arithmetic mean is the inner product of actual values and their probabilities how are these probabilities and weights determined they are determined using the context C context represents everything that recurrent neural network has output until now let's take a closer look at what happens we have input regions Y from the convolution neural net and the context see from the RN these inputs are applied to weights which constitute the learn about parameters of the attention unit this means the weight vectors update as we get more training data we apply a tange activation so that of very high values tend to have very small differences and be close to one and very low values also a very small difference is closer to minus one this leads to a much smoother choice of regions of interest within each sub region it is more fine-grained so to speak note we don't necessarily have to apply a tange function we only need to ensure the regions that we output are relevant to the context in the simplest form this similarity can be determined with a simple dot product between the regions Y and the context C the more similar they are the higher is the product hence the output is guaranteed to weight the more relevant region why I hire the difference of using the simple inner product and tange function would be grin you ality of the output regions of interest tange is more fine-grained with less choppy and smoother parts of sub regions chosen regardless of how they are calculated these M's are then passed through a softmax function which outputs them as probabilities s finally we take the inner product of this probability vector s and the sub regions Y to get the final output Z of relevant regions of the entire image understand the probabilities as correspond to the relevance of the sub regions Y given the context C now there are two types of attention mechanisms the first is soft attention and then we have hard attention the main difference here is that in soft attention the main relevant region C consists of different parts of different sub regions wide in heart attention the main relevant region Z consists of only one of the regions why I'll explain them both in detail the entire mechanism of attention that I described until now is all soft attention Z has relevant parts of different regions soft attention is deterministic so deterministic what's that a system is said to be deterministic if the application of an action a on a state s always leads to the same state s prime a dumb example would be you're at a corner of your room at coordinates 0 0 and you're facing forward consider an action a which is moving 5 feet forward the system is now at a new state with the coordinates 5 0 and still facing forward no matter how many times you stand at the corner of your room forward facing and walk five feet forward you will always end up 5 feet from the door and facing forward try it trust me it works hence the system is deterministic let us apply the same concept to soft attention initially we have an image just split into a number of regions why with an input context see this is our initial state on the application of soft attention we end up with a localized image representing the new state s Prime these regions of interest are determined from Z the RO eyes will always be the same regardless of how many times we execute soft attention with these same inputs this is because we consider all the regions Y anyways to determine Z now consider heart attention looking at the architecture heart attention is very similar to soft attention however instead of taking the weighted arithmetic mean of all regions heart attention only considers one region randomly so heart attention is a stochastic process now stochastic when you hear the word stochastic think about randomness in such a stochastic process performing an action a on a state s may lead to different states every time typical example is like in a board game with the dice like snakes and ladders the initial state is the position of the players the action is rolling a dice and depending on the roll there are multiple possibilities for the next board state what makes hard attention stochastic is that a region Y I is chosen randomly with the probability si this means that the more relevant a region Y I as a whole is relevant to the context then greater the chance it is chosen for determining the next word of the caption using the word captions output until now by the RNN that is H along the current regions of interest in an image determined by the attention mechanism the RNN now tries to predict the next word in the caption as far as performance is concerned in the papers show attend Intel released by the University of Toronto and University of Montreal in 2016 results vary with the data set soft and heart attention both perform decently well with heart attention performing it slightly better this is pretty cool right so where else can we use attention attention is not only used for image inputs for example neural machine translation nmt systems they are used to translate one language to another words are fed in a sequence to an encoder one after another and the sentence is terminated by a specific input word or symbol once complete the special signal initiates the decoder phase where the translated words are generated another cool application would be Microsoft's attention generative adversarial networks or Microsoft's attention gann that can create images from text through natural language processing it can perform fine-grained tasks like generating parts of an image from a single word in the description another application would be in the paper of teaching machines to read and comprehend the altars do the same thing I talked about in the beginning of the video a recurrent neural network takes some text and a question as input and it is made to output an answer here are some things to remember attention involves focus in high resolution on certain parts of an input while the rest of the input is in low resolution or is blurred two types of attention are soft attention and hard attention soft attention is deterministic while hard attention is stochastic attention can be used for non image inputs like neural machine translation attention Gantz and answering questions from text and that's all I have for you now hope you guys got some newfound understanding of attention and its applications in this video I have left a link to the main paper show attend intel with other papers and blog posts in the description down below don't forget to give the video a thumbs up and subscribe for more awesome content please subscribe please you did it right guys",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "4tkgOzQ9yyo",
        "video_title": "Mask Region based Convolution Neural Networks - EXPLAINED!",
        "position_in_playlist": 11,
        "description": "In this video, we will take a look at new type of neural network architecture called \"Masked Region based Convolution Neural Networks\", Masked R-CNN for short. And in the process, highlight some key sub problems in computer vision.\n\nPlease SUBSCRIBE to the channel for more content on Machine Learning, Deep Learning, Data Science, and Artificial Intelligence. Hoping to build a community of AI geeks. You'll fit right in!\n\nREFERENCES\n\nMain paper: https://arxiv.org/pdf/1703.06870v3.pdf\nCode: https://github.com/facebookresearch/Detectron\n\nConvolution Neural networks: https://www.youtube.com/watch?v=m8pOnJxOcqY\nSemantic segmentation in deep learning: http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review\nTop papers: http://www.arxiv-sanity.com/top?timefilter=alltime&vfilter=all\nRecurrent Instance Segmentation: http://www.robots.ox.ac.uk/~tvg/publications/2016/RIS7.pdf\nMask R-CNN Presentation by the Author: https://www.youtube.com/watch?v=g7z4mkfRjI4\nMark Jay's Video: https://www.youtube.com/watch?v=2TikTv6PWDw\nCOCO dataset: http://cocodataset.org/#home\nFully Convolutional Networks: https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf\nFaster R-CNN explained: https://medium.com/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8\nNotes/summary of Masked R-CNN: http://www.shortscience.org/paper?bibtexKey=journals/corr/HeGDG17#aleju\n\nMusic at : https://www.bensound.com/royalty-free-music/track/tenderness",
        "transcript": "the most intriguing advancements brought by deep learning and neural networks is in the field of computer vision we associate any problem that has an image or camera input to encompass problems within computer vision self-driving cars fMRI analysis Mars exploration Rovers facial recognition systems object detection and augmented reality are just a few breakthroughs in the field in this video we will take a look at a new type of neural network architecture called masked region based convolutional neural networks masked are CNN for short and in the process highlight some key subproblems in computer vision as well masked our CNN works towards the problem of instant segmentation the process of detecting and delineating each distinct object of interest in an image and so instant segmentation is a combination of two subproblems the first is object detection and this is the problem of finding and classifying a variable number of objects in an image they are variable numbered because the number of objects detected in an image can vary image to image and the second part of instant segmentation is semantic segmentation semantic segmentation is the understanding of an image at the pixel level that is we want to assign an object class to each pixel in the image in this figure with the motorcyclists apart from recognizing the bike and the person riding it we also have to delineate the boundaries of each object using object detection and semantic segmentation together we get instant segmentation in these images the bounding box is created from object detection and the shaded masks are the output of semantic segmentation now that we have a high-level intuition of instant segmentation we'll take a look at the architecture behind massed our CNN since there are two phases we have two parts for object detection it uses an architecture similar to faster our cnn's for semantic segmentation it uses fully convolution networks so first off what is an AR CNN it is an approach to bounding box object detection thus creating a number of object regions or regions of interest ro eyes the next version faster our CNN performs a better job by incorporating an attention mechanism using a region proposal network an RPN faster our CNN performs object detection in two stages first determine the bounding box and hence determining the regions of interest this is done using the RPM protocol like I just said before and second for each ROI we determine the class label of the object this is done with ROI pooling masked our CNN does incorporate these tasks but there is a problem of data loss in ROI pooling this involves the applying of pooling usually max polling on a region of interest the bounding box computer during object detection hence the name ROI pool in this method the stride is quantized now pooling is used for down sampling of features and is used to introduce invariance to minor distortions and input these minor distortions could be something as simple as rotation of an image so consider this five but even if it is rotated our models should still consider this image as a five that is the same input so polling enables a model to become invariant to such rotation in this case stride is the number of cells by which we move our sliding window during pooling or during convolution if you want more information about pooling and the intuition on stride check out my video on convolution neural networks now coming back to ROI polling when I say that the stride is quantized what do I mean consider a region of interest of say 17 crossed 17 and we need to map it to a space of seven cross seven the required stride is 17 divided by 7 which is 2.4 - since a stride of 2.4 - is meaningless ROI pulling will quantize this value by rounding it down to 2 so it will use a stride of 2 along the width and the height however in doing so it only considers the top 14 cross 14 pixels in the 17 cross 17 region the remaining points are lost not only is there a loss of data but this can also lead to misalignment if we use an 18 cross 18 input and map it to a 7 cross 7 output the required stride becomes 2 point 5 7 this rounds up to 3 in ROI pooling so you can see that there's a misalignment when we perform polling here now to address this problem roi align is used no quantization takes place so in the case of the 17 car 17 input region we consider a 2.4 - stride as it is however this value is meaningless each cell is divided into a 2 cross 2 bin so that creates 4 regions in the top left the top right the bottom left and the bottom right and each of these sub cells is pulled through by linear interpolation leading to 4 values per cell and the final cell value is then computed by either an average or the maximum over the 4 sub values by addressing the loss and misalignment problems of ROI pooling the new ROI aligned leads to improve results ROI align is thus better than ROI pool as it allows us to preserve spatial pixel to pixel alignment for every region of interest and there is no information lost as there is no quantization conceptually the Mast our CNN is similar to the faster our CNN master our CNN additionally outputs the object mask using pixel to pixel alignment this mask is a binary mask outputted for each region of interest overhead isn't incurred when computing this mass as it is done in parallel with the bounding box creation and classification consider a region of interest of M cross M pixels let's assume that there is K possible objects that it could be for example in an image if we were trying to categorize humans dogs and cats then K would be equal to 3 for each type K a binary mask M cross M is constructed analog is 2 a 1 versus rest approach hence while computing the mask a loss of km square is incurred this is different from the typical approach of constructing a single mask from K classes as the classes would compete in the mask this lack of competition is the key to good performance in instance segmentation in each region of interest ROI determined in the object detection phase let's take a look at the semantic segmentation with FC ends fully convolution networks FC ends are used to predict the mass from each ROI so why are we using convolutional layers this is because convolution layers retain spatial orientation such information is crucial for location specific tasks like creating an object mask so you can see why the traditional use of fully connected layers won't work here in fully connected layers a spatial orientation of pixels with respect to each other is lost as there are squished together to form a feature vector in facebook AI research the cocoa dataset is used it's a large-scale data set for object detection segmentation and captioning there are over 200,000 labelled images consisting of 1.5 billion objects masked our CNN takes about 1 to 2 days to train on this data set using an a GPU machine it achieves good results even for challenging images here's a comparison with respect to the state-of-the-art fully convolution instant segmentation system FC is FC is is an alternate framework that also uses semantic segmentation and object detection to categorize box and mask objects in an image and it does it fast but FC is exhibits systematic errors on overlapping instances and creates spurs edges showing that it is challenged by the fundamental difficulties of segmenting instances here are some key things to remember in segmentation is object detection with semantic segmentation Masdar CN n is an architecture to achieve instance segmentation it combines faster our CN NS with fully convolution networks fcns masked our CN n uses ry align which preserves the spatial orientation of features and leads to no loss of information and there is that the new masked our CN n for instant segmentation I'll leave a link to the main paper their code and links to other cool blog posts and papers in the description down below so check that out to leave a like and comment down below on your thoughts on this new technology subscribe to the channel for more SuperDuper content and I will see you in the next one see you",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "NJBLx29JuHs",
        "video_title": "Deep Mind's AlphaGo Zero - EXPLAINED",
        "position_in_playlist": 12,
        "description": "We take a look at the best Go player of all time AlphaGo Zero. How does it work? What kind of Neural Network does it use? How does Monte Carlo Tree Search help in Learning? We'll answer these questions in this video. \n\nSUBSCRIBE on your way out for more awesome content!\n\n\nREFERENCES\n\nDeep mind blog on Alpha Zero: https://deepmind.com/blog/alphago-zero-learning-scratch/\nVideo on Alhpa Go Zero: https://www.youtube.com/watch?v=tXlM99xPQC8\nAlhpa Go: https://deepmind.com/research/alphago/\nTop list in forbes: https://www.forbes.com/sites/mariyayao/2018/02/05/12-amazing-deep-learning-breakthroughs-of-2017/#1a81502665db\nBlog post: https://web.stanford.edu/~surag/posts/alphazero.html\nMonte Carlo Tree Search: https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\nMCTS in Go: https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf\nAlphago Zero Vs Alpha Go: https://www.youtube.com/watch?v=jeVihsgCeeE\nTricks that AlphaGo zero used: https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef\n\n\nMIC REFENCES\n\nGAN Image from: https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39\nThumbnail Background: https://www.upi.com/Odd_News/2016/03/12/Googles-AlphaGo-computer-outplays-board-game-champion/7721457815287/\nAlphaGo symbol image in thumbnail: https://www.theinquirer.net/inquirer/news/3019507/googles-deepmind-learns-by-itself-as-alphago-zero-beats-its-predecessor\nMario Kart Image: http://mariokart.wikia.com/wiki/Ghosts",
        "transcript": "[Music] artificial intelligence is one of the biggest buzz words of the decade an AI is usually created through human generated data but such data is unreliable or simply just not available for example if we want to build a machine learning classifier that classifies objects we can't just tell a machine to do it it needs a label data set consisting of images containing an object and the corresponding object category you agree with that right now this category is manually labeled by humans and can be collected from one or many sources regardless for this problem and many other machine learning problems there may not be enough label data out there for a machine to effectively learn anything and even if there is data available there's no guarantee that all the labels are correct since we're going to talk about gameplay here is a more fitting example training data taken from humans playing games is only as perfect as the humans playing those games they are prone to errors so one thing is for certain incorrect data leads to misleading results so less human intervention in the machine learning process higher is the chance of fitting a better model that adequately generalizes the task at hand this is the philosophy behind deep Minds alphago zero and is the primary reason it is the best go player in the world of humans and human dependent AI go is an ancient two-player board game invented in China over 2,500 years ago it is one of the oldest board games played today alphago zero is a successor of the famous alphago now alphago was the first computer that was able to defeat one of the best go players in history Lisa doll with an overwhelming four to one victory alphago didn't just defeat prejudice go masters it also used techniques considered revolutionary and is currently studied by enthusiastic go players to think that a machine could contribute this much knowledge to the most studied game in history is a feat in itself alphago use reinforcement learning to learn the game it looked at thousands of games played by grandmasters and other professionals to climb the ranks and become the best go player of all time however all of this changed with alphago zero a system that doesn't learn from games played by humans but rather games played by itself by getting rid of human input data this new system was able to beat its predecessor alphago 100 to 0 games however we have learned success stories of many systems that learn this way for example dqn learns Atari games without human input data so what makes alphago zero anything special it is because we're considering the game of go a game that requires a lot of forethought and has a much larger search space because of the thousands of possible moves that can be made and even after making a move there are thousands of other possibilities that branch from it alphago was the first AI to achieve superhuman performance in the game I didn't find the exact implementation details of alphago but we have the published version of alphago fan this used to neural networks a policy network and a value network a policy is an action or a set of actions to take the policy network is a supervised learning model that determines the next best move or the optimal policy the value network predicts the outcome of the game depending on the move made by the policy model once trained these two neural networks are combined with the Monte Carlo tree search I'll explain this shortly but basically Monte Carlo tree search was used in the policy network to narrow down the best possible moves and the value network was used to evaluate future performance when these moves are made providing a look ahead type mechanism this allowed the AI to make an optimal move now alphago zero takes a different approach there are four key differences between alphago zero and its predecessor the first is self play reinforcement learning is used so there's no supervised learning using human data the second is that the only input features are the black and white stones that are used in the game then unlike golf ago which uses two neural networks this uses only a single neural network and the fourth it uses a simpler search tree mechanism to determine the next best move this will make a lot more sense when I go through the details of alphago zero so let's do that now consider a neural network F initialize with random weights theta zero this network consists of numerous layers of convolution bash normalization and activation and is finally made to output two quantities the first is a vector of move probabilities in other words a probability of taking an action a at a certain state and the second is a scalar V which takes two possible values one to indicate the current player wins given the current state S or negative one which indicates of loss from the current state s so these values are predicted by the neural network at the state say s T the goal here is to train our neural network to estimate the accurate move probabilities so it can make an optimal move in the actual gameplay in other words the goal is to make the neural network good at predicting P the move probabilities P over time will help us determine the optimal policy when you play Gogh or chess or any other two-player game what do you do before you make a move you think you think about the best move to make based on the board alphago zero thinks and makes a decision using the Monte Carlo tree search and CTS from a high level perspective MCTS takes the current state that is the board and the output of the latest Network F theta T minus one in order to make the next best move so how does use the neural network initially the neural network predicts some output move probabilities P but alphago 0 is not very confident in this policy determined by the neural network and it is right to think so after all we just randomly initialized our neural network with some weights we should just take the current predictions of P with a grain of salt so here is a scenario it's alphago 0 vs alphago zeroes ghosts and it's our turn time to think so alphago zero determines the current board state and executes MCTS Monte Carlo tree search it uses Monte Carlo roll out that is it hypothesizes random moves and determines the rewards along the way the deeper down the tree it goes the more into the future it simulates the current game visualizing as many scenarios as possible the Q values for every state are updated by backing up the tree this allows us to determine better move policies PI and better policies that alpha goes 0 is more confident in so since alpha go zero has determined a better policy it makes a move based on the new move probabilities PI now the weights of the neural network are updated such that its predicted move probabilities P more closely aligned with M CTS's move probabilities PI also the neural networks outcome predicted winner for them the current state V should closely map to the outcome predicted by Monte Carlo tree search which is Z and so we have an objective function or a loss that the neural network should minimize so it works to minimize the distance between the scalars Z and V and also to minimize the distance between two vectors PI and P once the ghost makes its move we see the board state at st plus 1 we do the same thing we did in the previous state the neural network generates move probabilities P and predicts the winner from the current board V alpha go 0 is now a little more confident in these probabilities so from the current state it executes the policy output P but after a few iterations it grows less and less confident of the policy and hence alphago zero defaults to rollout that is randomly hypothesizing actions it collects rewards along the way until time runs out for that move the queue values are backed up and updated for every state this new set of Q values determines a more confident move probability vector PI T plus one and hence exhibits stronger confidence in this new policy so it updates the neural network parameters to theta T plus 1 and PI T plus 1 is used to determine the next best move this process goes on I'm not sure if you already picked this up already but here's the analogy the idea of self learning involves alphago 0 playing itself to constantly improve its former self is the neural network this helps MCTS generate a better policy and allows the neural network to improve that is true its current self I would make the analogy of Mario Kart where you play against your own ghost but this isn't really the case because in this case the ghost is helping you get better I would say the concept is more similar to games generative adversarial networks where the generator gets better at fabricating data and the discriminator gets better at recognizing fabricated data and these models help each other improve over time the result alphago 0 initially started out with just input rules to the game and no prior knowledge after three days it surpassed human level of go playing after 21 days it reaches the level of alphago master that defeated go masters including the reigning world champion kg in May 2017 and after 40 days it surpassed all versions of alphago to become the best go player of all time a very impressive feat indeed makes you wonder about the rapid improvement of gameplay over the course of just a few years we've come a long way but there is still much to improve a very intriguing future for a very intriguing field thanks for stopping by today if you liked the video please give it a big sup and subscribe to the channel for more awesome content subscribing allows immediate access to this wonderful content and it makes me feel giddy to look at the numbers it's a win-win situation so do it see you guys in the next one bye bye",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "le1LH4nPfmE",
        "video_title": "Tacotron 2 - THE BEST TEXT TO SPEECH AI YET!",
        "position_in_playlist": 13,
        "description": "In this video, I am going to talk about the new Tacotron 2- google's the text to speech system that is as close to human speech till date.\n\nIf you like the video, SUBSCRIBE for more awesome content. \n\nResearch paper: https://arxiv.org/pdf/1712.05884.pdf\nRead some of my AI answers on Quora: https://www.quora.com/profile/Ajay-Halthor\nMusic at : https://www.bensound.com/royalty-free-music/track/tenderness",
        "transcript": "so last month Google released a paper on Taco Tron - a neural network architecture for text to speech synthesis it consists of two components a network to convert character sequences into mel spectrograms and a modified version of wavenet which acts as the vocoder to synthesize the speech wavenet was introduced by google's deepmind in september 2016 it's used to generate the voices in Google assistant so what was wrong with wavenet the input requires linguistic features fundamental frequencies and phoneme durations all of this which requires significant domain knowledge to interpret and process tako Tron introduced in August 2017 alleviated this problem with sequence a sequence architecture where the input character sequences were transformed into magnitude spectrograms this eliminated the need for complex linguistic and acoustic input features as we could now use raw data in the original taco Tron the spectrogram was passed into a vocoder which used the Griffin limb algorithm for speech synthesis however wavenet produced more human-like voices and so we have the topic of our video today replacing the old Griffin limb algorithm with a wave net like vocoder in taco Tron Google has now come up with taco Tron - as mentioned at the beginning the model architecture consists of two components a network which is a mix of both LS TM and CNN layers to predict mel spectrogram frames from input character sequences and a variation of wavenet to generate speech from predicted spectrogram frames so why use mel spectrogram as its intermediate form we have two main reasons for this it facilitates faster training of the network part and wavenet modification parts separately and second it emphasizes the recognition of lower frequency sounds over the bursts of high frequency auditory signals this ensures that the spectrogram can be used to produce intelligible speech patterns and this is the primary reason for its use over the years an advantage of using mel spectrograms is simplicity however it disregards information that would have been provided by linguistic and acoustic features used in the traditional wave net despite this lossy information it is still possible to generate high-quality audio using mel spectrograms and modified wavenet so let's take a look at the two components taco Tron - like its predecessor uses the short time Fourier series transforms to compute mel spectrograms this network consists of an encoder to convert character sequences to an internal feature representation and a decoder to convert these internal feature representations to frames of the spectrogram the input is a 512 dimensional vector of character embeddings this is passed through three layers of convolution using a five cross one filter each layers exceeded by batch normalization pass through a ray Loewy activation function the point of these convolution layers is to recognize engrams or phrases the result is then passed into a bi-directional LS TM with five and twelve units to generate encoded features the encoded features are passed into an attention unit which converts the features into a fixed-length context vectors of size 128 this is the internal feature representation from here on out we have the decoder part the prediction of the previous time step is first passed through a small pre net which is basically two fully connected layers each with 256 neurons and array low activation the pre net output and the attention context vector are concatenated by two unidirectional else TM layers with 1,024 neurons the output is projected on a linear transform to get the predicted spectrogram frame this is then passed to a post net which is essentially five convolutional layers each followed by batch normalization and at NH activation this is done to enhance spectrogram frame prediction which is then passed to the second component of tako Tron - that is the modified wavenet vocoder the original wavenet consisted of 30 dilated convolutional layers grouped into three dilated cycles such dilated convolution is much faster than the original convolution operation it uses less parameters captures a broad view of the input and also captures more detail this modification also uses pixel CN n plus plus a generative model which more accurately models gradients in the spectrogram input for more sophisticated speech synthesis more technically it uses a ten component mixture of logistic distributions to generate 16-bit samples at 24 kilohertz wavenet uses linguistic features phoneme durations and has a frame rate of 5 milliseconds this modification however spreads the frames out with the 12.5 millisecond frame rate to resolve issues of predicting the spectrogram frames the resulting system significantly outperforms all other Texas feed systems with its MOS that is mean opinion score comparable to the ground truth to prove this fact let's consider some generated audio we'll listen to two recordings one by human and the other by the AI George Washington was the first president of the United States here's the other one George Washington was the first president of the United States did you spot the differences now let's try another recording that girl did a video about Star Wars lipstick and the next one that girl did a video about Star Wars lipstick how about now could you tell which is human and witch's machine well I sure couldn't the Texas speech system could also handle enunciation through capital letters the buses aren't the problem they actually provide a solution the buses aren't the problem they actually provide a solution and there you have it Google's new AI - synthesized speech that is as close to human voice thus far it'll be interesting to see how new advancements take place in the field but that's all I have for you now if you like the video do me a favor and drop a like subscribe to the channel for more awesome content and I will see you in the next one bye you",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "EVgdkt-PAHc",
        "video_title": "When will AI take your job?",
        "position_in_playlist": 14,
        "description": "Ever wondered how fast AI is evolving? When do you think your job will be taken? Let me know your thoughts in the comments below.\n\nSource (When will AI exceed Human Performance):  https://arxiv.org/pdf/1705.08807.pdf\n\nSOCIAL LINKS\nFollow me on Quora: https://www.quora.com/profile/Ajay-Ha...\nEmail: ask.ajhalthor@gmail.com",
        "transcript": "we've all thought about this at some point of time you're sitting at your desk and you're like I wonder when I'm gonna lose my job some damn robot it doesn't matter what your job is it's more of a question of when you'll lose your job rather than if you lose your job we're gonna take a look at projections on how long it will take to achieve high-level machine intelligence HL mi a time when robots will be able to perform every task more economically than human beings this time tell HMI was determined by surveying a group of individuals who published in nipson ICML top machine learning conferences in 2016 so we know we're getting the opinions of the best of the best just a disclaimer these are just projections and not the absolute truth so don't come to me when you don't have your flying car by 2047 here are some numbers on HL mi there is about a 50% chance we'll reach the state of high-level machine intelligence by 2060 143 years from now interestingly Asian researchers predict it'll be much sooner than later about 30 years from now while American researchers say the same will happen much later while HL mi is defined for how well a robot will perform in tasks in general it's also interesting to see its consequences on employment how long would it take for a machine to do every person's job better and more economically than human workers so here are some projections twenty years from now machines will become as good as the best retail person in thirty years they get the qualities of a top journalists so much so to write a best-seller it'll take them about 35 to 40 years to perform successful surgeries it's about 85 to 90 years to take the job of an AI researcher and finally over a period of 122 years to take all jobs so we have some interesting milestones here but at what pace will these machines of all over time will we see a gradual increase in their capabilities through these 120 years or at some point in the middle will we see some burst in AI technology well according to these researchers the bursts will take place shortly after HMI and this is known as the intelligence explosion it is in this time that AI will be vastly superior to human beings in performing all tasks while this advancement is welcomed to many it is important to keep in mind the risk in such evolution about 48% of these researchers agree this is also probably why we're seeing more videos and content and blog posts on AI safety you can't raise an AI as you would raise a kid it's just not the same but as long as we're aware of such risks involved in advancement of AI and technology we should be fine right well whatever you think just leave your comments down below how off or on point do you think these projections are personally I think they're much shorter than expected because I can't imagine in my lifetime some robot coming up to me and just while I'm working and say it's all you tations I can do your work now get out good that's all I have for you now so if you like that video hit that like button if you're new here welcome and hit that subscribe button ring that bell while you're at it still looking for some more interesting content on AI and machine learning click or talk one of the videos right here for another awesome video and I'll see you in the next one bye",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "C1YUYWP-6rE",
        "video_title": "Evolution of Face Generation |  Evolution of GANs",
        "position_in_playlist": 15,
        "description": "Generating Fake faces in photos and videos has become prevalent in last few years (cough) Deep Fakes (cough). It's come to a point where we can't tell if photos and videos contain real people. It's fascinating (and scary?) ! But how did we get here? What is the technology behind these eerily real faces?\n\nThe main Technology behind this phenomenon are \"Generative Adversarial Networks\". Let's talk about them. \n\nREFERENCES\n[1] The OG GAN : https://arxiv.org/abs/1406.2661\n[2] Deep Convolutional GAN (DCGAN): https://arxiv.org/abs/1511.06434\n[3] Coupled GANs: https://arxiv.org/abs/1606.07536\n[4] Progressively growing GAN: https://arxiv.org/abs/1710.10196\n[5] Style-Transfer GAN: https://arxiv.org/abs/1812.04948\n[6] DeepFakes - Real Consequences (ColdFusion): https://www.youtube.com/watch?v=dMF2i3A9Lzw \n[7] First paper on Neural Networks: http://www.cse.chalmers.se/~coquand/AUTOMATA/mcp.pdf \n[8] How fast processing power increased over the years: https://www.eetimes.com/author.asp?section_id=36&doc_id=1330462\n[9] Deep Learning Vs Classical Machine Learning\n[10] Neural Networks in Security Systems: https://ieeexplore.ieee.org/document/8268746 \n[11] State of the Art Neural Networks for Image Captioning: https://cs.stanford.edu/people/karpathy/sfmltalk.pdf\n[12] Applications of Neural Networks: https://arxiv.org/pdf/1803.01164.pdf \n[13] Build a Music & Text Generator: https://towardsdatascience.com/deep-learning-with-tensorflow-part-3-music-and-text-generation-8a3fbfdc5e9b \n[14] CycleGANs: https://arxiv.org/pdf/1703.10593.pdf",
        "transcript": "hey guys so I'm making this video because of a tweet that I saw by Ian good fellow now Ian Goodfellow is an AI researcher at Google brain and he's responsible for creating something called a generative adversarial Network again way back in 2014 now ganz are responsible for creating those fake pictures with people who don't exist over the past year we've also seen the rise of deep fakes now deep fakes are videos where someone's face is plastered on some video so it creates a video of a person doing something that they didn't actually do in real life it's quite eerie to see how we're able to doctor images and videos in this way but at the same time it's also incredible to see how far technology has advanced in just five years but how did we get here we're gonna talk about just that and this video won't be too technical so if you're curious about AI but really don't know the main mechanics behind it many applications in deep learning then don't worry about it you'll still follow along I'll probably throw in some extra tech jargon in there for extra curious viewers this is code Emporium and so let's get started in this video we're going to take a look at how to create images of people who don't exist now these face generators are composed of a fundamental structure called a neural network these networks take information from one end they perform some processing in the middle and they spit out a result if you want to solve simple problems then simple networks are good enough a simple problem could be you throw an email into a network and it determines if the email is either spam or non-spam pretty simple but the problem of face generation that we're talking about is kind of complex it's complex because the network first needs to read an image input then needs to determine the features on your face like position of your eyes the nose the mouth it needs to determine the texture of facial features determine the contours of these features and so much more the neural network will only be able to generate its own faces only once it understands how phases work as you can imagine this could take a lot of time a lot of processing power a lot of data and a complex neural network so complex neural networks are required to solve complex problems now neural networks aren't anything new they've been around since the 1940s but we never had hardware with lots of processing power until fairly recently I'm 23 and I grew up in an age where we stored stuff on floppy disks and come on floppy disks or piece of plastic that holds like 2 megabytes of data and this was a thing 15 years ago so yeah processing power of computers grew exponentially in the last decade and this fact is important because neural networks need a ton of data and a ton of processing power so they were virtually useless it wasn't until 2010 to 2012 that the neural networks exploded in use before they were only good in theory but then they started outperforming everything language translation you feed an input English sentence and the system spits out an output French sentence the state of the art is a neural network Google Translate uses a neural network for its translations security and defense systems the state of the art is a neural network image captioning feed an image to the system and it spits out the description of the image the state of the art is once again a neural network and the list of applications just goes on in all of these applications and neural networks outperform everything else this is also the case with another application which I said we would be talking about face generation like you would guess neural networks are the state of the art for this problem too the neural net that made the major breakthrough by generating decent faces was generative adversarial networks organs these gains are a type of neural network and it's really interesting to note how they work the typical neural net take some information processes it and spits out a result ganz make this interesting by making it into a game of cops and robbers so again is a neural network that consists of two sub networks one of them is the robber and the other is the cop now when I say robber I'm not talking about the guy who steals stuff but he's more of the counterfeiter instead so in this context of face generation the counterfeiter Network creates fake faces and it is the job of the cop to catch them so the cop looks at some face image and it should try to tell which images are counterfeit images and which ones are the real thing the cop and counterfeiter play a game where they take turns it's round one the counterfeiter starts by generating a face image and it puts this image into a pile of containing two types of images the images of real people found online and the images it generated the fake images now it's the cops turn the cop takes an image from the top of the pile and answers a question is this image real or fake if the cop answers the question correctly then he wins otherwise the counterfeiter wins after the round the loser will tweak itself ever so slightly to improve its performance so if the cop network loses then it tweaks its network to become slightly better at detecting fakes and if the counterfeiter Network loses then it tweaks its network to become slightly better at generating fakes so after thousands of rounds the cop becomes better and better at spotting fakes and the counterfeiter becomes better and better at generating fakes and once the game is done we just asked the generator or counterfeiter to generate the image and this image should look exactly like a realistic face now this is the main idea behind generative adversarial networks if you understood this you understood the main point behind the landmark paper of 2014 so yay all along I've addressed the cop and counterfeiters boxes but what exactly are these boxes I said before they are neural networks but not just any neural network they are simple neural networks called multi-layer perceptrons these simple networks are typically used to solve simpler problems and not complex problems like face generation that we're doing now if that's the case then how are we able to get such results it's because we use multiple simple networks to create a complex network this isn't too efficient and we can still see the image quality isn't that great so in 2015 researchers Alec Radford and Luke Metz thought instead of using simple networks to make a complex network let's use slightly more complex networks to make an even more complex network and so instead of having the simple multi-layer perceptrons the cop and counterfeiter has now become more complex convolutional neural networks and this entire network was appropriately named deep convolutional ganz or d.c ganz this was a straightforward advancement we have the problem of dealing with images convolution neural networks works well with images so let's use convolutional neural networks and this showed more promising results around the same time we saw the introduction of another again called a coupled again or a Cogan instead of using one counterfeiter and one cop to learn to create fake images we would use two cops and two counterfeiters so we have two simultaneous games going on in every round here's the idea the counterfeiter networks both share information with each other but they also need to slightly tweak themselves to fool their corresponding cop the result is that we end up with counterfeiters that learn to counterfeit images with slightly different features so it can generate simultaneously a person with blond hair and the same person with brown hair it can also generate a person without glasses and the same person with the version wearing glasses that's pretty neat now over time we've seen different types of Ganz learning to generate faces but all of them have the same problem they aren't exactly high quality this is because the cops would easily tell if an image is fake if the counterfeit are always generated high resolution images since the counterfeiter knows it's going to be too easy for the cop to tell the difference the counterfeiter will just make sure the quality of the image is on the lower side clearly this has its disadvantages we cannot get high quality images from again however this all changed in 2018 with Nvidia usually the cop and counterfeiter played thousands of rounds using similar images of similar quality throughout the game but now we start the cop and counterfeiter out as simple networks with say just for a hundred rounds because of the simple Network the generator will only be able to generate low resolution images and the cop won't really be that good at telling the generated images apart from the real images we're also using real images of low quality too after the first 100 rounds or so we make both networks slightly more complex probably by just adding an additional layer and use high resolution images so progressively as the rounds go on the generator will generate higher and higher quality images and finally we get results that are even difficult for humans to discern all the images you see here are synthesized faces every one is fake none of these guys exist you need to look really hard to tell them apart from real people and this progressive growth addressed a fundamental problem with Ganz the lack of image quality so clearly this is pretty cool but the researchers didn't stop here it's amazing to generate these images but it would be even more amazing to have control over the images being generated say if we want a face with brown hair and that's smiling then we should be able to pass this in to the network and the network should spit out a counterfeit face which has the same features this transfer of style to an image was done by using a slightly more complex counterfeiter before the counterfeiter was just a typical convolutional neural network that generates an image but now the network has more components that allow us to define the kind of image we want to generate the result we have so much more control to generate high-quality images so much so that we can even create a database with these images since its inception in 2014 it's really amazing to see how we've progressed this far into face generation with Ganz and note these same networks can be used to generate all kinds of data it can learn to generate literature like Shakespeare it can learn to paint like Van Gogh it can even learn to play any kind of music to your taste if you want to know more technical details about these neural networks that I've discussed then check out all the other videos on my channel but that's all I got for you now and I hope you understood something from this video show some love with a like and subscribe for our deep learning videos buh bye",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "sIoHFPGOY0I",
        "video_title": "How does Google Translate's AI work?",
        "position_in_playlist": 16,
        "description": "Let\u2019s take a look at how Google Translate\u2019s Neural Network works behind the scenes! Read these references below for the best understanding of Neural Machine Translation! \n\nREFERENCES\n\n[1] Landmark paper of LSTM (Hochreiter et al., 1997): https://www.bioinf.jku.at/publications/older/2604.pdf \n[2] Landmark paper of Neural Machine Translation NMT (Kalchbrenner et al., 2013): https://arxiv.org/abs/1306.3584 \n[3] Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (Cho et al., 2014): https://arxiv.org/abs/1406.1078\n[4] Seq to Seq learning with neural networks (Sutskever et al., 2014): https://arxiv.org/abs/1409.3215)\n[5] The paper that introduced Bidirectional RNN : https://pdfs.semanticscholar.org/4b80/89bc9b49f84de43acc2eb8900035f7d492b2.pdf\n[6] On the properties of NMP: Encoder-Decoder Approaches (Cho et al., 2014): https://arxiv.org/pdf/1409.1259.pdf Fig. 4 (a)\n[7] NMT by jointly learning to align & translate (Bahdanau et al., 2016): https://arxiv.org/pdf/1409.0473.pdf 5.2.2\n[8] Google Translate Main paper (Wu et al., 2016): https://ai.google/research/pubs/pub45610",
        "transcript": "you have international friends talking smack behind your back you use Google Translate you're looking upwards for a French class you regret taking you use Google Translate you're in a foreign country and just want to ask the waiter for some extra cheese in your taco you use Google Translate Google Translate has quite the eclectic of applications but have you ever wondered how does it translate stuff how is that all working online we're going to answer these questions today take your non-techie I'm gonna make sure you all follow along and learn something interesting in the end this is code Emporium and with that let's get started language translation how do we translate a sentence in one language to another language to make things concrete let's say that we're translating from English to French our first trial would be you take every word in the English sentence for every word you find the corresponding French translation then spit it out and we repeat this for every word in the sentence it's a simple strategy and honestly we don't need machine learning for this if we just have a curated database with English to French word translations then we're all set for every English word look it up in the database get the corresponding French word and repeat this for every word that's great but there's a problem with this if you're bilingual or even if you just know English then you know that language has two important components that's tokens and grammar tokens are the smallest units of language grammar defines how these tokens should appear so that they make sense so in this context tokens are words every word is a token it's a beautiful day has five word tokens and grammar is basically a guide or a set of rules that defines an ordering for these words if language was constructed from tokens and grammar didn't matter then language translation would be so much easier and our simple word translation system we came up with would actually be the state-of-the-art translator however that isn't the case grammar exists and we need incorporated in translator logic in order to incorporate grammar we have to ensure many things the first is syntax analysis syntax is basic structure it's basically asking the question does the structure of the sentence look correct in English we could have an adverb followed by an adjective followed by a noun like very big cloud and then we have semantic analysis semantics is meaning and it asks the question does the sentence make sense in context if we don't follow this then we're just outputting gibberish language translation as to the chaos as we need to make sure the translated French sentence follows the similar roles clearly language is more complex than simply an assortment of tokens instead of trying to explicitly define our own grammar what if we let the machine's neural network do it for us if you haven't heard of neural networks don't worry about it too much just think of it as a component that learns to solve problems by looking at hundreds of thousands of examples this allows the network to learn patterns and data and eventually it would be able to translate a given English sentence to French all on its own now this sounds interesting but what exactly is this Network now we can actually derive the neural network architecture required based on the problem we are trying to solve in this case we need a neural network that solves a problem of language translation some English sentence is the input and it should spit out some French sentence the first thing you notice the input and outputs are both sentences or a sequence of words but computers don't understand sentences like humans do so we need to convert it into a form that they do understand and that's numbers more specifically vectors and matrices which are just an assortment of numbers representing data and so we have the first part of our network a sentence to vector mapper this part of the network takes an English sentence and spits out a vector of numbers that the computer can understand now this box here is a neural network and since we're dealing with sequences or sentences we use what's called a recurrent neural network now again if you haven't heard of a recurrent neural network think of it as a neural network that learns to solve problems that involve sentences since we're dealing with the problem of language translation and language translation requires sentences well we think recurrent neural network so we took her English sentence and with our current neural network we converted it into a vector now we need to convert this vector into a French sentence this vector to sentence mapping can be done with another network and once again since we're dealing with a sentence transformation we use another recurrent neural network and together these two recurrent neural nets make the bare-bones structure for our language translator what we've constructed here is a fundamental structure for the translation and it's called the encoder decoder architecture the first network encodes the English sentence 2 computer data and the second decodes the computer data to the French sentence but what are these boxes these are n ends exactly they are actually long short-term memory recurrent neural networks or LST M RN ends we use LST M shells specifically because they can deal with longer sentences fairly well it's a very interesting neural network that was conceived way back in the 90s as simple as it sounds this encoder decoder network with LST n cells is the basis of several papers and was a state-of-the-art network in 2014 not too long ago this was the first time recurrent neural networks became wildly successful for language translation in fact if we take a look at performance the x-axis here represents the number of words in the sentence and the y-axis is the blue score it's basically the accuracy of translation higher the blue score better as a performance so it looks like this encoder decoder architecture works well for medium length sentences with about 15 to 20 words let's see how this does with longer sentence translation with an example say we have an English sentence that we want to translate to French in admitting privilege is the right of a doctor to admit a patient to a hospital or a medical center to carry out a diagnosis or procedure based on his status as a healthcare worker at a hospital now this is a long winded sentence but a valid one it's saying a doctor has the right to admit a patient for further testing if we were to pass this into the LST RNN encoder/decoder that we talked about we would get this French translation I'm privileged a admission Malouda body a middle son dae-ho Quneitra bassiano Patel wasanta medical D and agnostic with the pond hand agnostic on Sunday Sunita s aunty now I don't know French so I can't directly verify how correct this is but let's pop this into Google Translate and see it's English translation a privilege of admission is the right of a physician to recognize a patient in the hospital or medical center of a diagnosis or to make a diagnosis according to his state of health by comparing this with the original we can see that the meaning of the sentence breaks just after the term medical center the phrase medical center of a diagnosis doesn't make much sense but still it's not bad it was able to keep up for about 20 words now let's try another one consider the English sentence this kind of experience is part of Disney's effort to extend the lifetime of its series and build new relationships with audiences via digital platforms that are becoming ever more important he added when popped into the Arnon encoder/decoder we get this french translation let's now once again pop this French translation into Google Translate and see what it spits out in English this type of experience is part of Disney's initiatives to extend the life of its news and develop links with digital players that are becoming more complex now first off it didn't generate a closing quotation mark instead of an audience with an online influencer they were addressed as digital players that's okay I guess but then it says the links are becoming more complex but that isn't the case in the original sentence where it says the relationship is becoming more important once again though not too bad but you can clearly see the quality of the model isn't quite optimal when translating much longer sentences so what can we do to improve this translation remember what I said before about language it has two components tokens and grammar and it is this grammar that makes language so complex the problem with the current model is that it's not entirely addressing this complexity the thing with recurrent neural networks is it's using past information to make decisions about the present this means that while generating the 10th word of a translation and French sentence it looks at the first nine words in the English source sentence but we know that a word not only depends on the words that come before it in a sentence but also the words that come after it in a sentence all this gives rise to the context of the word so in order to look in both directions forward and backward we replace the normal recurrent neural network with a bi-directional recurrent neural network interestingly these by rnns were introduced way back in 1993 but gained popularity recently with the emergence of deep learning so if we're performing English to French translation while joining some word in the French translation we are looking at words that come before it and the words that come after it sweet but which words exactly should we focus on more in a large sentence this could be difficult to figure out a method to figure this out was devised in a 2016 paper learning to jointly align and translate I'll explain what this is so don't worry consider an English sentence the agreement on European Economic Area was signed in August 1992 and this is the corresponding French translation locker solely space economy QoP is in yon many of socket revenues our translator will generate the translated French sentence one word at a time while generating some eighth word like EDA which words in the English sentence should be considered one solution would be for the eye French word consider the eye English word but then we get the old word word translator that we talked about in the beginning of the video and that's no fun since it's more complicated than this it needs to be something the translator learns on its own so given the English sentence and it's French translation our translator will try to align them in this example het day is lined up with the English words was unsigned really white means super aligned or more attention is focused on that English word while generating the French word while generating the French word europ\u00e9enne it looks like the only word it would consult is the English word European the same goes for oats the model learns to focus its attention only on the English word August while generating the French word boots in this way the model looks at thousands of other english sentences and their corresponding french translations and it learns which english words to focus its attention on while generating the words of the french translation this alignment is learned by an extra unit called an attention mechanism and it sits between the encoder and decoder so during translation an English sentence is fed to the encoder it's encoded into some vector which is just numbers the computer understands it's basically the same English sentence in the computer's eyes then we use an attention mechanism basically asking which French word will be generated by which English words the decoder will then generate the French translation one word at a time focusing its attention on the words determined by the attention mechanism so that's sweet this actually performs better than the original encoder/decoder architecture the sentence translation is now more closely aligned with the original Google translates AI works exactly like this the only difference is everything is scaled up by this I mean instead of using one LS TM for the encoder and decoder we use 8 and we do this because deeper networks help better model complex problems so this network is more capable of understanding the semantics of language and grammar just to recap on the final network you want to translate English to French you pass the English text or word by word to the encoder and it converts these words into a number of word vectors that's the numbers representing these words these are just numbers that represent the words themselves of the sentence these words are then just passed into an attention mechanism and this determines the English words to focus on while generating some french word this data is passed to the decoder which generates the translated French sentence one word at a time and that's it so if you understood this you understood how Google translates AI works so yeh just know that every time you use Google Translate from now on something not so magical is actually happening behind the scenes thank you guys so much for watching and if you liked the video it shows some love with the like and subscribe for more awesome content and I'll see you in the next one but bye",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "wDxTWp3KMMs",
        "video_title": "How does YouTube recommend videos? - AI EXPLAINED!",
        "position_in_playlist": 17,
        "description": "How does Youtube recommend videos? This is a question you\u2019ve all thought about at least once. There is much talk in the creator community that YouTube only values \u201cwatch time\u201d. But is this really true? In this video, I am going to break down google\u2019s paper on YouTube\u2019s recommender system. Along the way, we are going to look at different types of such recommender systems, listing the pros, cons and mode of Operation for each. Techie or not, you are going to learn a lot. \n\nSubscribe: https://www.youtube.com/c/CodeEmporium?sub_confirmation=1\n\nResources\n\n[1] Smarter everyday\u2019s youtube analysis: https://www.youtube.com/watch?v=1PGm8LslEb4\n[2] Markiplier\u2019s youtube is broken: https://www.youtube.com/watch?v=tPKXgrvdnaQ\n[3] Quora answer: https://www.quora.com/How-does-YouTubes-recommendation-algorithm-work\n[4] Google\u2019s 2017 paper : http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf\n[5] Candidate Sampling: https://www.tensorflow.org/extras/candidate_sampling.pdf\n[6] Simple blog to understand candidate sampling (big picture): https://www.linkedin.com/pulse/reducing-computational-cost-mathematically-shamane-siriwardhana/\n[7] Ecommerce Collaborative Filtering: https://pdfs.semanticscholar.org/7abd/05d0e17bdf24d7c7babad74b6118d91d84d7.pdf\n[8] Collaborative Filtering Stanford Lecture: https://www.youtube.com/watch?v=h9gpufJFF-0",
        "transcript": "a few days ago I was watching a video on the nature of YouTube's recommendation system I'm talking about YouTube's AI that recommends videos to you I'll play a clip of this video right now a couple of months ago I made a Twitter thread about some weird activity I saw online and after I posted that thread tons of engineers from many different tech companies reached out to me privately to tell me their stories my interest in all this started one day when I was scrolling on YouTube and the algorithm served up a pretty weird video for me to watch you know how the algorithm works right it looks at your past activity and tries to figure out what you could watch in the future that would keep you on the platform the longest it optimizes watch time the video is an excerpt from the channel smarter every day and in this video he goes on to explain how people are tailoring their videos to appeal to the algorithm so even if a videos of low quality it is still somehow picked up by the algorithm and recommended to a bunch of viewers he explains this as an attack on the YouTube algorithm and I highly recommend you watch his video for the full scoop but for this video I thought I would look at recommender systems from a more technical perspective from that of a deep learning engineer I'm not sure how much you know this as viewers but in the Creator community there's much talk going on about how YouTube tends to recommend videos with higher watch time and watch time is essentially just the length or duration for which a video is watched the idea is that longer the video is watched YouTube will push that up in the rankings and so more people will be able to see it that's great but is the algorithm really that simple does it only value watch time when recommending us videos and in this video we're going to actually take a look at just that check your non-techie you're gonna understand this video through and through even if you don't have any knowledge of recommender systems so if you want to learn more stay tuned so we're gonna talk about YouTube's recommendation system and more specifically deep learning and how neural networks fit in let's take a concrete yet simple example and see exactly how this is working under the hood say you're Susan you own this platform called the YouTube where users can watch videos let's also assume we only have five videos on the platform videos one through five every online platform has users and let's say that we have five of them two users a through e and now we have this little matric structure where each cell represents whether a user likes a particular video or not note a matrix is a 2d assortment of numbers and in this context each cell can take three types of values it takes one if the user liked the video a negative one if the user disliked the video and zero if the user hasn't watched the video and so hasn't rated the video now these five users have watched and rated whatever the videos they've seen we show the results here in this matrix we're making things simple by stating that if a person has watched a video they must either like or dislike a video they can't leave the video unrated a property of this matrix is sparsity every person has not watched every video on YouTube even in this mini matrix example so there is bound to be some empty values and a lot of them which I've labeled a zero here now that we have our data let's make some predictions lettuce predictive a user D will like or dislike video three now how do we do this traditionally it's done through a technique called collaborative filtering the intuition behind collaborative filtering is collaboration we use the ratings of older users to make predictions for newer users the same can be done for items we can use the ratings on older items to make predictions for newer items when I say items in this case items are the same as videos on the platform I'll use them interchangeably depending on whether we've used items or users we have a user user collaborative filtering and item item collaborative filtering I'll talk about both with a simple example now in user user collaborative filtering it's a two-step process first determine how similar other users are with this user D and then we use these users to predict whether D will like or dislike the video 3 the intuition here is that similar users will have similar interests so if there are users who have the same tastes as user D and like video 3 then the chances are that user D will also like video 3 we're making this prediction with numbers that's the only difference for the first step every user can be represented by the row of values this row of values is called a vector and we use the cosine similarity formula to compute the similarity between two vectors this is just how similarities are computed mathematically and it's only one single way there are other ways to also compute similarity the similarity score can take values in the range of well cos theta which ranges between negative 1 and positive 1 now why does cosine similarity work well it's a direct measure of how close two vectors are to each other if user a and user B are similar to each other then the angle between them will be small so if theta is 0 the users will have identical tastes and as the angle increases it means the two users tastes are more spread apart completely opposite vectors means that user a and user B have opposite tastes so if user a likes video 3 chances are that user B won't like it but while computing similarity between users we cannot just take the direct cosine similarity between the videos they like or dislike some users may be more critical than others so they tend to give more negative reviews now to account for this difference in grading we have to normalize or centralize the ratings this is done by computing the mean for every single user and then subtracting it from their corresponding ratings and this will now bring the average rating for every single user to zero from this example you can see that user B and user EE tend to be more lenient and give more positive ratings so in user ESA's that he dislikes video three this dislike is rated more heavily than their liking for the other videos in a similar way users C and D tend to be kind of strict in their ratings so when users he says they really like video two they really like video two and this like is rated more heavily than their dislike for the other videos I hope that makes sense now our goal is to determine if user D will like video three this way we can determine whether we should recommend the video to him or just not we normalize the user vectors already now the next step is computing the similarities between these vectors and user D and this is done with the cosine similarity that I mentioned before remember computing similarities is just to determine which users are more in line with user D so that we can make recommendations based on those users looks like users a and D tend to have opposite tastes and this is because user a really disliked video two while user D seemed to really enjoy it as for user B we can see that he agrees with D when it comes to the videos four and five user C and D agree with their dislike of video four and then we have user E whose similarity with user D can't really be attributed to a single video review at least in my opinion but I just calculated it like I did for the others and we actually see some agreement between users D and E next we make a prediction consider the users who rated video three in this case its user C and E now we take the weighted average as the predicted rating we get a value below zero remember the average user rating for all users is now zero so chances are that user D may not like video three so we don't necessarily to recommend the video to them since they chances are they don't like it what about video one though well we do the same thing we make the prediction based on all other users which is four of them here and we take the weighted average rating as the prediction once again we get the value which is less than zero so we don't recommend this video to user D either remember this type of collaborative filtering is called user user collaborative filtering it's called so because we are comparing different user interests in video 3 to predict a new users interest in video 3 it's really easy to implement right but there's a problem with this see that user D was a business magnate and he spread the word of this online platform called YouTube that's just amazing overnight the number of users increased from 5 to 500 and the next night to 5,000 and then by the end of the month I'm dealing with a million new users this is amazing but the big question now is how does my algorithm deal with this sudden burst well let's see when an algorithm wants to see if a user likes a particular video we have two steps to follow the first is determine how similar users are with this particular user and then the second step is we use these users to predict whether the current user will like or dislike the video the time to do the first step is heavily dependent on the number of users because of this an explosion and users will lead to a significant delay in the algorithm so user based collaborative filtering its pros are that well it's simple to implement whereas its major con is it doesn't scale well however there is another type of collaborative filtering technique called item based collaborative filtering it's quite easy to understand the mechanics because it's very similar to user based collaborative filtering for the sake of comparison we'll try to answer the same question as before let's predict if the user D will like or dislike the video 3 the first step is the same as we did with user based collaborative filter find the center cosign similarities but this time it's between items and not between the users themselves so in this case it's between video 3 and the other videos next we compute the weighted average rating this is done by considering all items rated by user D and like before we get a value that's less than zero so we don't recommend video 3 to user D notice that we do get actually similar results to the previous approach of user based collaborative filtering and this should usually be the case now why does item based collaborative filtering usually perform better than user based collaborative filtering well here's a few reasons items are much easier to categorize and users a user may be into Science and Technology entertainment videos educational videos or even comedy videos while a video itself doesn't belong to all of these genres so comparing them makes more sense than comparing something as complex as people users and another reason why item based collaborative filtering may be better is simply because that the number of videos or items on the platform doesn't really increase as fast as the number of new users on the platform so computing item Basin molarity x' is less computation heavy but even item item collaborative filtering still has a major problem sparsity the matrix of users versus items has a ton of zeros why is this though it's because users watch a fraction of videos and videos are only washed by a fraction of users now let's take a look at this problem more visually our platform called YouTube started with 5 users a through E and 5 videos 1 through 5 in user user collaborative filtering each user is represented by a 5 dimensional vector and each dimension of the vector is a measure of how much they liked or disliked each video geometrically we can plot these users as points in this space 1 now with item item collaborative filtering each item or video in this case is represent by a five dimensional vector each dimension of this vector is a measure of how much each user either liked or disliked the particular video here each point is a video in this space we'll call this space 2 we can use either one of these spaces to try recommending videos to users using one of the collaborative filtering techniques that we discussed the problem here is most of these points have no information on most of the axes so it should be possible to project them onto a much smaller space with not much loss of information right in simpler terms we can reduce the five dimensional user vectors in space one or the five dimensional item vectors or video vectors in space 2 to say some 2-dimensional vector in another space like space 3 by reducing dimensions we can increase computation power this is kind of like PCAs dimensionality reduction if you're familiar with machine learning you don't have to be but I'm just saying this reduction in dimensions is done by a technique called matrix factorization I won't get into the nitty gritties but I will explain at a high level so that you understand what it is and why we use it the user video matrix has tons of zeroes we establish this this is true with really high number of users or videos or both by projecting it onto a smaller space we increase the computation efficiency also both users and videos are projected onto the same space so we can compare them directly the result is that videos closer to certain users are likely to be recommended to them and we no longer need to depend on different videos themselves to rate a video or depend on different users themselves to help rate a user and this technique is the essence of matrix factorization and matrix factorization it uses something called SVD which is singular value decomposition and we break down a matrix into a product of three smaller matrices without loss of information the first matrix is set of user vectors in the new space the third Matrix is a set of video vectors in the new space but the second Matrix represents the strength of each dimension in this new space it's a diagonal matrix so only the diagonal elements are nonzero now some of you nerds out there just might be curious how does it find these three matrices it's by optimizing reconstruction loss between the original user item matrix and the product of these three matrices so the matrix factorization technique its major Pro is that it overcomes the scalability issue that collaborative filtering has we no longer care about the number of users and items on the platform just to rate a particular users interest in a particular item so yeah it's much more computation efficient but now we have another problem human interpretation using this technique we will be able to recommend videos to individuals but unlike collaborative filtering techniques the dimensions we are projecting to aren't well-defined remember they are mathematically determined while solving the optimization problem and they could be literally anything from genre to video length to something we may not even be able to comprehend as humans in other words matric factorization can recommend videos but it cannot tell us why it recommends the video this is vital information to drive new business decisions and without it we wouldn't know what improvements can be made to increase revenue for example on our platform so yeah that's actually a major con okay until now we've developed some recommender systems so let's briefly recapitulate each first off collaborative filtering in user user collaborative filtering the technique recommends videos to a user based on similar users where as an item item collaborative filtering if we recommend videos based on similar videos that a user liked the advantages of cloud route of filtering are it's pretty simple to implement whereas the cons of this are well you know YouTube we have thousands of new users and videos - every day so neither user-based nor item based collaborative filtering scale well to address this enter a matrix factorization they transpose users and items to the same space and so they can be directly compared the pros of this are well predictions are independent of the number of users and videos on the platform so it scales way better than collaborative filtering the cons however our users and items are transposed to a latent space an unknown space so essentially users are recommended videos but we have no idea why users are recommended these videos on top of all this we made a major assumption in the beginning if user watches a video they have to like or dislike it but in the real world on YouTube this is not the case users may comment rate subscribe or just watch the video and do nothing in fact most users do nothing so we need a mechanism that factors implicit feedback - I'll talk more about implicit feedback shortly but let's get back to the main question how does you to recommend videos what you're looking at right now is the main architecture that Google published way back in 2017 about their model architecture for how YouTube actually recommends videos and it's a two-stage process the vs. candidate generation and the second is ranking Canada Generation takes millions of videos on YouTube and filters out the potential videos that a user may like and the ranking part well takes these videos these hundreds of videos that were chosen from the first phase and sorts them in the order of relevance to the user these videos are then shown to the user let's now dive deeper into each of these processes until now we've only considered a physical click of the like or dislike button as a factor to recommend videos to users but like we mentioned before most users don't provide such explicit feedback so it makes sense to collect information about users ourselves based on this paper that was released by Google here are some implicit features YouTube now for while recommending videos one is watch history which is the videos that you watched in the recent past the second is search history which is a list of queries that you typed into YouTube's search bar to search for certain results then the third is users geographic location and this makes sense because if you're in Mexico we get more Spanish creators or videos for more Spanish creators and if you're watching from say India you're recommended more tech videos and then Nexus device type so we have mobiles tablets whether you're viewing from a laptop or desktop there's gender there's also age and there's video freshness and this is the age of the video I feel like this is significant because videos get the most views within the first two days of their upload moving forward I'll address these seven features as the user context because it's just a shorter name instead of saying the seven features that we described before I'll just use the phrase user context now we have this information in hand but we need to somehow feed this into a neural network if you don't know what a neural network is don't worry about it no need to know the details just know that is a magical black box that takes one type of variable as input and converts it to another type of variable at the output end for the video recommender this input will be the user features we just described the user context and the output would be the set of recommended videos but since a computer needs to deal with these we convert everything or re-encode everything into numbers and vectors let's consider the encoding of input to a vector for watch history we need to encode each of video into a fixed vector and then we take an average of all of these vectors to get a final vector for watch history of all the videos next is search history which we do pretty much the same thing we encode each query into a vector and then take the average of these queries to get the final query history vector geographic location embedding is just a vector representing the country of the user and all the other factors are just encoded as scalars we now take each of these vectors and scalars and concatenate them into one large vector and this final vector is the input to our network so the input to our network is sorted out that's great now for the output it's the set of recommended videos but how do we represent this in traditional deep learning we model each neuron as a probability the J DH neuron would be the probability of watching the video J completely it is well known as the full softmax method or better as just softmax but this would mean that the number of videos in the output layer be equal to the number of videos on YouTube and well that number can be in the millions it's way too large to compute so the big question how do we feasibly compute this and recommend videos the most common solution and the solution that YouTube uses here too is something called candidate sampling instead of considering every single video on the platform for recommendation we only consider a subset of say 100 or 200 videos we select these 100 or 200 videos by fancy term sampling videos from a distribution this concept gets very mathematical but I'll explain it in a big-picture way enough to understand what's going on and then I'll delve into specifics for the Nerds out there first off where do we get our training data from we monitor every user's activity over time and every time a user clicks on a video we have one training sample and at this point we would document the user context so we have a number of pairs of user contexts and the videos watched for every user context we construct the input vector X I remember user context is a fancy word that describes the seven features of the user that we discussed before and based on this user context we determine a set of videos to recommend to the person which we call s here mathematically the idea is to find the optimal set of videos that maximizes this equation yes here has a fixed size like 100 or 200 videos and s is the set of candidate videos that move on to the next stage that is the 100 or 200 videos that the algorithm thinks the user will like the end result is given a user's watch history search history location and other information and using that to maximize this equation we get the appropriate set of videos to recommend a user now that is good enough to know what's going on but if you're a math nerd keep listening Q is a distribution where Q of Y given X I is the probability of watching a set of videos Y given some user context X I in our training data users either watch a video or they don't so Q is a Bernoulli distribution that takes on two values during training one if the video was watched and 0 if it was not watched it's customary machine learning that while training we assume a parametric distribution and we'll call these parameters theta using our training data with our network we approximate this optimization and during test time we's a sample from p to give you a set of candidate videos when given context vector X okay end of the mat nerd sprint hope this candidate sampling is clear to an extent I'll leave links in the description for more details and also link to the paper too now once we have candidate videos we now need to go into the next step of YouTube's recommendation algorithm the ranking phase the big picture is that this involves sorting the candidate videos in the order of relevance each video is assigned a relevant score higher the score higher is the relevance for the user context and the video is pushed up higher onto the list now to compute this score YouTube uses hundreds of features some of these features are similar to user context we discussed before like watch history and search history some of the video specific features include how many videos has the user watched from this channel or when was the last time that you or watched a video on this topic also the score of a video is upgraded for every impression so if a user doesn't click on the recommended video then the score if this video for the user context is decreased and it drops down the list with such frequent updates to the score and hundreds of features to consider I think you can see why we need candidate sampling to weed out the less relevant videos before we rank videos it would be insane to just rank 100,000 or millions of videos every single time user clicked on a video now another big question how exactly are these scores computed you may have guessed this but the scores are directly proportional to watch time in fact in YouTube's ranking neural network these values are equal so higher the expected watch time of the video for a given user higher is its score off the bat I find this weird because it's obvious that longer videos have high raw expected watch time and so the ranking algorithm will likely recommend longer videos which is in alignment with what a number of creators are complaining about these days if you want more technical details on how the ranking algorithm predicts expected watch time put your nerd house on for a bit ok just put them on YouTube uses a technique called weighted logistic regression to come up with these scores it takes hundreds of features of the video and users input and it spits out the relevant score which is also the expected watch time of the video now why does this work let's build some intuition we want to assign a high score to a video if we believe the user will view it for a long time in other words if the odds of them watching a video for a long time are high we want to assign it a higher score the odds is proportional to the probability of watching a video it's actually the ratio of probability of watching a video to the probability of not watching it but here we are waiting each of the videos differently based on watch time so let's say that n is the number of candidate videos from candidate sampling which is like I don't know 100 or 200 or some large number then K is the number of videos which are actually watched by the user so n minus K will be the number of videos that were considered as candidates but not watched by the user now videos that are not watched don't have watch time so it is assigned to watch time of 1 second so the denominator just becomes the number of videos the user didn't click on but was recommended to them waited logistic regression maximizes the odds of seeing a video that is watched and minimizes the odds of seeing a video that is not watched this is similar to the philosophy of normal logistic regression but the difference here is well weights we weight the videos watched much more heavily than the weights of videos not watched simplifying this mathematically we can assume the number of videos watched is actually really less compared to the number of candidate videos and in the end we get the odds nearly equal to the expected watch time of a video for a given user now that's the basic math now put on your technical hats we're going to construct a neural network in a similar way as the candidate sampling Network during training the last fully connected layer is able to learn the logarithm of the odds or the log odds the logit so while testing we take the exponent of this value to get the odds just the raw odds of the user watching the video and this odds is the relevant score if the video for the given user and just as we proved it is also the expected watch time and that's the gist of YouTube's algorithm and the deep learning approach for building a recommender system so cool let's summarize everything that I just mentioned in this video we started out with the definition of a recommender system just using YouTube as context we wanted to details about the types of recommender systems one is collaborative filtering where we use ratings of different users to predict if a particular user will like a video but the disadvantage of this in a general store general situation is new users tend to join the platform at a very high rate so that doesn't scale well with users a solution is to use videos instead of users to determine or to make predictions about whether a user will like a particular item however here in YouTube users are continuously signing into the platform and videos are also continuously updated to the platform so collaborative filtering in general doesn't scale well regardless of whether it's with respect to users or items videos another problem is sparsity too much storage space too much computation complexity for the user video metrics a solution to this is to use the second major technique that we described matrix factorization this technique decomposes the user video matrix into three matrices geometrically it projects the user vector and a video vector into the same space so they can be directly compared we would recommend items whose video vectors are closer to the users vector the problem here is the users and the items are transposed to an unknown space we know the video to recommend to the user but we don't know why we are recommending this video to the user and then finally we introduced a deep learning technique to construct a recommendation system and explain YouTube's algorithm using this approach the recommendation system has actually splendid two parts the first is candidate sampling where we select hundreds of potentials videos from the millions of videos on the platform based on user interests and we also took a look at a list of features that the algorithm considers the second is the ranking algorithm where we assign a relevant score to each video and sword videos accordingly this is done using weighted logistic regression which is the weighted version of logistic regression where the weights of training videos are assigned based on watch time there are hundreds of features that the algorithm uses but YouTube has not made most of them public so our recommendations on YouTube solely based on watch time well that's actually not quite the case because in the two processes that we discussed the first is candidate sampling we actually get these candidate samples based on other factors such as what the user preview we saw before or what queries the user may have typed into the search bar but it's in the second phase of ranking that we do predominantly use watch time in order to generate the rankings of our videos and this is probably why you see longer content being recommended more than shorter content even though both of them are actually still based on watch history in search history and that's it I hope you guys liked the video hope you learnt about recommender systems in general and also how YouTube recommends videos using their deep learning techniques if you liked the video give it a like subscribe for more awesome content it took a long time to make so a subscribe would really be well appreciated keep up to date with my content and I will see you guys in the next one bye bye",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "C5G56N2AnQs",
        "video_title": "How to keep up with AI research?",
        "position_in_playlist": 18,
        "description": "AI is a one of the fastest progressing fields. 1000s of papers released every month, So how do we keep up?\n\nSUBSCRIBE to Code Emporium: https://www.youtube.com/c/CodeEmporium?sub_confirmation=1\nFor more videos on Data Science, Machine learning, and AI. \n\nREFERENCES\n\nArxiv-Sanity: Arxiv, but clean: http://arxiv-sanity.com  \nDataTau: Posts on AI from Blogs to Projects https://www.datatau.com/ \n\nFACEBOOK GROUPS\nDeep Learning & AI Applications: https://www.facebook.com/groups/112775596101336/?ref=group_browse_new\nAwesome AI Papers: https://www.facebook.com/groups/awesomeaipapers/?ref=group_browse_new\nMath for Data Sciences: https://www.facebook.com/groups/mathfordatascience/?ref=group_browse_new\nDeep Learning, Machine Learning, Data Sciences & AI: https://www.facebook.com/groups/1069997846490499/?ref=group_browse_new\nData Science: https://www.facebook.com/groups/DataScienceGroup/?ref=group_browse_new\nDeep Learning & AI: https://www.facebook.com/groups/cybots/?ref=group_browse_new\nData Science Python: https://www.facebook.com/groups/DataSciencePython/?ref=group_browse_new\nData Sciene with Python: https://www.facebook.com/groups/DataScienceWithPython/?ref=group_browse_new\nData Science Beginners: https://www.facebook.com/groups/648135408678836/?ref=group_browse_new\nBig Data & Statistics: https://www.facebook.com/groups/bigdatastatistics/?ref=group_browse_new\nMath for Data Sciences: https://www.facebook.com/groups/mathfordatascience/?ref=group_browse_new\nRobotics: https://www.facebook.com/groups/1539686832959717/?ref=group_browse_new\nBeginning Data Science, Statistics & Machine Learning: https://www.facebook.com/groups/995474220466742/?ref=group_browse_new\nData Science & Predictive Analytics: https://www.facebook.com/groups/DataMining/?ref=group_browse_new\n\n\nTWITTER\nAndrew Ng: https://twitter.com/AndrewYNg \nAndrej Karpathy: https://twitter.com/karpathy \nDeep Mind: https://twitter.com/DeepMindAI \nFei Fei Lee: https://twitter.com/drfeifei \nGoogleAI: https://twitter.com/GoogleAI \nGrant Sanderson (3blue1brown): https://twitter.com/3blue1brown \nIan GoodFellow: https://twitter.com/goodfellow_ian \nOpenAI: https://twitter.com/OpenAI \nKDNuggets: https://twitter.com/kdnuggets \nSiraj Raval: https://twitter.com/sirajraval \nTechCrunch: https://twitter.com/TechCrunch \nTensorflow: https://twitter.com/TensorFlow \nYann LeCun: https://twitter.com/ylecun \n\nREDDIT\nMachine Learning: https://www.reddit.com/r/MachineLearning/ \nLearn Machine Learning: https://www.reddit.com/r/learnmachinelearning/ \nDeep Learning: https://www.reddit.com/r/deeplearning/ \nData Science: https://www.reddit.com/r/datascience/ \nArtificial: https://www.reddit.com/r/artificial/\n\nBLOGS\nDeep Mind: https://deepmind.com/blog/ \nApple: https://machinelearning.apple.com \nTech Crunch: https://techcrunch.com/ \nNVIDIA: https://blogs.nvidia.com/ \n\nQUORA\nAjay Halthor: https://www.quora.com/profile/Ajay-Halthor \nSridhar Mahadevan: https://www.quora.com/profile/Sridhar-Mahadevan-6 \nZeeshan Zia: https://www.quora.com/profile/Zeeshan-Zia-1 \nAjit Rajashekaran: https://www.quora.com/profile/Ajit-Rajasekharan \nChris Luhrs: https://www.quora.com/profile/Chris-Luhrs \n\nYOUTUBE\nCode Emporium: https://www.youtube.com/c/CodeEmporium \nSiraj Raval: https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A \nTwo Minute Papers: https://www.youtube.com/user/keeroyz \nSentdex: https://www.youtube.com/user/sentdex \nArxiv Insights: https://www.youtube.com/channel/UCNIkB2IeJ-6AmZv7bQ1oBYg \n3Blue1Brown: https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw \nMark Jay: https://www.youtube.com/channel/UC2W0aQEPNpU6XrkFCYifRFQ \nRobert Miles: https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg",
        "transcript": "machine learning deep learning and AI have become the new buzzwords of the decade and everybody's hoppin on that bandwagon the number of papers published every month on archive has increased over eight fold since 2014 that's quite a bit and it makes you wonder as a reader how on earth can we keep up with research on my AI Channel I've made over 60 videos many of them tackling trending deep learning research and in this video i'll share my sources on keeping up with breaking machine learning and deep learning research so you can always be in the loop this is code emporium so let's get started I need to start out with the good old archive it's a repository of hundreds of thousands of papers in physics mathematics Computer Sciences and many other fields and it should be your go-to for AI research but look at this it's a mess it's really difficult to know what's important and what's not so it's best to go to archive sanity which is basically archive but cleaner you can view the most recent papers you can view the top papers these are papers with a large number of citations in other words these papers have been used in other research in the respective field and you can see the papers that people are talking about so what's the big hype this section is mostly new papers that introduce existing concepts and could eventually lead to another branch of research in the field basically archive sandy sheds a lot of time when you're trying to determine what paper should I read next oh and fun fact this repository was created by under its karpati because he was bored looks like the position of Director at AI at Tesla can become very monotonous oh and he has a YouTube channel on sweet cubing that I used to follow that's pretty neat so all you speak ubers out there check out Batman fest oh it's worth your while so archive sanity it's my go-to place to see what's poppin my go-to now for Big Ideas is Twitter it's important to follow people who tweet mostly AI I say this because following everyone that follows you can just decrease the quality of your feed I'd recommend starting out by following big people in AI like Coursera founder Andrew on computer vision pioneer faily the director of AI at Tesla under Scarpa thei young Lou Koons great to follow he's the man behind convolutional neural networks and you can also follow in Goodfellow the men behind generative adversarial networks in addition to these guys you can follow anyone interesting that you come across they could be tweeting about some projects they're working on or some ideas they just want to share it could be research work they just dropped or some are just bored in fact a recent video I made on the evolution of face generation research was based off a tweet by Ian good fellow he conveniently shared links to five landmark papers in modeling something called generative adversarial networks which is the technology used to generate those fake faces or faces of people who don't exist if you want to hear me explain these five papers in 12 minutes and you more about generative models like generative adversarial networks then check out the links in the description to my videos anyways I suggest creating a separate Twitter for just your AI following so that every tweet in your feed will be AI related so at this point you have archived sanity as your go-to for breaking AI research and Twitter as your go-to for big ideas in the field moving on YouTube and yes YouTube is considered social media for the most part lessen him a few channels that you should follow two minute papers as in the name this channel discusses new and trending research papers in about 5 minutes he explains these papers in a very easy to understand way that leaves you curious to learn more he states the abstract highlights the experiment and shows us results with really nice visuals no matter what your level of machine learning understanding is I definitely sub to two minute papers to know the most groundbreaking research when it drops next up we have three blue one brown this is traditionally a math Channel grant Sanderson the main man behind the scenes teaches mathematical concepts using stunning visuals for all you people aspiring to do something in the field of AI I highly recommend checking out his playlist titled the essence of calculus he explains concepts like limits and determinants in such a visual way that you look at them in a completely different light and these concepts are fundamental to many machine learning algorithms we see today so step 2 3 blue Ron Brown for establishing your math foundations you may think you know something but when you see his videos you'll probably think again Siraj of all of all the content creators i've mentioned this one is the most similar to my own channel in the sense that we both make content on AI Suraj has been on YouTube making content on AI for about three years at the time of making this video and he's the biggest on this list wizard of the week that's what he calls his followers regardless of what many say about him I personally like his content he has these weekly code challenges that are pretty fun and it gets you into the programming aspect of AI nowadays he targets more of a beginner intermediate level audience so I use this channel as a means for finding out breaking news in the field i watch his videos as an introduction to the concept and then read papers about it if I'm more curious for some cool programming projects in computer vision I would go check out centex he's been a creator for years now and he's the go-to guy for Python programming on YouTube but he's recently made a number of hands-on programming tutorials and machine learning you can check out his very cool playlist on self-driving cars that I thought was very impressive he'll walk you right through everything from the very basics so check them out if I were to put these guys on the line where the right is purely theory and the left side is purely application oriented then I put three blue one brown and two minute papers on the theory side sent Dex on the programming side and Suraj towards the middle slash programming side he's got some theoretical videos too though and there's also me for now I make quite theoretical videos explaining algorithms concepts and research papers in detail I do have some programming videos but not that much as I'm making this video but who knows how I'll evolve yes I'm aware I sound like a Pokemon so until now archive sanity used for breaking AI research Twitter for getting big ideas and YouTube for both AI research and tutorials on getting yourself up to speed on any concepts like YouTube another type of social media you can use for expanding your AI knowledge is Korra it's basically Yahoo Answers done right now I don't usually use this as a source of groundbreaking events in the fields of artificial intelligence but there are surprisingly some really high qualified AI scientists on the platform that know how to explain certain concepts let's mention a few Zi Shan Xia probably my favorite writer on the platform who isn't of course he consistently writes high-quality detailed answers on machine learning deep learning computer vision and other branches of artificial intelligence he has a PhD in computer vision so all you vision folks out there strongly recommend you follow him next up we have Chris lers he's my number two and is probably the most underrated data scientist on the platform I've been reading his answers and also see Shawn's answers through the last year and have learned so much doing so he has amazing answers on statistical concepts and very high quality content in general give him a follow his content speaks for himself Prasoon Goyal he answers more technical questions in machine learning and general artificial intelligence he's a frequent writer has great content a PhD student at UT Austin and a lot of my followers also follow him next up a jet fraud shokran at times very succinct but at other times very descriptive he's answered quite a number of questions on machine learning and natural language processing if you're interested in natural language processing then I strongly recommend you follow him sure either a Madhavan with the town over a hundred answers on the platform at the time of making this video he is not answered much on the platform at least compared to the other people I've mentioned however the quality and details of his answers to questions on machine learning is amazing reading his answers you can tell that he has experience last but not least Ajay Hathor yep me I write about machine learning deep learning data Sciences and artificial intelligence although I'm not as active now whoops there are many more people out there and this is just my personal list I value quality over quantity and I didn't want to just list out hundreds of names I recommend you follow these people look at who they are and who they follow and augment your list from there while we're on the topic of social media I'll also mention Facebook groups you may have already deleted your Facebook but there are some groups out there that are very helpful just note some of these groups may require you to answer a few questions before joining like why do you want to join or what will you contribute after joining as long as your chill you should be able to get into any of these groups if you don't see them all now it's fine I'll link everything in the description another great place to look for groundbreaking content is where it all starts tech blogs technically they are sources of information of all the hype on social media but there are so many blogs out there and each blog pumps out a ton of information so much so that it can be difficult to know what's important in the field and what isn't if you're a tech nerd and want to know about technology advancements big or small then add tech blogs to your source of research information and that's it that is my non exhaustive list of resources to recap use archived sanity for getting to know breaking AI research in the form of research papers use Twitter for getting big ideas for personal projects use YouTube for both understanding groundbreaking AI research and for tutorials on any concept it could be something as simple as water support vector machines for example use Korra for asking the big questions in understanding concepts and breaking research in machine learning deep learning statistics and data sciences you can be a part of a number of Facebook groups that specialize in artificial intelligence and deep learning so you can participate in forums and ask and answer questions in the field you can actually do something similar with reddit too and then there are tech blogs for the nerdy folks out there to keep up with research and technology and also how industries are making use of breaking AI research and that's the list now you know how to keep up with trends in the fields of AI remember the number one way to keep up is to subscribe to code Emporium at CES dojo because we do everything you will ever need to know ever the links to everything I mentioned is down in the description below the video thank you so much for watching until the end and I'll see you in the next one buh bye",
        "transcription_mode": "YouTube Transcript API"
    },
    {
        "video_id": "7IEEKvcudrA",
        "video_title": "Transformer Neural Net makes music! (JukeboxAI)",
        "position_in_playlist": 19,
        "description": "JukeboxAI can generate music in the voice of any artist with any style.\n\nPlease subscribe to keep me alive: https://www.youtube.com/c/CodeEmporium?sub_confirmation=1\n\nSPONSOR\nKite is a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\u2019re typing. I've been using Kite. Love it! https://www.kite.com/get-kite/?utm_medium=referral&utm_source=youtube&utm_campaign=codeemporium&utm_content=description-only\n\n\nREFERENCES\n[1] The blog for more info: https://openai.com/blog/jukebox/\n[2] The main paper: https://cdn.openai.com/papers/jukebox.pdf\n[3] Variational AutoEncoder Tutorial: https://jaan.io/what-is-variational-autoencoder-vae-tutorial/\n[4] Vector Quantized Variational AutoEncoders (VQ-VAE) - the main paper: https://arxiv.org/abs/1711.00937\n[5] Hierarchical Quantized AutoEncoders: https://arxiv.org/abs/2002.08111\n[6] Generating High quality images with VQ-VAE-2: https://arxiv.org/pdf/1906.00446.pdf\n[7] Disadvantage of Variational AutoEncoders is \"Posterior Collapse\". Learn more here: https://datascience.stackexchange.com/questions/48962/what-is-posterior-collapse-phenomenon\n[8] More reasoning on why posterior collapse occurs: https://papers.nips.cc/paper/9138-dont-blame-the-elbo-a-linear-vae-perspective-on-posterior-collapse.pdf",
        "transcript": "take me to church I'll worship like a dog at the shrine of your lies I'll tell you my sins so you can sharpen your knife offer me my deathless death good God let me give you my life no masters or kings when the ritual begins there is no sweeter innocence than our gentle sin in the madness and soil of that sad earthly scene only then I am human only then I am clean there's this AI that can generate music in the style of famous singers it can make it rapping Bruno Mars a rock-and-roll Katy Perry and so much more but how does it do this we're going to try to reconstruct this model intuitively and get into more details as we go in doesn't matter how much you know about AI you should just be able to walk away with some knowledge of how this works at any level of detail so let's get started so in the first pass we'll build this intuitively we have this AI that takes in some lyrics genre and an artist and it generates a song and it does so in fixed sized chunks the AI generates a chunk it can then take this chunk and use it to generate the next chunk of audio and it repeats this until we get the entire song this type of AI model takes in an audio sequence and generates the next audio sequence and hence it is a sequence to sequence model in deep learning literature transformer neural networks are the best at dealing with sequence data at least for now so we can replace the AI model with a transformer neural network pretty cool but we run into a problem when building this out this raw audio waveform it's huge you've seen songs on CDs have a sampling rate like forty four point one kilo Hertz that means that we use 44100 numbers just to represent one second of audio it's far too big for our model to handle so we need to compress the waveform while retaining the key aspects of the music and this compression is done through a type of neural network architecture called an autoencoder this autoencoder network can take in a raw waveform and learn to compress it and it can also learn to take a compressed audio and learn to decompress it back to the original waveform the jukebox uses the auto encoder and transformer together and we build this jukebox in two phases training and generation during the training phase we train the auto encoder to compress and decompress audio and then we train the transformer to take in some information about the song to generate and train it to generate a compressed vector representation of the song one chunk at a time during the generation phase though we use both of them together pass in the lyrics genre and artists to the transformer and have it generate the compressed vector 1 audio chunk at a time and we can pass each of these compressed vectors through the auto encoder to get back the raw audio waveforms stitch them together and we have the generated song if we build our jukebox out in this way it might sound something like this hey jukebox how the anonymous celebrity rap to lose yourself by Eminem music to my ears it sounds like this because we built this only based on intuition but there are some key details and modifications we need to make before this is usable and now let's take a look at these in pass to building the real architecture we're using an auto encoder to compress and decompress a waveform this is split between two parts of an auto encoder its encoder for compression and it's decoder for decompression during training the normal auto encoder seeks to minimize the reconstruction loss only it doesn't really care how we compress the vector as long as it's able to reconstruct the input as good as possible and so it learns some arbitrary function to compress and decompress data this is fine for training but it's a problem when generating music during generation we don't have the encoder and we're working with just a decoder since we don't know the function our auto encoder learned chances are if we pass in a compressed vector to the decoder this is going to generate gibberish particularly I'm talking about the last leg of the flow and that's why the anonymous rapping sounded so Shh or beautiful this is where variational auto-encoders are better like normal auto-encoders they minimize a reconstruction loss but with the added constraint that it learns to do so with a specific distribution typically like a standard Gaussian distribution in other words we know how the compression and decompression is happening so during the music generation phase we don't really have the encoder and we can sample a vector from the standard Gaussian distribution pass it to the decoder and it will give us some meaningful audio so replacing our auto encoder with a variational auto encoder let's see what we get with our jukebox hey jukebox have the anonymous celebrity rap to lose yourself by Eminem okay getting better are via learn to convert a continuous distribution of vectors to a sound but not all of these vectors produce meaningful and clear audio and this is one of the reasons why some muddled nests and unclarity is still there in our generated audio to remedy this we can have our VA e to learn to encode a specific set of discrete vectors to a clear sound this is the basis for vector quantized variational auto-encoders during training we pass in a raw audio the encoder compresses it to a vector and we determine the closest discrete vector and then decode that as the sound and during generation the output of our transformer will be mapped to one of these discrete vectors and we can get a meaningful audio output so the vector quantized V II's can get clearer sounds but we can make further improvements on this by using multiple vqv AES at different compression levels this draws inspiration from a study on hierarchical vector quantizer AES used in image compression the goal is to generate even more realistic images by using a hierarchy of two vqv AES so we have a top v QV AE and a bottom v QV AE both of these learn different compressed representations of the image the top-level vqv a II only learns about the global information of the image like the contours and the large strokes but the bottom level vqv AE is larger and it learns about local pixel information like texture shading and color gradients and both of these learn representations are fed into the bottom level decoder to reconstruct the original image the bottom representation is conditioned on the top representation so that it doesn't need to learn the entire representation itself it makes the best use of its space leading to stunning images during the generation phase so like this we have three levels of VQ v's that we train in our case atop a middle and a bottom the top is the most compressed and the bottom is the least compressed but we don't feed the top to the middle in the middle to the bottom like we saw in the image case this is because the researchers observe that the top and the middle were passing all the information to the bottom and that made them useless it's it was similar to just having a single vq via e and so they just trained three v QV II's separately and in parallel it's interesting how we went from auto-encoders to using variational autoencoders to vector quantizer variational autoencoders and then to a hierarchical structure of the same each of them making improvements to generate better and better audio here's the initial structure that we've built in past one but now this autom encoder is actually three v QV a ease with different compression levels but to have these representations interact with each other we introduced three transformers the top transformer takes in lyrics genre and artist information to give the top level compressed representation and the second transformer converts this to the middle compressed representation the third transformer converts it to the bottom compression representation and then we pass this into the bottom vqv a decoder that we trained previously to get a newly generated audio the song hey jukebox have the UH nonnamous celebrity rap to lose yourself by Eminem his palms are sweaty knees weak arms a heavy there's vomit on his sweater already mom's spaghetti he's nervous but on the surface he looks calm and ready to drop bombs but he keeps on forgetting this is the final architecture and the end of past - things are looking a lot more concrete now but how exactly are we training this thing so in past three let's start with the vq ve training and then move on to transformer training we start by training our 3 v QV II's in parallel raw audio is a continuous stream so we need to break it down into fixed sized chunks let's say that we're dealing with a 20 second piece of audio here the top layer we'll break it down into five chunks the mid layer we'll break it down into 10 chunks and the bottom layer let's say it breaks it onto 20 chunks we pass the chunks one at a time through the encoder to get these individual colored bars note that each color bar has the same vector dimensions despite the width being different this top blue bar for example represents like four seconds of audio encoded into a 64 dimensional vector this first purple bar in the middle is the first 2 seconds of audio encoded into a 64 dimensional vector this first bottom brown bar is just one second of audio that's encoded into a 64 dimensional vector passing in all these chunks through the encoder gives us the compressed representations now we perform vector quantization for each of these colored pellets we determine the closest codebook vector the codebook is a list of vectors the blue one here ran in the top level is closest to vector 3 the purple one is closest to the 5 and this magenta one is closest to the 4th and for the codebook lookup we replace each of these numbers with the actual corresponding codebook vector and then we just decode each vector one at a time to get back the audio chunks and stitch that to get the original signal during this training we want to minimize the reconstruction loss we want to learn these codebook vectors and we also have a commit loss to stabilize the encoder so that's the bulk of this hierarchical vqv a training now once all these are trained we train our transformers we take a piece of audio pass it through our three vqv AES to get the top middle and bottom vector quantized representations these are then used to train our transformers the Transformers have an encoder decoder architecture the top level transformer takes in the lyrics artist genre and other conditional information to generate some intermediate vectors the decoder then takes these vectors and a start token to just generate the first highly compressed vector that represents a part of your generated song this is compared to the top level vq v AE vectors to minimize the difference during training time and it does this sequentially generating one vector at a time by our decoder transformer now that's a top level transformer moving on to the mid level transformer this takes one of the top level vectors of the vqv a II take some lyrics some genre and artist information all to generate some encoding vectors and the decoder will use this to generate two vectors generated one at a time during training these vectors are compared to the mid-level vqv a representation to minimize the difference the input vector should represent two seconds of data but the transformer converted it into two vectors that represent one second of data each I say - assuming that the compression rate of the mid level v QV a is two times that of the top level in the actual implementation though the says four times as much but I think you get the idea the bottom level transformer works in a similar way it takes a vector generated from the mid-level vq v AE as input this along with lyrics and genre is encoded into vectors these are passed into the decoder to sequentially generate two vectors one at a time they are compared to the bottom vqv vectors to minimize that difference if the input corresponds to one second of audio the output would be two vectors corresponding to half a second of audio each and in this way we can pass in a number of audio clips - or transformers to train them sweet so now during generation time we would get lyrics genre and artist information all encoded into vectors pass it to our top level transformers to get a very compressed representations pass each of these to the mid level transformer to decompress and pass each of these to the bottom level transformer to further decompress and then pass each of these vectors to the bottom level vqv a decoder to generate audio chunks stitch them together and we get view music and that's it I hope this covered different levels of understanding there's still a lot of detail that I did leave out but if you made it this far I'll add some references down in the description below of research papers reference papers and blog posts that you can check out thanks for watching and I will see you soon bye bye",
        "transcription_mode": "YouTube Transcript API"
    }
]